{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# TPU Demo Notebook\n",
    "\n",
    "This notebook validates the Iris cluster's TPU functionality by running real JAX\n",
    "computations on TPU hardware.\n",
    "\n",
    "**Prerequisites:**\n",
    "- SSH tunnel to controller: `uv run python scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models tunnel`\n",
    "- Environment variables:\n",
    "  - `IRIS_CONTROLLER_ADDRESS=http://localhost:10000`\n",
    "  - `IRIS_WORKSPACE=/path/to/lib/iris`\n",
    "\n",
    "**Tests:**\n",
    "1. Basic TPU job - verify TPU provisioning\n",
    "2. JAX matrix multiplication - verify JAX execution\n",
    "3. Multi-device pmap - verify data parallelism\n",
    "4. Coscheduled multi-host job - verify distributed execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Connection\n",
    "\n",
    "Connect to the cluster via the SSH tunnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from iris.client import IrisClient\n",
    "from iris.cluster.types import (\n",
    "    CoschedulingConfig,\n",
    "    Entrypoint,\n",
    "    EnvironmentSpec,\n",
    "    ResourceSpec,\n",
    "    tpu_device,\n",
    ")\n",
    "from iris.rpc import cluster_pb2\n",
    "\n",
    "# Configuration\n",
    "TPU_TYPE = \"v5litepod-16\"  # 4 VMs per slice\n",
    "JOB_TIMEOUT = 600  # 10 minutes for TPU provisioning\n",
    "\n",
    "# Connect to the cluster\n",
    "controller_address = os.environ.get(\"IRIS_CONTROLLER_ADDRESS\", \"http://localhost:10000\")\n",
    "workspace_str = os.environ.get(\"IRIS_WORKSPACE\")\n",
    "if workspace_str is None:\n",
    "    raise RuntimeError(\n",
    "        \"IRIS_WORKSPACE not set. Set it to the iris project root:\\n\"\n",
    "        \"  export IRIS_WORKSPACE=/path/to/lib/iris\"\n",
    "    )\n",
    "workspace = Path(workspace_str)\n",
    "\n",
    "client = IrisClient.remote(controller_address, workspace=workspace)\n",
    "print(f\"Connected to cluster at {controller_address}\")\n",
    "print(f\"Workspace: {workspace}\")\n",
    "print(f\"TPU type: {TPU_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test1-header",
   "metadata": {},
   "source": [
    "## Test 1: Basic TPU Job\n",
    "\n",
    "Submit a simple job that runs on TPU and reports device info. This validates\n",
    "that the autoscaler can provision TPU slices and jobs execute correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpu_device_info():\n",
    "    \"\"\"Report TPU device information.\"\"\"\n",
    "    import jax\n",
    "    \n",
    "    devices = jax.devices()\n",
    "    print(f\"JAX devices: {len(devices)}\")\n",
    "    for i, device in enumerate(devices):\n",
    "        print(f\"  Device {i}: {device.device_kind} @ {device.platform}\")\n",
    "    \n",
    "    print(f\"\\nDefault backend: {jax.default_backend()}\")\n",
    "    print(f\"Local device count: {jax.local_device_count()}\")\n",
    "    \n",
    "    return {\n",
    "        \"device_count\": len(devices),\n",
    "        \"backend\": jax.default_backend(),\n",
    "        \"device_kinds\": [d.device_kind for d in devices],\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"Submitting TPU device info job (may trigger TPU provisioning)...\")\n",
    "print(f\"Note: First job can take 5-10 minutes while TPU slice provisions.\\n\")\n",
    "\n",
    "job = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(tpu_device_info),\n",
    "    name=\"tpu-device-info\",\n",
    "    resources=ResourceSpec(device=tpu_device(TPU_TYPE)),\n",
    "    environment=EnvironmentSpec(workspace=\"/app\"),\n",
    ")\n",
    "print(f\"Job submitted: {job.job_id}\")\n",
    "\n",
    "status = job.wait(timeout=JOB_TIMEOUT, stream_logs=True)\n",
    "state_name = cluster_pb2.JobState.Name(status.state)\n",
    "print(f\"\\nJob completed: {state_name}\")\n",
    "\n",
    "if status.state != cluster_pb2.JOB_STATE_SUCCEEDED:\n",
    "    raise RuntimeError(f\"Job failed: {status.error}\")\n",
    "\n",
    "print(\"\\nTest 1 PASSED: Basic TPU job executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test2-header",
   "metadata": {},
   "source": [
    "## Test 2: JAX Matrix Multiplication\n",
    "\n",
    "Run a real computation on TPU: matrix multiplication using JAX. This validates\n",
    "that JAX operations execute correctly on TPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_matmul():\n",
    "    \"\"\"Perform matrix multiplication on TPU.\"\"\"\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import time\n",
    "    \n",
    "    print(f\"Running on: {jax.default_backend()}\")\n",
    "    print(f\"Devices: {jax.device_count()}\")\n",
    "    \n",
    "    # Create large matrices for meaningful TPU utilization\n",
    "    n = 4096\n",
    "    print(f\"\\nCreating {n}x{n} matrices...\")\n",
    "    \n",
    "    key = jax.random.PRNGKey(42)\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    a = jax.random.normal(key1, (n, n))\n",
    "    b = jax.random.normal(key2, (n, n))\n",
    "    \n",
    "    # JIT-compile the matmul\n",
    "    @jax.jit\n",
    "    def matmul(x, y):\n",
    "        return jnp.dot(x, y)\n",
    "    \n",
    "    # Warmup (compilation)\n",
    "    print(\"Compiling...\")\n",
    "    _ = matmul(a, b).block_until_ready()\n",
    "    \n",
    "    # Benchmark\n",
    "    print(\"Running benchmark (10 iterations)...\")\n",
    "    times = []\n",
    "    for i in range(10):\n",
    "        start = time.perf_counter()\n",
    "        c = matmul(a, b).block_until_ready()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    flops = 2 * n**3 / avg_time / 1e12  # TFLOPS\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Matrix shape: ({n}, {n})\")\n",
    "    print(f\"  Avg time: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {flops:.2f} TFLOPS\")\n",
    "    print(f\"  Output shape: {c.shape}\")\n",
    "    print(f\"  Output sum: {float(jnp.sum(c)):.4e}\")\n",
    "    \n",
    "    return {\"tflops\": flops, \"avg_ms\": avg_time * 1000}\n",
    "\n",
    "\n",
    "print(\"Submitting JAX matmul job...\\n\")\n",
    "\n",
    "job = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(jax_matmul),\n",
    "    name=\"tpu-matmul\",\n",
    "    resources=ResourceSpec(device=tpu_device(TPU_TYPE)),\n",
    "    environment=EnvironmentSpec(workspace=\"/app\"),\n",
    ")\n",
    "print(f\"Job submitted: {job.job_id}\")\n",
    "\n",
    "status = job.wait(timeout=JOB_TIMEOUT, stream_logs=True)\n",
    "state_name = cluster_pb2.JobState.Name(status.state)\n",
    "print(f\"\\nJob completed: {state_name}\")\n",
    "\n",
    "if status.state != cluster_pb2.JOB_STATE_SUCCEEDED:\n",
    "    raise RuntimeError(f\"Job failed: {status.error}\")\n",
    "\n",
    "print(\"\\nTest 2 PASSED: JAX matrix multiplication executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test3-header",
   "metadata": {},
   "source": [
    "## Test 3: Multi-Device pmap\n",
    "\n",
    "Use JAX's `pmap` for data-parallel computation across multiple TPU chips.\n",
    "This validates that JAX can utilize all TPU cores on a single worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_pmap_demo():\n",
    "    \"\"\"Demonstrate pmap across TPU devices.\"\"\"\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    \n",
    "    n_devices = jax.local_device_count()\n",
    "    print(f\"Local devices: {n_devices}\")\n",
    "    print(f\"Devices: {[d.device_kind for d in jax.local_devices()]}\")\n",
    "    \n",
    "    # Define a simple parallel computation\n",
    "    @jax.pmap\n",
    "    def parallel_square(x):\n",
    "        return x ** 2\n",
    "    \n",
    "    # Create input data - one batch per device\n",
    "    batch_size = 1024\n",
    "    x = jnp.arange(n_devices * batch_size).reshape(n_devices, batch_size)\n",
    "    print(f\"\\nInput shape: {x.shape}\")\n",
    "    print(f\"Input (first 5 per device): {x[:, :5]}\")\n",
    "    \n",
    "    # Run parallel computation\n",
    "    y = parallel_square(x)\n",
    "    print(f\"\\nOutput shape: {y.shape}\")\n",
    "    print(f\"Output (first 5 per device): {y[:, :5]}\")\n",
    "    \n",
    "    # Verify results\n",
    "    expected = x ** 2\n",
    "    assert jnp.allclose(y, expected), \"pmap result mismatch!\"\n",
    "    print(\"\\nVerification: Results match expected values\")\n",
    "    \n",
    "    # Demonstrate psum (parallel sum)\n",
    "    @jax.pmap\n",
    "    def parallel_sum_reduce(x):\n",
    "        return jax.lax.psum(jnp.sum(x), axis_name='i')\n",
    "    \n",
    "    # Wrap in named axis for psum\n",
    "    @jax.pmap(axis_name='i')\n",
    "    def parallel_global_sum(x):\n",
    "        local_sum = jnp.sum(x)\n",
    "        global_sum = jax.lax.psum(local_sum, 'i')\n",
    "        return global_sum\n",
    "    \n",
    "    global_sums = parallel_global_sum(x)\n",
    "    print(f\"\\nGlobal sum via psum (each device sees): {global_sums[0]}\")\n",
    "    print(f\"Expected global sum: {jnp.sum(x)}\")\n",
    "    \n",
    "    return {\n",
    "        \"n_devices\": n_devices,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"global_sum\": float(global_sums[0]),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Submitting pmap demo job...\\n\")\n",
    "\n",
    "job = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(jax_pmap_demo),\n",
    "    name=\"tpu-pmap-demo\",\n",
    "    resources=ResourceSpec(device=tpu_device(TPU_TYPE)),\n",
    "    environment=EnvironmentSpec(workspace=\"/app\"),\n",
    ")\n",
    "print(f\"Job submitted: {job.job_id}\")\n",
    "\n",
    "status = job.wait(timeout=JOB_TIMEOUT, stream_logs=True)\n",
    "state_name = cluster_pb2.JobState.Name(status.state)\n",
    "print(f\"\\nJob completed: {state_name}\")\n",
    "\n",
    "if status.state != cluster_pb2.JOB_STATE_SUCCEEDED:\n",
    "    raise RuntimeError(f\"Job failed: {status.error}\")\n",
    "\n",
    "print(\"\\nTest 3 PASSED: Multi-device pmap executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test4-header",
   "metadata": {},
   "source": [
    "## Test 4: Coscheduled Multi-Host Job\n",
    "\n",
    "Submit a coscheduled job with multiple replicas. In a v5litepod-16 configuration,\n",
    "there are 4 VMs per slice. This test validates that:\n",
    "- All 4 tasks are scheduled on workers from the same TPU slice\n",
    "- Task indices are correctly assigned (0, 1, 2, 3)\n",
    "- All tasks can communicate their status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iris.cluster.types import get_tpu_topology\n",
    "\n",
    "def multi_host_task():\n",
    "    \"\"\"Task that runs as part of a coscheduled multi-host job.\"\"\"\n",
    "    import os\n",
    "    import jax\n",
    "    from iris.cluster.client import get_job_info\n",
    "    \n",
    "    info = get_job_info()\n",
    "    if info is None:\n",
    "        raise RuntimeError(\"Not running in Iris job context\")\n",
    "    \n",
    "    print(f\"=== Task {info.task_index} of {info.num_tasks} ===\")\n",
    "    print(f\"Worker ID: {info.worker_id}\")\n",
    "    print(f\"TPU worker ID: {os.environ.get('TPU_WORKER_ID', 'N/A')}\")\n",
    "    \n",
    "    # Report JAX device info\n",
    "    print(f\"\\nJAX backend: {jax.default_backend()}\")\n",
    "    print(f\"Local devices: {jax.local_device_count()}\")\n",
    "    \n",
    "    # Simple computation to verify TPU works\n",
    "    import jax.numpy as jnp\n",
    "    key = jax.random.PRNGKey(info.task_index)\n",
    "    x = jax.random.normal(key, (1000,))\n",
    "    result = float(jnp.sum(x ** 2))\n",
    "    print(f\"Computation result: {result:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"task_index\": info.task_index,\n",
    "        \"num_tasks\": info.num_tasks,\n",
    "        \"worker_id\": info.worker_id,\n",
    "        \"result\": result,\n",
    "    }\n",
    "\n",
    "\n",
    "# Get VM count for the TPU topology\n",
    "topo = get_tpu_topology(TPU_TYPE)\n",
    "replicas = topo.vm_count\n",
    "\n",
    "print(f\"TPU topology: {TPU_TYPE}\")\n",
    "print(f\"VMs per slice: {replicas}\")\n",
    "print(f\"Chips per VM: {topo.chips_per_vm}\")\n",
    "print(f\"\\nSubmitting coscheduled job with {replicas} replicas...\\n\")\n",
    "\n",
    "job = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(multi_host_task),\n",
    "    name=\"tpu-multi-host\",\n",
    "    resources=ResourceSpec(\n",
    "        device=tpu_device(TPU_TYPE),\n",
    "        replicas=replicas,\n",
    "    ),\n",
    "    environment=EnvironmentSpec(workspace=\"/app\"),\n",
    "    coscheduling=CoschedulingConfig(group_by=\"tpu-name\"),\n",
    ")\n",
    "print(f\"Job submitted: {job.job_id}\")\n",
    "\n",
    "status = job.wait(timeout=JOB_TIMEOUT, stream_logs=True)\n",
    "state_name = cluster_pb2.JobState.Name(status.state)\n",
    "print(f\"\\nJob completed: {state_name}\")\n",
    "\n",
    "if status.state != cluster_pb2.JOB_STATE_SUCCEEDED:\n",
    "    raise RuntimeError(f\"Job failed: {status.error}\")\n",
    "\n",
    "# Verify task placement\n",
    "print(\"\\nTask placement:\")\n",
    "for task in job.tasks():\n",
    "    task_status = task.status()\n",
    "    print(f\"  Task {task.task_index}: worker={task_status.worker_id}\")\n",
    "\n",
    "print(f\"\\nTest 4 PASSED: Coscheduled multi-host job ({replicas} tasks) completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All TPU tests completed. The cluster is functioning correctly for:\n",
    "- TPU slice provisioning via autoscaler\n",
    "- JAX execution on TPU hardware\n",
    "- Multi-device data parallelism (pmap)\n",
    "- Coscheduled multi-host distributed jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TPU DEMO COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCluster: {controller_address}\")\n",
    "print(f\"TPU type: {TPU_TYPE}\")\n",
    "print(f\"\\nAll 4 tests passed:\")\n",
    "print(\"  1. Basic TPU job\")\n",
    "print(\"  2. JAX matrix multiplication\")\n",
    "print(\"  3. Multi-device pmap\")\n",
    "print(\"  4. Coscheduled multi-host job\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
