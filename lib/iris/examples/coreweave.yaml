# Iris configuration for CoreWeave CKS clusters.
#
# Architecture: Shared NodePool model. `iris cluster start` creates one
# NodePool per scale group (plus a controller pool) with CoreWeave autoscaling
# enabled. Iris manages only Pods; CoreWeave manages node provisioning.
# NodePool names are derived from this config: {label_prefix}-{scale_group_name}.
# When Pods are deleted, NodePools scale to zero automatically.
#
# Workflow:
#
#   1. Set S3 object storage credentials (required for s3:// storage URIs):
#        export R2_ACCESS_KEY_ID=<your-r2-access-key-id>
#        export R2_SECRET_ACCESS_KEY=<your-r2-secret-access-key>
#      These are created in the Cloudflare dashboard under R2 > Manage R2 API Tokens.
#      `iris cluster start` creates a K8s Secret from these env vars automatically.
#
#   2. Start the cluster (creates RBAC, shared NodePools, ConfigMap, Deployment, Service):
#        iris --config=lib/iris/examples/coreweave.yaml cluster start
#
#   3. Use the Iris CLI:
#        iris --config=lib/iris/examples/coreweave.yaml cluster status
#        iris --config=lib/iris/examples/coreweave.yaml cluster dashboard
#
# This config file is used by:
#   - The CLI on the operator's laptop (for `cluster start`, `cluster status`, job submission)
#   - The controller and workers inside the cluster (mounted as ConfigMap at /etc/iris/config.yaml)
#
# To use a local kubeconfig (e.g. from CoreWeave Console > Tokens > Download):
#   Set platform.coreweave.kubeconfig_path below, or:
#   export KUBECONFIG=~/.kube/coreweave-iris

platform:
  label_prefix: iris
  coreweave:
    region: US-WEST-04A
    namespace: iris
    kubeconfig_path: ~/.kube/coreweave-iris  # Set this to your local kubeconfig, or use KUBECONFIG env var
    object_storage_endpoint: https://a5a1f66f973e2196a26bae21c81b899b.r2.cloudflarestorage.com

storage:
  bundle_prefix: s3://marin-test/iris/bundles
  log_prefix: s3://marin-test/iris/logs

controller:
  image: ghcr.io/marin-community/iris-controller:latest
  coreweave:
    port: 10000
    service_name: iris-controller-svc
    scale_group: cpu-erapids

defaults:
  autoscaler:
    evaluation_interval:
      milliseconds: 10000
    scale_up_delay:
      milliseconds: 60000
    scale_down_delay:
      milliseconds: 300000
    startup_grace_period:
      milliseconds: 2400000    # 40 min — covers autoscaler node provisioning + Pod startup
  worker:
    docker_image: ghcr.io/marin-community/iris-worker:latest
    port: 10001
    cache_dir: /mnt/local/iris-cache
    runtime: kubernetes
    default_task_image: ghcr.io/marin-community/iris-task:latest

scale_groups:
  # CPU general-purpose — used for data processing, orchestration, etc.
  cpu-erapids:
    accelerator_type: cpu
    num_vms: 1
    resources:
      cpu: 64
      ram: 256GB
      gpu_count: 0
      disk: 1TB
    worker:
      attributes:
        region: US-WEST-04A
        pool: cpu-erapids
    min_slices: 0
    max_slices: 1  # Cost-safe default; increase for production workloads
    priority: 100
    slice_template:
      accelerator_type: cpu
      num_vms: 1
      coreweave:
        region: US-WEST-04A
        instance_type: cd-gp-i64-erapids

  # 8x H100 with InfiniBand — primary training workhorse
  h100-8x:
    accelerator_type: gpu
    accelerator_variant: H100
    num_vms: 1
    resources:
      cpu: 128
      ram: 2048GB
      gpu_count: 8
      disk: 1TB
    worker:
      attributes:
        region: US-WEST-04A
        pool: h100-8x
    min_slices: 0
    max_slices: 1  # Cost-safe default; increase for production workloads
    priority: 50
    slice_template:
      accelerator_type: gpu
      accelerator_variant: H100
      num_vms: 1
      coreweave:
        region: US-WEST-04A
        instance_type: gd-8xh100ib-i128
