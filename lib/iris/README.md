# Iris

Iris is a distributed job orchestration and RPC framework designed to replace
Ray with simpler, more focused primitives. It provides job lifecycle management,
actor-based RPC communication, and task dispatch capabilities for distributed
Python workloads.

## Architecture Overview

Iris consists of four main components:

| Component | Description |
|-----------|-------------|
| **Controller** | Central coordinator managing job scheduling, worker registration, and service discovery |
| **Worker** | Execution agent that runs jobs in isolated containers with resource management |
| **Actor System** | RPC framework enabling Python object method invocation across processes |
| **WorkerPool** | High-level task dispatch abstraction for stateless parallel workloads |

```
┌─────────────────────────────────────────────────────────────────┐
│                         Controller                               │
│                                                                  │
│     Job Scheduling    │    Worker Registry    │  Endpoint Registry│
└─────────────────────────────────────────────────────────────────┘
        │                       │                       ▲
        │ dispatch              │ health                │ register
        ▼                       ▼                       │
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│     Worker      │     │     Worker      │     │   ActorServer   │
│                 │     │                 │     │   (in job)      │
│  runs jobs in   │     │  runs jobs in   │     │                 │
│  containers     │     │  containers     │     │                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘
```

## Directory Structure

```
src/iris/
├── actor/                    # Actor RPC system
│   ├── client.py            # Actor method invocation
│   ├── pool.py              # Multi-endpoint management
│   ├── resolver.py          # Endpoint discovery
│   └── server.py            # Actor hosting
├── client/                   # High-level client layer
│   ├── client.py            # IrisClient and IrisContext
│   ├── resolver.py          # ClusterResolver
│   └── worker_pool.py       # Task dispatch
├── cluster/                  # Cluster orchestration
│   ├── controller/          # Controller service
│   ├── worker/              # Worker service
│   └── client/              # Low-level cluster clients
├── proto/                    # Protocol definitions
└── *_pb2.py, *_connect.py   # Generated RPC code
```

## Component Documentation

- [Controller Overview](docs/controller.md) - Job scheduling and coordination
- [Worker Overview](docs/worker.md) - Job execution and container management
- [Actor System Overview](docs/actor.md) - RPC and service discovery

## Quick Start

### Submitting a Job

```python
from iris.client import IrisClient
from iris.cluster.types import Entrypoint, ResourceSpec

def my_task():
    print("Hello from iris!")

client = IrisClient.remote("http://controller:8080", workspace=Path("."))
job = client.submit(
    name="my-job",
    entrypoint=Entrypoint.from_callable(my_task),
    resources=ResourceSpec(cpu=1, memory="2GB"),
)
job.wait()  # Blocks until complete, raises JobFailedError on failure

# Access task-level information
for task in job.tasks():
    for entry in task.logs():
        print(entry.data)
```

### Running an Actor Server

```python
from iris.actor import ActorServer

class InferenceActor:
    def predict(self, data: list) -> list:
        return [x * 2 for x in data]

server = ActorServer(host="127.0.0.1")
server.register("inference", InferenceActor())
server.serve()
```

### Calling Actors

```python
from iris.actor import ActorPool
from iris.client.resolver import ClusterResolver

resolver = ClusterResolver("http://controller:8080")
pool: ActorPool = resolver.lookup("inference")
pool.wait_for_size(1)

result = pool.call().predict([1, 2, 3])
```

### Using WorkerPool for Task Dispatch

```python
from iris.client import IrisClient, WorkerPool, WorkerPoolConfig
from iris.cluster.types import ResourceSpec

client = IrisClient.remote("http://controller:8080", workspace=Path("."))
config = WorkerPoolConfig(num_workers=10, resources=ResourceSpec(cpu=2))
pool = WorkerPool(client, config)

futures = [pool.submit(process_shard, shard) for shard in shards]
results = [f.result() for f in futures]
pool.shutdown()
```

## Job Hierarchy and Name Prefixing

### Hierarchical Job IDs

Jobs form parent-child hierarchies. When a job launches sub-jobs, they become children:

```
Root job:     "abc123"                    (UUID generated by controller)
Child job:    "abc123/worker-0"           (parent_id/name)
Grandchild:   "abc123/worker-0/sub-task"  (nested hierarchy)
```

When a parent job terminates, all children are automatically terminated (cascade termination).

### Endpoint Name Prefixing

Actors register endpoints with simple names, but they're stored with a prefix derived from the root job ID:

```python
# Inside job "abc123/worker-0"
ctx.controller.endpoint_registry.register("calculator", "localhost:8080")
# Stored as: "abc123/calculator"
```

This provides automatic isolation:
- Jobs in the same hierarchy can discover each other's actors
- Jobs in different hierarchies cannot see each other's actors
- No explicit namespace parameter needed

### Resolution

```python
# Inside job "abc123/worker-0"
resolver = ctx.resolver
result = resolver.resolve("calculator")  # Looks up "abc123/calculator"
```

### IrisContext

The `IrisContext` available in job code provides convenient access:

```python
from iris.client import iris_ctx

ctx = iris_ctx()
print(f"Job ID: {ctx.job_id}")        # "abc123/worker-0"
print(f"Namespace: {ctx.namespace}")   # Namespace("abc123") - derived from root
print(f"Parent: {ctx.parent_job_id}")  # "abc123"
```

### Environment Variables

Jobs receive their identity via environment variables:

- `IRIS_JOB_ID`: Full hierarchical job ID (e.g., `"root-uuid/child-a/grandchild"`)

## Design Principles

1. **Shallow interfaces**: Components expose minimal APIs with clear responsibilities
2. **Explicit over implicit**: No magic discovery or hidden state synchronization
3. **Stateless workers**: Task retry and load balancing work because workers maintain no shared state
4. **Arbitrary callables**: Jobs and actor methods accept any picklable Python callable

## Related Documentation

- [Fray-Zero Design](docs/fray-zero.md) - Original design document and rationale
