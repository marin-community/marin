# Copyright 2025 The Marin Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Physical execution plan for zephyr pipelines.

Physical operations are generated by the planner from logical operations.
They encapsulate execution logic as callables, decoupling the backend from
knowledge of logical operation types.
"""

from __future__ import annotations

import heapq
import logging
import os
import zlib
from collections import defaultdict
from collections.abc import Callable, Iterable, Iterator
from dataclasses import dataclass, field
from enum import StrEnum, auto
from itertools import groupby, islice
from typing import TYPE_CHECKING, Any

import fsspec
import msgspec

from zephyr.dataset import (
    FilterOp,
    FlatMapOp,
    GroupByOp,
    JoinOp,
    LoadFileOp,
    MapOp,
    MapShardOp,
    ReduceOp,
    ReshardOp,
    SelectOp,
    TakePerShardOp,
    WindowOp,
    WriteOp,
)
from zephyr.readers import InputFileSpec

if TYPE_CHECKING:
    from zephyr.dataset import Dataset

logger = logging.getLogger(__name__)

# Default number of items per output chunk during streaming
DEFAULT_CHUNK_SIZE = 100_000


@dataclass
class SourceItem:
    """A source item with its shard assignment.

    The `data` field contains either:
    - InputFileSpec for LoadFileOp pipelines (file loading with optional chunking)
    - Raw data (int, dict, str, etc.) for from_list/from_iterable pipelines
    """

    shard_idx: int
    data: Any


@dataclass
class Map:
    """Transform stream → stream via composed callable.

    Attributes:
        fn: Composed function that transforms an iterator to an iterator
        requires_full_shard: True if any composed op needs full shard context
            (e.g., MapShardOp). When True, chunk parallelism is disabled.
    """

    fn: Callable[[Iterator], Iterator]
    requires_full_shard: bool = False


@dataclass
class Write:
    """Write stream to file, return path."""

    output_pattern: Callable[[int, int], str]  # (shard_idx, total_shards) → path
    writer_type: str  # For chunk aggregation: "jsonl", "parquet", "binary", "levanter_cache"
    skip_existing: bool = False
    # Writer-specific parameters
    levanter_metadata: dict | None = None
    schema: Any = None  # For parquet
    batch_size: int = 1000  # For parquet


@dataclass
class Scatter:
    """Distribute items to output shards by key hash."""

    key_fn: Callable[[Any], Any]  # item → key
    num_output_shards: int


@dataclass
class Reduce:
    """Merge sorted chunks and reduce per key."""

    key_fn: Callable[[Any], Any]
    reducer_fn: Callable[[Any, Iterator], Any]


@dataclass
class Fold:
    """Reduce stream to single value."""

    fn: Callable[[Iterator], Any]


@dataclass
class Reshard:
    """Redistribute chunks across shards."""

    num_shards: int


@dataclass
class Join:
    """Join two sorted streams."""

    fn: Callable[[Iterator, Iterator], Iterator]
    right_plan: PhysicalPlan | None = None


PhysicalOp = Map | Write | Scatter | Reduce | Fold | Reshard | Join


class StageType(StrEnum):
    """Type of stage execution."""

    WORKER = auto()  # Normal worker execution
    RESHARD = auto()  # Redistribute chunks (no worker execution)


def _map_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield fn(item)


def _filter_gen(stream: Iterator, predicate: Callable) -> Iterator:
    for item in stream:
        if predicate(item):
            yield item


def _flatmap_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield from fn(item)


def _reduce_gen(shard: Any, key_fn: Callable, reducer_fn: Callable) -> Iterator:
    for key, items_iter in _merge_sorted_chunks(shard, key_fn):
        yield reducer_fn(key, items_iter)


def _select_gen(stream: Iterator, columns: tuple[str, ...]) -> Iterator:
    cols_set = set(columns)
    for item in stream:
        yield {k: item[k] for k in cols_set if k in item}


def _load_file_gen(stream: Iterator) -> Iterator:
    from zephyr.readers import load_file

    for spec in stream:
        yield from load_file(spec)


def compose_map(operations: list) -> Callable[[Iterator], Iterator]:
    """Compose logical map/filter/flatmap ops into a single stream transformer.

    Args:
        operations: List of fusible logical ops (MapOp, FilterOp, LoadFileOp, etc.)

    Returns:
        A function that transforms an iterator to an iterator
    """

    def pipeline(stream: Iterator) -> Iterator:
        for op in operations:
            if isinstance(op, LoadFileOp):
                stream = _load_file_gen(stream)
            elif isinstance(op, MapOp):
                stream = _map_gen(stream, op.fn)
            elif isinstance(op, FilterOp):
                stream = _filter_gen(stream, op.predicate)
            elif isinstance(op, FlatMapOp):
                stream = _flatmap_gen(stream, op.fn)
            elif isinstance(op, MapShardOp):
                stream = op.fn(stream)
            elif isinstance(op, TakePerShardOp):
                stream = islice(stream, op.n)
            elif isinstance(op, WindowOp):
                stream = make_windows(stream, op.folder_fn, op.initial_state)
            elif isinstance(op, SelectOp):
                stream = _select_gen(stream, op.columns)
        return stream

    return pipeline


def compose_join(
    left_key_fn: Callable,
    right_key_fn: Callable,
    combiner_fn: Callable,
    join_type: str,
) -> Callable[[Iterator, Iterator], Iterator]:
    """Create a join function.

    Args:
        left_key_fn: Function to extract key from left items
        right_key_fn: Function to extract key from right items
        combiner_fn: Function to combine matched items
        join_type: "inner" or "left"

    Returns:
        Function that takes (left_stream, right_stream) and yields joined items
    """

    def join_fn(left: Iterator, right: Iterator) -> Iterator:
        return _sorted_merge_join(left, right, left_key_fn, right_key_fn, combiner_fn, join_type)

    return join_fn


@dataclass
class PhysicalStage:
    """A stage in the plan (operations between shuffle boundaries).

    A stage contains a sequence of physical operations that can be executed
    together. The stage_type tells the backend HOW to execute it:
    - WORKER: Normal worker execution
    - RESHARD: Redistribute chunks across shards (no worker execution)
    """

    operations: list[PhysicalOp] = field(default_factory=list)
    stage_type: StageType = StageType.WORKER
    output_shards: int | None = None


@dataclass
class PhysicalPlan:
    """Executable plan computed from a Dataset."""

    # inputs to the computation
    source_items: list[SourceItem]

    # list of stages to execute
    stages: list[PhysicalStage]

    @property
    def num_shards(self) -> int:
        """Total number of logical shards in the plan.

        This counts unique shard_idx values, not total SourceItems. When
        intra-shard parallelism is enabled, multiple SourceItems may belong
        to the same logical shard.
        """
        if not self.source_items:
            return 0
        return len({item.shard_idx for item in self.source_items})

    @property
    def num_chunks(self) -> int:
        """Total number of chunks across all shards."""
        return len(self.source_items)


@dataclass(frozen=True)
class ExecutionHint:
    """Hints for pipeline execution.

    Attributes:
        chunk_size: Number of items per output chunk during streaming. Use -1 for
            1 chunk per shard.
    """

    chunk_size: int = DEFAULT_CHUNK_SIZE


@dataclass
class FusionState:
    """Incremental state for fusing logical operations into physical stages."""

    stages: list[PhysicalStage] = field(default_factory=list)
    current_ops: list[PhysicalOp] = field(default_factory=list)
    pending_fusible: list = field(default_factory=list)
    output_shards: int | None = None
    stage_type: StageType = StageType.WORKER

    def flush_pending(self) -> None:
        """Convert pending fusible ops to a physical Map."""
        if not self.pending_fusible:
            return

        requires_full_shard = any(isinstance(op, MapShardOp) for op in self.pending_fusible)
        self.current_ops.append(
            Map(
                fn=compose_map(self.pending_fusible[:]),
                requires_full_shard=requires_full_shard,
            )
        )
        self.pending_fusible = []

    def add_op(
        self,
        op: PhysicalOp,
        *,
        output_shards: int | None = None,
        stage_type: StageType | None = None,
    ) -> None:
        """Add physical op to current stage.

        Flushes any pending fusible ops to a Map first, then adds this op.
        """
        self.flush_pending()
        self.current_ops.append(op)
        if output_shards is not None:
            self.output_shards = output_shards
        if stage_type is not None:
            self.stage_type = stage_type

    def end_stage(self) -> None:
        """Flush pending ops and close current stage."""
        self.flush_pending()
        if self.current_ops:
            self.stages.append(
                PhysicalStage(
                    operations=self.current_ops[:],
                    stage_type=self.stage_type,
                    output_shards=self.output_shards,
                )
            )
            self.current_ops = []
            self.output_shards = None
            self.stage_type = StageType.WORKER

    def finalize(self) -> list[PhysicalStage]:
        """Flush remaining ops and return completed stages."""
        self.end_stage()
        return self.stages


def _fuse_operations(operations: list, hints: ExecutionHint | None = None) -> list[PhysicalStage]:
    """Fuse logical operations into physical stages.

    Transforms logical ops into physical ops:
    - Fusible ops (MapOp, FilterOp, LoadFileOp, etc.) → Map(fn=compose_map([...]))
    - WriteOp → Write(...)
    - GroupByOp → Scatter + [shuffle] + Reduce
    - ReduceOp → Fold + [reshard to 1] + Fold
    - ReshardOp → Reshard
    - JoinOp → Join (with pre-computed right_plan)

    Args:
        operations: List of logical operations
        hints: Execution hints (used for pre-computing join right plans)

    Returns:
        List of PhysicalStages with physical operations and execution metadata
    """
    if not operations:
        return []

    if hints is None:
        hints = ExecutionHint()

    state = FusionState()

    for op in operations:
        if isinstance(op, WriteOp):
            state.add_op(
                Write(
                    output_pattern=op.output_pattern,
                    writer_type=op.writer_type,
                    skip_existing=op.skip_existing,
                    levanter_metadata=op.levanter_metadata,
                    schema=op.schema,
                    batch_size=op.batch_size,
                )
            )

        elif isinstance(op, GroupByOp):
            num_shards = op.num_output_shards if op.num_output_shards is not None else -1
            state.add_op(
                Scatter(key_fn=op.key_fn, num_output_shards=num_shards),
                output_shards=num_shards if num_shards > 0 else None,
            )
            state.end_stage()
            state.add_op(Reduce(key_fn=op.key_fn, reducer_fn=op.reducer_fn))

        elif isinstance(op, ReduceOp):
            state.add_op(Fold(fn=op.local_reducer))
            state.end_stage()
            state.add_op(Reshard(num_shards=1), output_shards=1, stage_type=StageType.RESHARD)
            state.end_stage()
            state.add_op(Fold(fn=op.global_reducer))

        elif isinstance(op, ReshardOp):
            state.end_stage()
            state.add_op(Reshard(num_shards=op.num_shards), output_shards=op.num_shards, stage_type=StageType.RESHARD)
            state.end_stage()

        elif isinstance(op, JoinOp):
            right_plan = compute_plan(op.right_dataset, hints)
            state.add_op(
                Join(
                    fn=compose_join(op.left_key_fn, op.right_key_fn, op.combiner_fn, op.join_type),
                    right_plan=right_plan,
                )
            )

        else:
            # Fusible ops: LoadFileOp, MapOp, FilterOp, FlatMapOp, MapShardOp, TakePerShardOp, WindowOp, SelectOp
            state.pending_fusible.append(op)

    return state.finalize()


def _compute_file_pushdown(
    paths: list[str],
    load_op: LoadFileOp,
    operations: list,
) -> tuple[list[SourceItem], list]:
    """Create source items for file pipeline with pushdown optimizations applied.

    Args:
        paths: List of file paths to load
        load_op: The LoadFileOp specifying format and default columns
        operations: Full operations list (first op is LoadFileOp)

    Returns:
        Tuple of (source_items, remaining_operations), where filter/select have been pushed down.
    """
    filter_expr = None
    select_columns = load_op.columns
    ops_to_skip: set[int] = set()

    # We don't try to do anything fancy, e.g. AND multiple filters.
    # We can however aggregate select operations, though this is obviously unusual.

    for i, op in enumerate(operations):
        if isinstance(op, FilterOp) and op.expr is not None and filter_expr is None:
            filter_expr = op.expr
            ops_to_skip.add(i)
        elif isinstance(op, SelectOp) and select_columns is None:
            select_columns = list(op.columns)
            ops_to_skip.add(i)
        elif isinstance(op, FilterOp) and op.expr is None:
            continue  # Lambda filter, can't push down
        elif isinstance(op, (MapOp, FlatMapOp)):
            break  # Transform ops stop pushdown
        else:
            break

    # Create InputFileSpecs with final columns/filter
    source_items = [
        SourceItem(
            shard_idx=i,
            data=InputFileSpec(
                path=path,
                format=load_op.format,
                columns=select_columns,
                filter_expr=filter_expr,
            ),
        )
        for i, path in enumerate(paths)
    ]

    # Build final operations list: LoadFileOp + remaining ops
    final_ops = [load_op] + [op for i, op in enumerate(operations) if i not in ops_to_skip]

    return source_items, final_ops


def compute_plan(dataset: Dataset, hints: ExecutionHint = ExecutionHint()) -> PhysicalPlan:
    """Compute physical execution plan from logical dataset."""
    operations = list(dataset.operations)

    if operations and isinstance(operations[0], LoadFileOp):
        source_items, operations = _compute_file_pushdown(
            list(dataset.source),
            operations[0],
            operations[1:],
        )
    else:
        source_list = list(dataset.source)
        source_items = [SourceItem(shard_idx=i, data=item) for i, item in enumerate(source_list)]

    stages = _fuse_operations(operations, hints)
    return PhysicalPlan(source_items=source_items, stages=stages)


@dataclass
class ChunkHeader:
    """Metadata for a chunk being streamed from a worker."""

    shard_idx: int
    count: int


@dataclass
class Chunk:
    """A single chunk of data with count metadata."""

    count: int
    data: Any  # The actual ref or raw data


def deterministic_hash(obj: object) -> int:
    """Compute a deterministic hash for an object."""
    s = msgspec.msgpack.encode(obj, order="deterministic")
    return zlib.adler32(s)


def make_windows(
    items: Iterable,
    folder_fn: Callable[[object, Any], tuple[bool, object]],
    initial_state: object,
) -> Iterator[list]:
    """Window items using a folder function.

    Args:
        items: Items to window
        folder_fn: Function (state, item) -> (should_continue, new_state)
        initial_state: Initial state for the folder function

    Yields:
        Windows of items (window closes when folder returns False)
    """
    window: list = []
    state = initial_state

    for item in items:
        should_continue, new_state = folder_fn(state, item)

        if not should_continue and window:
            # Close current window and start new one with this item
            yield window
            window = [item]
            state = initial_state
            # Re-apply folder with the item in the new window
            _, state = folder_fn(state, item)
        else:
            # Add item to current window
            window.append(item)
            state = new_state

    if window:
        yield window


def _group_items_by_hash(
    items: Iterable,
    key_fn: Callable,
    num_output_shards: int,
    chunk_size: int,
) -> dict[int, list[Chunk]]:
    """Group items by hash of key into num_output_shards target shards with sorted chunks.

    Args:
        items: Items to group
        key_fn: Function to extract grouping key from item
        num_output_shards: Number of output shards to distribute across
        chunk_size: Number of items per chunk

    Returns:
        Dict mapping shard index to list of chunks for that shard
    """
    output_chunks: dict[int, list[Chunk]] = defaultdict(list)
    output_tmp: dict[int, list] = defaultdict(list)

    for item in items:
        key = key_fn(item)
        target_shard = deterministic_hash(key) % num_output_shards
        output_tmp[target_shard].append(item)
        if chunk_size > 0 and len(output_tmp[target_shard]) >= chunk_size:
            sorted_items = sorted(output_tmp[target_shard], key=key_fn)
            output_chunks[target_shard].append(Chunk(count=len(sorted_items), data=sorted_items))
            output_tmp[target_shard] = []

    # Add all remaining chunks
    for target_shard, shard_items in output_tmp.items():
        if shard_items:
            sorted_items = sorted(shard_items, key=key_fn)
            output_chunks[target_shard].append(Chunk(count=len(sorted_items), data=sorted_items))

    return output_chunks


def _merge_sorted_chunks(shard, key_fn: Callable) -> Iterator[tuple[object, Iterator]]:
    """Merge sorted chunks using k-way merge, yielding (key, items_iterator) groups.

    Each chunk is assumed to be sorted by key. This function performs a k-way merge
    across all chunks and groups consecutive items with the same key.

    Args:
        shard: Shard containing sorted chunks (iterable of chunk lists)
        key_fn: Function to extract key from item

    Yields:
        Tuples of (key, iterator_of_items) for each unique key
    """
    chunk_iterators = []
    for chunk_data in shard.iter_chunks():
        chunk_iterators.append(iter(chunk_data))

    # Use heapq.merge to k-way merge sorted streams
    merged_stream = heapq.merge(*chunk_iterators, key=key_fn)
    yield from groupby(merged_stream, key=key_fn)


def _sorted_merge_join(
    left_stream: Iterable,
    right_stream: Iterable,
    left_key_fn: Callable,
    right_key_fn: Callable,
    combiner_fn: Callable,
    join_type: str,
) -> Iterator:
    """Perform a sorted merge join between two streams.

    Args:
        left_stream: Iterator of items from left side
        right_stream: Iterator of items from right side
        left_key_fn: Function to extract key from left items
        right_key_fn: Function to extract key from right items
        combiner_fn: Function to combine matched items
        join_type: "inner" or "left"

    Yields:
        Joined items according to join_type
    """
    # Materialize left stream and tag both streams
    left_items = list(left_stream)
    left_tagged = (("left", left_key_fn(item), item) for item in left_items)
    right_tagged = (("right", right_key_fn(item), item) for item in right_stream)

    # Merge both sorted streams by key
    merged = heapq.merge(left_tagged, right_tagged, key=lambda x: x[1])

    # Group by key and apply join logic
    for _key, group in groupby(merged, key=lambda x: x[1]):
        left_group: list[Any] = []
        right_group: list[Any] = []
        for side, _, item in group:
            (left_group if side == "left" else right_group).append(item)

        if join_type == "inner":
            if left_group and right_group:
                for left_item in left_group:
                    for right_item in right_group:
                        yield combiner_fn(left_item, right_item)
        elif join_type == "left":
            for left_item in left_group:
                if right_group:
                    for right_item in right_group:
                        yield combiner_fn(left_item, right_item)
                else:
                    yield combiner_fn(left_item, None)


@dataclass
class StageContext:
    """Context for executing a stage on a shard.

    Encapsulates all the metadata and auxiliary data needed to process a shard.

    Attributes:
        shard: The shard data to process (iterable, typically a Shard object)
        shard_idx: Index of this shard
        total_shards: Total number of shards
        chunk_size: Number of items per output chunk
        storage_path: Base path for spilling chunks (e.g., gs://bucket/tmp/job_xxx/stage_0)
        spill_threshold_bytes: Size threshold for spilling to storage
        aux_shards: Auxiliary shards for joins, keyed by op index
    """

    shard: Any  # Shard object (avoids circular import)
    shard_idx: int
    total_shards: int
    chunk_size: int
    storage_path: str
    spill_threshold_bytes: int
    aux_shards: dict[int, list[Any]] = field(default_factory=dict)

    def get_right_shard(self, op_index: int) -> Any:
        """Get right shard for join at given op index.

        Raises:
            ValueError: If no right shard is provided for the join
        """
        shards = self.aux_shards.get(op_index, [])
        if len(shards) != 1:
            raise ValueError(f"Expected exactly 1 right shard for join at op index {op_index}, got {len(shards)}")
        return shards[0]


def _collect_chunks(
    items: Iterator,
    shard_idx: int,
    chunk_size: int,
    storage_path: str,
    spill_threshold_bytes: int,
) -> list[tuple[ChunkHeader, Any]]:
    """Collect items into chunks and write via ChunkWriter.

    Returns list of (header, ChunkRef) where ChunkRef is InlineRef or StorageRef.
    """
    from zephyr.storage import ChunkWriter

    results: list[tuple[ChunkHeader, Any]] = []
    current_chunk: list = []
    chunk_idx = 0

    for item in items:
        current_chunk.append(item)

        if chunk_size > 0 and len(current_chunk) >= chunk_size:
            # Flush current chunk
            if current_chunk:
                spill_path = f"{storage_path}/shard_{shard_idx:05d}_chunk_{chunk_idx:05d}.vortex"
                writer = ChunkWriter(spill_path=spill_path, spill_threshold_bytes=spill_threshold_bytes)
                for chunk_item in current_chunk:
                    writer.write(chunk_item)
                ref = writer.finish()
                results.append((ChunkHeader(shard_idx=shard_idx, count=len(current_chunk)), ref))
                current_chunk = []
                chunk_idx += 1

    # Flush remaining items
    if current_chunk:
        spill_path = f"{storage_path}/shard_{shard_idx:05d}_chunk_{chunk_idx:05d}.vortex"
        writer = ChunkWriter(spill_path=spill_path, spill_threshold_bytes=spill_threshold_bytes)
        for chunk_item in current_chunk:
            writer.write(chunk_item)
        ref = writer.finish()
        results.append((ChunkHeader(shard_idx=shard_idx, count=len(current_chunk)), ref))

    return results


def run_stage(
    ctx: StageContext,
    ops: list[PhysicalOp],
) -> list[tuple[ChunkHeader, Any]]:
    """Execute a stage's physical ops in a single pass.

    This is the single worker function that backends call to execute physical ops.
    It only knows about physical op types (Map, Write, etc.) - not logical ops.

    Args:
        ctx: Stage execution context providing shard data and metadata
        ops: List of physical operations to execute in sequence

    Returns:
        List of (ChunkHeader, ChunkRef) tuples where ChunkRef is InlineRef or StorageRef
    """
    from zephyr.writers import write_binary_file, write_jsonl_file, write_levanter_cache, write_parquet_file

    stream: Iterator = iter(ctx.shard)

    op_index = 0
    while op_index < len(ops):
        op = ops[op_index]

        if isinstance(op, Map):
            stream = op.fn(stream)
            op_index += 1

        elif isinstance(op, Write):
            output_path = op.output_pattern(ctx.shard_idx, ctx.total_shards)

            if op.skip_existing:
                fs = fsspec.core.url_to_fs(output_path)[0]
                if op.writer_type == "levanter_cache":
                    test_path = os.path.join(output_path, ".success")
                else:
                    test_path = output_path

                if fs.exists(test_path):
                    logger.info(f"Skipping write, output exists: {output_path}")
                    return _collect_chunks(
                        iter([output_path]), ctx.shard_idx, ctx.chunk_size, ctx.storage_path, ctx.spill_threshold_bytes
                    )

            # Write based on type
            if op.writer_type == "jsonl":
                result = write_jsonl_file(stream, output_path)["path"]
            elif op.writer_type == "parquet":
                result = write_parquet_file(stream, output_path, op.schema, op.batch_size)["path"]
            elif op.writer_type == "levanter_cache":
                metadata = op.levanter_metadata if op.levanter_metadata is not None else {}
                result = write_levanter_cache(stream, output_path, metadata)["path"]
            elif op.writer_type == "binary":
                result = write_binary_file(stream, output_path)["path"]
            elif op.writer_type == "vortex":
                from zephyr.writers import write_vortex_file

                result = write_vortex_file(stream, output_path)["path"]
            else:
                raise ValueError(f"Unknown writer_type: {op.writer_type}")

            return _collect_chunks(
                iter([result]), ctx.shard_idx, ctx.chunk_size, ctx.storage_path, ctx.spill_threshold_bytes
            )

        elif isinstance(op, Scatter):
            # Hash items to output shards
            num_output_shards = op.num_output_shards if op.num_output_shards > 0 else ctx.total_shards
            output_chunks = _group_items_by_hash(stream, op.key_fn, num_output_shards, ctx.chunk_size)

            # Collect chunks for each output shard
            results: list[tuple[ChunkHeader, Any]] = []
            for target_shard_idx in range(num_output_shards):
                if output_chunks[target_shard_idx]:
                    for chunk in output_chunks[target_shard_idx]:
                        # Scatter chunks are already materialized, wrap as InlineRef
                        from zephyr.storage import InlineRef

                        ref = InlineRef(data=chunk.data)
                        results.append((ChunkHeader(shard_idx=target_shard_idx, count=chunk.count), ref))
                else:
                    # Empty chunk so controller knows this shard exists
                    from zephyr.storage import InlineRef

                    results.append((ChunkHeader(shard_idx=target_shard_idx, count=0), InlineRef(data=[])))
            return results

        elif isinstance(op, Reduce):
            # Merge sorted chunks and reduce per key
            stream = _reduce_gen(ctx.shard, op.key_fn, op.reducer_fn)
            op_index += 1

        elif isinstance(op, Fold):
            # Reduction to single value
            result = op.fn(stream)
            stream = iter([result])
            op_index += 1

        elif isinstance(op, Reshard):
            # Reshard is handled by the backend, not in worker
            raise ValueError("Reshard should not be executed in run_stage")

        elif isinstance(op, Join):
            right_shard = ctx.get_right_shard(op_index)
            stream = op.fn(stream, iter(right_shard))
            op_index += 1

    # Collect remaining items as chunks
    return _collect_chunks(stream, ctx.shard_idx, ctx.chunk_size, ctx.storage_path, ctx.spill_threshold_bytes)
