# Copyright 2025 The Marin Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Physical execution plan for zephyr pipelines.

This module separates the logical plan (Dataset operations) from the physical
execution plan. The compute_plan() function transforms a Dataset into a
PhysicalPlan that can be executed by the Backend.

Physical operations are generated by the planner from logical operations.
They encapsulate execution logic as callables, decoupling the backend from
knowledge of logical operation types.
"""

from __future__ import annotations

import heapq
import logging
import os
import zlib
from collections import defaultdict
from collections.abc import Callable, Iterable, Iterator
from dataclasses import dataclass, field
from enum import Enum, auto
from itertools import groupby, islice
from typing import TYPE_CHECKING, Any

import fsspec
import msgspec

from zephyr.dataset import (
    FilterOp,
    FlatMapOp,
    GroupByOp,
    JoinOp,
    LoadFileOp,
    MapOp,
    MapShardOp,
    ReduceOp,
    ReshardOp,
    SelectOp,
    TakePerShardOp,
    WindowOp,
    WriteOp,
)
from zephyr.readers import InputFileSpec

if TYPE_CHECKING:
    from zephyr.dataset import Dataset

logger = logging.getLogger(__name__)


@dataclass
class SourceItem:
    """A source item with its shard assignment.

    The `data` field contains either:
    - InputFileSpec for LoadFileOp pipelines (file loading with optional chunking)
    - Raw data (int, dict, str, etc.) for from_list/from_iterable pipelines
    - File path string for legacy flat_map(load_parquet) pattern
    """

    shard_idx: int
    data: Any


@dataclass
class Map:
    """Transform stream → stream via composed callable.

    Attributes:
        fn: Composed function that transforms an iterator to an iterator
        requires_full_shard: True if any composed op needs full shard context
            (e.g., MapShardOp). When True, chunk parallelism is disabled.
    """

    fn: Callable[[Iterator], Iterator]
    requires_full_shard: bool = False


@dataclass
class Write:
    """Write stream to file, return path."""

    output_pattern: Callable[[int, int], str]  # (shard_idx, total_shards) → path
    writer_type: str  # For chunk aggregation: "jsonl", "parquet", "binary", "levanter_cache"
    skip_existing: bool = False
    # Writer-specific parameters
    levanter_metadata: dict | None = None
    schema: Any = None  # For parquet
    batch_size: int = 1000  # For parquet


@dataclass
class Scatter:
    """Distribute items to output shards by key hash."""

    key_fn: Callable[[Any], Any]  # item → key (for sorting within chunks)
    num_output_shards: int


@dataclass
class Reduce:
    """Merge sorted chunks and reduce per key."""

    key_fn: Callable[[Any], Any]
    reducer_fn: Callable[[Any, Iterator], Any]


@dataclass
class Fold:
    """Reduce stream to single value."""

    fn: Callable[[Iterator], Any]


@dataclass
class Reshard:
    """Redistribute chunks across shards."""

    num_shards: int


@dataclass
class Join:
    """Join two sorted streams.

    The right_plan is pre-computed by the planner, so the backend can just
    execute it without needing to know about Dataset types.
    """

    fn: Callable[[Iterator, Iterator], Iterator]
    right_plan: PhysicalPlan | None = None  # Pre-computed plan for right side


@dataclass
class ForkChunks:
    """Fork stream into N parallel chunk streams, apply ops, merge results.

    At execution time:
    1. Consumes source items (typically InputFileSpecs)
    2. Introspects files to determine chunk count
    3. For each chunk: load file + apply parallel_ops
    4. Merge results as-available (non-deterministic order)

    Attributes:
        target_chunks: Target number of parallel chunks per file
        parallel_ops: Operations to apply to each chunk stream
    """

    target_chunks: int = 8
    parallel_ops: list = field(default_factory=list)  # list[PhysicalOp]


PhysicalOp = Map | Write | Scatter | Reduce | Fold | Reshard | Join | ForkChunks


class StageType(Enum):
    """Type of stage execution."""

    WORKER = auto()  # Normal worker execution
    RESHARD = auto()  # Redistribute chunks (no worker execution)


def _map_gen(stream: Iterator, fn: Callable) -> Iterator:
    """Apply a map function to each item in stream."""
    for item in stream:
        yield fn(item)


def _filter_gen(stream: Iterator, predicate: Callable) -> Iterator:
    """Filter stream to items matching predicate."""
    for item in stream:
        if predicate(item):
            yield item


def _flatmap_gen(stream: Iterator, fn: Callable) -> Iterator:
    """Apply a flatmap function, yielding all outputs."""
    for item in stream:
        yield from fn(item)


def _reduce_gen(shard: Any, key_fn: Callable, reducer_fn: Callable) -> Iterator:
    """Merge sorted chunks and reduce per key."""
    for key, items_iter in _merge_sorted_chunks(shard, key_fn):
        yield reducer_fn(key, items_iter)


def _select_gen(stream: Iterator, columns: tuple[str, ...]) -> Iterator:
    """Select specific columns from each record."""
    cols_set = set(columns)
    for item in stream:
        yield {k: v for k, v in item.items() if k in cols_set}


def _load_file_gen(stream: Iterator) -> Iterator:
    """Load files from InputFileSpecs in stream."""
    from zephyr.readers import load_file

    for spec in stream:
        yield from load_file(spec, columns=spec.columns)


def compose_map(operations: list) -> Callable[[Iterator], Iterator]:
    """Compose logical map/filter/flatmap ops into a single stream transformer.

    Args:
        operations: List of fusible logical ops (MapOp, FilterOp, LoadFileOp, etc.)

    Returns:
        A function that transforms an iterator to an iterator
    """

    def pipeline(stream: Iterator) -> Iterator:
        for op in operations:
            if isinstance(op, LoadFileOp):
                stream = _load_file_gen(stream)
            elif isinstance(op, MapOp):
                stream = _map_gen(stream, op.fn)
            elif isinstance(op, FilterOp):
                stream = _filter_gen(stream, op.predicate)
            elif isinstance(op, FlatMapOp):
                stream = _flatmap_gen(stream, op.fn)
            elif isinstance(op, MapShardOp):
                stream = op.fn(stream)
            elif isinstance(op, TakePerShardOp):
                stream = islice(stream, op.n)
            elif isinstance(op, WindowOp):
                stream = make_windows(stream, op.folder_fn, op.initial_state)
            elif isinstance(op, SelectOp):
                stream = _select_gen(stream, op.columns)
        return stream

    return pipeline


def compose_join(
    left_key_fn: Callable,
    right_key_fn: Callable,
    combiner_fn: Callable,
    join_type: str,
) -> Callable[[Iterator, Iterator], Iterator]:
    """Create a join function.

    Args:
        left_key_fn: Function to extract key from left items
        right_key_fn: Function to extract key from right items
        combiner_fn: Function to combine matched items
        join_type: "inner" or "left"

    Returns:
        Function that takes (left_stream, right_stream) and yields joined items
    """

    def join_fn(left: Iterator, right: Iterator) -> Iterator:
        return _sorted_merge_join(left, right, left_key_fn, right_key_fn, combiner_fn, join_type)

    return join_fn


@dataclass
class PhysicalStage:
    """A stage in the physical plan (operations between shuffle boundaries).

    A stage contains a sequence of physical operations that can be executed
    together. The stage_type tells the backend HOW to execute it:
    - WORKER: Normal worker execution
    - RESHARD: Redistribute chunks across shards (no worker execution)
    - JOIN: Has join ops requiring aux shards to be pre-computed

    Attributes:
        operations: List of physical operations to apply in sequence
        stage_type: How the backend should execute this stage
        output_shards: Target number of output shards (for reshard/scatter)
    """

    operations: list[PhysicalOp] = field(default_factory=list)
    stage_type: StageType = StageType.WORKER
    output_shards: int | None = None


@dataclass
class PhysicalPlan:
    """Executable plan computed from a Dataset.

    The physical plan contains:
    - source_items: How to partition the source data into shards
    - stages: Sequence of operation stages to execute

    The plan can be inspected before execution to understand parallelism,
    shard count, and operation fusion.

    Attributes:
        source_items: List of SourceItems defining initial shard boundaries
        stages: List of PhysicalStages to execute in order
    """

    source_items: list[SourceItem]
    stages: list[PhysicalStage]

    @property
    def num_shards(self) -> int:
        """Total number of logical shards in the plan.

        This counts unique shard_idx values, not total SourceItems. When
        intra-shard parallelism is enabled, multiple SourceItems may belong
        to the same logical shard.
        """
        if not self.source_items:
            return 0
        return len({item.shard_idx for item in self.source_items})

    @property
    def num_chunks(self) -> int:
        """Total number of chunks across all shards."""
        return len(self.source_items)


@dataclass(frozen=True)
class ExecutionHint:
    """Hints for pipeline execution.

    Attributes:
        chunk_size: Number of items per output chunk during streaming. Use -1 for
            1 chunk per shard.
        intra_shard_parallelism: Controls parallel processing of chunks within
            a shard. Set to -1 (default) for auto (parallel when chunks > 1),
            0 to disable, or N to limit max parallel chunks per shard.
    """

    chunk_size: int = 100_000
    intra_shard_parallelism: int = -1


@dataclass
class FusionState:
    """State for fusing logical operations into physical stages.

    Encapsulates all state needed during operation fusion, replacing parallel
    variables and nonlocal mutations with explicit state management.
    """

    stages: list[PhysicalStage] = field(default_factory=list)
    current_ops: list[PhysicalOp] = field(default_factory=list)
    pending_fusible: list = field(default_factory=list)
    output_shards: int | None = None
    stage_type: StageType = StageType.WORKER
    hints: ExecutionHint = field(default_factory=ExecutionHint)

    def _flush_pending_map(self) -> None:
        """Convert pending fusible ops to a physical Map or ForkChunks.

        When the first op is LoadFileOp and parallelism is enabled, creates ForkChunks
        for parallel chunk processing. ForkChunks handles file loading internally,
        so only user ops (after LoadFileOp) become parallel_ops.
        """
        if not self.pending_fusible:
            return

        has_load_file = isinstance(self.pending_fusible[0], LoadFileOp)
        requires_full_shard = any(isinstance(op, MapShardOp) for op in self.pending_fusible)

        # Create ForkChunks for file pipelines with parallelism enabled
        if has_load_file and self.hints.intra_shard_parallelism != 0 and not requires_full_shard:
            user_ops = self.pending_fusible[1:]  # Exclude LoadFileOp, ForkChunks handles file loading
            target_chunks = self.hints.intra_shard_parallelism if self.hints.intra_shard_parallelism > 0 else 8
            parallel_ops = [Map(fn=compose_map(user_ops), requires_full_shard=False)] if user_ops else []
            logger.info("Creating ForkChunks with %d parallel ops, %d target chunks", len(parallel_ops), target_chunks)
            self.current_ops.append(ForkChunks(target_chunks=target_chunks, parallel_ops=parallel_ops))
        else:
            # Regular Map
            self.current_ops.append(
                Map(
                    fn=compose_map(self.pending_fusible[:]),
                    requires_full_shard=requires_full_shard,
                )
            )

        self.pending_fusible = []

    def flush_stage(
        self,
        *,
        next_op: PhysicalOp | None = None,
        output_shards: int | None = None,
        stage_type: StageType = StageType.WORKER,
    ) -> None:
        """Flush pending map, close current stage, optionally start next stage.

        Args:
            next_op: Op to add to new stage after flushing
            output_shards: Output shards for the new stage (set before next flush)
            stage_type: Type for the new stage containing next_op
        """
        self._flush_pending_map()
        if self.current_ops:
            self.stages.append(
                PhysicalStage(
                    operations=self.current_ops[:],
                    stage_type=self.stage_type,
                    output_shards=self.output_shards,
                )
            )
            self.current_ops = []
            self.output_shards = None
            self.stage_type = StageType.WORKER

        if next_op is not None:
            self.current_ops.append(next_op)
            self.output_shards = output_shards
            self.stage_type = stage_type

    def add_op(self, op: PhysicalOp, *, output_shards: int | None = None) -> None:
        """Add op to current stage, optionally setting output_shards."""
        self._flush_pending_map()
        self.current_ops.append(op)
        if output_shards is not None:
            self.output_shards = output_shards

    def finalize(self) -> list[PhysicalStage]:
        """Flush remaining ops and return completed stages."""
        self._flush_pending_map()
        if self.current_ops:
            self.stages.append(
                PhysicalStage(
                    operations=self.current_ops,
                    stage_type=self.stage_type,
                    output_shards=self.output_shards,
                )
            )
        return self.stages


def _fuse_operations(operations: list, hints: ExecutionHint | None = None) -> list[PhysicalStage]:
    """Fuse logical operations into physical stages.

    Transforms logical ops into physical ops:
    - Fusible ops (MapOp, FilterOp, LoadFileOp, etc.) → Map(fn=compose_map([...]))
    - WriteOp → Write(...)
    - GroupByOp → Scatter + [shuffle] + Reduce
    - ReduceOp → Fold + [reshard to 1] + Fold
    - ReshardOp → Reshard
    - JoinOp → Join (with pre-computed right_plan)

    When a stage starts with LoadFileOp and parallelism is enabled, the leading Maps
    are wrapped in ForkChunks for parallel chunk processing.

    Args:
        operations: List of logical operations
        hints: Execution hints (used for pre-computing join right plans)

    Returns:
        List of PhysicalStages with physical operations and execution metadata
    """
    if not operations:
        return []

    if hints is None:
        hints = ExecutionHint()

    state = FusionState(hints=hints)

    for op in operations:
        if isinstance(op, WriteOp):
            state.add_op(
                Write(
                    output_pattern=op.output_pattern,
                    writer_type=op.writer_type,
                    skip_existing=op.skip_existing,
                    levanter_metadata=op.levanter_metadata,
                    schema=op.schema,
                    batch_size=op.batch_size,
                )
            )

        elif isinstance(op, GroupByOp):
            num_shards = op.num_output_shards if op.num_output_shards is not None else -1
            state.add_op(
                Scatter(key_fn=op.key_fn, num_output_shards=num_shards),
                output_shards=num_shards if num_shards > 0 else None,
            )
            state.flush_stage(next_op=Reduce(key_fn=op.key_fn, reducer_fn=op.reducer_fn))

        elif isinstance(op, ReduceOp):
            state.add_op(Fold(fn=op.local_reducer))
            state.flush_stage(
                next_op=Reshard(num_shards=1),
                output_shards=1,
                stage_type=StageType.RESHARD,
            )
            state.flush_stage(next_op=Fold(fn=op.global_reducer))

        elif isinstance(op, ReshardOp):
            state.flush_stage(
                next_op=Reshard(num_shards=op.num_shards),
                output_shards=op.num_shards,
                stage_type=StageType.RESHARD,
            )
            state.flush_stage()

        elif isinstance(op, JoinOp):
            right_plan = compute_plan(op.right_dataset, hints)
            state.add_op(
                Join(
                    fn=compose_join(op.left_key_fn, op.right_key_fn, op.combiner_fn, op.join_type),
                    right_plan=right_plan,
                )
            )

        else:
            # Fusible ops: LoadFileOp, MapOp, FilterOp, FlatMapOp, MapShardOp, TakePerShardOp, WindowOp, SelectOp
            state.pending_fusible.append(op)

    return state.finalize()


def _create_file_plan(
    paths: list[str],
    load_op: LoadFileOp,
    operations: list,
) -> tuple[list[SourceItem], list]:
    """Create source items for file pipeline with pushdown optimizations applied.

    Combines source item creation with filter/column pushdown into a single phase.
    Creates InputFileSpecs with the final columns/filter. LoadFileOp is kept in
    the operations list so fusion can detect file-loading pipelines.

    Args:
        paths: List of file paths to load
        load_op: The LoadFileOp specifying format and default columns
        operations: Full operations list (first op is LoadFileOp)

    Returns:
        Tuple of (source_items, remaining_operations) where remaining_operations
        keeps LoadFileOp and has any pushed-down ops removed.
    """
    remaining_ops = operations[1:]  # Skip LoadFileOp for pushdown analysis

    # Scan for pushdown candidates
    filter_expr = None
    select_columns = load_op.columns  # Default from LoadFileOp
    ops_to_skip: set[int] = set()

    for i, op in enumerate(remaining_ops):
        if isinstance(op, FilterOp) and op.expr is not None and filter_expr is None:
            filter_expr = op.expr
            ops_to_skip.add(i)
        elif isinstance(op, SelectOp) and select_columns is None:
            select_columns = list(op.columns)
            ops_to_skip.add(i)
        elif isinstance(op, FilterOp) and op.expr is None:
            continue  # Lambda filter, can't push down
        elif isinstance(op, (MapOp, FlatMapOp)):
            break  # Transform ops stop pushdown
        else:
            break

    # Create InputFileSpecs with final columns/filter
    source_items = [
        SourceItem(
            shard_idx=i,
            data=InputFileSpec(
                path=path,
                format=load_op.format,
                columns=select_columns,
                filter_expr=filter_expr,
            ),
        )
        for i, path in enumerate(paths)
    ]

    # Build final operations list: LoadFileOp + remaining ops (minus pushed-down ones)
    final_ops = [load_op] + [op for i, op in enumerate(remaining_ops) if i not in ops_to_skip]

    return source_items, final_ops


def compute_plan(dataset: Dataset, hints: ExecutionHint = ExecutionHint()) -> PhysicalPlan:
    """Compute physical execution plan from logical dataset."""
    operations = list(dataset.operations)

    if operations and isinstance(operations[0], LoadFileOp):
        source_items, operations = _create_file_plan(
            list(dataset.source),
            operations[0],
            operations,
        )
    else:
        source_list = list(dataset.source)
        source_items = [SourceItem(shard_idx=i, data=item) for i, item in enumerate(source_list)]

    stages = _fuse_operations(operations, hints)
    return PhysicalPlan(source_items=source_items, stages=stages)


@dataclass
class ChunkHeader:
    """Metadata for a chunk being streamed from a worker."""

    shard_idx: int
    chunk_idx: int
    count: int


@dataclass
class Chunk:
    """A single chunk of data with count metadata."""

    count: int
    data: Any  # The actual ref or raw data


def deterministic_hash(obj: object) -> int:
    """Compute a deterministic hash for an object."""
    s = msgspec.msgpack.encode(obj, order="deterministic")
    return zlib.adler32(s)


def make_windows(
    items: Iterable,
    folder_fn: Callable[[object, Any], tuple[bool, object]],
    initial_state: object,
) -> Iterator[list]:
    """Window items using a folder function.

    Args:
        items: Items to window
        folder_fn: Function (state, item) -> (should_continue, new_state)
        initial_state: Initial state for the folder function

    Yields:
        Windows of items (window closes when folder returns False)
    """
    window: list = []
    state = initial_state

    for item in items:
        should_continue, new_state = folder_fn(state, item)

        if not should_continue and window:
            # Close current window and start new one with this item
            yield window
            window = [item]
            state = initial_state
            # Re-apply folder with the item in the new window
            _, state = folder_fn(state, item)
        else:
            # Add item to current window
            window.append(item)
            state = new_state

    if window:
        yield window


def _stream_chunks(items: Iterator, shard_idx: int, chunk_size: int) -> Iterator[ChunkHeader | list[Any]]:
    """Stream chunks from an iterator, yielding header/data pairs."""
    chunk: list = []
    chunk_idx = 0
    for item in items:
        chunk.append(item)
        if chunk_size > 0 and len(chunk) >= chunk_size:
            header = ChunkHeader(shard_idx=shard_idx, chunk_idx=chunk_idx, count=len(chunk))
            yield header
            yield chunk
            chunk = []
            chunk_idx += 1
    # Yield final partial chunk
    if chunk:
        header = ChunkHeader(shard_idx=shard_idx, chunk_idx=chunk_idx, count=len(chunk))
        yield header
        yield chunk


def _group_items_by_hash(
    items: Iterable,
    key_fn: Callable,
    num_output_shards: int,
    chunk_size: int,
) -> dict[int, list[Chunk]]:
    """Group items by hash of key into output shards with sorted chunks.

    Args:
        items: Items to group
        key_fn: Function to extract grouping key from item
        num_output_shards: Number of output shards to distribute across
        chunk_size: Number of items per chunk

    Returns:
        Dict mapping shard index to list of chunks for that shard
    """
    output_chunks: dict[int, list[Chunk]] = defaultdict(list)
    output_tmp: dict[int, list] = defaultdict(list)

    for item in items:
        key = key_fn(item)
        target_shard = deterministic_hash(key) % num_output_shards
        output_tmp[target_shard].append(item)
        if chunk_size > 0 and len(output_tmp[target_shard]) >= chunk_size:
            sorted_items = sorted(output_tmp[target_shard], key=key_fn)
            output_chunks[target_shard].append(Chunk(count=len(sorted_items), data=sorted_items))
            output_tmp[target_shard] = []

    # Add all remaining chunks
    for target_shard, shard_items in output_tmp.items():
        if shard_items:
            sorted_items = sorted(shard_items, key=key_fn)
            output_chunks[target_shard].append(Chunk(count=len(sorted_items), data=sorted_items))

    return output_chunks


def _merge_sorted_chunks(shard, key_fn: Callable) -> Iterator[tuple[object, Iterator]]:
    """Merge sorted chunks using k-way merge, yielding (key, items_iterator) groups.

    Each chunk is assumed to be sorted by key. This function performs a k-way merge
    across all chunks and groups consecutive items with the same key.

    Args:
        shard: Shard containing sorted chunks (iterable of chunk lists)
        key_fn: Function to extract key from item

    Yields:
        Tuples of (key, iterator_of_items) for each unique key
    """
    # Create iterators for each chunk
    chunk_iterators = []
    for chunk_data in shard.iter_chunks():
        chunk_iterators.append(iter(chunk_data))

    # Use heapq.merge to k-way merge sorted streams
    merged_stream = heapq.merge(*chunk_iterators, key=key_fn)
    yield from groupby(merged_stream, key=key_fn)


def _compute_chunk_specs(spec, target_chunks: int) -> list:
    """Compute chunk specs for a file."""
    from zephyr.readers import open_file

    if target_chunks <= 1 or not isinstance(spec, InputFileSpec) or not spec.path.endswith((".parquet", ".vortex")):
        return [spec]

    if spec.path.endswith(".parquet"):
        import pyarrow.parquet as pq

        with open_file(spec.path, "rb") as f:
            dataset = pq.ParquetDataset(f)
            num_rows = dataset.num_rows
    else:
        import vortex

        f = vortex.open(spec.path)
        num_rows = f.to_dataset().count_rows()

    row_ranges = []
    rows_per_chunk = num_rows // target_chunks
    for i in range(target_chunks):
        start = i * rows_per_chunk
        end = (i + 1) * rows_per_chunk
        row_ranges.append((start, end))

    row_ranges[-1] = (row_ranges[-1][0], num_rows)

    return [
        InputFileSpec(
            path=spec.path,
            format=spec.format,
            columns=spec.columns,
            row_start=start,
            row_end=end,
            filter_expr=spec.filter_expr,
        )
        for start, end in row_ranges
    ]


def _merge_streams_as_available(exec_ctx, futures: list):
    """Merge streaming futures, yielding as-available."""
    active = {id(f): f for f in futures}

    while active:
        ready, _ = exec_ctx.wait(list(active.values()), num_returns=1)
        for gen in ready:
            try:
                item = exec_ctx.get(next(gen))
                yield item
            except StopIteration:
                del active[id(gen)]


def _execute_fork_join(
    exec_ctx,
    source_stream,
    parallel_ops: list[PhysicalOp],
    target_chunks: int,
):
    """Execute ops in parallel across chunks, merge results."""
    from zephyr.readers import load_file

    source_items = list(source_stream)

    logger.info("Source items: %s", source_items)

    # For each source item, compute chunk specs
    all_chunk_specs = []
    for item in source_items:
        if isinstance(item, InputFileSpec):
            chunk_specs = _compute_chunk_specs(item, target_chunks)
            all_chunk_specs.extend(chunk_specs)
        else:
            all_chunk_specs.append(item)

    logger.info("All chunk specs: %s", all_chunk_specs)

    if len(all_chunk_specs) <= 1 or exec_ctx is None:
        # No parallelism benefit - run sequentially
        for spec in all_chunk_specs:
            if isinstance(spec, InputFileSpec):
                stream = load_file(spec)
            else:
                stream = iter([spec])
            # Apply parallel_ops
            for op in parallel_ops:
                if isinstance(op, Map):
                    stream = op.fn(stream)
            yield from stream
        return

    # Build the chunk task function
    def process_chunk(chunk_spec):
        from zephyr.readers import load_file

        if isinstance(chunk_spec, InputFileSpec):
            stream = load_file(chunk_spec)
        else:
            stream = iter([chunk_spec])
        # Apply the parallel ops
        for op in parallel_ops:
            if isinstance(op, Map):
                stream = op.fn(stream)
        yield from stream

    # Spawn parallel tasks
    futures = [exec_ctx.run(process_chunk, spec) for spec in all_chunk_specs]

    # Merge results as-available
    yield from _merge_streams_as_available(exec_ctx, futures)


def _sorted_merge_join(
    left_stream: Iterable,
    right_stream: Iterable,
    left_key_fn: Callable,
    right_key_fn: Callable,
    combiner_fn: Callable,
    join_type: str,
) -> Iterator:
    """Perform a sorted merge join between two streams.

    Args:
        left_stream: Iterator of items from left side
        right_stream: Iterator of items from right side
        left_key_fn: Function to extract key from left items
        right_key_fn: Function to extract key from right items
        combiner_fn: Function to combine matched items
        join_type: "inner" or "left"

    Yields:
        Joined items according to join_type
    """
    # Materialize left stream and tag both streams
    left_items = list(left_stream)
    left_tagged = (("left", left_key_fn(item), item) for item in left_items)
    right_tagged = (("right", right_key_fn(item), item) for item in right_stream)

    # Merge both sorted streams by key
    merged = heapq.merge(left_tagged, right_tagged, key=lambda x: x[1])

    # Group by key and apply join logic
    for _key, group in groupby(merged, key=lambda x: x[1]):
        left_group: list[Any] = []
        right_group: list[Any] = []
        for side, _, item in group:
            (left_group if side == "left" else right_group).append(item)

        if join_type == "inner":
            if left_group and right_group:
                for left_item in left_group:
                    for right_item in right_group:
                        yield combiner_fn(left_item, right_item)
        elif join_type == "left":
            for left_item in left_group:
                if right_group:
                    for right_item in right_group:
                        yield combiner_fn(left_item, right_item)
                else:
                    yield combiner_fn(left_item, None)


# =============================================================================
# Stage Execution
# =============================================================================


@dataclass
class StageContext:
    """Context for executing a stage on a shard.

    Encapsulates all the metadata and auxiliary data needed to process a shard.

    Attributes:
        shard: The shard data to process (iterable, typically a Shard object)
        shard_idx: Index of this shard
        total_shards: Total number of shards
        chunk_size: Number of items per output chunk
        aux_shards: Auxiliary shards for joins, keyed by op index
        execution_context: Execution context for put/get/run/wait operations (for ForkChunks)
    """

    shard: Any  # Shard object (avoids circular import)
    shard_idx: int
    total_shards: int
    chunk_size: int
    aux_shards: dict[int, list[Any]] = field(default_factory=dict)
    execution_context: Any = None  # ExecutionContext (avoids circular import)

    def get_right_shard(self, op_index: int) -> Any | None:
        """Get right shard for join at given op index."""
        shards = self.aux_shards.get(op_index, [])
        if len(shards) == 1:
            return shards[0]
        return None


def run_stage(
    ctx: StageContext,
    ops: list[PhysicalOp],
) -> Iterator[ChunkHeader | list[Any]]:
    """Execute a stage's physical ops in a single pass.

    This is the single worker function that backends call to execute physical ops.
    It only knows about physical op types (Map, Write, etc.) - not logical ops.

    Args:
        ctx: Stage execution context providing shard data and metadata
        ops: List of physical operations to execute in sequence

    Yields:
        ChunkHeader followed by list of items for each chunk produced
    """
    from zephyr.writers import write_binary_file, write_jsonl_file, write_levanter_cache, write_parquet_file

    stream: Iterator = iter(ctx.shard)

    i = 0
    while i < len(ops):
        op = ops[i]

        if isinstance(op, Map):
            stream = op.fn(stream)
            i += 1
        elif isinstance(op, ForkChunks):
            # Execute chunk parallelism with contained parallel_ops
            stream = _execute_fork_join(
                ctx.execution_context,
                stream,
                op.parallel_ops,
                op.target_chunks,
            )
            i += 1

        elif isinstance(op, Write):
            output_path = op.output_pattern(ctx.shard_idx, ctx.total_shards)

            if op.skip_existing:
                fs = fsspec.core.url_to_fs(output_path)[0]
                if op.writer_type == "levanter_cache":
                    test_path = os.path.join(output_path, ".success")
                else:
                    test_path = output_path

                if fs.exists(test_path):
                    logger.info(f"Skipping write, output exists: {output_path}")
                    yield from _stream_chunks(iter([output_path]), ctx.shard_idx, ctx.chunk_size)
                    return

            # Write based on type
            if op.writer_type == "jsonl":
                result = write_jsonl_file(stream, output_path)["path"]
            elif op.writer_type == "parquet":
                result = write_parquet_file(stream, output_path, op.schema, op.batch_size)["path"]
            elif op.writer_type == "levanter_cache":
                metadata = op.levanter_metadata if op.levanter_metadata is not None else {}
                result = write_levanter_cache(stream, output_path, metadata)["path"]
            elif op.writer_type == "binary":
                result = write_binary_file(stream, output_path)["path"]
            elif op.writer_type == "vortex":
                from zephyr.writers import write_vortex_file

                result = write_vortex_file(stream, output_path)["path"]
            else:
                raise ValueError(f"Unknown writer_type: {op.writer_type}")

            yield from _stream_chunks(iter([result]), ctx.shard_idx, ctx.chunk_size)
            return

        elif isinstance(op, Scatter):
            # Hash items to output shards
            num_output_shards = op.num_output_shards if op.num_output_shards > 0 else ctx.total_shards
            output_chunks = _group_items_by_hash(stream, op.key_fn, num_output_shards, ctx.chunk_size)

            # Yield chunks for each output shard
            for shard_idx in range(num_output_shards):
                if output_chunks[shard_idx]:
                    for chunk in output_chunks[shard_idx]:
                        header = ChunkHeader(shard_idx=shard_idx, chunk_idx=0, count=chunk.count)
                        yield header
                        yield chunk.data
                else:
                    # Yield empty chunk so controller knows this shard exists
                    header = ChunkHeader(shard_idx=shard_idx, chunk_idx=0, count=0)
                    yield header
                    yield []
            return

        elif isinstance(op, Reduce):
            # Merge sorted chunks and reduce per key
            stream = _reduce_gen(ctx.shard, op.key_fn, op.reducer_fn)
            i += 1

        elif isinstance(op, Fold):
            # Reduction to single value
            result = op.fn(stream)
            stream = iter([result])
            i += 1

        elif isinstance(op, Reshard):
            # Reshard is handled by the backend, not in worker
            raise ValueError("Reshard should not be executed in run_stage")

        elif isinstance(op, Join):
            # Get right shard from context
            right_shard = ctx.get_right_shard(i)
            if right_shard is None:
                raise ValueError(f"No right shard provided for join at op index {i}")
            stream = op.fn(stream, iter(right_shard))
            i += 1

    # Yield remaining items as chunks
    yield from _stream_chunks(stream, ctx.shard_idx, ctx.chunk_size)


def _infer_extension(output_path: str) -> str:
    """Infer file extension from output path, handling compression."""
    if output_path.endswith(".jsonl.gz"):
        return ".jsonl.gz"
    if output_path.endswith(".jsonl.zst"):
        return ".jsonl.zst"
    if output_path.endswith(".parquet"):
        return ".parquet"
    parts = output_path.rsplit(".", 1)
    return f".{parts[-1]}" if len(parts) > 1 else ""
