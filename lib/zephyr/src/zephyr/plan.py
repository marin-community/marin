# Copyright 2025 The Marin Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Physical execution plan for zephyr pipelines.

Physical operations are generated by the planner from logical operations.
They encapsulate execution logic as callables, decoupling the backend from
knowledge of logical operation types.
"""

from __future__ import annotations

import heapq
import logging
import os
import zlib
from collections import defaultdict
from collections.abc import Callable, Iterable, Iterator
from dataclasses import dataclass, field
from enum import StrEnum, auto
from itertools import groupby, islice
from typing import TYPE_CHECKING, Any

import fsspec
import msgspec
from fray.job import JobContext

from zephyr.dataset import (
    FilterOp,
    FlatMapOp,
    GroupByOp,
    JoinOp,
    LoadFileOp,
    MapOp,
    MapShardOp,
    ReduceOp,
    ReshardOp,
    SelectOp,
    TakePerShardOp,
    WindowOp,
    WriteOp,
)
from zephyr.readers import InputFileSpec

if TYPE_CHECKING:
    from zephyr.dataset import Dataset

logger = logging.getLogger(__name__)

# Default number of items per output chunk during streaming
DEFAULT_CHUNK_SIZE = 100_000

# Default number of parallel chunks when splitting files for intra-shard parallelism
DEFAULT_INTRA_SHARD_PARALLELISM = 1

# Size of micro-batches yielded from parallel chunk workers to reduce overhead
DEFAULT_MICRO_BATCH_SIZE = 1024


@dataclass
class SourceItem:
    """A source item with its shard assignment.

    The `data` field contains either:
    - InputFileSpec for LoadFileOp pipelines (file loading with optional chunking)
    - Raw data (int, dict, str, etc.) for from_list/from_iterable pipelines
    """

    shard_idx: int
    data: Any


@dataclass
class Map:
    """Transform stream → stream via composed callable.

    Attributes:
        fn: Composed function that transforms an iterator to an iterator
        requires_full_shard: True if any composed op needs full shard context
            (e.g., MapShardOp). When True, chunk parallelism is disabled.
    """

    fn: Callable[[Iterator], Iterator]
    requires_full_shard: bool = False


@dataclass
class Write:
    """Write stream to file, return path."""

    output_pattern: Callable[[int, int], str]  # (shard_idx, total_shards) → path
    writer_type: str  # For chunk aggregation: "jsonl", "parquet", "binary", "levanter_cache"
    skip_existing: bool = False
    # Writer-specific parameters
    levanter_metadata: dict | None = None
    schema: Any = None  # For parquet
    batch_size: int = 1000  # For parquet


@dataclass
class Scatter:
    """Distribute items to output shards by key hash."""

    key_fn: Callable[[Any], Any]  # item → key
    num_output_shards: int


@dataclass
class Reduce:
    """Merge sorted chunks and reduce per key."""

    key_fn: Callable[[Any], Any]
    reducer_fn: Callable[[Any, Iterator], Any]


@dataclass
class Fold:
    """Reduce stream to single value."""

    fn: Callable[[Iterator], Any]


@dataclass
class Reshard:
    """Redistribute chunks across shards."""

    num_shards: int


@dataclass
class Join:
    """Join two sorted streams."""

    fn: Callable[[Iterator, Iterator], Iterator]
    right_plan: PhysicalPlan | None = None


@dataclass
class ForkChunks:
    """Fork stream into N parallel chunk streams.

    Child operations are applied in parallel, and merged as available.
    """

    target_chunks: int = DEFAULT_INTRA_SHARD_PARALLELISM
    parallel_ops: list = field(default_factory=list)  # list[PhysicalOp]


PhysicalOp = Map | Write | Scatter | Reduce | Fold | Reshard | Join | ForkChunks


class StageType(StrEnum):
    """Type of stage execution."""

    WORKER = auto()  # Normal worker execution
    RESHARD = auto()  # Redistribute chunks (no worker execution)


def _map_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield fn(item)


def _filter_gen(stream: Iterator, predicate: Callable) -> Iterator:
    for item in stream:
        if predicate(item):
            yield item


def _flatmap_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield from fn(item)


def _reduce_gen(shard: Any, key_fn: Callable, reducer_fn: Callable) -> Iterator:
    for key, items_iter in _merge_sorted_chunks(shard, key_fn):
        yield reducer_fn(key, items_iter)


def _select_gen(stream: Iterator, columns: tuple[str, ...]) -> Iterator:
    cols_set = set(columns)
    for item in stream:
        yield {k: item[k] for k in cols_set if k in item}


def _load_file_gen(stream: Iterator) -> Iterator:
    from zephyr.readers import load_file

    for spec in stream:
        yield from load_file(spec)


def compose_map(operations: list) -> Callable[[Iterator], Iterator]:
    """Compose logical map/filter/flatmap ops into a single stream transformer.

    Args:
        operations: List of fusible logical ops (MapOp, FilterOp, LoadFileOp, etc.)

    Returns:
        A function that transforms an iterator to an iterator
    """

    def pipeline(stream: Iterator) -> Iterator:
        for op in operations:
            if isinstance(op, LoadFileOp):
                stream = _load_file_gen(stream)
            elif isinstance(op, MapOp):
                stream = _map_gen(stream, op.fn)
            elif isinstance(op, FilterOp):
                stream = _filter_gen(stream, op.predicate)
            elif isinstance(op, FlatMapOp):
                stream = _flatmap_gen(stream, op.fn)
            elif isinstance(op, MapShardOp):
                stream = op.fn(stream)
            elif isinstance(op, TakePerShardOp):
                stream = islice(stream, op.n)
            elif isinstance(op, WindowOp):
                stream = make_windows(stream, op.folder_fn, op.initial_state)
            elif isinstance(op, SelectOp):
                stream = _select_gen(stream, op.columns)
        return stream

    return pipeline


def compose_join(
    left_key_fn: Callable,
    right_key_fn: Callable,
    combiner_fn: Callable,
    join_type: str,
) -> Callable[[Iterator, Iterator], Iterator]:
    """Create a join function.

    Args:
        left_key_fn: Function to extract key from left items
        right_key_fn: Function to extract key from right items
        combiner_fn: Function to combine matched items
        join_type: "inner" or "left"

    Returns:
        Function that takes (left_stream, right_stream) and yields joined items
    """

    def join_fn(left: Iterator, right: Iterator) -> Iterator:
        return _sorted_merge_join(left, right, left_key_fn, right_key_fn, combiner_fn, join_type)

    return join_fn


@dataclass
class PhysicalStage:
    """A stage in the plan (operations between shuffle boundaries).

    A stage contains a sequence of physical operations that can be executed
    together. The stage_type tells the backend HOW to execute it:
    - WORKER: Normal worker execution
    - RESHARD: Redistribute chunks across shards (no worker execution)
    """

    operations: list[PhysicalOp] = field(default_factory=list)
    stage_type: StageType = StageType.WORKER
    output_shards: int | None = None


@dataclass
class PhysicalPlan:
    """Executable plan computed from a Dataset."""

    # inputs to the computation
    source_items: list[SourceItem]

    # list of stages to execute
    stages: list[PhysicalStage]

    @property
    def num_shards(self) -> int:
        """Total number of logical shards in the plan.

        This counts unique shard_idx values, not total SourceItems. When
        intra-shard parallelism is enabled, multiple SourceItems may belong
        to the same logical shard.
        """
        if not self.source_items:
            return 0
        return len({item.shard_idx for item in self.source_items})

    @property
    def num_chunks(self) -> int:
        """Total number of chunks across all shards."""
        return len(self.source_items)


@dataclass(frozen=True)
class ExecutionHint:
    """Hints for pipeline execution.

    Attributes:
        chunk_size: Number of items per output chunk during streaming. Use -1 for
            1 chunk per shard.
        intra_shard_parallelism: Controls parallel processing of chunks within
            a shard. Set to -1 (default) for auto (parallel when chunks > 1),
            0 to disable, or N to limit max parallel chunks per shard.
    """

    chunk_size: int = DEFAULT_CHUNK_SIZE
    intra_shard_parallelism: int = -1


@dataclass
class FusionState:
    """Incremental state for fusing logical operations into physical stages."""

    stages: list[PhysicalStage] = field(default_factory=list)
    current_ops: list[PhysicalOp] = field(default_factory=list)
    pending_fusible: list = field(default_factory=list)
    output_shards: int | None = None
    stage_type: StageType = StageType.WORKER
    hints: ExecutionHint = field(default_factory=ExecutionHint)

    def flush_pending(self) -> None:
        """Convert pending fusible ops to a physical Map or ForkChunks.

        When the first op is LoadFileOp and parallelism is enabled, creates ForkChunks
        for parallel chunk processing.
        """
        if not self.pending_fusible:
            return

        has_load_file = isinstance(self.pending_fusible[0], LoadFileOp)
        requires_full_shard = any(isinstance(op, MapShardOp) for op in self.pending_fusible)

        # Create ForkChunks for file pipelines with parallelism enabled
        if has_load_file and self.hints.intra_shard_parallelism != 0 and not requires_full_shard:
            user_ops = self.pending_fusible[1:]  # Exclude LoadFileOp, ForkChunks handles file loading
            target_chunks = (
                self.hints.intra_shard_parallelism
                if self.hints.intra_shard_parallelism > 0
                else DEFAULT_INTRA_SHARD_PARALLELISM
            )
            parallel_ops = [Map(fn=compose_map(user_ops), requires_full_shard=False)] if user_ops else []
            logger.info("Creating ForkChunks with %d parallel ops, %d target chunks", len(parallel_ops), target_chunks)
            self.current_ops.append(ForkChunks(target_chunks=target_chunks, parallel_ops=parallel_ops))
        else:
            # Regular Map
            self.current_ops.append(
                Map(
                    fn=compose_map(self.pending_fusible[:]),
                    requires_full_shard=requires_full_shard,
                )
            )

        self.pending_fusible = []

    def add_op(
        self,
        op: PhysicalOp,
        *,
        output_shards: int | None = None,
        stage_type: StageType | None = None,
    ) -> None:
        """Add physical op to current stage.

        Flushes any pending fusible ops to a Map first, then adds this op.
        """
        self.flush_pending()
        self.current_ops.append(op)
        if output_shards is not None:
            self.output_shards = output_shards
        if stage_type is not None:
            self.stage_type = stage_type

    def end_stage(self) -> None:
        """Flush pending ops and close current stage."""
        self.flush_pending()
        if self.current_ops:
            self.stages.append(
                PhysicalStage(
                    operations=self.current_ops[:],
                    stage_type=self.stage_type,
                    output_shards=self.output_shards,
                )
            )
            self.current_ops = []
            self.output_shards = None
            self.stage_type = StageType.WORKER

    def finalize(self) -> list[PhysicalStage]:
        """Flush remaining ops and return completed stages."""
        self.end_stage()
        return self.stages


def _fuse_operations(operations: list, hints: ExecutionHint | None = None) -> list[PhysicalStage]:
    """Fuse logical operations into physical stages.

    Transforms logical ops into physical ops:
    - Fusible ops (MapOp, FilterOp, LoadFileOp, etc.) → Map(fn=compose_map([...]))
    - WriteOp → Write(...)
    - GroupByOp → Scatter + [shuffle] + Reduce
    - ReduceOp → Fold + [reshard to 1] + Fold
    - ReshardOp → Reshard
    - JoinOp → Join (with pre-computed right_plan)

    When a stage starts with LoadFileOp and parallelism is enabled, the leading Maps
    are wrapped in ForkChunks for parallel chunk processing.

    Args:
        operations: List of logical operations
        hints: Execution hints (used for pre-computing join right plans)

    Returns:
        List of PhysicalStages with physical operations and execution metadata
    """
    if not operations:
        return []

    if hints is None:
        hints = ExecutionHint()

    state = FusionState(hints=hints)

    for op in operations:
        if isinstance(op, WriteOp):
            state.add_op(
                Write(
                    output_pattern=op.output_pattern,
                    writer_type=op.writer_type,
                    skip_existing=op.skip_existing,
                    levanter_metadata=op.levanter_metadata,
                    schema=op.schema,
                    batch_size=op.batch_size,
                )
            )

        elif isinstance(op, GroupByOp):
            num_shards = op.num_output_shards if op.num_output_shards is not None else -1
            state.add_op(
                Scatter(key_fn=op.key_fn, num_output_shards=num_shards),
                output_shards=num_shards if num_shards > 0 else None,
            )
            state.end_stage()
            state.add_op(Reduce(key_fn=op.key_fn, reducer_fn=op.reducer_fn))

        elif isinstance(op, ReduceOp):
            state.add_op(Fold(fn=op.local_reducer))
            state.end_stage()
            state.add_op(Reshard(num_shards=1), output_shards=1, stage_type=StageType.RESHARD)
            state.end_stage()
            state.add_op(Fold(fn=op.global_reducer))

        elif isinstance(op, ReshardOp):
            state.end_stage()
            state.add_op(Reshard(num_shards=op.num_shards), output_shards=op.num_shards, stage_type=StageType.RESHARD)
            state.end_stage()

        elif isinstance(op, JoinOp):
            right_plan = compute_plan(op.right_dataset, hints)
            state.add_op(
                Join(
                    fn=compose_join(op.left_key_fn, op.right_key_fn, op.combiner_fn, op.join_type),
                    right_plan=right_plan,
                )
            )

        else:
            # Fusible ops: LoadFileOp, MapOp, FilterOp, FlatMapOp, MapShardOp, TakePerShardOp, WindowOp, SelectOp
            state.pending_fusible.append(op)

    return state.finalize()


def _compute_file_pushdown(
    paths: list[str],
    load_op: LoadFileOp,
    operations: list,
) -> tuple[list[SourceItem], list]:
    """Create source items for file pipeline with pushdown optimizations applied.

    Args:
        paths: List of file paths to load
        load_op: The LoadFileOp specifying format and default columns
        operations: Full operations list (first op is LoadFileOp)

    Returns:
        Tuple of (source_items, remaining_operations), where filter/select have been pushed down.
    """
    filter_expr = None
    select_columns = load_op.columns
    ops_to_skip: set[int] = set()

    # We don't try to do anything fancy, e.g. AND multiple filters.
    # We can however aggregate select operations, though this is obviously unusual.

    for i, op in enumerate(operations):
        if isinstance(op, FilterOp) and op.expr is not None and filter_expr is None:
            filter_expr = op.expr
            ops_to_skip.add(i)
        elif isinstance(op, SelectOp) and select_columns is None:
            select_columns = list(op.columns)
            ops_to_skip.add(i)
        elif isinstance(op, FilterOp) and op.expr is None:
            continue  # Lambda filter, can't push down
        elif isinstance(op, (MapOp, FlatMapOp)):
            break  # Transform ops stop pushdown
        else:
            break

    # Create InputFileSpecs with final columns/filter
    source_items = [
        SourceItem(
            shard_idx=i,
            data=InputFileSpec(
                path=path,
                format=load_op.format,
                columns=select_columns,
                filter_expr=filter_expr,
            ),
        )
        for i, path in enumerate(paths)
    ]

    # Build final operations list: LoadFileOp + remaining ops
    final_ops = [load_op] + [op for i, op in enumerate(operations) if i not in ops_to_skip]

    return source_items, final_ops


def compute_plan(dataset: Dataset, hints: ExecutionHint = ExecutionHint()) -> PhysicalPlan:
    """Compute physical execution plan from logical dataset."""
    operations = list(dataset.operations)

    if operations and isinstance(operations[0], LoadFileOp):
        source_items, operations = _compute_file_pushdown(
            list(dataset.source),
            operations[0],
            operations[1:],
        )
    else:
        source_list = list(dataset.source)
        source_items = [SourceItem(shard_idx=i, data=item) for i, item in enumerate(source_list)]

    stages = _fuse_operations(operations, hints)
    return PhysicalPlan(source_items=source_items, stages=stages)


@dataclass
class ChunkHeader:
    """Metadata for a chunk being streamed from a worker."""

    shard_idx: int
    count: int


@dataclass
class Chunk:
    """A single chunk of data with count metadata."""

    count: int
    data: Any  # The actual ref or raw data


def deterministic_hash(obj: object) -> int:
    """Compute a deterministic hash for an object."""
    s = msgspec.msgpack.encode(obj, order="deterministic")
    return zlib.adler32(s)


def make_windows(
    items: Iterable,
    folder_fn: Callable[[object, Any], tuple[bool, object]],
    initial_state: object,
) -> Iterator[list]:
    """Window items using a folder function.

    Args:
        items: Items to window
        folder_fn: Function (state, item) -> (should_continue, new_state)
        initial_state: Initial state for the folder function

    Yields:
        Windows of items (window closes when folder returns False)
    """
    window: list = []
    state = initial_state

    for item in items:
        should_continue, new_state = folder_fn(state, item)

        if not should_continue and window:
            # Close current window and start new one with this item
            yield window
            window = [item]
            state = initial_state
            # Re-apply folder with the item in the new window
            _, state = folder_fn(state, item)
        else:
            # Add item to current window
            window.append(item)
            state = new_state

    if window:
        yield window


def _stream_chunks(items: Iterator, shard_idx: int, chunk_size: int) -> Iterator[ChunkHeader | list[Any]]:
    """Stream chunks from an iterator, yielding header/data pairs."""
    chunk: list = []
    for item in items:
        chunk.append(item)
        if chunk_size > 0 and len(chunk) >= chunk_size:
            header = ChunkHeader(shard_idx=shard_idx, count=len(chunk))
            yield header
            yield chunk
            chunk = []
    # Yield final partial chunk
    if chunk:
        header = ChunkHeader(shard_idx=shard_idx, count=len(chunk))
        yield header
        yield chunk


def _group_items_by_hash(
    items: Iterable,
    key_fn: Callable,
    num_output_shards: int,
    chunk_size: int,
) -> dict[int, list[Chunk]]:
    """Group items by hash of key into num_output_shards target shards with sorted chunks.

    Args:
        items: Items to group
        key_fn: Function to extract grouping key from item
        num_output_shards: Number of output shards to distribute across
        chunk_size: Number of items per chunk

    Returns:
        Dict mapping shard index to list of chunks for that shard
    """
    output_chunks: dict[int, list[Chunk]] = defaultdict(list)
    output_tmp: dict[int, list] = defaultdict(list)

    for item in items:
        key = key_fn(item)
        target_shard = deterministic_hash(key) % num_output_shards
        output_tmp[target_shard].append(item)
        if chunk_size > 0 and len(output_tmp[target_shard]) >= chunk_size:
            sorted_items = sorted(output_tmp[target_shard], key=key_fn)
            output_chunks[target_shard].append(Chunk(count=len(sorted_items), data=sorted_items))
            output_tmp[target_shard] = []

    # Add all remaining chunks
    for target_shard, shard_items in output_tmp.items():
        if shard_items:
            sorted_items = sorted(shard_items, key=key_fn)
            output_chunks[target_shard].append(Chunk(count=len(sorted_items), data=sorted_items))

    return output_chunks


def _merge_sorted_chunks(shard, key_fn: Callable) -> Iterator[tuple[object, Iterator]]:
    """Merge sorted chunks using k-way merge, yielding (key, items_iterator) groups.

    Each chunk is assumed to be sorted by key. This function performs a k-way merge
    across all chunks and groups consecutive items with the same key.

    Args:
        shard: Shard containing sorted chunks (iterable of chunk lists)
        key_fn: Function to extract key from item

    Yields:
        Tuples of (key, iterator_of_items) for each unique key
    """
    chunk_iterators = []
    for chunk_data in shard.iter_chunks():
        chunk_iterators.append(iter(chunk_data))

    # Use heapq.merge to k-way merge sorted streams
    merged_stream = heapq.merge(*chunk_iterators, key=key_fn)
    yield from groupby(merged_stream, key=key_fn)


def _compute_chunk_specs(spec, target_chunks: int) -> list:
    """Compute chunk specs for a file."""
    from zephyr.readers import open_file

    if target_chunks <= 1 or not isinstance(spec, InputFileSpec) or not spec.path.endswith((".parquet", ".vortex")):
        return [spec]

    if spec.path.endswith(".parquet"):
        import pyarrow.parquet as pq

        with open_file(spec.path, "rb") as f:
            parquet_file = pq.ParquetFile(f)
            num_rows = parquet_file.metadata.num_rows
    else:
        import vortex

        f = vortex.open(spec.path)
        num_rows = f.to_dataset().count_rows()

    row_ranges = []
    rows_per_chunk = num_rows // target_chunks
    for i in range(target_chunks):
        start = i * rows_per_chunk
        end = (i + 1) * rows_per_chunk
        row_ranges.append((start, end))

    row_ranges[-1] = (row_ranges[-1][0], num_rows)

    return [
        InputFileSpec(
            path=spec.path,
            format=spec.format,
            columns=spec.columns,
            row_start=start,
            row_end=end,
            filter_expr=spec.filter_expr,
        )
        for start, end in row_ranges
    ]


def _merge_chunk_streams(exec_ctx, futures: list):
    active = {id(f): f for f in futures}

    while active:
        ready, _ = exec_ctx.wait(list(active.values()), num_returns=1)
        for gen in ready:
            try:
                items = exec_ctx.get(next(gen))
                yield from items
            except StopIteration:
                del active[id(gen)]


def _execute_fork_join(
    exec_ctx,
    source_stream,
    parallel_ops: list[PhysicalOp],
    target_chunks: int,
):
    """Execute ops in parallel across chunks, merging results."""
    from zephyr.readers import load_file

    source_items = list(source_stream)

    logger.info("Source items: %s", source_items)

    # For each source item, compute chunk specs
    all_chunk_specs = []
    for item in source_items:
        if isinstance(item, InputFileSpec):
            chunk_specs = _compute_chunk_specs(item, target_chunks)
            all_chunk_specs.extend(chunk_specs)
        else:
            all_chunk_specs.append(item)

    logger.info("All chunk specs: %s", all_chunk_specs)

    def process_chunk(chunk_spec):
        if isinstance(chunk_spec, InputFileSpec):
            stream = load_file(chunk_spec)
        else:
            stream = iter([chunk_spec])
        for op in parallel_ops:
            assert isinstance(op, Map)
            stream = op.fn(stream)

        # batch into micro-chunks to reduce overhead
        micro_chunks = []
        for item in stream:
            micro_chunks.append(item)
            if len(micro_chunks) >= DEFAULT_MICRO_BATCH_SIZE:
                yield micro_chunks
                micro_chunks = []
        if micro_chunks:
            yield micro_chunks

    if len(all_chunk_specs) == 1:
        for batch in process_chunk(all_chunk_specs[0]):
            yield from batch
    else:
        futures = [exec_ctx.run(process_chunk, spec) for spec in all_chunk_specs]
        yield from _merge_chunk_streams(exec_ctx, futures)


def _sorted_merge_join(
    left_stream: Iterable,
    right_stream: Iterable,
    left_key_fn: Callable,
    right_key_fn: Callable,
    combiner_fn: Callable,
    join_type: str,
) -> Iterator:
    """Perform a sorted merge join between two streams.

    Args:
        left_stream: Iterator of items from left side
        right_stream: Iterator of items from right side
        left_key_fn: Function to extract key from left items
        right_key_fn: Function to extract key from right items
        combiner_fn: Function to combine matched items
        join_type: "inner" or "left"

    Yields:
        Joined items according to join_type
    """
    # Materialize left stream and tag both streams
    left_items = list(left_stream)
    left_tagged = (("left", left_key_fn(item), item) for item in left_items)
    right_tagged = (("right", right_key_fn(item), item) for item in right_stream)

    # Merge both sorted streams by key
    merged = heapq.merge(left_tagged, right_tagged, key=lambda x: x[1])

    # Group by key and apply join logic
    for _key, group in groupby(merged, key=lambda x: x[1]):
        left_group: list[Any] = []
        right_group: list[Any] = []
        for side, _, item in group:
            (left_group if side == "left" else right_group).append(item)

        if join_type == "inner":
            if left_group and right_group:
                for left_item in left_group:
                    for right_item in right_group:
                        yield combiner_fn(left_item, right_item)
        elif join_type == "left":
            for left_item in left_group:
                if right_group:
                    for right_item in right_group:
                        yield combiner_fn(left_item, right_item)
                else:
                    yield combiner_fn(left_item, None)


@dataclass
class StageContext:
    """Context for executing a stage on a shard.

    Encapsulates all the metadata and auxiliary data needed to process a shard.

    Attributes:
        shard: The shard data to process (iterable, typically a Shard object)
        shard_idx: Index of this shard
        total_shards: Total number of shards
        chunk_size: Number of items per output chunk
        aux_shards: Auxiliary shards for joins, keyed by op index
        execution_context: Execution context for put/get/run/wait operations (for ForkChunks)
    """

    shard: Any  # Shard object (avoids circular import)
    shard_idx: int
    total_shards: int
    chunk_size: int
    aux_shards: dict[int, list[Any]] = field(default_factory=dict)
    execution_context: JobContext = None

    def get_right_shard(self, op_index: int) -> Any:
        """Get right shard for join at given op index.

        Raises:
            ValueError: If no right shard is provided for the join
        """
        shards = self.aux_shards.get(op_index, [])
        if len(shards) != 1:
            raise ValueError(f"Expected exactly 1 right shard for join at op index {op_index}, got {len(shards)}")
        return shards[0]


def run_stage(
    ctx: StageContext,
    ops: list[PhysicalOp],
) -> Iterator[ChunkHeader | list[Any]]:
    """Execute a stage's physical ops in a single pass.

    This is the single worker function that backends call to execute physical ops.
    It only knows about physical op types (Map, Write, etc.) - not logical ops.

    Args:
        ctx: Stage execution context providing shard data and metadata
        ops: List of physical operations to execute in sequence

    Yields:
        ChunkHeader followed by list of items for each chunk produced
    """
    from zephyr.writers import write_binary_file, write_jsonl_file, write_levanter_cache, write_parquet_file

    stream: Iterator = iter(ctx.shard)

    op_index = 0
    while op_index < len(ops):
        op = ops[op_index]

        if isinstance(op, Map):
            stream = op.fn(stream)
            op_index += 1
        elif isinstance(op, ForkChunks):
            # Execute chunk parallelism with contained parallel_ops
            stream = _execute_fork_join(
                ctx.execution_context,
                stream,
                op.parallel_ops,
                op.target_chunks,
            )
            op_index += 1

        elif isinstance(op, Write):
            output_path = op.output_pattern(ctx.shard_idx, ctx.total_shards)

            if op.skip_existing:
                fs = fsspec.core.url_to_fs(output_path)[0]
                if op.writer_type == "levanter_cache":
                    test_path = os.path.join(output_path, ".success")
                else:
                    test_path = output_path

                if fs.exists(test_path):
                    logger.info(f"Skipping write, output exists: {output_path}")
                    yield from _stream_chunks(iter([output_path]), ctx.shard_idx, ctx.chunk_size)
                    return

            # Write based on type
            if op.writer_type == "jsonl":
                result = write_jsonl_file(stream, output_path)["path"]
            elif op.writer_type == "parquet":
                result = write_parquet_file(stream, output_path, op.schema, op.batch_size)["path"]
            elif op.writer_type == "levanter_cache":
                metadata = op.levanter_metadata if op.levanter_metadata is not None else {}
                result = write_levanter_cache(stream, output_path, metadata)["path"]
            elif op.writer_type == "binary":
                result = write_binary_file(stream, output_path)["path"]
            elif op.writer_type == "vortex":
                from zephyr.writers import write_vortex_file

                result = write_vortex_file(stream, output_path)["path"]
            else:
                raise ValueError(f"Unknown writer_type: {op.writer_type}")

            yield from _stream_chunks(iter([result]), ctx.shard_idx, ctx.chunk_size)
            return

        elif isinstance(op, Scatter):
            # Hash items to output shards
            num_output_shards = op.num_output_shards if op.num_output_shards > 0 else ctx.total_shards
            output_chunks = _group_items_by_hash(stream, op.key_fn, num_output_shards, ctx.chunk_size)

            # Yield chunks for each output shard
            for shard_idx in range(num_output_shards):
                if output_chunks[shard_idx]:
                    for chunk in output_chunks[shard_idx]:
                        header = ChunkHeader(shard_idx=shard_idx, count=chunk.count)
                        yield header
                        yield chunk.data
                else:
                    # Yield empty chunk so controller knows this shard exists
                    header = ChunkHeader(shard_idx=shard_idx, count=0)
                    yield header
                    yield []
            return

        elif isinstance(op, Reduce):
            # Merge sorted chunks and reduce per key
            stream = _reduce_gen(ctx.shard, op.key_fn, op.reducer_fn)
            op_index += 1

        elif isinstance(op, Fold):
            # Reduction to single value
            result = op.fn(stream)
            stream = iter([result])
            op_index += 1

        elif isinstance(op, Reshard):
            # Reshard is handled by the backend, not in worker
            raise ValueError("Reshard should not be executed in run_stage")

        elif isinstance(op, Join):
            right_shard = ctx.get_right_shard(op_index)
            stream = op.fn(stream, iter(right_shard))
            op_index += 1

    # Yield remaining items as chunks
    yield from _stream_chunks(stream, ctx.shard_idx, ctx.chunk_size)
