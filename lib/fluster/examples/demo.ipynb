{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Fluster Demo Notebook\n",
    "\n",
    "Interactive exploration of the fluster cluster system.\n",
    "\n",
    "This notebook connects to a running fluster cluster and demonstrates how to:\n",
    "- Submit jobs to the cluster\n",
    "- Wait for job completion\n",
    "- Submit jobs with arguments\n",
    "- View cluster state\n",
    "- Run stateful actors and call methods on them\n",
    "- Use WorkerPool for parallel task execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup and Connection\n",
    "\n",
    "Connect to the demo cluster using environment variables set by `demo_cluster.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to cluster at http://127.0.0.1:60670\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from fluster.client import FlusterClient\n",
    "from fluster.cluster.types import Entrypoint\n",
    "from fluster.rpc import cluster_pb2\n",
    "\n",
    "# Connect to the demo cluster\n",
    "# FLUSTER_CONTROLLER_ADDRESS and FLUSTER_WORKSPACE are set by demo_cluster.py\n",
    "controller_address = os.environ.get(\"FLUSTER_CONTROLLER_ADDRESS\", \"http://127.0.0.1:8080\")\n",
    "workspace_str = os.environ.get(\"FLUSTER_WORKSPACE\")\n",
    "if workspace_str is None:\n",
    "    raise RuntimeError(\n",
    "        \"FLUSTER_WORKSPACE not set. Run this notebook via: \"\n",
    "        \"uv run python examples/demo_cluster.py\"\n",
    "    )\n",
    "workspace = Path(workspace_str)\n",
    "\n",
    "client = FlusterClient.remote(controller_address, workspace=workspace)\n",
    "print(f\"Connected to cluster at {controller_address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Submit a Simple Job\n",
    "\n",
    "Submit a basic job that prints a message and returns a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted job: notebook-hello\n"
     ]
    }
   ],
   "source": [
    "def hello_world():\n",
    "    print(\"Hello from the cluster!\")\n",
    "    return 42\n",
    "\n",
    "job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(hello_world),\n",
    "    name=\"notebook-hello\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    ")\n",
    "print(f\"Submitted job: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Wait and Check Status\n",
    "\n",
    "Wait for the job to complete and check its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job notebook-hello: JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "status = client.wait(job_id, timeout=30.0, stream_logs=True)\n",
    "print(f\"Job {job_id}: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Submit a Job with Arguments\n",
    "\n",
    "Submit a job that takes arguments and performs a computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "def compute(a: int, b: int) -> int:\n",
    "    result = a * b\n",
    "    print(f\"Computing {a} * {b} = {result}\")\n",
    "    return result\n",
    "\n",
    "job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(compute, 7, 6),\n",
    "    name=\"multiply-job\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    ")\n",
    "status = client.wait(job_id, stream_logs=True)\n",
    "print(f\"Result: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Submit Multiple Jobs\n",
    "\n",
    "Submit multiple jobs and wait for all of them to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted: square-1\n",
      "Submitted: square-2\n",
      "Submitted: square-3\n",
      "Submitted: square-4\n",
      "Submitted: square-5\n",
      "\n",
      "Waiting for jobs...\n",
      "square-1: JOB_STATE_SUCCEEDED\n",
      "square-2: JOB_STATE_SUCCEEDED\n",
      "square-3: JOB_STATE_SUCCEEDED\n",
      "square-4: JOB_STATE_SUCCEEDED\n",
      "square-5: JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "def square(n: int) -> int:\n",
    "    result = n * n\n",
    "    print(f\"{n}^2 = {result}\")\n",
    "    return result\n",
    "\n",
    "# Submit 5 jobs\n",
    "job_ids = []\n",
    "for i in range(1, 6):\n",
    "    job_id = client.submit(\n",
    "        entrypoint=Entrypoint.from_callable(square, i),\n",
    "        name=f\"square-{i}\",\n",
    "        resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    "    )\n",
    "    job_ids.append(job_id)\n",
    "    print(f\"Submitted: {job_id}\")\n",
    "\n",
    "# Wait for all\n",
    "print(\"\\nWaiting for jobs...\")\n",
    "for job_id in job_ids:\n",
    "    status = client.wait(job_id, stream_logs=True)\n",
    "    print(f\"{job_id}: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Remote Actor Demo\n",
    "\n",
    "Demonstrate running a stateful actor as a remote job. The actor maintains state\n",
    "across method calls, enabling persistent services within the cluster.\n",
    "\n",
    "This example:\n",
    "1. Submits a job that starts an ActorServer with a Counter actor\n",
    "2. Discovers the actor endpoint via the controller\n",
    "3. Calls methods and verifies state is maintained across calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter actor class defined (for illustration)\n"
     ]
    }
   ],
   "source": [
    "class Counter:\n",
    "    \"\"\"A simple stateful actor that maintains a count.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._count = 0\n",
    "\n",
    "    def increment(self, amount: int = 1) -> int:\n",
    "        \"\"\"Increment the counter and return the new value.\"\"\"\n",
    "        self._count += amount\n",
    "        return self._count\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        \"\"\"Return the current count.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        \"\"\"Reset the counter to zero and return the old value.\"\"\"\n",
    "        old = self._count\n",
    "        self._count = 0\n",
    "        return old\n",
    "\n",
    "print(\"Counter actor class defined (for illustration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor job function defined\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from fluster.actor import ActorServer\n",
    "from fluster.client import fluster_ctx\n",
    "\n",
    "\n",
    "def run_counter_actor():\n",
    "    \"\"\"Job entrypoint that starts a Counter actor server.\n",
    "\n",
    "    The actor server:\n",
    "    1. Binds to the allocated port (from context)\n",
    "    2. Registers its endpoint with the controller for discovery\n",
    "    3. Serves requests until the job is terminated\n",
    "    \"\"\"\n",
    "    # Define Counter inside the function so it gets pickled with the entrypoint\n",
    "    # (required for Docker mode where the class isn't available in the container)\n",
    "    class Counter:\n",
    "        \"\"\"A simple stateful actor that maintains a count.\"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            self._count = 0\n",
    "\n",
    "        def increment(self, amount: int = 1) -> int:\n",
    "            \"\"\"Increment the counter and return the new value.\"\"\"\n",
    "            self._count += amount\n",
    "            return self._count\n",
    "\n",
    "        def get_count(self) -> int:\n",
    "            \"\"\"Return the current count.\"\"\"\n",
    "            return self._count\n",
    "\n",
    "        def reset(self) -> int:\n",
    "            \"\"\"Reset the counter to zero and return the old value.\"\"\"\n",
    "            old = self._count\n",
    "            self._count = 0\n",
    "            return old\n",
    "    \n",
    "    ctx = fluster_ctx()\n",
    "    print(f\"Starting counter actor for job {ctx.job_id}\")\n",
    "\n",
    "    # Get the allocated port from context (works in both Docker and local modes)\n",
    "    port = ctx.get_port(\"actor\")\n",
    "\n",
    "    # Bind to all interfaces - works in both Docker and local modes\n",
    "    bind_host = \"0.0.0.0\"\n",
    "    \n",
    "    # Create and register the actor\n",
    "    server = ActorServer(host=bind_host, port=port)\n",
    "    server.register(\"counter\", Counter())\n",
    "\n",
    "    # Start the server (uses port from __init__)\n",
    "    actual_port = server.serve_background()\n",
    "    print(f\"Actor server started on {bind_host}:{actual_port}\")\n",
    "\n",
    "    # Register endpoint with controller for discovery via context registry\n",
    "    # The registry handles namespace prefixing automatically\n",
    "    if ctx.registry:\n",
    "        endpoint_id = ctx.registry.register(\"counter\", f\"127.0.0.1:{actual_port}\")\n",
    "        print(f\"Registered endpoint: counter -> 127.0.0.1:{actual_port} (id={endpoint_id})\")\n",
    "    else:\n",
    "        print(\"WARNING: No registry in context, endpoint not registered\")\n",
    "\n",
    "    # Keep running to serve requests\n",
    "    # The job will be terminated externally when no longer needed\n",
    "    print(\"Actor ready, serving requests...\")\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "print(\"Actor job function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted actor job: counter-actor\n"
     ]
    }
   ],
   "source": [
    "# Submit the actor job\n",
    "# Request ports=[\"actor\"] so the worker allocates a port for the actor server\n",
    "actor_job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(run_counter_actor),\n",
    "    name=\"counter-actor\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    "    ports=[\"actor\"],\n",
    ")\n",
    "print(f\"Submitted actor job: {actor_job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for actor to start...\n",
      "Actor job is running\n"
     ]
    }
   ],
   "source": [
    "from fluster.time_utils import ExponentialBackoff\n",
    "\n",
    "# Wait for the actor job to start running\n",
    "# Unlike regular jobs, we don't wait for completion - we wait for RUNNING state\n",
    "print(\"Waiting for actor to start...\")\n",
    "\n",
    "_job_status = None\n",
    "\n",
    "def job_is_running_or_failed() -> bool:\n",
    "    global _job_status\n",
    "    _job_status = client.status(actor_job_id)\n",
    "    return _job_status.state in (\n",
    "        cluster_pb2.JOB_STATE_RUNNING,\n",
    "        cluster_pb2.JOB_STATE_FAILED,\n",
    "        cluster_pb2.JOB_STATE_KILLED,\n",
    "    )\n",
    "\n",
    "ExponentialBackoff(initial=0.1, maximum=1.0).wait_until_or_raise(\n",
    "    job_is_running_or_failed,\n",
    "    timeout=15.0,\n",
    "    error_message=\"Actor job did not start in time\",\n",
    ")\n",
    "\n",
    "if _job_status.state != cluster_pb2.JOB_STATE_RUNNING:\n",
    "    raise RuntimeError(f\"Actor job failed: {cluster_pb2.JobState.Name(_job_status.state)}\")\n",
    "\n",
    "print(\"Actor job is running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for actor endpoint to be registered...\n",
      "Resolved endpoint: [ResolvedEndpoint(url='http://127.0.0.1:30001', actor_id='030ec174-3d9e-4bbe-b666-321127b795ee', metadata={})]\n",
      "Actor client created\n",
      "\n",
      "Testing actor state persistence...\n",
      "Initial count: 0\n",
      "After increment 1: 1\n",
      "After increment 2: 2\n",
      "After increment 3: 3\n",
      "Final count: 3\n",
      "After increment(10): 13\n",
      "Reset returned: 13\n",
      "After reset: 0\n",
      "\n",
      "All actor tests passed! State is correctly maintained across calls.\n"
     ]
    }
   ],
   "source": [
    "from fluster.actor import ActorClient\n",
    "from fluster.client import FlusterContext, fluster_ctx_scope, ClusterResolver\n",
    "from fluster.time_utils import ExponentialBackoff\n",
    "\n",
    "# Enter the actor job's namespace context\n",
    "# This enables ClusterResolver to discover endpoints in that namespace\n",
    "ctx = FlusterContext(job_id=actor_job_id, client=client)\n",
    "\n",
    "with fluster_ctx_scope(ctx):\n",
    "    resolver = ClusterResolver(controller_address)\n",
    "    \n",
    "    # Wait for the actor to register its endpoint\n",
    "    print(\"Waiting for actor endpoint to be registered...\")\n",
    "    \n",
    "    ExponentialBackoff(initial=0.1, maximum=1.0).wait_until_or_raise(\n",
    "        lambda: not resolver.resolve(\"counter\").is_empty,\n",
    "        timeout=15.0,\n",
    "        error_message=\"Actor endpoint not registered in time\",\n",
    "    )\n",
    "    \n",
    "    # Log the resolved endpoint\n",
    "    resolved = resolver.resolve(\"counter\")\n",
    "    print(f\"Resolved endpoint: {resolved.endpoints}\")\n",
    "    \n",
    "    # Create actor client using ClusterResolver\n",
    "    counter = ActorClient(resolver, \"counter\", timeout=10.0)\n",
    "    print(\"Actor client created\")\n",
    "    \n",
    "    # Test the actor: verify state is maintained across calls\n",
    "    print(\"\\nTesting actor state persistence...\")\n",
    "    \n",
    "    # Initial count should be 0\n",
    "    initial = counter.get_count()\n",
    "    print(f\"Initial count: {initial}\")\n",
    "    assert initial == 0, f\"Expected 0, got {initial}\"\n",
    "    \n",
    "    # Increment 3 times\n",
    "    for i in range(1, 4):\n",
    "        result = counter.increment()\n",
    "        print(f\"After increment {i}: {result}\")\n",
    "        assert result == i, f\"Expected {i}, got {result}\"\n",
    "    \n",
    "    # Verify final count\n",
    "    final = counter.get_count()\n",
    "    print(f\"Final count: {final}\")\n",
    "    assert final == 3, f\"Expected 3, got {final}\"\n",
    "    \n",
    "    # Test increment with custom amount\n",
    "    result = counter.increment(10)\n",
    "    print(f\"After increment(10): {result}\")\n",
    "    assert result == 13, f\"Expected 13, got {result}\"\n",
    "    \n",
    "    # Test reset\n",
    "    old_value = counter.reset()\n",
    "    print(f\"Reset returned: {old_value}\")\n",
    "    assert old_value == 13, f\"Expected 13, got {old_value}\"\n",
    "    \n",
    "    # Verify count is now 0\n",
    "    after_reset = counter.get_count()\n",
    "    print(f\"After reset: {after_reset}\")\n",
    "    assert after_reset == 0, f\"Expected 0, got {after_reset}\"\n",
    "    \n",
    "    print(\"\\nAll actor tests passed! State is correctly maintained across calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminating actor job: counter-actor\n",
      "Actor job terminated: JOB_STATE_KILLED\n"
     ]
    }
   ],
   "source": [
    "# Cleanup: terminate the actor job\n",
    "print(f\"Terminating actor job: {actor_job_id}\")\n",
    "client.terminate(actor_job_id)\n",
    "\n",
    "# Wait for termination to complete\n",
    "status = client.wait(actor_job_id, timeout=10.0)\n",
    "print(f\"Actor job terminated: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2s9tbyxffbb",
   "metadata": {},
   "source": [
    "## WorkerPool Demo\n",
    "\n",
    "WorkerPool provides a high-level interface for parallel task execution. Unlike\n",
    "submitting individual jobs (which have scheduling overhead), WorkerPool maintains\n",
    "a persistent pool of workers that can execute arbitrary callables with minimal latency.\n",
    "\n",
    "Key features:\n",
    "- **Persistent workers**: Workers stay running and accept tasks via RPC\n",
    "- **Task queuing**: Submit many tasks; they queue and dispatch to idle workers\n",
    "- **map() interface**: Familiar parallel map semantics for batch processing\n",
    "\n",
    "WorkerPool must run from within a job context (it needs FlusterContext for\n",
    "endpoint discovery). This demo submits a \"coordinator\" job that creates and\n",
    "uses a WorkerPool internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ymqtun2oxwa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinator function defined\n"
     ]
    }
   ],
   "source": [
    "def workerpool_coordinator():\n",
    "    \"\"\"Coordinator job that demonstrates WorkerPool usage.\n",
    "    \n",
    "    This runs inside a job context, which provides the FlusterContext needed\n",
    "    for WorkerPool's endpoint discovery mechanism.\n",
    "    \"\"\"\n",
    "    from fluster.client import fluster_ctx, WorkerPool, WorkerPoolConfig\n",
    "    from fluster.rpc import cluster_pb2\n",
    "\n",
    "    ctx = fluster_ctx()\n",
    "    print(f\"Coordinator starting (job_id={ctx.job_id})\")\n",
    "\n",
    "    # Define a simple computation function\n",
    "    def square(n: int) -> int:\n",
    "        return n * n\n",
    "\n",
    "    # Create pool configuration: 3 workers with minimal resources\n",
    "    config = WorkerPoolConfig(\n",
    "        num_workers=3,\n",
    "        resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    "        name_prefix=\"pool-worker\",\n",
    "    )\n",
    "\n",
    "    print(f\"Creating WorkerPool with {config.num_workers} workers...\")\n",
    "\n",
    "    # Use WorkerPool as a context manager for automatic cleanup\n",
    "    with WorkerPool(ctx.client, config, timeout=30.0) as pool:\n",
    "        print(f\"Pool ready: {pool.size} workers available\")\n",
    "        pool.print_status()\n",
    "\n",
    "        # Use map() to compute squares of 1-10 in parallel\n",
    "        items = list(range(1, 11))\n",
    "        print(f\"\\nComputing squares of {items}...\")\n",
    "\n",
    "        futures = pool.map(square, items)\n",
    "        results = [f.result(timeout=30.0) for f in futures]\n",
    "\n",
    "        print(f\"Results: {results}\")\n",
    "        \n",
    "        # Verify results\n",
    "        expected = [i * i for i in items]\n",
    "        assert results == expected, f\"Expected {expected}, got {results}\"\n",
    "        \n",
    "        print(\"\\nFinal pool status:\")\n",
    "        pool.print_status()\n",
    "\n",
    "    print(\"\\nWorkerPool demo completed successfully!\")\n",
    "\n",
    "\n",
    "print(\"Coordinator function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "q1tqas2ikp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted coordinator job: workerpool-demo\n"
     ]
    }
   ],
   "source": [
    "# Submit the coordinator job\n",
    "coordinator_job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(workerpool_coordinator),\n",
    "    name=\"workerpool-demo\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    ")\n",
    "print(f\"Submitted coordinator job: {coordinator_job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "y7mt92v49w",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for WorkerPool demo to complete...\n",
      "(The coordinator will launch 3 worker sub-jobs internally)\n",
      "\n",
      "Coordinator job finished: JOB_STATE_FAILED\n",
      "WARNING: Job did not succeed (state=JOB_STATE_FAILED)\n"
     ]
    }
   ],
   "source": [
    "# Wait for the coordinator to complete with log streaming\n",
    "# This may take a minute as it launches worker sub-jobs and waits for them\n",
    "print(\"Waiting for WorkerPool demo to complete...\")\n",
    "print(\"(The coordinator will launch 3 worker sub-jobs internally)\")\n",
    "print()\n",
    "\n",
    "status = client.wait(coordinator_job_id, timeout=120.0, stream_logs=True)\n",
    "state_name = cluster_pb2.JobState.Name(status.state)\n",
    "print(f\"Coordinator job finished: {state_name}\")\n",
    "\n",
    "if status.state != cluster_pb2.JOB_STATE_SUCCEEDED:\n",
    "    print(f\"WARNING: Job did not succeed (state={state_name})\")\n",
    "else:\n",
    "    print(\"\\nWorkerPool demo completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
