{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Fluster Demo Notebook\n\nInteractive exploration of the fluster cluster system.\n\nThis notebook connects to a running fluster cluster and demonstrates how to:\n- Submit jobs to the cluster\n- Wait for job completion\n- Submit jobs with arguments\n- View cluster state\n- Run stateful actors and call methods on them\n- Use WorkerPool for parallel task execution"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup and Connection\n",
    "\n",
    "Connect to the demo cluster using environment variables set by `demo_cluster.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from fluster.client.rpc_client import RpcClusterClient\n",
    "from fluster.cluster.types import Entrypoint\n",
    "from fluster.rpc import cluster_pb2\n",
    "\n",
    "# Connect to the demo cluster\n",
    "# FLUSTER_CONTROLLER_ADDRESS and FLUSTER_WORKSPACE are set by demo_cluster.py\n",
    "controller_address = os.environ.get(\"FLUSTER_CONTROLLER_ADDRESS\", \"http://127.0.0.1:8080\")\n",
    "workspace_str = os.environ.get(\"FLUSTER_WORKSPACE\")\n",
    "if workspace_str is None:\n",
    "    raise RuntimeError(\n",
    "        \"FLUSTER_WORKSPACE not set. Run this notebook via: \"\n",
    "        \"uv run python examples/demo_cluster.py\"\n",
    "    )\n",
    "workspace = Path(workspace_str)\n",
    "\n",
    "client = RpcClusterClient(controller_address, workspace=workspace)\n",
    "print(f\"Connected to cluster at {controller_address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Submit a Simple Job\n",
    "\n",
    "Submit a basic job that prints a message and returns a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world():\n",
    "    print(\"Hello from the cluster!\")\n",
    "    return 42\n",
    "\n",
    "job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(hello_world),\n",
    "    name=\"notebook-hello\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    ")\n",
    "print(f\"Submitted job: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Wait and Check Status\n",
    "\n",
    "Wait for the job to complete and check its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = client.wait(job_id, timeout=30.0)\n",
    "print(f\"Job {job_id}: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Submit a Job with Arguments\n",
    "\n",
    "Submit a job that takes arguments and performs a computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(a: int, b: int) -> int:\n",
    "    result = a * b\n",
    "    print(f\"Computing {a} * {b} = {result}\")\n",
    "    return result\n",
    "\n",
    "job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(compute, 7, 6),\n",
    "    name=\"multiply-job\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    ")\n",
    "status = client.wait(job_id)\n",
    "print(f\"Result: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Submit Multiple Jobs\n",
    "\n",
    "Submit multiple jobs and wait for all of them to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(n: int) -> int:\n",
    "    result = n * n\n",
    "    print(f\"{n}^2 = {result}\")\n",
    "    return result\n",
    "\n",
    "# Submit 5 jobs\n",
    "job_ids = []\n",
    "for i in range(1, 6):\n",
    "    job_id = client.submit(\n",
    "        entrypoint=Entrypoint.from_callable(square, i),\n",
    "        name=f\"square-{i}\",\n",
    "        resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    "    )\n",
    "    job_ids.append(job_id)\n",
    "    print(f\"Submitted: {job_id}\")\n",
    "\n",
    "# Wait for all\n",
    "print(\"\\nWaiting for jobs...\")\n",
    "for job_id in job_ids:\n",
    "    status = client.wait(job_id)\n",
    "    print(f\"{job_id}: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## View Job Status\n",
    "\n",
    "Check the status of a job without waiting for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_job():\n",
    "    for i in range(5):\n",
    "        print(f\"Working... {i+1}/5\")\n",
    "        time.sleep(1)\n",
    "    return \"done\"\n",
    "\n",
    "job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(slow_job),\n",
    "    name=\"slow-job\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    ")\n",
    "print(f\"Submitted: {job_id}\")\n",
    "\n",
    "# Check status a few times while it runs\n",
    "for _ in range(3):\n",
    "    time.sleep(1)\n",
    "    status = client.status(job_id)\n",
    "    print(f\"Status: {cluster_pb2.JobState.Name(status.state)}\")\n",
    "\n",
    "# Wait for completion\n",
    "status = client.wait(job_id)\n",
    "print(f\"Final: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Remote Actor Demo\n",
    "\n",
    "Demonstrate running a stateful actor as a remote job. The actor maintains state\n",
    "across method calls, enabling persistent services within the cluster.\n",
    "\n",
    "This example:\n",
    "1. Submits a job that starts an ActorServer with a Counter actor\n",
    "2. Discovers the actor endpoint via the controller\n",
    "3. Calls methods and verifies state is maintained across calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stateful Counter actor\n",
    "class Counter:\n",
    "    \"\"\"A simple stateful actor that maintains a count.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._count = 0\n",
    "\n",
    "    def increment(self, amount: int = 1) -> int:\n",
    "        \"\"\"Increment the counter and return the new value.\"\"\"\n",
    "        self._count += amount\n",
    "        return self._count\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        \"\"\"Return the current count.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        \"\"\"Reset the counter to zero and return the old value.\"\"\"\n",
    "        old = self._count\n",
    "        self._count = 0\n",
    "        return old\n",
    "\n",
    "print(\"Counter actor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nfrom fluster.actor import ActorServer\nfrom fluster.client import fluster_ctx\n\n\ndef run_counter_actor():\n    \"\"\"Job entrypoint that starts a Counter actor server.\n\n    The actor server:\n    1. Binds to an allocated port (from FLUSTER_PORT_ACTOR)\n    2. Registers its endpoint with the controller for discovery\n    3. Serves requests until the job is terminated\n    \"\"\"\n    import os\n    ctx = fluster_ctx()\n    print(f\"Starting counter actor for job {ctx.job_id}\")\n\n    # Create and register the actor\n    server = ActorServer(host=\"127.0.0.1\")\n    server.register(\"counter\", Counter())\n\n    # Start the server on the allocated port\n    port = server.serve_background()\n    print(f\"Actor server started on port {port}\")\n\n    # Register endpoint with controller for discovery\n    # Use controller address from environment, fixing host.docker.internal for in-process mode\n    controller_addr = os.environ.get(\"FLUSTER_CONTROLLER_ADDRESS\", \"\")\n    if \"host.docker.internal\" in controller_addr:\n        controller_addr = controller_addr.replace(\"host.docker.internal\", \"127.0.0.1\")\n\n    address = f\"127.0.0.1:{port}\"\n\n    if controller_addr:\n        from fluster.rpc.cluster_connect import ControllerServiceClientSync\n        from fluster.rpc import cluster_pb2 as pb2\n\n        ctrl_client = ControllerServiceClientSync(address=controller_addr, timeout_ms=5000)\n        prefixed_name = f\"{ctx.namespace}/counter\"\n        request = pb2.Controller.RegisterEndpointRequest(\n            name=prefixed_name,\n            address=address,\n            job_id=ctx.job_id,\n            metadata={},\n        )\n        response = ctrl_client.register_endpoint(request)\n        print(f\"Registered endpoint: {prefixed_name} -> {address} (id={response.endpoint_id})\")\n    else:\n        print(\"WARNING: No controller address, endpoint not registered\")\n\n    # Keep running to serve requests\n    # The job will be terminated externally when no longer needed\n    print(\"Actor ready, serving requests...\")\n    while True:\n        time.sleep(1)\n\n\nprint(\"Actor job function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the actor job\n",
    "# Request ports=[\"actor\"] so the worker allocates a port for the actor server\n",
    "actor_job_id = client.submit(\n",
    "    entrypoint=Entrypoint.from_callable(run_counter_actor),\n",
    "    name=\"counter-actor\",\n",
    "    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n",
    "    ports=[\"actor\"],\n",
    ")\n",
    "print(f\"Submitted actor job: {actor_job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "from fluster.rpc.cluster_connect import ControllerServiceClientSync\nfrom fluster.time_utils import wait_until_with_exception\n\n# Wait for the actor job to start running and register its endpoint\n# Unlike regular jobs, we don't wait for completion - we wait for RUNNING state\nprint(\"Waiting for actor to start...\")\n\ncontroller_client = ControllerServiceClientSync(\n    address=controller_address,\n    timeout_ms=30000,\n)\n\n_job_status = None\n\ndef job_is_running_or_failed() -> bool:\n    global _job_status\n    _job_status = client.status(actor_job_id)\n    return _job_status.state in (\n        cluster_pb2.JOB_STATE_RUNNING,\n        cluster_pb2.JOB_STATE_FAILED,\n        cluster_pb2.JOB_STATE_KILLED,\n    )\n\nwait_until_with_exception(\n    job_is_running_or_failed,\n    timeout=15.0,\n    error_message=\"Actor job did not start in time\",\n    initial_interval=0.1,\n    max_interval=1.0,\n)\n\nif _job_status.state != cluster_pb2.JOB_STATE_RUNNING:\n    raise RuntimeError(f\"Actor job failed: {cluster_pb2.JobState.Name(_job_status.state)}\")\n\nprint(\"Actor job is running\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "from fluster.actor import ActorClient, FixedResolver\n\n# Discover the actor endpoint via the controller\n# The endpoint name is prefixed with the namespace (root job ID)\nendpoint_prefix = f\"{actor_job_id}/counter\"\nprint(f\"Discovering endpoint with prefix: {endpoint_prefix}\")\n\n_endpoint = None\n\ndef endpoint_discovered() -> bool:\n    global _endpoint\n    request = cluster_pb2.Controller.ListEndpointsRequest(prefix=endpoint_prefix)\n    response = controller_client.list_endpoints(request)\n    if response.endpoints:\n        _endpoint = response.endpoints[0]\n        return True\n    return False\n\nwait_until_with_exception(\n    endpoint_discovered,\n    timeout=15.0,\n    error_message=f\"No endpoints found for prefix: {endpoint_prefix}\",\n    initial_interval=0.1,\n    max_interval=1.0,\n)\n\nactor_url = f\"http://{_endpoint.address}\"\nprint(f\"Discovered actor endpoint: {_endpoint.name} -> {actor_url}\")\n\n# Create actor client with FixedResolver (we know the exact address)\nresolver = FixedResolver({\"counter\": actor_url})\ncounter = ActorClient(resolver, \"counter\", timeout=10.0)\nprint(\"Actor client created\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the actor: verify state is maintained across calls\n",
    "print(\"Testing actor state persistence...\")\n",
    "\n",
    "# Initial count should be 0\n",
    "initial = counter.get_count()\n",
    "print(f\"Initial count: {initial}\")\n",
    "assert initial == 0, f\"Expected 0, got {initial}\"\n",
    "\n",
    "# Increment 3 times\n",
    "for i in range(1, 4):\n",
    "    result = counter.increment()\n",
    "    print(f\"After increment {i}: {result}\")\n",
    "    assert result == i, f\"Expected {i}, got {result}\"\n",
    "\n",
    "# Verify final count\n",
    "final = counter.get_count()\n",
    "print(f\"Final count: {final}\")\n",
    "assert final == 3, f\"Expected 3, got {final}\"\n",
    "\n",
    "# Test increment with custom amount\n",
    "result = counter.increment(10)\n",
    "print(f\"After increment(10): {result}\")\n",
    "assert result == 13, f\"Expected 13, got {result}\"\n",
    "\n",
    "# Test reset\n",
    "old_value = counter.reset()\n",
    "print(f\"Reset returned: {old_value}\")\n",
    "assert old_value == 13, f\"Expected 13, got {old_value}\"\n",
    "\n",
    "# Verify count is now 0\n",
    "after_reset = counter.get_count()\n",
    "print(f\"After reset: {after_reset}\")\n",
    "assert after_reset == 0, f\"Expected 0, got {after_reset}\"\n",
    "\n",
    "print(\"\\nAll actor tests passed! State is correctly maintained across calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: terminate the actor job\n",
    "print(f\"Terminating actor job: {actor_job_id}\")\n",
    "client.terminate(actor_job_id)\n",
    "\n",
    "# Wait for termination to complete\n",
    "status = client.wait(actor_job_id, timeout=10.0)\n",
    "print(f\"Actor job terminated: {cluster_pb2.JobState.Name(status.state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2s9tbyxffbb",
   "source": "## WorkerPool Demo\n\nWorkerPool provides a high-level interface for parallel task execution. Unlike\nsubmitting individual jobs (which have scheduling overhead), WorkerPool maintains\na persistent pool of workers that can execute arbitrary callables with minimal latency.\n\nKey features:\n- **Persistent workers**: Workers stay running and accept tasks via RPC\n- **Task queuing**: Submit many tasks; they queue and dispatch to idle workers\n- **map() interface**: Familiar parallel map semantics for batch processing\n\nWorkerPool must run from within a job context (it needs FlusterContext for\nendpoint discovery). This demo submits a \"coordinator\" job that creates and\nuses a WorkerPool internally.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ymqtun2oxwa",
   "source": "def workerpool_coordinator():\n    \"\"\"Coordinator job that demonstrates WorkerPool usage.\n    \n    This runs inside a job context, which provides the FlusterContext needed\n    for WorkerPool's endpoint discovery mechanism.\n    \"\"\"\n    from fluster.client import fluster_ctx\n    from fluster.worker_pool import WorkerPool, WorkerPoolConfig\n    from fluster.rpc import cluster_pb2\n\n    ctx = fluster_ctx()\n    print(f\"Coordinator starting (job_id={ctx.job_id})\")\n\n    # Define a simple computation function\n    def square(n: int) -> int:\n        return n * n\n\n    # Create pool configuration: 3 workers with minimal resources\n    config = WorkerPoolConfig(\n        num_workers=3,\n        resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n        name_prefix=\"pool-worker\",\n    )\n\n    print(f\"Creating WorkerPool with {config.num_workers} workers...\")\n\n    # Use WorkerPool as a context manager for automatic cleanup\n    with WorkerPool(ctx.controller, config, timeout=30.0) as pool:\n        print(f\"Pool ready: {pool.size} workers available\")\n        pool.print_status()\n\n        # Use map() to compute squares of 1-10 in parallel\n        items = list(range(1, 11))\n        print(f\"\\nComputing squares of {items}...\")\n\n        futures = pool.map(square, items)\n        results = [f.result(timeout=30.0) for f in futures]\n\n        print(f\"Results: {results}\")\n        \n        # Verify results\n        expected = [i * i for i in items]\n        assert results == expected, f\"Expected {expected}, got {results}\"\n        \n        print(\"\\nFinal pool status:\")\n        pool.print_status()\n\n    print(\"\\nWorkerPool demo completed successfully!\")\n\n\nprint(\"Coordinator function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "q1tqas2ikp",
   "source": "# Submit the coordinator job\ncoordinator_job_id = client.submit(\n    entrypoint=Entrypoint.from_callable(workerpool_coordinator),\n    name=\"workerpool-demo\",\n    resources=cluster_pb2.ResourceSpec(cpu=1, memory=\"512m\"),\n)\nprint(f\"Submitted coordinator job: {coordinator_job_id}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y7mt92v49w",
   "source": "# Wait for the coordinator to complete\n# This may take a minute as it launches worker sub-jobs and waits for them\nprint(\"Waiting for WorkerPool demo to complete...\")\nprint(\"(The coordinator will launch 3 worker sub-jobs internally)\")\nprint()\n\nstatus = client.wait(coordinator_job_id, timeout=120.0)\nstate_name = cluster_pb2.JobState.Name(status.state)\nprint(f\"Coordinator job finished: {state_name}\")\n\nif status.state != cluster_pb2.JOB_STATE_SUCCEEDED:\n    print(f\"WARNING: Job did not succeed (state={state_name})\")\nelse:\n    print(\"\\nWorkerPool demo completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
