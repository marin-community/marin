model_name_or_path: huggyllama/llama-7b
data: lucasmccabe-lmi/CodeAlpaca-20k
data_cache_dir: code_alpaca_cache
prompts:
  prompt_input: |-
    ### Instruction: {instruction}
    ### Input: {input}
    ### Output:
  prompt_no_input: |-
    ### Instruction: {instruction}
    ### Output:
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    project: "levanter-alpaca"
    tags: ["code", "lora", "llama1"]
  num_train_steps: 500  # 128 * 500 = 64000, which is a bit more than 3 epochs
  train_batch_size: 128

  # if using model parallelism, this is useful:
  tensor_parallel_axes: ["mlp", "heads"]
optimizer:
  learning_rate: 3e-4
  weight_decay: 0.0
