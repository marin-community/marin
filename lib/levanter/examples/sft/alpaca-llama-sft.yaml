# Model configuration
model:
  type: llama
  seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_heads: 32
  num_kv_heads: 32
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: false

# Training configuration
trainer:
  mp: p=f32,c=bfloat16
  tracker:
    type: wandb
    project: "levanter-sft"
    tags: ["llama", "sft"]
  num_train_steps: 750000
  train_batch_size: 64
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  steps_per_eval: 1000

# Optimizer settings
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
  min_lr_ratio: 0.1
  warmup: 100

# Supervised data configuration
supervised_data:
  cache_dir: "gs://levanter-checkpoints/marin/sft_cache/alpaca-olmo"
  input_field: "instruction"
  output_field: "output"
  hf_dataset_name: "tatsu-lab/alpaca"  # Changed from id
  hf_dataset_split: "train"
  name: "alpaca"  # Optional metadata
  tags: ["instruction-tuning"]  # Optional metadata
  validation_urls: []  # Empty list for no validation files

# Additional settings
tokenizer: "allenai/OLMo-1B"
max_tune_length: 2048
epoch: 0

initialize_from_hf: false
