data:
  # Multi-document splice from Wikimedia with coverage-balanced placement
  tokenizer: "meta-llama/Meta-Llama-3.1-8B"
  base:
    cache_dir: null
    shuffle: false
    shuffle_per_epoch: true
    configs:
      common_pile/wikimedia:
        cache_dir: "gs://marin-us-central2/tokenized/common_pile/wikimedia-53a667"
        train_urls:
          - "gs://marin-us-central2/raw/common_pile/wikimedia_filtered-0641bb8"
        validation_urls: []
        tags: []
        format:
          text_key: "text"
    train_weights:
      common_pile/wikimedia: 1
    max_train_batches:
      common_pile/wikimedia: 1
    mixture_block_size: 2048
    cache_options:
      batch_size: 128
      num_shard_groups: 128
      target_size_per_flush: "512MB"

  # Stream infinitely via per-epoch reshuffle
  shuffle_per_epoch: true

  dataset_name: common_pile/wikimedia

  # Select N documents near ~40k tokens. If fewer than N satisfy the bounds, FAIL loudly.
  num_docs: 20                 # adjust as needed
  min_doc_length: 38000       # around 40k
  max_doc_length: 42000
  doc_select_mode: longest    # among those in range, prefer longer
  strict_num_docs: true       # required to error when not enough docs match

  # Splice parameters (balanced placement)
  content_length: 4096
  content_stride: 1
  offset_stride: 1
  content_start_mode: coverage_balanced
  min_copy_len: 128
  alpha: 0.8

  # Multi-doc balancing using temperature over lengths
  balance_mode: by_temperature
  balance_tau: 0.7
  adaptive_k: true
  offset_jitter: 2

model:
  type: llama
  hidden_dim: 1024
  intermediate_dim: 3584
  num_heads: 16
  num_kv_heads: 8
  num_layers: 24
  seq_len: 4096
  gradient_checkpointing: true
  tie_word_embeddings: false
  reference_checkpoint: "NousResearch/Llama-2-7b-hf"
  activation_function: silu
  initializer_range: 0.02
  layer_norm_epsilon: 0.00001
  use_bias: false
  scan_layers: true
  rope:
    type: llama3
    theta: 500000
    factor: 8
    low_freq_factor: 1
    high_freq_factor: 4
    original_max_position_embeddings: 8192

trainer:
  seed: 0
  ray:
    auto_start_cluster: false
    start_workers: false
  tracker:
    type: wandb
    project: "marin"
    tags: ["memorize", "splice", "600m", "wikimedia", "central2", "multi", "40k"]
    resume: allow
    save_code: true
  wandb:
    project: "marin"
    name: "llama_600m_splice_wikimedia_multi_40k_tau07"

  mp: p=f32,c=bfloat16
  model_axis_size: 1
  per_device_parallelism: -1

  train_batch_size: 256
  num_train_steps: 15000
  allow_nondivisible_batch_size: true

  steps_per_eval: 1000000
  max_eval_batches: 10

  checkpointer:
    save_interval: 100000h
    keep:
      - every: 1000000

  tensor_parallel_axes: ["heads", "mlp"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  axis_resources:
    token: ["replica", "data"]
    token_repeat: ["replica", "data"]

optimizer:
  learning_rate: 0.002
  weight_decay: 0.0
  beta1: 0.9
  beta2: 0.95
  warmup: 0.01
  min_lr_ratio: 0.0
  epsilon: 1e-08
  lr_schedule: cosine
  max_grad_norm: 1

data_seed: 1
z_loss_weight: 0
log_entropy: false

