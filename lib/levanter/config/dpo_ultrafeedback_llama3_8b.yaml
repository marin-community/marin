# Example DPO config for train_dpo.py using the Ultrafeedback preference dataset.
# Update the GCS paths to match your transformed preference dataset and desired cache location.
# python infra/launch.py --zone us-central1-a --tpu_name debug --tpu_type v5p-8 -- python src/levanter/main/train_dpo.py --config_path config/dpo_ultrafeedback_llama3_8b.yaml
data:
  train_urls:
    # Replace with the output path from transform_preference_dataset_step (train_prefs split).
    - gs://marin-us-central1/preference/HuggingFaceH4--ultrafeedback_binarized-3949bf5-69e206/train_prefs/*.jsonl.gz
  format:
    type: preference_chat
  tokenizer: stanford-crfm/marin-tokenizer
  cache_dir: gs://marin-us-central1/tokenized/ultrafeedback_binarized_marin_tokenizer-651338
  shuffle: true

model:
  type: llama
  max_seq_len: 4096
  hidden_dim: 4096
  intermediate_dim: 14336
  num_layers: 32
  num_heads: 32
  num_kv_heads: 8
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: true
  initializer_range: 0.02
  rope:
    type: "llama3"

train_seq_len: 4096

trainer:
  seed: 0
  per_device_parallelism: -1
  per_device_eval_parallelism: 8
  tracker:
    type: wandb
    project: "marin"
    tags: ["dpo", "ultrafeedback", "llama3"]
  mp: p=f32,c=bfloat16
  train_batch_size: 64
  num_train_steps: 5000
  steps_per_eval: 100
  model_averaging: null
  checkpointer:
    save_interval: 30m
    base_path: gs://marin-us-central1/checkpoints/dpo/ultrafeedback_llama3_8b

optimizer:
  learning_rate: 4e-5
  weight_decay: 0.0
  min_lr_ratio: 0.0
  lr_schedule: "linear"
  warmup: 0.03

reference_model_path: meta-llama/Llama-3.1-8B
reference_is_hf: true
beta: 0.1
validation_split_fraction: 0.1

initialize_from_hf: meta-llama/Llama-3.1-8B
use_hf_model_config: false

hf_save_steps: 1000
hf_save_path: gs://marin-us-central1/checkpoints/dpo/ultrafeedback_llama3_8b/hf
