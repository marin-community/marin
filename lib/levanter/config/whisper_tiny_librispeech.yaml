data:
  cache_dir: "gs://diva-flash/processed/mixture"
  # The Whisper Tokenizer is way too large for Librispeech
  tokenizer: "facebook/wav2vec2-base-960h"
  configs:
    librispeech:
      id: WillHeld/librispeech_parquet
      cache_dir: "gs://diva-flash/processed/librispeech"
      train_split: "train.360"
      validation_split: "validation"
  train_weights:
    librispeech: 1.0
model:
  type: whisper
  vocab_size: 32
trainer:
  tracker:
    - type: wandb
      project: "levanter"
      tags: [ "librispeech", "whisper"]

  mp: p=f32,c=bf16
  model_axis_size: 1
  per_device_parallelism: -1

  train_batch_size: 128
  num_train_steps: 16000
optimizer:
  learning_rate: 3E-3
  weight_decay: 0.1
  warmup: 0.01
hf_save_steps: 16000
