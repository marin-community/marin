data:
  components:
    main:
      source:
        type: hf
        id: dlwh/wikitext_103_detokenized
model:
  type: gpt2_hyena
  hyena:
    hidden_dim: 32
    filter_order: 16
  num_layers: 2
trainer:
  num_train_steps: 100
  require_accelerator: false
  checkpointer:
    keep:
    - every: 50
    save_interval: 5m
  per_device_parallelism: -1
  train_batch_size: 32
  mesh:
    shared_mapping:
      hyena_filter_order: model
      mlp: model
