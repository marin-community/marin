data:
  cache_dir: gs://marin-data/tokenized/OLMo-1B/dolma-v1.7
  tokenizer: allenai/OLMo-1B
  stop_strategy: restart
  shuffle_buffer_size: 100000
  train_weights:
    dolma-algebraic-stack: 12.6
    dolma-arxiv: 28.0
    dolma-gutenberg: 5.3
    dolma-c4: 69.2
    dolma-cc: 597.75
    dolma-cc-news: 14.3
    dolma-falcon: 456.4
    dolma-megawika: 4.6
    dolma-owmath: 12.6
    dolma-pes2o: 57.2
    dolma-reddit: 79.9
    dolma-stackexchange: 19.6
    dolma-starcoder: 263.8
    dolma-flan: 16.5
    dolma-wiki: 7.4
    paloma/4chan: 0.0
    paloma/c4_100_domains: 0.0
    paloma/c4_en: 0.0
    paloma/dolma-v1_5: 0.0
    paloma/dolma_100_programing_languages: 0.0
    paloma/dolma_100_subreddits: 0.0
    paloma/falcon-refinedweb: 0.0
    paloma/gab: 0.0
    paloma/m2d2_s2orc_unsplit: 0.0
    paloma/m2d2_wikipedia_unsplit: 0.0
    paloma/manosphere_meta_sep: 0.0
    paloma/mc4: 0.0
    paloma/ptb: 0.0
    paloma/redpajama: 0.0
    paloma/twitterAAE_HELM_fixed: 0.0
    paloma/wikitext_103: 0.0
  components:
    dolma-algebraic-stack:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/algebraic-stack-train-{0000..0015}.json.gz
    dolma-arxiv:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/arxiv-{0000..0099}.json.gz
    dolma-gutenberg:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/books-{0000..0002}.json.gz
    dolma-c4:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/c4-{0000..0170}.json.gz
    dolma-cc:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_en_head-{0000..0274}.json.gz
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_en_middle-{0000..0238}.json.gz
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_en_middle-{0240..0379}.json.gz
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_en_tail-{0000..0152}.json.gz
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_en_tail-{0154..0444}.json.gz
    dolma-cc-news:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_news_head-{0000..0004}.json.gz
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_news_middle-{0000..0002}.json.gz
        - gs://marin-data/raw/dolma/dolma-v1.7/cc_news_tail-0000.json.gz
    dolma-falcon:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/falcon-{0000..0499}.json.gz
    dolma-megawika:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/megawika-{0000..0261}.json.gz
    dolma-owmath:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/open-web-math-train-{0000..0012}.json.gz
    dolma-pes2o:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/pes2o-{0000..0025}.json.gz
    dolma-reddit:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/reddit-{0000..0077}.json.gz
    dolma-stackexchange:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/stackexchange-{0000..0025}.json.gz
    dolma-starcoder:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/starcoder-{0000..0048}.json.gz
    dolma-flan:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/tulu_flan-{0000..0065}.json.gz
    dolma-wiki:
      source:
        type: url
        train_urls:
        - gs://marin-data/raw/dolma/dolma-v1.7/wiki-{0000..0001}.json.gz
    paloma/4chan:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/4chan_meta_sep/val/val*.jsonl.gz
    paloma/c4_100_domains:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/c4_100_domains/val/val*.jsonl.gz
    paloma/c4_en:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/c4_en/val/val*.jsonl.gz
    paloma/dolma-v1_5:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/dolma-v1_5/val/val*.jsonl.gz
    paloma/dolma_100_programing_languages:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/dolma_100_programing_languages/val/val*.jsonl.gz
    paloma/dolma_100_subreddits:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/dolma_100_subreddits/val/val*.jsonl.gz
    paloma/falcon-refinedweb:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/falcon-refinedweb/val/val*.jsonl.gz
    paloma/gab:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/gab/val/val*.jsonl.gz
    paloma/m2d2_s2orc_unsplit:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/m2d2_s2orc_unsplit/val/val*.jsonl.gz
    paloma/m2d2_wikipedia_unsplit:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/m2d2_wikipedia_unsplit/val/val*.jsonl.gz
    paloma/manosphere_meta_sep:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/manosphere_meta_sep/val/val*.jsonl.gz
    paloma/mc4:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/mc4/val/val*.jsonl.gz
    paloma/ptb:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/ptb/val/val*.jsonl.gz
    paloma/redpajama:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/redpajama/val/val*.jsonl.gz
    paloma/twitterAAE_HELM_fixed:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/twitterAAE_HELM_fixed/val/val*.jsonl.gz
    paloma/wikitext_103:
      source:
        type: url
        validation_urls:
        - gs://levanter-data/paloma/wikitext_103/val/val*.jsonl.gz
model:
  type: llama
  max_seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_heads: 32
  num_kv_heads: 32
  use_bias: false
  use_layer_norm_weight: false
train_seq_len: 2048
trainer:
  tracker:
    type: wandb
    project: marin
    tags:
    - dolma
    - olmo
    - llama
  mp: p=f32,c=bfloat16
  train_batch_size: 2048
  num_train_steps: 750000
  steps_per_eval: 1000
optimizer:
  learning_rate: 3E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1
  beta1: 0.9
  beta2: 0.95
  warmup: 2000
