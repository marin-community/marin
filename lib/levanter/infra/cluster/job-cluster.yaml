# Configures a Ray Cluster with TPU Slices of various sizes
# If you're at Stanford CRFM, you probably don't need to change this file
# If you're not at Stanford CRFM, you should change this file to match your GCP project
# Specifically:
# - Change `project_id` to your GCP project
# - Change the `availability_zone` to match where you have TPUs available
# - Change the `region` to match where you have TPUs available
# - Change to the TPU quota you have available
# cf: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/gcp/example-full.yaml
# cf: https://docs.ray.io/en/latest/cluster/vms/references/ray-cluster-configuration.html
# Unique Identifier for the Head Node + Workers
cluster_name: levanter-cluster

# Configure GCP
provider:
  type: gcp
  region: us-west4
  availability_zone: us-west4-a
  project_id: hai-gcp-models

# Maximum Workers (excluding Head Node)
max_workers: 1024
upscaling_speed: 4.0  # for bursty

# List of Available Node Types
available_node_types:
  # Head Node =>> On-Demand, sets Min/Max Workers = 0 (Prevent Scheduling Tasks on Head Node)
  head_default:
    min_workers: 0
    max_workers: 0
    resources: {"CPU": 32}

    # GCP-Specific Configuration; by default, Ray will configure unspecified fields (e.g., subnets, ssh-keys)
    #   => Ref: https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
    node_config:
      machineType: n2-standard-2

      # Create a Persistent Disk w/ 100 GBs
      disks:
        - boot: true
          autoDelete: true
          type: PERSISTENT
          initializeParams:
            diskSizeGb: 100

            # Set Source Image =>> Ubuntu 22.04 Base VM
            sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2204-lts

  # Worker Nodes =>>
  tpu_slice_v4_8:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v4-8
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v4_32:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v4-32
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v4_64:
    min_workers: 0
    max_workers: 1024
    resources: {"CPU": 120, "TPU": 4}

    node_config:
      acceleratorType: v4-64
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  # more slices
  tpu_slice_v4_128:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v4-128
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v4_256:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v4-256
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v4_512:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v4-512
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v5e_16:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v5litepod-16
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v5e_64:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v5litepod-64
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

  tpu_slice_v5e_256:
    min_workers: 0
    max_workers: 1024
    resources: { "CPU": 120, "TPU": 4 }

    node_config:
      acceleratorType: v5litepod-256
      runtimeVersion: tpu-ubuntu2204-base

      # [IMPORTANT] Configure all TPU Workers to be Preemptible!
      schedulingConfig:
        preemptible: true

docker:
    image: "ghcr.io/stanford-crfm/levanter-cluster:latest"
    container_name: "ray_docker"
    pull_before_run: true
    worker_run_options:
        - --privileged
        - --ulimit memlock=-1:-1  #
        - --shm-size=32gb
        - -e TPU_WORKER_ID
        - -v "/tmp:/tmp"
        # this lets the worker run docker commands and have them run as sibling containers
        - -v "/var/run/docker.sock:/var/run/docker.sock"

initialization_commands:
  - yes | gcloud auth configure-docker us-west4-docker.pkg.dev
  - "export TPU_WORKER_ID=$(curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/instance/attributes/agent-worker-number) || true"
  - which docker || (curl -fsSL https://get.docker.com -o get-docker.sh; sudo sh get-docker.sh; sudo usermod -aG docker $USER; sudo systemctl restart docker -f)
  # always run this because ray doesn't run with sudo
  - sudo usermod -aG docker $USER
  # we want to launch docker containers from inside docker, which means we need to loosen the permissions on the docker
  # socket. This isn't the best security practice, but it's the easiest way to get this working.
  - sudo chmod 666 /var/run/docker.sock

head_setup_commands:
  - mkdir $HOME/.cache/huggingface -p
  - gcloud secrets versions access latest --secret=HF_TOKEN > $HOME/.cache/huggingface/token || true

worker_setup_commands:
  - mkdir $HOME/.cache/huggingface -p
  - gcloud secrets versions access latest --secret=HF_TOKEN > $HOME/.cache/huggingface/token || true

# Set Head Node == `ray_head_default`
head_node_type: head_default
