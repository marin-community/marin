# Setting up a TPU Cluster

This guide will walk you through the steps to set up and manage a TPU cluster using Google Cloud Platform (GCP) and Ray.

## Prerequisites

Before you begin, ensure you have:
- Completed the [basic installation](installation.md)
- Access to a Google Cloud project with TPU quota
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) installed
- [Ray](https://docs.ray.io/en/latest/installation.html) installed

## TPU Cluster Architecture

A typical TPU cluster consists of:
- **Head Node**: A persistent GCP VM that coordinates the cluster
- **Worker Nodes**: Autoscaling TPU VMs that execute the actual computation

## Cluster Configuration

The cluster configuration is defined in a [YAML template file](../../infra/marin-cluster-template.yaml). This template allows you to customize various aspects of your cluster:

- Machine types for head and worker nodes
- Number of CPUs and amount of RAM
- Disk size and type
- TPU types and versions
- Minimum and maximum worker counts
- Docker images and initialization commands
- Network configuration
- Environment variables and secrets

For our clusters, each config is in a separate file in the `infra` directory. These files are automatically generated by the
`infra/update-cluster-configs.py` script, which reads the `infra/marin-cluster-template.yaml` file.
More information can be found in the [infra/README.md](../../infra/README.md).

## Setting Up the Cluster

1. Install Marin with TPU support:
   ```bash
   pip install -e ".[tpu]"
   ```

2. Set up your GCP environment:
   ```bash
   # Configure Docker for GCP artifact registry
   gcloud auth configure-docker us-central2-docker.pkg.dev
   gcloud auth configure-docker europe-west4-docker.pkg.dev
   gcloud auth configure-docker us-west4-docker.pkg.dev
   ```

3. Start the cluster (replace `$CLUSTER` with your desired region, e.g., `us-central2`):
   ```bash
   ray up -y infra/marin-$CLUSTER.yaml
   ```

## Executing a Job on the cluster

### Using Ray Submit

1. Connect to the cluster using ray dashboard:
   ```bash
   ray dashboard infra/marin-$CLUSTER.yaml
   ```
2. In another terminal, submit a job using ray submit:
   ```bash
   ray job submit --working-dir . -- python experiments/tutorials/hello_world.py
   ```

### Using Ray Run

1. Connect to the cluster using ray dashboard:
   ```bash
   ray dashboard infra/marin-$CLUSTER.yaml
   ```
2. We have made a thin wrapper on top of `ray submit` called [ray-run](../../marin/run/ray_run.py) which can be used to easily specify the pip requirements and environment variables.
   ```bash
   python marin/run/ray_run.py --env_vars WANDB_API_KEY ${WANDB_API_KEY}  --pip_deps jax==0.4.35,async-lru --  python experiments/tutorials/hello_world.py
   ```

## Managing the Cluster

### Basic Commands

```bash
# Monitor cluster status
ray status infra/marin-$CLUSTER.yaml

# SSH into head node
ray attach infra/marin-$CLUSTER.yaml

# View autoscaler logs
ray exec infra/marin-$CLUSTER.yaml "tail -n 100 -f /tmp/ray/session_latest/logs/monitor*"

# Stop the cluster
ray down -y infra/marin-$CLUSTER.yaml
```

## Best Practices

1. **Data Location**: Use a bucket in the same region as your cluster to minimize data transfer costs.

2. **Preemption Handling**: Most compute is preemptible, meaning VMs can be shut down at any time. Design your jobs to:
   - Be idempotent (can run multiple times safely)
   - Handle interruptions gracefully
   - Save checkpoints regularly

3. **Resource Management**:
   - Avoid running heavy computation on the head node
   - Break jobs into tasks that complete within 1-10 minutes
   - Use appropriate TPU types for your workload (v4-8 for v4 clusters, v5e-1 for v5e clusters)

## Troubleshooting

If you encounter issues:

1. **Cluster Not Scaling**: Check autoscaler logs for errors:
   ```bash
   ray exec infra/marin-$CLUSTER.yaml "tail -n 100 -f /tmp/ray/session_latest/logs/monitor*"
   ```

2. **Worker Connection Issues**: If workers aren't connecting, try:
   - Checking GCP Console for VM/TPU status
   - Reviewing Ray dashboard on the head node
   - Restarting the cluster if needed

3. **Cluster State Issues**: If the cluster state becomes inconsistent:
   - Bring down the cluster completely
   - Delete any orphaned resources in GCP Console
   - Bring the cluster back up

## Next Steps

1. Try running a basic training job to verify your setup
2. Read the [Ray documentation](https://docs.ray.io/en/latest/cluster/key-concepts.html) for more details on cluster management
3. Explore the [Language Modeling Pipeline](../explanations/lm-pipeline.md) to understand how Marin uses TPUs for training
