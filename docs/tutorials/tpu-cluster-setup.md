# Setting up a TPU Cluster

This guide will walk you through the steps to set up and manage a TPU cluster using [Google Cloud Platform (GCP)](https://cloud.google.com/tpu/docs/ray-guide) and [Ray](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/gcp.html). Currently, this is the only way to use multinode TPU (i.e. more than a v4-8) with Marin.
## Prerequisites

Before you begin, ensure you have:
- Completed the [basic installation](installation.md)
- Access to a Google Cloud project with TPU quota
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) installed
- [Ray](https://docs.ray.io/en/latest/installation.html) installed

## TPU Cluster Architecture

A typical TPU cluster consists of:
- **Head Node**: A persistent GCP VM that coordinates the cluster
- **Worker Nodes**: Autoscaling TPU VMs that execute the actual computation

## Cluster Configuration

The cluster configuration is defined in a [YAML template file](../../infra/marin-cluster-template.yaml). This template allows you to customize various aspects of your cluster:

- Machine types for head and worker nodes
- Number of CPUs and amount of RAM
- Disk size and type
- TPU types and versions
- Minimum and maximum worker counts
- Docker images and initialization commands
- Network configuration
- Environment variables and secrets. We use [GCP secrets manager](https://cloud.google.com/sdk/gcloud/reference/secrets) to store secrets.

For our clusters, each config is in a separate file in the `infra` directory. These files are automatically generated by the
`infra/update-cluster-configs.py` script, which reads the `infra/marin-cluster-template.yaml` file.
More information can be found in the [infra/README.md](../../infra/README.md).

## Building and Uploading the Docker Image

We use docker images to run the jobs inside the ray cluster. We provide a convieninet make target `make cluster_docker` to build the docker image and upload it to GCP artifact registry for various different regions. This make target performs the following steps:

1. Build the docker image inside [Dockerfile.cluster](../../docker/marin/Dockerfile.cluster)
2. Tag the docker image with the different region names
3. Push the docker images to respective GCP artifact registries


## Setting Up the Cluster

1. Install Marin with TPU support:
   ```bash
   pip install -e ".[tpu]"
   ```

2. Set up your GCP environment:
   ```bash
   # Configure Docker for GCP artifact registry
   gcloud auth configure-docker <region>-docker.pkg.dev
   ```

3. Start the cluster (replace `$REGION` with your desired region, e.g., `us-central2`):
   ```bash
   ray up -y infra/marin-$REGION.yaml
   ```

## Executing a Job on the cluster

For running any meaningful job on the cluster, you will have to make a GCS bucket and also set the `MARIN_PREFIX` environment variable to the bucket name, e.g:- `gs://marin-us-central2`. We use this bucket to store all the checkpoints, datasets, and other artifacts.

### Using Ray Submit

1. Connect to the cluster using ray dashboard:
   ```bash
   ray dashboard infra/marin-$REGION.yaml
   ```
2. In another terminal, submit a job using ray submit:
   ```bash
   ray job submit --working-dir . -- python experiments/tutorials/hello_world.py
   ```

### Using Ray Run

1. Connect to the cluster using ray dashboard:
   ```bash
   ray dashboard infra/marin-$REGION.yaml
   ```
2. We have made a thin wrapper on top of `ray submit` called [ray-run](../../marin/run/ray_run.py) which can be used to easily specify the additional pip requirements and environment variables (Apart from those specified in the Dockerfile or cluster config). Ray run ensures following stuff:
   - All the dependencies in pyproject.toml are installed
   - Some of the necessary environment variables are set from file [vars.py](../../marin/run/vars.py)
   - `src` directories of submodules are added to `PYTHONPATH`. This is useful for [co-developing](./co-develop.md)


You can use `--env_vars` and `--pip_deps` to specify additional environment variables and pip dependencies. For example, to run a job with additional pip dependencies `jax==0.4.35 and async-lru, along with the environment variable WANDB_API_KEY, you can use:
```bash
python marin/run/ray_run.py --env_vars WANDB_API_KEY ${WANDB_API_KEY}  --pip_deps jax==0.4.35,async-lru --  python experiments/tutorials/hello_world.py
```

## Managing the Cluster

### Basic Commands

```bash
# Monitor cluster status
ray status infra/marin-$REGION.yaml

# SSH into head node
ray attach infra/marin-$REGION.yaml

# View autoscaler logs
ray exec infra/marin-$REGION.yaml "tail -n 100 -f /tmp/ray/session_latest/logs/monitor*"

# Stop the cluster
ray down -y infra/marin-$REGION.yaml
```

## Best Practices

1. **Data Location**: Use a bucket in the same region as your cluster to minimize data transfer costs.

2. **Preemption Handling**: If your compute is preemptible, meaning VMs can be shut down at any time. Design your jobs to:
   - Be idempotent (can run multiple times safely)
   - Handle interruptions gracefully
   - Save checkpoints regularly

3. **Resource Management**:
   - Avoid running heavy computation on the head node
   - Break jobs into tasks that complete within 1-10 minutes
   - Use appropriate TPU types for your workload (v4-8 for v4 clusters, v5e-1 for v5e clusters)

## Troubleshooting

If you encounter issues:

1. **Cluster Not Scaling**: Check autoscaler logs for errors:
   ```bash
   ray exec infra/marin-$REGION.yaml "tail -n 100 -f /tmp/ray/session_latest/logs/monitor*"
   ```

2. **Worker Connection Issues**: If workers aren't connecting, try:
   - Checking GCP Console for VM/TPU status
   - Reviewing Ray dashboard on the head node
   - Restarting the cluster if needed

3. **Cluster State Issues**: If the cluster state becomes inconsistent:
   - Bring down the cluster completely
   - Delete any orphaned resources in GCP Console
   - Bring the cluster back up

## Next Steps

1. Try running a [basic training](./first-experiment.md) job to verify your setup
2. Explore the [Language Modeling Pipeline](./train-an-lm.md) to understand how Marin uses TPUs for training
3. Read the [Ray documentation](https://docs.ray.io/en/latest/cluster/key-concepts.html) for more details on cluster management
4. Levanter Ray autoscaling [setup](https://levanter.readthedocs.io/en/latest/Getting-Started-TPU-VM/#using-the-ray-autoscaler)
