# How to Add a New Optimizer in Marin

Marin builds on [Levanter](https://levanter.readthedocs.io/) for training code, meaning any training changes made in Levanter are automatically available in Marin. However, you can also add new optimizers **directly in Marin**- thanks to Levanter‚Äôs support for [Optax](https://optax.readthedocs.io/)-- without needing to merge a pull request upstream.

This makes it easy to experiment with new optimizers (especially for speedruns) before integrating them into Levanter.

In this guide, we‚Äôll walk through adding an [AdaMax](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.adamax) optimizer as an example.

---

## Steps to Add an Optimizer

1. Import Optax and OptimizerConfig:

```python
import optax
from levanter.optim import OptimizerConfig
```

2. Define a new optimizer by subclassing `OptimizerConfig` and add optimizer-specific parameters as class variables:

```python
@dataclass
class AdamaxConfig(OptimizerConfig):
    beta1: float = 0.9
    beta2: float = 0.95
    epsilon: float = 1e-8
    max_grad_norm: float | None = 1.0
```
`OptimizerConfig` has a number of fields that are common to all optimizer; these include `weight_decay`, `learning_rate`, `lr_schedule`, `min_lr_ratio`, `warmup`, `decay`, `rewarmup`, `cycles`, and `cycle_length`. You can find documentation for the OptimizerConfig class, along with further details about the fields [here](https://levanter.readthedocs.io/en/latest/reference/Configuration/#standard-options).

3. Implement the `build()` method to define the optimizer's update rule. This method should return an Optax optimizer. Optax allows you to define components that are gradient transformations, and then chain them together to obtain a final gradient update rule.

```python
def build(self, num_train_steps):
    print(f"Building optimizer: {self.__class__.__name__}")

    # Register the optimizer class if not already registered
    try:
        OptimizerConfig.register_subclass("adamax")(AdamaxConfig)
    except ValueError:
        pass

    def _optimizer(learning_rate):
        components = []

        # Add gradient clipping if specified
        if self.max_grad_norm is not None:
            components.append(optax.clip_by_global_norm(self.max_grad_norm))

        # Add the Adamax optimizer
        components.append(
            optax.adamax(
                b1=self.beta1,
                b2=self.beta2,
                eps=self.epsilon,
            )
        )

        # Add weight decay if specified
        if self.weight_decay > 0:
            components.append(
                optax.add_decayed_weights(
                    self.weight_decay,
                    self.build_weight_decay_mask()
                )
            )

        # Scale the learning rate
        components.append(optax.scale(-learning_rate))

        # Chain all components together
        optimizer = optax.chain(*components)
        return optimizer

    # Inject hyperparameters using the learning rate scheduler
    return optax.inject_hyperparams(_optimizer)(
        learning_rate=self.lr_scheduler(num_train_steps)
    )
```

Note that `optax.inject_hyperparams` is a wrapper in Optax that can be used to pass schedules (or stateful hyperparameters) into the optimizer. This also allows us to log the learning rate in the tracker.

!!! note

     You should also register your optimizer class with an identifier, as shown above.

5. Use the optimizer in your training script: You can instantiate and pass it directly into your training config:

```python
optimizer = AdamaxConfig(
    beta1=0.9,
    beta2=0.95,
    epsilon=1e-8,
    max_grad_norm=1.0,
    weight_decay=0.1,
    lr=1e-4
)
```

and use it in `TrainLmConfig`:

```python
from levanter.trainer import TrainLmConfig

trainer_config = TrainLmConfig(
    ...
    optimizer=optimizer,
    ...
)
```

Or inside a `SimpleTrainConfig`:

```python
train_config = SimpleTrainConfig(
    ...
    optimizer_config=AdamaxConfig(
        beta1=0.9,
        beta2=0.95,
        epsilon=1e-8,
        max_grad_norm=1.0,
        weight_decay=0.1,
        lr=1e-4
    ),
    ...
)
```

Then pass it into `default_train`, which will set the optimizer config correctly in the training step.

### Sample usage in a speedrun script

Here's an example of a speedrun configuration that leverages `AdamaxConfig`:

```python
speedrun_config = SpeedrunConfig(
    model_config=llama_75m,
    train_config=SimpleTrainConfig(
        TpuPodConfig(tpu_type="v4-128"),
        train_batch_size=512,
        num_train_steps=6000,
        learning_rate=3e-3,
        weight_decay=0.0,
        steps_per_eval=2000,
        steps_per_task_eval=2000,
        optimizer_config=AdamaxConfig(),
    ),
    hardware_config=HardwareConfig(
        device_type="v4-128",
        num_devices=64,
        device_flops=275e12,  # from https://cloud.google.com/tpu/docs/v4
    ),
)

if __name__ == "__main__":
    executor_main(steps=default_speedrun("75M_llama_adamax", speedrun_config))
```

üéâ That‚Äôs it! You can now define new optimizers in this manner and train models using them, all within Marin. For optimizers that are widely useful or ‚Äústandard,‚Äù consider submitting a pull request to Levanter.
See a full working example [in this GitHub link](https://github.com/marin-community/marin/blob/main/experiments/speedrun/llama_75m_fineweb_edu_adamax/llama_75m_fineweb_edu_adamax.py).

Further reading:

- [Levanter Optimizer Documentation](https://levanter.readthedocs.io/en/latest/optimizers.html)
- [Optax Optimizer Reference](https://optax.readthedocs.io/en/latest/api/optimizers.html)
