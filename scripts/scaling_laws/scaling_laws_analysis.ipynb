{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "from collections.abc import Sequence\n",
    "from typing import Any\n",
    "\n",
    "from levanter.data.text import LMMixtureDatasetConfig\n",
    "from levanter.models.llama import LlamaConfig\n",
    "from experiments.llama import llama_1_4b\n",
    "from experiments.simple_train_config import SimpleTrainConfig\n",
    "from marin.execution.executor import ExecutorStep, InputName\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd: Any = None\n",
    "\n",
    "def _round_to_multiple(x, multiple):\n",
    "    return int(multiple * round(x / multiple))\n",
    "\n",
    "ENTITY = \"stanford-mercury\"\n",
    "PROJECT = \"marin\"\n",
    "\n",
    "def pull_metrics_from_wandb(runs: Sequence[str],\n",
    "                            metrics: Sequence[str],\n",
    "                            x_axis: str = \"throughput/total_gflops\",\n",
    "                            summary_fields: Sequence[str] = (\"parameter_count\",)\n",
    "                            ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls metrics from wandb for a set of runs.\n",
    "\n",
    "    Args:\n",
    "        runs: runs to pull metrics from\n",
    "        metrics: metrics to pull\n",
    "        summary_fields: list of summary fields to include, defaults to ['parameter_count']\n",
    "\n",
    "    Returns:\n",
    "        DataFrame where rows are (run, step), columns are metrics and run properties\n",
    "    \"\"\"\n",
    "    import wandb\n",
    "    import pandas as pd\n",
    "\n",
    "    print(f\"Pulling metrics for runs {runs}\")\n",
    "\n",
    "    data = []\n",
    "    for run_id in runs:\n",
    "        api = wandb.Api()\n",
    "\n",
    "        print(\"API: \", api)\n",
    "        run = api.run(f\"{ENTITY}/{PROJECT}/{run_id}\")\n",
    "\n",
    "        print(f\"Pulling metrics for run {run.name}\")\n",
    "        run_data = {'run': run.name}\n",
    "\n",
    "        # Add specified summary fields\n",
    "        for field in summary_fields:\n",
    "            run_data[field] = run.summary.get(field, None)\n",
    "\n",
    "        history = run.history(keys=metrics, x_axis=x_axis)\n",
    "        for i in range(len(history)):\n",
    "            step_data = {f'{metric}': history[metric][i] for metric in metrics}\n",
    "            step_data.update(run_data)  # Include run properties\n",
    "            step_data['step'] = i\n",
    "            data.append(step_data)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = [\n",
    "    \"tootsie-scaling-512-81c36c\",\n",
    "    \"tootsie-scaling-768-d17a90\",\n",
    "    \"tootsie-scaling-1024-f4e4be\",\n",
    "    \"tootsie-scaling-1536-e2a6d8\",\n",
    "    \"tootsie-scaling-2048-72c648\", # these are trained for 200B tokens max, but have multiple checkpoints\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling metrics for runs ['tootsie-scaling-512-81c36c', 'tootsie-scaling-768-d17a90', 'tootsie-scaling-1024-f4e4be', 'tootsie-scaling-1536-e2a6d8', 'tootsie-scaling-2048-72c648']\n",
      "API:  <wandb.apis.public.api.Api object at 0x283cfde10>\n",
      "Pulling metrics for run tootsie-scaling-512-81c36c\n",
      "API:  <wandb.apis.public.api.Api object at 0x13807d950>\n",
      "Pulling metrics for run tootsie-scaling-768-d17a90\n",
      "API:  <wandb.apis.public.api.Api object at 0x138088290>\n",
      "Pulling metrics for run tootsie-scaling-1024-f4e4be\n",
      "API:  <wandb.apis.public.api.Api object at 0x13812c610>\n",
      "Pulling metrics for run tootsie-scaling-1536-e2a6d8\n",
      "API:  <wandb.apis.public.api.Api object at 0x283dba490>\n",
      "Pulling metrics for run tootsie-scaling-2048-72c648\n"
     ]
    }
   ],
   "source": [
    "# TODO: try out with eval/paloma/c4_en/loss for metric\n",
    "tootsie_metrics = pull_metrics_from_wandb(RUNS, metrics=[\"eval/loss\", \"throughput/total_tokens\"], summary_fields=[\"parameter_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval/loss</th>\n",
       "      <th>throughput/total_tokens</th>\n",
       "      <th>run</th>\n",
       "      <th>parameter_count</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.218495</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>tootsie-scaling-512-81c36c</td>\n",
       "      <td>248791552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.011875</td>\n",
       "      <td>4.194304e+10</td>\n",
       "      <td>tootsie-scaling-512-81c36c</td>\n",
       "      <td>248791552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.958518</td>\n",
       "      <td>8.388608e+10</td>\n",
       "      <td>tootsie-scaling-512-81c36c</td>\n",
       "      <td>248791552</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.945167</td>\n",
       "      <td>1.258291e+11</td>\n",
       "      <td>tootsie-scaling-512-81c36c</td>\n",
       "      <td>248791552</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.930376</td>\n",
       "      <td>1.677722e+11</td>\n",
       "      <td>tootsie-scaling-512-81c36c</td>\n",
       "      <td>248791552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.885564</td>\n",
       "      <td>2.097110e+11</td>\n",
       "      <td>tootsie-scaling-512-81c36c</td>\n",
       "      <td>248791552</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.218498</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>tootsie-scaling-768-d17a90</td>\n",
       "      <td>461267712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.844769</td>\n",
       "      <td>4.194304e+10</td>\n",
       "      <td>tootsie-scaling-768-d17a90</td>\n",
       "      <td>461267712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.787246</td>\n",
       "      <td>8.388608e+10</td>\n",
       "      <td>tootsie-scaling-768-d17a90</td>\n",
       "      <td>461267712</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.764123</td>\n",
       "      <td>1.258291e+11</td>\n",
       "      <td>tootsie-scaling-768-d17a90</td>\n",
       "      <td>461267712</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.751829</td>\n",
       "      <td>1.677722e+11</td>\n",
       "      <td>tootsie-scaling-768-d17a90</td>\n",
       "      <td>461267712</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.722111</td>\n",
       "      <td>2.097110e+11</td>\n",
       "      <td>tootsie-scaling-768-d17a90</td>\n",
       "      <td>461267712</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.240883</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>tootsie-scaling-1024-f4e4be</td>\n",
       "      <td>732464128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.689837</td>\n",
       "      <td>8.388608e+10</td>\n",
       "      <td>tootsie-scaling-1024-f4e4be</td>\n",
       "      <td>732464128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.664604</td>\n",
       "      <td>1.258291e+11</td>\n",
       "      <td>tootsie-scaling-1024-f4e4be</td>\n",
       "      <td>732464128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.647790</td>\n",
       "      <td>1.677722e+11</td>\n",
       "      <td>tootsie-scaling-1024-f4e4be</td>\n",
       "      <td>732464128</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.625665</td>\n",
       "      <td>2.097110e+11</td>\n",
       "      <td>tootsie-scaling-1024-f4e4be</td>\n",
       "      <td>732464128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12.226378</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>tootsie-scaling-1536-e2a6d8</td>\n",
       "      <td>1451017728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.639062</td>\n",
       "      <td>4.194304e+10</td>\n",
       "      <td>tootsie-scaling-1536-e2a6d8</td>\n",
       "      <td>1451017728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.570570</td>\n",
       "      <td>8.388608e+10</td>\n",
       "      <td>tootsie-scaling-1536-e2a6d8</td>\n",
       "      <td>1451017728</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.542662</td>\n",
       "      <td>1.258291e+11</td>\n",
       "      <td>tootsie-scaling-1536-e2a6d8</td>\n",
       "      <td>1451017728</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.506964</td>\n",
       "      <td>2.097110e+11</td>\n",
       "      <td>tootsie-scaling-1536-e2a6d8</td>\n",
       "      <td>1451017728</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12.271461</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>tootsie-scaling-2048-72c648</td>\n",
       "      <td>2337343488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.588113</td>\n",
       "      <td>4.194304e+10</td>\n",
       "      <td>tootsie-scaling-2048-72c648</td>\n",
       "      <td>2337343488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.513157</td>\n",
       "      <td>8.388608e+10</td>\n",
       "      <td>tootsie-scaling-2048-72c648</td>\n",
       "      <td>2337343488</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.481663</td>\n",
       "      <td>1.258291e+11</td>\n",
       "      <td>tootsie-scaling-2048-72c648</td>\n",
       "      <td>2337343488</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.462406</td>\n",
       "      <td>1.677722e+11</td>\n",
       "      <td>tootsie-scaling-2048-72c648</td>\n",
       "      <td>2337343488</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.444145</td>\n",
       "      <td>2.097110e+11</td>\n",
       "      <td>tootsie-scaling-2048-72c648</td>\n",
       "      <td>2337343488</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eval/loss  throughput/total_tokens                          run  \\\n",
       "0   12.218495             0.000000e+00   tootsie-scaling-512-81c36c   \n",
       "1    3.011875             4.194304e+10   tootsie-scaling-512-81c36c   \n",
       "2    2.958518             8.388608e+10   tootsie-scaling-512-81c36c   \n",
       "3    2.945167             1.258291e+11   tootsie-scaling-512-81c36c   \n",
       "4    2.930376             1.677722e+11   tootsie-scaling-512-81c36c   \n",
       "5    2.885564             2.097110e+11   tootsie-scaling-512-81c36c   \n",
       "6   12.218498             0.000000e+00   tootsie-scaling-768-d17a90   \n",
       "7    2.844769             4.194304e+10   tootsie-scaling-768-d17a90   \n",
       "8    2.787246             8.388608e+10   tootsie-scaling-768-d17a90   \n",
       "9    2.764123             1.258291e+11   tootsie-scaling-768-d17a90   \n",
       "10   2.751829             1.677722e+11   tootsie-scaling-768-d17a90   \n",
       "11   2.722111             2.097110e+11   tootsie-scaling-768-d17a90   \n",
       "12  12.240883             0.000000e+00  tootsie-scaling-1024-f4e4be   \n",
       "13   2.689837             8.388608e+10  tootsie-scaling-1024-f4e4be   \n",
       "14   2.664604             1.258291e+11  tootsie-scaling-1024-f4e4be   \n",
       "15   2.647790             1.677722e+11  tootsie-scaling-1024-f4e4be   \n",
       "16   2.625665             2.097110e+11  tootsie-scaling-1024-f4e4be   \n",
       "17  12.226378             0.000000e+00  tootsie-scaling-1536-e2a6d8   \n",
       "18   2.639062             4.194304e+10  tootsie-scaling-1536-e2a6d8   \n",
       "19   2.570570             8.388608e+10  tootsie-scaling-1536-e2a6d8   \n",
       "20   2.542662             1.258291e+11  tootsie-scaling-1536-e2a6d8   \n",
       "21   2.506964             2.097110e+11  tootsie-scaling-1536-e2a6d8   \n",
       "22  12.271461             0.000000e+00  tootsie-scaling-2048-72c648   \n",
       "23   2.588113             4.194304e+10  tootsie-scaling-2048-72c648   \n",
       "24   2.513157             8.388608e+10  tootsie-scaling-2048-72c648   \n",
       "25   2.481663             1.258291e+11  tootsie-scaling-2048-72c648   \n",
       "26   2.462406             1.677722e+11  tootsie-scaling-2048-72c648   \n",
       "27   2.444145             2.097110e+11  tootsie-scaling-2048-72c648   \n",
       "\n",
       "    parameter_count  step  \n",
       "0         248791552     0  \n",
       "1         248791552     1  \n",
       "2         248791552     2  \n",
       "3         248791552     3  \n",
       "4         248791552     4  \n",
       "5         248791552     5  \n",
       "6         461267712     0  \n",
       "7         461267712     1  \n",
       "8         461267712     2  \n",
       "9         461267712     3  \n",
       "10        461267712     4  \n",
       "11        461267712     5  \n",
       "12        732464128     0  \n",
       "13        732464128     1  \n",
       "14        732464128     2  \n",
       "15        732464128     3  \n",
       "16        732464128     4  \n",
       "17       1451017728     0  \n",
       "18       1451017728     1  \n",
       "19       1451017728     2  \n",
       "20       1451017728     3  \n",
       "21       1451017728     4  \n",
       "22       2337343488     0  \n",
       "23       2337343488     1  \n",
       "24       2337343488     2  \n",
       "25       2337343488     3  \n",
       "26       2337343488     4  \n",
       "27       2337343488     5  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tootsie_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 23 23\n"
     ]
    }
   ],
   "source": [
    "# Create a tuple of (N, D) where N is an array of parameter_count and D is an array of throughput/total_tokens\n",
    "\n",
    "N = tootsie_metrics[\"parameter_count\"].values\n",
    "D = tootsie_metrics[\"throughput/total_tokens\"].values\n",
    "eval_losses = tootsie_metrics[\"eval/loss\"].values\n",
    "\n",
    "# remove places where D is 0 across all three arrays\n",
    "N = N[D != 0]\n",
    "eval_losses = eval_losses[D != 0]\n",
    "D = D[D != 0]\n",
    "\n",
    "x = (N, D)\n",
    "y = eval_losses\n",
    "print(len(N), len(D), len(eval_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fit_power_law\n",
    "\n",
    "power_law_params = fit_power_law(x, y, use_log_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.0000000018496602),\n",
       " np.float64(1.0),\n",
       " np.float64(0.9999999652292553),\n",
       " np.float64(0.9999999998410559),\n",
       " np.float64(2.664603709767195))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_law_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/nikilravi/miniconda3/envs/crfm_new/lib/python3.11/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/nikilravi/miniconda3/envs/crfm_new/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.029429058872780533\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABitklEQVR4nO3dd3hUZfrG8XvSJgkp1BBKCEhHigjKBqVIFbCAigooYMFVwYbKYtmlKLAWBHdXARUBRRQbuCsIZFHaBlGqiHQwIC2wQEISkkwy7+8PfpllTGEGkswc8v1c11wXc+aU58yTkDsn73nHZowxAgAAACwowNcFAAAAABeLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAFrJixQrZbDatWLHC16VY2tixY2Wz2XxdBoASQJgFUKjZs2fLZrO5HqGhoWrUqJFGjBihY8eO+bq8S9a5c2c1b97c12UUkB+yCntMnz690G3mzZunqVOnenWcvLw8zZo1S507d1blypVlt9tVt25d3XfffVq/fn0JnAkAlI0gXxcAwL+NHz9e9erVU1ZWltasWaNp06Zp8eLF+vnnnxUeHu7r8i5b06ZNU0REhNuydu3aqX79+jp79qxCQkJcy+fNm6eff/5ZTz75pEf7Pnv2rG677TYtWbJEHTt21PPPP6/KlSvr119/1aeffqo5c+bowIEDql27dkmekl958cUXNXr0aF+XAaAEEGYBFKtXr15q27atJOnBBx9UlSpV9MYbb+irr77SgAEDfFxd0ZxOp3JychQaGurrUi7KHXfcoapVqxb62qWe07PPPqslS5ZoypQpBQLwmDFjNGXKlEvavz/LyMhQhQoVFBQUpKAgfgQClwOGGQDwSpcuXSRJ+/fvlyTl5ubqpZdeUv369V1/qn7++eeVnZ3t2mbkyJGqUqWKjDGuZY899phsNpv+9re/uZYdO3ZMNptN06ZNcy3Lzs7WmDFj1KBBA9ntdsXFxWnUqFFu+5ckm82mESNG6KOPPtKVV14pu92uJUuWXNK5/vTTTxo6dKiuuOIKhYaGKjY2Vvfff7/++9//uq1js9n0z3/+07Vsw4YNstlsuvrqq93216tXL7Vr1+6Savr9mNnOnTtr0aJFSk5Odg1HqFu3bpHb//bbb5oxY4a6d+9e6JXcwMBAPfPMM25XZTdt2qRevXopKipKERER6tq1q77//nu37fKHpaxZs0aPP/64qlWrpooVK+qPf/yjcnJydPr0aQ0ePFiVKlVSpUqVNGrUKLevh19//VU2m02vv/66pkyZovj4eIWFhalTp076+eef3Y7lSV+k/w3Z+OWXXzRw4EBVqlRJ119/vdtr50tMTNT111+vihUrKiIiQo0bN9bzzz/vtk5KSooeeOABVa9eXaGhoWrVqpXmzJnjts755/LOO++4vjeuueYa/fjjj0X2BsDF4ddSAF7Zu3evJKlKlSqSzl2tnTNnju644w49/fTTWrdunSZNmqTt27drwYIFkqQOHTpoypQp2rZtm2uc6urVqxUQEKDVq1fr8ccfdy2TpI4dO0o6d3X1lltu0Zo1a/TQQw+padOm2rp1q6ZMmaJdu3Zp4cKFbrV9++23+vTTTzVixAhVrVq12FDnicTERO3bt0/33XefYmNjtW3bNr3zzjvatm2bvv/+e9lsNjVv3lwVK1bUqlWrdMstt7id25YtW5SWlqaoqCg5nU4lJSXpoYce8ujYJ0+edHseGBioSpUqFVjvhRdeUGpqqn777TfXFdXfD0843zfffKPc3Fzde++9HtWxbds2dejQQVFRURo1apSCg4M1Y8YMde7cWStXriwQzh977DHFxsZq3Lhx+v777/XOO++oYsWKSkpKUp06dTRx4kQtXrxYr732mpo3b67Bgwe7bf/BBx/ozJkzGj58uLKysvTmm2+qS5cu2rp1q6pXry7Js76cr3///mrYsKEmTpzoFqB/f5433XSTWrZsqfHjx8tut2vPnj36z3/+41rn7Nmz6ty5s/bs2aMRI0aoXr16+uyzzzR06FCdPn1aTzzxhNs+582bpzNnzuiPf/yjbDabXn31Vd12223at2+fgoODPXr/AXjAAEAhZs2aZSSZf//73+b48ePm4MGD5pNPPjFVqlQxYWFh5rfffjObN282ksyDDz7otu0zzzxjJJlvv/3WGGNMSkqKkWTefvttY4wxp0+fNgEBAaZ///6mevXqru0ef/xxU7lyZeN0Oo0xxnz44YcmICDArF692m3/06dPN5LMf/7zH9cySSYgIMBs27bNo/Pr1KmTufLKK4tdJzMzs8Cyjz/+2Egyq1atci3r06ePufbaa13Pb7vtNnPbbbeZwMBA88033xhjjNm4caORZL766qtijzlmzBgjqcAjPj7eGGPMd999ZySZ7777zu34+a9fyFNPPWUkmU2bNnm0ft++fU1ISIjZu3eva9nhw4dNZGSk6dixo2tZ/tdLz549Xf0zxpiEhARjs9nMww8/7FqWm5trateubTp16uRatn//fiPJ9bWVb926dUaSeeqpp1zLPO1L/ns5YMCAAuvnv5ZvypQpRpI5fvx4ke/F1KlTjSQzd+5c17KcnByTkJBgIiIiTFpamtu5VKlSxZw8edK17ldffWUkmX/9619FHgOA9xhmAKBY3bp1U7Vq1RQXF6e7775bERERWrBggWrVqqXFixdLOjeM4HxPP/20JGnRokWSpGrVqqlJkyZatWqVJOk///mPAgMD9eyzz+rYsWPavXu3pHNXNK+//nrXlbXPPvtMTZs2VZMmTXTixAnXI3+ow3fffed23E6dOqlZs2Yldu5hYWGuf2dlZenEiRP6wx/+IEnauHGj67UOHTpo48aNysjIkCStWbNGvXv31lVXXeW62rx69WrZbDbXn7kv5IsvvlBiYqLr8dFHH5XIOaWlpUmSIiMjL7huXl6eli1bpr59++qKK65wLa9Ro4YGDhyoNWvWuPaX74EHHnC7MtquXTsZY/TAAw+4lgUGBqpt27bat29fgWP27dtXtWrVcj2/9tpr1a5dO9fXmuR5X/I9/PDDFzzXihUrSpK++uorOZ3OQtdZvHixYmNj3caKBwcH6/HHH1d6erpWrlzptv5dd93ldjW9Q4cOklToeQO4eOU6zK5atUo333yzatasKZvNVuBPliXtzJkzevLJJ11jwdq3b39J46e2bNmiAQMGKC4uTmFhYWratKnefPNNj7ZdtGiR2rVrp7CwMFWqVEl9+/YtsM7s2bPVsmVLhYaGKiYmRsOHDy90X3v27FFkZKTrh8H5Tp8+reHDh6tGjRqy2+1q1KiR2w8lSXrrrbdUt25dhYaGql27dvrhhx/cXv/jH/+o+vXrKywsTNWqVdOtt96qHTt2uNVZ1FRGKSkpkqQjR45o4MCBatSokQICAjy+6xvn+pOYmKjvvvtOv/zyi/bt26eePXtKkpKTkxUQEKAGDRq4bRMbG6uKFSsqOTnZtaxDhw5uwa5t27Zq27atKleurNWrVystLU1btmxx/cCXpN27d2vbtm2qVq2a26NRo0aS5Opvvnr16pXouZ88eVJPPPGEqlev7vr6yz9Gamqq27nl5uZq7dq12rlzp1JSUtShQwd17NjR7ZybNWumypUre3Tsjh07qlu3bq7HddddVyLnFBUVJenc/0cXcvz4cWVmZqpx48YFXmvatKmcTqcOHjzotrxOnTpuz6OjoyVJcXFxBZafOnWqwH4bNmxYYFmjRo3066+/up572pd8nnxd3HXXXbruuuv04IMPqnr16rr77rv16aefugXb5ORkNWzYUAEB7j86mzZt6nr9fL9/L/KDbWHnDeDilesxsxkZGWrVqpXuv/9+3XbbbaV+vAcffFA///yzPvzwQ9WsWVNz585Vt27d9Msvv7hdiThf3bp1NXv2bHXu3LnAaxs2bFBMTIzmzp2ruLg413i8wMBAjRgxosg6vvjiCw0bNkwTJ05Uly5dlJubW+AGizfeeEOTJ0/Wa6+9pnbt2ikjI8Pth0k+h8OhAQMGqEOHDkpKSnJ7LScnR927d1dMTIw+//xz1apVS8nJyW6hd/78+Ro5cqSmT5+udu3aaerUqerZs6d27typmJgYSVKbNm00aNAg1alTRydPntTYsWPVo0cP7d+/X4GBgbrrrrt04403uh176NChysrKcu0jOztb1apV04svvnhZ36ldGq699lrXbAZF8WTy+euvv17vvvuu9u3bp9WrV6tDhw6uK5WrV69WzZo15XQ63cKs0+lUixYt9MYbbxS6z98HpPOv2JWEO++8U0lJSXr22Wd11VVXKSIiQk6nUzfeeKNbyGnbtq1CQ0O1atUq1alTRzExMWrUqJE6dOigt99+W9nZ2Vq9erX69etXovVdjCZNmkiStm7dqquuuqrE9x8YGOjxclPE+NUL8bQv+Tz5uggLC9OqVav03XffadGiRVqyZInmz5+vLl26aNmyZUWeV3GK2uZizxtA4cp1mO3Vq5d69epV5OvZ2dl64YUX9PHHH+v06dNq3ry5XnnllUKD5YWcPXtWX3zxhb766ivXzS1jx47Vv/71L02bNk0vv/yy1/u8//773Z5fccUVWrt2rb788ssiw2xubq6eeOIJvfbaa25/9jv/T7OnTp3Siy++qH/961/q2rWra3nLli0L7O/FF19UkyZN1LVr1wJh9v3339fJkyeVlJTkutnh9zfkvPHGGxo2bJjuu+8+SdL06dO1aNEivf/++645IM+/YaZu3bp6+eWX1apVK/3666+uK7bn/7A6fvy4vv32W82cOdNtu/yr1u+//36h7w28Fx8fL6fTqd27d7uuTknnZiU4ffq04uPjXcvyQ2piYqJ+/PFHV387duyoadOmqWbNmqpQoYLatGnj2qZ+/frasmWLunbtWuaf1nTq1CktX75c48aN01/+8hfX8vwhEecLCQnRtddeq9WrV6tOnTquc+3QoYOys7P10Ucf6dixY67v/ZLmzXvTq1cvBQYGau7cuRe8CaxatWoKDw/Xzp07C7y2Y8cOBQQEFPiF4lIV9v7u2rXL9X+HN33xVkBAgLp27aquXbvqjTfe0MSJE/XCCy/ou+++U7du3RQfH6+ffvpJTqfT7eps/l+Kzv96B1B2yvUwgwsZMWKE1q5dq08++UQ//fST+vfvrxtvvPGi/tPMzc1VXl5egfkhw8LCtGbNmpIqWampqcX+GXPjxo06dOiQAgIC1Lp1a9WoUUO9evVyuzKbmJgop9OpQ4cOqWnTpqpdu7buvPPOAn9O/Pbbb/XZZ5/prbfeKvRY//znP5WQkKDhw4erevXqat68uSZOnKi8vDxJ567cbtiwQd26dXNtExAQoG7dumnt2rWF7jMjI0OzZs1SvXr1ivwh+sEHHyg8PFx33HFHke8DSkbv3r0lqcCnT+VfSe3Tp49rWb169VSrVi1NmTJFDofD9WfzDh06aO/evfr888/1hz/8wW3uzzvvvFOHDh3Su+++W+DYZ8+edY1RLQ35V9V+fxWtqE/a6tChg9atW6fvvvvOFWarVq2qpk2b6pVXXnGtUxoqVKhQ6J/XCxMXF6dhw4Zp2bJl+vvf/17gdafTqcmTJ+u3335TYGCgevTooa+++srtLzPHjh3TvHnzdP3117uGLZSUhQsX6tChQ67nP/zwg9atW+e68OBtXzz1+9kjJLmuXOdPA9e7d28dPXpU8+fPd62Tm5urv//974qIiFCnTp0uqQYAF6dcX5ktzoEDBzRr1iwdOHBANWvWlCQ988wzWrJkiWbNmqWJEyd6tb/IyEglJCTopZdeUtOmTVW9enV9/PHHWrt2bYHxhhcrKSlJ8+fPd910U5j8Gw/Gjh2rN954Q3Xr1tXkyZPVuXNn7dq1S5UrV9a+ffvkdDo1ceJEvfnmm4qOjtaLL76o7t2766efflJISIj++9//aujQoZo7d26RP8z27dunb7/9VoMGDdLixYu1Z88ePfroo3I4HBozZoxOnDihvLw813Q7+apXr+42JlaS3n77bY0aNUoZGRlq3LixEhMT3T4B6XwzZ87UwIEDS/xPziioVatWGjJkiN555x2dPn1anTp10g8//KA5c+aob9++uuGGG9zW79Chgz755BO1aNHCNX7w6quvVoUKFbRr1y4NHDjQbf17771Xn376qR5++GF99913uu6665SXl6cdO3bo008/1dKlSy84BKI4x48fL/SvIvXq1dOgQYPUsWNHvfrqq3I4HKpVq5aWLVvmml/39zp06KAJEybo4MGDbqG1Y8eOmjFjhurWrVtqn6jVpk0b15Cda665RhEREbr55puLXH/y5Mnau3evHn/8cX355Ze66aabVKlSJR04cECfffaZduzYobvvvluS9PLLL7vmX3300UcVFBSkGTNmKDs7W6+++mqJn0uDBg10/fXX65FHHlF2dramTp2qKlWqaNSoUZLOjfn1pi+eGj9+vFatWqU+ffooPj5eKSkpevvtt1W7dm3XTXsPPfSQZsyYoaFDh2rDhg2qW7euPv/8c/3nP//R1KlTPbqpDkAp8OlcCn5EklmwYIHr+ddff20kmQoVKrg9goKCzJ133mmMMWb79u2FTqFz/uNPf/qTa5979uwxHTt2NJJMYGCgueaaa8ygQYNMkyZNXOv88Y9/dDuezWYzoaGhbssKs3XrVlO1alXz0ksvFXueH330kZFkZsyY4VqWlZVlqlataqZPn26MMWbChAlGklm6dKlrnZSUFBMQEGCWLFlijDGmX79+buc2a9YsEx0d7Xashg0bmri4OJObm+taNnnyZBMbG2uMMebQoUNGkklKSnLb7tlnn3Wb5siYc1M57dq1y6xcudLcfPPN5uqrrzZnz54tcH5JSUlGklm/fn2R70GnTp3ME088UeTrOCd/qqUff/yx2PUcDocZN26cqVevngkODjZxcXHmueeeM1lZWQXWfeutt4wk88gjj7gt79atm5Fkli9fXmCbnJwc88orr5grr7zS2O12U6lSJdOmTRszbtw4k5qa6lpPkhk+fLjH59epU6civ2+7du1qjDHmt99+M/369TMVK1Y00dHRpn///ubw4cNGkhkzZozb/tLS0kxgYKCJjIx0+5qfO3eukWTuvfdej+rKnzKqqCmiCpuaKz093QwcONBUrFjRbRqv4uTm5pr33nvPdOjQwURHR5vg4GATHx9v7rvvvgLTdm3cuNH07NnTREREmPDwcHPDDTcU+L4t6uulqPMZMmSI2/9n+dNZvfbaa2by5MkmLi7O2O1206FDB7Nlyxa3bT3tS3Hv5e+n5lq+fLm59dZbTc2aNU1ISIipWbOmGTBggNm1a5fbdseOHTP33XefqVq1qgkJCTEtWrQws2bNclvn/HP5vcK+dgBcGsLs//t9mP3kk09MYGCg2bFjh9m9e7fb48iRI8YYY7Kzs8327duLfaSkpBQ4Vnp6ujl8+LAxxpg777zT9O7d2/XasWPH3I5Vq1YtM3fuXLdlv7dt2zYTExNjnn/++Que57fffmskFZi389prr3Vt//777xtJ5uDBg27rxMTEmHfeeccYY0x0dLQJDAx0PQICAlwhfebMmcYYYzp27OgKBfkWL15sJJns7GyTnZ1tAgMD3d53Y4wZPHiwueWWW4o8h+zsbBMeHm7mzZtX4LX777/fXHXVVcW+B4RZwP8UFwABoDgMMyhC69atlZeX55pipzAhISGuO4O9UaFCBVWoUEGnTp3S0qVL3f5UFxMT47oDX5KCgoJUq1atIocibNu2TV26dNGQIUM0YcKECx67TZs2stvt2rlzp+tPZw6HQ7/++qvr5oX8sYw7d+50/Vn05MmTOnHihGudtWvXusa+SufmZnzllVeUlJTkmpnhuuuu07x589xulti1a5dq1KjhGiLQpk0bLV++3DU1mNPp1PLly4udjcGc+yWswMeZpqen69NPP9WkSZMu+D4AAIDLQ7kOs+np6dqzZ4/r+f79+7V582ZVrlxZjRo10qBBgzR48GBNnjxZrVu31vHjx7V8+XK1bNnS7cYWTy1dulTGGDVu3Fh79uzRs88+qyZNmrju5PfWzz//rC5duqhnz54aOXKkjh49KuncDRLVqlWTdO7micGDB2v58uWqVauWoqKi9PDDD2vMmDGKi4tTfHy8XnvtNUnnPvJROjen46233qonnnhC77zzjqKiovTcc8+pSZMmrjGQ59+5Lknr169XQECA66NKJemRRx7RP/7xDz3xxBN67LHHtHv3bk2cONH10aXSucn2hwwZorZt2+raa6/V1KlTlZGR4XpP9u3bp/nz56tHjx6qVq2afvvtN/31r39VWFiY6+ajfPPnz1dubq7uueeeQt+vzZs3SzrX9+PHj2vz5s0KCQkp0Un2AQBAGfPxlWGfyh979vvHkCFDjDHnxur95S9/MXXr1jXBwcGmRo0apl+/fuann366qOPNnz/fXHHFFSYkJMTExsaa4cOHm9OnTxe7TXx8vNvYuPNd6GMvzz/H/fv3u5bl5OSYp59+2sTExJjIyEjTrVs38/PPP7vtOzU11dx///2mYsWKpnLlyqZfv37mwIEDRdZZ2JhZY86NYW3Xrp2x2+3miiuuMBMmTHAbT2iMMX//+99NnTp1TEhIiLn22mvN999/73rt0KFDplevXiYmJsYEBweb2rVrm4EDB5odO3YUOFZCQoIZOHBgkTVe6L0C4DsMMwBwsWzGMHszAAAArIl5ZgEAAGBZhFkAAABYVrm7AczpdOrw4cOKjIws84/HBAAAwIUZY3TmzBnVrFnT7eOjC1Puwuzhw4dL/LPEAQAAUPIOHjx4wU9PLHdhNv/jBg8ePFjinyleHjkcDi1btkw9evRQcHCwr8sp1+iF/6AX/oV++A964T/8vRdpaWmKi4vz6GOiy12YzR9aEBUVRZgtAQ6HQ+Hh4YqKivLLb4byhF74D3rhX+iH/6AX/sMqvfBkSCg3gAEAAMCyCLMAAACwLMIsAAAALKvcjZn1hDFGubm5ysvL83Upfs/hcCgoKEhZWVm8Xz5WXC+Cg4MVGBjoo8oAACg9hNnfycnJ0ZEjR5SZmenrUizBGKPY2FgdPHiQeXt9rLhe2Gw21a5dWxERET6qDgCA0kGYPY/T6dT+/fsVGBiomjVrKiQkhIB2AU6nU+np6YqIiLjgpMYoXUX1whij48eP67ffflPDhg25QgsAuKwQZs+Tk5Mjp9OpuLg4hYeH+7ocS3A6ncrJyVFoaChh1seK60W1atX066+/yuFwEGYBAJcV0kchCGW43PAXBgDA5YrUBgAAAMsizAIAAMCyCLPw2tChQ9W3b1/X85tuuklPPfVUmdexYsUK2Ww2nT59usyP7Y/GjRunDh06+LoMAADKFGH2MjF06FDZbDbZbDaFhISoQYMGGj9+vHJzc0v92B9++KHGjx/v0bqXSwDt2bOnAgMD9eOPP3q13ezZs1WxYsXSKQoAgHKIMFtKnE6jHUfTtG7ff7XjaJqcTlPqx7zxxht15MgR7d69W08//bTGjh2r1157rdB1c3JySuy4lSpVUmRkZIntz98dOHBASUlJGjFihN5//31flwMAQLlGmC0FG5JP6sn5mzVy/ha9sGCrRs7foifnb9aG5JOlely73a7Y2FjFx8frkUceUbdu3fTPf/5T0v+GBkyYMEE1a9ZU48aNJUkHDx7UnXfeqYoVK6py5cq69dZb9euvv7r2mZeXp5EjR6pixYqqUqWKRo0aJWPcg/nvhxlkZ2frT3/6k+Li4mS329WgQQPNnDlTv/76q2644QZJ5wKwzWbT0KFDJZ2bVmrSpEmqV6+ewsLC1KpVK33++edux1m8eLEaNWqksLAw3XDDDW51FmbgwIG666673JY5HA5VrVpVH3zwgSTp888/V4sWLRQWFqYqVaqoW7duysjIKHa/s2bN0k033aRHHnlEH3/8sc6ePev2+unTp/XHP/5R1atXV2hoqJo3b66vv/5aK1as0H333afU1FTXVfSxY8dKOjfbwMKFC932U7FiRc2ePdv1/E9/+pMaNWqk8PBwXXHFFfrzn/8sh8NRbK0AAFwqX1yg8wbzzJawDcknNWHRdp3OdCgm0q7QYLuyHHnadjhVExZt1wt9mqpNfOUyqSUsLEz//e9/Xc+XL1+uqKgoJSYmSjoX7Hr27KmEhAStXr1aQUFBevnll3XjjTfqp59+UkhIiCZPnqzZs2fr/fffV9OmTTV58mQtWLBAXbp0KfK4gwcP1tq1a/W3v/1NrVq10v79+3XixAnFxcXpiy++0O23366dO3cqKipKYWFhkqRJkyZp7ty5mj59uho2bKhVq1bpnnvuUbVq1dSpUycdPHhQt912m4YPH66HHnpI69ev19NPP13s+Q8aNEj9+/d3fZCAJC1dulSZmZnq16+fjhw5ogEDBujVV19Vv379dObMGa1evbpAWD+fMUazZs3SW2+9pSZNmqhBgwb6/PPPde+990o6F8p79eqlM2fOaO7cuapfv75++eUXBQYGqn379po6dar+8pe/aOfOnZLk1SdyRUZGavbs2apZs6a2bt2qYcOGKTIyUqNGjfJ4HwCA0uV0Gu1KOaPUTIeiw4PVKCZSAQHWnR5xQ/JJzUlK1p6UdOXk5ikkKFANYiI0pH18meWZCyHMliCn02hOUrJOZzpUt0q4a27PCvYghYcEKvlkpj5ISlbruEql+oVtjNHy5cu1dOlSPfbYY67lFSpU0HvvvaeQkBBJ0ty5c+V0OvXee++5ap01a5YqVqyoFStWqEePHpo6daqee+453XbbbZKk6dOna+nSpUUee9euXfr000+VmJiobt26SZKuuOIK1+uVK5/7wo+JiXGNHc3OztbEiRP173//WwkJCa5t1qxZoxkzZqhTp06aNm2a6tevr8mTJ0uSGjdurK1bt+qVV14pspaePXuqQoUKWrBggStszps3T7fccosiIyO1e/du5ebm6rbbblN8fLwkqUWLFsW+t//+97+VmZmpnj17SpLuuecezZw507X/f//73/rhhx+0fft2NWrUqMD5R0dHy2azKTY2ttjjFObFF190/btu3bp65pln9MknnxBmAZQpq4W1S6nX222tEPy84U8X6IpDmC1Bu1LOaE9KumIi7QUmqbfZbKoWYdfulHTtSjmjJrFRJX78r7/+WhEREXI4HHI6nRo4cKDrz9jSuaCWH2QlacuWLdqzZ0+B8a5ZWVnau3evUlNTdeTIEbVr1871WlBQkNq2bVvk1cvNmzcrMDBQnTp18rjuPXv2KDMzU927d3dbnpOTo9atW0uStm/f7laHJFfwLUpQUJDuvPNOffTRR7r33nuVkZGhr776Sp988okkqVWrVuratatatGihnj17qkePHrrjjjtUqVKlIvf5/vvv66677lJQ0LlvnQEDBujZZ5/V3r17Vb9+fW3evFm1a9d2BdmSNH/+fP3tb3/T3r17lZ6ertzcXEVFlfzXEQAUxWph7VLq9XZbqwQ/T/nLBTpPMGa2BKVmOpSTm6fQ4MI/LjQ0OFA5uXlKzSydcY433HCDNm/erN27d+vs2bOaM2eOKlSo4Hr9/H9LUnp6utq0aaPNmze7PXbt2qWBAwdeVA35wwa8kZ6eLklatGiRWx2//PJLgXGz3ho0aJCWL1+ulJQULVy4UGFhYbrxxhslSYGBgUpMTNQ333yjZs2a6e9//7saN26s/fv3F7qvkydPasGCBXr77bcVFBSkoKAg1apVS7m5ua4bwS7m/KVzv+z8/heE88fDrl27VoMGDVLv3r319ddfa9OmTXrhhRdK9EY+AChOflj7+VCqokKDVLtSuKJCg1xhrbTvC/HWpdTr7ba/D34V7EEKDLCpgj1I8ZXDlXrWoQ+Skv1urGlxvLlA52uE2RIUHR6skKBAZTnyCn09y3HuN7vo8OBSOX6FChXUoEED1alTx3XlsDhXX321du/erZiYGDVo0MDtER0drejoaNWoUUPr1q1zbZObm6sNGzYUuc8WLVrI6XRq5cqVhb6ef2U4L+9/71GzZs1kt9t14MCBAnXExcVJkpo2baoffvjBbV/ff//9Bc+xffv2iouL0/z58/XRRx+pf//+Cg7+3/tvs9l03XXXady4cdq0aZNCQkK0YMGCQvf10UcfqXbt2tqyZYtb6M4fV5yXl6eWLVvqt99+065du4o8//PPPV+1atV05MgR1/Pdu3crMzPT9TwpKUnx8fF64YUX1LZtWzVs2FDJyckXPH8AKAlWC2uXUu/FbGul4OcpX1+g8wZhtgQ1iolUg5gIHU/PLnCVzRij4+nZahgToUYx/jGN1aBBg1S1alXdeuutWr16tfbv368VK1bo8ccf12+//SZJeuKJJ/TXv/5VCxcu1I4dO/Too48WO0ds3bp1NWTIEN1///1auHCha5+ffvqpJCk+Pl42m01ff/21jh8/rvT0dEVGRuqZZ57RU089pTlz5mjv3r3auHGj/v73v2vOnDmSpIcffli7d+/Ws88+q507d2revHlud/oXZ+DAgZo+fboSExM1aNAg1/J169Zp4sSJWr9+vQ4cOKAvv/xSx48fV9OmTQvdz8yZM3XHHXeoefPmbo8HHnhAJ06c0JIlS9SpUyd17NhRt99+uxITE7V//3598803WrJkiev9SU9P1/Lly3XixAlXYO3SpYv+8Y9/aNOmTVq/fr0efvhht9DdsGFDHThwQJ988on27t2rv/3tb0WGbgAoaVYLa5dS78Vsa6Xg5ylfX6DzBmG2BAUE2DSkfbyiw4KVfDJTGdm5ynMaZWTnKvlkpqLDgjW4fbzPx5bkCw8P16pVq1SnTh3ddtttatq0qR544AFlZWW5xmI+/fTTuvfeezVkyBAlJCQoMjJS/fr1K3a/06ZN0x133KFHH31UTZo00bBhw1zTXdWqVUvjxo3T6NGjVb16dY0YMUKS9NJLL+nPf/6zJk2apKZNm+rGG2/UokWLVK9ePUlSnTp19MUXX2jhwoVq1aqVpk+frokTJ3p0noMGDdIvv/yiWrVq6brrrnMtj4qK0qpVq9S7d281atRIL774oiZPnqxevXoV2MeGDRu0ZcsW3X777QVei46OVteuXTVz5kxJ0hdffKFrrrlGAwYMULNmzTRq1CjX1dj27dvr4Ycf1l133aVq1arp1VdflSRNnjxZcXFx6tChgwYOHKhnnnlG4eHhrmPccssteuqppzRixAhdddVVSkpK0p///GePzh8ALpXVwtql1Hsx21op+HnKShfobKa4eYguQ2lpaYqOjlZqamqBm2eysrK0f/9+1atXT6GhoRd9jMIGjTeMidBgPx0gfymcTqfS0tIUFRWlgAB+N/Kl4npRUl/b8IzD4dDixYvVu3dvtyvs8A36cel2HE3TyPlbFBUapAr2gsPYMrJzlZaVqzfualXsDc5l1YtLqfditnU6jZ6cv1nbDqcqvnK42xVdY4yST2aqec1oTbnrKr+5oOVJL/LHDqeedahahF2hwecC+/H0bEWHBZfqTW3F5bXfYzaDUtAmvrJax1Wy1NQlAAAUJf8q3bbDqQoPCSwQ1o6nZ6t5zWi/uEonXVq9F7Nt/l9mJyzaruSTmYUGP3/6y6yn2sRX1gt9mrou0J1Iz1ZIUKCa14z2qwt0hNlSEhBgK5XptwAAKGtWC2uXUu/FbmuV4OctK1ygI8wCAIALslpYu5R6L3ZbKwS/i+HvF+gIswAAwCNWC2uXUu/Fbuvvwe9yRJgtRDm7Jw7lAF/TAEqK1cLapdRrtXMtr7j9/Dz5d/OdP1k9cDnI/6SwwMDCp5oBAMCquDJ7nsDAQFWsWFEpKSmSzs3D+vsJk+HO6XQqJydHWVlZTM3lY0X1wul06vjx4woPD/fok+EAALASfrL9TmxsrCS5Ai2KZ4zR2bNnFRYWRvD3seJ6ERAQoDp16tAjAMBlhzD7OzabTTVq1FBMTIwcDv/4JBN/5nA4tGrVKnXs2JHJyH2suF6EhIRw5RwAcFkizBYhMDCQ8YUeCAwMVG5urkJDQwmzPkYvAADlEZdqAAAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFiWT8PstGnT1LJlS0VFRSkqKkoJCQn65ptvilx/9uzZstlsbo/Q0NAyrBgAAAD+JMiXB69du7b++te/qmHDhjLGaM6cObr11lu1adMmXXnllYVuExUVpZ07d7qe22y2sioXAAAAfsanYfbmm292ez5hwgRNmzZN33//fZFh1mazKTY2tizKAwAAgJ/zaZg9X15enj777DNlZGQoISGhyPXS09MVHx8vp9Opq6++WhMnTiwy+EpSdna2srOzXc/T0tIkSQ6HQw6Ho+ROoJzKfw95L32PXvgPeuFf6If/oBf+w9974U1dNmOMKcVaLmjr1q1KSEhQVlaWIiIiNG/ePPXu3bvQddeuXavdu3erZcuWSk1N1euvv65Vq1Zp27Ztql27dqHbjB07VuPGjSuwfN68eQoPDy/RcwEAAMCly8zM1MCBA5WamqqoqKhi1/V5mM3JydGBAweUmpqqzz//XO+9955WrlypZs2aXXBbh8Ohpk2basCAAXrppZcKXaewK7NxcXE6ceLEBd8cXJjD4VBiYqK6d++u4OBgX5dTrtEL/0Ev/Av98B/0wn/4ey/S0tJUtWpVj8Ksz4cZhISEqEGDBpKkNm3a6Mcff9Sbb76pGTNmXHDb4OBgtW7dWnv27ClyHbvdLrvdXui2/tg8q+L99B/0wn/QC/9CP/wHvfAf/toLb2ryu3lmnU6n25XU4uTl5Wnr1q2qUaNGKVcFAAAAf+TTK7PPPfecevXqpTp16ujMmTOaN2+eVqxYoaVLl0qSBg8erFq1amnSpEmSpPHjx+sPf/iDGjRooNOnT+u1115TcnKyHnzwQV+eBgAAAHzEp2E2JSVFgwcP1pEjRxQdHa2WLVtq6dKl6t69uyTpwIEDCgj438XjU6dOadiwYTp69KgqVaqkNm3aKCkpyaPxtQAAALj8+DTMzpw5s9jXV6xY4fZ8ypQpmjJlSilWBAAAACvxuzGzAAAAgKcIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsy6dhdtq0aWrZsqWioqIUFRWlhIQEffPNN8Vu89lnn6lJkyYKDQ1VixYttHjx4jKqFgAAAP7Gp2G2du3a+utf/6oNGzZo/fr16tKli2699VZt27at0PWTkpI0YMAAPfDAA9q0aZP69u2rvn376ueffy7jygEAAOAPfBpmb775ZvXu3VsNGzZUo0aNNGHCBEVEROj7778vdP0333xTN954o5599lk1bdpUL730kq6++mr94x//KOPKAQAA4A+CfF1Avry8PH322WfKyMhQQkJCoeusXbtWI0eOdFvWs2dPLVy4sMj9ZmdnKzs72/U8LS1NkuRwOORwOC698HIu/z3kvfQ9euE/6IV/oR/+g174D3/vhTd1+TzMbt26VQkJCcrKylJERIQWLFigZs2aFbru0aNHVb16dbdl1atX19GjR4vc/6RJkzRu3LgCy5ctW6bw8PBLKx4uiYmJvi4B/49e+A964V/oh/+gF/7DX3uRmZnp8bo+D7ONGzfW5s2blZqaqs8//1xDhgzRypUriwy03nruuefcruampaUpLi5OPXr0UFRUVIkcozxzOBxKTExU9+7dFRwc7OtyyjV64T/ohX+hH/6DXvgPf+9F/l/SPeHzMBsSEqIGDRpIktq0aaMff/xRb775pmbMmFFg3djYWB07dsxt2bFjxxQbG1vk/u12u+x2e4HlwcHBftk8q+L99B/0wn/QC/9CP/wHvfAf/toLb2ryu3lmnU6n2xjX8yUkJGj58uVuyxITE4scYwsAAIDLm0+vzD733HPq1auX6tSpozNnzmjevHlasWKFli5dKkkaPHiwatWqpUmTJkmSnnjiCXXq1EmTJ09Wnz599Mknn2j9+vV65513fHkaAAAA8BGfhtmUlBQNHjxYR44cUXR0tFq2bKmlS5eqe/fukqQDBw4oIOB/F4/bt2+vefPm6cUXX9Tzzz+vhg0bauHChWrevLmvTgEAAAA+5NMwO3PmzGJfX7FiRYFl/fv3V//+/UupIgAAAFiJ342ZBQAAADxFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWT4Ns5MmTdI111yjyMhIxcTEqG/fvtq5c2ex28yePVs2m83tERoaWkYVAwAAwJ/4NMyuXLlSw4cP1/fff6/ExEQ5HA716NFDGRkZxW4XFRWlI0eOuB7JycllVDEAAAD8SZAvD75kyRK357Nnz1ZMTIw2bNigjh07FrmdzWZTbGxsaZcHAAAAP+fTMPt7qampkqTKlSsXu156erri4+PldDp19dVXa+LEibryyisLXTc7O1vZ2dmu52lpaZIkh8Mhh8NRQpWXX/nvIe+l79EL/0Ev/Av98B/0wn/4ey+8qctmjDGlWIvHnE6nbrnlFp0+fVpr1qwpcr21a9dq9+7datmypVJTU/X6669r1apV2rZtm2rXrl1g/bFjx2rcuHEFls+bN0/h4eEleg4AAAC4dJmZmRo4cKBSU1MVFRVV7Lp+E2YfeeQRffPNN1qzZk2hobQoDodDTZs21YABA/TSSy8VeL2wK7NxcXE6ceLEBd8cXJjD4VBiYqK6d++u4OBgX5dTrtEL/0Ev/Av98B/0wn/4ey/S0tJUtWpVj8KsXwwzGDFihL7++mutWrXKqyArScHBwWrdurX27NlT6Ot2u112u73Q7fyxeVbF++k/6IX/oBf+hX74D3rhP/y1F97U5NPZDIwxGjFihBYsWKBvv/1W9erV83ofeXl52rp1q2rUqFEKFQIAAMCf+fTK7PDhwzVv3jx99dVXioyM1NGjRyVJ0dHRCgsLkyQNHjxYtWrV0qRJkyRJ48eP1x/+8Ac1aNBAp0+f1muvvabk5GQ9+OCDPjsPAAAA+IZPw+y0adMkSZ07d3ZbPmvWLA0dOlSSdODAAQUE/O8C8qlTpzRs2DAdPXpUlSpVUps2bZSUlKRmzZqVVdkAAADwEz4Ns57ce7ZixQq351OmTNGUKVNKqSIAAABYiU/HzAIAAACXgjALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsy+swO2vWLGVmZpZGLQAAAIBXvA6zo0ePVmxsrB544AElJSWVRk0AAACAR7wOs4cOHdKcOXN04sQJde7cWU2aNNErr7yio0ePlkZ9AAAAQJG8DrNBQUHq16+fvvrqKx08eFDDhg3TRx99pDp16uiWW27RV199JafTWRq1AgAAAG4u6Qaw6tWr6/rrr1dCQoICAgK0detWDRkyRPXr19eKFStKqEQAAACgcBcVZo8dO6bXX39dV155pTp37qy0tDR9/fXX2r9/vw4dOqQ777xTQ4YMKelaAQAAADdeh9mbb75ZcXFxmj17toYNG6ZDhw7p448/Vrdu3SRJFSpU0NNPP62DBw+WeLEAAADA+YK83SAmJkYrV65UQkJCketUq1ZN+/fvv6TCAAAAgAvxOszOnDnzguvYbDbFx8dfVEEAAACAp/gEMAAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFke3QCWlpbm8Q6joqIuuhgAAADAGx6F2YoVK8pms3m0w7y8vEsqCAAAAPCUR2H2u+++c/37119/1ejRozV06FDXXLNr167VnDlzNGnSpNKpEgAAACiER2G2U6dOrn+PHz9eb7zxhgYMGOBadsstt6hFixZ65513+BhbAAAAlBmvbwBbu3at2rZtW2B527Zt9cMPP5RIUQAAAIAnvA6zcXFxevfddwssf++99xQXF1ciRQEAAACe8PrjbKdMmaLbb79d33zzjdq1aydJ+uGHH7R792598cUXJV4gAAAAUBSvr8z27t1bu3bt0s0336yTJ0/q5MmTuvnmm7Vr1y717t27NGoEAAAACuX1lVnp3FCDiRMnlnQtAAAAgFcu6hPAVq9erXvuuUft27fXoUOHJEkffvih1qxZU6LFAQAAAMXxOsx+8cUX6tmzp8LCwrRx40ZlZ2dLklJTU7laCwAAgDLldZh9+eWXNX36dL377rsKDg52Lb/uuuu0cePGEi0OAAAAKI7XYXbnzp3q2LFjgeXR0dE6ffp0SdQEAAAAeMTrMBsbG6s9e/YUWL5mzRpdccUVJVIUAAAA4Amvw+ywYcP0xBNPaN26dbLZbDp8+LA++ugjPfPMM3rkkUdKo0YAAACgUF5PzTV69Gg5nU517dpVmZmZ6tixo+x2u5555hk99thjpVEjAAAAUCivw6zNZtMLL7ygZ599Vnv27FF6erqaNWumiIiI0qgPAAAAKJLXwwzuv/9+nTlzRiEhIWrWrJmuvfZaRUREKCMjQ/fff39p1AgAAAAUyuswO2fOHJ09e7bA8rNnz+qDDz4okaIAAAAAT3g8zCAtLU3GGBljdObMGYWGhrpey8vL0+LFixUTE1MqRQIAAACF8TjMVqxYUTabTTabTY0aNSrwus1m07hx40q0OAAAAKA4Hg8z+O6777R8+XIZY/T555/r22+/dT3WrFmjAwcO6IUXXvDq4JMmTdI111yjyMhIxcTEqG/fvtq5c+cFt/vss8/UpEkThYaGqkWLFlq8eLFXxwUAAMDlweMrs506dZIk7d+/X3Xq1JHNZrvkg69cuVLDhw/XNddco9zcXD3//PPq0aOHfvnlF1WoUKHQbZKSkjRgwABNmjRJN910k+bNm6e+fftq48aNat68+SXXBAAAAOvwemqub7/9VhEREerfv7/b8s8++0yZmZkaMmSIx/tasmSJ2/PZs2crJiZGGzZsKPQjcyXpzTff1I033qhnn31WkvTSSy8pMTFR//jHPzR9+nQvzwYAAABW5nWYnTRpkmbMmFFgeUxMjB566CGvwuzvpaamSpIqV65c5Dpr167VyJEj3Zb17NlTCxcuLHT97OxsZWdnu56npaVJkhwOhxwOx0XXinPy30PeS9+jF/6DXvgX+uE/6IX/8PdeeFOX12H2wIEDqlevXoHl8fHxOnDggLe7c3E6nXryySd13XXXFTtc4OjRo6pevbrbsurVq+vo0aOFrj9p0qRCb0xbtmyZwsPDL7peuEtMTPR1Cfh/9MJ/0Av/Qj/8B73wH/7ai8zMTI/X9TrMxsTE6KefflLdunXdlm/ZskVVqlTxdncuw4cP188//6w1a9Zc9D4K89xzz7ldyU1LS1NcXJx69OihqKioEj1WeeRwOJSYmKju3bsrODjY1+WUa/TCf9CL0uF0Gu05nq60sw5FhQWrQbUIBQRc+P4Nq/Rj88FTmrfuoPYdz1BObp5CggJ1RbUKGtguTlfFVfJ1eSXCKr0oD/y9F/l/SfeE12F2wIABevzxxxUZGeka17py5Uo98cQTuvvuu73dnSRpxIgR+vrrr7Vq1SrVrl272HVjY2N17Ngxt2XHjh1TbGxsoevb7XbZ7fYCy4ODg/2yeVbF++k/6IX/KMleOJ1Gu1LOKDXToejwYDWKifQoyJX1cUurzg3JJzUnKVl7UtJdQa9BTISGtI9Xm/iih6adz5+/NzYkn9SkJbt1OtOhmEi7KkWEKsuRp58On1Hykt16oU9Tj8/TCvy5F+WNv/bCm5q8DrMvvfSSfv31V3Xt2lVBQec2dzqdGjx4sCZOnOjVvowxeuyxx7RgwQKtWLGi0OELv5eQkKDly5frySefdC1LTExUQkKCV8cGAKsoiSBXFsctrTo3JJ/UhEXbXUEvNNiuLEeeth1O1YRF2y0f9JxOozlJyTqd6VDdKuGu2YIq2IMUHhKo5JOZ+iApWa3jKpXJLzCA1Xj9cbYhISGaP3++duzYoY8++khffvml9u7dq/fff18hISFe7Wv48OGaO3eu5s2bp8jISB09elRHjx51+7jcwYMH67nnnnM9f+KJJ7RkyRJNnjxZO3bs0NixY7V+/XqNGDHC21MBAL+XH+R+PpSqqNAg1a4UrqjQIFeQ25B80i+OW1p1/j7oVbAHKTDApgr2IMVXDlfqWYc+SEqW02lK4rR9YlfKGe1JSVdMpL3AtJc2m03VIuzanZKuXSlnfFQh4N+8vjKbr1GjRoV+Epg3pk2bJknq3Lmz2/JZs2Zp6NChks7dcBYQ8L/M3b59e82bN08vvviinn/+eTVs2FALFy5kjlkAlx1fXbHz9rilWac3Qa9JrDXvg0jNdCgnN0+hwQWHxElSaHCgTqRnKzXTP+86B3zNozA7cuRIvfTSS6pQoUKBabF+74033vD44MZc+DfpFStWFFjWv3//AvPcAsDlxldBztvjlmad5SHoRYcHKyQoUFmOPFWwF/yxnOU4N2QjOtz/xjUC/sCjMLtp0ybXfF+bNm0qcr2S+FQwAMA5vgpy3h63NOssD0GvUUykGsREaNvhVIWHBLr9LDXG6Hh6tprXjFajmEgfVgn4L4/C7HfffVfovwEApcdXQc7b45ZmneUh6AUE2DSkfbwmLNqu5JOZqhZhV2jwuffzeHq2osOCNbh9PDd/AUXw+gYwAEDZyA9yx9OzCwzLyg9yDWMiSjzIeXvc0qwzP+hFhwUr+WSmMrJzlec0ysjOVfLJzMsm6LWJr6wX+jTVlTWjlZaVq99OZSotK1fNa0ZbfrYGoLR5dGX2tttu83iHX3755UUXAwD4H19dsfP2uKVdZ37Qy5/260R6tkKCAtW8ZrQGl/L0ZGWpTXxltY6r5JP5hAEr8yjMRkdHu/5tjNGCBQsUHR2ttm3bSpI2bNig06dPexV6AQAX5qsg5+1xS7vO8hL0AgJslp2VAfAVj8LsrFmzXP/+05/+pDvvvFPTp09XYGCgJCkvL0+PPvooHw8LAKXAV0HO2+OWdp0EPQCF8Xqe2ffff19r1qxxBVlJCgwM1MiRI9W+fXu99tprJVogAMB3Qc7b4xI4AZQ1r28Ay83N1Y4dOwos37Fjh5xOZ4kUBQAAAHjC6yuz9913nx544AHt3btX1157rSRp3bp1+utf/6r77ruvxAsEAAAAiuJ1mH399dcVGxuryZMn68iRI5KkGjVq6Nlnn9XTTz9d4gUCAAAARfE6zAYEBGjUqFEaNWqU0tLSJIkbvwAAAOATF/WhCbm5ufr3v/+tjz/+2PVpLIcPH1Z6enqJFgcAAAAUx+srs8nJybrxxht14MABZWdnq3v37oqMjNQrr7yi7OxsTZ8+vTTqBAAAAArw+srsE088obZt2+rUqVMKCwtzLe/Xr5+WL19eosUBAAAAxfH6yuzq1auVlJSkkJAQt+V169bVoUOHSqwwAAAA4EK8vjLrdDqVl5dXYPlvv/2myMjIEikKAAAA8ITXYbZHjx6aOnWq67nNZlN6errGjBmj3r17l2RtAAAAQLEuap7ZG2+8Uc2aNVNWVpYGDhyo3bt3q2rVqvr4449Lo0YAAACgUF6H2bi4OG3ZskXz58/Xli1blJ6ergceeECDBg1yuyEMAAAAKG1ehVmHw6EmTZro66+/1qBBgzRo0KDSqgsAAAC4IK/GzAYHBysrK6u0agEAAAC84vUNYMOHD9crr7yi3Nzc0qgHAAAA8JjXY2Z//PFHLV++XMuWLVOLFi1UoUIFt9e//PLLEisOAAAAKI7XYbZixYq6/fbbS6MWAAAAwCteh9lZs2aVRh0AAACA1zweM+t0OvXKK6/ouuuu0zXXXKPRo0fr7NmzpVkbAAAAUCyPw+yECRP0/PPPKyIiQrVq1dKbb76p4cOHl2ZtAAAAQLE8DrMffPCB3n77bS1dulQLFy7Uv/71L3300UdyOp2lWR8AAABQJI/D7IEDB9S7d2/X827duslms+nw4cOlUhgAAABwIR6H2dzcXIWGhrotCw4OlsPhKPGiAAAAAE94PJuBMUZDhw6V3W53LcvKytLDDz/sNtcs88wCAACgrHgcZocMGVJg2T333FOixQAAAADe8DjMMr8sAAAA/I3HY2YBAAAAf0OYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZPg2zq1at0s0336yaNWvKZrNp4cKFxa6/YsUK2Wy2Ao+jR4+WTcEAAADwKz4NsxkZGWrVqpXeeustr7bbuXOnjhw54nrExMSUUoUAAADwZ0G+PHivXr3Uq1cvr7eLiYlRxYoVPVo3Oztb2dnZrudpaWmSJIfDIYfD4fWx4S7/PeS99D164T/ohX+hH/6DXvgPf++FN3X5NMxerKuuukrZ2dlq3ry5xo4dq+uuu67IdSdNmqRx48YVWL5s2TKFh4eXZpnlSmJioq9LwP+jF/6DXvgX+uE/6IX/8NdeZGZmeryuzRhjSrEWj9lsNi1YsEB9+/Ytcp2dO3dqxYoVatu2rbKzs/Xee+/pww8/1Lp163T11VcXuk1hV2bj4uJ04sQJRUVFlfRplDsOh0OJiYnq3r27goODfV1OuUYv/Ae98C/0w3/QC//h771IS0tT1apVlZqaesG8Zqkrs40bN1bjxo1dz9u3b6+9e/dqypQp+vDDDwvdxm63y263F1geHBzsl82zKt5P/0Ev/Ae98C/0w3/QC//hr73wpibLT8117bXXas+ePb4uAwAAAD5g+TC7efNm1ahRw9dlAAAAwAd8OswgPT3d7arq/v37tXnzZlWuXFl16tTRc889p0OHDumDDz6QJE2dOlX16tXTlVdeqaysLL333nv69ttvtWzZMl+dAgAAAHzIp2F2/fr1uuGGG1zPR44cKUkaMmSIZs+erSNHjujAgQOu13NycvT000/r0KFDCg8PV8uWLfXvf//bbR8AAAAoP3waZjt37qziJlOYPXu22/NRo0Zp1KhRpVwVAAAArMLyY2YBAABQfhFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZQX5ugAAuFROp9GulDNKzXQoOjxYjWIiFRBgs9wxAADeI8wC8FhpBbpL2e+G5JOak5SsPSnpysnNU0hQoBrERGhI+3i1ia98ybWV1TEAABeHMAvAI6UV6C5lvxuST2rCou06nelQTKRdocF2ZTnytO1wqiYs2q4X+jS95LBZFscAAFw8xswCuKD8QPfzoVRFhQapdqVwRYUGuQLdhuSTZb5fp9NoTlKyTmc6VLdKuCrYgxQYYFMFe5DiK4cr9axDHyQly+k0F3vaZXIMAMClIcwCKFZpBbpL3e+ulDPak5KumEi7bDb3IQk2m03VIuzanZKuXSlnLvrcy+IYAIBLQ5gFUKzSCnSXut/UTIdycvMUGhxY6OuhwYHKyc1TaqbDq7rK+hgAgEtDmAVQrNIKdJe63+jwYIUEBSrLkVfo61mOc+Nvo8ODvaqrrI8BALg0hFkAxSqtQHep+20UE6kGMRE6np4tY9yHIhhjdDw9Ww1jItQoJtKrusr6GACAS0OYBVCs0gp0l7rfgACbhrSPV3RYsJJPZiojO1d5TqOM7Fwln8xUdFiwBrePv6Spw8riGACAS0OYBVCs0gp0JbHfNvGV9UKfprqyZrTSsnL126lMpWXlqnnN6BKbMqssjgEAuHjMMwvggvIDXf58sCfSsxUSFKjmNaM1+BLmmS2J/baJr6zWcZVK9dO5yuIYAICLQ5gF4JHSCnQlsd+AAJuaxEZdUh3+cAwAgPcIswA8VlqBjqAIALhYjJkFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZfk0zK5atUo333yzatasKZvNpoULF15wmxUrVujqq6+W3W5XgwYNNHv27FKvEwAAAP7Jp2E2IyNDrVq10ltvveXR+vv371efPn10ww03aPPmzXryySf14IMPaunSpaVcKQAAAPyRT+eZ7dWrl3r16uXx+tOnT1e9evU0efJkSVLTpk21Zs0aTZkyRT179iytMgEAAOCnLPWhCWvXrlW3bt3clvXs2VNPPvlkkdtkZ2crOzvb9TwtLU2S5HA45HA4SqXO8iT/PeS99D164T/ohX+hH/6DXvgPf++FN3VZKswePXpU1atXd1tWvXp1paWl6ezZswoLCyuwzaRJkzRu3LgCy5ctW6bw8PBSq7W8SUxM9HUJ+H/0wn/QC/9CP/wHvfAf/tqLzMxMj9e1VJi9GM8995xGjhzpep6Wlqa4uDj16NFDUVF8fOalcjgcSkxMVPfu3RUcHOzrcso1euE/6IV/oR/+g174D3/vRf5f0j1hqTAbGxurY8eOuS07duyYoqKiCr0qK0l2u112u73A8uDgYL9snlXxfvoPeuE/6IV/oR/+g174D3/thTc1WWqe2YSEBC1fvtxtWWJiohISEnxUEQAAAHzJp2E2PT1dmzdv1ubNmyWdm3pr8+bNOnDggKRzQwQGDx7sWv/hhx/Wvn37NGrUKO3YsUNvv/22Pv30Uz311FO+KB8AAAA+5tMwu379erVu3VqtW7eWJI0cOVKtW7fWX/7yF0nSkSNHXMFWkurVq6dFixYpMTFRrVq10uTJk/Xee+8xLRcAAEA55dMxs507d5YxpsjXC/t0r86dO2vTpk2lWBUAAACswlJjZgEAAIDzEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWJZfhNm33npLdevWVWhoqNq1a6cffvihyHVnz54tm83m9ggNDS3DagEAAOAvfB5m58+fr5EjR2rMmDHauHGjWrVqpZ49eyolJaXIbaKionTkyBHXIzk5uQwrBuANp9Nox9E0rdv3X+04mian0/i6JADAZSTI1wW88cYbGjZsmO677z5J0vTp07Vo0SK9//77Gj16dKHb2Gw2xcbGlmWZgMecTqNdKWeUmulQdHiwGsVEKiDA5uuyfGJD8knNSUrWnpR05eTmKSQoUA1iIjSkfbzaxFf2dXkAgMuAT8NsTk6ONmzYoOeee861LCAgQN26ddPatWuL3C49PV3x8fFyOp26+uqrNXHiRF155ZWFrpudna3s7GzX87S0NEmSw+GQw+EooTMpv/Lfw9+/l06n0Z7j6Uo761BUWLAaVIvwWaAry1o2HzyleesOat/xDFd4u6JaBQ1sF6er4iqVyjHzFdULX9l88JReX7pTqWdzVTUiRKHBwcpy5Gn3kdN6dXG6nunZuNTfE1/xt16Ud/TDf9AL/+HvvfCmLpsxxmd/8zt8+LBq1aqlpKQkJSQkuJaPGjVKK1eu1Lp16wpss3btWu3evVstW7ZUamqqXn/9da1atUrbtm1T7dq1C6w/duxYjRs3rsDyefPmKTw8vGRPCAAAAJcsMzNTAwcOVGpqqqKioopd1+fDDLyVkJDgFnzbt2+vpk2basaMGXrppZcKrP/cc89p5MiRrudpaWmKi4tTjx49Lvjm4MIcDocSExPVvXt3BQcHF3I1LlBZjjz9Nz1HUWFBZXo1rixrcTqNRn+5VduPpKlOpTDZbP+78muM0cFTZ9W0RpQm3dai1K4K/74XvrTr2Bk9/+XPigwNVIWQgv/NZOTk6kxWnibe1lyNqkf6oMLS5U+9AP3wJ/TCf/h7L/L/ku4Jn4bZqlWrKjAwUMeOHXNbfuzYMY/HxAYHB6t169bas2dPoa/b7XbZ7fZCt/PH5llVcHCwAgOD9OG6Qzqekae6VSrIZrPJSLKHBKpGpWAln8zU3HWH1KZutVIfcuB0mjKtZcfRNO1MyVTFCqHKswW6v2iToiuEakdKpvafylKT2NL9JcofvrbTc4wyHE5VighVrgq+v0FBwcpwOJSeY3xea2nyh17gf+iH/6AX/sNfe+FNTT6dzSAkJERt2rTR8uXLXcucTqeWL1/udvW1OHl5edq6datq1KhRWmXCQ7tSzmhPSrpiIu1uVyalczftVYuwa3dKunalnLnsaknNdCgnN0+hwYGFvh4aHKic3DylZvrn2KSSFh0erJCgc1fCC5PlODeeODrc//4DBQBYi8+n5ho5cqTeffddzZkzR9u3b9cjjzyijIwM1+wGgwcPdrtBbPz48Vq2bJn27dunjRs36p577lFycrIefPBBX50C/p8/BbqyroXw5q5RTKQaxEToeHq2fj8s3xij4+nZahgToUYxl98QAwBA2fL5mNm77rpLx48f11/+8hcdPXpUV111lZYsWaLq1atLkg4cOKCAgP9l7lOnTmnYsGE6evSoKlWqpDZt2igpKUnNmjXz1Sng/50f6CrYC35plWWgK+ta8sPbtsOpCg8JLDBm9nh6tprXjC434S0gwKYh7eM1YdF2JZ/MVLUIu2vM8vH0bEWHBWtw+/hyO2UZAKDk+DzMStKIESM0YsSIQl9bsWKF2/MpU6ZoypQpZVAVvOVPga6sayG8FdQmvrJe6NPUNc/sifRshQQFqnnNaA1mnlkAQAnxizCLy4M/BTpf1EJ4K6hNfGW1jqvEh0gAAEoNYRYlyp8CnS9qIbwVFBBgK/UZHAAA5RdhFiXOnwKdL2ohvAEAUHYIsygV/hTo/KkWAABQsnw+NRcAAABwsQizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsoJ8XQCAgpxOo10pZ5Sa6VB0eLAaxUQqIMDm67IAAPA7hNlyyJdBiZB2YRuST2pOUrL2pKQrJzdPIUGBahAToSHt49UmvrKvywMAwK8QZktRSQa3ktqXL4MSIe3CNiSf1IRF23U606GYSLtCg+3KcuRp2+FUTVi0XS/0acp7BQDAeQizpaQkg1tJ7cuXQYmQdmFOp9GcpGSdznSobpVw2WznflmpYA9SeEigkk9m6oOkZLWOq8TVbAAA/h83gJWC/OD286FURYUGqXalcEWFBrmC24bkk2W+r98HpQr2IAUG2FTBHqT4yuFKPevQB0nJcjrNxZ62Xx7bSnalnNGelHTFRNpdQTafzWZTtQi7dqeka1fKGR9VCACA/yHMlrCSDG4luS9fBiVCmmdSMx3Kyc1TaHBgoa+HBgcqJzdPqZmOMq4MAAD/RZgtYSUZ3EpyX74MSoQ0z0SHByskKFBZjrxCX89ynBtiEh0eXMaVAQDgvwizJawkg1tJ7suXQYmQ5plGMZFqEBOh4+nZMsb9arsxRsfTs9UwJkKNYiJ9VCEAAP6HMFvCSjK4leS+fBmUCGmeCQiwaUj7eEWHBSv5ZKYysnOV5zTKyM5V8slMRYcFa3D7eG7+AgDgPITZElaSwa0k9+XLoERI81yb+Mp6oU9TXVkzWmlZufrtVKbSsnLVvGY0Mz4AAFAIpuYqYfnBbcKi7Uo+malqEXaFBp+7uno8Pdur4FaS+5L+F5Typ/k6kZ6tkKBANa8ZrcGlPNerL49tNW3iK6t1XCU+XAIAAA8QZktBSQa3kg6BvgxKhDTPBQTY1CQ2ytdlAADg9wizpaQkg1tJh0BfBiVCGgAAKEmE2VJUksGNEAgAAFAQN4ABAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsoJ8XUBZM8ZIktLS0nxcyeXB4XAoMzNTaWlpCg4O9nU55Rq98B/0wr/QD/9BL/yHv/ciP6fl57bilLswe+bMGUlSXFycjysBAABAcc6cOaPo6Ohi17EZTyLvZcTpdOrw4cOKjIyUzWbzdTmWl5aWpri4OB08eFBRUVG+Lqdcoxf+g174F/rhP+iF//D3XhhjdObMGdWsWVMBAcWPii13V2YDAgJUu3ZtX5dx2YmKivLLb4byiF74D3rhX+iH/6AX/sOfe3GhK7L5uAEMAAAAlkWYBQAAgGURZnFJ7Ha7xowZI7vd7utSyj164T/ohX+hH/6DXviPy6kX5e4GMAAAAFw+uDILAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizCLIk2aNEnXXHONIiMjFRMTo759+2rnzp3FbjN79mzZbDa3R2hoaBlVfPm6mF5I0unTpzV8+HDVqFFDdrtdjRo10uLFi8ug4svbxfSjc+fOBb43bDab+vTpU0ZVX54u9ntj6tSpaty4scLCwhQXF6ennnpKWVlZZVDx5etieuFwODR+/HjVr19foaGhatWqlZYsWVJGFV++pk2bppYtW7o+ECEhIUHffPNNsdt89tlnatKkiUJDQ9WiRQtL/awgzKJIK1eu1PDhw/X9998rMTFRDodDPXr0UEZGRrHbRUVF6ciRI65HcnJyGVV8+bqYXuTk5Kh79+769ddf9fnnn2vnzp169913VatWrTKs/PJ0Mf348ssv3b4vfv75ZwUGBqp///5lWPnl52J6MW/ePI0ePVpjxozR9u3bNXPmTM2fP1/PP/98GVZ++bmYXrz44ouaMWOG/v73v+uXX37Rww8/rH79+mnTpk1lWPnlp3bt2vrrX/+qDRs2aP369erSpYtuvfVWbdu2rdD1k5KSNGDAAD3wwAPatGmT+vbtq759++rnn38u48ovkgE8lJKSYiSZlStXFrnOrFmzTHR0dNkVVU550otp06aZK664wuTk5JRhZeWTJ/34vSlTppjIyEiTnp5eipWVP570Yvjw4aZLly5uy0aOHGmuu+660i6vXPGkFzVq1DD/+Mc/3JbddtttZtCgQaVdXrlTqVIl89577xX62p133mn69Onjtqxdu3bmj3/8Y1mUdsm4MguPpaamSpIqV65c7Hrp6emKj49XXFxcsb8J4uJ50ot//vOfSkhI0PDhw1W9enU1b95cEydOVF5eXlmVWW54+r1xvpkzZ+ruu+9WhQoVSquscsmTXrRv314bNmzQDz/8IEnat2+fFi9erN69e5dJjeWFJ73Izs4uMBQtLCxMa9asKdXaypO8vDx98sknysjIUEJCQqHrrF27Vt26dXNb1rNnT61du7YsSrx0vk7TsIa8vDzTp0+fC165SEpKMnPmzDGbNm0yK1asMDfddJOJiooyBw8eLKNKL3+e9qJx48bGbreb+++/36xfv9588sknpnLlymbs2LFlVGn54Gk/zrdu3Tojyaxbt64UKyt/vOnFm2++aYKDg01QUJCRZB5++OEyqLD88LQXAwYMMM2aNTO7du0yeXl5ZtmyZSYsLMyEhISUUaWXr59++slUqFDBBAYGmujoaLNo0aIi1w0ODjbz5s1zW/bWW2+ZmJiY0i6zRBBm4ZGHH37YxMfHex1Kc3JyTP369c2LL75YSpWVP572omHDhiYuLs7k5ua6lk2ePNnExsaWdonlysV8bzz00EOmRYsWpVhV+eRpL7777jtTvXp18+6775qffvrJfPnllyYuLs6MHz++jCq9/Hnai5SUFHPrrbeagIAAExgYaBo1amQeffRRExoaWkaVXr6ys7PN7t27zfr1683o0aNN1apVzbZt2wpd1+phNsjXV4bh/0aMGKGvv/5aq1atUu3atb3aNjg4WK1bt9aePXtKqbryxZte1KhRQ8HBwQoMDHQta9q0qY4ePaqcnByFhISUdrmXvYv53sjIyNAnn3yi8ePHl3J15Ys3vfjzn/+se++9Vw8++KAkqUWLFsrIyNBDDz2kF154QQEBjMC7FN70olq1alq4cKGysrL03//+VzVr1tTo0aN1xRVXlFG1l6+QkBA1aNBAktSmTRv9+OOPevPNNzVjxowC68bGxurYsWNuy44dO6bY2NgyqfVS8R2LIhljNGLECC1YsEDffvut6tWr5/U+8vLytHXrVtWoUaMUKiw/LqYX1113nfbs2SOn0+latmvXLtWoUYMge4ku5Xvjs88+U3Z2tu65555SrLD8uJheZGZmFgis+b/0GWNKpc7y4FK+L0JDQ1WrVi3l5ubqiy++0K233lqKlZZPTqdT2dnZhb6WkJCg5cuXuy1LTEwscoyt3/HlZWH4t0ceecRER0ebFStWmCNHjrgemZmZrnXuvfdeM3r0aNfzcePGmaVLl5q9e/eaDRs2mLvvvtuEhoYW+acNeOZienHgwAETGRlpRowYYXbu3Gm+/vprExMTY15++WVfnMJl5WL6ke/66683d911V1mWe1m7mF6MGTPGREZGmo8//tjs27fPLFu2zNSvX9/ceeedvjiFy8bF9OL77783X3zxhdm7d69ZtWqV6dKli6lXr545deqUD87g8jF69GizcuVKs3//fvPTTz+Z0aNHG5vNZpYtW2aMKdiH//znPyYoKMi8/vrrZvv27WbMmDEmODjYbN261Ven4BXCLIokqdDHrFmzXOt06tTJDBkyxPX8ySefNHXq1DEhISGmevXqpnfv3mbjxo1lX/xl5mJ6Ycy5G/LatWtn7Ha7ueKKK8yECRPcxtDi4lxsP3bs2GEkuX6g4NJdTC8cDocZO3asqV+/vgkNDTVxcXHm0UcfJUBdoovpxYoVK0zTpk2N3W43VapUMffee685dOhQ2Rd/mbn//vtNfHy8CQkJMdWqVTNdu3Z1+3+nsP+fPv30U9OoUSMTEhJirrzyymJvGPM3NmP4mwoAAACsiTGzAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAFBO2Ww2LVy40NdlAMAlIcwCQClbu3atAgMD1adPH6+3rVu3rqZOnVryRQHAZYIwCwClbObMmXrssce0atUqHT582NflAMBlhTALAKUoPT1d8+fP1yOPPKI+ffpo9uzZBdb517/+pWuuuUahoaGqWrWq+vXrJ0nq3LmzkpOT9dRTT8lms8lms0mSxo4dq6uuusptH1OnTlXdunVdz3/88Ud1795dVatWVXR0tDp16qSNGzd6XPcHH3ygKlWqKDs722153759de+993q8HwAobYRZAChFn376qZo0aaLGjRvrnnvu0fvvvy9jjOv1RYsWqV+/furdu7c2bdqk5cuX69prr5Ukffnll6pdu7bGjx+vI0eO6MiRIx4f98yZMxoyZIjWrFmj77//Xg0bNlTv3r115swZj7bv37+/8vLy9M9//tO1LCUlRYsWLdL999/vcR0AUNqCfF0AAFzOZs6cqXvuuUeSdOONNyo1NVUrV65U586dJUkTJkzQ3XffrXHjxrm2adWqlSSpcuXKCgwMVGRkpGJjY706bpcuXdyev/POO6pYsaJWrlypm2666YLbh4WFaeDAgZo1a5b69+8vSZo7d67q1Knjqh0A/AFXZgGglOzcuVM//PCDBgwYIEkKCgrSXXfdpZkzZ7rW2bx5s7p27Vrixz527JiGDRumhg0bKjo6WlFRUUpPT9eBAwc83sewYcO0bNkyHTp0SJI0e/ZsDR061DXcAQD8AVdmAaCUzJw5U7m5uapZs6ZrmTFGdrtd//jHPxQdHa2wsDCv9xsQEOA2VEGSHA6H2/MhQ4bov//9r958803Fx8fLbrcrISFBOTk5Hh+ndevWatWqlT744AP16NFD27Zt06JFi7yuFwBKE1dmAaAU5Obm6oMPPtDkyZO1efNm12PLli2qWbOmPv74Y0lSy5YttXz58iL3ExISory8PLdl1apV09GjR90C7ebNm93W+c9//qPHH39cvXv31pVXXim73a4TJ054fR4PPvigZs+erVmzZqlbt26Ki4vzeh8AUJoIswBQCr7++mudOnVKDzzwgJo3b+72uP32211DDcaMGaOPP/5YY8aM0fbt27V161a98sorrv3UrVtXq1at0qFDh1xhtHPnzjp+/LheffVV7d27V2+99Za++eYbt+M3bNhQH374obZv365169Zp0KBBF3UVeODAgfrtt9/07rvvcuMXAL9EmAWAUjBz5kx169ZN0dHRBV67/fbbtX79ev3000/q3LmzPvvsM/3zn//UVVddpS5duuiHH35wrTt+/Hj9+uuvql+/vqpVqyZJatq0qd5++2299dZbatWqlX744Qc988wzBY5/6tQpXX311br33nv1+OOPKyYmxuvziI6O1u23366IiAj17dvX6+0BoLTZzO8HXgEAcJ6uXbvqyiuv1N/+9jdflwIABRBmAQCFOnXqlFasWKE77rhDv/zyixo3buzrkgCgAGYzAAAUqnXr1jp16pReeeUVgiwAv8WVWQAAAFgWN4ABAADAsgizAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADL+j9ldqhs1r7QXwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assume these are your fitted parameters\n",
    "A, B, alpha, beta, E = power_law_params\n",
    "\n",
    "# Using the fitted parameters to make predictions:\n",
    "# According to the model, y_pred = A / (N^alpha) + B / (D^beta) + E\n",
    "y_pred = A / (N**alpha) + B / (D**beta) + E\n",
    "\n",
    "# Check how well predictions match the actual values:\n",
    "residuals = y - y_pred\n",
    "mse = np.mean(residuals**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y, y_pred, alpha=0.7, label='Predicted vs Actual')\n",
    "plt.xlabel(\"Actual y\")\n",
    "plt.ylabel(\"Predicted y\")\n",
    "plt.title(\"Power Law Fit Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# # If you want a more direct visualization of the fit, you might try plotting\n",
    "# # the predicted surface as a function of N and D, or plot slices holding one\n",
    "# # variable constant.\n",
    "\n",
    "# # For instance, if you want to see how predictions vary with N for a fixed D:\n",
    "# fixed_D = np.median(D)  # choose a representative D\n",
    "# N_space = np.linspace(min(N), max(N), 100)\n",
    "# y_line = A / (N_space**alpha) + B / (fixed_D**beta) + E\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(N, y, alpha=0.7, label='Actual data')\n",
    "# plt.plot(N_space, y_line, 'r-', label=f'Fit at D={fixed_D:.2f}')\n",
    "# plt.xlabel(\"N\")\n",
    "# plt.ylabel(\"y\")\n",
    "# plt.title(\"Power Law Fit with Fixed D\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling metrics for runs ['llama-8b-tootsie-0.001-19ad63']\n",
      "API:  <wandb.apis.public.api.Api object at 0x13d402d10>\n",
      "Pulling metrics for run llama-8b-tootsie-0.001-19ad63\n"
     ]
    }
   ],
   "source": [
    "PRED_RUN = \"llama-8b-tootsie-0.001-19ad63\"\n",
    "tootsie_pred_run_metrics = pull_metrics_from_wandb([PRED_RUN], metrics=[\"eval/loss\", \"throughput/total_tokens\"], summary_fields=[\"parameter_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval/loss</th>\n",
       "      <th>throughput/total_tokens</th>\n",
       "      <th>run</th>\n",
       "      <th>parameter_count</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.239254</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.512954</td>\n",
       "      <td>2.097152e+10</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.406456</td>\n",
       "      <td>4.194304e+10</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361698</td>\n",
       "      <td>6.291456e+10</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.332177</td>\n",
       "      <td>8.388608e+10</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.314757</td>\n",
       "      <td>1.048576e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.300089</td>\n",
       "      <td>1.258291e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.291108</td>\n",
       "      <td>1.468006e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.283978</td>\n",
       "      <td>1.677722e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.277572</td>\n",
       "      <td>1.887437e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.271530</td>\n",
       "      <td>2.097152e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.265843</td>\n",
       "      <td>2.306867e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.261999</td>\n",
       "      <td>2.516582e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.257791</td>\n",
       "      <td>2.726298e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.254954</td>\n",
       "      <td>2.936013e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.252457</td>\n",
       "      <td>3.145728e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.250006</td>\n",
       "      <td>3.355443e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.245322</td>\n",
       "      <td>3.565158e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.241582</td>\n",
       "      <td>3.774874e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.239729</td>\n",
       "      <td>3.984589e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.236531</td>\n",
       "      <td>4.194304e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.237351</td>\n",
       "      <td>4.404019e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.235256</td>\n",
       "      <td>4.613734e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.233087</td>\n",
       "      <td>4.823450e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.231997</td>\n",
       "      <td>5.033165e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.230420</td>\n",
       "      <td>5.242880e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.230186</td>\n",
       "      <td>5.452595e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.228145</td>\n",
       "      <td>5.662310e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.226569</td>\n",
       "      <td>5.872026e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.223045</td>\n",
       "      <td>6.081741e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.222917</td>\n",
       "      <td>6.291456e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.222554</td>\n",
       "      <td>6.501171e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.223377</td>\n",
       "      <td>6.710886e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.220619</td>\n",
       "      <td>6.920602e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.219745</td>\n",
       "      <td>7.130317e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.219056</td>\n",
       "      <td>7.340032e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.218404</td>\n",
       "      <td>7.549747e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.217349</td>\n",
       "      <td>7.759462e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.213822</td>\n",
       "      <td>7.969178e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.215168</td>\n",
       "      <td>8.178893e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.225626</td>\n",
       "      <td>8.388608e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.223433</td>\n",
       "      <td>8.808038e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.220759</td>\n",
       "      <td>9.646899e+11</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.218600</td>\n",
       "      <td>1.048576e+12</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.216938</td>\n",
       "      <td>1.090519e+12</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.216292</td>\n",
       "      <td>1.132462e+12</td>\n",
       "      <td>llama-8b-tootsie-0.001-19ad63</td>\n",
       "      <td>8030261248</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eval/loss  throughput/total_tokens                            run  \\\n",
       "0   12.239254             0.000000e+00  llama-8b-tootsie-0.001-19ad63   \n",
       "1    2.512954             2.097152e+10  llama-8b-tootsie-0.001-19ad63   \n",
       "2    2.406456             4.194304e+10  llama-8b-tootsie-0.001-19ad63   \n",
       "3    2.361698             6.291456e+10  llama-8b-tootsie-0.001-19ad63   \n",
       "4    2.332177             8.388608e+10  llama-8b-tootsie-0.001-19ad63   \n",
       "5    2.314757             1.048576e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "6    2.300089             1.258291e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "7    2.291108             1.468006e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "8    2.283978             1.677722e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "9    2.277572             1.887437e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "10   2.271530             2.097152e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "11   2.265843             2.306867e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "12   2.261999             2.516582e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "13   2.257791             2.726298e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "14   2.254954             2.936013e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "15   2.252457             3.145728e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "16   2.250006             3.355443e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "17   2.245322             3.565158e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "18   2.241582             3.774874e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "19   2.239729             3.984589e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "20   2.236531             4.194304e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "21   2.237351             4.404019e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "22   2.235256             4.613734e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "23   2.233087             4.823450e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "24   2.231997             5.033165e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "25   2.230420             5.242880e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "26   2.230186             5.452595e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "27   2.228145             5.662310e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "28   2.226569             5.872026e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "29   2.223045             6.081741e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "30   2.222917             6.291456e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "31   2.222554             6.501171e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "32   2.223377             6.710886e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "33   2.220619             6.920602e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "34   2.219745             7.130317e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "35   2.219056             7.340032e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "36   2.218404             7.549747e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "37   2.217349             7.759462e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "38   2.213822             7.969178e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "39   2.215168             8.178893e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "40   2.225626             8.388608e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "41   2.223433             8.808038e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "42   2.220759             9.646899e+11  llama-8b-tootsie-0.001-19ad63   \n",
       "43   2.218600             1.048576e+12  llama-8b-tootsie-0.001-19ad63   \n",
       "44   2.216938             1.090519e+12  llama-8b-tootsie-0.001-19ad63   \n",
       "45   2.216292             1.132462e+12  llama-8b-tootsie-0.001-19ad63   \n",
       "\n",
       "    parameter_count  step  \n",
       "0        8030261248     0  \n",
       "1        8030261248     1  \n",
       "2        8030261248     2  \n",
       "3        8030261248     3  \n",
       "4        8030261248     4  \n",
       "5        8030261248     5  \n",
       "6        8030261248     6  \n",
       "7        8030261248     7  \n",
       "8        8030261248     8  \n",
       "9        8030261248     9  \n",
       "10       8030261248    10  \n",
       "11       8030261248    11  \n",
       "12       8030261248    12  \n",
       "13       8030261248    13  \n",
       "14       8030261248    14  \n",
       "15       8030261248    15  \n",
       "16       8030261248    16  \n",
       "17       8030261248    17  \n",
       "18       8030261248    18  \n",
       "19       8030261248    19  \n",
       "20       8030261248    20  \n",
       "21       8030261248    21  \n",
       "22       8030261248    22  \n",
       "23       8030261248    23  \n",
       "24       8030261248    24  \n",
       "25       8030261248    25  \n",
       "26       8030261248    26  \n",
       "27       8030261248    27  \n",
       "28       8030261248    28  \n",
       "29       8030261248    29  \n",
       "30       8030261248    30  \n",
       "31       8030261248    31  \n",
       "32       8030261248    32  \n",
       "33       8030261248    33  \n",
       "34       8030261248    34  \n",
       "35       8030261248    35  \n",
       "36       8030261248    36  \n",
       "37       8030261248    37  \n",
       "38       8030261248    38  \n",
       "39       8030261248    39  \n",
       "40       8030261248    40  \n",
       "41       8030261248    41  \n",
       "42       8030261248    42  \n",
       "43       8030261248    43  \n",
       "44       8030261248    44  \n",
       "45       8030261248    45  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tootsie_pred_run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248 8030261248 8030261248\n",
      " 8030261248 8030261248 8030261248 8030261248] [0.00000000e+00 2.09715200e+10 4.19430400e+10 6.29145600e+10\n",
      " 8.38860800e+10 1.04857600e+11 1.25829120e+11 1.46800640e+11\n",
      " 1.67772160e+11 1.88743680e+11 2.09715200e+11 2.30686720e+11\n",
      " 2.51658240e+11 2.72629760e+11 2.93601280e+11 3.14572800e+11\n",
      " 3.35544320e+11 3.56515840e+11 3.77487360e+11 3.98458880e+11\n",
      " 4.19430400e+11 4.40401920e+11 4.61373440e+11 4.82344960e+11\n",
      " 5.03316480e+11 5.24288000e+11 5.45259520e+11 5.66231040e+11\n",
      " 5.87202560e+11 6.08174080e+11 6.29145600e+11 6.50117120e+11\n",
      " 6.71088640e+11 6.92060160e+11 7.13031680e+11 7.34003200e+11\n",
      " 7.54974720e+11 7.75946240e+11 7.96917760e+11 8.17889280e+11\n",
      " 8.38860800e+11 8.80803840e+11 9.64689920e+11 1.04857600e+12\n",
      " 1.09051904e+12 1.13246208e+12]\n",
      "Power law model parameters: (np.float64(1.0000000018496602), np.float64(1.0), np.float64(0.9999999652292553), np.float64(0.9999999998410559), np.float64(2.664603709767195))\n",
      "Actual: 12.2393, Predicted: inf\n",
      "Actual: 2.5130, Predicted: 2.6646\n",
      "Actual: 2.4065, Predicted: 2.6646\n",
      "Actual: 2.3617, Predicted: 2.6646\n",
      "Actual: 2.3322, Predicted: 2.6646\n",
      "Actual: 2.3148, Predicted: 2.6646\n",
      "Actual: 2.3001, Predicted: 2.6646\n",
      "Actual: 2.2911, Predicted: 2.6646\n",
      "Actual: 2.2840, Predicted: 2.6646\n",
      "Actual: 2.2776, Predicted: 2.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/zgr0v5mj6vzbtwp109vw8b780000gn/T/ipykernel_4241/1259256342.py:9: RuntimeWarning: divide by zero encountered in divide\n",
      "  y_pred_new = A / (N_pred**alpha) + B / (D_pred**beta) + E\n"
     ]
    }
   ],
   "source": [
    "N_pred = tootsie_pred_run_metrics[\"parameter_count\"].values\n",
    "D_pred = tootsie_pred_run_metrics[\"throughput/total_tokens\"].values\n",
    "\n",
    "print(N_pred, D_pred)\n",
    "\n",
    "print(\"Power law model parameters:\", power_law_params)\n",
    "\n",
    "# Apply the power law model to generate predictions\n",
    "y_pred_new = A / (N_pred**alpha) + B / (D_pred**beta) + E\n",
    "\n",
    "# If you have actual targets (e.g., 'eval/loss'), compare them\n",
    "y_actual_new = tootsie_pred_run_metrics[\"eval/loss\"].values\n",
    "\n",
    "# Print a few predictions alongside the actual values\n",
    "for i in range(min(10, len(y_actual_new))):\n",
    "    print(f\"Actual: {y_actual_new[i]:.4f}, Predicted: {y_pred_new[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax.nn\n",
    "# import wandb\n",
    "\n",
    "\n",
    "# api = wandb.Api()\n",
    "\n",
    "# filters = {\n",
    "#     \"duration\": {\"$gte\": 60 * 10},\n",
    "# }\n",
    "\n",
    "# runs = api.runs(f\"{entity}/{project}\", filters)\n",
    "\n",
    "\n",
    "\n",
    "# #%%\n",
    "# # get final eval losses, parameter_count, batch_size and num steps, and seqlen for each run\n",
    "# # key_of_interest = \"eval/loss\"\n",
    "# # loss_curves = [[row[key_of_interest] for row in run.scan_history([key_of_interest], 1000)] for run in runs]\n",
    "\n",
    "\n",
    "# def process_run(run):\n",
    "#     return {\n",
    "#         \"loss\": run.summary[\"eval/loss\"],\n",
    "#         \"parameter_count\": run.summary[\"parameter_count\"],\n",
    "#         \"total_tokens\": run.summary[\"throughput/total_tokens\"],\n",
    "#         \"num_steps\": run.config[\"trainer\"][\"num_train_steps\"],\n",
    "#         \"seq_len\": run.config[\"model\"][\"seq_len\"],\n",
    "#         \"mlp_scale\": run.config[\"model\"].get(\"mlp_scale\", 4),\n",
    "#         \"lr\": run.config.get(\"optimizer\", run.config[\"trainer\"])[\"learning_rate\"]\n",
    "#     }\n",
    "\n",
    "# data = [process_run(run) for run in runs]\n",
    "\n",
    "\n",
    "\n",
    "# #%%\n",
    "# # regress loss against parameter count and log total tokens\n",
    "\n",
    "# import numpy as np\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# TRANSFORMS = {\n",
    "#     \"eval/loss\": np.log,\n",
    "#     \"parameter_count\": np.log,\n",
    "#     \"throughput/total_tokens\": np.log,\n",
    "#     \"seq_len\": np.log,\n",
    "#     # \"num_steps\": np.log,\n",
    "# }\n",
    "\n",
    "# def prep_data(data: list[dict], target=\"eval/loss\", transforms = TRANSFORMS):\n",
    "#     # Extract and transform the data\n",
    "#     data_matrix = []\n",
    "#     target_data = [x[target] for x in data]\n",
    "#     transform = transforms.get(target, None)\n",
    "#     if transform:\n",
    "#         target_data = transform(target_data)\n",
    "    \n",
    "#     keys = list(data[0].keys())\n",
    "#     names = [\"constant\"]\n",
    "\n",
    "#     for var in keys:\n",
    "#         if var == target:\n",
    "#             continue\n",
    "#         names.append(var)\n",
    "#         extracted_data = [x[var] for x in data]\n",
    "#         transform = transforms.get(var, None)\n",
    "#         if transform:\n",
    "#             extracted_data = transform(extracted_data)\n",
    "#         data_matrix.append(extracted_data)\n",
    "\n",
    "#     # Create a design matrix\n",
    "#     X = np.column_stack(data_matrix)\n",
    "#     X = sm.add_constant(X, has_constant=\"add\")\n",
    "    \n",
    "#     return names, X, target_data\n",
    "\n",
    "# names, X, log_losses = prep_data(data, target=\"loss\")\n",
    "\n",
    "# # Perform the regression\n",
    "# model = sm.OLS(log_losses, X)\n",
    "# results = model.fit()\n",
    "\n",
    "# # Print the results\n",
    "# print(results.summary())\n",
    "\n",
    "\n",
    "\n",
    "# #%%\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # \n",
    "# # # 1. Scatter plot with regression plane (for two predictors)\n",
    "# # fig = plt.figure(figsize=(10, 7))\n",
    "# # ax = fig.add_subplot(111, projection='3d')\n",
    "# # ax.scatter(log_parameter_counts, log_total_tokens, log_losses, c='b', marker='o')\n",
    "# # xx1, xx2 = np.meshgrid(np.linspace(log_parameter_counts.min(), log_parameter_counts.max(), 100),\n",
    "# #                        np.linspace(log_total_tokens.min(), log_total_tokens.max(), 100))\n",
    "# # yy = results.params[0] + results.params[1] * xx1 + results.params[2] * xx2\n",
    "# # ax.plot_surface(xx1, xx2, yy, alpha=0.5, color='r')\n",
    "# # ax.set_xlabel('Log(Parameter Counts)')\n",
    "# # ax.set_ylabel('Log(Total Tokens)')\n",
    "# # ax.set_zlabel('Log(Loss)')\n",
    "# # plt.show()\n",
    "\n",
    "# #%%\n",
    "# # a few test points:\n",
    "\n",
    "# test_project = \"levanter\"\n",
    "\n",
    "# filters = {\n",
    "#     \"duration\": {\"$gte\": 60 * 60 * 2},\n",
    "#     \"tags\": {\"$in\": [\"pile\"]},\n",
    "#     # \"id\": {\"$in\": [\"6ns1msth\"]}\n",
    "# }\n",
    "\n",
    "\n",
    "# test_runs = api.runs(f\"{entity}/{test_project}\", filters)\n",
    "# # id = \"6ns1msth\"\n",
    "# id = \"r88xoomz\"\n",
    "# test_runs = [api.run(f\"{entity}/{test_project}/{id}\")]\n",
    "\n",
    "# def process_run_or_none(run):\n",
    "#     try:\n",
    "#         return process_run(run)\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# test_data = [process_run_or_none(run) for run in test_runs if process_run_or_none(run) is not None and run.summary[\"eval/loss\"] > 0.8 and run.summary[\"throughput/total_tokens\"] > 100]\n",
    "\n",
    "\n",
    "# print(len(test_data))\n",
    "\n",
    "\n",
    "\n",
    "# #%%\n",
    "# _, X_test, log_test_losses = prep_data(test_data, target=\"loss\")\n",
    "# # test the model on the test data\n",
    "# train_losses_predicted = results.predict(X)\n",
    "# log_losses_predicted = results.predict(X_test)\n",
    "\n",
    "# print(\"train relative error: \", np.mean(np.abs(train_losses_predicted - log_losses) / log_losses))\n",
    "# print(\"test relative error: \", np.mean(np.abs(log_losses_predicted - log_test_losses) / log_test_losses))\n",
    "\n",
    "# print(np.exp(log_losses_predicted), np.exp(log_test_losses))\n",
    "\n",
    "\n",
    "# #%%\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# sm.graphics.plot_partregress_grid(results, fig=fig)\n",
    "# plt.show()\n",
    "# #%%\n",
    "# predicted = results.predict(X)\n",
    "# residuals = log_losses - predicted\n",
    "# log_parameter_counts = np.log([x[\"parameter_count\"] for x in data])\n",
    "# cmin = np.min(log_parameter_counts)\n",
    "# cmax = np.max(log_parameter_counts)\n",
    "# sc = plt.scatter(predicted, residuals, c=log_parameter_counts, cmap='viridis', vmin=cmin, vmax=cmax)\n",
    "# plt.colorbar(sc, label='Log(Parameters)')\n",
    "# plt.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "# # add test points\n",
    "# predicted_test = results.predict(X_test)\n",
    "# residuals_test = log_test_losses - predicted_test\n",
    "# plt.scatter(predicted_test, residuals_test, color=\"red\")\n",
    "\n",
    "# # flip x axis\n",
    "# plt.xlim(plt.xlim()[::-1])\n",
    "\n",
    "# print(f\"Min log param count: {cmin:.3f}, exp'd: {np.exp(cmin):.3e}\")\n",
    "# print(f\"Max log param count: {cmax:.3f}, exp'd: {np.exp(cmax):.3e}\")\n",
    "# plt.xlabel('Predicted log(loss)')\n",
    "# plt.ylabel('Residuals')\n",
    "# plt.title('Residuals vs. Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# #%%\n",
    "# from tqdm import tqdm\n",
    "# import jax\n",
    "# import jaxopt\n",
    "# import jax.numpy as jnp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def inv_exp_predict(params, x):\n",
    "#     (log_constant, log_scale, exponent) = params\n",
    "#     # NOTE: chinchilla uses huber\n",
    "#     z = jax.nn.logsumexp(log_scale - exponent * x, axis=1)\n",
    "#     # log(exp(a) + exp(b)) = log(exp(a) * (1 + exp(b - a))) = a + log(1 + exp(b - a))\n",
    "#     predicted = log_constant + jnp.log1p(jnp.exp(z - log_constant))\n",
    "#     return predicted\n",
    "\n",
    "\n",
    "# def mse_loss(params, y, x):\n",
    "#     # ignore the scale\n",
    "#     (log_constant, _, exponent) = params\n",
    "#     targets = log_constant + jnp.einsum(\"ij,j->i\", x, exponent)\n",
    "    \n",
    "#     return jnp.mean((y - targets) ** 2)\n",
    "\n",
    "# # our predictor is:\n",
    "# # log(exp(a dot x) + \n",
    "\n",
    "    \n",
    "# new_X = X[:, 1:]\n",
    "\n",
    "# loss_fn = chinchilla_loss\n",
    "\n",
    "# min_loss = 1e9\n",
    "# best_state = None\n",
    "\n",
    "# seed = 0\n",
    "# rng = jax.random.PRNGKey(seed)\n",
    "# pbar = tqdm(range(10))\n",
    "# for i in pbar:\n",
    "#     this_rng, rng = jax.random.split(rng)\n",
    "#     k_c, k_s, k_e = jax.random.split(rng, 3)\n",
    "#     init_constant = jax.random.uniform(k_c, shape=(), minval=-1, maxval=1)\n",
    "#     init_scale = jax.random.uniform(k_s, shape=(len(new_X[1]), ), minval=0, maxval=2)\n",
    "#     init_exponent = jax.random.uniform(k_e, shape=(len(new_X[1]), ), minval=0, maxval=2)\n",
    "    \n",
    "#     state = jaxopt.LBFGS(loss_fn, tol=1E-9, maxiter=1000).run(\n",
    "#         (init_constant, init_scale, init_exponent),\n",
    "#         log_losses,\n",
    "#         new_X\n",
    "#     )\n",
    "    \n",
    "#     # print(state.params)\n",
    "#     final_loss = loss_fn(state.params, log_losses, new_X)\n",
    "#     if final_loss < min_loss:\n",
    "#         min_loss = final_loss\n",
    "#         best_state = state\n",
    "#         pbar.set_postfix({\"loss\": min_loss})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# if False:\n",
    "#     for alpha in jnp.linspace(0, 2, num=5, endpoint=True):\n",
    "#         for beta in jnp.linspace(0, 2, num=5, endpoint=True):\n",
    "#             for e in jnp.linspace(-1, 1, num=5, endpoint=True):\n",
    "#                 for a in jnp.linspace(0, 25, num=6):\n",
    "#                     for b in jnp.linspace(0, 25, num=6):\n",
    "#                         print(\".\", end=\"\", flush=True)\n",
    "#                         init_constant = e\n",
    "#                         init_scale = jnp.array([a, b])\n",
    "#                         init_exponent = jnp.array([alpha, beta])\n",
    "                        \n",
    "#                         state = jaxopt.LBFGS(loss_fn, tol=1E-9, maxiter=1000).run(\n",
    "#                             (init_constant, init_scale, init_exponent),\n",
    "#                             log_losses,\n",
    "#                             new_X\n",
    "#                         )\n",
    "                        \n",
    "#                         # print(state.params)\n",
    "#                         final_loss = loss_fn(state.params, log_losses, new_X)\n",
    "#                         if final_loss < min_loss:\n",
    "#                             min_loss = final_loss\n",
    "#                             best_state = state\n",
    "#                         # print(mse_loss(state.params, log_losses, new_X))\n",
    "           \n",
    "     \n",
    "    \n",
    "# print(best_state.params)\n",
    "# print(\"Loss:\", loss_fn(best_state.params, log_losses, new_X))\n",
    "# # print(mse_loss(state.params, log_losses, new_X))\n",
    "\n",
    "# log_constant, log_scale, exponent = best_state.params\n",
    "\n",
    "# print(f\"Constant: {jnp.exp(log_constant).item():.3f}\")\n",
    "# print(\"Params:\")\n",
    "# for i, (s, e) in enumerate(zip(jnp.exp(log_scale), exponent)):\n",
    "#     name = names[i + 1]\n",
    "#     print(f\"  {i} {name}: {s.item():.3f}, {e.item():.3f}\")\n",
    "\n",
    "# print(jnp.exp(log_constant), jnp.exp(log_scale), exponent)\n",
    "# #%%\n",
    "# print(inv_exp_predict(best_state.params, X_test[:, 1:]), log_test_losses)\n",
    "# #%% md\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
