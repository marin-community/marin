# Kelp: Tree Diffusion for Program Synthesis

## Overview

Kelp is a novel research project that bridges three research directions:

1. **Tree diffusion** (Berkeley's work on AST-based diffusion for program synthesis) https://arxiv.org/html/2405.20519v1, https://tree-diffusion.github.io/
2. **AR-to-diffusion transfer** (Dream, DiffuGPT approaches for initializing diffusion from pretrained LLMs) https://arxiv.org/html/2410.17891v2, https://github.com/HKUNLP/DiffuLLaMA
3. **Marin for open R&D** (This project lives as a Marin experiment and thus has a strong foundation for open LLM development) https://marin.community/, https://marin.readthedocs.io/en/latest/, https://github.com/marin-community/marin

In this experiment, we aim to create a novel program synthesis algorithm. The goal is to make use of the pre-trained Marin 8b model and adapt it into perform diffusion on trees, specifically abstract syntax trees.

Tree diffusion should restrict the search space for generating programs based on the ASTs of the programming language. By adapting the diffusion model from a pre-trained model, we're more likely to competently generate complex programs than tree-diffusion would be able to accomplish alone.

## Technical Details

- We should target program synthesis in the Python language at first. We can expand to other languages later. 
- We should try to re-use as many components from the Marin project as possible -- Levanter has most of the utilities for creating and training models that we'll even need. Further, we aim to build on Marin's base 8b model (` "gs://marin-us-central2/checkpoints/tootsie-8b-deeper-starling/hf/step-1419999"`). Let's re-use as much of that infrastructure as possible -- e.g. the [Marin executor](https://marin.readthedocs.io/en/latest/tutorials/executor-101/) and [LM training pipeline](https://marin.readthedocs.io/en/latest/tutorials/train-an-lm/). Another example: Levanter has routines for saving and loading checkpointing based on tensorstore.
- Ideally, we should be able to run a small/toy version of the model on this laptop (an M3 Macbook Air) for fast iteration cycles. We should still scale up to run on TPU clusters.
- Our tree diffusion model should be conditioned on english text prompts for programs; for example, the docstrings (and type signatures) of a function or file is the prompt, then the code body is the target program.
- We should start with three levels of datasets to train. The first dataset is a toy dataset, and it is a hard-coded list of toy Python functions and programs. The second dataset is called "quine", and it consists of reading all the Python code in this codebase (the Marin monorepo) -- applying processing to break it up into a training dataset. The last dataset we'll use for creating our tree diffusion model are the Python contents of the StackEdu dataset from the Marin project's dataset catalog. More (Python) coding datasets will come later (ideally, the codebase should be designed to change in this way).
- In our data preparation for training, we won't need to filter out complex programs like the UC Berkeley research project did. The reason is that our ultimate model will make use of a pre-trained LLM, and thus should be able to handle complex challenges.
- We will evaluate on Google Research's MBPP dataset and evaluation. See `experiments/eval_datasets.py:311` for accessing the eval dataset. Ideally, we should be able to add OpenAI's HumanEval evaluation in few ideal lines of code (see how that is configured in this file).
- We will build this experiment iteratively. Thus, we need to pursue fast iteration cycles (running on CPU/a laptop; creating models from scratch before transfer learning). To develop iteratively, we should look for ways to verify correctness from experience (such as eval metrics, training logs, simulation tests). From this kind of feedback and critical self-review, we should strive to improve the codebase early and often. 
- All experiment sources should live in this experiments/kelp directory (and tests, in the commensurate folder in tests/).

## Plan

1. First, we flush out a plan. Research my proposal, including background papers and source code. Read the Marin documentation and codebase and get a sense of how to add maintainable experiments to the project. Then, update this Markdown file with a plan that I can review and approve. Please add specific ways to verify the correctness of each step in the plan.
2. Next, we start by creating a model evaluation system specific to this model architecture. I believe that Marin's existing system for evaluation is more geared towards AR LLMs, not diffusion models. Please try to understand the existing system before creating a kelp-specific eval suited for our experiment in order to re-use infrastructure to make it maintainable. Please establish several core metrics we can use to gain "observability" about how our model is performing. These will be our guides to prioritize development on the rest of the project. Ideally, the evals should resemble this script: `marin/experiments/evals/run_base_model_evals.py`, which uses the executor system.
3. Create the Minimal Data + Tree Diffusion POC (the toy model). It should use the toy dataset mentioned above. It should include a training loop that writes checkpoints in such a way that they can be read by the evaluation system.
4. Train a toy model and run evaluations against it. While I wouldn't expect the model to be very good, I do expect that the system should work without error. At this stage, be very skeptical of the system that has been built so far. What are possible logical errors in our formulation of the research experiment? Where are there system related errors (e.g. normal programming error)? This is an opportunity to reduce the complexity of the system to make it more maintable/robust to change. Don't be afriad to "kill your darlings" when code doesn't serve our ultimate goal. Reading the experiment code so far, are there any "quick wins" we could make to the model or toy data preparation pipeline to improve upon our evaluation metrics? Once the tensor/tree conversion, dataset, model, and training loop are created, I expect there to be sufficient verification that everything is correct (round trip tests, proof the model trains, proof that we can generate syntactically valid python).
5. Scale up the data pipeline by creating the "quine" dataset mentioned above. We should aim to run this on local first, but also potentially on a TPU cluster via Marin (if we wanted to scale up the model). After the quinte dataset is established, we should run another training-eval loop. Does the model improve with additional data? Does this reveal other types of errors (logical errors, programmatic error)? Please read the sources and outputs and skeptically create a critique of the project thusfar. Then, address those issues.
6. Scale up the data and training pipeline that it can work on larger models and datasets. Specifically, create a Marin dataset pipeline that integrates with the Stack-Edu Python dataset. While it would be nice if it could run on my laptop, it should primarily be designed to run on a TPU cluster via the normal Marin framework. 
7. At this point, please review the entirety of the kelp experiment and look for opportunities to improve it. The codebase should be low complexity and follow Marin project conventions. Look to the grugformer for inspiration (see https://github.com/marin-community/marin/pull/2171, `experiments/speedrun/grugformer_starter/grugformer_speedrun.py`; `.agents/projects/grugformer.md`; and `lib/levanter/src/levanter/grug`, https://grugbrain.dev/).
8. Add a module to perform the AR to Tree Diffusion transfer. This transfer module be configurable from the main training script (`train.py`) and backwards compatible with training a model from scratch. The transfer should adapt the `deeper_starling_path` 8b Marin model.


