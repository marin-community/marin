def royal_order(last_id=-1):
    '''Retrive royal orders
    if last_id not defiend it will return the max
    return list of royal orders tuples up to MAX_PAGES_TO_SEARCH (page=10)
    [(id, title, url, text)...]
    '''
    orders = []
    _news = retrieve_news(royal=1, last_id=last_id)
    for item in _news:
        _detail = retrieve_detail(item)
        orders.append(_detail)
    return orders
# ---
def get_default(cls) -> "BlockSizes":
        return cls()
# ---
def clean_html(html: BeautifulSoup | str) -> str:
    if isinstance(html, str):
        html = BeautifulSoup(html, "html.parser")
    remove_authors(html)
    remove_title_page(html)
    clean_li(html)
    remove_biblio(html)
    remove_footnotes(html)
    remove_biblinks(html)
    linelisting_to_newline(html)
    deconstruct_eqn(html)
    remove_ar5iv_footer(html)
    remove_before_section(html)
    remove_title(html)
    return str(html)
# ---
def _weight_transfer_hook(info: levanter.callbacks.StepInfo):
            self.weight_transfer_hook(trainer, info)
# ---
def without_axes(axis_spec: ShapeDict, to_remove: AxisSelection, allow_mismatched_sizes=False) -> ShapeDict:  # type: ignore
    ...
# ---
def __init__(self):
        self._loop = None
        self._thread = threading.Thread(target=self._run_event_loop, daemon=True)
        self._started = threading.Event()
# ---
def __call__(self, x, key=None):
        if key is not None:
            k1, k2 = jax.random.split(key)
            return self.lora(x, key=k2) + self.wrapped(x, key=k1)
        else:

            return self.lora(x) + self.wrapped(x)
# ---
def setup(self, links, footnotes):
        self.footnote_index = 0
        self.links = links or {}
        self.footnotes = footnotes or {}
# ---
def write_provenance_json(output_path, metadata: dict[str, Any]) -> None:
    print(f"[*] Writing Dataset `provenance.json` to `{output_path}`")
    metadata["access_time"] = datetime.now(timezone.utc).isoformat()

    with fsspec.open(f"{output_path}/provenance.json", "w") as f:
        json.dump(metadata, f, indent=4, sort_keys=True)
# ---
def make_sscc(self, cr, uid, context=None):
        sequence = self.pool.get('ir.sequence').get(cr, uid, 'stock.lot.tracking')
        try:
            return sequence + str(self.checksum(sequence))
        except Exception:
            return sequence
# ---
def compute_logits(model: LmHeadModel, example: LmExample):
            model = trainer.mp.cast_to_compute(model)
            activations = model.activations(example.tokens, key=None, attn_mask=example.attn_mask)
            head = model.get_lm_head()
            logits = hax.dot(activations, head, axis=model.Embed)
            return logits
# ---
def _escape(s: str) -> str:
    out = html.escape(s, quote=False)
    # we want newlines and other special characters to be visible
    # we also want strings of spaces to be visible
    out = out.replace("\n", "âŽ").replace("\t", "â‡¥")
    out = re.sub(r"  +", lambda m: " " + "â£" * (len(m.group(0)) - 1), out)
    return out
# ---
def height(self, new_height):
        self.set_size(self.width, new_height)
# ---
def __post_init__(self):
        if len(self._axes) != len(self._prefix):
            raise ValueError("Prefix entries must align with axes")
# ---
def __pos__(self) -> "NamedArray":  # pragma: no cover
        return haliax.positive(self)
# ---
def on_test(self):
            pass
# ---
def _dummy(v):
    return {"p": jnp.array(v, jnp.float32)}
# ---
def get_level(self, nick):
        return self.data[nick.lower()]['level']
# ---
def string_to_bytes( text ):
    return bin(int.from_bytes(text.encode(), 'big'))
# ---
def testAugAssignBitAnd(self):
    self.assertEqual((0, '3\n'), _GrumpRun(textwrap.dedent("""\
        foo = 7
        foo &= 3
        print foo""")))
# ---
def NumLinesInBuffer( buffer_object ):
  # This is actually less than obvious, that's why it's wrapped in a function
  return len( buffer_object )
# ---
def load_parquet(self, columns: list[str] | None = None) -> Dataset[dict]:
        """Load records from parquet files."""
        return Dataset(self.source, [*self.operations, LoadFileOp("parquet", columns)])
# ---
def get_account_id(
    profile_name, aws_access_key_id, aws_secret_access_key, region=None,
):
    """Query STS for a users' account_id"""
    client = get_client(
        "sts", profile_name, aws_access_key_id, aws_secret_access_key, region,
    )
    return client.get_caller_identity().get("Account")
# ---
def __call__(
        self,
        mel: NamedArray,
        input_ids: NamedArray,
        attn_mask: Optional[AttentionMask | NamedArray] = None,
        *,
        key=None,
    ) -> NamedArray:
        pass
# ---
def _poll_loop(self):
        """Poll task status every 5 seconds."""
        while not self._stop_event.is_set():
            for job_id in list(self._tracked_jobs):
                try:
                    self._poll_job_tasks(job_id)
                except Exception as e:
                    self._logger.log(f"Error polling job {job_id}: {e}", level="WARN")
            self._stop_event.wait(SCHEDULING_POLL_INTERVAL_SECONDS)
# ---
def resolve_axis(self, axis: tuple[AxisSelector, ...]) -> tuple[Axis, ...]: ...
# ---
def i0(a: A) -> A:
    return wrap_elemwise_unary(jnp.i0, a)
# ---
def sample(self, node_type: str, rng: random.Random) -> SubtreeEntry | None:
        """Sample a random subtree of the given AST node type.

        Returns None if no subtrees of that type are in the bank.
        """
        candidates = self.entries.get(node_type)
        if not candidates:
            return None
        return rng.choice(candidates)
# ---
def test_parameter(self):
        param1 = ctds.Parameter(b'123', output=True)
        self.assertEqual(param1.value, b'123')
        self.assertTrue(isinstance(param1, ctds.Parameter))

        param2 = ctds.Parameter(b'123')
        self.assertEqual(param1.value, b'123')
        self.assertEqual(type(param1), type(param2))
        self.assertTrue(isinstance(param2, ctds.Parameter))
# ---
def forwards(self, orm):
        # Adding field 'UserProject.drive_auth'
        db.add_column(u'user_project', 'drive_auth',
                      self.gf('django.db.models.fields.BooleanField')(default=False),
                      keep_default=False)
# ---
def _normalize_record(raw: Any, dataset: UncheatableEvalDataset, index: int) -> dict[str, str]:
    text = _extract_text(raw)
    if text is None or not str(text).strip():
        raise ValueError(f"Record {index} in {dataset.name} does not contain text")
    record_id = _extract_id(raw, dataset, index)
    return {"id": record_id, "text": text, "source": dataset.source_label}
# ---
def test_dense_constraints(self):
    k_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = keras.layers.Dense(
        3, kernel_constraint=k_constraint, bias_constraint=b_constraint)
    layer(keras.backend.variable(np.ones((2, 4))))
    self.assertEqual(layer.kernel.constraint, k_constraint)
    self.assertEqual(layer.bias.constraint, b_constraint)
# ---
def cleanup(cache_dir: str):
    """Clean up cached bundles, venvs, and images."""
    cache_path = Path(cache_dir).expanduser()
    if cache_path.exists():
        shutil.rmtree(cache_path)
        click.echo(f"Removed cache directory: {cache_path}")
    else:
        click.echo(f"Cache directory does not exist: {cache_path}")
# ---
def resize_vocab(self, new_size: int, key: PRNGKeyArray | None = None) -> "HackableLMHeadModel":
        pass
# ---
def test_device_flops_for_invalid():
    assert device_flops_for_jax_device("Unknown Device XYZ") is None
    assert device_flops("invalid", "bf16") is None
    assert device_flops("v4", "invalid_dtype") is None
# ---
def hidden_reparam(use_mup: bool = True) -> type[AbstractLinearReparam]:
        """Return the reparameterization class for a hidden linear layer."""

        return mup.HiddenLinearMup if use_mup else mup.LinearStandardParam
# ---
def findPlugin(plugin_name):
    """
    @type plugin_name: str
    """
    pass
# ---
def poke(self, context):
        dag = context['dag']
        target_dttm = dag.following_schedule(context['execution_date'])
        target_dttm += self.delta
        logging.info('Checking if the time ({0}) has come'.format(target_dttm))
        return datetime.now() > target_dttm
# ---
def _default_sort(self, resource):
        datasource = self._datasource(resource)
        return datasource[3]
# ---
def _unique_segment_ids(max_Segments, segment_ids):
    # Extract unique segment IDs with padding
    # TODO: add unique to haliax
    unique_segment_ids = jnp.unique(segment_ids.array, size=max_Segments.size, fill_value=-1)
    unique_segment_ids = hax.named(unique_segment_ids, max_Segments)
    return unique_segment_ids
# ---
def vm_registry(self) -> VmRegistry:
        """Access the VM registry for RPC/status use."""
        return self._vm_registry
# ---
def attention_config(self) -> AttentionConfig:  # type: ignore[override]
        cfg = super().attention_config()
        return dataclasses.replace(cfg, qk_norm=self.norm_config)
# ---
def __call__(self, carry: hax.NamedArray) -> hax.NamedArray:
            return carry + self.weight
# ---
def release(self, ports: list[int]) -> None:
        with self._lock:
            for port in ports:
                self._allocated.discard(port)
# ---
def is_remote_branch(git_path, module, dest, remote, version):
    cmd = '%s ls-remote %s -h refs/heads/%s' % (git_path, remote, version)
    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)
    if to_native(version, errors='surrogate_or_strict') in out:
        return True
    else:
        return False
# ---
def main(config: EvaluationConfig) -> None:
    evaluate(config)
# ---
def test_bound_limit(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).limit(bindparam("l")),
            [(1, 1, 2), (2, 2, 3)],
            params={"l": 2},
        )
# ---
def LookupEndpoint(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def fmin(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.fmin](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fmin.html)
    """
    return jnp.fmin(x1, x2)
# ---
def on_fuzzing_range_end_changed(self, value: int):
        self.ui.sBAddRangeStart.setMaximum(value - 1)
        self.ui.sBAddRangeStep.setMaximum(value - self.ui.sBAddRangeStart.value())
# ---
def preempted_until_n():
        return ray.get(actor.run.remote())
# ---
def coords(self):
        """Dictionary of xray.DataArray objects corresponding to coordinate
        variables
        """
        return DatasetCoordinates(self)
# ---
def gen_header(data):
    if data['start_year'] == data['end_year']:
        data['dates'] = data['start_year']
    else:
        data['dates'] = '{} - {}'.format(data['start_year'], data['end_year'])
    return '\n'.join(line.rstrip() for line in data['header'].format(**data).strip().splitlines())
# ---
def evaluate(self, record: dict) -> Any:
        return self.value
# ---
def get_short_name(self):
        return self.name
# ---
def __call__(self, env):
        response = self.proxy(env)
        if response:
            return response

        response = super(ProxyArchivalRouter, self).__call__(env)
        if response:
            return response
# ---
def test_no_date_range(self):
        """Test when no start or end date is provided"""
        self.assertTrue(check_create_time("2023-01-01 12:00:00 PDT"))
# ---
def total(self):
        return self.__total
# ---
def _on_worker_heartbeat_failed(self, txn: TransactionLog, event: WorkerHeartbeatFailedEvent) -> None:
        worker = self._workers.get(event.worker_id)
        if not worker:
            return
        worker.consecutive_failures += 1
        txn.log("heartbeat_failed", event.worker_id, consecutive=worker.consecutive_failures)
        if worker.consecutive_failures >= HEARTBEAT_FAILURE_THRESHOLD:
            self._on_worker_failed(txn, WorkerFailedEvent(worker_id=event.worker_id, error=event.error))
# ---
def _sanitize_shard_name(name: str) -> str:
    safe = "".join(ch if ch.isalnum() or ch in ("-", "_", ".") else "_" for ch in name)
    return safe or "shard"
# ---
def playlist_next(self, mode='weak'):
        self.command('playlist_next', mode)
# ---
def partitionLabels(self, S: str) -> List[int]:
        lastPos, seen, currMax = {}, set(), -1
        res = []
        for i in range(0, 26):
            c = chr(97+i)
            lastPos[c] = S.rfind(c)
        for i, c in enumerate(S):
            # Encounter new index higher than currMax
            if i > currMax:
                res.append(currMax+1)
            currMax = max(currMax, lastPos[c])
        res.append(len(S))
        ans = [res[i]-res[i-1] for i in range(1, len(res))]
        return ans
# ---
def testSizeNonmultiple12(self):
        self.assertConfigureFails(HPCP(), {'size':13})
# ---
def publisher(self):
        del self._publisher
# ---
def _cached_filter_eval_shape(fun, *args, **kwargs):
    """
    eval_shape is surprisingly expensive, so we cache it. We use this for named_pjit for evaluating resource partitions
    of the output.
    """
    dynamic, static = hashable_partition((fun, args, kwargs), is_array)
    if static not in _eval_shape_cache:
        _eval_shape_cache[static] = eqx.filter_eval_shape(fun, *args, **kwargs)

    return _eval_shape_cache[static]
# ---
def test_best_of_n_no_duplicate_sources(params, model_cfg, tokenizer):
    """Best-of-N should deduplicate by source."""
    results = best_of_n(
        params=params,
        source="x = 1 + 2\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(6),
        n=8,
        max_depth=2,
    )
    sources = [c.source for c in results]
    assert len(sources) == len(set(sources))
# ---
def baby_llama_config():
    return InferenceServerConfig(
        service=InferenceEngineConfig(
            max_seq_len=32,
            max_seqs=2,
            page_size=4,
            max_queued_tokens=32,
            hbm_utilization=0.1,
        ),
        temperature=0.7,
        seed=42,
    )
# ---
def test_load_datasets_update(self):

        dataset_code = "nipa-section1-10101-a"
        self._load_files(dataset_code)
        self.assertLoadDatasetsUpdate([dataset_code])
# ---
def get_legacy_sigopcount_block(block, accurate=True):
    count = 0
    for tx in block.vtx:
        count += get_legacy_sigopcount_tx(tx, accurate)
    return count
# ---
def euler(f, x0, y0, xf, n):
    pas = (xf - x0) / n
    courbe = XY()
    courbe.title = "Methode d Euler"
    courbe.add("Solution approchee", listeEuler(f, x0, y0, pas, n))
    courbe.render_to_file("courbeEulerPython.svg")
# ---
def TerminateJob(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def testExprCall(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          print 'bar'
        foo()""")))
# ---
def client():
    """Create a local cluster client for testing."""
    client = LocalClusterClient.create()
    yield client
    client.shutdown()
# ---
def _find_data_browser_local_conf(max_parents: int = 6) -> Path | None:
    here = Path.cwd().resolve()
    for _ in range(max_parents + 1):
        candidate = here / _LOCAL_DATA_BROWSER_CONFIG_REL
        if candidate.exists():
            return candidate
        parent = here.parent
        if parent == here:
            break
        here = parent
    return None
# ---
def _count_reduce(key: str, items: Iterator[pa.StructScalar], *, canonical_id: str) -> DupeReduceResult:
    head = next(items)
    doc_cnt = sum(map(lambda _: 1, items)) + 1
    if doc_cnt == 1:
        return {
            "hash": None,
            "cnt": 1,
            "canonical": None,
        }

    return {
        "hash": key,
        "cnt": doc_cnt,
        "canonical": head[canonical_id],
    }
# ---
def __init__(self, fp, msg, status, reason):
        self.fp = fp
        self.msg = msg
        self.status = status
        self.reason = reason
        self.code = 200
# ---
def from_spec(spec: dict[str, list[str]]) -> Cluster:
        logger.info(f"Creating local cluster with spec: {spec}")
        use_isolated_env = spec.get("use_isolated_env", ["false"])[0].lower() == "true"
        config = LocalClusterConfig(use_isolated_env=use_isolated_env)
        logger.info(f"Local cluster config: {config}")
        return LocalCluster(config=config)
# ---
def model_type(self) -> type["ParallelLlamaLMHeadModel"]:
        return ParallelLlamaLMHeadModel
# ---
def hf_checkpoint_converter(cls) -> "HFCheckpointConverter":
        """The default HFCheckpointConverter to use for this config class. We recommend that you
        define this as a @cached_property on your config class."""
        pass
# ---
def __ne__(self, other):
        if not isinstance(other, GeomCoverage):
            return NotImplemented
        return not self.__eq__(other)
# ---
def cli(ctx):
    """Token Decoder REPL."""
    if ctx.invoked_subcommand is None:
        ctx.invoke(repl_cmd)
# ---
def compute_ray_retry_count(request: JobRequest) -> int:
    """Map separate failure/preemption retry counts to a single Ray retry count."""
    return request.max_retries_failure + request.max_retries_preemption
# ---
def track_job(self, job_id: str):
        """Add a job to monitor."""
        self._tracked_jobs.add(job_id)
# ---
def __getattr__(self, method_name: str) -> "_RpcMethod":
        return _RpcMethod(self, method_name)
# ---
def _test_do_execute_no_params(self, retval):
        with self._run_test(retval) as (conn, m1):
            result = conn.execution_options(no_parameters=True).\
                execute("insert into table foo")
        self._assert(
            retval,
            m1.do_execute_no_params, m1.real_do_execute_no_params,
            [call(
                    result.context.cursor,
                    "insert into table foo", result.context)]
        )
# ---
def __repr__(self) -> str:
        return f"({self.left} {_ARITHMETIC_SYMBOLS[self.op]} {self.right})"
# ---
def python_compute_document_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> list[dict[str, str]]:
    results = []
    for record in batch.to_pylist():
        text, record_id = record[text_col], record[id_col]
        results.append({"hash": _str_hash_legacy(text), "id": record_id})
    return results
# ---
def to_grug_model_config(self) -> GrugModelConfig:
        return GrugModelConfig(
            vocab_size=llama3_tokenizer_vocab_size,
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            max_seq_len=self.max_seq_len,
        )
# ---
def max_queued_tokens(self) -> int:
        """Expose queue capacity from ``TokenQueue``."""
        return self.tqueue.max_queued_tokens
# ---
def bitwise_and(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_and](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_and.html)
    """
    return jnp.bitwise_and(x1, x2)
# ---
def test_list_instances_0(self):
        instances = self.conn.list_instances()
        self.assertEquals(instances, [])
# ---
def strip_region_tags(sample_text):
    """Remove blank lines and region tags from sample text"""
    magic_lines = [
        line for line in sample_text.split("\n") if len(line) > 0 and "# [" not in line
    ]
    return "\n".join(magic_lines)
# ---
def the_object_name_should_display_as_mode(name, mode):
    assert the_object_name_exists(name).display_type == mode
# ---
def norm_grads(g):
            return g / (jnp.linalg.norm(g) + 1e-16)
# ---
def __init__(self, leader: _LmEvalHarnessWorker):
        super().__init__()
        self.leader = leader
        self.axis_resources = leader.axis_resources
        # Storage for prompts and generations to include in outputs
        self.sample_outputs: dict[str, list[dict]] = {}
        self.sample_logging_config = leader.sample_logging_config
        self.profiler_config = leader.profiler_config
        self._current_step = 0
        self._profiler_started = False
# ---
def nanmax(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.nanmax, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def opendb(self):
        return smadata2.db.mock.MockDatabase()
# ---
def __init__(self,**kwargs):
        self.register_event_type('on_answer')
        super(ConfirmPopup,self).__init__(**kwargs)
# ---
def to_path(val):
  return val
# ---
def get(self):
            self.response.write('CSRF Token:%s' % self.csrf_token)
# ---
def test_force_conn_events_false(self):
        canary = Mock()
        e1 = create_engine(config.db_url)
        assert not e1._has_events

        event.listen(e1, "before_execute", canary.be1)

        conn = e1._connection_cls(e1, connection=e1.raw_connection(),
                            _has_events=False)

        conn.execute(select([1]))

        eq_(canary.be1.call_count, 0)

        conn._branch().execute(select([1]))
        eq_(canary.be1.call_count, 0)
# ---
def test_clone_volume(self):
        drv = self._driver
        mox = self._prepare_clone_mock('pass')

        mox.ReplayAll()

        volume_name = 'volume_name'
        clone_name = 'clone_name'
        volume_id = volume_name + str(hash(volume_name))
        share = 'ip:/share'

        drv._clone_volume(volume_name, clone_name, volume_id, share)

        mox.VerifyAll()
# ---
def stop(self):
        """Stop the training worker."""
        self._should_stop = True
        self.transfer_server.cleanup()
# ---
def full_like(a: NamedArray, fill_value: T, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to `fill_value`"""
    return NamedArray(jnp.full_like(a.array, fill_value, dtype=dtype), a.axes)
# ---
def mock_scheduler():
    """Create a mock scheduler with wake() method."""
    return MockSchedulerWake()
# ---
def test_score_candidate_empty_tests():
    candidate = _make_candidate("x = 1\n")
    result = score_candidate(candidate, [])
    assert result.tests_passed == 0
    assert result.tests_total == 0
    assert result.test_pass_rate == 0.0
# ---
def _failed_job():
    raise RuntimeError("Intentional failure for screenshot demo")
# ---
def copy(self, cr, uid, id, default=None, context=None):
        context = context or {}
        default = default and default.copy() or {}
        default.update(date=time.strftime('%Y-%m-%d %H:%M:%S'), move_ids=[])
        return super(stock_production_lot, self).copy(cr, uid, id, default=default, context=context)
# ---
def isfinite(a: A) -> A:
    return wrap_elemwise_unary(jnp.isfinite, a)
# ---
def test_repr(self):
        expr = col("score") > 50
        assert repr(expr) == "(col('score') > lit(50))"
# ---
def append(self, item: T):
        if self.is_complete:
            raise ValueError("Cannot append to a finalized dataset")
        self.data.append(item)
        asyncio.create_task(self.notify_length_update())
# ---
def get_supported_boot_devices(self, task):
        """Get a list of the supported boot devices.

        :param task: a task from TaskManager.
        :returns: A list with the supported boot devices defined
                  in :mod:`ironic.common.boot_devices`.

        """
        return [boot_devices.PXE, boot_devices.DISK, boot_devices.CDROM,
                boot_devices.BIOS, boot_devices.SAFE]
# ---
def monitor(self, job_id: JobId) -> JobInfo:
        """Stream logs from a running job, blocking until completion.

        Logs are emitted directly via the logger. Blocks until the job
        completes or is terminated.

        Args:
            job_id: Job identifier returned by launch()

        Returns:
            JobInfo with final job status

        Raises:
            KeyError: If job_id is not found
        """
        ...
# ---
def with_tpu(tpu_type: str, slice_count: int = 1, **kwargs) -> ResourceConfig:
        device = TpuConfig(variant=tpu_type)
        return ResourceConfig(device=device, replicas=slice_count, **kwargs)
# ---
def post(self):
        self.handle_request('POST')
# ---
def test_device_flops(device_type, dtype, flops):
    assert device_flops(device_type, dtype) == flops
# ---
def _event_generator(handle):
    while True:
        event = _mpv_wait_event(handle, -1).contents
        if event.event_id.value == MpvEventID.NONE:
            raise StopIteration()
        yield event
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        return subprocess.run(self._build_cmd(command), capture_output=True, text=True, timeout=timeout.to_seconds())
# ---
def InsertRecord(x, y):
	student_phoneNumber_name[x] = y
	return;
# ---
def reraise(self):
        raise self.exc.with_traceback(self.tb.as_traceback())
# ---
def update_replicas(self, replicas):
        ''' update replicas value '''
        self.put(DeploymentConfig.replicas_path, replicas)
# ---
def test_fdfuncarg_skips_on_no_osdup(testdir):
    testdir.makepyfile("""
        import os
        if hasattr(os, 'dup'):
            del os.dup
        def test_hello(capfd):
            pass
    """)
    result = testdir.runpytest_subprocess("--capture=no")
    result.stdout.fnmatch_lines([
        "*1 skipped*"
    ])
# ---
def evaluate(self, record: dict) -> bool:
        return not self.child.evaluate(record)
# ---
def _matches_target(entry: dict) -> bool:
        if entry.get("dataset") not in {None, dataset_name}:
            return False
        if config_name and entry.get("config") != config_name:
            return False
        return True
# ---
def remove(self, container_id: str) -> None:
        result = subprocess.run(
            ["docker", "rm", "-f", container_id],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode != 0:
            raise RuntimeError(f"Failed to remove container: {result.stderr}")
# ---
def grpc_channel(self) -> aio.Channel:
        """Create the channel designed to connect to this service.

        This property caches on the instance; repeated calls return
        the same channel.
        """
        # Return the channel from cache.
        return self._grpc_channel
# ---
def test_sample_edit_with_validation_valid(tokenizer):
    source = "x = 1 + 2\n"
    replacement_tokens = tokenizer.encode_source("3 * 4")

    mutation = sample_edit_with_validation(
        source=source,
        edit_position=4,
        original_span_end=9,
        replacement_tokens=replacement_tokens,
        tokenizer=tokenizer,
    )
    assert mutation is not None
    assert mutation.apply(source) == "x = 3 * 4\n"
# ---
def scale_group_config() -> config_pb2.ScaleGroupConfig:
    """A standard scale group configuration."""
    return config_pb2.ScaleGroupConfig(
        name="test-group",
        min_slices=0,
        max_slices=5,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def test_set_mesh_defaults_to_explicit_axis_types():
    mesh = _build_explicit_mesh()
    with set_mesh(mesh):
        abstract = get_abstract_mesh()
        assert abstract is not None
        assert abstract.axis_types == (AxisType.Explicit,)
# ---
def receive_tally():
    '''
    This is a test route to be able to test that callbacks are correctly sent
    '''
    print("ATTENTION received tally callback: ")
    print(request.get_json(force=True, silent=True))
    return make_response("", 202)
# ---
def isLeft(a, b, c):
    return 1 if ((b[0] - a[0])*(c[1] - a[1]) - (b[1] - a[1])*(c[0] - a[0])) > 0 else -1;
# ---
def __init__(self, rng: random.Random, swap_prob: float = 0.3):
        self.rng = rng
        self.swap_prob = swap_prob
        self._changed = False
# ---
def set_icon(self, *images):
        """Set the window icon.

        If multiple images are provided, one with an appropriate size
        will be selected (if the correct size is not provided, the image
        will be scaled).

        Useful sizes to provide are 16x16, 32x32, 64x64 (Mac only) and
        128x128 (Mac only).

        :Parameters:
            `images` : sequence of `pyglet.image.AbstractImage`
                List of images to use for the window icon.

        """
        pass
# ---
def copy(self):
        return DSN(self.get_url())
# ---
def model_type(cls) -> Type[LmT]:
        pass
# ---
def validate(self, value, model_instance):
		invalid_values = []
		for val in value:
			try:
				validate_slug(val)
			except ValidationError:
				invalid_values.append(val)

		if invalid_values:
			# should really make a custom message.
			raise ValidationError(self.error_messages['invalid_choice'] % invalid_values)
# ---
def is_inexact_arrayish(x):
    """
    Similar to [equinox.is_inexact_array][] but works on anything that has a shape and dtype
    and the dtype is inexact.

    Specifically, we want to work with [jax.ShapeDtypeStruct][]s, which are not arrays.
    """
    if hasattr(x, "shape") and hasattr(x, "dtype"):
        return jnp.issubdtype(x.dtype, jnp.inexact)
    else:
        return False
# ---
def list_tasks(self, request: cluster__pb2.Controller.ListTasksRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListTasksResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def init_app(self, app):
        app.config.setdefault('ELASTICSEARCH_URL', 'http://localhost:9200/')
        app.config.setdefault('ELASTICSEARCH_INDEX', 'eve')

        self.index = app.config['ELASTICSEARCH_INDEX']
        self.es = get_es(app.config['ELASTICSEARCH_URL'])

        self.create_index(self.index)
        self.put_mapping(app)
# ---
def test_byte_length_of_token_multi():
    tok = load_tokenizer("NousResearch/Llama-2-7b-hf")
    multi_checks = [
        "ðŸ‘ä½ å¥½",
    ]

    for expr in multi_checks:
        # stupid llama adds a prefix space
        token_ids = tok.encode(expr, add_special_tokens=False)[1:]
        total_length = sum(byte_length_of_token(tok, token_id) for token_id in token_ids)
        assert total_length == len(expr.encode("utf-8"))
# ---
def test_bound_limit_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table])
            .order_by(table.c.id)
            .limit(bindparam("l"))
            .offset(bindparam("o")),
            [(2, 2, 3), (3, 3, 4)],
            params={"l": 2, "o": 1},
        )
# ---
def matchsubinclude(f):
            for prefix, matcherargs in subincludes:
                if f.startswith(prefix):
                    mf = submatchers.get(prefix)
                    if mf is None:
                        mf = match(*matcherargs)
                        submatchers[prefix] = mf

                    if mf(f[len(prefix) :]):
                        return True
            return False
# ---
def _product_all_get(self, cr, uid, id, product_ids=False, context=None, states=None):
        if states is None:
            states = ['done']
        # build the list of ids of children of the location given by id
        ids = id and [id] or []
        location_ids = self.search(cr, uid, [('location_id', 'child_of', ids)])
        return self._product_get_multi_location(cr, uid, location_ids, product_ids, context, states)
# ---
def from_state_dict(self: M, state_dict: StateDict, prefix: str | None = None) -> M:
        # this method needs to "vectorize" the blocks, so that we have a single block h.FOO
        # first just do the normal thing with our own dict, which we'll post-process
        stacked = _stack_state_dict(state_dict, prefix=prefix)
        out = super().from_state_dict(stacked, prefix=prefix)  # type: ignore
        return out
# ---
def arctan2(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.arctan2](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.arctan2.html)
    """
    return jnp.arctan2(x1, x2)
# ---
def start_side_effect(container_id):
        call_count[0] += 1
        if call_count[0] == 1:
            raise RuntimeError("failed to bind host port: address already in use")
        return None
# ---
def do_block(carry, block, *args, **kwargs):
            carry, out = block(carry, *args, **kwargs)
            return carry, out
# ---
def testImportConflictingPackage(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        import time
        from "__go__/time" import Now""")))
# ---
def is_even(item):
            return (item % 2) == 0
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)
        return x
# ---
def analyse(self):
        return globals()["Occurrence_"+self.name[1:]](self)
# ---
def num_new_tokens(self) -> jnp.ndarray:
        return self.cu_q_lens["seq", self.num_seqs].scalar()
# ---
def contains(self, bbox, srs):
        return any(c.contains(bbox, srs) for c in self.coverages)
# ---
def is_ancestor_of(self, other: "JobName", *, include_self: bool = True) -> bool:
        """True if this job name is an ancestor of another job name."""
        if include_self and self == other:
            return True
        if len(self._parts) >= len(other._parts):
            return False
        return other._parts[: len(self._parts)] == self._parts
# ---
def close(self):
        """Close any files linked to this dataset
        """
        if self._file_obj is not None:
            self._file_obj.close()
        self._file_obj = None
# ---
def __contains__(self, item):
        try:
            self._queue.index(item)
            return True
        except Exception:
            return False
# ---
def get_output_shape_for(self, input_shape):
        length = conv_output_length(input_shape[1], self.pool_length, self.border_mode, self.stride)
        return (input_shape[0], length, input_shape[2])
# ---
def cbrt(a: A) -> A:
    return wrap_elemwise_unary(jnp.cbrt, a)
# ---
def finalize_loss():
        lse_ref[...] = (jnp.log(l_scratch_ref[...]) + m_scratch_ref[...]).astype(lse_ref.dtype)
        label_logits_ref[...] = label_logits_scratch_ref[...].astype(label_logits_ref.dtype)
# ---
def discover_new(self) -> list[ActorHandle]:
        """Return handles not yet yielded. After wait_ready, returns empty."""
        if self._yielded:
            return []
        self._yielded = True
        return self._handles
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: Optional[AttentionMask | NamedArray] = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        x = self.embeddings.embed(input_ids)
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def gen_keys(self):
        '''
        Generate minion RSA public keypair
        '''
        salt.crypt.gen_keys(
                self.opts['gen_keys_dir'],
                self.opts['gen_keys'],
                self.opts['keysize'])
        return
# ---
def avatarExitNextState(self):
        if len(self.nearbyAvatars) == 0:
            if self.fsm.getCurrentState().getName() != 'Walk':
                self.fsm.request('Lonely')
# ---
def print_list_field(label, values):
    print_field(label, ('-' if len(values) == 0 else DELIMITER(', ').join(values)))
# ---
def unique(
    array: NamedArray, Unique: Axis, *, axis: AxisSelector | None = None, fill_value: ArrayLike | None = None
) -> NamedArray: ...
# ---
def abs(self, f):
        return self._matcher.abs(self._path + "/" + f)
# ---
def shard_names(self) -> Sequence[str]:
        return self._shard_names
# ---
def _recover_preserve_chars(text: str, replacements: str) -> str:
    replacement_iterator = iter(replacements)
    return "".join(
        next(replacement_iterator) if c == PRESERVE_CHAR else c for c in text
    )
# ---
def has_type(self, node_type: str) -> bool:
        """Check whether the bank has entries for the given node type."""
        entries = self.entries.get(node_type)
        return entries is not None and len(entries) > 0
# ---
def clear(self):
		"""
		Delete every row in the file
		"""
		for k in self.keys(): # Use key, otherwise we get RuntimeError: dictionary changed size during iteration
			del self[k]
# ---
def use_docker(request):
    return request.config.getoption("--use-docker")
# ---
def getFirstGrid(self):
        """
        Implements function to get the first grid.

        :return: the grid.
        """
        li = []
        with open(self.fichier, 'r') as fi:
            for line in fi.readlines():
                li.append(line)
        return li
# ---
def char_offset_to_token_index(
        self,
        source: str,
        char_offset: int,
    ) -> int:
        """Convert a character offset in source to a token index.

        For this byte-level tokenizer, the mapping is 1:1 -- character i
        maps to token i. For subword tokenizers, this would need a mapping
        table.
        """
        return min(char_offset, len(source) - 1)
# ---
def wait_for_result(self, timeout=None):
        """Wait for worker completion, raise an error if it failed."""
        try:
            status, error = self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return WaitResult.TIMEOUT

        if status == "error":
            raise RuntimeError(f"{self.__class__.__name__} failed") from error

        return WaitResult.SUCCESS
# ---
def update(self, other):
		"""
		Update file from iterable other
		"""
		for k in other:
			self[k] = other[k]
# ---
def testAssignTuple(self):
    self.assertEqual((0, 'a b\n'), _GrumpRun(textwrap.dedent("""\
        baz = ('a', 'b')
        foo, bar = baz
        print foo, bar""")))
# ---
def _create(self):
        raise NotImplementedError('abstract')
# ---
def _serialize_json_and_commit(path: str, obj):
    fs: AbstractFileSystem = fsspec.core.url_to_fs(path)[0]
    fs.mkdirs(os.path.dirname(path), exist_ok=True)
    if fs.exists(path):
        fs.copy(path, f"{path}.bak")

    for _ in range(10):
        try:
            with fsspec.open(path, "w") as file:
                file.write(obj.to_json())
            break
        except FileNotFoundError:
            logger.exception(f"Failed to write {path}")
# ---
def selected_pane(self):
        return self.panes[self.selected_pane_index] if self.panes else None
# ---
def reload(self) -> "JaggedArrayStore":
        offsets = ts.open(_unshaped_spec(self.offsets))
        data = ts.open(_unshaped_spec(self.data))
        shapes = None if self.shapes is None else ts.open(_unshaped_spec(self.shapes.spec())).result()

        offsets = offsets.result()
        data = data.result()

        return JaggedArrayStore(offsets, data, shapes, self.item_rank)
# ---
def is_leaf(x):
        return isinstance(x, list)
# ---
def matchfn(self, f):
        # Some information is lost in the superclass's constructor, so we
        # can not accurately create the matching function for the subdirectory
        # from the inputs. Instead, we override matchfn() and visitdir() to
        # call the original matcher with the subdirectory path prepended.
        return self._matcher.matchfn(self._path + "/" + f)
# ---
def named_array_to_tensor(named_array):
        return torch.from_numpy(np.array(named_array.array))
# ---
def start_date(self):
        """ Returns a date object of the todo's start date. """
        return self.get_date(config().tag_start())
# ---
def test_axis_names_static_exclusive():
    with pytest.raises(ValueError):
        hax.field(static=True, axis_names=("x",))
# ---
def __call__(self, carry: Carry, *args: Args.args, **kwargs: Args.kwargs) -> tuple[Carry, Y]: ...
# ---
def test_is_position_token_false_for_specials(tok):
    assert not tok.is_position_token(tok.pad_token_id)
    assert not tok.is_position_token(tok.sos_token_id)
    assert not tok.is_position_token(tok.eos_token_id)
# ---
def get_session_list(self):
        return url_xpath('https://www.revisor.mn.gov/revisor/pages/'
                         'search_status/status_search.php?body=House',
                         '//select[@name="session"]/option/text()')
# ---
def visit_msup(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            sup = self._visit(children[1])
            return BracedNode(f"{{{base}}}^{{{sup}}}")
        return TextNode("")
# ---
def get_queryset(self):
        queryset = Ticket.get_user_related_tickets(self.request.user)
        return queryset
# ---
def reject_all(self, include_accepted=False):
        '''
        Reject all keys

        :param bool include_accepted: Whether or not to accept a matched key that was formerly accepted
        '''
        self.reject('*', include_accepted=include_accepted)
# ---
def test_valid_position_mask_simple_assignment(tok):
    source = "x = 1 + 2\n"
    mask = tok.valid_position_mask(source)
    assert any(mask)
# ---
def prod(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.prod, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype)
# ---
def _as_tuple(si):
            if si is None:
                return None
            if isinstance(si, tuple):
                return si
            else:
                return (si, si)
# ---
def test_not_equal(self):
        expr = col("score") != 100
        assert expr.evaluate({"score": 100}) is False
        assert expr.evaluate({"score": 50}) is True
# ---

def is_bored(S):
    """
    You'll be given a string of words, and your task is to count the number
    of boredoms. A boredom is a sentence that starts with the word "I".
    Sentences are delimited by '.', '?' or '!'.

    For example:
    >>> is_bored("Hello world")
    0
    >>> is_bored("The sky is blue. The sun is shining. I love this weather")
    1
    """
    import re
    sentences = re.split(r'[.?!]\s*', S)
    return sum(sentence[0:2] == 'I ' for sentence in sentences)
# ---
def get_spendable_output(self):
        self.log.debug("getting spendable output %s" % self.spendable_outputs[0].vtx[0])
        return self.spendable_outputs.pop(0).vtx[0]
# ---
def before_execute(conn, clauseelement, multiparams, params):
            assert isinstance(multiparams, (list, tuple))
            assert isinstance(params, dict)
# ---
def __init__(self,occurrence):
        self.occurrence = occurrence
        self.number_of_arguments = 0
        if self.occurrence[1][1] == "[]":
            self.number_of_arguments = self.occurrence[1][0]
        self.name = self.occurrence[0][0]#[0]
        self.definition = self.occurrence[-1][0]
# ---
def list_slices(self) -> list[FakeVmGroup]:
        """List all VM groups."""
        with self._lock:
            return list(self._slices.values())
# ---
def get_widget_id(self, prefix, name, key=''):
        if self.instance:
            opts = self.instance._meta
            widget_id = '%s-%s-%s_%s-%s' % (prefix, name, opts.app_label, opts.module_name, self.instance.pk)
        else:
            widget_id = '%s-%s' % (prefix, name)
        if key:
            widget_id = '%s_%s' % (widget_id, slugify(key))
        return widget_id
# ---
def presentToolTipsChecked(self, widget):
        """Signal handler for the "toggled" signal for the
           presentToolTipsCheckButton GtkCheckButton widget.
           The user has [un]checked the 'Present ToolTips'
           checkbox. Set the 'presentToolTips'
           preference to the new value if the user can present tooltips.

        Arguments:
        - widget: the component that generated the signal.
        """

        self.prefsDict["presentToolTips"] = widget.get_active()
# ---
def shuffle_ds(ds, key):
            if self.shuffle is True:
                ds = ds.shuffle(key)
            elif isinstance(self.shuffle, int):
                ds = ds.era_shuffle(self.shuffle, key=key)

            return ds
# ---
def test_always_escaped():
    """Tests characters that are always escaped."""
    test_cases = [
        ("[brackets]", r"\[brackets\]"),
        (r"Backslash " "\\", r"Backslash " "\\"),
        ("`code block`", r"\`code block\`"),
    ]

    for text, expected in test_cases:
        assert minimal_markdown_escape(text) == expected
# ---
def __init__(self, ray_method: Any):
        self._ray_method = ray_method
# ---
def testFor(self):
    self.assertEqual((0, '1\n2\n3\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1, 2, 3):
          print i""")))
# ---
def grilleEmpty(self):
        """
        Implement function to check if the grid is empty.

        """
        for line in self.grille:
            for car in line[:len(line) - 1]:
                if car != '0':
                    return False
        return True
# ---
def characterization_insertion_CRISPR(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'method': 'CRISPR',
        'modified_site_nonspecific': 'random',
        'introduced_elements': 'synthesized DNA'
    }
# ---
def corofunc2():
            called.append(corofunc2)
# ---
def device_mesh(self) -> Mesh:
        ici, dcn = self.mesh.axis_shapes(jax.device_count(), self.num_slices)
        axis_types = None
        if self.use_explicit_mesh_axes:
            axis_names = list(ici.keys()) + [k for k in dcn.keys() if k not in ici]
            axis_types = tuple(AxisType.Explicit for _ in axis_names)
        return create_mesh_from_axis_specs(ici_axes=ici, dcn_axes=dcn, axis_types=axis_types)
# ---
def updateColumns(self, database, additionalStatement: str = ""):

        if not self.dbFileExists:
            database.createTable(self.tableName, self.columnsDict, additionalStatement)
        else:
            try:
                database.deleteTable(self.tableName)
            except:
                database.createTable(self.tableName, self.columnsDict, additionalStatement)
# ---
def match(self, item):
        return all(q.match(item) for q in self.subqueries)
# ---
def tok(self):
        t = self.token['type']

        # sepcial cases
        if t.endswith('_start'):
            t = t[:-6]

        return getattr(self, 'output_%s' % t)()
# ---
def test_beam_search_respects_beam_size(params, model_cfg, tokenizer):
    """Should not return more candidates than beam_size."""
    beam_size = 3
    results = beam_search(
        params=params,
        initial_programs=["x = 1\n", "y = 2\n", "z = 3\n", "w = 4\n"],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(2),
        beam_size=beam_size,
        expansions_per_beam=2,
        max_depth=2,
    )
    assert len(results) <= beam_size
# ---
def get_values(self, min_value, max_value, step=1):
        decimal_step = Decimal(str(step))
        value = Decimal(str(min_value))
        while value <= max_value:
            yield value
            value += decimal_step
# ---
def state(self):
        """Return the state of the sensor."""
        return self._state
# ---
def parse_text(self, m):
        text = m.group(0)
        self.tokens.append({'type': 'text', 'text': text})
# ---
def test_1dim_distance():
    """See if this contraption works in 1 dimension"""
    num1 = random.random()
    num2 = random.random()
    assert kmeans.ndim_euclidean_distance(num1, num2) == abs(num1-num2)
# ---
def test_update_demand_tracks_peak(self, unbounded_config: config_pb2.ScaleGroupConfig):
        """update_demand() tracks peak demand."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(unbounded_config, manager)

        group.update_demand(5)
        group.update_demand(10)
        group.update_demand(3)

        assert group.current_demand == 3
        assert group.peak_demand == 10
# ---
def test_dropout_supports_masking(self):
    dropout = keras.layers.Dropout(0.5)
    self.assertEqual(True, dropout.supports_masking)
# ---
def on_close():
            """The user attempted to close the window.

            This event can be triggered by clicking on the "X" control box in
            the window title bar, or by some other platform-dependent manner.

            The default handler sets `has_exit` to ``True``.  In pyglet 1.1, if
            `pyglet.app.event_loop` is being used, `close` is also called,
            closing the window immediately.

            :event:
            """
# ---
def read_cfg(path_to_config_file, profile_name):
    cfg = read(path_to_config_file, loader=yaml.full_load)
    if profile_name is not None:
        cfg["profile"] = profile_name
    elif "AWS_PROFILE" in os.environ:
        cfg["profile"] = os.environ["AWS_PROFILE"]
    return cfg
# ---
def convert_to_export(self, value, env):
        if env.context.get('export_raw_data'):
            return value
        return ustr(value)
# ---
def getresponse(self):
        return MockHTTPResponse(MockFile(), {}, 200, "OK")
# ---
def editor_popup(title, content, answerCallback):
    content = EditorPopup(content=content)
    content.bind(on_answer=answerCallback)
    popup = Popup(title=title,
                    content=content,
                    size_hint=(0.7, 0.8),
                    auto_dismiss= False,
                  title_size=sp(18))
    popup.open()
    return popup
# ---
def __ror__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_or(other, self)
# ---
def standardchar():
        return redirect(
            url_for("chargen", a=3, b=5, c=7, abia=5, abib=9, abic=13, shuffle=1)
        )
# ---
def __init__(self, x=0.0, y=0.0, z=0.0):
        self.x = x
        self.y = y
        self.z = z
# ---
def request_context():
    """Create a mock RequestContext for RPC calls."""
    return Mock(spec=RequestContext)
# ---
def _load_data(self, directory):
        try:
            with open(directory, 'r') as f:
                data = f.read()
                return literal_eval(data)
        except OSError as e:
            raise
# ---
def _get_member(self):
        if not self.team_member:
            self.team_member = self.startupteammember_set.last()
        return self.team_member
# ---
def test_unicode_and_str_mixture(self):
        f = capture.CaptureIO()
        if sys.version_info >= (3, 0):
            f.write("\u00f6")
            pytest.raises(TypeError, "f.write(bytes('hello', 'UTF-8'))")
        else:
            f.write(unicode("\u00f6", 'UTF-8'))
            f.write("hello")  # bytes
            s = f.getvalue()
            f.close()
            assert isinstance(s, unicode)
# ---
def list_commands(self, _ctx: click.Context) -> list[str]:
        svc = get_service(self.service_name)
        if not svc:
            return []
        self.available_methods = svc.methods
        return [to_kebab_case(m) for m in sorted(svc.methods.keys())]
# ---
def disable_wandb(monkeypatch):
    """Disable WANDB logging during tests."""
    monkeypatch.setenv("WANDB_MODE", "disabled")
# ---
def _get_random_inputs(config: MixtralConfig, override_Pos=None):
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()
    return x, mask
# ---
def get_latest_weight_id(self) -> int | None:
        """Get the latest weight ID."""
        with self._lock:
            return self._latest_weight_id
# ---
def synthetic_target_type(self, target):
    return GoThriftGenLibrary
# ---
def setUp(self):
        self.get = Request("http://www.python.org/~jeremy/")
        self.post = Request("http://www.python.org/~jeremy/",
                            "data",
                            headers={"X-Test": "test"})
# ---
def _as_spec(source: str | InputFileSpec) -> InputFileSpec:
    """Normalize source to InputFileSpec for consistent downstream handling."""
    if isinstance(source, InputFileSpec):
        return source
    return InputFileSpec(path=source)
# ---
def __init__(self, p_str):
        TodoBase.__init__(self, p_str)
        self.attributes = {}
# ---
def test_namedarray_runtime_check_with_dtype():
    Batch = Axis("batch", 2)
    arr = NamedArray(jnp.zeros((Batch.size,), dtype=jnp.float32), (Batch,))
    assert arr.matches_axes(f32["batch"])  # type: ignore
    assert not arr.matches_axes(i32["batch"])
# ---
def get_description(self, env):
        """ Return a dictionary that describes the field ``self``. """
        desc = {'type': self.type}
        for attr, prop in self.description_attrs:
            value = getattr(self, prop)
            if callable(value):
                value = value(env)
            if value is not None:
                desc[attr] = value

        return desc
# ---
def to_hf_config(self) -> tuple[float, dict | None]:
        if self.factor == 1.0:
            return self.theta, None
        return self.theta, {"factor": self.factor}
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.describe()
# ---
def key():
    return jax.random.PRNGKey(0)
# ---
def startup_status_names(self):
        if self._get_startup():
            return [startup_status.program_startup_status.startup_status
                    for startup_status in self.startup.startupstatus_set.all()]
# ---
def deeplift_tensor_grad(grad):
    return_grad = complex_module_gradients[-1]
    del complex_module_gradients[-1]
    return return_grad
# ---
def build_translations(srcdir, blddir, langs):
    for lang in langs:
        outdir = os.path.join(blddir, lang)
        os.makedirs(outdir, exist_ok=True)
        subprocess.call([
            'msgfmt', os.path.join(srcdir, lang, lang + '.po'),
            '-o', os.path.join(outdir, lang + '.gmo')
        ])
# ---
def model_type(self) -> Type["WhisperModel"]:
        return WhisperModel
# ---
def _connect_to_textboxes(self, unitview, textboxes):
        for target in textboxes:
                self.autocomp.add_widget(target)
# ---
def start(self) -> None: ...
# ---
def __floordiv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.floor_divide(self, other)
# ---
def f(s):
        spc = player.get_action_space()
        act = func([[s]])[0][0].argmax()
        if random.random() < 0.001:
            act = spc.sample()
        if verbose:
            print(act)
        return act
# ---
def print_nofill_field(label, value):
    sys.stdout.write(label + DELIMITER(" " * (FIELD_NAME_WIDTH - len(label))) + value + '\n')
# ---
def __init__(self, models=None):
        self.models = models or []
# ---
def test_wait_all_all_succeed(client: LocalClient):
    handles = [client.submit(JobRequest(name=f"ok-{i}", entrypoint=Entrypoint.from_callable(_noop))) for i in range(3)]
    statuses = wait_all(handles)
    assert all(s == JobStatus.SUCCEEDED for s in statuses)
# ---
def loss_recur(q_arr):
        qn = hax.named(q_arr, q.axes)
        out, _ = recurrent_gated_delta_rule(qn, k, v, g, beta, output_final_state=False)
        return jnp.sum(out.array)
# ---
def install_package(self):
        sudo('pkg_add libevent')
        with cd('/tmp'):
            run('wget %s' %self.pgbouncer_src)
            sudo('pkg_add %s' %self.pkg_name)
# ---
def train_shards(self):
    raise NotImplementedError()
# ---
def __init__(self):
        self.beforeToRtl = []
        self.beforeToRtlImpl = []
        self.afterToRtlImpl = []

        self.beforeHdlArchGeneration = [
            extract_part_drivers,
            removeUnconnectedSignals,
            markVisibilityOfSignalsAndCheckDrivers,
        ]
        self.afterToRtl = []
# ---
def fold_via(
        self, fn: Callable[..., CarryT], *, unroll: int | bool | None = None
    ) -> Callable[Concatenate[CarryT, P], CarryT]: ...
# ---
def folio(self):
        return self.__folio
# ---
def _format_static_arrays(self):
        return [f"  Static array at field {field}" for field in self.static_arrays]
# ---
def fake_add_to_aggregate(context, aggregate, host):
            fake_add_to_aggregate.called = True
# ---
def bmarks():
    return_data = get_tags()
    return return_data
# ---
def discover_vm_groups(self) -> list[VmGroupProtocol]:
        """Return empty list - no recovery for local demo."""
        return []
# ---
def __init__(self, iterable=None):
        self._data = {}
        if iterable is not None:
            self.update(iterable)
# ---
def getResiduePositions(residue, positions):
  """ Returns array w. atomic positions of residue """
  ndx = atomIndexInResidue(residue)
  return np.array(positions)[ndx]
# ---
def _to_np(x):
    return np.array(x.detach().cpu().numpy())
# ---
def test_impl(df):
            B = df.A.str.split(',')
            return B.str.get(1)
# ---
def controller(self) -> ControllerProtocol:
        """Access the underlying controller (must call start() first)."""
        if self._controller is None:
            raise RuntimeError("ClusterManager.start() not called")
        return self._controller
# ---
def l3(*arg, **kw):
            canary.append("l3")
# ---
def __init__(self, monitor, parent=None):
        """
        Observe the given ``monitor`` (a :class:`~pyudev.Monitor`):

        ``parent`` is the parent :class:`~PyQt4.QtCore.QObject` of this
        object.  It is passed unchanged to the inherited constructor of
        :class:`~PyQt4.QtCore.QObject`.
        """
        QObject.__init__(self, parent)
        self._setup_notifier(monitor, QSocketNotifier)
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        alpha_p = hnn.softplus(self.alpha_p)
        alpha_n = hnn.softplus(self.alpha_n)
        beta = self.beta
        eps = self.eps
        alpha_n = beta + alpha_n

        positive = alpha_p * x * x + beta * x
        negative = (hax.expm1(hax.minimum(x, eps)) - x) * alpha_n + beta * x
        return hax.where(x > 0, positive, negative)
# ---
def BufferIsVisibleForFilename( filename ):
  """Check if a buffer exists for a specific file."""
  buffer_number = GetBufferNumberForFilename( filename, False )
  return BufferIsVisible( buffer_number )
# ---
def test_dontreadfrominput():
    from _pytest.capture import DontReadFromInput
    f = DontReadFromInput()
    assert not f.isatty()
    pytest.raises(IOError, f.read)
    pytest.raises(IOError, f.readlines)
    pytest.raises(IOError, iter, f)
    pytest.raises(UnsupportedOperation, f.fileno)
    f.close()
# ---
def task_index(self) -> int:
        """0-indexed task number within the job."""
        return self._task_name.require_task()[1]
# ---
def save_data(self):
        try:
            f = open('data/er_nick-data.csv', 'wt')
            writer = csv.writer(f)
            for u in self.data:
                writer.writerow([u, self.data[u]['id'], self.data[u]['nick'], self.data[u]['level'], self.data[u]['strength'], self.data[u]['rank_points'], self.data[u]['citizenship']])
            f.close()
        except:
            pass
# ---
def default_view_class(self):
		"""ã‚¹ã‚ºãƒ¡ãƒãƒã®ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ“ãƒ¥ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã‚’å¿œç­”ã™ã‚‹ã€‚"""
		if TRACE: print(__name__), self.default_view_class.__doc__

		return WaspView
# ---
def __le__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "le")
# ---
def update_config(self) -> Optional[WebConfig]:
        if self.config_arg is None:
            return None
        config, web_config = parse_config(self.config_arg)
        self.cron_jobs = OrderedDict((job.name, job) for job in config)
        return web_config
# ---
def output_code(self):
        return self.renderer.block_code(
            self.token['text'], self.token['lang']
        )
# ---
def test_collect_capturing(testdir):
    p = testdir.makepyfile("""
        print ("collect %s failure" % 13)
        import xyz42123
    """)
    result = testdir.runpytest(p)
    result.stdout.fnmatch_lines([
        "*Captured stdout*",
        "*collect 13 failure*",
    ])
# ---
def body(i, rc):
                page = src_pages["page", i]

                def inc(rc):
                    return rc.at["page", page].add(1)

                return jax.lax.cond(is_valid(page).scalar(), inc, lambda x: x, rc)
# ---
def extra_flags(extra: list[str] | None = None) -> list[str]:
    if not extra:
        extra = []

    accel_type = accelerator_type_from_extra(extra)
    if accel_type == AcceleratorType.NONE:
        extra.append("cpu")

    extra_set = set(extra)

    cmd = []
    for ex in extra_set:
        if ex.strip():
            cmd.append(f"--extra={ex}")
    return cmd
# ---
def write_jsonl_gz():
    """Fixture to write JSONL gzipped files."""

    def _write(path: Path, records: list[dict]) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        with gzip.open(path, "wt", encoding="utf-8") as handle:
            for record in records:
                handle.write(json.dumps(record))
                handle.write("\n")

    return _write
# ---
def test_binary_repr(self):
        """Binary repr should wrap the contained value"""
        self.assertEqual(repr(Binary("a")), "Binary(%r)" % b"a")
# ---
def _run_task(stop_event: threading.Event) -> None:
            attempt.run()
# ---
def _transfer_to_cpu(self, model) -> PyTree:
        """Transfer params to CPU devices."""
        try:
            with hax.set_mesh(self.cpu_mesh):
                cpu_devices = jax.devices("cpu")
                return jax.device_put(model, cpu_devices[0])
        except Exception as e:
            logger.warning(f"Failed to transfer to CPU: {e}, using original params")
            return model
# ---
def submit_job(
    state: ControllerState,
    job_id: str,
    request: cluster_pb2.Controller.LaunchJobRequest,
) -> list[ControllerTask]:
    """Submit a job via event and return tasks."""
    jid = JobName.from_string(job_id) if job_id.startswith("/") else JobName.root(job_id)
    request.name = jid.to_wire()
    state.handle_event(
        JobSubmittedEvent(
            job_id=jid,
            request=request,
            timestamp=Timestamp.now(),
        )
    )
    return state.get_job_tasks(jid)
# ---
def shutdown(self):
        self.llm.shutdown()
# ---
def run_streaming(self, command: str) -> MagicMock:
        return make_fake_popen()
# ---
def gh_pages(c):
    """Publish to GitHub Pages"""
    preview(c)
    c.run('ghp-import -b {github_pages_branch} '
          '-m {commit_message} '
          '{deploy_path} -p'.format(**CONFIG))
# ---
def test_conversion_to_jnp_bfloat16():
    import torch

    x = torch.arange(10, dtype=torch.bfloat16) / 3.14
    with pytest.raises(TypeError):
        x.cpu().numpy()

    x_jnp = _convert_to_jnp(x, None)
    assert x_jnp.dtype == jnp.bfloat16
    assert x_jnp.shape == x.shape
    assert_trees_all_close(x_jnp, jnp.arange(10, dtype=jnp.bfloat16) / 3.14)
# ---
def update_variants(variants, template, publish_progress=True):
	count=0
	for d in variants:
		variant = frappe.get_doc("Item", d)
		copy_attributes_to_variant(template, variant)
		variant.save()
		count+=1
		if publish_progress:
				frappe.publish_progress(count*100/len(variants), title = _("Updating Variants..."))
# ---
def openai_client(self):
            """Return a mock AsyncOpenAI client that returns proper ChatCompletion objects."""
            mock_client = AsyncMock()
            # Configure the mock to return a proper ChatCompletion
            mock_client.chat.completions.create = AsyncMock(return_value=create_mock_chat_completion())
            return mock_client
# ---
def list_status(self, status):
        self._call_all('list_status', status)
# ---
def shutdown(self, wait: bool = True) -> None:
        """Shutdown the client and all managed resources."""
        ...
# ---
def __init__(self, do_open):
        self._do_open = do_open
# ---
def get_actor_name_from_actor_info(self, actor_info: TPUHostInfo) -> str:
        return str(actor_info.worker_index)
# ---
def removeLink( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def test_column_mean(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            return df.A.mean()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
# ---
def testMaxShifted(self):
        # Tests whether a HPCP reading with only the dominant semitone
        # activated is correctly shifted so that the dominant is at the
        # position 0
        tonic = 440
        dominant = tonic * 2**(7./12.)
        hpcp = HPCP(maxShifted=True)([dominant], [1])

        self.assertEqualVector(hpcp, [1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
# ---
def _stop_actor(actor: ActorHandle) -> None:
    try:
        ray.get(actor.teardown.remote(), timeout=TEARDOWN_ACTOR_TIMEOUT)
        ray.get(actor.__ray_terminate__.remote(), timeout=TERMINATE_ACTOR_TIMEOUT)
    except (ActorDiedError, ActorUnavailableError):
        pass
    except GetTimeoutError as e:
        logger.warning(
            f"Failed to gracefully shut down actor in {TERMINATE_ACTOR_TIMEOUT} seconds; killing it instead: {e}"
        )
    finally:
        ray.kill(actor)
# ---
def _signature(self):
    self._create_definition_if_needed()
    return self._op_def
# ---
def destroy(self, context):
        """Delete a the pf from the DB."""
        del self.virtual_function_list[:]
        super(PhysicalFunction, self).destroy(context)
# ---
def test_smoke(cluster):
    _url, client = cluster
    job = submit(client, _quick, "chaos-smoke")
    status = wait(client, job, timeout=30)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def __init__(self, context: JobContext, config: BackendConfig):
        """Initialize backend with execution context and configuration.

        Args:
            context: Execution context providing put/get/run/wait primitives
            config: Backend configuration
        """
        self.context = context
        self.config = config
        self.dry_run = config.dry_run
# ---
def create_op(self, op_type, inputs, data_types, **kwargs):
    for i, x in enumerate(inputs):
      if isinstance(x, ops.EagerTensor) or x.graph is not self:
        inputs[i] = self.capture(x)
    return super(_FuncGraph, self).create_op(op_type, inputs, data_types,
                                             **kwargs)
# ---
def checkpoint_path(self) -> str:
        base = f"{BASE_CHECKPOINT_PREFIX}/{self.run_name}/checkpoints"
        if self.checkpoint_step is None:
            return base
        return f"{base}/step-{self.checkpoint_step}"
# ---
def _assert_result(self, select, result, params=()):
        eq_(config.db.execute(select, params).fetchall(), result)
# ---
def _parse_ports_from_env(env: dict[str, str] | None = None) -> dict[str, int]:
    source = env if env is not None else os.environ
    ports = {}
    for key, value in source.items():
        if key.startswith("IRIS_PORT_"):
            port_name = key[len("IRIS_PORT_") :].lower()
            ports[port_name] = int(value)
    return ports
# ---
def find_volume_by_name(self, volume, mounts=False):
        ''' return the index of a volume '''
        volumes = []
        if mounts:
            volumes = self.get_volume_mounts()
        else:
            volumes = self.get_volumes()
        for exist_volume in volumes:
            if exist_volume['name'] == volume['name']:
                return exist_volume

        return None
# ---
def test_pareto():
    b = hax.arange(Width, start=0.1)

    check_gen_is_equal(
        lambda k, s: jax.random.pareto(k, b.array.reshape(1, -1), shape=s), lambda k, s: hax.random.pareto(k, s, b)
    )
# ---
def create_item(cls, text):
        return MyMusicItem(text)
# ---
def test_get_full_url(self):
        self.assertEqual("http://www.python.org/~jeremy/",
                         self.get.get_full_url())
# ---
def __enter__(self):
        """Start controller with autoscaler (or connect to remote)."""
        if self._remote_url:
            return self

        config = self._load_or_default_config()
        config = make_local_config(config)
        self._manager = ClusterManager(config)
        self._manager.start()
        return self
# ---
def loss_blk_fn(x):
        return fused_linear_softmax_cross_entropy_loss(
            x,
            lm_head,
            labels,
            reduction="mean",
            dtype=jnp.float32,
            precision=jax.lax.Precision.HIGHEST,
        )
# ---
def with_name_like(self, name):
        Util.validate_type(name, "str")
        return self._with_name_like(name)
# ---
def create_integration_run_task_request(bundle_path: str, task_id: str):
    """Create a RunTaskRequest for integration testing."""
    entrypoint = create_integration_entrypoint()

    return cluster_pb2.Worker.RunTaskRequest(
        task_id=task_id,
        num_tasks=1,
        entrypoint=entrypoint.to_proto(),
        bundle_gcs_path=bundle_path,
        environment=cluster_pb2.EnvironmentConfig(),
        resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=512 * 1024**2),
    )
# ---
def parse_block_code(self, m):
        # clean leading whitespace
        code = _block_code_leading_pattern.sub('', m.group(0))
        self.tokens.append({
            'type': 'code',
            'lang': None,
            'text': code,
        })
# ---
def position_token_id(self, pos: int) -> int:
        """Get the token ID for <POS pos>."""
        if pos < 0 or pos >= self.num_position_tokens:
            raise ValueError(f"Position {pos} out of range [0, {self.num_position_tokens})")
        return self.position_token_offset + pos
# ---
def sharded_route(router_probs_):
            selected_weights_, selected_experts_ = jax.lax.top_k(router_probs_, TopExperts.size)
            selected_weights_ = selected_weights_ / selected_weights_.sum(-1, keepdims=True)

            return selected_weights_, selected_experts_
# ---
def write_binary_file(records: Iterable[bytes], output_path: str) -> dict:
    """Write binary records to a file."""
    ensure_parent_dir(output_path)

    count = 0
    with atomic_rename(output_path) as temp_path:
        with fsspec.open(temp_path, "wb", block_size=64 * 1024 * 1024) as f:
            for record in records:
                f.write(record)
                count += 1

    return {"path": output_path, "count": count}
# ---
def load_data(self):
        print(self.datafolder)
        self.samplefile = glob.glob(os.path.join(self.datafolder, "*_SAMPLES.csv"))[0]
        if os.path.isfile(self.samplefile):
            self.samplesdf = pd.read_csv(self.samplefile, encoding='ISO-8859-1')
        else:
            print("File not found: ", self.samplefile)
            self.samplesdf = None

        self.combodefaults = {'cuvette': ['600', '2000', '4000']}
# ---
def height(self):
        """The height of the window, in pixels.  Read-write.

        :type: int
        """
        return self.get_size()[1]
# ---
def batch_completions(
        self,
        prompts: list[str] | list[list[dict]],
        temperature: float,
        n: int,
        max_tokens: int | None = None,
        top_k: int | None = None,
        stop: list[str] | None = None,
        system_prompt: str | None = None,
    ) -> list[ChatCompletion]:
        """Batch completions from the inference server."""
        raise NotImplementedError
# ---
def with_sliding_window(self, sliding_window: int | None) -> "AttentionMask":
        return AttentionMask(
            is_causal=self.is_causal,
            segment_ids=self.segment_ids,
            sliding_window=sliding_window,
        )
# ---
def load_jsonl(self) -> Dataset[dict]:
        """Load records from JSONL files."""
        return Dataset(self.source, [*self.operations, LoadFileOp("jsonl", None)])
# ---
def maybe_untuple(x: Sequence[T] | T) -> T | Sequence[T]:
    """
    If x is a tuple with one element, return that element. Otherwise return x.
    """
    if isinstance(x, tuple) and len(x) == 1:
        return x[0]
    return x
# ---
def _remove_if_possible(path):
    try:
        os.remove(path)
    except OSError:
        pass
# ---
def test_impl(df):
            C = df.B == 'two'
            return C.sum()
# ---
def get_converted_vm(job_id):
    try:
        job = _get_job(job_id)
        _validate_job_done(job)
        ovf = _read_ovf(job_id)
    except ClientError as e:
        logging.info('Converted VM error %s', e)
        return errCode[e.err_name]
    except V2VError as e:
        logging.error('Converted VM error %s', e)
        return errCode[e.err_name]
    return {'status': doneCode, 'ovf': ovf}
# ---
def create_vm_side_effect(**kwargs):
        vm = MagicMock()
        vm.info = vm_pb2.VmInfo(
            vm_id=kwargs["vm_id"],
            slice_id=kwargs["slice_id"],
            scale_group=kwargs["scale_group"],
            zone=kwargs["zone"],
            address=kwargs.get("address", ""),
            state=vm_pb2.VM_STATE_BOOTING,
        )
        return vm
# ---
def __init__(self, milliseconds: int):
        self._ms = milliseconds
# ---
def axis_size(self, axis: AxisSelector) -> int:  # type: ignore
        ...
# ---
def test_py36_windowsconsoleio_workaround_non_standard_streams():
    """
    Ensure _py36_windowsconsoleio_workaround function works with objects that
    do not implement the full ``io``-based stream protocol, for example execnet channels (#2666).
    """
    from _pytest.capture import _py36_windowsconsoleio_workaround

    class DummyStream(object):
        def write(self, s):
            pass

    stream = DummyStream()
    _py36_windowsconsoleio_workaround(stream)
# ---
def test_use_pass_for_iterations_with_no_body(self):
        for num in range(1,5):
            pass

        self.assertEqual(4, num)
# ---
def is_terminal(self) -> bool:
        """True if all VMs are in a terminal state (no further transitions expected)."""
        terminal = {
            vm_pb2.VM_STATE_READY,
            vm_pb2.VM_STATE_FAILED,
            vm_pb2.VM_STATE_TERMINATED,
            vm_pb2.VM_STATE_PREEMPTED,
        }
        return all(v.state in terminal for v in self.vms)
# ---
def distributed_work():
            from iris.cluster.client import get_job_info

            info = get_job_info()
            if info is None:
                raise RuntimeError("Not running in an Iris job context")
            print(f"Task {info.task_index} of {info.num_tasks} on worker {info.worker_id}")
            return f"Task {info.task_index} done"
# ---
def _use_controller_table(self):
        """
            Set the resource and the table to be the imported resource
        """

        self.resource = self.controller_resource
        self.table = self.controller_table
        self.tablename = self.controller_tablename
# ---
def __rxor__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_xor(other, self)
# ---
def test_plain_union(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2)
        s2 = select([table]).where(table.c.id == 3)

        u1 = union(s1, s2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def error_page_404(status, message, traceback, version):
    html = header
    html += "%s<br>" % (status)
    html += "%s" % (traceback)
    html += getfooter()
    return html
# ---
def initialize(self):
        if self.auto_start_cluster:
            auto_ray_cluster(address=self.address, start_workers=self.start_workers)
# ---
def make_post(with_comments=True, with_author=True, with_keywords=True):
    comments = [make_comment() for _ in range(2)] if with_comments else []
    keywords = [make_keyword() for _ in range(3)] if with_keywords else []
    author = make_author() if with_author else None
    return Post(
        id=fake.random_int(),
        title=fake.catch_phrase(),
        author=author,
        author_id=author.id if with_author else None,
        comments=comments,
        keywords=keywords,
    )
# ---
def __init__(self, default, routes=None):
		self._default = default
		if routes is None:
			routes = {}
		self._routes = routes
# ---
def _update(self, records, value):
        """ Update the cached value of ``self`` for ``records`` with ``value``. """
        for record in records:
            if self in record._cache:
                record._cache[self] = record[self.name] | value
            else:
                record._cache[self] = UnionUpdate(self, record, value)
# ---
def _zeros_like(mapping, dtype, n):
    if isinstance(n, hax.NamedArray):
        return hax.shard(hax.zeros_like(n, dtype=dtype), mapping)
    elif is_jax_array_like(n):
        return jnp.zeros_like(n, dtype)
    else:
        assert jnp.isscalar(n)
        if dtype is None:
            # if it's a nan, we want to go to 0
            if n != n:
                return 0
            return n - n
        else:
            return jnp.zeros((), dtype=dtype)
# ---
def foo():
            with profiler.Trace("foo"):
                raise ValueError("bar")
# ---
def append(self, obj: T) -> int:
        index = len(self)
        self._index_to_obj.append(obj)
        self._obj_to_index[obj] = index
        return index
# ---
def _run(stop_event: threading.Event) -> None:
            logger.debug("Running server %s (%s)", name, server)
            server.run()
            logger.debug("Server %s exited", name)
# ---
def match(self, item):
        return True
# ---
def par_heading(self, elem):
        '''Handle a "paragraph heading", i.e., a chaper heading or part
        of a chapter heading inside paragraph tags. If it is immediately
        followed by a heading, they will be merged into one.'''
        self.par_h = elem
# ---
def list_jobs(ctx):
    """List Ray jobs."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        print(json.dumps(_list_jobs(), indent=2))
# ---
def __init__(self, state):
        self._state = state
        self.tx_pool = TransactionPool(None)
        self._last_block = Block.deserialize(GenesisBlock().serialize())
        self.current_difficulty = StringToUInt256(str(config.user.genesis_difficulty))

        self.trigger_miner = False
        self.lock = threading.RLock()
# ---
def __call__(self, x):
                return self.second(self.first(x))
# ---
def __new__(cls, df, lmbda = 0):
        assert df >= 1
        d1 = NormalDistr(sqrt(lmbda))**2
        if df == 1:
            return d1
        d2 = ChiSquareDistr(df - 1)
        ncc2 = super(NoncentralChiSquareDistr, cls).__new__(cls, d1, d2)
        super(NoncentralChiSquareDistr, ncc2).__init__(d1, d2)
        ncc2.df = df
        ncc2.lmbda = lmbda
        return ncc2
# ---
def name(self) -> str:
        """Get the local name (last component)."""
        return self._parts[-1]
# ---
def vm_count(self) -> int:
        """Number of VMs in the TPU pod."""
        return get_tpu_topology(self.variant).vm_count
# ---
def transform(self, x):
            return x + self.w
# ---
def to_proto(self) -> "time_pb2.Timestamp":
        """Convert to proto Timestamp message."""
        return time_pb2.Timestamp(epoch_ms=self._epoch_ms)
# ---
def hasMore(I):
	return (I.peek() != None)
# ---
def user_facebook_url(self):
        return self._get_profile().facebook_url
# ---
def strip(s):
    return s.strip()
# ---
def __init__(self, source: ShardedDataSource[T_co], fn: Callable[[T_co], T]):
        self.source = source
        self.fn = fn
        self._transform = _MapTransform(fn)
# ---
def __hash__(self):
        return hash((self.field, self.ascending))
# ---
def drag_and_drop(self, source, target):
        """Holds down the left mouse button on the source element,
           then moves to the target element and releases the mouse button.
        Args:
            source: The element to mouse down.
            target: The element to mouse up.
        """
        self.click_and_hold(source)
        self.release(target)
        return self
# ---
def test_bank_has_call(bank):
    assert bank.has_type("Call"), "Should extract Call expressions"
# ---
def reset(self):
        self._elapsed = 0.0
# ---
def _make_table(pages=8, seqs=4, page_size=2, pages_per_seq=2):
    return PageTable.init(pages, seqs, page_size, pages_per_seq)
# ---
def disconnect(self):
        if ( self.connection ):
            try:
                self.connection.quit()
            except:
                pass
        self.connection = None
# ---
def remove_all_iris_containers(self) -> int:
        """Force remove all iris-managed containers. Returns count attempted."""
        container_ids = self.list_iris_containers(all_states=True)
        if not container_ids:
            return 0

        subprocess.run(
            ["docker", "rm", "-f", *container_ids],
            capture_output=True,
            check=False,
        )
        return len(container_ids)
# ---
def _task_detail_page(self, request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Task Detail", "/static/worker/task-detail.js"))
# ---
def parse_cookie(self, name):
        if name in self.cookies:
            return self.cookies[name].split("\t")[6]
        else:
            return None
# ---
def uid(self, val):
        self._uid = _EpubMeta('dc:identifier', str(val), ('id', 'uid_id'))
# ---
def _select_gen(stream: Iterator, columns: tuple[str, ...]) -> Iterator:
    cols_set = set(columns)
    for item in stream:
        yield {k: item[k] for k in cols_set if k in item}
# ---
def _run_simple_tpu_job(self, client: IrisClient) -> TestResult:
        """Run a simple TPU job that just prints and returns."""
        return self._run_job_test(
            client=client,
            test_name=f"Simple TPU job ({self.config.tpu_type})",
            entrypoint=Entrypoint.from_callable(_hello_tpu_job),
            job_name=f"smoke-simple-{self._run_id}",
            resources=ResourceSpec(device=tpu_device(self.config.tpu_type)),
        )
# ---
def get_witness_script(witness_root, witness_nonce):
    witness_commitment = uint256_from_str(hash256(ser_uint256(witness_root) + ser_uint256(witness_nonce)))
    output_data = WITNESS_COMMITMENT_HEADER + ser_uint256(witness_commitment)
    return CScript([OP_RETURN, output_data])
# ---
def test_concat_str(self):
        def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1, df2])
            return (A3.two == 'foo').sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def __repr__(self) -> str:
        return f"Deadline(remaining={self.remaining_seconds():.3f}s)"
# ---
def do_eval_lm(config: LevanterEvalLmConfig) -> None:
    """
    Visualizes log probabilities of a language model.

    Args:
        config (EvalLmConfig): The configuration for visualizing log probabilities.
    """
    # Levanter can read `gs://` checkpoints directly via fsspec/tensorstore, and HF
    # checkpoints via fsspec as well. Avoid staging large directories locally.
    eval_lm_main(config)
# ---
def Pos(self) -> Axis:
        pass
# ---
def __str__(self):
        return str(self.__shortname)
# ---
def _extend_path(path: Optional[str], extra: str):
    if path == "memory" or path is None:
        return path
    else:
        return os.path.join(path, extra)
# ---
def __init__(self):
        self._saved_msg = []
# ---
def test_weird_inline_newlines():
    html = """<I>Murray
Montgomery</I><BR>"""
    expected = "*Murray Montgomery*  \n"
    assert to_markdown(html) == expected
# ---
def __init__(self, index):
        self.index = index
# ---
def slow_create(tags=None):
            time.sleep(0.5)  # Simulate slow VM creation
            return original_create(tags)
# ---
def test_suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.makeSuite(TestFuncs))
    return suite
# ---
def _get_value(self, original, split):
        if original:
            value = round(original * split) / split
            return Decimal(str(value))
# ---
def write_corpus(programs: list[str], output_path: Path) -> None:
    """Write programs to a corpus file, separated by '# ---' sentinel lines.

    This separator allows programs to contain internal blank lines,
    unlike the previous blank-line-separated format.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for i, prog in enumerate(programs):
            if i > 0:
                f.write(CORPUS_SEPARATOR + "\n")
            f.write(prog.rstrip("\n"))
            f.write("\n")
# ---
def call(*_, **__):
            """Dummy service call"""
            response = {
                "ResponseMetadata": {
                    "HTTPStatusCode": 400,
                },
                "Error": {
                    "Code": "ProvisionedThroughputExceededException",
                    "Message": "Does not matter",
                },
            }
            raise ClientError(response, "list_tables")
# ---
def __setitem__(self, field, value):
        setattr(self, field, value)
# ---
def test_remove_from_aggregate_called(self):
        def fake_remove_from_aggregate(context, aggregate, host):
            fake_remove_from_aggregate.called = True
        self.stubs.Set(self.conn._pool,
                       "remove_from_aggregate",
                       fake_remove_from_aggregate)

        self.conn.remove_from_aggregate(None, None, None)
        self.assertTrue(fake_remove_from_aggregate.called)
# ---
def _load_puzzle_file(self):
        filePath = "{dir}/{f}".format(dir=os.getcwd(), f=self._file_name)
        try:
            with open(filePath, mode='r') as puzzle_file:
                self._puzzle_input = puzzle_file.readlines()
        except IOError as err:
            errorMsg = (
                "ERROR: Failed to read the puzzle input from file '{file}'\n"
                "{error}"
            )
            print(errorMsg.format(file=self._file_name, error=err))
            exit(1)
# ---
def get_lm_head(self) -> hax.NamedArray:
        assert self.lm_head.bias is None
        return self.lm_head.weight
# ---
def _extract_node_name(resource_name: str) -> str:
    if "/" in resource_name:
        return resource_name.split("/")[-1]
    return resource_name
# ---
def setUp(self):
        self.ll = generate_ll()
        self.pl = self.ll.pl
# ---
def load_workers_from_config(self, configs: list[WorkerConfig]) -> None:
        """Load workers from static configuration."""
        now = Timestamp.now()
        for cfg in configs:
            worker = ControllerWorker(
                worker_id=WorkerId(cfg.worker_id),
                address=cfg.address,
                metadata=cfg.metadata,
                last_heartbeat=now,
            )
            self.add_worker(worker)
# ---
def getExcellon(self):
        return (self._preamble+
                self._content+
                self._postamble)
# ---
def current(self, value):
        self.voltage = self.resistance * value
# ---
def _run_command(*args, **kwargs):
    return subprocess.check_call(args, **kwargs)
# ---
def openai_client(self) -> AsyncOpenAI:
        return AsyncOpenAI(
            base_url=f"http://{self._inference_server.address()}/v1",
            api_key="marin",
        )
# ---
def unregister_event_callback(self, callback):
        self._event_callbacks.remove(callback)
# ---
def __ne__(self, other):  # pragma: no cover
        return haliax.not_equal(self, other)
# ---
def test_raw_metric_vm_network(metrics_collection, appliance, provider):
    vm_name = provider.data['cap_and_util']['capandu_vm']
    query = query_metric_db(appliance, provider, 'net_usage_rate_average',
        vm_name)

    for record in query:
        if record.net_usage_rate_average is not None:
            assert record.net_usage_rate_average > 0, 'Zero VM Network IO'
            break
# ---
def evt():
    return event.build_event("event")
# ---
def test_ics(self):
        """
        Check ICS output
        """
        escala = Escala('fixtures/escala_ics.xml')
        f_result = open(self.dir.get_data_dir() + 'fixtures/escala.ics')
        self.assertEqual(escala.ics(), f_result.read())
        f_result.close()
# ---
def flatten(axes):
            if axes is None:
                return axes
            result = []
            for ax in axes:
                if isinstance(ax, tuple):
                    result += list(ax)
                else:
                    result.append(ax)
            return tuple(result)
# ---
def test_shared(self):
        alice_pub = self.alice.get_public()
        bob_pub = self.bob.get_public()
        alice_shared = self.alice.compute_shared(bob_pub)
        bob_shared = self.bob.compute_shared(alice_pub)
        self.assertEquals(alice_shared, bob_shared)
# ---
def test_stdin_restored(self):
        old = sys.stdin
        with self.getcapture(in_=True):
            newstdin = sys.stdin
        assert newstdin != sys.stdin
        assert sys.stdin is old
# ---
def start_server(self, model: LmHeadModel) -> None:
        with hax.set_mesh(self.mesh), hax.axis_mapping(self.axis_mapping):
            self._inference_server = InferenceServer.create(
                self.inference_server_config,
                model=model,
                tokenizer=self.tokenizer,
            )
        self._inference_thread = threading.Thread(target=lambda: self._inference_server.serve(), daemon=True)
        self._inference_thread.start()
# ---
def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4')]
# ---
def build_shard_bloom(records: Iterator[dict]) -> Iterator[bytes]:
        """Build bloom filter from a shard of records and yield serialized bytes."""
        bf = Bloom(config.estimated_doc_count, config.false_positive_rate)

        for record in records:
            text = record.get(config.text_field, "")
            for feature in extract_features(text, config.ngram):
                bf.add(_bloom_hash(feature))

        yield bf.save_bytes()
# ---
def set_loglevel(self, level):
        _mpv_request_log_messages(self._event_handle, level.encode('utf-8'))
# ---
def logaddexp2(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logaddexp2(x1, x2)
# ---
def init(max_pages: int, max_seqs: int, page_size: int, max_pages_per_seq: int) -> "PageTable":
        ref_counts = hax.full({"page": max_pages}, 0, dtype=jnp.int32)
        return PageTable(ref_counts, page_size, max_seqs, max_pages_per_seq)
# ---
def terminate(self) -> None:
        """Mark VM group as terminated."""
        ts = Timestamp.now().epoch_ms()
        for vm in self._vms:
            vm.info.state = vm_pb2.VM_STATE_TERMINATED
            vm.info.state_changed_at.CopyFrom(time_pb2.Timestamp(epoch_ms=ts))
        self._terminated = True
# ---
def bloom(radius):
    turtle.colormode(255)

    for rad in range(40, 10, -5):
        for looper in range(360//rad):
            turtle.up()
            turtle.circle(radius+rad, rad)
            turtle.begin_fill()
            turtle.fillcolor((200+random.randint(0, rad),
                              200+random.randint(0, rad),
                              200+random.randint(0, rad)))
            turtle.down()
            turtle.circle(-rad)
            turtle.end_fill()
# ---
def replace_axis(axis_spec: AxisSpec, old: AxisSelector, new: AxisSpec) -> AxisSpec: ...
# ---
def test_check_health_returns_unhealthy_on_exception():
    """check_health returns unhealthy result on exception."""
    conn = MagicMock()
    conn.run.side_effect = Exception("Network error")
    result = check_health(conn, port=10001)
    assert result.healthy is False
    assert "Network error" in result.curl_error
# ---
def send_keys_to_element(self, element, *keys_to_send):
        """Sends keys to an element.
        Args:
            element: The element to send keys.
            keys_to_send: The keys to send.
        """
        self._actions.append(lambda:
            element.send_keys(*keys_to_send))
        return self
# ---
def list_nodes() -> list[dict[str, Any]]:
    """Get list of Ray nodes."""
    result = run_ray_command(
        ["ray", "list", "nodes", "--format=json", "--limit=10000"],
    )
    return json.loads(result.stdout)
# ---
def init(Vocab: Axis, config: HackableTransformerConfig, *, key):
        emb = hnn.Embedding.init(Vocab, config.Embed, key=key)
        ln = config.mk_LayerNorm(config.Embed) if config.input_embedding_norm else None
        return HackableEmbedding(emb, ln)
# ---
def __init__(self, tracker: "Tracker"):
        self.tracker = tracker
# ---
def kill_tasks_on_workers(self, task_ids: set[JobName]) -> None:
        """Send KILL RPCs to workers for tasks that were running."""
        ...
# ---
def as_async_dataset(self) -> "AsyncDataset[T_co]":
        return self
# ---
def __getattr__(self, method_name: str) -> "ThreadActorMethod":
        method = getattr(self._instance, method_name)
        return ThreadActorMethod(method, self._lock, self._context)
# ---
def address(self) -> str:
        return self.host
# ---
def subsystem_dependencies(cls):
    return (super(GoThriftGen, cls).subsystem_dependencies() +
            (ThriftDefaults, ThriftBinary.Factory.scoped(cls)))
# ---
def CurrentLineContentsAndCodepointColumn():
  """Returns the line contents as a unicode string and the 0-based current
  column as a codepoint offset. If the current column is outside the line,
  returns the column position at the end of the line."""
  line = CurrentLineContents()
  byte_column = CurrentColumn()
  # ByteOffsetToCodepointOffset expects 1-based offset.
  column = ByteOffsetToCodepointOffset( line, byte_column + 1 ) - 1
  return line, column
# ---
def register(self, vm: ManagedVm) -> None:
        """Register a VM for tracking.

        Called when a VM is created. If a VM with the same ID already exists,
        it will be replaced.
        """
        with self._lock:
            self._vms[vm.info.vm_id] = vm
# ---
def _load_cal_db(self):
        self._cal_db = load_IQMX_calibration_database(self._cal_db_name, 0)
# ---
def test_add_different_tables(self):
        """Cannot add ConsumedCapacity of two different tables"""
        c1 = ConsumedCapacity("foobar", Capacity(1, 28))
        c2 = ConsumedCapacity("boofar", Capacity(3, 0))
        with self.assertRaises(TypeError):
            c1 += c2
# ---
def __init__(self, somedict):
        self._dict = dict(somedict)   # make a copy
        self._hash = None
# ---
def __rsub__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.subtract(other, self)
# ---
def temporal_diff(self, m, s):
        """
        The temporal diffence value for state s to state (s+1) in the mth game
        """
        return (self.get_reward(m, s) + self.eval_func(m, s + 1, self.rt) -
                self.eval_func(m, s, self.rt))
# ---
def update(self: S, model: M, step: int) -> S:
        pass
# ---
def test_wordpress_resolved(self):
        self.compare_stacks('WordPress_Single_Instance.template',
                            'WordPress_Single_Instance.yaml',
                            {'KeyName': 'test'})
# ---
def scan(
    f: Callable,
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable: ...
# ---
def set_caption(self, caption):
        """Set the window's caption.

        The caption appears in the titlebar of the window, if it has one,
        and in the taskbar on Windows and many X11 window managers.

        :Parameters:
            `caption` : str or unicode
                The caption to set.

        """
        raise NotImplementedError('abstract')
# ---
def accelerator_type_name(accel_type: int) -> str:
    """Return enum name like 'ACCELERATOR_TYPE_TPU'."""
    try:
        return _ACCELERATOR_TYPE.values_by_number[accel_type].name
    except KeyError:
        return f"UNKNOWN({accel_type})"
# ---
def __init__(self):
        self.hooks = []
        self.jit_hooks = []
# ---
def _stop_heartbeat(self) -> None:
        """Stop the heartbeat thread."""
        self._stop_event.set()
        if self._heartbeat_thread is not None:
            self._heartbeat_thread.join(timeout=5)
# ---
def crispr_knockout(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'knockout',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def copy_page(self, src_page: int, dst_page: int) -> Self:
        """Return a copy of this cache with ``src_page`` cloned into ``dst_page``."""
        raise NotImplementedError
# ---
def unauthenticated_userid(self, request):
		params = _parse_authorization(request, self.secret, self.realm)
		if params is None:
			return None
		if not _is_valid_nonce(params['nonce'], self.secret):
			_add_www_authenticate(request, self.secret, self.realm)
			return None
		return 'u:%s' % params['username']
# ---
def _get_current_node_tpu_worker_id() -> int | None:
    """Return the TPU worker ID for the current node across Ray versions."""
    if hasattr(TPUAcceleratorManager, "_get_current_node_tpu_worker_id"):
        return TPUAcceleratorManager._get_current_node_tpu_worker_id()
    if hasattr(TPUAcceleratorManager, "get_current_node_tpu_worker_id"):
        return TPUAcceleratorManager.get_current_node_tpu_worker_id()
    raise AttributeError("TPUAcceleratorManager is missing TPU worker ID helpers")
# ---
def _setup_parameter(self, file_name, **kwargs):
        for key, value in kwargs.items():
            origin = "%s =" %key
            new = "%s = %s" %(key, value)
            sudo('sed -i "/%s/ c\%s" %s' %(origin, new, file_name))
# ---
def _mode_to_open_mode(mode: str):
    if mode == "r":
        return {"open_mode": ts.OpenMode(open=True)}
    elif mode == "w":
        return {"open_mode": ts.OpenMode(create=True, delete_existing=True)}
    elif mode == "a":
        return {"open_mode": ts.OpenMode(create=True, open=True, delete_existing=False)}
    else:
        raise ValueError(f"Invalid mode: {mode}")
# ---
def dealias_binding(self, binding: str) -> AxisSelector | None:
        return self.bindings.get(binding, None)
# ---
def test_all_options(self):
        for optdict in [dict(nowrap=True),
                        dict(linenos=True),
                        dict(linenos=True, full=True),
                        dict(linenos=True, full=True, noclasses=True)]:

            outfile = StringIO.StringIO()
            fmt = HtmlFormatter(**optdict)
            fmt.format(tokensource, outfile)
# ---
def tricky():
        try:
            print('Tricky called')
            return 1
        finally:
            print('Tricky finally called')
            return 42
        return 0
# ---
def name(self):
        """Return the name of the switch."""
        return self._name
# ---
def is_delta_time(self):
        '''
        Return a boolean if this is a DeltaTime subclass.

        >>> mt = MidiTrack(1)
        >>> dt = DeltaTime(mt)
        >>> dt.is_delta_time()
        True
        '''
        if self.type_ == "DeltaTime":
            return True
        return False
# ---
def delete_image_metadata_item(self, image_id, key):
        """Deletes a single image metadata key/value pair."""
        resp, body = self.delete("images/%s/metadata/%s" %
                                 (str(image_id), key))
        self.validate_response(schema.delete, resp, body)
        return service_client.ResponseBody(resp, body)
# ---
def copy(self, cr, uid, id, default=None, context=None):
        if default is None:
            default = {}
        default = default.copy()
        default.update({'move_history_ids2': [], 'move_history_ids': []})
        return super(stock_move, self).copy(cr, uid, id, default, context=context)
# ---
def trunc(a: A) -> A:
    return wrap_elemwise_unary(jnp.trunc, a)
# ---
def _write_jsonl_gz(path: Path, rows: list[dict]) -> None:
    with gzip.open(path, "wt", encoding="utf-8") as handle:
        for row in rows:
            handle.write(json.dumps(row) + "\n")
# ---
def test_parse_string_template(self):
        tmpl_str = 'just string'
        msg = 'The template is not a JSON object or YAML mapping.'
        self._parse_template(tmpl_str, msg)
# ---
def num_cpus(self):
        return self._num_cpus
# ---
def test_amin_alias():
    Height, Width = hax.make_axes(Height=2, Width=3)
    data = jnp.arange(6.0).reshape(2, 3)
    arr = hax.named(data, (Height, Width))
    assert jnp.array_equal(hax.amin(arr).array, jnp.amin(data))
    assert jnp.array_equal(hax.amin(arr, axis=Height).array, jnp.amin(data, axis=0))
    assert hax.amin(arr, axis=Height).axes == (Width,)
# ---
def static_method(arg):
        return arg
# ---
def enterWalk(self):
        self.notify.debug('going for a walk')
        self.walk.enter()
        self.acceptOnce(self.walkDoneEvent, self.__decideNextState)
# ---
def startup_industry(self):
        return self.startup.primary_industry if self._get_startup() else None
# ---
def std(
        self, axis: AxisSelection | None = None, *, dtype=None, ddof=0, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.std(self, axis=axis, dtype=dtype, ddof=ddof, where=where)
# ---
def send_bit(self, bit):
        """ Send out a single bit, and pulse clock."""
        if self._MOSI == 0:
            return
        #
        # The input is read on the rising edge of the clock.
        #
        GPIO.output(self._MOSI, bit)  # Set the bit.
        GPIO.output(self._CLK, 1)     # Rising edge sends data
        GPIO.output(self._CLK, 0)
# ---
def loss_api(x_raw, w_raw):
        return fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
            x_raw.reshape(6, 4),
            y.reshape(6),
            w_raw,
            reduction="mean",
            logsumexp_weight=logsumexp_weight,
            implementation="xla",
        )
# ---
def test_empty_string(datadir):
    datadir.join('reader').chdir()
    src = 'sharedStrings-emptystring.xml'
    with open(src) as content:
        assert read_string_table(content.read()) == ['Testing empty cell', '']
# ---
def task_id(self) -> JobName:
        """Full task identifier (/job/.../index)."""
        return self._task_name
# ---
def tearDown(self):
        del self.cj
# ---
def get_message(self):
        return self.emailMessage
# ---
def __repr__(self):
        return ('<{0.__class__.__name__}'
                ' event={0.event_type!r}'
                ' subject={0.subject!r}>').format(self)
# ---
def _indices_to_selector(axes: Sequence[Axis], indices: Sequence[Any]) -> dict[AxisSelector, Any]:
    selector: dict[AxisSelector, Any] = {}
    for axis, idx in zip(axes, indices):
        if _is_trivial_index(idx):
            continue
        selector[axis] = idx
    return selector
# ---
def alt(self) -> ast.Alternative:
        assert self.is_alt
        return self.group.seq[0]
# ---
def _with_dtype(axes: NamedArrayAxes, dtype):
        """Attach dtype to axes metadata if not already set."""
        return axes if axes.dtype is not None else replace(axes, dtype=dtype)
# ---
def init(named):
            return Module(w=named)
# ---
def __setitem__(self, index, value):
		if not isinstance(index, int):
			raise TypeError("Expected int instance, got %s instead (%r)" % (type(index), index))
		list.__setitem__(self, index, value)
		col = self.structure[index]
		self._values[col.name] = col.to_python(value, row=self)
# ---
def _convert_frac_or_steps(frac_or_steps: float | int, num_train_steps: int):
    # if it's greater than 1, it must be a whole number of steps
    if frac_or_steps < 0.0 or (frac_or_steps > 1.0 and frac_or_steps % 1 != 0):
        raise ValueError(f"Invalid fraction {frac_or_steps}. Must be between 0 and 1. You can also use (whole) steps.")
    if frac_or_steps <= 1.0:
        return int(frac_or_steps * num_train_steps)

    return int(frac_or_steps)
# ---
def add_file(self, arcname, str_or_bytes, in_spine = False,
      guide_title = None, guide_type = None):
        '''Add the string or bytes instance str_or_bytes to the archive
        under the name arcname.'''
        self.opf.filelist.append(_Fileinfo(arcname, in_spine, guide_title,
                                 guide_type))
        self.epub_f.writestr(arcname, str_or_bytes)
# ---
def __unicode__(self):
        return 'SSOIDVerification for {name}, status: {status}'.format(
            name=self.name,
            status=self.status,
        )
# ---
def make_mock_connection(host, **kwargs):
        mock_conn = MagicMock()
        mock_conn.host = host
        # Simulate healthy for 10.0.0.1 and 10.0.0.3, unhealthy for 10.0.0.2
        if host in ["10.0.0.1", "10.0.0.3"]:
            mock_conn.run.return_value = MagicMock(returncode=0)
        else:
            mock_conn.run.return_value = MagicMock(returncode=1)
        return mock_conn
# ---
def on_step(self, step_info: S, cb_info: dict[str, jax.Array | Histogram]):
        levanter.tracker.log(cb_info, step=int(step_info.step))
# ---
def get_autoscaler_status(self) -> cluster_pb2.Controller.GetAutoscalerStatusResponse:
        """Get autoscaler status including recent actions and group states.

        Returns:
            GetAutoscalerStatusResponse proto with autoscaler status and recent actions
        """
        request = cluster_pb2.Controller.GetAutoscalerStatusRequest()
        return self._client.get_autoscaler_status(request)
# ---
def lookup_endpoints(self, name: str) -> list[ControllerEndpoint]:
        """Find endpoints by exact name match for non-terminal jobs."""
        with self._lock:
            return self._visible_endpoints(lambda ep: ep.name == name)
# ---
def testDoubleBasic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.float64)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.float64)
    self._testCpuMatmul(x, y)
# ---
def test_dropout():
    H = Axis("H", 10)
    key = jrandom.PRNGKey(0)
    hax_dropout = hax.nn.Dropout(0.5)
    eqx_dropout = eqx.nn.Dropout(0.5)

    f = _compare_eqx_and_haliax(hax_dropout, eqx_dropout)
    out = f(hax.random.uniform(jrandom.PRNGKey(0), (H,)), key=key, inference=False)

    assert out.axes == (H,)
# ---
def __call__(self, text, rules=None):
        return self.parse(text, rules)
# ---
def __init__(self, parent: OpeningOfGroup):
        self.parent = parent
# ---
def main(config: Config):
        assert config.data is not None
# ---
def test_sentinel_file_timeout(tmp_path: Path) -> None:
    """Test SentinelFile.wait raises TimeoutError when file doesn't appear."""
    sentinel = SentinelFile(str(tmp_path / "nonexistent.txt"))

    import pytest

    start = time.monotonic()
    with pytest.raises(TimeoutError, match="not signalled within"):
        sentinel.wait(timeout=Duration.from_seconds(0.1))
    elapsed = time.monotonic() - start

    # Should wait approximately the timeout duration
    assert 0.09 < elapsed < 0.2
# ---
def are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str):
    if not given_normalized:
        return False

    are_equal = False
    try:
        expr = f"({ground_truth_normalized})-({given_normalized})"
        if should_allow_eval(expr):
            sympy_diff = _sympy_parse(expr)
            simplified = sympy.simplify(sympy_diff)
            if simplified == 0:
                are_equal = True
    except BaseException:
        pass
    return are_equal
# ---
def extract_document(self, item):
        raise NotImplementedError
# ---
def test_contains_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b##cde", escape="#"), {7})
# ---
def test_bpe_round_trip_various_texts(llama3_tokenizer):
    """Validate BPE round-trip for diverse text patterns."""
    for text in ["!!}", "Hello world", "  spaces  ", "123", "\n\n"]:
        for token_id in llama3_tokenizer.encode(text, add_special_tokens=False):
            token_str = llama3_tokenizer.convert_ids_to_tokens(token_id)
            assert llama3_tokenizer.convert_tokens_to_ids(token_str) == token_id
# ---
def gmm_impl(lhs, rhs, group_sizes):
        return gmm_sharded(lhs.array, rhs.array, group_sizes.array, ar=ar)
# ---
def set_grad_func(self, grad_func):
    """Specifies the gradient function of this function."""
    assert not self._grad_func
    assert isinstance(grad_func, _DefinedFunction)
    self._grad_func = grad_func
# ---
def next(self, psm: PSM):
        if psm.char.isdigit():
            self.min.append(psm.char)
            return self
        elif psm.char == ",":
            self._interpret()
            return MaximumOfRepetition(self)
        elif psm.char == "}":
            self._interpret()
            return self.parent
        else:
            psm.error = 'expected digit, "," or "}"'
# ---
def nanargmax(array: NamedArray, axis: AxisSelector | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.nanargmax, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def get_device_variant(device: cluster_pb2.DeviceConfig) -> str | None:
    """Extract device variant (e.g., GPU model) from config."""
    if device.HasField("gpu"):
        return device.gpu.variant if device.gpu.variant else None
    elif device.HasField("tpu"):
        return device.tpu.variant if device.tpu.variant else None
    return None
# ---
def run(cmd: list, **kwargs) -> subprocess.CompletedProcess:
    """Run command with logging."""
    logging.info(f"Running: {' '.join(cmd)}")
    return subprocess.run(cmd, **kwargs)
# ---
def __iter__(self):
        return iter(self._calls)
# ---
def _update_word_list(self):
        """Update and sort found words according to frequency."""
        wordlist = self._word_freq.items()
        wordlist.sort(key=lambda x:x[1], reverse=True)
        self._word_list = [items[0] for items in wordlist]
# ---
def sort(self, items):
        """Sort the list of objects and return a list.
        """
        return sorted(items)
# ---
def _docker_inspect(container_name: str) -> str:
    result = subprocess.run(
        ["docker", "inspect", container_name],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        stderr = (result.stderr or "").strip()
        return f"failed to inspect container {container_name}: {stderr}"
    return result.stdout
# ---
def test_is_stop_signal_empty_stop_sequences():
    # stop_sequences is empty
    tail_tokens = hax.named(jnp.array([1, 2, 3], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.zeros((0, 3), dtype=jnp.int32), axis=("seq", "position"))
    assert not is_stop_signal(tail_tokens, stop_sequences)
# ---
def __len__(self) -> int:
        """
        Returns the final length of the data store.
        May raise if the length is not known.
        """
# ---
def unbind(self, axis: AxisSelector) -> Sequence["NamedArray"]:  # pragma: no cover
        return haliax.unbind(self, axis=axis)
# ---
def quit(self, code=None):
        self.command('quit', code)
# ---
def __init__(self, x=0.0, y=0.0):
        self.x = x
        self.y = y
# ---
def _map(example: dict) -> LmExample:
                return _create_lm_example(example[input_ids_key])
# ---
def get_task_status(self, request: cluster__pb2.Worker.GetTaskStatusRequest, ctx: RequestContext) -> cluster__pb2.TaskStatus:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def downgrade():
    conn = op.get_bind()
    acl_template_ids = _find_acl_templates(conn, ACL_TEMPLATES)
    if not acl_template_ids:
        return

    policy_uuid = _get_policy_uuid(conn, POLICY_NAME)
    if not policy_uuid:
        return

    delete_query = policy_template.delete().where(
        sa.sql.and_(
            policy_template.c.policy_uuid == policy_uuid,
            policy_template.c.template_id.in_(acl_template_ids),
        )
    )
    op.execute(delete_query)
# ---
def __call__(self, model: M, *inputs: X, **input_kwargs) -> Tuple[Scalar, M]: ...
# ---
def __init__(
        self,
        job_id: str,
        *,
        ref: ray.ObjectRef | None = None,
        submission_id: str | None = None,
        dashboard_address: str | None = None,
    ):
        self._job_id = job_id
        self._ref = ref
        self._submission_id = submission_id
        self._dashboard_address = dashboard_address
# ---
def shape(self) -> dict[str, int]:
        if not len(self.axis_names) == jnp.ndim(self.array):
            raise ValueError(
                f"Number of axes {len(self.axes)} does not match number of dimensions {jnp.ndim(self.array)} of array"
            )
        return {axis.name: axis.size for axis in self.axes}
# ---
def do_append(first, second, underneath=False):
  sign = "-" if underneath else "+"
  background = "-background black" if PARAMS["DO_POLAROID"] else ""
  command = "convert -gravity center %s %sappend %s %s %s" % (background, sign, first, second, first)
  ret = subprocess.call(command, shell=True)

  if ret != 0:
    raise Exception("Command failed: ", command)
# ---
def _to_dataframe(self, ordered_dims):
        columns = [k for k in self if k not in self.dims]
        data = [self._variables[k].expand_dims(ordered_dims).values.reshape(-1)
                for k in columns]
        index = self.coords.to_index(ordered_dims)
        return pd.DataFrame(OrderedDict(zip(columns, data)), index=index)
# ---
def isplayfile(pathname) :
    if os.path.isfile(pathname) == False:
        return False
    ext = os.path.splitext(pathname)[1]
    ext = ext.lower()
    if (ext == '.mp2') : return True;
    if (ext == '.mp3') : return True;
    if (ext == '.ogg') : return True;
    return False
# ---
def _py_blake2b(text: bytes) -> bytes:
    return hashlib.blake2b(text).digest()
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_index: typing.Literal[True],
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def __init__(self,apiKey,licenseId):
    self.headers = {"content-type": "application/json",
                    "Authorization": apiKey}
    self.params = {"licenseId" : licenseId }
    retries = Retry(total=5,
                    backoff_factor=0.75)
    self.session = requests.Session()
    self.session.mount(KANGROUTER_WEBSERVICE_APPLICATION_ROOT,
                       HTTPAdapter(max_retries=retries))
# ---
def set_setting(id, value):
    if not isinstance(value, basestring): value = str(value)
    addon.setSetting(id, value)
# ---
def prepared_geom(self):
        # GEOS internal data structure for prepared geometries grows over time,
        # recreate to limit memory consumption
        if not self._prepared_geom or self._prepared_counter > self._prepared_max:
            self._prepared_geom = shapely.prepared.prep(self.geom)
            self._prepared_counter = 0
        self._prepared_counter += 1
        return self._prepared_geom
# ---
def create_floatingip_precommit(self, context, fip_context):
        pass
# ---
def success(self) -> bool:
        return self.exception is None
# ---
def convert_hn(self, n, el, text, convert_as_inline):
        if convert_as_inline:
            return text

        style = self.options["heading_style"].lower()
        text = text.strip()
        if style == markdownify.UNDERLINED and n <= 2:
            line = "=" if n == 1 else "-"
            return self.underline(text, line)
        hashes = "#" * n
        if style == markdownify.ATX_CLOSED:
            return f"{hashes} {text} {hashes}\n\n"

        return f"\n\n{hashes} {text}\n\n"
# ---
def __init__(self):
        self._queue = []
# ---
def assign_task(self, task_id: JobName, resources: cluster_pb2.ResourceSpecProto) -> None:
        """Assign a task to this worker, updating committed resources."""
        self.running_tasks.add(task_id)
        self.committed_cpu += resources.cpu
        self.committed_mem += resources.memory_bytes
        self.committed_gpu += get_gpu_count(resources.device)
        self.committed_tpu += get_tpu_chip_count(resources.device)
# ---
def on_load(self):
        if self.doc.xpath('//p[contains(text(), "incident technique")]'):
            raise BrowserIncorrectPassword("Vous n'avez aucun compte sur cet espace. " \
                                           "Veuillez choisir un autre type de compte.")
# ---
def __len__(self):
        if not self.has_len():
            raise ValueError("DataLoader has no length")
        total_length = blocking_wait(self.data_store.current_len())
        step = self.scheduler.find_step_containing_offset(total_length) + 1
        return step
# ---
def data_vars(self):
        """Dictionary of xray.DataArray objects corresponding to data variables
        """
        return Variables(self)
# ---
def dev_shards(self):
    return 1
# ---
def delete_tpu_node(node_name: str, project: str, zone: str, quiet: bool = False) -> None:
    """Delete a TPU node."""
    cmd = [
        "gcloud",
        "compute",
        "tpus",
        "tpu-vm",
        "delete",
        node_name,
        f"--project={project}",
        f"--zone={zone}",
    ]

    if quiet:
        cmd.append("--quiet")

    run_gcloud_command(cmd)
# ---
def generateUUID():  # pylint: disable=invalid-name
    """ Utility function; generates UUIDs """
    return str(uuid.uuid4())
# ---
def health_check(
        self,
        request: cluster_pb2.Empty,
    ) -> cluster_pb2.Worker.HealthResponse: ...
# ---
def last_scale_up_ms(self) -> int:
        """Timestamp of last scale-up operation."""
        return self._last_scale_up.epoch_ms()
# ---
def __init__(self):
        try:
            _check_option_support(['single_bridge', 'dual_bridge'])
        except OSError:
            raise exception.DriverLoadError(
                driver=self.__class__.__name__,
                reason=_("Unable to locate usable ipmitool command in "
                         "the system path when checking ipmitool version"))
        _check_temp_dir()
# ---
def parquet_file() -> str:
    print(f"\n[Setup] Ensuring {FILENAME} is available...")
    file_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type="dataset", revision=REVISION)

    # Warm-up OS page cache to prevent disk I/O jitter from affecting the results.
    print(f"[Setup] Warming up OS cache for {file_path}...")
    with open(file_path, "rb") as f:
        while f.read(1024**2):
            pass

    return file_path
# ---
def init(cls, Vocab: Axis, config: WhisperConfig, *, key) -> "WhisperModel":
        k_t, k_embeddings = haliax.jax_utils.maybe_rng_split(key, 2)
        encoder = WhisperEncoder.init(config, key=k_embeddings)
        decoder = WhisperDecoder.init(config, key=k_t)

        return cls(encoder, decoder)
# ---
def move_by_offset(self, xoffset, yoffset):
        """Moving the mouse to an offset from current mouse position.
        Args:
            xoffset: X offset to move to.
            yoffset: Y offset to move to.
        """
        self._actions.append(lambda:
            self._driver.execute(Command.MOVE_TO, {
                'xoffset': xoffset,
                'yoffset': yoffset}))
        return self
# ---
def shuffle(self, key: PRNGKeyArray, *, perm_type: PermType = "feistel"):
        import levanter.data.permutation as permutation

        return permutation.PermutationDataset(self, key, perm_type=perm_type)
# ---
def test_dispatch_intermittent_failure(cluster):
    """Test intermittent heartbeat failure during dispatch (30%). Task assignments are
    buffered and retried on next heartbeat cycle. Task should eventually succeed.
    """
    _url, client = cluster
    enable_chaos("controller.heartbeat", failure_rate=0.3)
    job = submit(client, _quick, "intermittent-dispatch")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def evaluate(self, record: dict) -> bool:
        left_val = self.left.evaluate(record)
        if self.op == "and":
            return bool(left_val and self.right.evaluate(record))
        return bool(left_val or self.right.evaluate(record))
# ---
def testFormatSource(self):
    """Tests the _FormatSource function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))

    source_string = test_helper._FormatSource(
        event, event_data, event_data_stream)

    self.assertEqual(source_string, 'Test log file')
# ---
def get_tracker(name: str) -> Tracker: ...
# ---
def test_complex_logical(self):
        # (a > 0 AND b > 0) OR c > 0
        expr = ((col("a") > 0) & (col("b") > 0)) | (col("c") > 0)
        assert expr.evaluate({"a": 1, "b": 1, "c": -1}) is True
        assert expr.evaluate({"a": -1, "b": -1, "c": 1}) is True
        assert expr.evaluate({"a": -1, "b": -1, "c": -1}) is False
# ---
def hello():
        print("Hello from validation job!")
        return 42
# ---
def read_jsonl_gz_file(filepath: str) -> list[dict]:
    """Helper function to read a JSONL.GZ file"""
    results = []
    with fsspec.open(filepath, "rt", encoding="utf-8", compression="infer") as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    return results
# ---
def writeManagerFileThread(managerFilename, qin):
	m = open(managerFilename + ".csv", "w")
	m.write("Source,Target\n")
	while True:
		data = qin.get()
		if data == None:
			break
		m.write(str(data) + "\n")
		qin.task_done()
# ---
def imprime_reporte(nf, nr):
        reporte  = "NÃºmero de archivos procesados:\t {}\n".format(nf)
        reporte += "NÃºmero de filas en tsv:\t {}\n".format(nr)
        if(nf!=nr):
            reporte += "\n\n**** AtenciÃ³n ****\n"

        return reporte
# ---
def get(self) -> int:
        """Get current counter value."""
        return self._value
# ---
def test_tree_broadcast_to_mixed_types():
    from levanter.utils.jax_utils import tree_broadcast_to

    # Test with mixed types
    prefix = {"a": 1, "b": 2}
    target = {"a": [10, 20], "b": {"x": 30, "y": 40}}
    result = tree_broadcast_to(prefix, target)

    assert result == {"a": [1, 1], "b": {"x": 2, "y": 2}}
# ---
def wait_until_status(client, job_id, status_to_wait_for, timeout_seconds=5):
    start = time.time()
    while time.time() - start <= timeout_seconds:
        status = client.get_job_status(job_id)
        print(f"status: {status}")
        if status in status_to_wait_for:
            break
        time.sleep(1)

    return status
# ---
def __hash__(self):
        return hash(('not', hash(self.subquery)))
# ---
def testForBreak(self):
    self.assertEqual((0, '1\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1, 2, 3):
          print i
          break""")))
# ---
def __contains__(self, id):
		return id in self._addresses
# ---
def __call__(self, x, *, key):
            return x + self.array + self.static
# ---
def __repr__(self):
        return "WindowOp"
# ---
def unregister_endpoint(self, request: cluster__pb2.Controller.UnregisterEndpointRequest, ctx: RequestContext) -> cluster__pb2.Empty:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def peek(self):
		oldPos = self.tell()
		b = self.read(1)
		newPos = self.tell()
		if((newPos == (oldPos+1)) and (b != '')):
			r = ord(b)
		else:
			r = None

		self.seek(oldPos, 0)
		return r
# ---
def __init__(self, string, rind=None):
        CmdText.__init__(self)
        self.insert(string)

        if (rind is not None):
            self.response = rind
# ---
def _assert_result(self, select, result):
        eq_(config.db.execute(select).fetchall(), result)
# ---
def _join_prefix(prefix: str, tag: str) -> str:
    if prefix:
        return f"{prefix}/{tag}"
    return tag
# ---
def unload(self):
        """Unload the current model."""
        if self.server:
            console.print(f"[blue]Unloading {self.model_name}...[/blue]")
            self.server.shutdown()
            self.server = None
            console.print("[green]âœ“ Model unloaded[/green]")
        else:
            console.print("[yellow]No model loaded[/yellow]")
# ---
def include_constructor(loader, node):
        filepath = loader.construct_scalar(node)
        # Resolve relative to the current YAML file's directory
        base_dir = os.path.dirname(loader.name) if hasattr(loader, "name") else "."
        full_path = os.path.join(base_dir, filepath)

        with open(full_path, "r") as f:
            return yaml.safe_load(f)
# ---
def name_get(self, cr, uid, ids, context=None):
        """Append the serial to the name"""
        if not len(ids):
            return []
        res = [ (r['id'], r['serial'] and '%s [%s]' % (r['name'], r['serial'])
                                      or r['name'] )
                for r in self.read(cr, uid, ids, ['name', 'serial'],
                                   context=context) ]
        return res
# ---
def ReportTaskState(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __init__(self):
        super().__init__()
        self._failed = False
        self._slice_info: SliceInfo | None = None
# ---
def tiny_asr_corpus_config(path):
    return AudioIODatasetConfig(
        id="WillHeld/test_librispeech_parquet",
        text_key="text",
        train_split="validation",
        validation_split="validation",
        cache_dir=f"{path}/cache_asr",
    )
# ---
def locale_from_currency_code(dx_code):
    """
    This is a (temporary) hardcoded mapping between currency_list.json in nucleus and standard
    locale string useful for further formatting

    :param dx_code: An id of nucleus/commons/pricing_models/currency_list.json collection
    :return: standardised locale, eg 'en_US'; None when no mapping found
    """
    currency_locale_map = {0: 'en_US', 1: 'en_GB'}
    return currency_locale_map[dx_code] if dx_code in currency_locale_map else None
# ---
def __str__(self):
        return '[' + ', '.join('({},{})'.format(*el) for el in self._queue) + ']'
# ---
def test_current_pixel_updates_on_status_read(self):
        self.assertEqual(self.mda.current_pixel, [0, 0])
        self.mda.io_read_byte(0x3BA)
        self.assertEqual(self.mda.current_pixel, [1, 0])
# ---
def test_multiply(self):
        expr = col("a") * col("b")
        assert expr.evaluate({"a": 4, "b": 5}) == 20
# ---
def get_job_status(self, job_id: JobName) -> cluster_pb2.JobStatus:
        request = cluster_pb2.Controller.GetJobStatusRequest(job_id=job_id.to_wire())
        response = self._client.get_job_status(request)
        return response.job
# ---
def get_job_from_jbor(thing):
    '''
    :returns: Job ID from a JBOR

    Assumes :func:`is_job_ref` evaluates to True
    '''
    if '$dnanexus_link' in thing:
        return thing['$dnanexus_link']['job']
    else:
        return thing['job']
# ---
def centres(request):
	#Pythonç»ƒä¹ é¡¹ç›®ç®¡ç†ä¸­å¿ƒCenter
	return render(request, 'centres/centres.html')
# ---
def __init__(self):
            self.tokenizer = create_test_tokenizer()
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Current State", device_id)
# ---
def hashable_combine(dynamic, static):
    static_leaves, static_treedef = static
    static = jtu.tree_unflatten(static_treedef, static_leaves)
    return eqx.combine(dynamic, static)
# ---
def test_permute_errors_on_invalid_set_of_dims_indices(self):
    with self.assertRaisesRegexp(ValueError, r'Invalid permutation .*dims.*'):
      testing_utils.layer_test(
          keras.layers.Permute,
          kwargs={'dims': (1, 4, 2)}, input_shape=(3, 2, 4))
# ---
def _reduce_buckets(bucket: str, items: Iterator[CCInput]) -> BucketWithIds:
    # TODO: do we want/need this optimization?
    # if len(all_items) <= 1:
    #    return None  # No duplicates in this bucket
    return {
        "bucket": bucket,
        "ids": [RecordId(record_id=item["id"], record_id_norm=_internal_orderable_id(item["id"])) for item in items],
    }
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if not self._m2.visitdir(dir):
            return self._m1.visitdir(dir)

        if self._m2.visitdir(dir) == "all":
            # There's a bug here: If m1 matches file 'dir/file' and m2 excludes
            # 'dir' (recursively), we should still visit 'dir' due to the
            # exception we have for exact matches.
            return False
        return bool(self._m1.visitdir(dir))
# ---
def __sub__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "sub")
# ---
def max(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.max, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def sharding_for_axis(
    axis: AxisSelection, mapping: ResourceMapping | None = None, mesh: MeshLike | None = None
) -> NamedSharding:
    """Get the sharding for a single axis"""
    resolved_mesh = _resolve_mesh(mesh)
    if resolved_mesh is None:
        raise ValueError("No mesh found")

    return NamedSharding(resolved_mesh, pspec_for_axis(axis, mapping))
# ---
def test_directed_raises(self):
        with pytest.raises(nx.NetworkXNotImplemented):
            dir_G = nx.gn_graph(n=5)
            prev_cc = None
            edge = self.pick_add_edge(dir_G)
            insert = True
            nx.incremental_closeness_centrality(dir_G, edge, prev_cc, insert)
# ---
def test_layer_norm():
    H = Axis("H", 10)
    hax_ln = hax.nn.LayerNorm.init(H)
    eqx_ln = eqx.nn.LayerNorm(shape=(H.size,))

    f = _compare_eqx_and_haliax(hax_ln, eqx_ln)
    out = f(hax.random.uniform(jrandom.PRNGKey(0), (H,)))

    assert out.axes == (H,)
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        dtype = x.dtype
        mean = x.mean(self.axis)
        var = x.var(self.axis)
        inv = hax.rsqrt(var + self.eps)
        out = (x - mean) * inv
        out = out.astype(dtype)

        if self.weight is not None:
            out = self.weight * out
        if self.bias is not None:
            out = out + self.bias
        return out
# ---
def __mul__(self, factor: float) -> "Duration":
        return Duration(int(self._ms * factor))
# ---
def __init__(self, matcher):
        self._matcher = matcher
# ---
def output_reparam(use_mup: bool = True) -> type[AbstractLinearReparam]:
        """Return the reparameterization class for an output linear layer."""

        return mup.OutputLinearMup if use_mup else mup.LinearStandardParam
# ---
def test_ar_generate_respects_max_tokens(params, model_cfg, tokenizer):
    """Generation should not exceed max_new_tokens."""
    context = tokenizer.encode_source("x = 1\n")
    max_tokens = 5
    generated, _ = _ar_generate_tokens(
        params=params,
        context_token_ids=context,
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(0),
        temperature=1.0,
        max_new_tokens=max_tokens,
    )
    assert len(generated) <= max_tokens
# ---
def quantize_dequantize(x, q_dtype, scale, compute_dtype):
    qx = quantize(x, q_dtype, scale, compute_dtype)
    return dequantize(qx, x.dtype, scale)
# ---
def test_raw_metric_vm_disk(metrics_collection, appliance, provider):
    vm_name = provider.data['cap_and_util']['capandu_vm']
    query = query_metric_db(appliance, provider, 'disk_usage_rate_average',
        vm_name)

    for record in query:
        if record.disk_usage_rate_average is not None:
            assert record.disk_usage_rate_average > 0, 'Zero VM Disk IO'
            break
# ---
def step_impl(context, user_name):
    user_exists = context.user_service.exists(user_name)
    assert context.base_user.username == user_exists['username']
    assert context.base_user.password == user_exists['password']
    assert context.base_user.email == user_exists['email']
    assert context.base_user.first_name == user_exists['first_name']
    assert context.base_user.last_name == user_exists['last_name']
    assert user_exists['_id'] is not None
# ---
def reload(self) -> "TreeStore":
        """
        Close the builder and return a TreeStore.
        """
        tree = jtu.tree_map(lambda builder: builder.reload(), self.tree, is_leaf=heuristic_is_leaf)
        return TreeStore(tree, self.path, self.mode)
# ---
def worker(mock_bundle_cache, mock_image_cache, mock_runtime):
    """Create Worker with mocked dependencies."""
    config = WorkerConfig(
        port=0,
        port_range=(50000, 50100),
        poll_interval=Duration.from_seconds(0.1),  # Fast polling for tests
    )
    return Worker(
        config,
        bundle_provider=mock_bundle_cache,
        image_provider=mock_image_cache,
        container_runtime=mock_runtime,
    )
# ---
def shutdown(self) -> None:
        raise NotImplementedError
# ---
def is_named_array(leaf):
    from .core import NamedArray

    "Typically used as the is_leaf predicate in tree_map"
    return isinstance(leaf, NamedArray)
# ---
def variance(self) -> Scalar:
        """
        Calculate the variance of the histogram.
        Variance = E[X^2] - (E[X])^2
        where E[X] is the mean and E[X^2] is the mean of squares.
        """
        mean = self.mean
        mean_of_squares = self.sum_squares / self.num
        variance = mean_of_squares - (mean**2)
        return variance
# ---
def values(self):
        """ADC values presented as a list."""
        return self._values
# ---
def reducer(key, items):
        items_list = list(items)
        if len(items_list) > 1:
            return key
        return None
# ---
def get_ip():
  """Get primary IP (the one with a default route) of local machine.

  This works on both Linux and Windows platforms, and doesn't require working
  internet connection.
  """

  s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
  try:
    # doesn't even have to be reachable
    s.connect(('10.255.255.255', 1))
    return s.getsockname()[0]
  except:
    return '127.0.0.1'
  finally:
    s.close()
# ---
def num_cpus(self) -> int:
        return 0
# ---
def out_qdq_fwd(compute_dtype, out, scale, amax_history):
    return out, (scale, amax_history)
# ---
def get_matching_files(
    patterns: list[str], all_files_list: list[pathlib.Path], exclude_patterns: list[str]
) -> list[pathlib.Path]:
    matched = []
    for file_path in all_files_list:
        if should_exclude(file_path):
            continue
        if matches_pattern(file_path, exclude_patterns):
            continue
        if matches_pattern(file_path, patterns):
            matched.append(file_path)
    return matched
# ---
def save(self, context):
        """In addition to save the pf, it should also save the
        vfs associated with this pf
        """
        # To ensure the saving type is PF
        if self.type != 'pf':
            raise exception.InvalidDeployType()

        for exist_vf in self.virtual_function_list:
            exist_vf.save(context)
        super(PhysicalFunction, self).save(context)
# ---
def delete_comment(self):
        "Delete the selected comment"

        if self.get_selected_item()['type'] == 'Comment':
            self.delete_item()
        else:
            self.term.flash()
# ---
def test_host_maintenance_on(self):
        self._test_host_action(self.conn.host_maintenance_mode,
                               True, 'on_maintenance')
# ---
def __enter__(self):
        self.start()
        return self
# ---
def slice(self, start: Mapping[AxisSelector, int], length: Mapping[AxisSelector, int | Axis]) -> "NamedArray": ...
# ---
def __init__(self, conn_id, sql, *args, **kwargs):
        self.sql = sql
        self.conn_id = conn_id
        super(SqlSensor, self).__init__(*args, **kwargs)
# ---
def log_metrics(
    metrics: typing.Mapping[str, LoggableValue | Any], *, step: Optional[int], commit: Optional[bool] = None
):
    """
    Deprecated. Use log instead.
    """
    warnings.warn("log_metrics is deprecated in favor of log", DeprecationWarning)
    log(metrics, step=step, commit=commit)
# ---
def __repr__(self):
        return ['NONE', 'STRING', 'OSD_STRING', 'FLAG', 'INT64', 'DOUBLE', 'NODE', 'NODE_ARRAY', 'NODE_MAP',
                'BYTE_ARRAY'][self.value]
# ---
def tri(N, M=None, k=0, typecode=None, dtype=None):
    """ returns a N-by-M array where all the diagonals starting from
        lower left corner up to the k-th are all ones.
    """
    dtype = convtypecode(typecode, dtype)
    if M is None: M = N
    m = np.greater_equal(np.subtract.outer(np.arange(N), np.arange(M)),-k)
    if m.dtype != dtype:
        return m.astype(dtype)
# ---
def next_power_of_two(n: int) -> int:
    return 1 << (n - 1).bit_length()
# ---
def test_lat_lon_output(self):
        """Asserts that the vertices in the lat-lon output are in the
        right order (lat before long)."""
        for vertex in self.polycircle.to_lat_lon():
            assert_almost_equal(vertex[0], self.latitude, places=2)
            assert_almost_equal(vertex[1], self.longitude, places=2)
# ---
def _attrs_copy(self):
        return None if self._attrs is None else OrderedDict(self._attrs)
# ---
def test_limit_offset_selectable_in_unions(self):
        table = self.tables.some_table
        s1 = (
            select([table])
            .where(table.c.id == 2)
            .limit(1)
            .order_by(table.c.id)
        )
        s2 = (
            select([table])
            .where(table.c.id == 3)
            .limit(1)
            .order_by(table.c.id)
        )

        u1 = union(s1, s2).limit(2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "not %s" % self.rule
# ---
def __repr__(self):
        return "<nevermatcher>"
# ---
def __init__(self, deadline_monotonic: float):
        self._deadline = deadline_monotonic
# ---
def forget(self, request):
		return [('WWW-Authenticate', _generate_digest_challenge(
			round(time.time()),
			self.secret,
			self.realm,
			'NPDIGEST'
		))]
# ---
def logical_xor(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logical_xor(x1, x2)
# ---
def __init__(self, config: HtmlToMarkdownConfig, **kwargs):
        self.include_links = config.include_links
        self.include_images = config.include_images

        kwargs = config.markdownify_kwargs
        super().__init__(**kwargs)
# ---
def make_input_name(name, step_name) -> InputName:
    return InputName(name=name, step=SimpleNamespace(name=step_name))
# ---
def from_string(value):
        """ Convert an ORM ``value`` into a :class:`datetime` value. """
        if not value:
            return None
        value = value[:DATETIME_LENGTH]
        if len(value) == DATE_LENGTH:
            value += " 00:00:00"
        return datetime.strptime(value, DATETIME_FORMAT)
# ---
def __init__(self, capacity):
        self.examplers = deque(maxlen=capacity)
        self.capacity = capacity
# ---
def __post_init__(self):
        self._rng = np.random.default_rng(seed=self.seed)
# ---
def resolve_axis(self, axis: PartialShapeDict) -> ShapeDict: ...
# ---
def __init__(self, manager, task_id, sub_ids=None, dry_run=False, resubmit=False):
		self.__manager = manager
		self.__task	= self.__manager.load_task(task_id)
		self.__sub_ids = sub_ids
		self.__dry_run = dry_run
		self.__resubmit = resubmit

		self.__logger = logging.getLogger('JSUB')
		if self.__sub_ids==None:
			self.__sub_ids=range(len(self.__task.data['jobvar']))

		self.__initialize_manager()
# ---
def _add_www_authenticate(request, secret, realm):
	resp = request.response
	if not resp.www_authenticate:
		resp.www_authenticate = _generate_digest_challenge(
			round(time.time()),
			secret, realm, 'NPDIGEST'
		)
# ---
def __shortCodeToWordnetCode(self, shortCode):
        """
        It returns the wordnet code from a given language short code
        """
        if not self.__isLanguageAvailable(code=shortCode):
            raise Exception("Wordnet code not found for the language short code %s " % shortCode)

        code = shortCode.lower()
        wordnetCode = AVAILABLE_LANGUAGES[code]
        return wordnetCode
# ---
def test_named_jit_with_donation():
    with axis_mapping(resource_map):

        class MyModule(eqx.Module):
            array: jnp.ndarray
            array2: jnp.ndarray

        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModule, donate_args=(True, False))(jnp.zeros((8, 8)), jnp.zeros((8, 16)))
            assert mod.array.sharding.is_fully_replicated
# ---
def __repr__(self):
        return "<intersectionmatcher m1=%r, m2=%r>" % (self._m1, self._m2)
# ---
def device_is_address(self):
        if is_valid_ip(self.device):
            return True
        return False
# ---
def test_grad_scan():
    X = hax.Axis("x", 4)

    def f(x):
        x_ref = hax.new_ref(hax.zeros(X))

        def scan_fn(_, i):
            slice = x_ref.slice({"x": i})
            slice[...] = jnp.sin(x * i)
            return None, None

        hax.scan(scan_fn, X)(None, jnp.arange(X.size))
        return x_ref[...].sum().scalar()

    df = jax.grad(f)(1.0)
    assert pytest.approx(df) == jnp.sum(jnp.cos(jnp.arange(X.size)) * jnp.arange(X.size))
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)  # First moment
        second_moment_buffer = otu.tree_zeros_like(params)  # Second moment
        count = jnp.zeros([], jnp.int32)

        return ScaleByMiniState(
            count=count, momentum_buffer=momentum_buffer, second_moment_buffer=second_moment_buffer
        )
# ---
def __init__(self, backend):
        import matplotlib.pyplot as plt
        from IPython.core.interactiveshell import InteractiveShell
        from IPython.core.pylabtools import backend2gui

        self.shell = InteractiveShell.instance()
        self.old_backend = backend2gui[str(plt.get_backend())]
        self.new_backend = backend
# ---
def test_count_empty(backend):
    """Test count on empty dataset."""
    ds = Dataset.from_list([]).count()
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 0
# ---
def _command(self):
        raise NotImplementedError("Subclass must implement this")
# ---
def run(self):
            with self.default_sess():
                player = get_player_fn(train=False)
                while not self.stopped():
                    try:
                        score = play_one_episode(player, self.func)
                        # print("Score, ", score)
                    except RuntimeError:
                        return
                    self.queue_put_stoppable(self.q, score)
# ---
def rng():
    return random.Random(42)
# ---
def test_tracker_plugin_default_works():
    config = """
    tracker:
        entity: foo
    """
    parsed = yaml.safe_load(config)

    @dataclasses.dataclass
    class ConfigHolder:
        tracker: TrackerConfig

    import draccus

    tconfig = draccus.decode(ConfigHolder, parsed).tracker

    assert isinstance(tconfig, TrackerConfig.get_choice_class("wandb"))

    assert tconfig.entity == "foo"
# ---
def check_chainlink_initialized():
    """Check if .chainlink directory exists."""
    cwd = os.getcwd()
    current = cwd

    while True:
        candidate = os.path.join(current, ".chainlink")
        if os.path.isdir(candidate):
            return True
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent

    return False
# ---
def test_no_hang_if_empty_shard_source():
    class EmptyShardSource(ShardedDataSource[list[int]]):
        @property
        def shard_names(self) -> Sequence[str]:
            return []

        def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
            raise RuntimeError("This should not be called")

    with tempfile.TemporaryDirectory() as tmpdir:
        reader = build_or_load_cache(tmpdir, EmptyShardSource(), TestProcessor())
        assert list(reader) == []
# ---
def _validate(record: dict) -> None:
        """Validate Dolma record has required fields."""
        assert "id" in record, "Dolma record missing 'id' field"
        assert "text" in record, "Dolma record missing 'text' field"
        assert "source" in record, "Dolma record missing 'source' field"
        # metadata and created are optional but commonly used
        if "metadata" in record:
            assert isinstance(record["metadata"], dict), "metadata must be a dict"
# ---
def __init__(self):
        super().__init__()
# ---
def reserve_slot(self, slot_id: int | jnp.ndarray | None = None) -> tuple["DecodeState", int]:
        sequences, slot = self.sequences.reserve_slot(slot_id)
        return dataclasses.replace(self, sequences=sequences), slot
# ---
def on_line(line: str) -> None:
            logger.info("[%s] %s", self._vm_name, line)
# ---
def testSComplexRandom(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.complex64)
      y = self._randMatrix(k, m, np.complex64)
      self._testCpuMatmul(x, y)
# ---
def _get_device_type(self) -> DeviceType:
        """Get device type from config."""
        accel = self._config.accelerator_type
        if accel == config_pb2.ACCELERATOR_TYPE_GPU:
            return DeviceType.GPU
        elif accel == config_pb2.ACCELERATOR_TYPE_TPU:
            return DeviceType.TPU
        return DeviceType.CPU
# ---
def setup_triggers(self, env):
        """ Add the necessary triggers to invalidate/recompute ``self``. """
        model = env[self.model_name]
        for path in self.depends:
            self._setup_dependency([], model, path.split('.'))
# ---
def handle_schedule(self, channel, data):
        msg = Forseti.Schedule.decode(data)
        for i in range(msg.num_matches):
            self.match_list_box.Insert(format_match(msg.matches[i]), i,
                    msg.matches[i])
# ---
def _reshape_axes_for_bshd_bins(q, q_class, output_order=("B", "S", "H", "D")):
    """
    Reshape the axes of a qkv as BSHD to match the bins in q_class
    """

    q = _maybe_flatten(q, q_class["B"], "B")
    q = _maybe_flatten(q, q_class["S"], "S")
    q = _maybe_flatten(q, q_class["H"], "H")
    q = _maybe_flatten(q, q_class["D"], "D")
    q = q.rearrange(output_order)
    return q
# ---
def make_dialogue(self, seeds):
        """Returns a list of sentences, each being a list of tokens."""
        acts = self.make_speech_bits(seeds)
        seq_map = self.simplify(acts)
        sents = self.report_seq(seq_map)
        return(sents)
# ---
def scan_via(
        self, fn: ScanFunction[M, CarryT, P, OutputT_co], *, unroll: int | bool | None = None
    ) -> Callable[Concatenate[CarryT, P], tuple[CarryT, OutputT_co]]: ...
# ---
def ones(shape: AxisSpec, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to 1"""
    if dtype is None:
        dtype = jnp.float32
    return full(shape, 1, dtype)
# ---
def unembed_active_scale(self):
        return 1
# ---
def test_hf_audio_loading_source():
    # Use the Real Librispeech Valudation. Testing one doesn't support streaming.
    ac = AudioDatasetSourceConfig(id="WillHeld/test_librispeech_parquet", text_key="text")
    audio_iterator = iter(ac.get_shard_source("validation"))
    for i in range(10):
        audio, sample, text = next(audio_iterator)
# ---
def _fill_queue_with_batches(self):
        with local_cpu_mesh():
            super()._fill_queue_with_batches()
# ---
def _handle_interrupt(self, _signum: int, _frame: object):
        self.logger.log("Interrupted! Cleaning up...", level="WARN")
        signal.signal(signal.SIGINT, None)
        signal.signal(signal.SIGTERM, None)
        self._interrupted = True
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        examples = []
        for _ in range(n_examples):
            a = rng.integers(self.min_val, self.max_val)
            b = rng.integers(self.min_val, self.max_val)
            result = a + b
            prompt = f"What is {a}+{b}? Output just the number:"
            answer = str(result)
            examples.append({"prompt": prompt, "answer": answer})
        return examples
# ---
def playlist_move(self, index1, index2):
        self.command('playlist_move', index1, index2)
# ---
def setUp (self):
    utils.mktemp ()
    for filename in self.filenames:
      with open (os.path.join (utils.TEST_ROOT, filename), "w"):
        pass
# ---
def tearDown(self):
        os.remove(self.path)
# ---
def test_with_unflatten_flatten_unordered():
    Z = Axis("Z", B.size * C.size)
    assert einops_rearrange(zq, "{ W D C (Q: B H)} -> D (Z: B C) W H", H=H).axes == (D, Z, W, H)
    # make sure the values are right too
    z_t = (
        zq.array.reshape((B.size, H.size, D.size, W.size, C.size))
        .transpose((2, 0, 4, 3, 1))
        .reshape((D.size, Z.size, W.size, H.size))
    )
    assert (einops_rearrange(zq, "{ W D C (Q: B H)} -> D (Z: B C) W H", H=H).array == z_t).all()
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._exception: BaseException | None = None
# ---
def __bool__(self) -> bool:
        return self.healthy
# ---
def as_async_dataset(self) -> "AsyncDataset[T_co]":
        return AsyncifiedDataset(self)
# ---
def _class_name(self):
        return "LicenseInfo"
# ---
def start(self) -> None:
        """Start background thread for loading data."""
        if self._thread is not None:
            raise RuntimeError("ReplayDataLoader already running")

        self._stop_event.clear()
        self._thread = threading.Thread(target=self._worker_loop, daemon=True)
        self._thread.start()
        logger.info("Started ReplayDataLoader background thread")
# ---
def ClosePreviewWindow():
  """ Close the preview window if it is present, otherwise do nothing """
  vim.command( 'silent! pclose!' )
# ---
def distance_to_point(self, point):
        point_to_surface = point - self.top_left_corner3d()
        distance_to_surface = self.normal.dot(point_to_surface)
        return distance_to_surface
# ---
def load_json(cls, data, default_rule=None):
        """Allow loading of JSON rule data."""

        # Suck in the JSON data and parse the rules
        rules = dict((k, parse_rule(v)) for k, v in
                     jsonutils.loads(data).items())

        return cls(rules, default_rule)
# ---
def test_deduplicate_empty(backend):
    """Test deduplication on empty dataset."""
    ds = Dataset.from_list([]).deduplicate(key=lambda x: x["id"])
    results = list(Backend.execute(ds, context=backend))
    assert results == []
# ---
def rel(self, f):
        """Convert repo path back to path that is relative to cwd of matcher."""
        return util.pathto(self._root, self._cwd, f)
# ---
def Request_Information (self, Reception_ID):
        self.Step (Message = "Requesting (updated) information about reception " + str (Reception_ID))

        Data_On_Reception = self.Reception_Database.Single (Reception_ID)

        self.Step (Message = "Received information on reception " + str (Reception_ID))

        return Data_On_Reception
# ---
def hard_swish(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_swish, a)
# ---
def remove_title_page(html: BeautifulSoup):
    # Remove title page since we only care about information after first section
    title_page = html.findAll("div", {"class": "ltx_titlepage"})
    for tp in title_page:
        tp.decompose()
# ---
def ptp(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.ptp, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def addcommissioninfo(self, comminfo, name=None):
        self.comminfo[name] = comminfo
# ---
def unregister(self, endpoint_id: str) -> None:
        """Unregister an endpoint.

        Args:
            endpoint_id: Endpoint ID to remove
        """
        self._cluster.unregister_endpoint(endpoint_id)
# ---
def get_size_str(size):
    """
    Formats a byte size as a string.

    The returned string is no more than 9 characters long.
    """
    if size is None:
        return "0 " + SIZE_LEVEL[0]
    if size == 0:
        magnitude = 0
        level = 0
    else:
        magnitude = math.floor(math.log(size, 10))
        level = int(min(math.floor(magnitude // 3), 4))
    return ('%d' if level == 0 else '%.2f') % (float(size) / 2**(level*10)) + ' ' + SIZE_LEVEL[level]
# ---
def __repr__(self):
        return f"FlatMapOp(fn={_get_fn_name(self.fn)})"
# ---
def parse(self, x, y):
        if (x,y) not in self.resolutions:
            resolutions = ', '.join(['%sx%s' % (a, b) for a,b in self.resolutions])
            raise Exception('Resolution %s x %s not supported. Available resolutions: %s' % (x,y, resolutions)  )
        return Resolution(x, y)
# ---
def metadata(self) -> Dict[str, Any]:
        return {
            "tokenizer": self.bt.metadata,
            "processor": self.feature_extractor.to_dict(),
        }
# ---
def length(self):
        """
        Returns the length (in days) of the task, by considering the start date
        and the due date. When there is no start date, its creation date is
        used. Returns 0 when one of these dates is missing.
        """
        start = self.start_date() or self.creation_date()
        due = self.due_date()

        if start and due and start < due:
            diff = due - start
            return diff.days
        else:
            return 0
# ---
def decrease(rank):
    return dispatch('user', mutation_types.DECREASE, rank)
# ---
def from_read_session(read_session):
        schema_type = read_session._pb.WhichOneof("schema")
        if schema_type == "avro_schema":
            return _AvroStreamParser(read_session)
        elif schema_type == "arrow_schema":
            return _ArrowStreamParser(read_session)
        else:
            raise TypeError(
                "Unsupported schema type in read_session: {0}".format(schema_type)
            )
# ---
def _start_heartbeat(self) -> None:
        """Start background thread that periodically refreshes the lease."""

        def heartbeat_loop():
            while not self._stop_event.wait(HEARTBEAT_INTERVAL):
                self._status_file.refresh_lock()

        self._heartbeat_thread = Thread(target=heartbeat_loop, daemon=True)
        self._heartbeat_thread.start()
# ---
def compute_rloo_advantages(rollouts: list[Rollout]) -> np.ndarray:
    """Compute RLOO (Reward Leave-One-Out) advantages for a group of rollouts."""
    rewards = np.array([r.episode_reward for r in rollouts])

    n = len(rewards)
    if n <= 1:
        return np.zeros_like(rewards)

    total = rewards.sum()
    leave_one_out_baselines = (total - rewards) / (n - 1)
    advantages = rewards - leave_one_out_baselines
    return advantages
# ---
def __init__(self):
        self.rebus_controller = None
# ---
def id(self):
		"Temporary hack to transition between _id and id"
		return self._id
# ---
def invalidate_cache_for_item(doc):
	invalidate_cache_for(doc, doc.item_group)

	website_item_groups = list(set((doc.get("old_website_item_groups") or [])
								+ [d.item_group for d in doc.get({"doctype": "Website Item Group"}) if d.item_group]))

	for item_group in website_item_groups:
		invalidate_cache_for(doc, item_group)

	if doc.get("old_item_group") and doc.get("old_item_group") != doc.item_group:
		invalidate_cache_for(doc, doc.old_item_group)
# ---
def _check_value(value):
    """ Return ``value``, or call its getter if ``value`` is a :class:`SpecialValue`. """
    return value.get() if isinstance(value, SpecialValue) else value
# ---
def calc(A):
        A = A / max_abs
        aa = A * A
        aa_sum0 = jnp.sum(aa, axis=0)
        i = jnp.argmax(aa_sum0, 0)
        x = jax.lax.dynamic_index_in_dim(A, i, 1, keepdims=False)
        x = x @ A
        return max_abs * jnp.linalg.norm((x / jnp.linalg.norm(x)) @ A.T)
# ---
def find_src(i):
        src = gen_state.decode_state.clone_sources["seq", i].scalar()

        def do(src):
            # match positions where slot_ids == src; take first
            eq = (slot_ids == src).array
            idx = jnp.nonzero(eq, size=1, fill_value=INVALID)[0][0]
            return idx

        return jax.lax.cond(is_valid(src), do, lambda x: x, src)
# ---
def the_object_name1_and_name2_are_different_elements(name1, name2):
    ifc = an_ifc_file_exists()
    element1 = ifc.by_id(the_object_name_exists(name1).BIMObjectProperties.ifc_definition_id)
    element2 = ifc.by_id(the_object_name_exists(name2).BIMObjectProperties.ifc_definition_id)
    assert element1 != element2, f"Objects {name1} and {name2} have same elements {element1} and {element2}"
# ---
def shutdown(self):
        """Signal shutdown and wait for threads to finish"""
        logger.info("Shutting down inference context.")
        self.shutdown_event.set()
        self.inference_thread.join(timeout=1)
        self.batch_thread.join(timeout=1)
# ---
def test_put_block_bytes_large(self, storage_account_name, storage_account_key):
        self._setup(storage_account_name, storage_account_key)
        blob = self._create_blob()

        # Act
        for i in range(5):
            resp = blob.stage_block(
                'block {0}'.format(i).encode('utf-8'), urandom(LARGE_BLOCK_SIZE))
            self.assertIsNotNone(resp)
            assert 'content_md5' in resp
            assert 'content_crc64' in resp
            assert 'request_id' in resp
# ---
def decorator(fn):
            return pytest.mark.skipif("CI" in os.environ, reason=fn_or_msg)(fn)
# ---
def testWithAs(self):
    self.assertEqual((0, '1 2 3\n'),
                     _GrumpRun(textwrap.dedent("""\
        class ContextManager(object):
          def __enter__(self):
            return (1, (2, 3))
          def __exit__(self, *args):
            pass
        with ContextManager() as [x, (y, z)]:
          print x, y, z
        """)))
# ---
def create_container(self, config: ContainerConfig) -> str:
        container_id = f"local-{uuid.uuid4().hex[:8]}"
        self._containers[container_id] = _LocalContainer(
            config=config,
        )
        return container_id
# ---
def uri(self):
        """
        Gets the uri of this ContributorOrcid.

        :return: The uri of this ContributorOrcid.
        :rtype: str
        """
        return self._uri
# ---
def test_filters_for_instance_without_ip_v6(self):
        self.flags(use_ipv6=False)
        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)
        rulesv4, rulesv6 = self.fw._filters_for_instance("fake", network_info)
        self.assertEquals(len(rulesv4), 2)
        self.assertEquals(len(rulesv6), 0)
# ---
def get_secrets(self):
        ''' returns all of the defined secrets '''
        return self.get(Secret.secret_path) or {}
# ---
def output_link(self, m):
        return self._process_link(m, m.group(3), m.group(4))
# ---
def delete(self):
        """ Deletes all the resources associated with the deployment (instance template, network, firewall, instance
        group manager and all its instances.
        """
        self.deployment.delete()
# ---
def filter(self, label):
                    for pattern, actype in AccountsPage.TYPES.items():
                        if label.startswith(pattern):
                            return actype
                    return Account.TYPE_UNKNOWN
# ---
def sinh(a: A) -> A:
    return wrap_elemwise_unary(jnp.sinh, a)
# ---
def test_param_usage(self):
        @event.event('evt_test', priority=-12, enable=False)
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.event_name == 'evt_test'
        assert h_info.handler is on_test
        assert h_info.priority == -12
        assert not h_info.should_enable
        assert not h_info.is_async
# ---
def handle_partial(state):
            ref_counts, indices = state
            has_free = hax.any(ref_counts == 0).scalar()
            ref_counts = eqx.error_if(ref_counts, ~has_free, "Out of free pages during clone_pages_from")
            free_idx = hax.argmax(ref_counts == 0, "page")
            indices = indices.at["seq", dst_seq_id, "page", last_idx].set(free_idx)
            ref_counts = ref_counts.at["page", free_idx].add(1)
            return ref_counts, indices
# ---
def KeyPos(self) -> Axis:
        return self.max_Pos.alias("key_position")
# ---
def group_id(self) -> str:
        """Unique identifier for this VM group."""
        ...
# ---
def _batchify_ctor(ctor):
        # this is gross but it basically just vmaps the ctor over each batch dimension
        return functools.reduce(lambda ctor, batch_axis: hax.vmap(ctor, batch_axis), reversed(batch_dims), ctor)
# ---
def __init__(self, m1, m2):
        super(xormatcher, self).__init__(m1._root, m1._cwd)
        self.traversedir = m1.traversedir
        self.m1 = m1
        self.m2 = m2
# ---
def test_create_experiment_view(self):
        """ Tests edit_experiment template renders for url 'create_experiment' """
        response = self.client.get(reverse("ab_testing_tool_create_experiment"))
        self.assertOkay(response)
        self.assertTemplateUsed(response, "ab_tool/edit_experiment.html")
# ---
def get_github_issues():
    """Get all issues with the `experiment` label."""
    g = Github()
    repo = g.get_repo("marin-community/marin")
    return repo.get_issues(labels=["experiment"], state="all")
# ---
def remaining_ms(self) -> int:
        """Get remaining milliseconds until deadline (0 if expired)."""
        remaining_seconds = self._deadline - time.monotonic()
        return max(0, int(remaining_seconds * 1000))
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        """Submit method call to thread pool, returning a future."""
        return self._executor.submit(self._method, *args, **kwargs)
# ---
def _cancel_pending_call(self, ar):
        """
        Cancels a pending call (keyed by the AsyncResult returend by _routing_call).

        @return True if the call was truly pending.
        """
        if self.has_pending_call(ar):
            ar.set(False)
            return True

        return False
# ---
def test_length(self):
        """Test that cmp_version compares by length as last resort"""
        self.assertTrue(vmops.cmp_version('1.2.3', '1.2.3.4') < 0)
# ---
def model_type(self):  # noqa: D401
        return Gemma3LMHeadModel
# ---
def get_stats(self, container_id: str) -> ContainerStats:
        del container_id
        return ContainerStats(memory_mb=100, cpu_percent=10, process_count=1, available=True)
# ---
def terminate(self) -> None:
        """Terminate this VM group and all its VMs.

        This stops VM lifecycle threads, unregisters VMs from the registry,
        and deletes the underlying cloud resource.
        """
        ...
# ---
def constant_time_compare(val1, val2):
    if len(val1) != len(val2):
        return False
    result = 0
    for x, y in zip(val1, val2):
        result |= ord(x) ^ ord(y)
    return result == 0
# ---
def test_capturing_readouterr_unicode(self):
        with self.getcapture() as cap:
            print("hx\xc4\x85\xc4\x87")
            out, err = cap.readouterr()
        assert out == py.builtin._totext("hx\xc4\x85\xc4\x87\n", "utf8")
# ---
def testImportWildcardMemberRaises(self):
    regexp = 'wildcard member import is not implemented'
    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,
                            'from foo import *')
    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,
                            'from "__go__/foo" import *')
# ---
def test_ckpt_path_invalid_input_type():
    with pytest.raises(ValueError, match="Unknown type"):
        ckpt_path_to_step_name(12345)
# ---
def test_gpt2_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    check_load_config(TrainLmConfig, config_file)
# ---
def get_vm(self, vm_id: str) -> ManagedVm | None:
        """Get a specific VM by ID."""
        with self._lock:
            return self._vms.get(vm_id)
# ---
def lambda_fn(x):
      return x
# ---
def __init__(self):
        args = self.arg_parser.parse_known_args()[0]
        super(ScikitBase, self).__init__()
        self.pipeline = self.load_pipeline(args.pipeline)
        if args.feature_names:
            self.feature_names = self.load_pipeline(args.feature_names)
# ---
def terminate_job(self, request: cluster__pb2.Controller.TerminateJobRequest, ctx: RequestContext) -> cluster__pb2.Empty:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def dummy_fn(cfg: DummyCfg):
    # write one tiny file so the step "does something"
    out_path = os.path.join(cfg.output_path, "dummy")
    os.makedirs(out_path, exist_ok=True)
    with open(os.path.join(out_path, "done.txt"), "w") as f:
        f.write(str(cfg.x))
    return cfg.x
# ---
def dict_value(self, decode_str=False):
        return { self.keys[i].decode('utf-8'): self.values[i].node_value(decode_str) for i in range(self.num) }
# ---
def __init__(self, width, height, depth=None):
        self.__width = width
        self.__height = height
        self.__depth = depth
# ---
def __init__(self, client: "IrisClient", job_id: JobName):
        self._client = client
        self._job_id = job_id
# ---
def cli():
    """TPU VM Manager - Manage preemptible TPU VMs for GitHub Actions CI."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )
# ---
def forward(
    params: GrugModelParameters,
    token_ids: Int[Array, "B S"],
    cfg: GrugModelConfig,
    *,
    mask: AttentionMask | jax.Array | None = None,
) -> Float[Array, "B S V"]:
    hidden = _transformer_hidden(params, token_ids, cfg, mask=mask)
    logits = jnp.einsum("bsh,hd->bsd", hidden, params.output_proj, out_sharding=Pbatch)
    return logits
# ---
def __lt__(self, other: "Duration") -> bool:
        return self._ms < other._ms
# ---
def test_classify_textfiles_to_db_no_connection(mock_db, mock_jw):
    mock_db.connected_to_db.return_value = False
    classify_documents.classify_textfiles_to_db(0, None)
    assert not mock_jw.called
# ---
def __post_init__(self):
        if not self._parts:
            raise ValueError("JobName cannot be empty")
        for part in self._parts:
            if "/" in part:
                raise ValueError(f"JobName component cannot contain '/': {part}")
            if not part or not part.strip():
                raise ValueError("JobName component cannot be empty or whitespace")
# ---
def width(self):
        """The width of the window, in pixels.  Read-write.

        :type: int
        """
        return self.get_size()[0]
# ---
def _write_jsonl_gz(path: Path, records: list[dict]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with gzip.open(path, "wt", encoding="utf-8") as handle:
        for record in records:
            handle.write(json.dumps(record))
            handle.write("\n")
# ---
def started_job(method, notebook, data):
    return {'job': notebook.current_job, 'username': SAAGIE_USERNAME}
# ---
def _index_of_name(names: Sequence[str | Axis], name) -> int:
    for i, x in enumerate(names):
        if isinstance(x, Axis):
            x = axis_name(x)
        if x == name:
            return i
    return -1
# ---
def test_from_corpus_builds_nonempty_bank(bank):
    assert bank.total_entries > 0
    assert len(bank.entries) > 0
# ---
def task_state_name(state: int) -> str:
    """Return enum name like 'TASK_STATE_RUNNING'."""
    try:
        return _TASK_STATE.values_by_number[state].name
    except KeyError:
        return f"UNKNOWN({state})"
# ---
def output_strikethrough(self, m):
        text = self.output(m.group(1))
        return self.renderer.strikethrough(text)
# ---
def in_memory_table(small_parquet_path: str) -> pa.Table:
    """
    Loads 250k rows into memory once. Used for marshaling and batch size tuning benchmarks.
    """
    return pq.read_table(small_parquet_path)
# ---
def _is_jit_tracer(x) -> bool:
    if isinstance(x, NamedArray):
        x = x.array
    return isinstance(x, jax.core.Tracer)
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["Gpt2Config"]:  # type: ignore
        # We trust this code because it's in our hub repo
        return HFCheckpointConverter(self.__class__, reference_checkpoint="gpt2", ignore_prefix="transformer")
# ---
def _end_message_token(self) -> int:
        tokens = self.tokenizer.encode("<|im_end|>", add_special_tokens=False)
        assert len(tokens) == 1, f"Expected single token for <|im_end|>, got {len(tokens)}"
        return tokens[0]
# ---
def test_maxiters():
    """ensure the iteration ceiling works"""
  #  assert kmeans.should_iter([], [], iterations=29) == True
    assert kmeans.should_iter([], [], iterations=30) == False
    assert kmeans.should_iter([], [], iterations=31) == False
# ---
def do_save(self, e):
        self.remote.do_save(self.get_match())
# ---
def test_startswith_unescaped(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab%c"), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10})
# ---
def create_parquet_file(data: list[dict], filepath: str):
    """Helper function to create a Parquet file"""
    import pandas as pd

    df = pd.DataFrame(data)
    df.to_parquet(filepath, index=False)
# ---
def test_get_process_logs_no_buffer():
    """Test GetProcessLogs returns empty when buffer is None."""
    state = ControllerState()
    mock_scheduler = MockSchedulerWake()
    service = ControllerServiceImpl(state, mock_scheduler, bundle_prefix="file:///tmp/test-bundles", log_buffer=None)

    response = service.get_process_logs(cluster_pb2.Controller.GetProcessLogsRequest(prefix="", limit=0), None)
    assert len(response.records) == 0
# ---
def intersects(self, bbox, srs):
        bbox = self._geom_in_coverage_srs(bbox, srs)
        with self._prep_lock:
            return self.prepared_geom.intersects(bbox)
# ---
def tryLoad(self, fs, node_id):
        """
        @type fs: EnkfFS
        @type node_id: NodeId
        @rtype: bool
        """
        assert isinstance(fs, EnkfFs)
        assert isinstance(node_id, NodeId)

        return EnkfNode.cNamespace().try_load(self, fs, node_id)
# ---
def remaining(self):
        """int: Remaining items in the page."""
        return self._remaining
# ---
def Embed(self) -> Axis:
        return cast(Axis, self.token_embeddings.Embed)
# ---
def generate_random_id() -> str:
    """Generate a short random ID (4 alphanumeric characters)."""
    import random
    import string

    chars = string.ascii_lowercase + string.digits
    return "".join(random.choices(chars, k=4))
# ---
def recombination_knockout(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'knockout',
        'purpose': 'repression',
        'method': 'site-specific recombination',
        'modified_site_by_coordinates': {
            "assembly": "GRCh38",
            "chromosome": "11",
            "start": 60000,
            "end": 62000
        }
    }
# ---
def test_task_with_ports(worker):
    """Test task with port allocation."""
    request = create_run_task_request(ports=["http", "grpc"])
    task_id = worker.submit_task(request)

    task = worker.get_task(task_id)
    assert len(task.ports) == 2
    assert "http" in task.ports
    assert "grpc" in task.ports
    assert task.ports["http"] != task.ports["grpc"]

    task.thread.join(timeout=15.0)
# ---
def submit_job(
    state: ControllerState,
    job_id: str,
    request: cluster_pb2.Controller.LaunchJobRequest,
) -> JobName:
    """Submit a job via event."""
    jid = JobName.from_string(job_id) if job_id.startswith("/") else JobName.root(job_id)
    request.name = jid.to_wire()
    state.handle_event(
        JobSubmittedEvent(
            job_id=jid,
            request=request,
            timestamp=Timestamp.now(),
        )
    )
    return jid
# ---
def _l3_plugin(self):
        return bc.get_plugin(bc.constants.L3)
# ---
def __enter__(self) -> "WorkerPool":
        self._launch_workers()
        self.wait_for_workers()
        return self
# ---
def create_tmpfile(prefix='tmp'):
        ''' Generates and returns a temporary file name '''

        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False) as tmp:
            return tmp.name
# ---
def consecutive_failures(self) -> int:
        """Number of consecutive scale-up failures."""
        return self._consecutive_failures
# ---
def test_repr(self):
        expr = col("meta")["score"]
        assert repr(expr) == "col('meta')['score']"
# ---
def wrapped(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return fn(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    logger.warning(f"{fn.__name__} failed on attempt {attempt + 1}, retrying: {e}")
                    time.sleep(delay)

            raise RuntimeError(f"{fn.__name__} failed after {max_retries} attempts")
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["Qwen3Config"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfQwen3Config,
        )
# ---
def test_equal_5(self):
        self.assertEqual(string_color('Mathew Smith'), '8B00F1')
# ---
def selected_target_account(self):
        return self.selected_pane.selected_target
# ---
def get_channels(self):
        '''Get all channels used in this Track.
        '''
        post = []
        for event in self.events:
            if event.channel not in post:
                post.append(event.channel)
        return post
# ---
def convert_to_display_name(self, value, record=None):
        assert record, 'Record expected'
        return Datetime.to_string(Datetime.context_timestamp(record, Datetime.from_string(value)))
# ---
def ix(self, word):
        """
        Returns the index on self.vocab and self.clusters for 'word'
        """
        temp = np.where(self.vocab == word)[0]
        if temp.size == 0:
            raise KeyError("Word not in vocabulary")
        else:
            return temp[0]
# ---
def _run_callable(entry: CallableEntrypoint) -> None:
    entry.callable(*entry.args, **entry.kwargs)
# ---
def _create_test_curriculum_config() -> CurriculumConfig:
    """Minimal curriculum config with 3 independent lessons."""
    lessons = {
        name: LessonConfig(lesson_id=name, env_config=EnvConfig(env_class="test.FakeEnv", env_args={}))
        for name in ("easy", "medium", "hard")
    }
    return CurriculumConfig(lessons=lessons, max_seq_len=42)
# ---
def test_mutation_apply():
    m = Mutation(start=4, end=9, replacement="world", node_type="Name", original="hello")
    result = m.apply("say hello there")
    assert result == "say world there"
# ---
def get_available_ports(only_free=False):
    ports = _get_available_ports()

    if only_free:
        ports = list(set(ports) - set(DxlIO.get_used_ports()))

    return ports
# ---
def test_invalid_user_pass_returns_auth_error(self):

        self.assertRaises(AuthError, IOSXE, node=node, username='stuff', password='things',
                          disable_warnings=True)
# ---
def unregister_key_binding(self, keydef):
        binding_name = MPV._binding_name(keydef)
        self.command('disable-section', binding_name)
        self.command('define-section', binding_name, '')
        if callable(callback):
            del self._key_binding_handlers[binding_name]
            if not self._key_binding_handlers:
                self.unregister_message_handler('key-binding')
# ---
def get_default_zone() -> str | None:
    """Get the default GCP zone."""
    try:
        result = run_gcloud_command(["gcloud", "config", "get-value", "compute/zone"])
        return result.stdout.strip() or None
    except RuntimeError:
        return None
# ---
def __init__(self, status_code):
        self.status_code = status_code
        super(ResponseError, self).__init__(status_code)
# ---
def count_dir(path: str) -> int:
        pattern = os.path.join(path.rstrip("/"), "**", "*.jsonl*")
        pipeline = Dataset.from_files(pattern, empty_glob_ok=True).flat_map(load_file).map(lambda _: 1).reduce(sum)
        results = Backend.execute(pipeline)
        return results[0]
# ---
def check_vm_params_for_linux_with_external_kernel(self):
        self.assertEquals(self.vm['platform']['nx'], 'false')
        self.assertEquals(self.vm['PV_args'], 'root=/dev/xvda1')
        self.assertNotEquals(self.vm['PV_kernel'], '')
        self.assertNotEquals(self.vm['PV_ramdisk'], '')

        # check that these are not set
        self.assertEquals(self.vm['HVM_boot_params'], {})
        self.assertEquals(self.vm['HVM_boot_policy'], '')
# ---
def message(self):
        return self.protocol.messages[int(self.ui.spinBoxFuzzMessage.value() - 1)]
# ---
def __init__(self):
        # Load available backends
        for entry in pkg_resources.iter_entry_points("gosa.object.backend"):
            clazz = entry.load()
            ObjectBackendRegistry.backends[clazz.__name__] = clazz()
# ---
def _assert_covariance_matrix(cov: NamedArray, axis: Axis, expected: jnp.ndarray) -> None:
    assert jnp.allclose(cov.array, expected)  # type: ignore[union-attr]
    assert cov.axes[0] == axis  # type: ignore[union-attr]
    assert cov.axes[1].name == f"{axis.name}_cov"  # type: ignore[union-attr]
    assert cov.axes[1].size == axis.size
# ---
def test_one_step_edit_identical_returns_none():
    source = "x = 1\n"
    assert one_step_edit(source, source) is None
# ---
def any(
        self, axis: AxisSelection | None = None, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.any(self, axis=axis, where=where)
# ---
def h_fs_touch (_,path): open(path,'w').close()
# ---
def test_set_current_client_restores_on_exception():
    """Context manager should restore previous client even on exception."""
    explicit = LocalClient(max_threads=2)
    with pytest.raises(RuntimeError):
        with set_current_client(explicit):
            raise RuntimeError("boom")
    assert current_client() is not explicit
# ---
def test_worker_delayed_registration(cluster):
    """Worker registration delayed by 5s on first attempt. Task pends, then
    schedules once registration completes."""
    _url, client = cluster
    enable_chaos("worker.register", delay_seconds=5.0, max_failures=1)
    job = submit(client, _quick, "delayed-reg")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def run(self, command, *args):
        self.command('run', command, *args)
# ---
def quick_task(task_id: int):
        import time as time_module

        time_module.sleep(1.0)
        print(f"Task {task_id} completed")
        return task_id
# ---
def test_mem_write_byte_calls_char_generator_bottom_right(self):
        self.mda.mem_write_byte(3998, 0xFF)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_GREEN, MDA_BLACK))
# ---
def name(self, val):
        self.name_ctrl.SetValue(val)
# ---
def is_leaf(x):
        return x is None or isinstance(x, OverwriteWithGradient)
# ---
def stop_job(ctx, job_id):
    """Stop a running Ray job."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        _stop_job(job_id)
        print(f"Job {job_id} stop requested")
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        from fray.v2.device_flops import device_flops

        flops = device_flops(self.variant, dtype)
        if flops is None:
            raise ValueError(f"Unknown device/dtype: {self.variant}/{dtype}")
        return flops
# ---
def test_column_select_pushdown(self, sync_backend, vortex_file):
        """Test column selection pushdown."""
        ds = Dataset.from_files(str(vortex_file)).load_vortex().select("id", "score")

        results = list(Backend.execute(ds, context=sync_backend))
        assert len(results) == 100
        assert set(results[0].keys()) == {"id", "score"}
# ---
def logaddexp(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.logaddexp](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.logaddexp.html)
    """
    return jnp.logaddexp(x1, x2)
# ---
def __init__(self, vocab, clusters):
        self.vocab = vocab
        self.clusters = clusters
# ---
def add_portal_ip(self, pip):
        '''add cluster ip'''
        self.put(Service.portal_ip, pip)
# ---
def speech_sequence(self, n):
        speech_acts_seq = []
        next_speech_id = 0
        for i in range(n):
            next_speech_id = searchsorted(cumsum(self._acts_transitions), rand() * sum(self._acts_transitions))
            speech_acts_seq.append(self._speech_acts[next_speech_id])
        return speech_acts_seq
# ---
def convert_environment(env: EnvironmentConfig | None) -> EnvironmentSpec | None:
    """Convert fray v2 EnvironmentConfig to Iris EnvironmentSpec."""
    if env is None:
        return None
    from iris.cluster.types import EnvironmentSpec

    return EnvironmentSpec(
        pip_packages=list(env.pip_packages),
        env_vars=dict(env.env_vars),
        extras=list(env.extras),
    )
# ---
def _jit_stub(fn, *args, **kwargs):
        def _wrapped(x):
            return fn(x)

        return _wrapped
# ---
def parsed_error_msg(self):
        """
        Sometimes, the error message we've received needs to be parsed into
        something more human readable

        The default behavior is to return the current error message as is.
        """
        return self.error_msg
# ---
def docs():
    test_resources = Path(__file__).parent.joinpath("resources", "docs")
    docs = {}
    for doc_file in test_resources.glob("*.txt"):
        docs[doc_file.stem] = doc_file.read_text()
    return docs
# ---
def start(self):
        """Start background polling thread."""
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._poll_loop, daemon=True)
        self._thread.start()
# ---
def create_test_rollout_group(key: str, n_rollouts: int, start_idx: int = 0) -> RolloutGroup:
    """Create a test rollout group."""
    rollouts = [create_test_rollout(start_idx + i) for i in range(n_rollouts)]
    return RolloutGroup(rollouts=rollouts)
# ---
def get_directory_friendly_name(model_name: str) -> str:
    return model_name.replace("/", "--").replace(".", "-")
# ---
def EvalBatch(self):
        """Get the evaluation batch axis from the trainer configuration."""
        return self.trainer.EvalBatch
# ---
def root(self):
        return self.__root
# ---
def wait(self) -> None:
        """Block until the server exits."""
        self._threads.wait()
# ---
def is_leaf(x):
                return eqx.is_array(x) or isinstance(x, eqx.Module) or haliax.is_named_array(x)
# ---
def dispatch_event(self, *args):
        if not self._enable_event_queue or self._allow_dispatch_event:
            if EventDispatcher.dispatch_event(self, *args) != False:
                self._legacy_invalid = True
        else:
            self._event_queue.append(args)
# ---
def test_shard_plain_array_in_module():
    with axis_mapping(resource_map):

        class MyModule(eqx.Module):
            array: jnp.ndarray

            def __init__(self):
                self.array = jnp.zeros((8, 8))

        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModule)()
            assert mod.array.sharding.is_fully_replicated
# ---
def main(argv=sys.argv[1:]):
    try:
        docopt.docopt(__doc__, argv=argv, version=hello.__version__)
    except docopt.DocoptExit as e:
        print(str(e), file=sys.stderr)
        return 2
    except SystemExit as e:
        return 0
# ---
def __init__(self, service_name: str, **attrs):
        super().__init__(**attrs)
        self.service_name = service_name
        self.available_methods = {}
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)
        return x
# ---
def convert_sup(self, el, text, convert_as_inline):
        if not text:
            return ""
        return f"<sup>{text}</sup>"
# ---
def __init__(self):
        self.lc = lcm.LCM('udpm://239.255.76.67:7667?ttl=1')
        self.lc.subscribe('Schedule/Schedule', self.handle_schedule)
        self.lc.subscribe('Timer/Time', self.handle_time)
        self.match_list_box = None
        self.match_control = None
        self.thread = threading.Thread(target=self._loop)
        self.thread.daemon = True
# ---
def is_point_on_left(self, point):
        return self.a * point[0] + self.b * point[1] + self.c > 0
# ---
def selected_target(self):
        return self._selected_target
# ---
def __str__(self) -> str:
        if self.revision is None:
            return self.model_name_or_path
        return f"{self.model_name_or_path}@{self.revision}"
# ---
def sort(self, axis: AxisSelector) -> Any:  # pragma: no cover
        return haliax.sort(self, axis=axis)
# ---
def is_empty(self) -> bool:
        return len(self.endpoints) == 0
# ---
def shutdown(self) -> None:
        pass
# ---
def config_get(self):
        return bottle.template('{{!ret}}',
                               ret=json.dumps(self.config))
# ---
def image_url(self, pixel_size=None):
        """
        Get the URL for the user icon in the desired pixel size, if it exists. If no
        size is supplied, give the URL for the full-size image.
        """
        if "profile" not in self._raw:
            return
        profile = self._raw["profile"]
        if (pixel_size):
            img_key = "image_%s" % pixel_size
            if img_key in profile:
                return profile[img_key]
        return profile[self._DEFAULT_IMAGE_KEY]
# ---
def shaped_rng_split(key, split_shape: int | Sequence[int] = 2) -> PRNGKeyArray:
    if isinstance(split_shape, int):
        num_splits = split_shape
        split_shape = (num_splits,) + key.shape
    else:
        num_splits = np.prod(split_shape)
        split_shape = tuple(split_shape) + key.shape

    if num_splits == 1:
        return jnp.reshape(key, split_shape)

    unshaped = maybe_rng_split(key, num_splits)
    return jnp.reshape(unshaped, split_shape)
# ---
def has_len(self) -> bool:
        return self.dataset.is_finite()
# ---
def write(self, data: bytes) -> int:
            self.write_calls.append(len(data))
            return super().write(data)
# ---
def test_rust_native(benchmark: Any, small_parquet_path: str) -> None:
    """
    Baseline: Rust reads file from disk, parses Parquet, transforms, returns RecordBatch.
    """

    def _run() -> int:
        return len(dupekit.process_native(small_parquet_path))

    assert benchmark(_run) > 0
# ---
def wrap(q_bhsd, k_bhsd, v_bhsd, seg_ids, kernel):
        return jax.vmap(kernel, in_axes=(0, 0, 0, segment_batch_axis))(q_bhsd, k_bhsd, v_bhsd, seg_ids)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """Peak FLOP/s for a single device."""
        return self.device.device_flops(dtype)
# ---
def resources(self) -> Dict[str, float]:
        """Any resources that this processor needs to run. Ray uses this to schedule tasks."""
        return {}
# ---
def test_csv(self):
        """
        Check CSV output
        """
        f_result = open(self.dir.get_data_dir() + 'fixtures/escala.csv')

        self.assertEqual(self.escala.csv(), f_result.read())
        f_result.close()
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
        # parse the shard name to get the shard number
        shard_num = int(shard_name.split("_")[1])
        return ([shard_num * 10 + i] * 10 for i in range(row, self._rows_per_shard))
# ---
def match(self, item):
        if self.field not in item:
            return False
        timestamp = float(item[self.field])
        date = datetime.fromtimestamp(timestamp)
        return self.interval.contains(date)
# ---
def unembed(self, input_embeds: NamedArray):
        """
        Unembed the input embeddings back to the vocabulary space.

        Equivalent to `input_embeds.dot(self.weight, axis=self.Embed)`.
        """
        return input_embeds.dot(self.weight, axis=self.Embed) * self.reparam.unembed_active_scale
# ---
def prefix(self) -> str:
        return self._prefix
# ---
def dev():
    import knowledge.names as names

    mcW = mc.MarkovChain()
    nm = names.NameMaker()
    speakers = [nm.random_person() for i in range(1, 4)]
    dm = dialogue_maker([n['name'] for n in speakers], [n['pronoun'] for n in speakers], mcW)
    dlg = dm.make_dialogue(["dog", "run", "spot"])
    print(dlg)
# ---
def large_company(self):
        """
        :example: 'SOCAR'
        """
        return self.random_element(self.large_companies)
# ---
def rights(self, val):
        self.opt_meta['rights'] = _EpubMeta('dc:rights', '' + val)
# ---
def shard_map(
    f: Callable[Args, R],
    *,
    in_specs: Any = None,
    out_specs: Any = None,
    mesh: Mesh | None = None,
    axis_mapping: ResourceMapping | None = None,
    check_rep: bool = False,
    **kwargs,
) -> Callable[Args, R]: ...
# ---
def convert_to_cache(self, value, record, validate=True):
        if not validate:
            return value or False
        if value in self.get_values(record.env):
            return value
        elif not value:
            return False
        raise ValueError("Wrong value for %s: %r" % (self, value))
# ---
def wait_async_writes():
        w_write_future.wait()
# ---
def run(
        self,
        cmd: list[str],
        *,
        check: bool = True,
        env: dict[str, str] | None = None,
        **kwargs,
    ) -> subprocess.CompletedProcess:
        """Run a command and wait for completion."""
        self._check_entered()

        if "cwd" not in kwargs and self._cwd:
            kwargs["cwd"] = self._cwd

        logger.info("Running %s", " ".join(cmd))
        return subprocess.run(cmd, env=env, check=check, **kwargs)
# ---
def _testCpuMatmul(self, x, y, transpose_x=False, transpose_y=False):
    x_mat = np.matrix(x).T if transpose_x else np.matrix(x)
    y_mat = np.matrix(y).T if transpose_y else np.matrix(y)
    np_ans = x_mat * y_mat
    with self.test_session(use_gpu=False):
      tf_ans = tf.matmul(x, y, transpose_x, transpose_y).eval()
    self.assertAllClose(np_ans, tf_ans)
    self.assertAllEqual(np_ans.shape, tf_ans.shape)
# ---
def are_shape_checks_enabled():
    return _ENABLE_SHAPE_CHECKS
# ---
def test_dontreadfrominput_buffer_python2():
    from _pytest.capture import DontReadFromInput
    f = DontReadFromInput()
    with pytest.raises(AttributeError):
        f.buffer
    f.close()
# ---
def test_stdin_nulled_by_default(self):
        print("XXX this test may well hang instead of crashing")
        print("XXX which indicates an error in the underlying capturing")
        print("XXX mechanisms")
        with self.getcapture():
            pytest.raises(IOError, "sys.stdin.read()")
# ---
def test_basic_auth_with_single_quoted_realm(self):
        self.test_basic_auth(quote_char="'")
# ---
def create_weight_transfer_config():
    return WeightTransferConfig(
        mode=WeightTransferMode.ARROW_FLIGHT,
        sync_interval_steps=1,
        max_weight_transfer_wait_time=10.0,
    )
# ---
def assert_equal_out(hax_out, torch_out: torch.Tensor):
        assert np.isclose(
            torch_out.numpy(), np.array(hax_out.array), rtol=1e-2, atol=1e-2
        ).all(), f"{torch_out} != {hax_out}"
# ---
def _get_available_subsets(cfg: TransformSFTDatasetConfig) -> Sequence[str | None]:
    configured_subsets = unwrap_versioned_value(cfg.subsets)
    if configured_subsets:
        return configured_subsets

    try:
        subsets = datasets.get_dataset_config_names(cfg.source)
    except Exception as exc:
        logging.log(logging.WARNING, f"Unable to fetch dataset configs for {cfg.source}: {exc}")
        subsets = []
    if not subsets:
        return [None]
    return subsets
# ---
def test_number_comparison_task_format_bonus():
    task = NumberComparisonTask()

    digit_reward = task.compute_reward("42", "42")
    non_digit_reward = task.compute_reward("42", "forty-two")

    assert digit_reward > non_digit_reward
    assert digit_reward == pytest.approx(1.0)
    assert non_digit_reward == pytest.approx(0.0)
# ---
def bitwise_not(a: A) -> A:
    return wrap_elemwise_unary(jnp.bitwise_not, a)
# ---

def right_angle_triangle(a, b, c):
    '''
    Given the lengths of the three sides of a triangle. Return True if the three
    sides form a right-angled triangle, False otherwise.
    A right-angled triangle is a triangle in which one angle is right angle or
    90 degree.
    Example:
    right_angle_triangle(3, 4, 5) == True
    right_angle_triangle(1, 2, 3) == False
    '''
    return a*a == b*b + c*c or b*b == a*a + c*c or c*c == a*a + b*b
# ---
def update_model(old_model, new_state_dict):
    return hsd.from_state_dict(old_model, new_state_dict)
# ---
def test_sample_with_size_respects_limit(bank):
    rng = random.Random(42)
    entry = bank.sample_with_size("If", max_stmts=2, rng=rng)
    if entry is not None:
        assert entry.stmt_count <= 2
# ---
def get_tpu_chip_count(device: cluster_pb2.DeviceConfig) -> int:
    """Extract TPU chip count from config."""
    if device.HasField("tpu"):
        return device.tpu.count or 0
    return 0
# ---
def test_permutation_handles_large_length_no_overflow(PermutationClass):
    large_length = 2**34
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(large_length, prng_key)
    index = 2**32  # A large index within the range
    result = permutation(index)
    assert isinstance(result, int)
    assert 0 <= result < large_length
# ---
def contractedBrailleToggled(self, checkbox):
        grid = self.get_widget('contractionTableGrid')
        grid.set_sensitive(checkbox.get_active())
        self.prefsDict["enableContractedBraille"] = checkbox.get_active()
# ---
def test_wait_all_timeout():
    stop = threading.Event()

    def hang():
        stop.wait(10)

    c = LocalClient(max_threads=4)
    handle = c.submit(JobRequest(name="hang", entrypoint=Entrypoint.from_callable(hang)))
    with pytest.raises(TimeoutError):
        wait_all([handle], timeout=0.2)
    handle.terminate()
    stop.set()
    c.shutdown(wait=True)
# ---
def quantize(x, q_dtype, scale, compute_dtype):
    # Explicitly cast the max values to the compute dtype to avoid unnecessary
    # casting to FP32 during the subsequent math operations."
    dtype_max = get_fp8_max(q_dtype, compute_dtype)
    scaled_x = x / jnp.broadcast_to(scale.astype(compute_dtype), x.shape)
    clipped_x = jnp.clip(scaled_x, -dtype_max, dtype_max)
    return clipped_x.astype(q_dtype)
# ---
def cron():
    reminders = load_json('data/reminders.json', True)
    for id, reminder in reminders.items():
        if now() > reminder['alarm']:
            send_message(reminder['chat_id'], reminder['text'])
            del reminders[id]
            save_json('data/reminders.json', reminders)
# ---
def output_exemplar(self) -> dict:
        return dict(**self.tokenizer("hi there", return_attention_mask=self.return_attention_mask, verbose=False))
# ---
def test_unschedulable_maps_to_failed(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_UNSCHEDULABLE) == JobStatus.FAILED
# ---
def unembed(self, x: NamedArray):
        return hax.dot(x, self.token_embeddings.weight, axis="embed")
# ---
def __eq__(self, other):
        return type(self) == type(other) or other is None
# ---
def shard_names(self) -> Sequence[str]:
        return [str(i) for i in range(len(self.docs))]
# ---
def intersect_axes(ax1: AxisSpec, ax2: AxisSelection) -> AxisSpec:  # type: ignore
    ...
# ---
def libretroToPhoenix(self, libretroSystem):
        return self.libretroToPhoenixMap[libretroSystem]
# ---
def top_left_corner3d(self):
        return self.edge_points3d[0]
# ---
def __init__(self, artist, title, year=None):
        self.__artist = artist
        self.__title = title
        self.__year = year
# ---
def item(self):  # pragma: no cover
        """Returns the value of this NamedArray as a python scalar."""
        return self.array.item()
# ---
def test_impl(n):
            df1 = pd.DataFrame({'key1': np.arange(n), 'A': np.arange(n) + 1.0})
            df2 = pd.DataFrame({'key2': n - np.arange(n), 'A': n + np.arange(n) + 1.0})
            A3 = pd.concat([df1.A, df2.A])
            return A3.sum()
# ---
def __getitem__(self, word):
        return self.get_cluster(word)
# ---
def _get_normal_vector(self):
        """
        :return: the normal vector of the surface. It determined the front side
        of the surface and it's not necessarily a unit vector
        """
        p0 = self.edge_points3d[0]
        p1 = self.edge_points3d[1]
        p3 = self.edge_points3d[3]
        v1 = p3 - p0
        v2 = p1 - p0
        normal = np.cross(v1, v2)
        norm = np.linalg.norm(normal)
        return normal / norm
# ---
def test_bound_in_heterogeneous_two_tuple(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.z).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(
            stmt,
            [(2,), (3,), (4,)],
            params={"q": [(2, "z2"), (3, "z3"), (4, "z4")]},
        )
# ---
def min(x, axis=0):
    return _Nmin(x, axis)
# ---
def testTryElse(self):
    self.assertEqual((0, 'foo baz\n'), _GrumpRun(textwrap.dedent("""\
        try:
          print 'foo',
        except:
          print 'bar'
        else:
          print 'baz'""")))
# ---
def __init__(self, req = None):
        if req is not None:
            self.req = req
            etalage_env = req.environ.get('etalage', {})
            for key in object.__getattribute__(self, 'env_keys'):
                value = etalage_env.get(key)
                if value is not None:
                    setattr(self, key, value)
# ---
def corrcoef(x, y=None):
    c = cov(x, y)
    d = diag(c)
    return c/sqrt(multiply.outer(d,d))
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"transformer": None, "embeddings": None}
# ---
def genetic_modification_7_valid_reagent(lab, award, crispr_gm):
    return {
        'purpose': 'characterization',
        'category': 'deletion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'description': 'blah blah description blah',
        "method": "CRISPR",
        "modified_site_by_target_id": "/targets/FLAG-ZBTB43-human/",
        "reagents": [
            {
                "identifier": "ABC123",
                "source": "/sources/sigma/"
            }
        ]
    }
# ---
def mock_load_datasets(config):
        return [DatasetWithMetaData(mock_dataset, "arithmetic", "test", "main")]
# ---
def unregister_message_handler(self, target):
        del self._message_handlers[target]
# ---
def __init__(self, name, crypto, description, pluginMethod):
		self.name = name #No Spaces please...
		self.cryptoMethod = crypto
		self.description = description
		self.plugin = pluginMethod
# ---
def _check_batch_size_divisibility(self):
        if self._data_axis_size is None:
            return
        for size in self.scheduler.unique_batch_sizes():
            if size % self._data_axis_size != 0:
                raise ValueError(
                    f"Batch size {size} is not divisible by data axis size {self._data_axis_size}. "
                    "This will lead to incorrect sharding. Set allow_non_divisible_batch_size=True to allow this."
                )
# ---
def evaluate(self, record: dict) -> Any:
        return _ARITHMETIC_OPS[self.op](self.left.evaluate(record), self.right.evaluate(record))
# ---
def get_indices(es):
    return elasticsearch.client.IndicesClient(es)
# ---
def ptp(x, axis=0):
    return _Nptp(x, axis)
# ---
def native_value(self) -> datetime | None:
        """Return sensor state."""
        job: OctoprintJobInfo = self.coordinator.data["job"]
        if (
            not job
            or not job.progress.print_time_left
            or not _is_printer_printing(self.coordinator.data["printer"])
        ):
            return None

        read_time = self.coordinator.data["last_read_time"]

        return read_time + timedelta(seconds=job.progress.print_time_left)
# ---
def test_swap_returns_previous_value():
    X = hax.Axis("x", 4)
    ref = hax.new_ref(hax.zeros(X))
    prev = hax.swap(ref, {"x": slice(1, 3)}, hax.ones(X.resize(2)))
    assert isinstance(prev, hax.NamedArray)
    assert prev.axes == (X.resize(2),)
    assert jnp.allclose(prev.array, 0.0)
    assert jnp.allclose(ref.value()[{"x": slice(1, 3)}].array, jnp.ones((2,), dtype=jnp.float32))
# ---
def _slow_job():
    import time as _time

    _time.sleep(120)
    return "done"
# ---
def test_dslice_oob_read_and_write():
    Seq = hax.Axis("seq", 5)
    from haliax import ds

    arr = hax.arange((Seq,), dtype=int)
    out = arr[{"seq": ds(3, 4)}]
    ref = jnp.take(arr.array, jnp.arange(3, 7), mode="fill", fill_value=0)
    assert jnp.array_equal(out.array, ref)

    upd = hax.arange((Seq.resize(4),), dtype=int)
    updated = arr.at[{"seq": ds(3, 4)}].set(upd)
    ref_upd = arr.array.at[jnp.arange(3, 7)].set(upd.array, mode="drop")
    assert jnp.array_equal(updated.array, ref_upd)
# ---
def list_all_jobs(self) -> list[ControllerJob]:
        with self._lock:
            return list(self._jobs.values())
# ---
def get_page(url):
    """Retrieve the given page."""
    return urllib2.urlopen(url).read()
# ---
def __eq__(self, other):  # pragma: no cover
        # special case because Jax sometimes call == on
        # types when they're in PyTrees
        if self.array is None:  # pragma: no cover
            return other.array is None

        if hasattr(other, "array") and other.array is None:  # pragma: no cover
            return False

        return haliax.equal(self, other)
# ---
def test_model_info_patch_for_fsspec_urls():
    """transformers calls model_info() in _patch_mistral_regex to check if a model is a base Mistral model."""
    import huggingface_hub

    with _patch_hf_hub_download():
        # This should NOT raise or make a network call - it should return a mock
        result = huggingface_hub.hf_api.model_info("memory://some/path")
        assert result.id == "monkeypatched"
        assert result.tags is None
# ---
def _resolve_vllm_backend(
    mode: Literal["native", "docker"],
    *,
    docker_image: str | None,
    docker_run_args: list[str] | None,
) -> VllmServerBackend:
    if mode == "docker":
        return DockerVllmServerBackend(docker_image, docker_run_args)
    if mode == "native":
        return NativeVllmServerBackend()
    raise ValueError(f"Unknown vLLM mode {mode!r}; expected 'native' or 'docker'.")
# ---
def table_row(self, content):
        """Rendering a table row. Like ``<tr>``.

        :param content: content of current table row.
        """
        return '<tr>\n%s</tr>\n' % content
# ---
def __ne__(self, other):
        """
        Returns true if both objects are not equal
        """
        return not self == other
# ---
def template_cv(minstitch, maxstitch):
    return "minstitch: %d, maxstitch: %d" % (minstitch, maxstitch), ["--min_overlap", str(minstitch), "--max_overlap", str(maxstitch)]
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        k_up, k_down = maybe_rng_split(key, 2)
        x = self.up_proj(x, key=k_up)
        x = self.act_fn(x)
        x = self.down_proj(x, key=k_down)
        return x
# ---
def __init__(self, config_path=CONFIG_PATH):
        self.config = configparser.ConfigParser(allow_no_value=True)
        self.config.read(config_path)
# ---
def test_default_returns_local_client():
    """When no context is set, should return LocalClient."""
    with patch("iris.client.client.get_iris_ctx", return_value=None):
        with patch("ray.is_initialized", return_value=False):
            client = current_client()
            assert isinstance(client, LocalClient)
# ---
def execute(self):
        with self._volumes(), self._password_file():
            yield self._start_helper()
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetDepartmentCount(), 9)
# ---
def test_token_uri(self):
        self.assertEqual(self.xe.token_uri, '/auth/token-services')
# ---
def load_llama3_tokenizer() -> PreTrainedTokenizer:
    """
    Load the base llama3 tokenizer.

    Returns:
        The llama3 tokenizer instance

    Raises:
        OSError, GatedRepoError, HTTPError: If access to the tokenizer is not available
    """
    return AutoTokenizer.from_pretrained(llama3_tokenizer_hf_path)
# ---
def visit_UnaryOp(self, node: ast.UnaryOp) -> ast.UnaryOp:
        node.op = self._maybe_swap(node.op, _UNARY_OPS)
        self.generic_visit(node)
        return node
# ---
def compute_svm_cv(K, y, C=100.0, n_folds=5,
                   scoring=balanced_accuracy_scoring):
    """Compute cross-validated score of SVM with given precomputed kernel.
    """
    cv = StratifiedKFold(y, n_folds=n_folds)
    clf = SVC(C=C, kernel='precomputed', class_weight='auto')
    scores = cross_val_score(clf, K, y,
                             scoring=scoring, cv=cv)
    return scores.mean()
# ---
def poly(seq_of_zeros: NamedArray | ArrayLike) -> NamedArray:
    """Named version of [jax.numpy.poly][].

    If ``seq_of_zeros`` is not a [haliax.NamedArray][], the returned coefficient axis
    is named ``degree``.
    """

    (roots,) = unwrap_namedarrays(seq_of_zeros)
    result = jnp.poly(roots)
    axis = _poly_axis_from_input(seq_of_zeros, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def tale_deletion(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'deletion',
        'purpose': 'repression',
        'method': 'TALEN',
        'zygosity': 'heterozygous'
    }
# ---
def stack(axis: AxisSelector, arrays: Sequence[NamedArray]) -> NamedArray:
    """Version of [jax.numpy.stack][] that returns a NamedArray"""
    if isinstance(axis, str):
        axis = Axis(axis, len(arrays))
    if len(arrays) == 0:
        return zeros(axis)
    arrays = [a.rearrange(arrays[0].axes) for a in arrays]
    return NamedArray(jnp.stack([a.array for a in arrays], axis=0), (axis,) + arrays[0].axes)
# ---
def add(self, sample):
        self.examplers.append(sample)
# ---
def test_scoring_autocomplete():
    assert score_autocomplete('}}]])})]') == 288957
    assert score_autocomplete(')}>]})') == 5566
    assert score_autocomplete('}}>}>))))') == 1480781
# ---
def test_is_server_error(self):
        self.assertFalse(status.is_server_error(499))
        self.assertFalse(status.is_server_error(600))

        for i in range(500, 599):
            self.assertTrue(status.is_server_error(i))
# ---
def get_lm_head(self) -> hax.NamedArray:
        """
        The language modeling head of the model. Should have shape {Embed, Vocab}.
        """
        raise NotImplementedError("get_lm_head not implemented")
# ---
def test_mup_coordinate_check_is_width_invariant():
    result = coord_check(widths=(32, 128, 512), steps=120, base_lr=3e-3)
    mup_span = result["mup_span"]
    ctrl_span = result["ctrl_span"]

    if ctrl_span < 1e-5:
        assert mup_span <= ctrl_span + 1e-6, f"Î¼P not at least as invariant: {result}"
    else:
        assert mup_span <= 0.6 * ctrl_span, f"Î¼P did not improve width invariance enough.\n{result}"
# ---
def the_variable_key_is_value(key, value):
    variables[key] = eval(value)
# ---
def destroy(self):
        """Remove all signal-connections."""
        self.autocomp.clear_words()
        self.autocomp.clear_widgets()
        self.main_controller.store_controller.disconnect(self._store_loaded_id)
        if getattr(self, '_cursor_changed_id', None):
            self.store_cursor.disconnect(self._cursor_changed_id)
        if self._unitview_id:
            self.main_controller.unit_controller.view.disconnect(self._unitview_id)
# ---
def model_type(self):
        """Return our custom GMM-enabled model type"""
        return MixtralLMHeadModelGMM
# ---
def _convert_to_jnp(v, dtype):
    import torch

    # we'd rather not convert to float32 to conserve memory, so we convert direct to jax.numpy
    with use_cpu_device():
        if v is None:
            return None
        elif v.dtype == torch.bfloat16:
            arr = jax.numpy.array(v.cpu().view(torch.float16).numpy()).view(jax.numpy.bfloat16)
        else:
            arr = jax.numpy.array(v.cpu().numpy())

        if dtype is not None:
            arr = arr.astype(dtype)

        return arr
# ---
def test_get_names(self):
        self.assertEqual(self.config.get_names(), ['å±±ç”°', 'ä½è—¤'])
# ---
def test_compute_bracket_mask_open_paren(tokenizer):
    mask = compute_bracket_mask("f(", tokenizer)
    # ) should be allowed (matches open paren).
    assert float(mask[tokenizer.encode_char(")")]) == 1.0
    # ] and } should be blocked (wrong bracket type).
    assert float(mask[tokenizer.encode_char("]")]) == 0.0
    assert float(mask[tokenizer.encode_char("}")]) == 0.0
# ---
def matchfn(self, f):
        return True
# ---
def _schedule(n):
        """Exponential anneal with flat start."""
        return jnp.clip(max_prob * jnp.exp(-decay * (n - flat_start)), min_prob, max_prob)
# ---
def _handle_gl_func(name, args=[], restype=None):
    _handle_func(name, args, restype, errcheck=None, ctx=MpvOpenGLCbContext)
# ---
def __init__(self):
        self._actor_pool: list[ActorPoolMember] = []
# ---
def init_log(self, tail: int | None = None) -> str:
        """Return bootstrap log (empty if not yet initializing)."""
        lines = self._log_lines[-tail:] if tail else self._log_lines
        return "\n".join(lines)
# ---
def __setitem__(self, key, value):
        self._data[id(key)] = [key, value]
# ---
def main():
    if os.getenv("CI", None) is not None:
        logger.info("Skipping experiment execution on CI environment, needs HF access.")
        return

    for step in build_steps():
        executor_main(steps=[step])
# ---
def i_enable_prop(prop):
    exec(f"bpy.context.{prop} = True")
# ---
def _multislice_info_from_head(head: SliceInfo, slice_id: int, num_slices: int) -> MultisliceInfo:
    """
    Create a MultisliceInfo object from the head slice info and the slice ID and number of slices.
    """
    return MultisliceInfo(
        coordinator_ip=head.ip_address,
        slice_id=slice_id,
        num_slices=num_slices,
        port=8081,  # default port for megascale
    )
# ---
def make_floating_point_trainable_filter(is_trainable: FilterTree) -> FilterTree:
    """
    Combines the is_trainable filter with a filter that only allows floating point parameters.
    """

    def is_trainable_and_floating_point(x):
        if x is True:
            return is_inexact_arrayish
        elif x is False:
            return False
        else:
            return lambda y: is_inexact_arrayish(y) and x(y)

    return haliax.tree_util.tree_map(is_trainable_and_floating_point, is_trainable)
# ---
def onCurrentTabChanged(self, index, tabs=None):
        if tabs is None:
            tabs = self.tabs
        widget = tabs.widget(index)
        try:
            signal = widget.shown
        except AttributeError:
            pass
        else:
            signal.emit()
# ---
def _update_state(self, result):
        super()._update_state(result)
        if isinstance(result, TemplateError):
            self._state = None
            return
        self._state = result.lower() in ("true", STATE_ON)
# ---
def add_worker(ctx, tpu_type, capacity, name):
    """Add manual TPU worker to cluster."""
    config_obj = ctx.obj.config_obj
    print(f"Adding {tpu_type} worker with {capacity} capacity...")
    _add_manual_worker(config_obj, tpu_type, capacity, name)
    print("Worker added successfully!")
# ---
def test_filters_for_instance_with_ip_v6(self):
        self.flags(use_ipv6=True)
        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)
        rulesv4, rulesv6 = self.fw._filters_for_instance("fake", network_info)
        self.assertEquals(len(rulesv4), 2)
        self.assertEquals(len(rulesv6), 1)
# ---
def __ne__(self, other: object) -> CompareExpr:  # type: ignore[override]
        return CompareExpr(self, _to_expr(other), "ne")
# ---
def template_paired_end(bool):
    # bool, paired_end
    if bool:
        return "paired_end\t{}".format(str(bool)), None
    else:
        return "paired_end\t{}".format(str(bool)), ["-SE"]
# ---
def _to_list_of_dicts(batch: dict) -> list[dict]:
    """
    Convert a batch of dictionaries to a list of dictionaries, suitable for writing to a cache.
    """
    keys = list(batch.keys())
    values = list(batch.values())
    num_rows = len(values[0])
    return [{key: values[i][j] for i, key in enumerate(keys)} for j in range(num_rows)]
# ---
def BufferIsUsable( buffer_object ):
  return not BufferModified( buffer_object ) or HiddenEnabled( buffer_object )
# ---
def trace_me():
            pass
# ---
def _declares_service(self, source):
    with open(source) as thrift:
      return any(line for line in thrift if self.SERVICE_PARSER.search(line))
# ---
def __init__(self, df1 = 1, df2 = 1, lmbda = 0):
        d1 = NoncentralChiSquareDistr(df1, lmbda) / df1
        d2 = ChiSquareDistr(df2) / df2
        super(NoncentralFDistr, self).__init__(d1, d2)
        self.df1 = df1
        self.df2 = df2
        self.lmbda = lmbda
# ---
def __init__(self, start: int = 0):
        self._value = start
# ---
def getName(self):
        return "NoncBeta({0},{1},{2})".format(self.alpha, self.beta, self.lmbda)
# ---
def create_test_entrypoint():
    """Create a simple test entrypoint."""
    from dataclasses import dataclass

    @dataclass
    class TestEntrypoint:
        callable: object
        args: tuple = ()
        kwargs: dict | None = None

        def __post_init__(self):
            if self.kwargs is None:
                self.kwargs = {}

    def test_fn():
        print("Hello from test")

    return TestEntrypoint(callable=test_fn)
# ---
def make_tpu_worker_config(generation: str, count: int, min_workers: int = 4) -> dict:
    """Create TPU worker configuration."""
    _, config = next(iter(make_tpu_slice_config(generation, count, min_workers).items()))
    return {"tpu_worker": config}
# ---
def calculate_information(self):
        """
        http://en.wikipedia.org/wiki/Information_ratio
        """
        return information_ratio(self.algorithm_returns,
                                 self.benchmark_returns)
# ---
def _check_worker_timeouts(self) -> None:
        """Check for worker timeouts and send kill RPCs for affected tasks."""
        # State computes failed workers and marks them atomically under lock
        tasks_to_kill = self._state.check_worker_timeouts(self._config.worker_timeout)

        # Send kill RPCs outside lock
        if tasks_to_kill:
            self.kill_tasks_on_workers(tasks_to_kill)
# ---
def obj__formdata(self):
                js = Attr('./td/a[1]', 'onclick', default=None)(self)
                if js is None:
                    return
                args = re.search(r'\((.*)\)', js).group(1).split(',')

                form = args[0].strip().split('.')[1]
                idx = args[2].strip()
                idroot = args[4].strip().replace("'", "")
                return (form, idx, idroot)
# ---
def compute_document_hashes(batch: pa.RecordBatch) -> pa.RecordBatch:
        pipeline = [
            Transformation.ResolveIds(text_col=config.text_field, id_col="id", output_col="resolved_id"),
            Transformation.Hash(input_col=config.text_field, output_col="hash", algo=dupekit.HashAlgorithm.Xxh3_128),
            Transformation.SelectColumns(columns=["hash", "resolved_id"]),
        ]
        return dupekit.transform(batch, pipeline)
# ---
def test_variable_with_encryption(self):
        """
        Test variables with encryption
        """
        Variable.set('key', 'value')
        session = settings.Session()
        test_var = session.query(Variable).filter(Variable.key == 'key').one()
        self.assertTrue(test_var.is_encrypted)
        self.assertEqual(test_var.val, 'value')
# ---
def overlay_remove(self, overlay_id):
        self.command('overlay_remove', overlay_id)
# ---
def resources(self):
        return self._resources
# ---
def _initialize_global_tracker(config, run_id):
    if isinstance(config, Sequence):
        tracker = levanter.tracker.CompositeTracker([c.init(run_id) for c in config])
    else:
        tracker = config.init(run_id)

    levanter.tracker.set_global_tracker(tracker)
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        return self.device_flops(dtype) * self.count
# ---
def _tpu_rpa_available() -> bool:
        if tpu_ragged_paged_attention is None:
            return False
        if jax.default_backend() != "tpu":
            return False
        kind = str(getattr(jax.devices()[0], "device_kind", "")).lower()
        if "tpu v2" in kind or "tpu v3" in kind:
            return False
        return True
# ---
def logical_and(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logical_and(x1, x2)
# ---
def describe_array(arr):
    if isinstance(arr, NamedArray):
        return f"NamedArray(axes={arr.axes}, dtype={arr.dtype})"
    else:
        return f"ndarray(shape={arr.shape}, dtype={arr.dtype})"
# ---
def start(self):
        """Start the inference and batch processing threads"""
        logger.info("Starting inference context...")
        self.inference_thread.start()
        self.batch_thread.start()
# ---
def _pipeline() -> int:
        batches = pq.ParquetFile(small_parquet_path).iter_batches(batch_size=batch_size)
        return sum(len(dupekit.process_arrow_batch(b)) for b in batches)
# ---
def __del__(self):
        """Ensure cleanup when object is destroyed."""
        try:
            self.kill()
        except Exception:
            pass
# ---
def grad_fn(x_in, w_in, y_in):
        return jax.grad(loss_fn, argnums=(0, 1))(x_in, w_in, y_in)
# ---
def future_from_value(value):
    future = asyncio.Future()
    future.set_result(value)
    return future
# ---
def save(self):

        if self.instance.pk:
            return super(FarmworkForm, self).save()

        instance = super(FarmworkForm, self).save(commit=False)
        instance.slug = slugify(instance.get_job_fruit_display() + '-' + instance.get_job_role_display() + '-in-' + instance.loc_city)
        instance.save()

        return instance
# ---
def test_main_wrapper_loads_from_fsspec():
    with fsspec.open("memory://test.yaml", "w") as f:
        f.write(
            """
        project: test
        """
        )

    args = ["--config_path", "memory://test.yaml", "--x", "2"]

    @dataclasses.dataclass
    class Config:
        project: str
        x: int = 1

    @levanter.config.main(args=args)
    def main(config: Config):
        assert config.project == "test"
        assert config.x == 2

    main()
# ---
def _apply_block(self, block: Block, batch) -> bool:
        address_set = self._state.prepare_address_list(block)  # Prepare list for current block
        addresses_state = self._state.get_state_mainchain(address_set)
        if not block.apply_state_changes(addresses_state):
            return False
        self._state.put_addresses_state(addresses_state, batch)
        return True
# ---
def lang(self, val):
        self._lang = _EpubLang(val)
# ---
def date_represent(date_obj):
        """
            Represent a datetime object as string

            @param date_obj: the datetime object

            @todo: replace by S3DateTime method?
        """
        return date_obj.strftime("%d %B %Y, %I:%M%p")
# ---
def _apply_manual_overrides(ssh_config: SshConfig, manual: config_pb2.ManualProvider) -> SshConfig:
    return SshConfig(
        user=manual.ssh_user or ssh_config.user,
        key_file=manual.ssh_key_file or ssh_config.key_file,
        connect_timeout=ssh_config.connect_timeout,
    )
# ---
def run_jit_hooks_outside_step(self, info: StepInfo, cb_infos: Sequence[PyTree], force: bool = False):
        for s_hook, cb_info in zip(self.jit_hooks, cb_infos):
            if force or (info.step % s_hook.every == 0):
                s_hook.fn.on_step(info, cb_info)
# ---
def setLDAPDirectory(self, directory=None):
        if directory is None:
                self.directory = []
        else:
            try:
                with open(DIRECTORY, 'w+') as f:
                    f.write(str(directory))
                    self.directory = directory
            except OSError as e:
                raise
# ---
def manipulate(text):
            for key in rules:
                pattern = getattr(self.rules, key)
                m = pattern.match(text)
                if not m:
                    continue
                self.line_match = m
                out = getattr(self, 'output_%s' % key)(m)
                if out is not None:
                    return m, out
            return False
# ---
def double_emphasis(self, text):
        """Rendering **strong** text.

        :param text: text content for emphasis.
        """
        return '<strong>%s</strong>' % text
# ---
def _description_help(self, env):
        if self.help and env.lang:
            name = "%s,%s" % (self.model_name, self.name)
            trans = env['ir.translation']._get_source(name, 'help', env.lang)
            return trans or self.help
        return self.help
# ---
def inside_step(self, state: S, inside_info: InsideJitInfo[M]) -> CBInfo:
        """
        This function is called inside the JIT-compiled function. You have access to the `inside_info` which contains
        information about the gradients, updates, and other information that was computed during the step.
        Args:
            state:
            inside_info:

        Returns:

        """
        ...
# ---
def make_target(self) -> Callable[..., None]:
        """Create a thread target that carries the current context.

        Must be called from the main thread so copy_context() captures
        the right contextvars (iris_ctx, etc.).
        """
        ctx = copy_context()

        def target(stop_event: threading.Event) -> None:
            ctx.run(self._run, stop_event)

        return target
# ---
def add_plugin(self, route, policy):
		self._routes[route] = policy
# ---
def cancel_job_fetch(self):
        self.main_model.cancel_job_fetch()
# ---
def decode(x):
                text = x[self.text_key]
                audio_pointer = x[self.audio_key]
                audio = AudioTextUrlDataSource.resolve_audio_pointer(audio_pointer, self.sampling_rate)
                return (audio["array"], audio["sampling_rate"], text)
# ---
def has_a_finalist_role(self):
        return len(self.finalist_user_roles()) > 0
# ---
def axis_mapping(mapping: ResourceMapping, *, merge: bool = False, **kwargs):
    """Context manager for setting the global resource mapping"""
    mapping = dict(mapping)

    old_mapping = current_thread_local_mapping()
    if merge:
        mapping.update(old_mapping or {})

    if len(kwargs):
        mapping.update(kwargs)

    _mapping_holder.thread_data.resource_mapping = mapping
    try:
        yield
    finally:
        _mapping_holder.thread_data.resource_mapping = old_mapping
# ---
def train(self, x, y):
        data = dict(zip(self.loss.arguments, [y, x]))
        self.trainer.train_minibatch(data, outputs=[self.loss.output])
# ---
def _run(self):
        """Thread target - runs the worker with error handling."""
        try:
            self._create_and_run_worker()
            self.result_queue.put(("success", None))
        except Exception as e:
            logger.error(f"{self.__class__.__name__} failed", exc_info=True)
            self.result_queue.put(("error", e))
# ---
def get_vpn_services_on_host(self, context, host):
        """Get list of vpnservices.

            The vpnservices including related ipsec_site_connection,
            ikepolicy and ipsecpolicy on this host
        """
        cctxt = self.client.prepare()
        return cctxt.call(context, 'get_vpn_services_on_host', host=host)
# ---
def export(self, request):
        serializer = ConditionExportSerializer(self.get_queryset(), many=True)
        xml = ConditionRenderer().render(serializer.data)
        return XMLResponse(xml, name='conditions')
# ---
def variables(self):
        """Frozen dictionary of xray.Variable objects constituting this
        dataset's data
        """
        return Frozen(self._variables)
# ---
def _record_dry_run(self, step: ExecutorStep, action: str, reason: str, output_path: str) -> None:
        """Track dry-run decisions for summary output."""
        self._dry_run_plan.append((step.name, action, reason, output_path))
# ---
def pick_add_edge(g):
        u = nx.utils.arbitrary_element(g)
        possible_nodes = set(g.nodes())
        neighbors = list(g.neighbors(u)) + [u]
        possible_nodes.difference_update(neighbors)
        v = nx.utils.arbitrary_element(possible_nodes)
        return (u, v)
# ---
def run_logged(cmd: list[str] | str, **kwargs) -> subprocess.CompletedProcess:
    """Run a subprocess command with logging.

    Args:
        cmd: Command to run (list or string)
        **kwargs: Arguments passed to subprocess.run

    Returns:
        CompletedProcess result from subprocess.run
    """
    if isinstance(cmd, list):
        cmd_str = shlex.join(cmd)
    else:
        cmd_str = cmd

    logger.info(f"Running command: {cmd_str}")
    return subprocess.run(cmd, **kwargs)
# ---
def closeEvent(self, event):
        """
        Perform necessary operations before closing the window.
        """
        self.saveWindowState()
        #do any other thing before closing...
        event.accept()
# ---
def make_keyword():
    return Keyword(keyword=fake.domain_word())
# ---
def __call__(self, carry):
            return carry + self.w
# ---
def deploy_rpc_test_contract(deploy_client, name):
    contract_path, contracts = get_test_contract(f"{name}.sol")
    contract_proxy, _ = deploy_client.deploy_solidity_contract(
        name, contracts, libraries=dict(), constructor_parameters=None, contract_path=contract_path
    )

    return contract_proxy
# ---
def _track_training_step(self):
        """Called after each training step."""
        self.steps_completed += 1
# ---
def output_open_html(self):
        text = self.token['text']
        tag = self.token['tag']
        if self._parse_block_html and tag not in _pre_tags:
            text = self.inline(text, rules=self.inline.inline_html_rules)
        extra = self.token.get('extra') or ''
        html = '<%s%s>%s</%s>' % (tag, extra, text, tag)
        return self.renderer.block_html(html)
# ---
def __init__(self, path: str):
        self._path = path
# ---
def __missing__(self, key):
        try:
            return super().__missing__(key)
        except KeyError:
            return NotImplemented
# ---
def hashivault_approle_role_get(params):
    name = params.get('name')
    client = hashivault_auth_client(params)
    result = client.get_role(name, mount_point=params.get('mount_point'))
    return {'role': result}
# ---
def __or__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_or(self, other)
# ---
def __init__(self, cache: TreeCache[AudioTextDict]):
        super().__init__()
        self.cache = cache
# ---
def get_lm_head(self) -> hax.NamedArray:
        return self.embeddings.token_embeddings.weight
# ---
def test_without_private(self, mock_start, mock_stop):
        fake_cls = FakeTraceClassHideArgs()
        self.assertEqual(10, fake_cls._method(10))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def logical_cpu_core_count() -> int:
    """Returns the number of logical CPU cores available to the process."""
    num_cpus = os.getenv("SLURM_CPUS_ON_NODE", None)
    if num_cpus is not None:
        return int(num_cpus)

    try:
        return os.cpu_count() or 1
    except NotImplementedError:
        return 1
# ---
def to_hf_config(self) -> tuple[float, dict]:
        return self.theta, {
            "factor": self.factor,
            "low_freq_factor": self.low_freq_factor,
            "high_freq_factor": self.high_freq_factor,
            "original_max_position_embeddings": self.original_max_position_embeddings,
            "rope_type": "llama3",
        }
# ---
def test_composed_int(self):
        table = self.tables.some_table
        lx = (table.c.x + table.c.y).label("lx")
        self._assert_result(select([lx]).order_by(lx), [(3,), (5,), (7,)])
# ---
def add_parent(self, parent):
        self.parent = parent
        self.parent.calls = []
# ---
def fn():
        if done:
            return end - start
        else:
            return time.perf_counter() - start
# ---
def as_short_time(self) -> str:
        """Format as HH:MM:SS for log lines."""
        dt = datetime.fromtimestamp(self.epoch_seconds(), tz=timezone.utc)
        return dt.strftime("%H:%M:%S")
# ---
def __init__(self, buffer: LogRingBuffer):
        super().__init__()
        self._buffer = buffer
# ---
def get_x_from_y(self, y):
        if self.a == 0:
            return 0.0

        return 1.0 * (-self.c - self.b * y) / self.a
# ---
def setUp(self):
        self.escala = Escala('fixtures/escala.xml')
        self.dir = dirs.TestDir()
        self.maxDiff = None
# ---
def setUp(self):
        self.fd, self.path = tempfile.mkstemp()
# ---
def _apply_logit_soft_cap(logits: Float[Array, "B V"], logit_soft_cap: Optional[float]) -> Float[Array, "B V"]:
    if logit_soft_cap is None:
        return logits
    return jnp.tanh(logits / logit_soft_cap) * logit_soft_cap
# ---
def __repr__(self):
        return "{0.__class__.__name__}({0.subquery!r})".format(self)
# ---
def play_model(cfg, player):
    predfunc = OfflinePredictor(cfg)
    while True:
        score = play_one_episode(player, predfunc)
        print("Total:", score)
# ---
def _validate_job_finished(job):
    if job.status not in (STATUS.DONE, STATUS.FAILED, STATUS.ABORTED):
        raise JobNotDone("Job %r is %s" % (job.id, job.status))
# ---
def mock_image_cache():
    """Create mock ImageBuilder."""
    builder = Mock(spec=ImageBuilder)
    builder.build = Mock(
        return_value=BuildResult(
            image_tag="test-image:latest",
            build_time_ms=1000,
            from_cache=False,
        )
    )
    builder.protect = Mock()
    builder.unprotect = Mock()
    return builder
# ---
def run(self, fn: Callable, *args, name: str | None = None):
        if self.ray_options:
            remote_fn = ray.remote(**self.ray_options)(fn)
        else:
            remote_fn = ray.remote(max_retries=100)(fn)

        options: dict[str, Any] = {"scheduling_strategy": "SPREAD"}
        if name:
            options["name"] = name
        return remote_fn.options(**options).remote(*args)
# ---
def __init__(self) -> None:
            super().__init__()
            self.write_calls: list[int] = []
# ---
def model_capture_hook(info):
                # Make a copy of the model on the CPU.
                self.trained_model = jax.device_get(info.state.model)
# ---
def node_value(self, decode_str=False):
        return MpvNode.node_cast_value(byref(c_void_p(self.val)), self.format.value, decode_str)
# ---
def test_transaction_tlocal_engine_ctx_commit(self):
        fn = self._trans_fn()
        engine = engines.testing_engine(options=dict(
                                strategy='threadlocal',
                                pool=testing.db.pool))
        ctx = engine.begin()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def __call__(self, fn):
        return self.matchfn(fn)
# ---
def main(config: ServerConfig):
    print("ServerConfig:", config)

    config = standardize_config(config)

    global server
    server = Server(config)

    debug = os.environ.get("DEV") == "true"
    assert debug, "This function must be run in debug mode"
    port = config.port if config.port is not None else (5000 if debug else 80)
    app.run(host="0.0.0.0", port=port, debug=debug)
# ---
def do_pause(self):
        self.do_time_ctrl('pause')
# ---
def Vocab(self) -> Axis:
        pass
# ---
def i_duplicate_the_selected_objects():
    bpy.ops.object.duplicate_move()
    blenderbim.bim.handler.active_object_callback()
# ---
def __iter__(self):
        return self.iter_from_step(None)
# ---
def _is_printer_printing(printer: OctoprintPrinterInfo) -> bool:
    return (
        printer
        and printer.state
        and printer.state.flags
        and printer.state.flags.printing
    )
# ---
def deploymentconfig(self, config):
        ''' setter for deploymentconfig property '''
        self.dconfig = config
# ---
def compute_action(self, state, t):
        u_in_base, u_out = self.base_controller(state, t)

        err = self.sim.pressure - state
        self.I = self.I * (1 - self.decay) + err * self.decay

        pid_correction = self.K[0] * err + self.K[1] * self.I

        u_in = torch.clamp(u_in_base + pid_correction, min=0.0, max=100.0)
        self.sim(u_in, u_out, t)

        return u_in, u_out
# ---
def put(self, obj: Any):
        return ray.put(obj)
# ---
def native_value(self) -> datetime | None:
        """Return sensor state."""
        job: OctoprintJobInfo = self.coordinator.data["job"]

        if (
            not job
            or not job.progress.print_time
            or not _is_printer_printing(self.coordinator.data["printer"])
        ):
            return None

        read_time = self.coordinator.data["last_read_time"]

        return read_time - timedelta(seconds=job.progress.print_time)
# ---
def __post_init__(self):
        if self.max_samples_per_benchmark is not None and self.max_samples_per_benchmark < 0:
            raise ValueError("max_samples_per_benchmark must be non-negative or None")
# ---
def __init__(self, stream_parser, message):
        self._stream_parser = stream_parser
        self._message = message
        self._iter_rows = None
        self._num_items = self._message.row_count
        self._remaining = self._message.row_count
# ---
def test_within_range(self):
        """Test when create_time is within the given range"""
        self.assertTrue(check_create_time("2023-01-15 12:00:00 PDT", "2023-01-01", "2023-01-31"))
# ---
def run(self) -> dict[str, Any]:
        """Main entry point for replaying completions."""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            return loop.run_until_complete(self.run_async())
        finally:
            loop.close()
# ---
def test_class_method_skip(self, mock_start, mock_stop):
        self.assertEqual("foo", FakeTraceClassMethodSkip.class_method("foo"))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def _mpv_client_api_version():
    ver = backend.mpv_client_api_version()
    return ver>>16, ver&0xFFFF
# ---
def extract_cookies(self, response, request):
        self.ec_req, self.ec_r = request, response
# ---
def writestr(self):
        '''
        generate the midi data header and convert the list of
        midi_track objects in self_tracks into midi data and return it as a string_
        '''
        midi_str = self.write_m_thd_str()
        for trk in self.tracks:
            midi_str = midi_str + trk.get_bytes()
        return midi_str
# ---
def broadcast(self) -> "_PoolBroadcastProxy[T]":
        return _PoolBroadcastProxy(self)
# ---
def simhash(doc: dict[str, Any]) -> int:
    words = doc["body"].split()
    unique_words = sorted(set(words))
    return hash(" ".join(unique_words)) % 10000
# ---
def put(self, obj: Any) -> Any:
        """Identity operation - in-process, no serialization needed."""
        return obj
# ---
def __init__(self, conf, topic_name):
        self.topic_name = topic_name
        self.producer = Producer(conf)
        self.counter = 0
        self.running = True
# ---
def __truediv__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "truediv")
# ---
def area(self):
        raise NotImplemented
# ---
def the_feature_extractor(self) -> SequenceFeatureExtractor:
        return self.the_processor.feature_extractor
# ---
def counting_map(self, x):
        self.map_count += 1
        self.processed_ids.append(x["id"])
        return {**x, "processed": True}
# ---
def phoenixSystems(self):
        return OrderedDict(sorted(self.phoenixSystemDatabase.items(), key=lambda t: t[0]))
# ---
def list_available_configs() -> list[str]:
    """List all available cluster configurations."""
    infra_dir = Path("infra")
    if not infra_dir.exists():
        return []

    configs = []
    for yaml_file in infra_dir.glob("marin-*.yaml"):
        # Skip template file
        if yaml_file.name == "marin-cluster-template.yaml":
            continue
        configs.append(str(yaml_file))

    return sorted(configs)
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        self.metrics["artifact"] = {"path": artifact_path, "name": name, "type": type}
# ---
def stop(self):
        self._elapsed += time.time() - self._start_time
# ---
def __new__(cls, name, bases, members):
        cls.members = [v for k, v in members.items() if not k.startswith("__") and not callable(v)]
        return super().__new__(cls, name, bases, members)
# ---
def binrecv(self, timeout=None):
		if timeout is None:
			timeout = self.RECEIVE_TIMEOUT

		try:
			bindata = self.q.get(True, timeout)
		except Queue.Empty:
			raise RadioTimeout
		else:
			return bindata
# ---
def test_nunique(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df.A[2] = 0
            return df.A.nunique()

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        # test compile again for overload related issues
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# ---
def get_timerules(hid):
    """Gets list of timerules for given hid"""
    timerules= home_services.read_time_rules(hid)
    return jsonify(timerules)
# ---
def decode(self, token_ids, skip_special_tokens=True):
        words = []
        for tid in token_ids:
            token = self.TOKENS[tid]
            if skip_special_tokens and token in (self.bos_token, self.eos_token):
                continue
            words.append(token)
        return "".join(words)
# ---
def style_css(self):
        '''CSS stylesheet for the files that are generated by the EpubBuilder
        instance. Can be overwritten or extended, but not deleted.'''
        return self._style_css
# ---
def from_state_dict(self: M, state_dict: StateDict, prefix: str | None = None) -> M:
        out_blocks = []
        for i, block in enumerate(self.blocks):
            my_prefix = with_prefix(prefix, str(i))
            block = block.from_state_dict(state_dict, my_prefix)
            out_blocks.append(block)

        return eqx.tree_at(lambda m: m.blocks, self, out_blocks)
# ---
def add_hop(self, actor_id: str):
        """Record this actor in the path."""
        self.path.append(actor_id)
# ---
def _invoice_hook(self, cr, uid, picking, invoice_id):
        '''Call after the creation of the invoice'''
        return
# ---
def test_lots_of_ellipsis():
    partial_order = ("apple", ..., "banana", ..., "cherry", ...)
    candidates = ("banana", "orange", "cherry", "apple", "grape")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("apple", "banana", "orange", "cherry", "grape")
# ---
def init(
        cls,
        axis: AxisSpec,
        eps: float = 1e-5,
        *,
        use_weight: bool = True,
        use_bias: bool = True,
        dtype: jnp.dtype | None = None,
    ):
        if use_weight:
            weight = hax.ones(axis)
        else:
            weight = None

        if use_bias:
            bias = hax.zeros(axis)
        else:
            bias = None

        return cls(axis, weight, bias, eps, dtype)
# ---
def test_permute_errors_on_invalid_starting_dims_index(self):
    with self.assertRaisesRegexp(ValueError, r'Invalid permutation .*dims.*'):
      testing_utils.layer_test(
          keras.layers.Permute,
          kwargs={'dims': (0, 1, 2)}, input_shape=(3, 2, 4))
# ---
def _run() -> None:
        with remove_tpu_lockfile_on_exit():
            launch(model, evals, output_path, max_eval_instances, wandb_tags)
# ---
def get_user() -> str | None:
    return subprocess.check_output("whoami", shell=True).strip().decode("utf-8")
# ---
def test_image():
    """Build test image with cloudpickle."""
    if not build_test_image():
        pytest.skip("Failed to build test image")
    return TEST_IMAGE
# ---
def test_entrypoint_params_gpu():
    from fray.v2.ray_backend.backend import get_entrypoint_params

    request = JobRequest(
        name="gpu-job",
        entrypoint=Entrypoint.from_binary("train", []),
        resources=ResourceConfig(device=GpuConfig(variant="H100", count=4)),
    )
    params = get_entrypoint_params(request)
    assert params["entrypoint_num_gpus"] == 4.0
# ---
def _make_info():
        return {
            'driver_volume_type': 'iscsi',
            'data': {
                'volume_id': 1,
                'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                'target_portal': '127.0.0.1:3260,fake',
                'target_lun': None,
                'auth_method': 'CHAP',
                'auth_method': 'fake',
                'auth_method': 'fake',
            }
        }
# ---
def parse_typespec(thing):
    if isinstance(thing, basestring):
        return thing
    elif '$and' in thing:
        return '(' + ' AND '.join(map(parse_typespec, thing['$and'])) + ')'
    elif '$or' in thing:
        return '(' + ' OR '.join(map(parse_typespec, thing['$or'])) + ')'
    else:
        return 'Type spec could not be parsed'
# ---
def hparams_to_dict(hparams, **extra_hparams):
    if hparams is None:
        hparams_to_save = {}
    elif dataclasses.is_dataclass(hparams):
        hparams_to_save = dataclasses.asdict(hparams)
    else:
        hparams_to_save = dict(hparams)
    if extra_hparams:
        hparams_to_save.update(extra_hparams)
    return hparams_to_save
# ---
def new_queue(self, name: str) -> "HttpQueue":
        """Create or get a named queue, returns client."""
        if name not in self.queues:
            self.queues[name] = MemoryQueue()
        # Use client-accessible host instead of bind host
        client_host = self.get_client_host()
        return HttpQueue(host=client_host, port=self.port, queue_name=name)
# ---
def get_msg(self):
        return self._saved_msg
# ---
def _grug_125m_config() -> GrugformerH2HConfig:
    return GrugformerH2HConfig(
        max_seq_len=2048,
        hidden_dim=512,
        intermediate_dim=1792,
        num_layers=6,
        num_heads=8,
        num_kv_heads=8,
        head_dim=None,
    )
# ---
def compute_dapo_loss(
    loss_objective: jax.Array,
    loss_masks: jax.Array,
) -> jax.Array:
    """Compute DAPO-like loss (global token normalization).

    Divides by total tokens across all examples in the batch, not per-example.
    """
    return -1 * jnp.mean(jnp.sum(loss_objective * loss_masks, axis=1) / jnp.sum(loss_masks))
# ---
def test_without_private(self, mock_start, mock_stop):
        fake_cls = FakeTraceWithMetaclassHideArgs()
        self.assertEqual(10, fake_cls._method(10))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def binding_genetic_modification(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'CRISPR dCas',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def test_permutation_is_bijective_over_full_range(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    indices = jnp.arange(length)
    permuted = permutation(indices)
    # Check if all elements are unique, which is a necessary condition for a bijective function
    assert len(jnp.unique(permuted)) == length
# ---
def test_simple_only_fd(self, testdir):
        testdir.makepyfile("""
            import os
            def test_x():
                os.write(1, "hello\\n".encode("ascii"))
                assert 0
        """)
        result = testdir.runpytest_subprocess()
        result.stdout.fnmatch_lines("""
            *test_x*
            *assert 0*
            *Captured stdout*
        """)
# ---
def __rand__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_and(other, self)
# ---
def create_vm_record(self, conn, os_type, name):
        instances = conn.list_instances()
        self.assertEquals(instances, [name])

        # Get Nova record for VM
        vm_info = conn.get_info({'name': name})
        # Get XenAPI record for VM
        vms = [rec for ref, rec
               in xenapi_fake.get_all_records('VM').iteritems()
               if not rec['is_control_domain']]
        vm = vms[0]
        self.vm_info = vm_info
        self.vm = vm
# ---
def _ensure_verifiers_installed(self):
        """Ensure verifiers package is installed."""
        try:
            import verifiers  # noqa: F401
        except ImportError as e:
            raise ImportError(
                "The 'verifiers' package is required to use PrimeIntellectEnv. "
                "Please install it with: uv pip install 'marin[rl]' or uv pip install verifiers"
            ) from e
# ---
def __init__(self, key):
            self.weight = jnp.ones((2, 2))
            self.bias = jnp.ones(2)
# ---
def takephoto():
	camera.capture(imagefile)
	image1 = Image.open(imagefile)
	return image1
# ---
def send_token(self, token: Token) -> str:
        """Queue token for processing - returns immediately."""
        self._queue.put(token)
        return "queued"
# ---
def visit_Name(self, node: ast.Name) -> ast.Name:
        if node.id in self.mapping:
            return ast.Name(id=self.mapping[node.id], ctx=node.ctx)
        return node
# ---
def setcommission(self, commission=0.0, margin=None, mult=1.0, name=None):
        comm = CommissionInfo(commission=commission, margin=margin, mult=mult)
        self.comminfo[name] = comm
# ---
def __init__(self, instance: Any, lock: threading.Lock, context):
        self._instance = instance
        self._lock = lock  # Serializes all method calls
        self._context = context
# ---
def _format_multiplier_label(mult: float) -> str:
    s = f"{mult:.6g}"
    s = s.rstrip("0").rstrip(".") if "." in s else s
    return s.replace(".", "_")
# ---
def function_def_from_tf_function(c_func):
  """Converts a SWIG-wrapped TF_Function* to a FunctionDef proto."""
  with c_api_util.tf_buffer() as buf:
    c_api.TF_FunctionToFunctionDef(c_func, buf)
    data = c_api.TF_GetBuffer(buf)
  fdef = function_pb2.FunctionDef()
  fdef.ParseFromString(compat.as_bytes(data))
  return fdef
# ---
def test_fn():
        print("Hello from test")
# ---
def save_apartments(self, apartments_dto):
        apartments_dtos = []
        for apartment in apartments_dto:
            apartment_saved = self.save_apartment(apartment, send_event=False)
            apartments_dtos.append(apartment_saved)
        self.send_config_change_event('save')
        return apartments_dtos
# ---
def __init__(self, difficulty: str = "medium"):
        if difficulty not in self.DIFFICULTY_RANGES:
            raise ValueError(f"Unknown difficulty: {difficulty}. Must be one of {list(self.DIFFICULTY_RANGES.keys())}")
        self.difficulty = difficulty
        self.min_val, self.max_val = self.DIFFICULTY_RANGES[difficulty]
# ---
def device_flops_for_jax_device(jax_device_kind: str, dtype: str = "bf16") -> float | None:
    """Get peak FLOP/s given a JAX device kind string.

    Args:
        jax_device_kind: JAX device kind string (e.g., "TPU v4", "NVIDIA H100 80GB HBM3")
        dtype: Data type (e.g., "bf16", "fp16")

    Returns:
        Peak FLOP/s, or None if device/dtype unknown
    """
    fray_device_type = jax_device_kind_to_fray_device_type(jax_device_kind)
    return device_flops(fray_device_type, dtype)
# ---
def _match_less_than(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) < str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def convert_ids_to_tokens(self, token_id):
        """Convert token ID to token string (BPE format)."""
        if isinstance(token_id, list):
            return [self.TOKENS[tid] for tid in token_id]
        return self.TOKENS[token_id]
# ---
def sendShutdownSignal(self):
		requests.post('http://{}:{}/shutdown'.format(self.ip, self.port))
# ---
def as_local(self) -> "IrisConfig":
        """Create local variant of this config.

        Returns:
            New IrisConfig configured for local testing
        """
        local_proto = make_local_config(self._proto)
        return IrisConfig(local_proto)
# ---
def ceil(a: A) -> A:
    return wrap_elemwise_unary(jnp.ceil, a)
# ---
def test_is_cloneable_share_goodformat4(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://netapp.com/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def r(a=4):
            for _ in range(a):
                yield rx.randint(1, 10)
# ---
def _strip_properly_formatted_commas(expr: str):
    # We want to be careful because we don't want to strip tuple commas
    p1 = re.compile(r"(\d)(,)(\d\d\d)($|\D)")
    while True:
        next_expr = p1.sub("\\1\\3\\4", expr)
        if next_expr == expr:
            break
        expr = next_expr
    return next_expr
# ---
def config(self) -> MConfig:
        pass
# ---
def h_fs_unlink(_,path): os.unlink(path)
# ---
def block_size():
    return 10
# ---
def test_braces_in_config_values_preserved(self, config_with_special_chars: config_pb2.BootstrapConfig):
        """Config values with braces are preserved in output."""
        script = _build_bootstrap_script(config_with_special_chars, vm_address="10.0.0.1")

        assert "v1.0-{tag}" in script
        assert "/cache/{project}/iris" in script
        assert "Hello {world}!" in script
        assert '{"key": "value"}' in script
# ---
def mnist_many_cols_gbm_large():
  train = h2o.import_file(path=pyunit_utils.locate("bigdata/laptop/mnist/train.csv.gz"))
  train.tail()


  gbm_mnist = H2OGradientBoostingEstimator(ntrees=1,
                                           max_depth=1,
                                           min_rows=10,
                                           learn_rate=0.01)
  gbm_mnist.train(x=range(784), y=784, training_frame=train)
  gbm_mnist.show()
# ---
def autoscaler(self) -> AutoscalerProtocol | None:
        """Get the autoscaler instance, if autoscaling is enabled."""
        ...
# ---
def bar(x: i32["batch"]):  # type: ignore  # noqa: F722
        pass
# ---
def test_group_by_composed(self):
        table = self.tables.some_table
        expr = (table.c.x + table.c.y).label("lx")
        stmt = (
            select([func.count(table.c.id), expr])
            .group_by(expr)
            .order_by(expr)
        )
        self._assert_result(stmt, [(1, 3), (1, 5), (1, 7)])
# ---
def _iter_mesh_axes(spec_entry):
        if spec_entry is None or spec_entry is PartitionSpec.UNCONSTRAINED:
            return
        if isinstance(spec_entry, str):
            yield spec_entry
        else:
            for item in spec_entry:
                if item is None or item is PartitionSpec.UNCONSTRAINED:
                    continue
                yield item
# ---
def test_corrupt_program_zero_steps(bank):
    source = CORPUS[0]
    corrupted, mutations = corrupt_program(source, num_steps=0, bank=bank)
    assert corrupted == source
    assert mutations == []
# ---
def create(self, cr, user, vals, context=None):
        if ('name' not in vals) or (vals.get('name')=='/'):
            seq_obj_name =  self._name
            vals['name'] = self.pool.get('ir.sequence').get(cr, user, seq_obj_name)
        new_id = super(stock_picking, self).create(cr, user, vals, context)
        return new_id
# ---
def test_tokens_are_equal(self):
        # It should fail if the tokens aren't equal length.
        self.assertFalse(csrf._tokens_are_equal('a', 'ab'))
        # It should fail if the tokens are different.
        self.assertFalse(csrf._tokens_are_equal('abcde', 'abcdf'))
        # It should succeed if the tokens are the same.
        self.assertTrue(csrf._tokens_are_equal('abcde', 'abcde'))
# ---
def value(self) -> NamedArray:
        """Materialize this reference view as a `NamedArray`."""
        _, axes_spec, index_tuple = self._prepare(Ellipsis)
        result = self._ref[tuple(index_tuple)]
        return named(result, axes_spec)
# ---
def _get_step_size(self, budget: float) -> int:
        """Get hidden_size search step size based on budget."""
        if budget > self.budget_step_threshold:
            return self.large_budget_step_size
        return self.small_budget_step_size
# ---
def test_all_network_labels(self):
        CONF.network_label_regex = '.*'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(3, len(ip))
        self.assertTrue('10.123.123.123' in ip)
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)
# ---
def __contains__(self, item):
        for _, element in self._queue:
            if item == element:
                return True
        return False
# ---
def shutdown(self):
        """Shutdown the inference context."""
        self.inference_context.shutdown()
# ---
def static_proxy(path):
    if os.environ.get("DEV") == "true":
        return proxy_to_dev_server(path)
    return send_from_directory(app.static_folder, path)
# ---
def _print_deleted(matches, after_match):
            deleted = []
            for keydir in (self.key.ACC, self.key.PEND, self.key.REJ):
                deleted.extend(list(
                    set(matches.get(keydir, [])).difference(
                        set(after_match.get(keydir, []))
                    )
                ))
            for key in sorted(deleted):
                print('Key for minion {0} deleted.'.format(key))
# ---
def protect(self, tag: str) -> None:
        """No-op for local provider (no eviction)."""
        del tag
# ---
def run_check(runnable):
    sys.stderr.write('Test command: %s\n' % ' '.join(runnable))

    try:
        ret = subprocess.check_call(runnable)
    except subprocess.CalledProcessError as err:
        return err.returncode

    return ret
# ---
def root(self) -> Path:
        return self._root
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "model.embed_tokens"}
# ---
def test_count_statements_simple():
    tree = ast.parse("x = 1")
    # Module body has one Assign statement.
    assert count_statements(tree) == 1
# ---
def getPreprocessFunction(preprocessType):

    if preprocessType == "dummy":
        return dummyPreprocessInput
    elif preprocessType == "mobilenet":
        return mobilenet.preprocess_input
    elif preprocessType == "imagenet":
        return imagenet_utils.preprocess_input
    else:
        raise Exception(preprocessType + " not supported")
# ---
def move_tip(self, number):
        self.tip = self.blocks[number]
# ---
def local_client():
    """Create a LocalClusterClient-backed IrisClient for true E2E testing.

    This fixture starts a real Controller and Worker with in-process execution,
    ensuring WorkerPool tests go through the full job submission infrastructure.
    """
    client = IrisClient.local()
    yield client
    client.shutdown(wait=True)
# ---
def _remove_textbox(self, textbox):
        """Remove the given L{TextBox} from the list of widgets to do
            auto-correction on.
            """
        if not hasattr(self, '_textbox_insert_ids'):
            return
        # Disconnect the "insert-text" event handler
        textbox.disconnect(self._textbox_insert_ids[textbox])

        self.widgets.remove(textbox)
# ---
def compute_ppo_loss(
    loss_objective: jax.Array,
    loss_masks: jax.Array,
) -> jax.Array:
    """Compute PPO loss (per-example normalization)."""
    return -1 * jnp.mean(jnp.sum(loss_objective * loss_masks, axis=1) / jnp.sum(loss_masks, axis=1))
# ---
def __eq__(self, other):
        return (isinstance(other, EntityRef) and
                self.entity_id == other.entity_id)
# ---
def scan_compatible_fn(_, x):
        del _
        return None, fn(x)
# ---
def _require_docker_available() -> None:
    if not os.path.exists("/var/run/docker.sock"):
        raise RuntimeError(
            "Docker socket not available at /var/run/docker.sock. "
            "This job requires docker-alongside-docker (mount the socket into the Ray container)."
        )
    if shutil.which("docker") is None:
        raise RuntimeError(
            "`docker` CLI not found on PATH. Install the Docker client in the Ray image to run vLLM as a sidecar."
        )
# ---
def __post_init__(self):
        # Normalize legacy single-array segment_ids to a tuple for consistency
        if self.segment_ids is not None and not isinstance(self.segment_ids, tuple):
            warnings.warn("Storing segment_ids as a single NamedArray is deprecated. Use a tuple instead.")
            object.__setattr__(self, "segment_ids", (self.segment_ids, self.segment_ids))
# ---
def set_broadcast_tx(self, broadcast_tx):
        with self.lock:
            self.tx_pool.set_broadcast_tx(broadcast_tx)
# ---
def _run_binary(entry: BinaryEntrypoint) -> None:
    subprocess.run(
        [entry.command, *entry.args],
        check=True,
        capture_output=True,
        text=True,
    )
# ---
def test_set_enable_host_disable(self):
        self._test_host_action(self.conn.set_host_enabled, False, 'disabled')
# ---
def _resolved_shared_axis_mapping(self):
        mapping = dict(DEFAULT_SHARED_MAPPING)
        for logical, physical in self.shared_mapping.items():
            mapping[logical] = _norm(physical)

        return mapping
# ---
def _block(s):
    """Block until sentinel is signalled. Pass a SentinelFile instance."""
    s.wait()
# ---
def model_name_or_path(model: ModelConfig) -> str:
        """Return a reference Levanter can read without staging to local disk."""
        if model.path is None:
            return model.name
        return model.path
# ---
def lr_scale(self):
        return 1 / hax.axis_size(self.In)
# ---
def saveConfig(plugin_name, data):
    """
    @type plugin_name: str
    @type data: object
    """
    pass
# ---
def _massage_env(env):
    # Ray pretends it's running in a TTY, which leads to a ton of log spam from tqdm.
    # Levanter uses tqdm_loggable, which tries to sniff out the TTY, but it doesn't work with Ray.
    # So we force it
    env = dict(env)
    if "TERM" not in env:
        env["TERM"] = "dumb"

    if "TF_CPP_MIN_LOG_LEVEL" not in env:
        # Suppress TensorFlow logs, which can be very verbose
        env["TF_CPP_MIN_LOG_LEVEL"] = "3"

    return env
# ---
def get_bootstrap_host(self) -> Host:
        return list(self.deployment.hosts)[0]
# ---
def create_after_delay():
        time.sleep(0.05)
        sentinel.signal()
# ---
def set_class_name(self, cls, name):
        super(Many2one, self).set_class_name(cls, name)
        # determine self.delegate
        if not self.delegate:
            self.delegate = name in cls._inherits.values()
# ---
def test_build_runtime_env_gpu_clears_jax_platforms():
    from fray.v2.ray_backend.backend import build_runtime_env

    request = JobRequest(
        name="gpu-test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=GpuConfig(variant="H100")),
    )
    env = build_runtime_env(request)
    assert env["env_vars"]["JAX_PLATFORMS"] == ""
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        for tracker in self.loggers:
            tracker.log_artifact(artifact_path, name=name, type=type)
# ---
def log(self, metrics: typing.Mapping[str, typing.Any], *, step: Optional[int], commit: Optional[bool] = None):
        """
        Log metrics to the tracker. Step is always required.

        Args:
            metrics: Metrics to log
            step: Step to log at
            commit: Whether to commit the metrics. If None, uses the default for the tracker.
        """
        pass
# ---
def format_category(category):
    return categories_to_out_names[category]
# ---
def output_emphasis(self, m):
        text = m.group(2) or m.group(1)
        text = self.output(text)
        return self.renderer.emphasis(text)
# ---
def test_list_tasks(worker):
    """Test listing all tasks."""
    requests = [create_run_task_request(task_id=JobName.root("test-job").task(i).to_wire()) for i in range(3)]

    for request in requests:
        worker.submit_task(request)

    tasks = worker.list_tasks()
    assert len(tasks) == 3
# ---
def to_str(self):
        """
        Returns the string representation of the model
        """
        return pformat(self.to_dict())
# ---
def test_duplicate_trace_disallow(self, mock_start, mock_stop):

        @profiler.trace("test")
        def trace_me():
            pass

        self.assertRaises(
            ValueError,
            profiler.trace("test-again", allow_multiple_trace=False),
            trace_me)
# ---
def __init__(self, size=0):
        self.size = size
        self.id = hash(self)
        self.name = None
# ---
def test_impl(df):
            A = df.A.str.split(',')
            B = A.str.get(1)
            return B
# ---
def keyModifiedToggle(self, cell, path, model, col):
        """When the user changes a checkbox field (boolean field)"""

        model[path][col] = not model[path][col]
        return
# ---
def __rshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.right_shift(self, other)
# ---
def script_message_to(self, target, *args):
        self.command('script_message_to', target, *args)
# ---
def the_object_name_has_a_type_representation_of_context(name, type, context):
    ifc = an_ifc_file_exists()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    context, subcontext, target_view = context.split("/")
    assert ifcopenshell.util.representation.get_representation(
        element, context, subcontext or None, target_view or None
    )
# ---
def unprotect(self, tag: str) -> None:
        """Remove protection from an image (job completed)."""
        ...
# ---
def loss_pallas(x_raw, y_raw, w_raw):
        return fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
            x_raw,
            y_raw,
            w_raw,
            reduction="mean",
            logsumexp_weight=logsumexp_weight,
            block_sizes=block_sizes,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
            implementation="pallas_tpu",
        )
# ---
def make_render_children(separator: str) -> Render:
    def render_children(
        node: RenderTreeNode,
        context: RenderContext,
    ) -> str:
        return separator.join(child.render(context) for child in node.children)

    return render_children
# ---
def __init__(self, uri, username, password, vminfo, vmid, irs):
        super(LibvirtCommand, self).__init__(vminfo, vmid, irs)
        self._uri = uri
        self._username = username
        self._password = password
# ---
def __neg__(self) -> "PyExpr": ...
# ---
def plugin_on_unpaused(self):
        xid = dbus.UInt32(get_toplevel_xid())
        flags = dbus.UInt32(InhibitFlags.IDLE)

        try:
            bus = dbus.SessionBus()
            obj = bus.get_object(self.DBUS_NAME, self.DBUS_PATH)
            iface = dbus.Interface(obj, self.DBUS_INTERFACE)
            self.__cookie = iface.Inhibit(
                self.APPLICATION_ID, xid, self.INHIBIT_REASON, flags)
        except dbus.DBusException:
            pass
# ---
def union_axes(a1: ShapeDict, a2: AxisSpec) -> ShapeDict: ...
# ---
def test_wrong_size_prev_cc_raises(self):
        with pytest.raises(nx.NetworkXError):
            G = self.undirected_G.copy()
            edge = self.pick_add_edge(G)
            insert = True
            prev_cc = self.undirected_G_cc.copy()
            prev_cc.pop(0)
            nx.incremental_closeness_centrality(G, edge, prev_cc, insert)
# ---
def __call__(self, x, *, key):
            return self.down(self.up(x), key=key)
# ---
def flatten_axes(axis: Axis, old_axes: Axis, new_axis: AxisSelector) -> Axis:
    pass
# ---
def setWidth(self, width):
        self.__width = width
# ---
def test_endpoints_exist(test_client):
    """Test that the endpoints are properly defined"""
    _, server = test_client
    routes = [route.path for route in server.app.routes]
    assert "/health" in routes
    assert "/v1/completions" in routes
    assert "/v1/chat/completions" in routes
# ---
def __init__(self,domain='rds.aliyuncs.com',port=80):
		RestApi.__init__(self,domain, port)
		self.AccountName = None
		self.DBInstanceId = None
		self.resourceOwnerAccount = None
# ---
def _get_random_inputs(config, override_Pos=None):
    """Generate random inputs for testing."""
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()

    return x, mask
# ---
def sink_attention_vanilla(
    query,
    key,
    value,
    sinks,
    sm_scale: float = 0.125,
    sliding_window: int | None = None,
    start_q=0,
):
    return sink_attention(
        query,
        key,
        value,
        sinks,
        sm_scale,
        sliding_window,
        start_q,
        attn_backend=None,
        block_size=None,
        inference=True,
    )
# ---
def find_step_containing_offset(self, offset: int) -> int:
        """
        Find the step that contains the given global data offset.
        """
        for seg in self.segments:
            if seg.offset <= offset < seg.offset + (seg.until - seg.start) * seg.value:
                return seg.start + (offset - seg.offset) // seg.value
        raise ValueError(f"Offset {offset} is beyond the last defined segment.")
# ---
def chart_title(self):
        """Returns a title for the chart."""
        if self.field:
            return 'Top filtered results for "{0:s}"'.format(self.field)
        return 'Top results for an unknown field after filtering'
# ---
def mock_image_cache():
    """Create mock ImageCache."""
    cache = Mock(spec=ImageCache)
    cache.build = Mock(
        return_value=BuildResult(
            image_tag="test-image:latest",
            build_time_ms=1000,
            from_cache=False,
        )
    )
    return cache
# ---
def logs_tail(self, handle: VllmServerHandle, *, max_lines: int = 200) -> str:
        raise NotImplementedError
# ---
def resolve(self, host, port, family=socket.AF_UNSPEC):
        addrinfo = socket.getaddrinfo(host, port, family)
        results = []
        for family, socktype, proto, canonname, address in addrinfo:
            results.append((family, address))
        return results
# ---
def init_worker(ctx, name):
    """Initialize Ray on a manual TPU worker."""
    config_obj = ctx.obj.config_obj
    print(f"Initializing Ray on worker {name}...")
    _initialize_manual_worker(config_obj.config_file, name)
    print("Worker initialized successfully!")
# ---
def dump(sql, *multiparams, **params):
            buf.write(util.text_type(sql.compile(dialect=engine.dialect)))
# ---
def test_scale_down_nonexistent_vm_group_is_noop(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """scale_down() on a nonexistent VM group does nothing."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        # Should not raise
        group.scale_down("nonexistent-slice")

        assert group.slice_count() == 0
# ---
def validate_is_stock_item(item_code, is_stock_item=None, verbose=1):
	if not is_stock_item:
		is_stock_item = frappe.db.get_value("Item", item_code, "is_stock_item")

	if is_stock_item != 1:
		msg = _("Item {0} is not a stock Item").format(item_code)

		_msgprint(msg, verbose)
# ---
def _no_ssl_required_on_debug(app, **kwargs):
    if app.debug or app.testing:
        os.environ['AUTHLIB_INSECURE_TRANSPORT'] = '1'
# ---
def _assert(self, retval, m1, m2, mock_calls):
        eq_(m1.mock_calls, mock_calls)
        if retval:
            eq_(m2.mock_calls, [])
        else:
            eq_(m2.mock_calls, mock_calls)
# ---
def test_fillna(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            B = df.A.fillna(5.0)
            return B.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
# ---
def number(self, val):
        self.num_ctrl.SetValue(str(val))
# ---
def uipath(self, f):
        return self._matcher.uipath(self._path + "/" + f)
# ---
def testParse(self):
        current = self.parser.parse(self.lines, '1024x768')
        for i in range(len(current)):
            self.assertEquals(self.expected[i], current[i], 'Entry incorrect')
# ---
def cached_fn_impl(leaves, treedef):
        user_fn_names, args, kwargs = jtu.tree_unflatten(treedef, leaves)
        return cacheable_fn(user_fn_names, *args, **kwargs)
# ---
def index_where(pred: Callable[[T], bool], xs: Sequence[T], start: int = 0) -> int:
    for i in range(start, len(xs)):
        if pred(xs[i]):
            return i
    raise ValueError("No element satisfies predicate")
# ---
def _collect_rollouts(self) -> None:
        """Collect available rollouts from reader and add to buffer."""
        batches = self.rollout_reader.read_all_available()

        if not batches:
            return

        start_time = time.time()

        self.replay_buffer.add_batches(batches)

        elapsed = time.time() - start_time
        if batches:
            logger.info(f"Collected {len(batches)} rollout batches, updated replay buffer in {elapsed}")
# ---
def tester(code):
            def func(cls):
                return cls.run_test(code)
            return func
# ---
def _jit_train_step_fn_no_hook(self):
        return named_jit(
            functools.partial(self._train_step, _no_hooks=True),
            axis_resources=self.parameter_axis_mapping,
            out_axis_resources=self.parameter_axis_mapping,
            donate_args=(True,),
        )
# ---
def device_type(self) -> str:
        """Device type from worker metadata."""
        return get_device_type(self.metadata.device)
# ---
def __call__(self, x: NamedArray, *, key=None):
        k1, k2 = haliax.jax_utils.maybe_rng_split(key, 2)
        x = self.fc1(x, key=k1)
        x = self.act(x)
        x = self.fc2(x, key=k2)
        return x
# ---
def send_reset(self):
        jsonstring = json.dumps({"resetpid": 1})
        self.serial.write(bytearray(jsonstring, 'utf-8'))
# ---
def test_task_fails_once_then_succeeds(cluster):
    """Container creation fails once, succeeds on retry."""
    _url, client = cluster
    enable_chaos(
        "worker.create_container",
        failure_rate=1.0,
        max_failures=1,
        error=RuntimeError("chaos: transient container failure"),
    )
    job = submit(client, _quick, "retry-once", max_retries_failure=2)
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def broadcast_to(self, axes: AxisSpec) -> "NamedArray":  # pragma: no cover
        return haliax.broadcast_to(self, axes=axes)
# ---
def camel_case(string):
    """Return camel case string from a space-separated string.

    Example
    -------
    >>> camel_case('good job')
    'GoodJob'
    """
    return ''.join(w.capitalize() for w in string.split())
# ---
def default() -> "SeqDecodingParams":
        """
        Returns a default SeqDecodingParams with the given number of stop sequences and maximum stop tokens.
        """
        max_int_jnp = jnp.iinfo(jnp.int32).max
        return SeqDecodingParams(
            max_num_tokens=jnp.array(max_int_jnp - 100000, dtype=jnp.int32),
            stop_tokens=None,
            temperature=jnp.array(0.0, dtype=jnp.float32),
            key=jax.random.PRNGKey(0),
        )
# ---
def default_config(self) -> LevConfig:
        return self.config_from_hf_config(self.default_hf_config)
# ---
def test_typeerror(self):
        for case in (None, object(), 123, 'foobar'):
            self.assertRaises(TypeError, ctds.Parameter, case, b'123')

        self.assertRaises(TypeError, ctds.Parameter)
        self.assertRaises(TypeError, ctds.Parameter, output=False)

        for case in (None, object(), 123, 'foobar'):
            self.assertRaises(TypeError, ctds.Parameter, b'123', output=case)
# ---
def test_multiselector_broadcast():
    B, S, V = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 6)
    a = hax.arange((B, S, V))
    idx1 = hax.arange((B, S), dtype=jnp.int32) % V.size
    out = a["vocab", idx1]
    assert out.axes == (B, S)
    assert jnp.array_equal(out.array, _ref_gather(a, V, idx1))
# ---
def silu(a: A) -> A:
    return wrap_elemwise_unary(jnn.silu, a)
# ---
def _add_video(root, params):
    e = root.find('./devices/video/model/[@type]')
    if e is not None:
        params['video'] = e.get('type')
# ---
def test_weird_list_item():
    html = r"""<html><body><ol start="'3..'"><li>Item 3</li></ol></body></html>"""
    expected = "1. Item 3\n"  # back off to no start
    assert to_markdown(html) == expected
# ---
def block(idx: int, size: int) -> "dslice":
        """
        Returns a dslice that selects a single block of size `size` starting at `idx`
        """
        return dslice(idx * size, size)
# ---
def test_addition_generates_near_miss():
    variants = generate_expression_variants("a + b")
    assert "a - b" in variants
# ---
def dot(
        self,
        axis: AxisSelection | None,
        *b,
        precision: PrecisionLike = None,
        dot_general=jax.lax.dot_general,
    ) -> "NamedArray": ...
# ---
def test_char_offset_to_token_index(tok):
    source = "hello"
    assert tok.char_offset_to_token_index(source, 0) == 0
    assert tok.char_offset_to_token_index(source, 3) == 3
    assert tok.char_offset_to_token_index(source, 10) == 4
# ---
def _format_run_name(
    budget: float,
    hidden_size: int,
    num_layers: int,
    batch_size: int,
    experiment_name: str,
) -> str:
    """Format run name using architecture details (hidden size and layers).

    Format: isoflop-{budget}-d{hidden}-L{layers}-B{batch}-{experiment_name}
    """
    return f"isoflop-{budget:.0e}-d{hidden_size}-L{num_layers}-B{batch_size}-{experiment_name}"
# ---
def index():
        form = ExampleForm()
        form.validate_on_submit()  # to get error messages to the browser
        flash('critical message', 'critical')
        flash('error message', 'error')
        flash('warning message', 'warning')
        flash('info message', 'info')
        flash('debug message', 'debug')
        flash('different message', 'different')
        flash('uncategorized message')
        return render_template('index.html', form=form)
# ---
def register_endpoint(
        self,
        name: str,
        address: str,
        job_id: JobName,
        metadata: dict[str, str] | None = None,
    ) -> str:
        return self._remote_client.register_endpoint(name=name, address=address, job_id=job_id, metadata=metadata)
# ---
def generate(self, prompts: list[str], sampling_params: SamplingParams) -> str:
        """
        Synchronous generate method - runs async code under the hood.

        Args:
            prompt: Input prompt
            max_tokens: Max tokens to generate

        Returns:
            Generated text
        """
        return self.bridge.run(self._generate_batch_async(prompts, sampling_params))
# ---
def fn(config: MyConfig):
            pass
# ---
def minimize(self):
        """Minimize the window.
        """
        raise NotImplementedError('abstract')
# ---
def test_capsysbinary_forbidden_in_python2(self, testdir):
        testdir.makepyfile("""
            def test_hello(capsysbinary):
                pass
        """)
        result = testdir.runpytest()
        result.stdout.fnmatch_lines([
            "*test_hello*",
            "*capsysbinary is only supported on python 3*",
            "*1 error in*",
        ])
# ---
def decodeFinished(self):
        self.status.completed = True
# ---
def usage(n):
    sys.stderr.write("Usage: " + n + " port...\n")
# ---
def _api_path(self):
        return "/product/license"
# ---
def test_ref_get_with_invalid_election_id_non_integer_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': 'hey' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified a non-integer election ID.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def test_profiler_get_shorten_id_int(self):
        short_id_int = 42
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        result = prof.get_shorten_id(short_id_int)
        expected = "2a"
        self.assertEqual(expected, result)
# ---
def build(self, reference_model: eqx.Module) -> eqx.Module:
        """Initialize any learned components (e.g., value heads)."""
        ...
# ---
def test_connection_region(self):
        """Connection can access name of connected region"""
        self.assertTrue(isinstance(self.dynamo.region, str))
# ---
def read(self, count=None): pass
# ---
def _prepare_bucket(self, task_name: str) -> list[dict[str, str]] | None:
        if not self.sample_logging_config.should_log():
            return None

        bucket = self.sample_outputs.setdefault(task_name, [])
        if self.sample_logging_config.allow_more(len(bucket)):
            return bucket

        return None
# ---
def model_cfg(tokenizer):
    return TreeDiffusionConfig(
        vocab_size=tokenizer.vocab_size,
        hidden_dim=64,
        intermediate_dim=128,
        num_layers=2,
        num_heads=4,
        num_kv_heads=4,
        max_seq_len=MAX_SEQ_LEN,
    )
# ---
def count_sources(batch):
        counts = {"ds1": 0, "ds2": 0, "ds3": 0}
        for x in batch:
            if x < 10:
                counts["ds1"] += 1
            elif x < 100:
                counts["ds2"] += 1
            else:
                counts["ds3"] += 1
        return counts
# ---
def needs_space_before(self, other: "LatexNode") -> bool:
        """Determine if a space is needed before the other node."""
        return False
# ---
def argmax(self, axis: AxisSelector | None = None) -> "NamedArray":  # pragma: no cover
        return haliax.argmax(self, axis=axis)
# ---
def func(cls):
                return cls.run_test(code)
# ---
def num_items(self):
        """int: Total items in the page."""
        return self._num_items
# ---
def after(self, other: "Timestamp") -> bool:
        """Check if this timestamp is after another."""
        return self._epoch_ms > other._epoch_ms
# ---
def format_type(s):
    if s not in _VALID_FORMATS:
      raise argparse.ArgumentTypeError(f'Invalid Format specified: "{s}".')
    return s
# ---
def time_series_granularity_type(s):
    if s not in _VALID_TIME_SERIES_GRANULARITIES:
      raise argparse.ArgumentTypeError('Invalid TimeSeriesGranularity '
                                       f'specified: "{s}".')
    return s
# ---
def list(self, body, ordered=True):
        """Rendering list tags like ``<ul>`` and ``<ol>``.

        :param body: body contents of the list.
        :param ordered: whether this list is ordered or not.
        """
        tag = 'ul'
        if ordered:
            tag = 'ol'
        return '<%s>\n%s</%s>\n' % (tag, body, tag)
# ---
def index():
    return render_template('index.html'), 200
# ---
def _fix_fracs(string):
    """Convert \\frac{a}{b} to (a)/(b), handling all variants. (Legacy - use process_latex_fractions)"""
    # Use new abstraction
    return process_latex_fractions(string)
# ---
def name_search(self, cr, user, name, args=None, operator='ilike', context=None, limit=100):
        if not args:
            args = []
        ids = self.search(cr, user, [('serial', '=', name)]+ args, limit=limit, context=context)
        ids += self.search(cr, user, [('name', operator, name)]+ args, limit=limit, context=context)
        return self.name_get(cr, user, ids, context)
# ---
def predict(self, df):
        """
        Returns predictions based on the model/pipeline
        """
        try:
            return self.pipeline.predict(df)
        except (ValueError, TypeError):
            print(colored('Got ValueError while using scikit model.. ', 'red'))
            return None
# ---
def test_decode_token_base(tok):
    tid = tok.encode_char("Z")
    assert tok.decode_token(tid) == "Z"
# ---
def an_ifc_file_exists():
    ifc = IfcStore.get_file()
    if not ifc:
        assert False, "No IFC file is available"
    return ifc
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> ListCache[KvPageCache]:
        return hax.auto_sharded(self.transformer.initial_cache(spec, dtype=dtype))
# ---
def speedrun_paloma_tokenized(tokenizer: str = llama3_tokenizer):
    return paloma_tokenized(base_path="raw/paloma-speedrun", tokenizer=tokenizer, paloma_raw=paloma_speedrun)
# ---
def __init__(self, filename):
        self.filename = filename
        tfile, members = self.get_archive_object_tar()
        self.read_files(tfile, members)
# ---
def display(self):
        """The display this window belongs to.  Read-only.

        :type: `Display`
        """
        return self._display
# ---
def getName( self, DN = '' ):
    """ Get the file catalog type name
    """
    return self.name
# ---
def __iter__(self):
        for f in self.FIELDS:
            yield getattr(self, f, '')
# ---
def play(self, filename):
        self.loadfile(filename)
# ---
def run_train_test_overlap(config: DeconConfig) -> str:
    logger.info(f"Starting train-test overlap dedupe with config: {config}")
    decontaminate(config)
    logger.info(f"Train-test overlap completed! Results written to {config.output_path}")
    return config.output_path
# ---
def limit(self, count):
        Util.validate_type(count, "int")
        return self._limit(count)
# ---
def add_volume_mount(self, volume_mount):
        ''' add a volume or volume mount to the proper location '''
        exist_volume_mounts = self.get_volume_mounts()

        if not exist_volume_mounts and volume_mount:
            self.put(DeploymentConfig.volume_mounts_path, [volume_mount])
        else:
            exist_volume_mounts.append(volume_mount)
# ---
def data_updater(self):
        while self.run:
            for irc_nick in self.data:
                self.update_data(irc_nick)
                time.sleep(30)

            time.sleep(600)
# ---
def init(cls, Vocab: Axis, config: MConfig, *, key: PRNGKeyArray) -> "LmWithHfSerializationMixin":
        pass
# ---
def main():
    """Simple flower, using global turtle instance"""
    turtle.speed(0)
    turtle.colormode(1.0)
    bloom(5)
    turtle.exitonclick()
# ---
def hidden_dim(self) -> int:
        return self.core.hidden_dim
# ---
def _copy(_):
        last_idx = (src_len + page_size - 1) // page_size - 1
        src_page = decode_state.sequences.page_indices["seq", parent_local_id, "page", last_idx].scalar()
        dst_page = decode_state.sequences.page_indices["seq", child_local_id, "page", last_idx].scalar()
        return state.cache.copy_page(src_page, dst_page)
# ---
def unregister_endpoint(self, endpoint_id: str) -> None:
        """Unregister an endpoint via RPC.

        This is a no-op for the RPC implementation. The controller automatically
        cleans up endpoints when jobs terminate, so explicit unregistration
        is not required.

        Args:
            endpoint_id: Endpoint ID (ignored)
        """
        # No-op: controller auto-cleans endpoints on job termination
        del endpoint_id
# ---
def _create_resource_impl(self, obj, wrapped=False):
        Util.validate_type(wrapped, "bool")
        return LicenseInfo(self._client, obj, wrapped)
# ---
def init_accumulators():
        m_scratch_ref[...] = jnp.full_like(m_scratch_ref, -jnp.inf)
        l_scratch_ref[...] = jnp.zeros_like(l_scratch_ref)
        label_logits_scratch_ref[...] = jnp.zeros_like(label_logits_scratch_ref)
# ---
def infer_xla_v_block_size(
    b: int,
    h: int,
    v: int,
    *,
    dtype: Optional[jnp.dtype],
    device_kind: Optional[str] = None,
) -> int:
    """Heuristic v-block size for the XLA streaming path."""
    del b, h, dtype, device_kind  # currently unused
    target = min(v, 32768)
    if target <= 0:
        return 1
    # Keep the block size <= v to avoid excess padding work.
    if target == v:
        return target
    return max(128, 128 * (target // 128))
# ---
def __exit__(self, exc_type, exc_value, traceback):
        self.close()
# ---
def era_shuffle(self, era_length: int, key: PRNGKeyArray, *, perm_type: PermType = "feistel"):
        import levanter.data.permutation as permutation

        return permutation.EraShufflingDataset(self, era_length, key=key, perm_type=perm_type)
# ---
def calcDottedNetmask(mask):
    bits = 0
    for i in xrange(32 - mask, 32):
        bits |= (1 << i)
    packed_value = pack('!I', bits)
    addr = inet_ntoa(packed_value)
    return addr
# ---
def _get_go_namespace(self, source):
    with open(source) as thrift:
      namespace = self.NAMESPACE_PARSER.search(thrift.read())
      if not namespace:
        raise TaskError('Thrift file {} must contain "namespace go "', source)
      return namespace.group(1)
# ---
def setUp(self):
        super().setUp()

        permission = Permission.objects.get(codename='search')
        self.user = User.objects.create()
        self.user.user_permissions.add(permission)
        self.group.add_member(self.user.essauth_member)

        self.client.force_authenticate(user=self.user)
# ---
def movie(self, imdb, title, localtitle, aliases, year):
        try:
            title = cleantitle.geturl(title)
            url = self.base_link + '/%s-%s' % (title, year)
            return url
        except:
            return
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df.A[2] = 0
            return df.A.nunique()
# ---
def recomb_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'method': 'site-specific recombination'
    }
# ---
def __eq__(self, other):
        return self.id == other.id
# ---
def __init__(self, endpoints: dict[str, str | list[str]]):
        """Initialize with a mapping of actor names to URLs."""
        self._endpoints: dict[str, list[str]] = {}
        for name, urls in endpoints.items():
            if isinstance(urls, str):
                self._endpoints[name] = [urls]
            else:
                self._endpoints[name] = list(urls)
# ---
def test_instance_auto_disk_config_doesnt_pass_fail_safes(self):
        """Should not partition unless fail safes pass"""
        self.instance_values['auto_disk_config'] = True

        def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4'), (2, 100, 200, 'ext4')]
        self.stubs.Set(vm_utils, "_get_partitions",
                       fake_get_partitions)

        self.assertIsPartitionCalled(False)
# ---
def totext(obj):
        if isinstance(obj, str):
            obj = unicode(obj, 'UTF-8')
        assert isinstance(obj, unicode)
        return obj
# ---
def spawn_executor(self, max_workers: int, prefix: str) -> ThreadPoolExecutor:
        """Create a ThreadPoolExecutor that will be shut down when this container stops."""
        executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=prefix)
        with self._lock:
            self._executors.append(executor)
        return executor
# ---
def t_ID(self, t):
        r'\d+\.([uU]|[lL]|[uU][lL]|[lL][uU])?'
        t.value = int(t.value[:-1])
        return t
# ---
def do_alloc(state):
                    indices, ref_counts = state
                    # choose a page with the smallest ref count; when has_free, argmin will pick a zero-ref page
                    free_page_idx = hax.argmin(ref_counts, "page")
                    ref_counts = ref_counts.at["page", free_page_idx].add(1)
                    indices = indices.at["seq", seq_id, "page", page_idx].set(free_page_idx)
                    return indices, ref_counts
# ---
def timed(self, stat, sample_rate=1):
        log.debug('Entering timed context for %r' % (stat,))
        start = time.time()
        yield
        duration = int((time.time() - start) * 1000)
        log.debug('Exiting timed context for %r' % (stat,))
        self.timing(stat, duration, sample_rate)
# ---
def minimal_bootstrap_config() -> config_pb2.BootstrapConfig:
    """Minimal valid bootstrap config."""
    return config_pb2.BootstrapConfig(
        docker_image="gcr.io/test/iris-worker:latest",
        worker_port=10001,
        cache_dir="/var/cache/iris",
    )
# ---
def shutdown(self) -> None:
        """Kill all Ray actors."""
        for handle in self._handles:
            try:
                ray.kill(handle._actor_ref)
            except Exception as e:
                logger.warning("Failed to kill Ray actor: %s", e)
# ---
def sample_params():
    """Generate sample JAX parameters for testing."""
    return create_sample_pytree(seed=42)
# ---
def corofunc1():
            called[0] += 1
            return event.ReturnValue(insert_events=[evt2], append_events=[evt])
# ---
def set_power(self, power):
        if power > self._default_calibration_power + 10:
            raise ValueError("Power can be % dBm max, requested %d dBm" % (
                self._default_calibration_power + 10, power))

        self._power = power
        self._requested_cal = self.get_calibration(self._frequency,
                                                   self._power)
        self._lo.set_power(self._requested_cal.get_lo_power())
        self._output_SSB()
# ---
def get_address_is_used(self, address: bytes) -> bool:
        with self.lock:
            return self._state.get_address_is_used(address)
# ---
def check_plateau_status(trajectory: np.ndarray, window: int = 100) -> bool:
    """Check if trajectory is plateaued using curriculum logic."""
    stats = LessonStats(
        training_stats=PerformanceStats(
            total_samples=len(trajectory), reward_history=trajectory, last_update_step=len(trajectory)
        )
    )
    return is_plateaued(stats, window=window, threshold=0.01)
# ---
def perform(self):
        """Performs all stored actions."""
        for action in self._actions:
            action()
# ---
def evt(ctx):
            ctx.is_disconnect = evt_value
# ---
def __init__(self, df = 2, mu = 0):
        d1 = NormalDistr(mu, 1)
        d2 = distr_sqrt(ChiSquareDistr(df) / df)
        super(NoncentralTDistr, self).__init__(d1, d2)
        self.df = df
        self.mu = mu
# ---
def test_permute(self):
    testing_utils.layer_test(
        keras.layers.Permute, kwargs={'dims': (2, 1)}, input_shape=(3, 2, 4))
# ---
def to_string(value):
        """ Convert a :class:`date` value into the format expected by the ORM. """
        return value.strftime(DATE_FORMAT) if value else False
# ---
def dict_from_parsed_args(parsed_args, attrs):
    d = {}
    for attr in attrs:
        value = getattr(parsed_args, attr)
        if value is not None:
            d[attr] = value
    return d
# ---
def close(self, request, *args, **kwargs):
        instance = self.get_object()
        serializer = self.get_serializer(instance)
        instance.close(processor=request.user)
        return Response(serializer.data)
# ---
def loggamma(key, shape: AxisSpec, a: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    a = broadcast_to(a, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.loggamma(key, a.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def test_cursor_execute_w_replace(self):
        self._test_cursor_execute(True)
# ---
def get_puzzle_solution(self, alt_input=None):
        if alt_input is None:
            self._load_puzzle_file()
        else:
            self._puzzle_input = alt_input
        old_nice_count, new_nice_count = self._solve_puzzle_parts()
        return self._solved_output.format(old_nice_count, new_nice_count)
# ---
def compare(self, param):
        try:
            result = getattr(self, param)
            return result
        except AttributeError:
            return self.__default(param)
# ---
def test_scan_doesnt_scan_scalars():
    Height = Axis("Height", 10)
    named1 = hax.random.uniform(PRNGKey(0), (Height,))

    def scan_fun(acc, z, x):
        return (acc + z * x).scalar(), x * z

    total, selected = hax.scan(scan_fun, Height)(0.0, 4.0, named1)

    assert jnp.all(jnp.isclose(total, jnp.sum(named1.array * 4.0)))
    assert jnp.all(jnp.equal(selected.array, named1.array * 4.0))
# ---
def test_not(self):
        expr = ~(col("flag") == True)  # noqa: E712
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def test_list_migrations(self):
        """Test admin user can get the migrations list"""
        self.client.list_migrations()
# ---
def date(self):
        del self._date
# ---
def summary(self) -> str:
        """Return a human-readable summary of bank contents."""
        lines = [f"SubtreeBank: {self.total_entries} entries, {len(self.entries)} types"]
        for node_type in sorted(self.entries.keys()):
            entries = self.entries[node_type]
            lines.append(f"  {node_type}: {len(entries)} entries")
        return "\n".join(lines)
# ---
def get_lm_head(self) -> hax.NamedArray:
        if self.lm_head is None:
            return self.embeddings.token_embeddings.weight
        else:
            return self.lm_head.weight
# ---
def _poly_axis_from_input(p: NamedArray | ArrayLike, size: int) -> Axis:
    if isinstance(p, NamedArray):
        if p.ndim != 1:
            raise ValueError("Polynomial coefficient arrays must be 1D")
        return p.axes[0].resize(size)
    else:
        return Axis(DEFAULT_POLY_AXIS_NAME, size)
# ---
def foo(ref, xs):
        def scan_fn(_, x):
            ref_slice = ref.slice({"x": x})
            ref_slice[...] = (x * x).astype(ref_slice.dtype)
            return None, x * 2

        return hax.scan(scan_fn, X)(None, xs)[1]
# ---
def map(self, fn: Callable[[T], R]) -> Dataset[R]:
        """Map a function over the dataset.

        Args:
            fn: Function to apply to each element

        Returns:
            New dataset with map operation appended

        Example:
            >>> from zephyr import Backend
            >>> ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)
            >>> Backend.execute(ds)
            [2, 4, 6]
        """
        return Dataset(self.source, [*self.operations, MapOp(fn)])
# ---
def test_list_type_access_private(self):
        expected = {'volume_type_access': [
            {'volume_type_id': fake.VOLUME_TYPE3_ID,
             'project_id': PROJ2_UUID},
            {'volume_type_id': fake.VOLUME_TYPE3_ID,
             'project_id': PROJ3_UUID}]}
        result = self.type_access_controller.index(self.req,
                                                   fake.VOLUME_TYPE3_ID)
        self.assertEqual(expected, result)
# ---
def withdraw(account, amount):
    account['balance'] -= amount
    return account['balance']
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()
# ---
def result(self):
        """Obtain the final result of the parse.

        Raises ValueError if the parse failed to reduce to a single result.
        """

        if len(self.values) != 1:
            raise ValueError("Could not parse rule")
        return self.values[0]
# ---
def next(self, psm: PSM):
        if psm.char.isdigit():
            self.max.append(psm.char)
            return self
        elif psm.char == "}":
            self._interpret()
            return self.repeat.parent
        else:
            psm.error = 'expected digit, "," or "}"'
# ---
def test_fully_buffered_result_proxy(self):
        self._test_proxy(_result.FullyBufferedResultProxy)
# ---
def testForElseContinueNotNested(self):
    self.assertRaisesRegexp(
        util.ParseError, "'continue' not in loop",
        _ParseAndVisit, 'for i in (1,):\n  pass\nelse:\n  continue')
# ---
def set_children(self, children):
        self.children = children
# ---
def __len__(self):
        return self.vocab_size
# ---
def num_lines(string):
    """
    Return number of lines.
    """
    line_list = string.split("\n")
    num = len(line_list)
    for l in line_list:
        num += (len(string) // LINEWIDTH + 1)

    return num
# ---
def action_print_survey(self):
        return self.survey_id.action_print_survey()
# ---
def test_no_port_reuse_before_release(allocator):
    """Test that allocated ports are not reused before release."""
    ports1 = allocator.allocate(count=5)
    ports2 = allocator.allocate(count=5)

    assert len(set(ports1) & set(ports2)) == 0
# ---
def is_small(self):
    raise NotImplementedError()
# ---
def reparam(self) -> AbstractEmbeddingReparam:
        return self._reparam_cls(self.Embed, self.Vocab)
# ---
def _restart_daemon(self):
        self._stop_daemon()
        return self._start_daemon()
# ---
def _find_user(self, dn):
        return next(i for (i, d) in enumerate(self.directory) if d["dn"] == dn)
# ---
def main():
    configs = {
        "mixtral_300m": build_speedrun_config("mixtral_300m", train_batch_size=256, lr=5e-4),
        "mixtral_1b": build_speedrun_config("mixtral_1b", train_batch_size=128, lr=3e-4),
        "mixtral_1_5b": build_speedrun_config("mixtral_1_5b", train_batch_size=96, lr=2e-4),
    }
    for name in MODEL_ORDER:
        cfg = configs[name]
        LOGGER.info("Launching %s", name)
        executor_main(steps=default_speedrun(f"pranshu_mixtral_moe_gmm_sweep_{name}", cfg))
# ---
def test_submit_and_complete(self, test_cluster):
        """Job completes successfully."""

        def hello():
            return 42

        job_id = test_cluster.submit(hello, name=unique_name("test-job"))
        status = test_cluster.wait(job_id, timeout=30)
        assert status["state"] == "JOB_STATE_SUCCEEDED"
# ---
def __init__(self,occurrence):
        self.occurrence=occurrence
        self.label=self.occurrence.arguments[0]
# ---
def spine_entry(self):
        '''Write the XML element for the spine.
        (Empty string if in_spine is False.)'''
        if self.in_spine:
            return _make_xml_elem('itemref', '', [('idref', self.ident)])
        else:
            return ''
# ---
def test_as_remote_kwargs_tpu():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=TpuConfig(variant="v4-32"))
    kwargs = as_remote_kwargs(config)
    assert kwargs["num_cpus"] == 8
# ---
def x_read():
        x_read_future.start()
# ---
def _configure_logging():
    """Configure logging to show all iris module output."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        stream=sys.stdout,
        force=True,
    )
    # Ensure iris modules are visible
    logging.getLogger("iris").setLevel(logging.INFO)
# ---
def diagnostics(self, handle: VllmServerHandle, *, max_lines: int = 200) -> dict[str, str]:
        diagnostics: dict[str, str] = {}
        if handle.docker_run_cmd:
            diagnostics["vLLM Docker run command (redacted)"] = handle.docker_run_cmd
        diagnostics["vLLM Docker logs (tail)"] = self.logs_tail(handle, max_lines=max_lines)
        diagnostics["vLLM Docker inspect"] = _docker_inspect(handle.docker_container_name)
        return diagnostics
# ---
def __init__(self, exc_info):
        self.exc_type = exc_info[0]
        self.exc = exc_info[1]
        self.tb = tblib.Traceback(exc_info[2])
# ---
def resolved_param_mapping(self) -> ResourceMapping:
        # Parameter mapping should inherit shared defaults so parameters on those logical axes shard the
        # same way as compute unless explicitly overridden.
        mapping = self._resolved_shared_axis_mapping()
        for logical, physical in self.param_mapping.items():
            mapping[logical] = _norm(physical)
        return mapping
# ---
def _fake_compile_metrics(cls, start_time, stop_time=None):
        raise exception.CouldNotFetchMetrics()
# ---
def open_builder(tree_path, item):
        item = np.asarray(item)
        rank = item.ndim
        render_tree_path = "/".join(_render_path_elem(x) for x in tree_path)
        return JaggedArrayStore.open(
            os.path.join(path, render_tree_path),
            mode=mode,
            item_rank=rank,
            dtype=item.dtype,
            cache_metadata=cache_metadata,
        )
# ---
def geomspace(
    axis: AxisSelector, *, start: float, stop: float, endpoint: bool = True, dtype: DTypeLike | None = None
) -> NamedArray:
    """
    Version of jnp.geomspace that returns a NamedArray.
    If `axis` is a string, the default number of samples (50, per numpy) will be used.
    """
    if isinstance(axis, str):
        axis = Axis(axis, 50)
    return NamedArray(jnp.geomspace(start, stop, axis.size, endpoint=endpoint, dtype=dtype), (axis,))
# ---
def __call__(self, batch: dict[str, Any]) -> dict[str, Any]:
        return self.cls.__call__(batch)
# ---
def get_id_from_row(row: dict, id_path: tuple[str, ...]) -> str | None:
    """Traverse a tuple path in a row to extract the ID, or return None if missing."""
    obj = row
    for key in id_path:
        obj = obj.get(key)
        if obj is None:
            raise ValueError(f"ID path {id_path} not found in row: {row}")
    return obj
# ---
def bytearray_to_utf8(x):
    return x.decode('utf-8')
# ---
def __getAvailableProfiles(self):
        """Get available user profiles."""
        return _settingsManager.availableProfiles()
# ---
def parse_block_quote(self, m):
        self.tokens.append({'type': 'block_quote_start'})
        # clean leading >
        cap = _block_quote_leading_pattern.sub('', m.group(0))
        self.parse(cap)
        self.tokens.append({'type': 'block_quote_end'})
# ---
def test_repr(self):
        expr = (col("a") > 0) & (col("b") > 0)
        assert repr(expr) == "((col('a') > lit(0)) & (col('b') > lit(0)))"
# ---
def vm_ops(self) -> PlatformOps:
        return _ManualPlatformOps(self._ssh, self._bootstrap)
# ---
def zeros_like_tree(tree: T, axis_mapping: Optional[ResourceMapping] = None, dtype: Optional[jnp.dtype] = None) -> T:
    """
    Creates a tree of zeros with the same structure as the input tree. If the input tree contains NamedArrays, then
    those will be sharded according to the axis_mapping (or the context axis mapping if not provided).
    """
    _zeros = functools.partial(_zeros_like, axis_mapping, dtype)
    acc = jax.tree_util.tree_map(_zeros, tree, is_leaf=is_named_array)
    return acc
# ---
def test_beam_search_no_duplicate_sources(params, model_cfg, tokenizer):
    """Beam should deduplicate candidates with the same source."""
    results = beam_search(
        params=params,
        initial_programs=["x = 1 + 2\n"],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(3),
        beam_size=8,
        expansions_per_beam=3,
        max_depth=3,
    )
    sources = [c.source for c in results]
    assert len(sources) == len(set(sources))
# ---
def poisson(key, shape: AxisSpec, lam: NamedOrNumeric, dtype=int):
    shape = axis_spec_to_shape_dict(shape)
    lam = broadcast_to(lam, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.poisson(key=key, lam=lam, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def getName(self):
        return "NoncF({0},{1},{2})".format(self.df1, self.df2, self.lmbda)
# ---
def script_BIP34_coinbase_height(height):
    if height <= 16:
        res = CScriptOp.encode_op_n(height)
        # Append dummy to increase scriptSig size above 2 (see bad-cb-length consensus rule)
        return CScript([res, OP_1])
    return CScript([CScriptNum(height)])
# ---
def rename(self, renames: Mapping[AxisSelector, AxisSelector]) -> "NamedArray":  # pragma: no cover
        return haliax.rename(self, renames=renames)
# ---
def call(self, request: actor__pb2.ActorCall, ctx: RequestContext) -> actor__pb2.ActorResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def validation_sets(self) -> Mapping[str, ProcessedAudioCache]:
        if self._has_validation_set:
            validation_set = self.validation_set()
            if validation_set is not None:
                return {"": validation_set}
        return {}
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "wte", "position_embeddings": "wpe"}
# ---
def _fail():
    raise RuntimeError("intentional failure")
# ---
def __init__(self, callback, debounce_seconds=0.5):
        super().__init__()
        self.callback = callback
        self.debounce_seconds = debounce_seconds
        self.last_trigger = 0
        self._timer = None
# ---
def total_coin_supply(self):
        with self.lock:
            return self._state.total_coin_supply
# ---
def test_column(self):
        expr = col("score")
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def _trigger(self):
        t = time.time()
        mean, max = eval_with_funcs(
            self.pred_funcs, self.eval_episode, self.get_player_fn)
        t = time.time() - t
        if t > 10 * 60:  # eval takes too long
            self.eval_episode = int(self.eval_episode * 0.94)
        self.trainer.monitors.put_scalar('mean_score', mean)
        self.trainer.monitors.put_scalar('max_score', max)
# ---
def test_boolean_commutativity():
    variants = generate_expression_variants("a and b")
    assert "b and a" in variants
# ---
def __init__(self, alpha = 1, beta = 1, lmbda = 0):
        d = 1 + ChiSquareDistr(2.0 * beta) / NoncentralChiSquareDistr(2 * alpha, lmbda)
        super(NoncentralBetaDistr, self).__init__(d)
        self.alpha = alpha
        self.beta = beta
        self.lmbda = lmbda
# ---
def __init__(self, remote, match_control, *args, **kwargs):
        self.remote = remote
        super(ScheduleControl, self).__init__(*args, **kwargs)
        self.InitUI()
        self.remote.match_list_box = self.match_list
        self.match_control = match_control
# ---
def floor(a: A) -> A:
    return wrap_elemwise_unary(jnp.floor, a)
# ---
def test_impl(df):
            return df.A.iloc[0]
# ---
def remove_endpoints_for_job(self, job_id: JobName) -> list[ControllerEndpoint]:
        """Remove all endpoints for a job by removing endpoints for all its tasks."""
        with self._lock:
            all_removed = []
            for task_id in self._tasks_by_job.get(job_id, []):
                removed = self._remove_endpoints_for_task(task_id)
                all_removed.extend(removed)
            return all_removed
# ---
def test_olmo2_lm_head_model(num_kv_heads):
    config = _get_olmo2_config(num_kv_heads=num_kv_heads)
    Batch = hax.Axis("batch", 2)
    Vocab = hax.Axis("vocab", 1000)
    Pos = config.max_Pos
    input_ids = hax.random.randint(random.PRNGKey(0), (Batch, Pos), 0, Vocab.size)
    mask = AttentionMask.causal()

    olmo2_model = Olmo2LMHeadModel.init(Vocab=Vocab, config=config, key=random.PRNGKey(0))
    out = olmo2_model(input_ids, mask)
    assert out.array.shape == (Batch.size, Pos.size, Vocab.size)
# ---
def changed(self) -> bool:
        return self._changed
# ---
def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):
        super(LW_AveragePooling3D, self).__init__(pool_size, strides, border_mode, dim_ordering)
# ---
def __repr__(self):
        return "{0.__class__.__name__}({0.field!r}, {0.fast})".format(self)
# ---
def true_divide(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.true_divide(x1, x2)
# ---
def isPrime(num):
    if num < 2:
        return False  # 0, 1ä¸æ˜¯è´¨æ•°

    # numä¸º100æ—¶, å®ƒæ˜¯ä¸å¯èƒ½æœ‰å› å­æ˜¯å¤§äºŽ50çš„. æ¯”å¦‚è¯´60 * ? = 100, è¿™æ˜¯ä¸å¯èƒ½çš„, æ‰€ä»¥è¿™é‡Œåªè¦æ¯”è¾ƒsqrt(), å¹³æ–¹æ ¹
    boundary = int(math.sqrt(num)) + 1
    for i in range(2, boundary):
        if num % i == 0:
            return False

    return True
# ---
def remote(self, *args, **kwargs):
        # Check if context has executor (ThreadContext) or not (SyncContext)
        if hasattr(self._context, "executor"):

            def locked_call():
                with self._lock:
                    return self._method(*args, **kwargs)

            return self._context.executor.submit(locked_call)
        else:
            with self._lock:
                result = self._method(*args, **kwargs)
            return _ImmediateFuture(result)
# ---
def unset_updated_pending_status(self, process):
        process.updated_pending_status = False
        for connection_status in process.connection_status.values():
            connection_status['updated_pending_status'] = False
# ---
def num_train_steps(self) -> int:
        return self.config.num_train_steps
# ---
def new(self):
        self.objectexplorer.model().beginResetModel()
        self.rootnode.clear()
        self.objectexplorer.model().endResetModel()
# ---
def load_vortex(self, columns: list[str] | None = None) -> Dataset[dict]:
        """Load records from Vortex files."""
        return Dataset(self.source, [*self.operations, LoadFileOp("vortex", columns)])
# ---
def removeSelected(self):
        self.model.remove_selected()
# ---
def finished(self, name):
    print("==========")
# ---
def visitdir(self, dir):
        """Decides whether a directory should be visited based on whether it
        has potential matches in it or one of its subdirectories. This is
        based on the match's primary, included, and excluded patterns.

        Returns the string 'all' if the given directory and all subdirectories
        should be visited. Otherwise returns True or False indicating whether
        the given directory should be visited.
        """
        return True
# ---
def trim_to_size(self, size: int):
        """
        Trim the store to a given size.
        """
        # TODO These all return ts Futures so in theory we could await them all at once
        jtu.tree_map(lambda writer: writer.trim_to_size(size), self.tree, is_leaf=heuristic_is_leaf)
# ---
def as_dict(self):
        return { 'prefix': self.prefix.decode('utf-8'),
                 'level':  self.level.decode('utf-8'),
                 'text':   self.text.decode('utf-8').rstrip() }
# ---
def _init_buffers():
        new_model = lm_model_cls.init(Vocab, config, key=key)

        def select_if_missing(missing_leaf, new_value):
            if isinstance(missing_leaf, jax.ShapeDtypeStruct):
                return new_value
            else:
                return None

        return jax.tree.map(select_if_missing, dtype_structs, new_model, is_leaf=lambda x: x is None)
# ---
def hf_config_from_config(self, config: LevConfig, vocab_size: Optional[int] = None) -> HfConfig:
        if vocab_size is None:
            vocab_size = self.Vocab.size
        return config.to_hf_config(vocab_size=vocab_size)
# ---
def get_well_known_file_ids(self, neuronal_model_id):
        '''Query the current RMA endpoint with a neuronal_model id
        to get the corresponding well known file ids.

        Returns
        -------
        list
            A list of well known file id strings.
        '''
        rma_builder_fn = self.build_rma
        json_traversal_fn = self.read_json

        return self.do_query(rma_builder_fn, json_traversal_fn, neuronal_model_id)
# ---
def test_lambda_output_shape_function_multiple_outputs(self):

    def lambda_fn(x):
      return x

    def output_shape_fn(input_shape):
      return input_shape

    l = keras.layers.Lambda(lambda_fn, output_shape=output_shape_fn)
    output_shape = l.compute_output_shape([(10, 10), (10, 20)])
    self.assertAllEqual([(10, 10), (10, 20)], output_shape)
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["LlamaConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfLlamaConfig,
        )
# ---
def editingCanceledKey(self, editable):
        """Stops user input of a Key for a selected key binding"""

        orca_state.capturingKeys = False
        self._capturedKey = []
        return
# ---
def header(self, text, level, raw=None):
        """Rendering header/heading tags like ``<h1>`` ``<h2>``.

        :param text: rendered text content for the header.
        :param level: a number for the header level, for example: 1.
        :param raw: raw text content of the header.
        """
        return '<h%d>%s</h%d>\n' % (level, text, level)
# ---
def __unicode__(self):
        return self.name
# ---
def test_revert_cold_migration(self):
        """Test cold migrating server and then revert the migration"""
        self._test_cold_migrate_server(revert=True)
# ---
def cluster_restore_jobs(ctx, backup_dir):
    """Restore Ray jobs from specified directory."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        _restore_jobs(backup_dir)
        print(f"Jobs restored successfully from {backup_dir}")
# ---
def test_page_table_max_len_per_seq():
    pt = _make_table(page_size=2, pages_per_seq=3)
    assert pt.max_len_per_seq == 6
# ---
def setUp(self):
        self.conf = tecaconf.ConfigHandler(
            "tests/test_data/configuration.json",
            {"starting_path": "tests/test_data/images"}
        )
        self.files_list = [
          "foo.doc",
          "yukinon.jpg",
          "cuteflushadoingflushathings.webm"
        ]
# ---
def new(self, **kwargs):
        """ Return a field of the same type as ``self``, with its own parameters. """
        return type(self)(**kwargs)
# ---
def tasks(self) -> list[Task]:
        """Get all tasks for this job.

        Returns:
            List of Task handles, one per task in the job
        """
        task_statuses = self._client._cluster_client.list_tasks(self._job_id)
        return [Task(self._client, JobName.from_wire(ts.task_id)) for ts in task_statuses]
# ---
def __post_init__(self):
        if self.workspace and self.docker_image:
            raise ValueError("Cannot specify both workspace and docker_image")
        if not self.workspace and not self.docker_image:
            raise ValueError("Must specify either workspace or docker_image")
# ---
def check_vm_params_for_windows(self):
        self.assertEquals(self.vm['platform']['nx'], 'true')
        self.assertEquals(self.vm['HVM_boot_params'], {'order': 'dc'})
        self.assertEquals(self.vm['HVM_boot_policy'], 'BIOS order')

        # check that these are not set
        self.assertEquals(self.vm['PV_args'], '')
        self.assertEquals(self.vm['PV_bootloader'], '')
        self.assertEquals(self.vm['PV_kernel'], '')
        self.assertEquals(self.vm['PV_ramdisk'], '')
# ---
def clear_queue(self) -> None:
        """Clear all batches from the queue."""
        with self._lock:
            self._queue.clear()
# ---
def remove_boxed(s):
    """Remove \\boxed wrapper and return content."""
    if "\\boxed " in s:
        left = "\\boxed "
        if s.startswith(left):
            return s[len(left) :]

    idx = s.find("\\boxed{")
    if idx >= 0:
        content = extract_braced_content(s, idx + 6)
        return content if content is not None else s

    return s
# ---
def test_add_scalar():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)

    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))
    named2 = named1 + 1.0
    assert jnp.all(jnp.isclose(named2.array, named1.array + 1.0))

    named3 = 1.0 + named1
    assert jnp.all(jnp.isclose(named3.array, named1.array + 1.0))
# ---
def define_tables(cls, metadata):
        Table(
            "stuff",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("data", String(50)),
        )
# ---
def __init__(self):
        # ast is CharClass or may be changed to PatternClass in one case
        self.ast = ast.CharClass()
# ---
def _files(self):
        if self.isexact():
            return [f for f in self._m1.files() if self(f)]
        # If m1 is not an exact matcher, we can't easily figure out the set of
        # files, because its files() are not always files. For example, if
        # m1 is "path:dir" and m2 is "rootfileins:.", we don't
        # want to remove "dir" from the set even though it would match m2,
        # because the "dir" in m1 may not be a file.
        return self._m1.files()
# ---
def visit_munder(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            under = self._visit(children[1])
            # Check if base is an operator usually taking limits
            if "\\sum" in str(base) or "\\prod" in str(base) or "\\lim" in str(base):
                return BracedNode(f"{base}_{{{under}}}")
            return BracedNode(f"\\underset{{{under}}}{{{base}}}")
        return TextNode("")
# ---
def test_column_std(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.std()

        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(), test_impl())
# ---
def host(self, host):
        """
        Sets the host of this ContributorOrcid.

        :param host: The host of this ContributorOrcid.
        :type: str
        """

        self._host = host
# ---
def canPlayLine(self, line, col):
        """
        Function to check if we can fill the line with a token.
        :param line: which line
        :param col: which column
        :return: true or false
        """
        if line == 5:
            return self.grille[line][col] == '0'
        else:
            return self.grille[line][col] == '0' and self.grille[line + 1][col] != '0'
# ---
def method3(self, g=10, h=20):
        return g * h
# ---
def __call__(self, *args, **kwargs):
        return self._call(False, *args, **kwargs)
# ---
def log_hyperparameters(self, hparams: typing.Mapping[str, Any]):
        self.writer.add_hparams(hparams, {"dummy": 0})
# ---
def clear_caches():
    """Clears internal Equinox caches.

    Best used before calling `jax.clear_caches()` or `jax.clear_backends()`.

    **Arguments:**

    None.

    **Returns:**

    None.
    """
    for cache in internal_caches:
        cache.clear()
    for cache in internal_lru_caches:
        cache.cache_clear()
# ---
def test_sample_nonexistent_type_returns_none(bank):
    rng = random.Random(42)
    assert bank.sample("NonexistentType", rng) is None
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"transformer": "model", "embeddings": None}
# ---
def model_type(self) -> Type["ApertusLMHeadModel"]:
        return ApertusLMHeadModel
# ---
def merge_after(self, obj):
        """
        Merge with another CmdText object by appending the input objects content.
        """
        self.lines
# ---
def __call__(
        self,
        model: M_con,
        *inputs: X,
        reduction: Optional[hax.ReductionFunction] = cast(Optional[hax.ReductionFunction], hax.mean),
        reduction_axis: Optional[hax.AxisSelection] = None,
        **kwargs,
    ) -> Scalar | hax.NamedArray: ...
# ---
def test_k5_closeness(self):
        c = nx.closeness_centrality(self.K5)
        d = {0: 1.000, 1: 1.000, 2: 1.000, 3: 1.000, 4: 1.000}
        for n in sorted(self.K5):
            assert almost_equal(c[n], d[n], places=3)
# ---
def test_blob_to_dict(self):
        """
        Test convertion of git blobs to dictionary
        """
        valuesmap = { "foo" : "1", "bar" : "2" }
        self.commit_vars(to_add = valuesmap)

        blob = self.repo.head.commit.tree.blobs[0]
        self.assertEqual(valuesmap, blob_to_dict(blob),
            "commit was not translated correctly to dictionary")
# ---
def test_caching_behavior(temp_cache_dir, test_bundle):
    """Test that bundles are cached and not re-downloaded."""
    cache = BundleCache(temp_cache_dir)

    file_url = f"file://{test_bundle}"

    # First download
    extract_path1 = cache.get_bundle(file_url)

    # Second request - should use cache and return same path
    extract_path2 = cache.get_bundle(file_url)

    assert extract_path1 == extract_path2
# ---
def cumprod(a: NamedArray, axis: AxisSelector, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.cumprod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumprod.html)
    """
    return wrap_axiswise_call(jnp.cumprod, a, axis, dtype=dtype, single_axis_only=True)
# ---
def _safe_remove(path: str):
    try:
        if fsspec_exists(path):
            fsspec_remove(path, recursive=True)
    except Exception:  # noqa: BLE001
        logger.exception(f"Failed to remove temporary cache path {path}")
# ---
def tok_text(self):
        text = self.token['text']
        while self.peek()['type'] == 'text':
            text += '\n' + self.pop()['text']
        return self.inline(text)
# ---
def err2(context):
            stmt = context.statement
            if ("ERROR ONE" in str(stmt) or "ERROR FOUR" in str(stmt)) \
                    and isinstance(context.chained_exception, MyException1):
                raise MyException2("my exception chained")
            elif "ERROR TWO" in str(stmt):
                return context.chained_exception
            else:
                return None
# ---
def _read_files():
            for path in paths:
                path = Path(path)
                if not path.exists() or not path.suffix == ".py":
                    continue
                try:
                    yield path.read_text(encoding="utf-8")
                except (OSError, UnicodeDecodeError):
                    continue
# ---
def the_object_name_has_number_vertices(name, number):
    total = len(the_object_name_exists(name).data.vertices)
    assert total == int(number), f"We found {total} vertices"
# ---
def authors():
    return [make_author() for _ in range(3)]
# ---
def fold(self, init: T, *args, unroll: int | bool | None = None, **kwargs) -> T: ...
# ---
def test_heartbeat_temporary_failure(cluster):
    """Test heartbeat fails 3 times (30s gap), but worker timeout is 60s. Worker should
    NOT be marked failed. Task should still succeed.
    """
    _url, client = cluster
    enable_chaos("worker.heartbeat", failure_rate=1.0, max_failures=3)
    job = submit(client, _quick, "temp-hb-fail")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def test_compute_bracket_mask_no_open_brackets(tokenizer):
    mask = compute_bracket_mask("", tokenizer)
    assert mask.shape == (tokenizer.vocab_size,)
    # Close brackets should be blocked.
    for ch in ")]}":
        tid = tokenizer.encode_char(ch)
        assert float(mask[tid]) == 0.0
    # Open brackets and regular chars should be allowed.
    for ch in "([{abc":
        tid = tokenizer.encode_char(ch)
        assert float(mask[tid]) == 1.0
# ---
def _vm_has_snapshot(vm):
    try:
        return vm.hasCurrentSnapshot() == 1
    except libvirt.libvirtError:
        logging.exception('Error checking if snapshot exist for vm: %s.',
                          vm.name())
        return False
# ---
def test_from_string(self):
        """Can convert from string to type id."""
        self.assertEquals(
            vm_utils.ImageType.from_string(vm_utils.ImageType.KERNEL_STR),
            vm_utils.ImageType.KERNEL)
# ---
def test_uniform_with_bounds_scalar():
    check_gen_is_equal(jax.random.uniform, hax.random.uniform)

    key = jax.random.PRNGKey(0)
    u = hax.random.uniform(key, shape=(Height, Width), minval=-3.0, maxval=0.5)

    assert u.axes == (Height, Width)

    assert hax.all(u >= -3.0)
    assert hax.all(u <= 0.5)
# ---
def ping_endpoint():
    """API endpoint determines potential article hash exists in db

    :return: status code 204 -- hash not present, continue submission
    :return: status code 201 -- hash already exists, drop submission
    """
    db = get_db()
    target_hash = request.form.get('hash')
    if db.raw.find({'hash': target_hash}).count():
        return Response(status=201)
    else:
        return Response(status=204)
# ---
def _sort_matches(self):
        self.matches.sort(key=lambda t: t[0].date if t[0] is not None else t[1].date)
# ---
def test_arrow_batch_sizes(benchmark: Any, in_memory_table: pa.Table, batch_size: int) -> None:
    """
    Benchmarks the effect of PyArrow batch size on marshaling throughput.
    """

    def _pipeline() -> int:
        return sum(len(dupekit.process_arrow_batch(b)) for b in in_memory_table.to_batches(max_chunksize=batch_size))

    assert benchmark(_pipeline) > 0
# ---
def test_init_params_no_timestep_embed(params):
    """EditModelParams should NOT have a timestep_embed field."""
    assert not hasattr(params, "timestep_embed")
# ---
def test_valid_position_mask_has_true_entries(tok):
    source = "def f(x):\n    return x + 1\n"
    mask = tok.valid_position_mask(source)

    assert len(mask) == tok.num_position_tokens
    assert any(mask), "Should have at least one valid edit position"

    # Position 0 should be valid (start of FunctionDef).
    assert mask[0] is True
# ---
def encode_char(self, char: str) -> int:
        """Encode a single character to a base vocabulary token ID."""
        code = ord(char)
        if code < self.base_vocab_size:
            return self.base_token_offset + code
        return self.base_token_offset
# ---
def test_collect_workdir_size_mb_nonexistent_directory():
    """Test workdir size returns 0 for non-existent directory."""
    nonexistent = Path("/nonexistent/path/does/not/exist")

    size_mb = collect_workdir_size_mb(nonexistent)

    assert size_mb == 0
# ---
def click_save_button(self):
        """
        :rtype: BrowseMoviePage
        """
        self._click(AddMoviePageLocators.SAVE_BUTTON_LOCATOR)
        return BrowseMoviePage(self._driver)
# ---
def __init__(self, perm):
        self.perm = perm
# ---
def test_olmo2_attention(use_flash, num_kv_heads):
    config = _get_olmo2_config(use_flash=use_flash, num_kv_heads=num_kv_heads)
    key = random.PRNGKey(0)

    attention = Olmo2Attention.init(config=config, key=key)

    x, mask = _get_random_inputs(config)
    out = attention(x, mask)

    # Check output has correct shape
    assert out.array.shape == x.array.shape
    assert out.axes == x.axes
# ---
def __enter__(self):
    self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()
    for objects in self.custom_objects:
      _GLOBAL_CUSTOM_OBJECTS.update(objects)
    return self
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.two.unique() == 'foo').sum()
# ---
def init(HeadDim, config):
        return YarnRotaryEmbeddings(HeadDim, config)
# ---
def the_object_name1_has_a_boolean_difference_by_name2(name1, name2):
    obj = the_object_name_exists(name1)
    for modifier in obj.modifiers:
        if modifier.type == "BOOLEAN" and modifier.object and modifier.object.name == name2:
            return True
    assert False, "No boolean found"
# ---
def get_actor_name_from_actor_info(self, actor_info) -> str:
        raise NotImplementedError()
# ---
def __init__(self, radius):
        self.radius = radius
# ---
def raise_if_expired(self, message: str = "Deadline exceeded") -> None:
        """Raise TimeoutError if deadline has passed."""
        if self.expired():
            raise TimeoutError(message)
# ---
def trainer_config():
    return TrainerConfig()
# ---
def handler(self):
                pass
# ---
def resolve_axis(self, axis: AxisSelector) -> Axis:
        """Resolve an axis selector to the corresponding axis in the current view."""
        name = axis_name(axis)
        for ax in self.axes:
            if ax.name == name:
                return ax
        raise ValueError(f"Axis {name} is not present in this reference view")
# ---
def generate_numbers(number_string):
        if "-" in number_string:  # it's a range
            start, end = map(int, number_string.split("-"))
            return [str(i).zfill(len(number_string.split("-")[0])) for i in range(start, end + 1)]
        else:  # it's a single number
            return [number_string]
# ---
def test_build_env_flags_quotes_values_with_spaces(self):
        """Env var values with spaces are properly quoted."""
        config = config_pb2.BootstrapConfig(
            docker_image="gcr.io/test/iris:latest",
            worker_port=10001,
            env_vars={"MSG": "hello world"},
        )
        flags = _build_env_flags(config, vm_address="10.0.0.1")
        assert "MSG='hello world'" in flags or 'MSG="hello world"' in flags
# ---
def load_mongo(self, mongo_uri: Union[str, None] = None):
        if mongo_uri:
            self.mongo_uri = mongo_uri
            self.client = MongoClient(mongo_uri)
        else:
            self.mongo_uri = "localhost:27017"
            self.client = MongoClient()

        self._core = self.client["core"]
# ---
def ray_cluster():
    if not ray.is_initialized():
        logging.info("Initializing Ray cluster")
        ray.init(
            address="local",
            num_cpus=8,
            ignore_reinit_error=True,
            logging_level=logging.INFO,
            log_to_driver=True,
            resources={"head_node": 1},
        )
        yield
# ---
def __add_fuzzing_boundaries(self):
        lower_bound = -1
        if self.ui.spinBoxLowerBound.isEnabled():
            lower_bound = self.ui.spinBoxLowerBound.value()

        upper_bound = -1
        if self.ui.spinBoxUpperBound.isEnabled():
            upper_bound = self.ui.spinBoxUpperBound.value()

        num_vals = self.ui.spinBoxBoundaryNumber.value()
        self.fuzz_table_model.add_boundaries(lower_bound, upper_bound, num_vals)
# ---
def visit_mroot(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            index = self._visit(children[1])
            return BracedNode(f"\\sqrt[{index}]{{{base}}}")
        return TextNode("")
# ---
def init(cls, Vocab: Axis, config: GemmaConfig, *, key):
        k_t, k_emb = jrandom.split(key, 2)
        transformer = Gemma2Transformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return Gemma2LMHeadModel(transformer, embeddings, lm_head)
# ---
def join_protocol(file):
        if protocol:
            return f"{protocol}://{file}"
        return file
# ---
def test_deduplicate_with_num_output_shards(backend):
    """Test deduplication with explicit num_output_shards."""
    data = [{"id": i % 3, "val": f"item_{i}"} for i in range(20)]

    ds = Dataset.from_list(data).deduplicate(key=lambda x: x["id"], num_output_shards=5)

    results = list(Backend.execute(ds, context=backend))

    # Should have exactly 3 unique items (ids 0, 1, 2)
    assert len(results) == 3
    ids = sorted([r["id"] for r in results])
    assert ids == [0, 1, 2]
# ---
def __init__(self, delta, *args, **kwargs):
        super(TimeDeltaSensor, self).__init__(*args, **kwargs)
        self.delta = delta
# ---
def _precond_grad(Q, G, exprs):
    """Precondition gradient G with preconditioner Q."""
    exprP = exprs[-1]
    return jnp.einsum(exprP, *Q, *Q, G)
# ---
def __init__(self, pattern, fields, cls):
        self.pattern = pattern
        self.fields = fields
        self.query_class = cls

        subqueries = []
        for field in self.fields:
            subqueries.append(cls(field, pattern, True))
        super().__init__(subqueries)
# ---
def test_dense(self):
    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 2))

    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 2))

    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(None, None, 2))

    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 5, 2))
# ---
def session():
    Session = sessionmaker()
    engine = create_engine(MYSQL_CONNECTION_STRING)
    Session.configure(bind=engine)
    metadata.create_all(engine)
    try:
        yield Session()
    except:
        pass
# ---
def action_traceability(self, cr, uid, ids, context=None):
        """ It traces the information of a product
        @param self: The object pointer.
        @param cr: A database cursor
        @param uid: ID of the user currently logged in
        @param ids: List of IDs selected
        @param context: A standard dictionary
        @return: A dictionary of values
        """
        return self.pool.get('action.traceability').action_traceability(cr,uid,ids,context)
# ---
def test_get_rrd_server(self):
        self.flags(xenapi_connection_url='myscheme://myaddress/')
        server_info = vm_utils.get_rrd_server()
        self.assertEqual(server_info[0], 'myscheme')
        self.assertEqual(server_info[1], 'myaddress')
# ---
def _get_memory_total_bytes() -> int:
    try:
        with open("/proc/meminfo") as f:
            for line in f:
                if line.startswith("MemTotal:"):
                    return int(line.split()[1]) * 1024  # kB to bytes
    except FileNotFoundError:
        pass
    # Fallback for non-Linux
    return 8 * 1024**3
# ---
def i_rename_the_object_name1_to_name2(name1, name2):
    the_object_name_exists(name1).name = name2
# ---
def union(matches, root, cwd):
    """Union a list of matchers.

    If the list is empty, return nevermatcher.
    If the list only contains one non-None value, return that matcher.
    Otherwise return a union matcher.
    """
    matches = list(filter(None, matches))
    if len(matches) == 0:
        return nevermatcher(root, cwd)
    elif len(matches) == 1:
        return matches[0]
    else:
        return unionmatcher(matches)
# ---
def test_unique_parallel(self):
        # TODO: test without file
        def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.four.unique() == 3.0).sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
# ---
def decrement(self, stats, sample_rate=1):
        """
        Decrements one or more stats counters
        """
        self.update_stats(stats, -1, sample_rate)
# ---
def status(self) -> JobStatus: ...
# ---
def _in_block(block_name: str, node: RenderTreeNode) -> bool:
    while node.parent:
        if node.parent.type == block_name:
            return True
        node = node.parent
    return False
# ---
def _create(self, fname):
        '''call oc create on a filename'''
        return self.openshift_cmd(['create', '-f', fname])
# ---
def __iter__(self):
        return iter(self._variables)
# ---
def Log (self,
             Message,
             Delay_In_Seconds = 0.0):
        if self.Next_Step is None:
            self.Next_Step = 1
        if self.Start_Time is None:
            self.Start_Time = clock ()

        logging.info ("     " + str (self.Next_Step - 1) + ": " + Message)
        sleep (Delay_In_Seconds)
# ---
def get(self, item):
        """Return the element in the queue identical to `item`.

        Parameters
        ----------
        item :
            The element to search for.

        Returns
        -------
        The element in the queue identical to `item`. If the element
        was not found, None is returned.

        """
        for _, element in self._queue:
            if item == element:
                return element
        return None
# ---
def refresh_window_title(self):
        s = "%s %s" % (QCoreApplication.applicationName(),
                       QCoreApplication.applicationVersion())
        if self.filename is not None:
            s += " - " + self.filename
        if self.dirty:
            s += "*"
        self.setWindowTitle(s)
# ---
def state():
    """Create a fresh ControllerState for each test."""
    return ControllerState()
# ---
def num_rows(self):
        if self._cached_num_rows is not None:
            return self._cached_num_rows
        result = int(self.offsets[0].read().result())
        if self._cache_metadata:
            self._cached_num_rows = result
        return result
# ---
def fn(x):
        return x + 5
# ---
def __call__(self, target, creds, enforcer):
        """Check that there is a matching role in the cred dict."""

        return self.match.lower() in [x.lower() for x in creds['roles']]
# ---
def tracker(name):
            def go(conn, *args, **kw):
                canary.append(name)
            return go
# ---
def test_to_obj():
    msg = '{"aa": 1, "bb": ["hoge", "hogi"], "cc": {"cc1" : 50}}'
    converted = jps.utils.to_obj(msg)
    assert converted.aa == 1
    assert converted.bb[0] == 'hoge'
    assert converted.bb[1] == 'hogi'
    assert len(converted.bb) == 2
    assert converted.cc.cc1 == 50
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        """Total peak FLOP/s across all devices."""
        if isinstance(self.device, CpuConfig):
            # just use some reasonable number
            return 100e9
        return self.device_flops(dtype) * self.chip_count()
# ---
def VHeadSize(self) -> Axis:
        return Axis("v_head_dim", self.v_head_dim)
# ---
def uninstall():
    paths = LAUNCHER_PATH, DATA_PATH
    for p in paths:
        print("Deleting", p, "...", end=" ", flush=True)
        shutil.rmtree(p)
        print("Success!")
    print("Removing desktop shortcut...", end=" ", flush=True)
    desktop = os.path.join(os.path.expanduser('~'), 'Desktop')
    shortcut_path = os.path.join(desktop, "Augur Launcher.lnk")
    os.remove(shortcut_path)
    print("Success!")
# ---
def on_trash(self):
		super(Item, self).on_trash()
		frappe.db.sql("""delete from tabBin where item_code=%s""", self.name)
		frappe.db.sql("delete from `tabItem Price` where item_code=%s", self.name)
		for variant_of in frappe.get_all("Item", filters={"variant_of": self.name}):
			frappe.delete_doc("Item", variant_of.name)
# ---
def __post_init__(self):
        if self.wandb is not None:
            warnings.warn(
                "wandb is deprecated. use tracker with type wandb instead",
                DeprecationWarning,
            )
            self.tracker = self.wandb
# ---
def flops_per_token(self, vocab_size: int, seq_len: int) -> float:
        """Return FLOPs per token for this model configuration."""
        ...
# ---
def do_time_ctrl(self, command):
        msg = Forseti.TimeControl()
        msg.command_name = command
        self.lc.publish('Timer/Control', msg.encode())
# ---
def user_name_represent(id):
            # @todo: use s3_present_user?

            rep_str = "-"
            table = db.auth_user
            query = (table.id == id)
            row = db(query).select(table.first_name,
                                   table.last_name,
                                   limitby=(0, 1)).first()
            if row:
                rep_str = "%s %s" % (row.first_name, row.last_name)
            return rep_str
# ---
def compute_advantages(self, rollout_group: list[Rollout]) -> list[float]:
        """Compute advantages for a group of rollouts."""
        return compute_rloo_advantages(rollout_group)
# ---
def maximum(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.maximum(x1, x2)
# ---
def chip_count(self) -> int:
        return get_tpu_topology(self.variant).chip_count
# ---
def address(self) -> str:
        port = self._actual_port or self._port
        return f"{self._host}:{port}"
# ---
def stub_add_volume_type_access(context, type_id, project_id):
            raise exception.VolumeTypeAccessExists(volume_type_id=type_id,
                                                   project_id=project_id)
# ---
def test_impl(df):
            return df.A.str.replace('AB*', 'EE', regex=True)
# ---
def shard_names(self) -> Sequence[str]:
        return self.source.shard_names
# ---
def _normalize(s):
        """Normalize a Unicode string's representation (used on both
        patterns and matched values).
        """
        return unicodedata.normalize('NFC', s)
# ---
def __call__(self, indices: int | np.ndarray) -> int | np.ndarray:
        """Apply the permutation to the given indices.

        Args:
            indices: An integer or an array of integers to be permuted.

        Returns:
            The permuted value(s).
        """
        pass
# ---
def prompt_subreddit(self):
        "Open a prompt to navigate to a different subreddit"

        name = self.term.prompt_input('Enter page: /')
        if name is not None:
            with self.term.loader('Loading page'):
                content = SubredditContent.from_name(
                    self.reddit, name, self.term.loader)
            if not self.term.loader.exception:
                self.selected_subreddit = content
                self.active = False
# ---
def is_worker_failure(self) -> bool:
        """Whether this attempt failed due to worker death (derived from state)."""
        return self.state == cluster_pb2.TASK_STATE_WORKER_FAILED
# ---
def get_remote_url(git_path, module, dest, remote):
    '''Return URL of remote source for repo.'''
    command = [git_path, 'ls-remote', '--get-url', remote]
    (rc, out, err) = module.run_command(command, cwd=dest)
    if rc != 0:
        # There was an issue getting remote URL, most likely
        # command is not available in this version of Git.
        return None
    return to_native(out).rstrip('\n')
# ---
def screen(self):
        """The screen this window is fullscreen in.  Read-only.

        :type: `Screen`
        """
        return self._screen
# ---
def action_traceability(self, cr, uid, ids, context=None):
        """ It traces the information of a product
        @param self: The object pointer.
        @param cr: A database cursor
        @param uid: ID of the user currently logged in
        @param ids: List of IDs selected
        @param context: A standard dictionary
        @return: A dictionary of values
        """
        value=self.pool.get('action.traceability').action_traceability(cr,uid,ids,context)
        return value
# ---
def active():
    proc = multiprocessing.active_children()
    arr = []
    for p in proc:
        print(p.pid)
        arr.append(p.pid)

    return str(arr)
# ---
def all_vms(self) -> list[ManagedVm]:
        """Return a snapshot of all tracked VMs."""
        with self._lock:
            return list(self._vms.values())
# ---
def sigmoid(a: A) -> A:
    return wrap_elemwise_unary(jnn.sigmoid, a)
# ---
def test_cpu_device_produces_no_device(self):
        resources = ResourceConfig(device=CpuConfig())
        spec = convert_resources(resources)
        assert spec.device is None
# ---
def testIf(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        if 123:
          print 'foo'
        if '':
          print 'bar'""")))
# ---
def title(self):
        return self.__title
# ---
def nonexist(id) :
    db = cherrypy.session['database']
    sql =  "UPDATE Radio set exist = 0 WHERE id = '%s'" % (id)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def _update(self, records, value):
        # special case, when an integer field is used as inverse for a one2many
        records._cache[self] = value.id or 0
# ---
def main(ad_exchange_buyer, owner_name, body, is_transient):
  try:
    # Construct and execute the request.
    filter_set = ad_exchange_buyer.bidders().filterSets().create(
        ownerName=owner_name, isTransient=is_transient, body=body).execute()
    print(f'FilterSet created for bidder: "{owner_name}".')
    pprint.pprint(filter_set)
  except HttpError as e:
    print(e)
# ---
def preprocess_image(screen_image):

    # crop the top and bottom
    screen_image = screen_image[35:195]

    # down sample by a factor of 2
    screen_image = screen_image[::2, ::2]

    # convert to grey scale
    grey_image = np.zeros(screen_image.shape[0:2])
    for i in range(len(screen_image)):
        for j in range(len(screen_image[i])):
            grey_image[i][j] = np.mean(screen_image[i][j])

    return np.array([grey_image.astype(np.float)])
# ---
def reload(self, weight_callback: WeightSource):
        """Reload the model weights using the provided callback.

        Args:
            weight_callback: Function that takes the current model and returns new model
        """
        self.inference_context.reload(weight_callback)
# ---
def test_filter_expression_equality(backend):
    """Test filter with equality expression."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"category": "A", "value": 1},
            {"category": "B", "value": 2},
            {"category": "A", "value": 3},
        ]
    ).filter(col("category") == "A")

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert all(r["category"] == "A" for r in results)
# ---
def ListSessions(self, request, context):
        """Lists all sessions in a given database.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def test_repr(self):
        expr = col("score")
        assert repr(expr) == "col('score')"
# ---
def test_str_contains_regex(self):
        def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('AB*', regex=True)
            return B.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), 2)
# ---
def walkSpeed(self):
        return ToontownGlobals.GoofySpeed
# ---
def to_numpy_histogram(self) -> tuple[np.ndarray, np.ndarray]:
        return np.array(self.bucket_counts), np.array(self.bucket_limits)
# ---
def make_axes(**kwargs: int) -> tuple[Axis, ...]:
    """
    Convenience function for creating a tuple of Axis objects.

    Example:
    ```
    X, Y = axes(X=10, Y=20)
    ```

    """
    return tuple(Axis(name, size) for name, size in kwargs.items())
# ---
def test_setup_failure_does_not_kill_capturing(testdir):
    sub1 = testdir.mkpydir("sub1")
    sub1.join("conftest.py").write(_pytest._code.Source("""
        def pytest_runtest_setup(item):
            raise ValueError(42)
    """))
    sub1.join("test_mod.py").write("def test_func1(): pass")
    result = testdir.runpytest(testdir.tmpdir, '--traceconfig')
    result.stdout.fnmatch_lines([
        "*ValueError(42)*",
        "*1 error*"
    ])
# ---
def wait_all(self, timeout: float | None = None) -> list[CallResult]:
        results = []
        for endpoint, future in self._futures:
            try:
                value = future.result(timeout=timeout)
                results.append(CallResult(endpoint=endpoint, value=value))
            except Exception as e:
                results.append(CallResult(endpoint=endpoint, exception=e))
        return results
# ---
def is_point_on_right(self, point):
        return self.a * point[0] + self.b * point[1] + self.c < 0
# ---
def rebuild(c):
    """`build` with the delete switch"""
    c.run('pelican -d -s pelicanconf.py')
# ---
def qInitResources():
    QtCore.qRegisterResourceData(rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data)
# ---
def validate(self, task):
        """Validate driver_info for ipmitool driver.

        Check that node['driver_info'] contains IPMI credentials.

        :param task: a TaskManager instance containing the node to act on.
        :raises: InvalidParameterValue if required ipmi parameters are missing.
        :raises: MissingParameterValue if a required parameter is missing.

        """
        _parse_driver_info(task.node)
# ---
def keep_first(k, items: Iterator[T]) -> T:
            """Reducer that keeps the first item."""
            return next(iter(items))
# ---
def cast_floats(x):
        if eqx.is_array(x) and jnp.issubdtype(x.dtype, jnp.floating):
            return x.astype(expected_float_dtype)
        return x
# ---
def test_fdcapture_tmpfile_remains_the_same(tmpfile, use):
    if not use:
        tmpfile = True
    cap = StdCaptureFD(out=False, err=tmpfile)
    try:
        cap.start_capturing()
        capfile = cap.err.tmpfile
        cap.readouterr()
    finally:
        cap.stop_capturing()
    capfile2 = cap.err.tmpfile
    assert capfile2 == capfile
# ---
def __repr__(self) -> str:
        return f"{self.child}.is_null()"
# ---
def load_results_file(path: str) -> dict:
    fs = fsspec.filesystem(path.split("://", 1)[0] if "://" in path else "file")
    with fs.open(path, "r") as f:
        data = json.load(f)
        return {"runs": [data]} if "runs" not in data else data
# ---
def python_path(self) -> str:
        """Path to the Python binary in the venv (e.g., venv/bin/python)."""
        return os.path.join(self.venv_path, "bin", "python")
# ---
def test_non_lexical(self):
        """Test that cmp_version compares non-lexically"""
        self.assertTrue(vmops.cmp_version('1.2.3.10', '1.2.3.4') > 0)
# ---
def run(self):
        logger.debug("Guardando en {} el catÃ¡logo {}".format(self.output().path, self.catalog_name))

        with closing(requests.get(self.catalog_url, stream= True)) as response, \
             self.output().open('w') as output_file:
            for chunk in response.iter_lines(chunk_size=1024*8):
                if chunk:
                    output_file.write(chunk.decode('utf-8') + '\n')
# ---
def test_actor_named_without_get_if_exists(job_context):
    """Test that named actors without get_if_exists create new instances."""
    actor1 = job_context.create_actor(SimpleActor, 10, name="actor", get_if_exists=False)
    job_context.get(actor1.increment.remote(5))

    with pytest.raises(ValueError):
        job_context.create_actor(SimpleActor, 20, name="actor", get_if_exists=False)
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        return ScaleByCautiousState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)
# ---
def eot_token_id(self) -> int:
        """Return the end-of-text token ID."""
        return self.tokenizer.eos_token_id
# ---
def clear_basic_auth_token():
    global SAAGIE_BASIC_AUTH_TOKEN
    SAAGIE_BASIC_AUTH_TOKEN = None
# ---
def files(self):
        """Explicitly listed files or patterns or roots:
        if no patterns or .always(): empty list,
        if exact: list exact files,
        if not .anypats(): list all files and dirs,
        else: optimal roots"""
        return self._files
# ---
def delete(self):
        """
            Delete this job and all its items from the job table
        """

        db = current.db

        _debug("Deleting job ID=%s" % self.job_id)
        self.__define_tables()
        item_table = self.item_table
        query = item_table.job_id == self.job_id
        db(query).delete()
        job_table = self.job_table
        query = job_table.job_id == self.job_id
        db(query).delete()
# ---
def crumble(self, instance, attrs):
        return {
            'id': instance.id.hex,
            'name': instance.label,
            'dateCreated': instance.date_created,
        }
# ---
def __len__(self):
        return len(self._index_to_obj)
# ---
def guide_entry(self):
        '''Write the XML element for the guide.
        (Empty string if no guide title and type are given.)'''
        if self.guide_title and self.guide_type:
            return _make_xml_elem('reference', '',
              [
                ('title', self.guide_title),
                ('type', self.guide_type),
                ('href', self.name)
              ])
        else:
            return ''
# ---
def test_df_input2(self):
        def test_impl(df):
            C = df.B == 'two'
            return C.sum()

        n = 11
        df = pd.DataFrame({'A': np.random.ranf(3 * n), 'B': ['one', 'two', 'three'] * n})
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))
# ---
def clear_widgets(self):
        """Release all registered widgets from the spell of auto-completion."""
        for w in set(self.widgets):
            self.remove_widget(w)
# ---
def saveProfileImage(self, filestorage):
        buffer = filestorage.stream
        buffer.seek(0)
        image = Image.open(buffer)
        image = ImageUtil.crop_image(image, 64)
        current_app.logger.info(image)
        dirpath = getDirectoryPath(current_app, '_settings')
        filepath = os.path.join(dirpath, "profile.png")
        image.save(filepath, optimize=True)
# ---
def update_linear(layer: Linear, grad_layer: Linear) -> Linear:
        lr_scale = layer.reparam.lr_scale
        new_weight = layer.weight - (base_lr * lr_scale) * grad_layer.weight
        if layer.bias is None or grad_layer.bias is None:
            new_bias = layer.bias
        else:
            new_bias = layer.bias - base_lr * grad_layer.bias
        return dataclasses.replace(layer, weight=new_weight, bias=new_bias)
# ---
def test_iterated_centroid():
    """ensure that the average across each dimension is returned"""
    new_centroid = kmeans.iterated_centroid([[1, 1, 1], [2, 2, 2]],\
            [[100, 200, 300]], [(0, 0), (1, 0)])
    np.testing.assert_allclose(new_centroid, np.array([[1.5, 1.5, 1.5]]),\
            rtol=1e-5)
# ---
def code_block(node: RenderTreeNode, context: RenderContext) -> str:
    return fence(node, context)
# ---
def set_visible(self, visible=True):
        """Show or hide the window.

        :Parameters:
            `visible` : bool
                If True, the window will be shown; otherwise it will be
                hidden.

        """
        raise NotImplementedError('abstract')
# ---
def inspect_side_effect(container_id):
        call_count[0] += 1
        if call_count[0] == 1:
            return ContainerStatus(running=True)
        return ContainerStatus(running=False, exit_code=1, error="Container crashed")
# ---
def addOperators(self, num, target):
        """
        Adapted from https://leetcode.com/discuss/58614/java-standard-backtrace-ac-solutoin-short-and-clear

        Algorithm:
        1. DFS
        2. Special handling for multiplication
        3. Detect invalid number with leading 0's
        :type num: str
        :type target: int
        :rtype: List[str]
        """
        ret = []
        self.dfs(num, target, 0, "", 0, 0, ret)
        return ret
# ---
def test_fn_exc():
    raise ValueError()
# ---
def setUp(self):
        fixture_path = "spec/fixtures/responses/whois.nic.pw/status_available.txt"
        host         = "whois.nic.pw"
        part         = yawhois.record.Part(open(fixture_path, "r").read(), host)
        self.record  = yawhois.record.Record(None, [part])
# ---
def ensure():
    """One-time check to ensure desired number of TPU VMs are running across all zones."""
    tpu_client = tpu_v2.TpuClient()
    for zone, count in config.TPU_ZONES_CONFIG.items():
        ensure_tpu_vms(tpu_client, zone, count)
# ---
def encrypt(pw):
    from hashlib import md5
    return md5(pw).hexdigest()
# ---
def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LmDatasetSourceConfigBase:
        return HfDatasetSourceConfig(
            id=self.id,
            name=self.name,
            tags=self.tags,
            cache_dir=actual_output_path,
            format=self.format,
        )
# ---
def __init__(
        self,
        cache_dir: str,
        exemplar: T,
        metadata: Optional["CacheMetadata"] = None,
        shard_name: str = "",
    ):
        self.cache_dir = cache_dir
        self.metadata = metadata
        self._exemplar = exemplar
        self._shard_name = shard_name
        self._tree_store = TreeStore.open(exemplar, self.cache_dir, mode="w", cache_metadata=True)
        self._is_closed = False
# ---
def _lookup_indices(self, axis: AxisSelector) -> int | None:  # type: ignore
        ...
# ---
def _get_vm_logs(controller_url: str, vm_id: str, tail: int) -> tuple[str, str, int]:
    client = cluster_connect.ControllerServiceClientSync(controller_url)
    request = cluster_pb2.Controller.GetVmLogsRequest(vm_id=vm_id, tail=tail)
    response = client.get_vm_logs(request)
    return response.logs, response.vm_id, response.state
# ---
def test_causal_mask():
    mask = _make_causal_mask(4)
    expected = jnp.array(
        [
            [True, False, False, False],
            [True, True, False, False],
            [True, True, True, False],
            [True, True, True, True],
        ]
    )
    assert jnp.array_equal(mask, expected)
# ---
def get_node(self):
        return self.node
# ---
def retranslateUi(self, Dialog):
        Dialog.setWindowTitle(_translate("Dialog", "Samples Manager", None))
# ---
def url(self):
        return (JOBS_URL_PATTERN + '/%s') % (self.platform_id, self.id)
# ---
def list_all_workers(self) -> list[ControllerWorker]:
        with self._lock:
            return list(self._workers.values())
# ---
def __iter__(self) -> Iterator[T]:
        return iter(self._index_to_obj)
# ---
def _get_view_id(self, cr, uid, type):
        """Get the view id suiting the given type

        @param type: the picking type as a string
        @return: view i, or False if no view found
        """
        res = self.pool.get('ir.model.data').get_object_reference(cr, uid,
            'stock', self._VIEW_LIST.get(type, 'view_picking_form'))
        return res and res[1] or False
# ---
def get_ready_event(self):
        """
        Returns an Event that is set when the control greenlet is up and running.
        """
        return self._ready_control
# ---
def md5_for_file(fname):
    hash_md5 = hashlib.md5()
    with open(fname, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return str(hash_md5.hexdigest())
# ---
def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the OrCheck object for convenience.
        """

        self.rules.append(rule)
        return self
# ---
def remove_title(html: BeautifulSoup):
    # Title is added by markdown parser
    title = html.find("title")
    if title:
        title.decompose()
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "google/gemma-2-2b"
    ) -> HFCheckpointConverter["Gemma2Config"]:  # type: ignore
        return HFCheckpointConverter(
            self,
            reference_checkpoint=ref_checkpoint,
            trust_remote_code=True,
            HfConfigClass=HfGemma2Config,
        )
# ---
def model_type(cls) -> type["MixtralLMHeadModel"]:
        return MixtralLMHeadModel
# ---
def body(i, rc):
                def dec(rc):
                    page = pages_row["page", i].scalar()
                    return rc.at["page", page].add(-1)

                return jax.lax.cond(valid["page", i].scalar(), dec, lambda x: x, rc)
# ---
def flatten_axes(axis: AxisSpec, new_axis: AxisSelector) -> Axis:
    pass
# ---
def _requeue_task(self, task: ControllerTask, txn: TransactionLog) -> None:
        """Put task back on scheduling queue for retry."""
        task.state = cluster_pb2.TASK_STATE_PENDING
        if task.task_id not in self._task_queue:
            self._task_queue.append(task.task_id)
        txn.log("task_requeued", task.task_id)
# ---
def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass
# ---
def filter_lora_params(params: M) -> M:
    """
    Filters LoRA parameters from the given parameter tree.
    """

    return eqx.filter(params, is_lora_param, is_leaf=lambda x: is_lora_param(x) or isinstance(x, haliax.NamedArray))
# ---
def test_end_with_ellipsis():
    partial_order = ("apple", ..., "banana", ...)
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)

    # this one could be either but we'll assert the order so we catch changes
    assert actual_output == ("apple", "banana", "cherry")
# ---
def transform(self, x, y, z, *, static1, static2):
            assert static1 is True
            assert static2 is False
            return x + self.w + y + z
# ---
def exp(a: A) -> A:
    return wrap_elemwise_unary(jnp.exp, a)
# ---
def set_trigger_recording_on_release(self, trigger_recording):
        self._should_trigger_recording = trigger_recording
# ---
def output(self):
        return luigi.LocalTarget(os.path.join(os.getcwd(), "data", "hola_mundo_desde_python.json"))
# ---
def test_cannot_scale_down_at_min_slices(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """can_scale_down() returns False when at min_slices."""
        discovered = [make_mock_vm_group("slice-001")]
        manager = make_mock_vm_manager(vm_groups_to_discover=discovered)
        group = ScalingGroup(scale_group_config, manager)
        group.reconcile()

        assert group.slice_count() == 1  # min_slices
        assert not group.can_scale_down()
# ---
def _merge_expand(aligned_self, other, overwrite_vars, compat):
    possible_conflicts = dict((k, v) for k, v in aligned_self._variables.items()
                              if k not in overwrite_vars)
    new_vars, new_coord_names = _expand_variables(other, possible_conflicts, compat)
    replace_vars = aligned_self._variables.copy()
    replace_vars.update(new_vars)
    return replace_vars, new_vars, new_coord_names
# ---
def _make_explicit_data_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape((len(devices),))
    return Mesh(mesh_devices, axis_names=("data",), axis_types=(AxisType.Explicit,))
# ---
def create_router(self, router):
        """Handling create router event."""
        pass
# ---
def matrix_replacer(content: str) -> str:
        return f"[{content}]"
# ---
def onload(self):
		super(Item, self).onload()

		self.set_onload('stock_exists', self.stock_ledger_created())
		self.set_asset_naming_series()
# ---
def __init__(self):
        self.lf = 0.2  # Learning factor lambda
        self.data = []  # The features' values for all the games
        self.rewards = []  # Reward values for moving from 1 state to the next
        self.rt = np.array([])
        self.max_iter = 50
# ---
def test_raw_metric_host_cpu(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'cpu_usagemhz_rate_average',
        host_name)

    for record in query:
        if record.cpu_usagemhz_rate_average is not None:
            assert record.cpu_usagemhz_rate_average > 0, 'Zero Host CPU Usage'
            break
# ---
def __init__(self, config: dict, env=None):
        creds_string, _ = get_credentials(env)
        self.gcp_wrapper = gcp.GcpWrapper(json.loads(creds_string))
        self.config = config
# ---
def test_group_by_empty(backend):
    """Test groupby on empty dataset."""
    ds = Dataset.from_list([]).group_by(
        key=lambda x: x["cat"], reducer=lambda key, items: {"cat": key, "count": sum(1 for _ in items)}
    )

    results = list(Backend.execute(ds, context=backend))
    assert results == []
# ---
def byte_size(self):
        return self.data.nbytes + self.offsets.nbytes + (self.shapes.nbytes if self.shapes is not None else 0)
# ---
def __init__(self, rules=None, default_rule=None):
        """Initialize the Rules store."""

        super(Rules, self).__init__(rules or {})
        self.default_rule = default_rule
# ---
def test_get_ages(self):
        self.assertEqual(self.config.get_ages(), ['15', '43'])
# ---
def push(self, item: Any) -> None:
        with httpx.Client() as client:
            client.post(f"http://{self.host}:{self.port}/queues/{self.queue_name}/push", content=pickle.dumps(item))
# ---
def match_info(self):
        """return matched info after resolving route"""
        return self.app.router.get(self)[2]
# ---


def derivative(xs: list):
    """ xs represent coefficients of a polynomial.
    xs[0] + xs[1] * x + xs[2] * x^2 + ....
     Return derivative of this polynomial in the same form.
    >>> derivative([3, 1, 2, 4, 5])
    [1, 4, 12, 20]
    >>> derivative([1, 2, 3])
    [2, 6]
    """
    return [(i * x) for i, x in enumerate(xs)][1:]
# ---
def resize_embeddings(self, new_size: int, key: Optional[PRNGKeyArray] = None):
        new_weights = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, Vocab=self.Vocab.resize(new_size), token_embeddings=new_weights)
# ---
def test_spawn_fail_cleanup_2(self):
        """Simulates an error while creating VM record.

        It verifies that VDIs created are properly cleaned up.

        """
        vdi_recs_start = self._list_vdis()
        stubs.stubout_create_vm(self.stubs)
        self.assertRaises(xenapi_fake.Failure,
                          self._test_spawn, 1, 2, 3)
        # No additional VDI should be found.
        vdi_recs_end = self._list_vdis()
        self._check_vdis(vdi_recs_start, vdi_recs_end)
# ---
def _iter_progress(self, stream):
        chunk = ''
        while True:
            c = stream.read(1)
            if not c:
                raise OutputParserError('copy-disk stream closed unexpectedly')
            chunk += c
            if c == '\r':
                yield chunk
                chunk = ''
# ---
def get_device_type_enum(device: cluster_pb2.DeviceConfig) -> DeviceType:
    """Extract device type as enum from DeviceConfig.

    Args:
        device: DeviceConfig proto from job request

    Returns:
        DeviceType enum value
    """
    if device.HasField("gpu"):
        return DeviceType.GPU
    elif device.HasField("tpu"):
        return DeviceType.TPU
    return DeviceType.CPU
# ---
def format_time(seconds):
    return '{}:{:02}'.format(seconds // 60,
                             seconds % 60)
# ---
def config(self) -> LmConfigT:
        pass
# ---
def current_demand(self) -> int:
        """Current demand level."""
        return self._current_demand
# ---
def execute(self, cmd_name: str, *args, **kwargs):
        """Execute a command by name."""
        if cmd_name in self.commands:
            return self.commands[cmd_name](*args, **kwargs)
        else:
            console.print(f"[red]Unknown command: {cmd_name}[/red]")
# ---
def get_init_log(self, vm_id: str, tail: int | None = None) -> str:
        """Get initialization log for a VM.

        Returns empty string if VM is not found.
        """
        vm = self.get_vm(vm_id)
        return vm.init_log(tail) if vm else ""
# ---
def test_axis_names_metadata():
    field = M.__dataclass_fields__["a"]
    assert field.metadata["axis_names"] == ("batch",)
# ---
def job_context():
    """Ensure a shared job context for all tests."""
    from fray.job.context import create_job_ctx, fray_default_job_ctx

    # Use threadpool context for tests to avoid Ray overhead unless needed
    ctx = create_job_ctx("threadpool")
    with fray_default_job_ctx(ctx):
        yield ctx
# ---
def decorate_as_link(self, url, link_type, name):
        allure_link_marker = '{prefix}.{link_type}'.format(prefix=ALLURE_LINK_PREFIX, link_type=link_type)
        pattern = dict(self.config.option.allure_link_pattern).get(link_type, u'{}')
        url = pattern.format(url)
        allure_link = getattr(pytest.mark, allure_link_marker)
        return allure_link(url, name=name, link_type=link_type)
# ---
def setLastColor(self, lastColor):
        self._program.setLastColor(lastColor)
# ---
def __enter__(self):
        self._prev = jax_config.abstract_mesh_context_manager.swap_local(jax_config.config_ext.unset)
        return self
# ---
def unregister_endpoint(
        self,
        request: cluster_pb2.Controller.UnregisterEndpointRequest,
        ctx: Any,
    ) -> cluster_pb2.Empty:
        """Unregister a service endpoint. Idempotent."""
        self._state.remove_endpoint(request.endpoint_id)
        return cluster_pb2.Empty()
# ---
def get_padding_count_from_batch(batch: LmExample, pad_token_id: int) -> tuple[int, int]:
    """
    Extract padding statistics from a batch (on host).
    """
    tokens = jax.device_get(batch.tokens.array)
    padding_count = int(np.sum(tokens == pad_token_id))
    total_tokens = batch.tokens.size
    return padding_count, total_tokens
# ---
def max_pages_per_seq(self) -> int:
        return (self.max_seq_len + self.page_size - 1) // self.page_size
# ---
def submit(self, coro: Coroutine[Any, Any, T]) -> concurrent.futures.Future[T]:
        """
        Submit an async function without waiting for result.

        Args:
            coro: Coroutine to run

        Returns:
            Future that can be awaited or checked later
        """
        if self._loop is None:
            raise RuntimeError("AsyncBridge not started. Call start() first.")

        return asyncio.run_coroutine_threadsafe(coro, self._loop)
# ---
def flatten_for_export(self: Mod) -> Mod:
        """
        Flatten articulated named arrays for export to torch. In general this method should, for a linear layer, flatten
        all input axes into a single axis, and all output axes into a single axis. You can do whatever else
        you want to support pytorch-compatible serialization if you want.

        This method is less general than to_state_dict and is only called when using to_torch_compatible_state_dict.
        """
        return self
# ---
def _set_value(self, name, value):
		index = self.structure.index(name)
		col = self.structure[index]
		self._values[name] = col.to_python(value, self)
		self[index] = value
# ---
def resizeable(self):
        """True if the window is resizable.  Read-only.

        :type: bool
        """
        return self._resizable
# ---
def chordHelper(self, half_steps, tunning, strength):
        notes = [tunning*(2.**(half_steps[i]/12.)) for i in range(len(half_steps))]
        hpcp = HPCP(maxShifted=False)([notes[0], notes[1], notes[2]], strength)
        for i in range(len(hpcp)):
            if i in half_steps: self.assertTrue(hpcp[i]>0)
            elif (i - 12) in half_steps: self.assertTrue(hpcp[i]>0)
            else: self.assertEqual(hpcp[i], 0)
# ---
def _get_velocity(self):
        return self._parameter2
# ---
def __init__(self, internal_name, main_controller):
        self.internal_name = internal_name
        self.main_controller = main_controller

        self._init_plugin()
# ---
def __init__(self, m1, m2):
        super(intersectionmatcher, self).__init__(m1._root, m1._cwd)
        self._m1 = m1
        self._m2 = m2
        self.bad = m1.bad
        self.traversedir = m1.traversedir
# ---
def calls(self):
        return self._calls
# ---
def the_object_name_is_placed_in_the_collection_collection(name, collection):
    obj = the_object_name_exists(name)
    [c.objects.unlink(obj) for c in obj.users_collection]
    bpy.data.collections.get(collection).objects.link(obj)
# ---
def RegisterEndpoint(self, request, context):
        """Endpoint registry (generic service discovery)"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def show_info(self):
        QMessageBox.about(
            self, QApplication.applicationName(),
            "%s %s\n"
            "Copyright (c) by %s" %
            (
                QCoreApplication.applicationName(),
                QCoreApplication.applicationVersion(),
                QCoreApplication.organizationName(),
            )
        )
# ---
def _assert_no_data(self):
        eq_(
            testing.db.scalar(self.table.count()), 0
        )
# ---
def vocab_size(self) -> int:
        return llama3_tokenizer_vocab_size
# ---
def test_encrypt_with_leading_newlines(self):
        self._test_encryption('\n\nMessage with leading newlines.')
# ---
from typing import List


def intersperse(numbers: List[int], delimeter: int) -> List[int]:
    """ Insert a number 'delimeter' between every two consecutive elements of input list `numbers'
    >>> intersperse([], 4)
    []
    >>> intersperse([1, 2, 3], 4)
    [1, 4, 2, 4, 3]
    """
    if not numbers:
        return []

    result = []

    for n in numbers[:-1]:
        result.append(n)
        result.append(delimeter)

    result.append(numbers[-1])

    return result
# ---
def cleanup(files):
        '''Clean up on exit '''
        for sfile in files:
            if os.path.exists(sfile):
                if os.path.isdir(sfile):
                    shutil.rmtree(sfile)
                elif os.path.isfile(sfile):
                    os.remove(sfile)
# ---
def _check_cluster_head_running(config_path: str) -> bool:
    """Check if a Ray cluster head is already running.

    Returns True if a cluster head is detected, False otherwise.
    """
    try:
        with ray_dashboard(DashboardConfig.from_cluster(config_path)):
            return True
    except Exception:
        return False
# ---
def remove_all_iris_containers(self) -> int: ...
# ---
def test_finalize_tracks_no_tracks(self):
        """ Tests that finalize fails if there are no tracks for an experiment """
        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)
        response = self.client.post(reverse("ab_testing_tool_finalize_tracks", args=(experiment.id,)),
                                    follow=True)
        self.assertError(response, NO_TRACKS_FOR_EXPERIMENT)
# ---
def _find_free_port(self) -> int:
        for port in range(self._range[0], self._range[1]):
            if port in self._allocated:
                continue
            if self._is_port_free(port):
                return port
        logger.warning("Port allocation exhausted: no free ports in range %d-%d", self._range[0], self._range[1])
        raise RuntimeError("No free ports available")
# ---
def test_resolved_param_mapping_inherits_shared():
    cfg = MeshConfig()
    # shared mapping defaults map mlp/heads to model
    mapping = cfg.resolved_param_mapping
    assert mapping["mlp"] == "model"
    assert mapping["heads"] == "model"
    # embed should come from the default param_mapping override
    assert mapping["embed"] == "data"
# ---
def __call__(self, msg):
        self._saved_msg.append(msg)
# ---
def _is_retryable_hf_exception(exc: Exception) -> bool:
    if isinstance(exc, HfHubHTTPError):
        status_code = getattr(getattr(exc, "response", None), "status_code", None)
        return status_code in {408, 429} or (status_code is not None and 500 <= status_code < 600)

    return isinstance(exc, (requests.exceptions.Timeout, requests.exceptions.ConnectionError))
# ---
def SetQuickFixList( quickfix_list ):
  """Populate the quickfix list and open it. List should be in qflist format:
  see ":h setqflist" for details."""
  vim.eval( 'setqflist( {0} )'.format( json.dumps( quickfix_list ) ) )
# ---
def is_enabled(self) -> bool:
        return len(self.watch_targets) > 0 and self.interval > 0 and (self.include_norms or self.include_histograms)
# ---
def __init__(self, string=None, **kwargs):
        kwargs['string'] = string
        attrs = {key: val for key, val in kwargs.iteritems() if val is not None}
        self._attrs = attrs or EMPTY_DICT
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        """Run command synchronously and wait for completion."""
        ...
# ---
def visit_munderover(self, element):
        children = self._get_clean_children(element)
        if len(children) == 3:
            base = self._visit(children[0])
            under = self._visit(children[1])
            over = self._visit(children[2])
            return BracedNode(f"{{{base}}}_{{{under}}}^{{{over}}}")
        return TextNode("")
# ---
def Embed(self) -> Axis:
        # Not used by GrugWrapper (it returns logits directly), but LmConfig requires it.
        return Axis("embed", self.hidden_dim)
# ---
def build(self, ctx: LrScheduleContext):
        return optax.linear_schedule(ctx.learning_rate, ctx.min_lr, ctx.decay_steps)
# ---
def test_update(self):
        db = smadata2.db.sqlite.create_or_update(self.dbname)
# ---
def _build_service(vocab_size=10):
    model = DummyModel(vocab_size=vocab_size, eos_id=3)
    service = InferenceEngine.from_model_with_config(
        model=model,  # type: ignore
        tokenizer=None,
        config=InferenceEngineConfig(
            max_seq_len=32,
            max_pages=64,
            max_seqs=8,
            page_size=8,
            compute_dtype=jnp.float32,
            max_queued_tokens=64,
            max_seqs_in_prefill=4,
        ),
    )
    return service
# ---
def test_or_both_false(self):
        expr = (col("a") > 0) | (col("b") > 0)
        assert expr.evaluate({"a": -1, "b": -1}) is False
# ---
def __define_tables(self):
        """
            Define the database tables for jobs and items
        """

        self.job_table = self.define_job_table()
        self.item_table = self.define_item_table()
# ---
def __call__(self, *args):
        return self.handle(self.meth_name, self.action, *args)
# ---
def normalize_math_symbols(text: str) -> str:
    """Replace LaTeX symbols with text equivalents."""
    for latex, replacement in MATH_SYMBOL_REPLACEMENTS.items():
        text = text.replace(latex, replacement)
    return text
# ---
def _build_cluster_images(config) -> None:
    for tag, typ in [(config.defaults.bootstrap.docker_image, "worker"), (config.controller.image, "controller")]:
        if tag:
            params = _extract_image_params(tag, typ)
            if params:
                _build_and_push_image(params)
                click.echo()
# ---
def convert_to_export(self, value, env):
        if not isinstance(self.selection, list):
            # FIXME: this reproduces an existing buggy behavior!
            return value if value else ''
        for item in self._description_selection(env):
            if item[0] == value:
                return item[1]
        return False
# ---
def from_job(cls, job):
        """Create runner from RLJob."""

        _, rollout_config = job.to_worker_configs()
        return cls(rollout_config)
# ---
def enterChatty(self):
        self.chatty.enter()
        self.acceptOnce(self.chattyDoneEvent, self.__decideNextState)
# ---
def prefix(self):
        return self._prefix
# ---
def is_all_none(structure):
  iterable = nest.flatten(structure)
  # We cannot use Python's `any` because the iterable may return Tensors.
  for element in iterable:
    if element is not None:
      return False
  return True
# ---
def _reload(current_model: LmHeadModel) -> LmHeadModel:
                return weight_loader(self.server, self.config.server, current_model)
# ---
def list_tasks(self, job_id: JobName) -> list[cluster_pb2.TaskStatus]:
        return self._remote_client.list_tasks(job_id)
# ---
def test_context_run(job_context):
    future = job_context.run(lambda x: x * 2, 5)
    assert job_context.get(future) == 10
# ---
def add_arguments(parser: argparse.ArgumentParser):
    parser.add_argument("--model_path", type=lambda s: Path(s), required=True)
# ---
def __str__(self):
        materialString = ""
        if self.__material is not None:
            materialString = " ({0})".format(self.__material)
        return "{0}{1}".format(super(Sculpture, self).__str__(),
                               materialString)
# ---
def create_task(coro: Awaitable) -> asyncio.Task:
    return asyncio.get_event_loop().create_task(coro)
# ---
def initial_cache(pages: int) -> int:
            table = PageTable.init(pages, config.max_seqs, config.page_size, max_pages_per_seq)
            cache_shape = model.initial_cache(table.spec(), dtype=config.compute_dtype)
            return cache_shape
# ---
def test_transaction_engine_fn_commit(self):
        fn = self._trans_fn()
        testing.db.transaction(fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def set_if_frequency(self, if_frequency):
        self._if_frequency = if_frequency
        self._if_period = 1 / if_frequency * 1e9
# ---
def mock_open(path, mode="rb"):
            if path in files:
                import io

                return io.BytesIO(files[path])
            raise FileNotFoundError(f"File not found: {path}")
# ---
def __init__(self, workspace: Path):
        self._workspace = workspace
# ---
def _setup_pkgresources():
    import pkg_resources
    import os
    import plistlib

    pl = plistlib.readPlist(os.path.join(
        os.path.dirname(os.getenv('RESOURCEPATH')), "Info.plist"))
    appname = pl.get('CFBundleIdentifier')
    if appname is None:
        appname = pl['CFBundleDisplayName']
    path = os.path.expanduser('~/Library/Caches/%s/python-eggs' % (appname,))
    pkg_resources.set_extraction_path(path)
# ---
def check_vm_params_for_linux(self):
        self.assertEquals(self.vm['platform']['nx'], 'false')
        self.assertEquals(self.vm['PV_args'], '')
        self.assertEquals(self.vm['PV_bootloader'], 'pygrub')

        # check that these are not set
        self.assertEquals(self.vm['PV_kernel'], '')
        self.assertEquals(self.vm['PV_ramdisk'], '')
        self.assertEquals(self.vm['HVM_boot_params'], {})
        self.assertEquals(self.vm['HVM_boot_policy'], '')
# ---
def __hash__(self):
        return hash((self.field, hash(self.pattern)))
# ---
def test_remove_project_access_with_no_admin_user(self):
        req = fakes.HTTPRequest.blank('/v2/%s/types/%s/action' % (
            fake.PROJECT_ID, fake.VOLUME_TYPE3_ID), use_admin_context=False)
        body = {'removeProjectAccess': {'project': PROJ2_UUID}}
        self.assertRaises(exception.PolicyNotAuthorized,
                          self.type_action_controller._removeProjectAccess,
                          req, fake.VOLUME_TYPE3_ID, body)
# ---
def unwrap_namedarrays(*a):
    return tuple(x.array if isinstance(x, NamedArray) else x for x in a)
# ---
def getStatus(self):
        return self.status
# ---
def parameter_count(model: PyTree):
    # especially with jax.vjp, we get duplicate arrays and want to uniq them
    # NB we need to use object identity here, mostly because of ShapedDtypeStruct
    leaves = {id(x): x for x in jax.tree_util.tree_leaves(model) if is_jax_array_like(x)}
    return sum(x.size for x in leaves.values())
# ---
def sleep_forever():
        while True:
            time.sleep(1)
# ---
def merge_translations(blddir, sources, langs):
    for lang in langs:
        subprocess.call([
            'itstool', '-m', os.path.join(blddir, lang, lang + '.gmo'),
            '-o', os.path.join(blddir, lang)
        ] + sources)
# ---
def stop(self):
        self._send_message(_Message.STOP)
# ---
def test_robust_quad_logx_fits_quadratic():
    """Test that robust_quad_logx recovers known coefficients from synthetic data."""
    x = jnp.array([1e9, 1e10, 1e11, 1e12])
    L = jnp.log10(x)
    # y = 0.1 * L^2 - 2 * L + 20
    y = 0.1 * L**2 - 2 * L + 20

    a, b, c = robust_quad_logx(x, y)

    assert abs(a - 0.1) < 0.01
    assert abs(b - (-2)) < 0.1
    assert abs(c - 20) < 0.5
# ---
def __init__(self):
        self._actors: dict[str, Any] = {}
        self._actor_locks: dict[str, threading.Lock] = {}
# ---
def __init__(self,
                row_dict_I,
                ):
        self.analysis_id=row_dict_I['analysis_id'];
        self.experiment_id=row_dict_I['experiment_id'];
        self.sample_name_abbreviation=row_dict_I['sample_name_abbreviation'];
        self.sample_name=row_dict_I['sample_name'];
        self.time_point=row_dict_I['time_point'];
        self.analysis_type=row_dict_I['analysis_type'];
        self.used_=row_dict_I['used_'];
        self.comment_=row_dict_I['comment_'];
# ---
def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the AndCheck object for convenience.
        """

        self.rules.append(rule)
        return self
# ---
def pad_token_id(self) -> int:
        return self.the_tokenizer.pad_token_id
# ---
def status(self) -> ControllerStatus:
        if self._controller:
            return ControllerStatus(
                running=True,
                address=self._controller.url,
                healthy=True,
            )
        return ControllerStatus(running=False, address="", healthy=False)
# ---
def is_finite(self) -> bool:
        return True
# ---
def scheduler(state):
    """Create a Scheduler instance."""
    return Scheduler(state)
# ---
def matchfn(self, f):
        # XXX: is_dir is set to True here for performance.
        # It should be set to whether "f" is actually a directory or not.
        return self._matcher.match_relative(f, True)
# ---
def _coerce_iterator_output(self, expr, state=None):
        import supriya.patterns

        if not isinstance(expr, supriya.patterns.Event):
            expr = supriya.patterns.NoteEvent(**expr)
        if expr.get("uuid") is None:
            expr = new(expr, uuid=uuid.uuid4())
        return expr
# ---
def parse_rule(rule):
    """Parses a policy rule into a tree of Check objects."""

    # If the rule is a string, it's in the policy language
    if isinstance(rule, basestring):
        return _parse_text_rule(rule)
    return _parse_list_rule(rule)
# ---
def opendb(self):
        self.prepare_sqlite()
        return smadata2.db.sqlite.create_or_update(self.dbname)
# ---
def active_scale(self):
        return 1
# ---
def recipient(self, record):
        return record['person'].email
# ---
def default_choice_name(cls) -> str | None:
        return "url"
# ---
def getcapture(self, **kw):
        cap = self.__class__.captureclass(**kw)
        cap.start_capturing()
        try:
            yield cap
        finally:
            cap.stop_capturing()
# ---
def _run_event_loop(self):
        """Run the event loop in this thread."""
        self._loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self._loop)

        # Signal that loop is ready
        self._started.set()

        # Run forever until stopped
        self._loop.run_forever()

        # Cleanup
        self._loop.close()
# ---
def testRaiseTypeAndArg(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        try:
          raise KeyError('foo')
          print 'bad'
        except KeyError as e:
          print e""")))
# ---
def _summary_block() -> dict:
        if config_name:
            summary = size_info.get("config")
            if summary is not None:
                return summary
        summary = size_info.get("dataset")
        if summary is not None:
            return summary
        return {}
# ---
def __repr__(self) -> str:
        return f"({self.left} {_COMPARE_SYMBOLS[self.op]} {self.right})"
# ---
def cabinet_decision(last_id=-1):
    '''Retrive cabinet decisions
    if last_id not defiend it will return the max
    return list of cabinet decisions tuples up to MAX_PAGES_TO_SEARCH (page=10)
    [(id, title, url, text)...]
    '''
    decisions = []
    _news = retrieve_news(cabinet=1, last_id=last_id)
    for item in _news:
        _detail = retrieve_detail(item)
        decisions.append(_detail)
    return decisions
# ---
def __next__(self):
        if self._exhausted:
            raise StopIteration
        try:
            return self._run_async_task(self.async_iter.__anext__())
        except StopAsyncIteration:
            self._exhausted = True  # Mark the iterator as exhausted
            if self.loop.is_running():
                self.loop.call_soon_threadsafe(self.loop.stop)
            self.thread.join()
            raise StopIteration
# ---
def take(self, n: int):
        """
        Alias for `slice_dataset(end_index=n)`.
        """
        return self.slice_dataset(end_index=n)
# ---
def purge_queue_of_slot(self, slot_id: hax.NamedArray | int) -> "DecodeState":
        """Forward ``purge_queue_of_slot`` to ``TokenQueue`` and return an updated ``DecodeState``."""
        new_tqueue = self.tqueue.purge_queue_of_slot(slot_id)
        return dataclasses.replace(self, tqueue=new_tqueue)
# ---
def check_solver_status(status, raise_error=False):
    """Perform standard checks on a solver's status."""
    if status == optlang.interface.OPTIMAL:
        return
    elif status == optlang.interface.INFEASIBLE and not raise_error:
        warn("solver status is '{}'".format(status), UserWarning)
    elif status is None:
        raise RuntimeError(
            "model was not optimized yet or solver context switched")
    else:
        raise OptimizationError("solver status is '{}'".format(status))
# ---
def test_registrant_contacts(self):
        eq_(self.record.registrant_contacts.__class__.__name__, 'list')
        eq_(self.record.registrant_contacts, [])
# ---
def discover_new(self) -> list[ActorHandle]:
        """Return handles that are ready but haven't been yielded yet.

        After wait_ready(count=1), subsequent calls to discover_new() will
        return the remaining handles as they become available. For LocalClient
        all handles are ready immediately, so this returns whatever wait_ready
        didn't return on its first call.
        """
        ...
# ---
def dummy_entrypoint():
    """A simple entrypoint for testing."""
    pass
# ---
def test_line_start_escaping():
    """Tests -, #, + at the beginning of lines."""
    test_cases = [
        ("- hyphen", "\\- hyphen"),
        ("+ plus", "\\+ plus"),
        ("# header", "\\# header"),
        ("Normal line - not escaped", "Normal line - not escaped"),
    ]
    for text, expected in test_cases:
        assert minimal_markdown_escape(text) == expected
# ---
def test_rearrange_with_ellipsis():
    assert einops_rearrange(z, "... w c -> ... c w").axes == (B, D, H, C, W)
    assert einops_rearrange(z, "b d ... -> d ... b").axes == (D, H, W, C, B)

    assert einops_rearrange(z, "b ... c -> b c ...").axes == (B, C, D, H, W)
    # make sure the values are right too
    z_t = z.array.transpose((0, 4, 1, 2, 3))
    assert (einops_rearrange(z, "b ... c -> b c ...").array == z_t).all()
# ---
def extract_results(result: Any) -> tuple[list[str], list[str]]:
    if isinstance(result, (pa.RecordBatch, pa.Table)):
        id_col = "doc_id" if "doc_id" in result.schema.names else "id"
        return result["hash"].to_pylist(), result[id_col].to_pylist()
    return [x["hash"] for x in result], [x["id"] for x in result]
# ---
def __init__(self, remote, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)
        self.remote = remote
        self.InitUI()
# ---
def setUp(self):
        self.prepare_sqlite()
# ---
def __init__(self, *args, **kwargs):
        self.module = kwargs.get('module', None)
        self.client = F5RestClient(**self.module.params)
        self.want = ModuleParameters(params=self.module.params, client=self.client)
        self.have = ApiParameters()
        self.changes = UsableChanges()
# ---
def lcm(a, b):
        return a * b / gcd(a, b)
# ---
def test_format_shard_path_local_paths():
    """Test that local paths also get normalized."""
    pattern = "/tmp//output//data-{shard:05d}.jsonl"
    result = format_shard_path(pattern, 0, 1)
    assert result == "/tmp/output/data-00000.jsonl"
# ---
def indent(self, text, level):
        return markdownify.line_beginning_re.sub("    " * level, text) if text else ""
# ---
def balanced_accuracy_scoring(clf, X, y):
    """Scoring function that computes the balanced accuracy to be used
    internally in the cross-validation procedure.
    """
    y_pred = clf.predict(X)
    conf_mat = confusion_matrix(y, y_pred)
    bal_acc = 0.
    for i in range(len(conf_mat)):
        bal_acc += (float(conf_mat[i, i])) / np.sum(conf_mat[i])

    bal_acc /= len(conf_mat)
    return bal_acc
# ---
def ball(key, shape: AxisSpec, D: Axis, p: float = 2.0, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.ball(key=key, shape=jax_shape, d=D.size, p=p, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, concat_axes(shape, D)))
# ---
def open(exemplar: T, path: str, *, mode="a", cache_metadata: bool = False) -> "TreeStore":
        """
        Open a TreeStoreBuilder from a file.
        """
        tree = _construct_builder_tree(exemplar, path, mode, cache_metadata)
        return TreeStore(tree, path, mode)
# ---
def format_tmps(tmps):
    return format_line(prefix='temps'.rjust(RJUST), values=tmps)
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.var()
# ---
def loss_fn(params, token_ids, loss_mask):
        return ar_loss(params, token_ids, loss_mask, config)
# ---
def on_answer(self, *args):
        pass
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            Ac = df.A.rolling(3, center=True).apply(lambda a: a[0] + 2 * a[1] + a[2])
            return Ac.sum()
# ---
def _is_named_or_none(x):
    return x is None or is_named_array(x)
# ---
def url_query_params(url):
    """Return query parameters as a dict from the specified URL.

    :param url: URL.
    :type url: str
    :rtype: dict
    """
    return dict(parse_qsl(urlparse(url).query, True))
# ---
def GetBufferFilepath( buffer_object ):
  if buffer_object.name:
    return buffer_object.name
  # Buffers that have just been created by a command like :enew don't have any
  # buffer name so we use the buffer number for that.
  return os.path.join( GetCurrentDirectory(), str( buffer_object.number ) )
# ---
def _job_client(self) -> JobSubmissionClient:
        return JobSubmissionClient(self._dashboard_address)
# ---
def __call__(self, batch: Sequence[Sequence[int]]) -> Sequence[dict[str, np.ndarray]]:
        # return pa.RecordBatch.from_arrays([pa.array(batch)], ["test"])
        return [{"test": np.asarray(x)} for x in batch]
# ---
def init(Vocab: Axis, config: ParallelLlamaConfig, *, key) -> "ParallelLlamaEmbedding":
        token_embeddings = hnn.Embedding.init(Vocab, config.Embed, key=key)
        return ParallelLlamaEmbedding(token_embeddings)
# ---
def do_shard(x, y):
            x = hax.shard(x, resource_map)
            assert_inside_pjit(x, NamedSharding(mesh, PartitionSpec(None, ResourceAxis.DATA)))

            y = hax.shard(y, resource_map)
            assert_inside_pjit(y, NamedSharding(mesh, PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)))

            return x, y
# ---
def test_expression_filter_pushdown(self, sync_backend, vortex_file):
        """Test filter pushdown with expression."""
        ds = Dataset.from_files(str(vortex_file)).load_vortex().filter(col("score") > 500)

        results = list(Backend.execute(ds, context=sync_backend))
        assert len(results) == 49  # scores 510, 520, ..., 990
        assert all(r["score"] > 500 for r in results)
# ---
def create_tpu_group(vms: list[MagicMock]) -> VmGroupProtocol:
        from iris.time_utils import Timestamp

        return TpuVmGroup(
            group_id="test-slice-001",
            scale_group="test-group",
            zone="us-central1-a",
            project_id="test-project",
            vms=vms,
            vm_registry=registry,
            created_at=Timestamp.from_ms(1234567890),
        )
# ---
def init_x_grad():
        x_grad_tile_ref[...] = jax.lax.dot_general(
            xw_scratch_ref[...],
            w_ref[...],
            (((1,), (1,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        x_write_future.start()
# ---
def add_trigger(self, trigger):
        """ Add a recomputation trigger on ``self``. """
        if trigger not in self._triggers:
            self._triggers += (trigger,)
# ---
def add_instruction(self, math_problem: str) -> str:
        """Add the standard instruction to a math problem."""
        return f"{math_problem}{self.question_suffix()}"
# ---
def dump_to_store(self, store, encoder=None, sync=True):
        """Store dataset contents to a backends.*DataStore object."""
        variables, attrs = conventions.encode_dataset_coordinates(self)
        if encoder:
            variables, attrs = encoder(variables, attrs)
        store.store(variables, attrs)
        if sync:
            store.sync()
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "hparams",
                "hparams": hparams,
            }
        )
        try:
            self.logger.info(json.dumps(record))
        except TypeError as e:
            logger.info(f"Oh noes... {e}")
# ---
def config(self) -> QwenConfig:
        return self.transformer.config
# ---
def footnotes(self, text):
        """Wrapper for all footnotes.

        :param text: contents of all footnotes.
        """
        html = '<div class="footnotes">\n%s<ol>%s</ol>\n</div>\n'
        return html % (self.hrule(), text)
# ---
def do_clone(gs_clone: GenState) -> GenState:
                new_state, _ = gs_clone.clone_sequence(
                    parent_val,
                    child_local_id=slot_val,
                    seq_params=seq_params,
                )
                return new_state
# ---
def is_finite(self) -> bool:
        return self.dataset.is_finite() and self.end_index is not None
# ---
def chip_count(self) -> int:
        """Total number of TPU chips."""
        return get_tpu_topology(self.variant).chip_count
# ---
def bmarks():
    return_data = get_bmarklet()
    return return_data
# ---
def softbreak(node: RenderTreeNode, context: RenderContext) -> str:
    if context.do_wrap and _in_block("paragraph", node):
        return WRAP_POINT
    return "\n"
# ---
def __is_nice_string_using_new_rules(self, string):
        return (self.__regex_double_pair.search(string)
            and self.__regex_triplet.search(string))
# ---
def __init__(
        self, plotly_name="bordercolor", parent_name="sankey.hoverlabel", **kwargs
    ):
        super(BordercolorValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            array_ok=kwargs.pop("array_ok", True),
            edit_type=kwargs.pop("edit_type", "calc"),
            **kwargs
        )
# ---
def __init__(self, object_ref: ray.ObjectRef):
        self._object_ref = object_ref
# ---
def get_ls_l_header():
    return (BOLD() +
            'State' + DELIMITER('   ') +
            'Last modified' + DELIMITER('       ') +
            'Size' + DELIMITER('      ') +
            'Name' + DELIMITER(' (') +
            'ID' + DELIMITER(')') +
            ENDC())
# ---
def stop(self):
        """Stop streaming and wait for thread to exit."""
        self._stop_event.set()
        if self._thread:
            self._thread.join(timeout=5.0)
# ---
def create_mesh():
    """Create a simple JAX mesh for testing."""
    devices = jax.local_devices()[:1]
    return jax.sharding.Mesh(np.array(devices), axis_names=("batch",))
# ---
def grayscale():
   global image1
   r, g, b = image1.split()
   image1 = Image.merge("RGB", (g,g,g))

   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def fetch_logs(self):
        job_data = requests.get(self.url, auth=SAAGIE_BASIC_AUTH_TOKEN).json()
        run_data = job_data.get('last_instance')
        if run_data is None or run_data['status'] not in ('SUCCESS', 'FAILED'):
            return
        run_data = requests.get(
            get_absolute_saagie_url('/api/v1/jobtask/%s'
                                    % run_data['id']), auth=SAAGIE_BASIC_AUTH_TOKEN).json()
        self.last_run = SaagieJobRun(self, run_data)
# ---
def destroy_process(self, process_id):
        """Destroy process.

        Disable the process and remove the process
        manager for the processes that no longer are running vpn service.
        """
        if process_id in self.processes:
            process = self.processes[process_id]
            process.disable()
            if process_id in self.processes:
                del self.processes[process_id]
# ---
def test_log_summary():
    with tempfile.TemporaryDirectory() as tmpdir:
        with SummaryWriter(logdir=tmpdir) as writer:
            tracker = TensorboardTracker(writer)
            tracker.log_summary({"float": 2.0})
            tracker.log_summary({"str": "test"})
            tracker.log_summary({"scalar_jax_array": jnp.array(3.0)})
            tracker.log_summary({"scalar_np_array": np.array(3.0)})
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None:
        conn = _direct_ssh(_apply_manual_overrides(self._ssh_config, group_config.manual), slice_id)
        cmd = (
            "sudo docker stop iris-worker 2>/dev/null || "
            "sudo docker kill iris-worker 2>/dev/null || true; "
            "sudo docker rm -f iris-worker 2>/dev/null || true"
        )
        conn.run(cmd, timeout=Duration.from_seconds(60))
# ---
def test_order(self):
        assert (event.Priority.PRE_CORE
                > event.Priority.CORE
                > event.Priority.POST_CORE
                > event.Priority.PRE_DEFAULT
                > event.Priority.DEFAULT
                > event.Priority.POST_DEFAULT)
# ---
def fray_cluster():
    set_current_cluster(create_cluster("local"))
    yield
    set_current_cluster(None)
# ---
def retrying_upload_folder(*args, **kwargs):
    return upload_folder(*args, **kwargs)
# ---
def inspect(self, container_id: str) -> ContainerStatus: ...
# ---
def strip_axis(self, axis: AxisSelector):
        if isinstance(axis, Axis):
            index = self.main_axes.index(axis)
        else:
            index = index_where(lambda a: a.name == axis, self.main_axes)
        return NamedArray(self.array, self.main_axes[:index] + self.main_axes[index + 1 :])
# ---
def from_named_array(array: hax.NamedArray, num_bins: int = 31) -> "Histogram":
        raw_array = array.array
        min = raw_array.min()
        max = raw_array.max()
        num = array.size
        sum = raw_array.sum()
        sum_squares = (raw_array**2).sum()
        counts, edges = sharded_histogram(array, bins=num_bins)
        return Histogram(min, max, num, sum, sum_squares, edges, counts)
# ---
def dequantize(x, dq_dtype, scale):
    return x.astype(dq_dtype) * jnp.broadcast_to(scale.astype(dq_dtype), x.shape)
# ---
def VariableExists( variable ):
  return GetBoolValue( "exists( '{0}' )".format( EscapeForVim( variable ) ) )
# ---
def getposition(self, data):
        return self.positions[data]
# ---
def test_get_last_historic_missing(self):
        serial = "__TEST__"

        last = self.db.get_last_historic(serial)
        assert last is None
# ---
def get_default_screen(self):
            """Get the default screen as specified by the user's operating system
            preferences.

            :rtype: `Screen`
            """
            raise NotImplementedError('deprecated')
# ---
def test_scales_up_when_demand_exceeds_capacity(self, empty_autoscaler: Autoscaler):
        """Evaluates scale-up when demand > capacity."""
        demand = [DemandEntry(device_type=DeviceType.TPU, device_variant="v5p-8", count=2)]
        decisions = empty_autoscaler.evaluate(demand)

        assert len(decisions) == 1
        assert decisions[0].action == ScalingAction.SCALE_UP
        assert decisions[0].scale_group == "test-group"
        assert "demand=2 > capacity=0" in decisions[0].reason
# ---
def attention_config(self) -> AttentionConfig:
        """Convert this MixtralConfig to an AttentionConfig for use with Attention."""
        return AttentionConfig(
            Embed=self.Embed,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            use_bias=self.use_bias,
            upcast_attn=self.upcast_attn,
            attn_backend=self.attn_backend,
            flash_attention_block_size=self.flash_attention_block_size,
            rope=self.rope,
        )
# ---
def text(self):
        """Get indent text as \t or string of spaces
        """
        if self.useTabs:
            return '\t'
        else:
            return ' ' * self.width
# ---
def test_mods_to_tei(datadir):
    main("--inplace", datadir / "mods_to_tei.py", datadir / "mods_to_tei.xml")
    assert equal_documents(datadir / "mods_to_tei.xml", datadir / "mods_to_tei_exp.xml")
# ---
def extract_hashes(body_contents):
        hashers = {k: HASH_TYPES_DICT[k]() for k in HASH_TYPES_DICT}

        while True:
            chunk = body_contents.read(512 * 16)
            if not chunk:
                break
            for h in hashers.values():
                h.update(chunk)

        return hashers.items()
# ---
def __init__(self, root, cwd, badfn=None):
        super(nevermatcher, self).__init__(root, cwd, badfn)
# ---
def eq_(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def test_list_type_with_no_admin_default(self):
        expected = {'volume_types': [{'id': fake.VOLUME_TYPE_ID},
                                     {'id': fake.VOLUME_TYPE2_ID}]}
        req = fakes.HTTPRequest.blank('/v2/%s/types' % fake.PROJECT_ID,
                                      use_admin_context=False)
        result = self.type_controller_v2.index(req)
        self.assertVolumeTypeListEqual(expected['volume_types'],
                                       result['volume_types'])
# ---
def test_roll_bad_named_shift():
    H = Axis("H", 4)
    W = Axis("W", 3)

    arr = hax.arange((H, W))
    shift = hax.arange((Axis("dummy", 2),))

    with pytest.raises(TypeError):
        hax.roll(arr, shift, H)
# ---
def extent(self):
        from mapproxy.layer import MapExtent
        return MapExtent(self.bbox, self.srs)
# ---
def _merge_lora_modules(module):
        if isinstance(module, LoraLinear):
            return module.merge()
        else:
            return module
# ---
def the_object_name_is_voided_by_void(name, void):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    for rel in element.HasOpenings:
        if rel.RelatedOpeningElement.Name == void:
            return True
    assert False, "No void found"
# ---
def cluster(request, local_cluster, ray_cluster):
    if request.param == "local":
        return local_cluster
    elif request.param == "ray":
        return ray_cluster
# ---
def _log_edit_metrics(step: int, metrics: dict) -> None:
    """Log training metrics."""
    loss = float(metrics["loss"])
    acc = float(metrics["accuracy"])
    ppl = float(metrics["perplexity"])
    grad_norm = float(metrics["grad_norm"])
    num_tokens = float(metrics["num_loss_tokens"])

    logger.info(
        f"step={step:06d} loss={loss:.4f} acc={acc:.4f} "
        f"ppl={ppl:.2f} grad_norm={grad_norm:.4f} loss_tokens={num_tokens:.0f}"
    )
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        """Block until job completes."""
        ...
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: Optional[str] = None
    ) -> HFCheckpointConverter["Olmo2Config"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfOlmo2Config,
        )
# ---
def client(self) -> IrisClient:
        """IrisClient for this cluster."""
        if self._rpc_client is None:
            self._rpc_client = IrisClient.remote(
                self.controller_url,
                workspace=self._workspace,
            )
        return self._rpc_client
# ---
def __init__(self, id, *, split, streaming: bool = True, **kwargs):
        self.id = id
        self.split = split
        self.streaming = streaming
        self.kwargs = kwargs
        self._shard_names = self._compute_shard_names()
# ---
def __delitem__(self, key):
        del self.subqueries[key]
# ---
def _tri_upper_eq_mask(Ci: Axis, Cj: Axis) -> NamedArray:
    """Mask for i <= j (upper-triangular incl. diagonal) in (Ci, Cj) coordinates.

    Used to zero-out invalid contributions when building strictly lower-triangular
    in-chunk operators for the UT forward substitution.
    """
    ii = hax.arange(Ci)
    jj = hax.arange(Cj)
    I = ii.broadcast_axis(Cj)
    J = jj.broadcast_axis(Ci)
    return I <= J
# ---
def index(self, request):
        queryset = Condition.objects.select_related('source', 'target_option')
        serializer = ConditionIndexSerializer(queryset, many=True)
        return Response(serializer.data)
# ---
def materialize_mask(
    mask: Optional[NamedArray | AttentionMask],
    QPos: Axis,
    KPos: Axis,
    q_slice: Optional[haliax.dslice] = None,
    k_slice: Optional[haliax.dslice] = None,
) -> Optional[NamedArray]: ...
# ---
def _setup_related(self, env):
        super(Selection, self)._setup_related(env)
        # selection must be computed on related field
        field = self.related_field
        self.selection = lambda model: field._description_selection(model.env)
# ---
def dirty(self):
        return self._dirty
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        """Stop batch generation and wait for thread."""
        self.stop_flag.set()
        if self.thread:
            self.thread.join(timeout=2)
        return False
# ---
def unlink(self, cr, uid, ids, context=None):
        raise osv.except_osv(_('Error!'), _('You cannot remove a lot line.'))
# ---
def bytes_to_string( btext ):
    #btext = int('0b110100001100101011011000110110001101111', 2)
    return btext.to_bytes((btext.bit_length() + 7) // 8, 'big').decode()
# ---
def address(self) -> str:
        return f"{self.host}:{self.port}"
# ---
def host(self):
        # it appears that httptools doesn't return the host
        # so pull it from the headers
        return self.headers.get("Host", "")
# ---
def test_sharded_tree_size_shape_dtype_struct_without_sharding():
    mesh = jax.sharding.AbstractMesh((4,), ("data",))
    struct = jax.ShapeDtypeStruct((8, 4), jnp.float32)

    per_device_bytes = sharded_tree_size(struct, mesh=mesh)

    assert per_device_bytes == (8 * 4 * jnp.dtype(jnp.float32).itemsize)
# ---
def pad_non_aligned_v_block():
        if v_dim % v_block_size != 0:
            rem = v_dim % v_block_size
            w_ref[:, rem:] = jnp.zeros((w_ref.shape[0], w_ref.shape[1] - rem), dtype=w_ref.dtype)
# ---
def vmap_via(self, fn: VmapFunction[M, P, OutputT_co]) -> Callable[P, OutputT_co]: ...
# ---
def reject(self, match, include_accepted=False):
        self._call_all('reject', match, include_accepted)
# ---
def _node_type_name(node: ast.AST) -> str:
    return type(node).__name__
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "artifact",
                "path": artifact_path,
                "name": name,
                "artifact_type": type,
            }
        )
        self.logger.info(json.dumps(record))
# ---
def _get_discount_invoice(self, cr, uid, move_line):
        '''Return the discount for the move line'''
        return 0.0
# ---
def inner(method, notebook, data, *args, **kwargs):
        if not is_logged():
            return views.render('login_form', notebook)
        return view(method, notebook, data, *args, **kwargs)
# ---
def test_arrow_marshaling(benchmark: Any, in_memory_table: pa.Table, batch_size: int | None) -> None:
    """
    Benchmarks Python Memory -> Rust -> Arrow Batches with different chunk sizes.
    batch_size=None simulates the "Giant" case (one massive batch).
    """

    def _pipeline() -> int:
        return sum(len(dupekit.process_arrow_batch(b)) for b in in_memory_table.to_batches(max_chunksize=batch_size))

    assert benchmark(_pipeline) > 0
# ---
def testGetFormatStringAttributeNames(self):
    """Tests the GetFormatStringAttributeNames function."""
    event_formatter = chrome_cache.ChromeCacheEntryEventFormatter()

    expected_attribute_names = [u'original_url']

    self._TestGetFormatStringAttributeNames(
        event_formatter, expected_attribute_names)
# ---
def handle_selection():
    selected_drinker = drinkers[lb.current()]
    urllib2.urlopen("http://192.168.11.5:8080/drinkcounter/add_drink/%d/" % (selected_drinker.id))
    appuifw.note(u"A drink has been added to " + drinkers[lb.current()].name, 'info')

    new_drinkers = get_drinker_list()
    items = get_listbox_items(new_drinkers)

    lb.set_list(items, lb.current())
# ---
def _slice_multiplier(value) -> int:
    if isinstance(value, int):
        return value
    if value is None:
        return 1
    if isinstance(value, Iterable) and not isinstance(value, (str, bytes)):
        try:
            return max(value)
        except ValueError:
            return 1
    raise TypeError(f"Unsupported slice count value: {value!r}")
# ---
def visit_mn(self, element):
        return TextNode(element.get_text().strip())
# ---
def get_task_logs(self, request: cluster__pb2.Controller.GetTaskLogsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetTaskLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def increment(self, amount: int = 1) -> int:
        self.call_count += 1
        self.value += amount
        return self.value
# ---
def test_main(verbose=None):
    from test import test_urllib2
    support.run_doctest(test_urllib2, verbose)
    support.run_doctest(urllib.request, verbose)
    tests = (TrivialTests,
             OpenerDirectorTests,
             HandlerTests,
             MiscTests,
             RequestTests)
    support.run_unittest(*tests)
# ---
def test_rename_variables_deterministic_with_same_seed():
    rng1 = random.Random(123)
    rng2 = random.Random(123)
    source = "def compute(x, y):\n    result = x * y\n    return result"
    r1 = rename_variables(source, rng1)
    r2 = rename_variables(source, rng2)
    assert r1 == r2
# ---
def double_click(self, on_element):
        """Double-clicks an element.
        Args:
            on_element: The element to double-click.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.DOUBLE_CLICK, {}))
        return self
# ---
def in_jit(x, pspec):
        if isinstance(x, hax.NamedArray):
            arr = x.array
        else:
            arr = x
        arr = jax.lax.with_sharding_constraint(arr[valid_device_for_process], pspec)

        if isinstance(x, hax.NamedArray):
            return hax.named(arr, x.axis_names)
        else:
            return arr
# ---
def shutdown(self):
        self.server.should_exit = True
# ---
def hvp(f, x, v):
    """Compute the Hessian-vector product of a function."""
    return eqx.filter_jvp(eqx.filter_grad(f), (x,), (v,))[1]
# ---
def __init__(
        self,
        plotly_name="showexponent",
        parent_name="scatterpolar.marker.colorbar",
        **kwargs
    ):
        super(ShowexponentValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            edit_type=kwargs.pop("edit_type", "colorbars"),
            values=kwargs.pop("values", ["all", "first", "last", "none"]),
            **kwargs
        )
# ---
def revert_dispatch(self) -> None:
        """Revert dispatch if no tasks actually started."""
        self.state = cluster_pb2.JOB_STATE_PENDING
        self.started_at = None
# ---
def start_container(self, container_id: str) -> None:
        self._containers[container_id].start()
# ---
def test_quantile_sequential(self):
        def test_impl(A):
            df = pd.DataFrame({'A': A})
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        A = np.arange(0, n, 1, np.float64)
        np.testing.assert_almost_equal(hpat_func(A), test_impl(A))
# ---
def trainable_model(self) -> M:
        return trainables_only(self.model, self.is_trainable)
# ---
def possible_mock_calls(name, info):
    # NOTE(boris-42): py33 I hate you.
    return [mock.call(name, info=info), mock.call(name, info=py3_info(info))]
# ---
def __call__(
        self, model: Any, batch: Any, **batch_kwargs: dict[str, Any]
    ) -> tuple[jax.Array, dict[str, Metric]]: ...
# ---
def as_dataset(obj):
    """Cast the given object to a Dataset.

    Handles DataArrays, Datasets and dictionaries of variables. A new Dataset
    object is only created in the last case.
    """
    obj = getattr(obj, '_dataset', obj)
    if not isinstance(obj, Dataset):
        obj = Dataset(obj)
    return obj
# ---
def match(self, item):
        for subq in self.subqueries:
            if subq.match(item):
                return True
        return False
# ---
def QHeadSize(self) -> Axis:
        return Axis("q_head_dim", self.qk_rope_head_dim + self.qk_nope_head_dim)
# ---
def test_capture_and_logging(self, testdir):
        p = testdir.makepyfile("""
            import logging
            def test_log(capsys):
                logging.error('x')
            """)
        result = testdir.runpytest_subprocess(p)
        assert 'closed' not in result.stderr.str()
# ---
def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger("levanter.json_logger")
        self._last_metrics: dict[str, Any] = {}
        self._summary_metrics: dict[str, Any] = {}
# ---
def test_template_kernel_matches_reference():
    x = jnp.linspace(-3.0, 3.0, 1024, dtype=jnp.float32)
    y_ref = reference_impl_batched(x[None, :])[0]
    y_fast = template_op(x)
    assert jnp.allclose(y_ref, y_fast, atol=0.0, rtol=0.0)

    xb = x.reshape(32, 32)
    yb_ref = reference_impl_batched(xb)
    yb_fast = template_op(xb)
    assert jnp.allclose(yb_ref, yb_fast, atol=0.0, rtol=0.0)
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.subquery == other.subquery
# ---
def loadPlugin(plugin_name):
    """
    @type plugin_name: str
    """
    pass
# ---
def tearDown (self):
    utils.rmtemp ()
# ---
def kill_ssh_session(host_alias: str) -> None:
    """Kill SSH session using control socket."""
    try:
        subprocess.run(["ssh", "-O", "exit", host_alias], capture_output=True, check=False, timeout=5)
    except subprocess.TimeoutExpired:
        logger.warning(f"Timeout killing SSH control connection to {host_alias}")
    except Exception as e:
        logger.debug(f"Failed to kill SSH control connection to {host_alias}: {e}")
# ---
def _invert_results(self, candidates):
        inverted_candidates = list(self.directory)

        for candidate in candidates:
            try:
                inverted_candidates.remove(candidate)
            except ValueError:
                pass

        return inverted_candidates
# ---
def vsync(self):
        """True if buffer flips are synchronised to the screen's vertical
        retrace.  Read-only.

        :type: bool
        """
        return self._vsync
# ---
def _pipeline() -> int:
        return sum(len(dupekit.process_arrow_batch(b)) for b in in_memory_table.to_batches(max_chunksize=batch_size))
# ---
def __create_input(self, main_root):
		content = self.__manager.load_content()
		input_dir = os.path.join(main_root,'input')
		try:
			content.get(self.__task.data['id'], 'input', os.path.join(main_root, 'input'))
		except:
			safe_mkdir(input_dir)
# ---
def __init__(cls, name, bases, dict):
        cls._platform_event_names = set()
        for base in bases:
            if hasattr(base, '_platform_event_names'):
                cls._platform_event_names.update(base._platform_event_names)
        for name, func in dict.items():
            if hasattr(func, '_platform_event'):
                cls._platform_event_names.add(name)
        super(_WindowMetaclass, cls).__init__(name, bases, dict)
# ---
def __init__(self, routes, **kwargs):
        super(ProxyArchivalRouter, self).__init__(routes, **kwargs)
        self.proxy = ProxyRouter(routes, **kwargs)
# ---
def to_jax_shape(shape: AxisSpec):
    if isinstance(shape, Axis):
        return shape.size
    elif isinstance(shape, Sequence):
        return tuple(s.size for s in shape)
    return tuple(shape[a] for a in shape)
# ---
def test_forward_padding_masked(params, tiny_cfg):
    """Padding tokens should not affect non-padding logits."""
    tokens = jnp.array([[10, 20, 30, 0, 0]])  # Last two are padding.
    logits_a = forward(params, tokens, tiny_cfg)

    tokens_b = jnp.array([[10, 20, 30, 50, 60]])  # No padding.
    logits_b = forward(params, tokens_b, tiny_cfg)

    # First 3 positions should be the same (padding doesn't attend).
    assert jnp.allclose(logits_a[0, :3], logits_b[0, :3], atol=1e-5)
# ---
def clean_text(text: str) -> str:
    """Clean text for display."""
    return text.replace("\n", "\\n").replace("\t", "\\t")
# ---
def params(tiny_cfg):
    key = jax.random.PRNGKey(0)
    return init_edit_params(tiny_cfg, key=key)
# ---
def demo_client() -> IrisClient:
    manager = ClusterManager(_make_demo_config())
    manager.start()
    try:
        controller_url = manager.controller.discover()
        assert controller_url is not None

        client = IrisClient.remote(
            controller_url,
            workspace=Path(__file__).resolve().parents[3],
        )
        yield client
    finally:
        manager.stop()
# ---
def _rpa_tol() -> float:
    devices = jax.devices()
    # 2%?!?!
    return 2e-2 if any(device.platform == "tpu" for device in devices) else 1e-4
# ---
def test_capsys_results_accessible_by_attribute(capsys):
    sys.stdout.write("spam")
    sys.stderr.write("eggs")
    capture_result = capsys.readouterr()
    assert capture_result.out == "spam"
    assert capture_result.err == "eggs"
# ---
def area(self):
        return self.side ** 2
# ---
def mod(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._exception = None
# ---
def title_field_is_required_present(self):
        """
        :rtype: bool
        """
        return self._is_element_present(AddMoviePageLocators.TITLE_INPUT_ERROR_LOCATOR)
# ---
def to_proto(self) -> cluster_pb2.AttributeValue:
        """Convert to protobuf representation."""
        proto = cluster_pb2.AttributeValue()
        if isinstance(self.value, str):
            proto.string_value = self.value
        elif isinstance(self.value, int):
            proto.int_value = self.value
        elif isinstance(self.value, float):
            proto.float_value = self.value
        return proto
# ---
def intersect_axes(ax1: ShapeDict, ax2: AxisSelection) -> ShapeDict: ...
# ---


def strlen(string: str) -> int:
    """ Return length of given string
    >>> strlen('')
    0
    >>> strlen('abc')
    3
    """
    return len(string)
# ---
def dims(self):
        """Mapping from dimension names to lengths.

        This dictionary cannot be modified directly, but is updated when adding
        new variables.
        """
        return Frozen(SortedKeysDict(self._dims))
# ---
def get_unit (self, uids) :
        """
        Reconnect to and return (Compute or Data)-Unit object(s)
        """

        raise Exception ("%s.get_unit() is not implemented" % self.__class__.__name__)
# ---
def clean_ar5iv_record(html_blob: dict) -> dict:
    """Clean HTML in a single ar5iv record.

    Args:
        html_blob: Record with 'id' and 'text' (HTML content)

    Returns:
        Record with cleaned HTML text
    """
    content = clean_html(html_blob["text"])
    return {
        "id": html_blob["id"],
        "text": content,
        "source": "ar5iv",
        "added": datetime.datetime.now().isoformat(),
    }
# ---
def upload_python_script(notebook, data):
    code = notebook.get_code(map(int, data.get('code-lines', '').split('|')))
    files = {'file': (data['job-name'] + '.py', code)}
    return requests.post(
        SCRIPT_UPLOAD_URL_PATTERN % data['saagie-platform'],
        files=files, auth=SAAGIE_BASIC_AUTH_TOKEN).json()['fileName']
# ---
def __init__(self, ova_path, vminfo, vmid, irs):
        super(OvaCommand, self).__init__(vminfo, vmid, irs)
        self._ova_path = ova_path
# ---
def reload(self) -> str:
        return self.restart()
# ---
def height(self):
        with self.lock:
            if not self._last_block:
                return -1
            return self._last_block.block_number
# ---
def create(self, context):
        # To ensure the creating type is PF
        if self.type != 'pf':
            raise exception.InvalidDeployType()
        super(PhysicalFunction, self).create(context)
# ---
def exists(self, tag: str) -> bool:
        result = subprocess.run(
            ["docker", "image", "inspect", tag],
            capture_output=True,
            check=False,
        )
        return result.returncode == 0
# ---
def get_layer(self, index: int) -> M:
        """Return the ``index``th block in this sequential container."""

        return self.blocks[index]
# ---
def material(self):
        return self.__material
# ---
def determine_inverse(self, records):
        """ Given the value of ``self`` on ``records``, inverse the computation. """
        if self.inverse:
            self.inverse(records)
# ---
def label_prefix(self) -> str:
        return self.prefix or "iris"
# ---
def compute_grpo_loss(
    loss_objective: jax.Array,
    loss_masks: jax.Array,
    max_output_tokens: int,
) -> jax.Array:
    """Compute GRPO loss (token-level loss)."""
    return -1 * jnp.mean(jnp.sum(loss_objective * loss_masks, axis=1) / max_output_tokens)
# ---
def test_decode_token_position(tok):
    tid = tok.position_token_id(42)
    assert tok.decode_token(tid) == "<POS 42>"
# ---
def reset(self):
        self._calls = []
# ---
def __bool__(self):
        if self.transport:
            return True
        return False
# ---
def iter_inherited(self, name):
        for ctx in self.iter_containing(name):
            yield object.__getattribute__(ctx, name)
# ---
def test_cursor_execute_wo_replace(self):
        self._test_cursor_execute(False)
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[True] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[False] = ...,
) -> tuple[NamedArray, Array, Array, Array, Array]: ...
# ---
def __iter__(self):
        return self
# ---
def test_no_param_usage(self):
        @event.event
        def func_name(self):
            pass

        @event.event
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.event_name == "test"
        assert func_name._h_info.event_name == "func_name"
        assert h_info.handler is on_test
        assert h_info.priority is event.Priority.DEFAULT
        assert h_info.should_enable
        assert not h_info.is_async
# ---
def getshort(code) :
    maxl = 5
    newcode = code.replace('http://', '')
    if len(newcode) > maxl :
          newcode = newcode[0:maxl]
    return str(newcode)
# ---
def height(self):
        return self.__height
# ---
def __getitem__(self, index: int) -> T_co:
        return self._run_coroutine(self.dataset.getitem_async(index))
# ---
def test_auto_metric_inference(name, expected_reduction):
    """auto_metric_from_name infers correct reduction type."""
    m = auto_metric_from_name(name, 42.0)
    assert m.reduction == expected_reduction
# ---
def __len__(self):
        """The length of the array.

        Returns
        -------
        int :
            The size of the array

        """
        return self._size
# ---
def _has_type_access(type_id, project_id):
    for access in ACCESS_LIST:
        if access['volume_type_id'] == type_id and \
           access['project_id'] == project_id:
            return True
    return False
# ---
def get_jobs_status():
    ret = {}
    with _lock:
        items = tuple(_jobs.items())
    for job_id, job in items:
        ret[job_id] = {
            'status': job.status,
            'description': job.description,
            'progress': job.progress
        }
    return ret
# ---
def convert_to_write(self, value, target=None, fnames=None):
        """ convert ``value`` from the cache to a valid value for method
            :meth:`BaseModel.write`.

            :param target: optional, the record to be modified with this value
            :param fnames: for relational fields only, an optional collection of
                field names to convert
        """
        return self.convert_to_read(value)
# ---
def test_repr(self):
        assert repr(lit(42)) == "lit(42)"
        assert repr(lit("hello")) == "lit('hello')"
# ---
def __iter__(self) -> Iterator[PageCacheT]:
        return iter(self.caches)
# ---
def get_address_state(self, address: bytes) -> AddressState:
        with self.lock:
            return self._state.get_address_state(address)
# ---
def __len__(self) -> int:
        return len(self.caches)
# ---
def __init__(
        self,
        cache_dir: str,
        exemplar: T_co,
        ledger: "CacheLedger",
    ):
        super().__init__()
        self.cache_dir = cache_dir
        self.ledger = ledger
        self._exemplar = exemplar

        if not ledger.is_finished:
            raise RuntimeError(f"Cache at {cache_dir} is not finished.")

        self._store = TreeStore.open(self._exemplar, self.cache_dir, mode="r", cache_metadata=False)
# ---
def parse_slurm_args(args_list: list[list[str]]) -> dict[str, str]:
    slurm_args = DEFAULT_SLURM_ARGS.copy()
    if not args_list:
        return slurm_args
    for item in args_list:
        if len(item) != 2:
            logger.error(f"Invalid SLURM argument format: {item}. Expected 'KEY VALUE' format.")
            sys.exit(1)
        key, value = item
        slurm_args[key] = value
    return slurm_args
# ---
def start(
        self,
        *,
        model_name_or_path: str,
        host: str,
        port: int | None,
        timeout_seconds: int,
        extra_cli_args: list[str] | None,
    ) -> VllmServerHandle:
        return _start_vllm_native_server(
            model_name_or_path=model_name_or_path,
            host=host,
            port=port,
            timeout_seconds=timeout_seconds,
            extra_cli_args=extra_cli_args,
        )
# ---
def convert_task():
        export_lm_to_hf.main(convert_config)
# ---
def get_last_transactions(self):
        with self.lock:
            return self._state.get_last_txs()
# ---
def test_get_has_csrf_token(self):
        self.login()
        response = self.testapp.get('/', status=200).body
        self.assertIn('CSRF Token:', response)
        self.assertEqual(response.split(':')[-1], csrf.make_token())
# ---
def read_all_available(self) -> list[RolloutBatch]:
        """Read all currently available batches without blocking."""
        return self._queue.pop_all()
# ---
def inc(rc):
                    return rc.at["page", page].add(1)
# ---
def test_get_style_defs(self):
        fmt = HtmlFormatter()
        sd = fmt.get_style_defs()
        self.assert_(sd.startswith('.'))

        fmt = HtmlFormatter(cssclass='foo')
        sd = fmt.get_style_defs()
        self.assert_(sd.startswith('.foo'))
        sd = fmt.get_style_defs('.bar')
        self.assert_(sd.startswith('.bar'))
        sd = fmt.get_style_defs(['.bar', '.baz'])
        fl = sd.splitlines()[0]
        self.assert_('.bar' in fl and '.baz' in fl)
# ---
def _fix_a_slash_b(string: str) -> str:
    if len(string.split("/")) != 2:
        return string
    a = string.split("/")[0]
    b = string.split("/")[1]
    try:
        a = int(a)
        b = int(b)
        assert string == f"{a}/{b}"
        new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
        return new_string
    except ValueError:
        return string
# ---
def test_add_non_handler(self):
        class NonHandler(object):
            pass
        self.assertRaises(TypeError,
                          OpenerDirector().add_handler, NonHandler())
# ---
def test_multiplication_commutativity():
    variants = generate_expression_variants("a * b")
    assert "b * a" in variants
# ---
def rnai(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'RNAi'
    }
# ---
def strong(node: RenderTreeNode, context: RenderContext) -> str:
    text = make_render_children(separator="")(node, context)
    indicator = node.markup
    return indicator + text + indicator
# ---
def define_tables(cls, metadata):
        Table(
            "square",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("side", Integer),
            Column("area", Integer, Computed("side * side")),
            Column("perimeter", Integer, Computed("4 * side")),
        )
# ---
def __init__(self, server, host: str, port: int):
        self.server = server
        self.host = host
        self.port = port
        self.thread = threading.Thread(target=self.server.run, daemon=True)
# ---
def __call__(self, x):
            return x + self.w
# ---
def searchradio(radio, genre) :
    db = cherrypy.session['database']
    #o = 'order by radio'
    o = ''
    sql = "select id, radio, genre, url from Radio where exist > 0 and radio like '%%%s%%' and genre like '%%%s%%' and id > 0 %s" % (radio, genre, o)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
    except:
        return [(0, sql, o, genre)]

    rows = cur.fetchall()
    con.close()
    return rows
# ---
def render(self, name, value, attrs=None, renderer=None):
        output = []
        output.append(super(AdminFileWidget, self).render(name, value, attrs)) # really for AdminFileWidget
        instance = getattr(value, 'instance', None)
        if instance is not None and value:
            output = ['<a target="_blank" href="%s"><img src="%s" alt="%s"/></a>' % \
                (instance.image.url, instance.thumb.url, instance.image)] + output
        return mark_safe(u''.join(output))
# ---
def on_ok(self, *args):
        pass
# ---
def __init__(self):
            super().__init__()
            self.a = jnp.zeros(1)
            self.b = hax.named(jnp.zeros(3), "a")
# ---
def get_channel_max(self):
        """Return the maximum number of channels"""
        return self._ChannelMax
# ---
def Pos(self) -> Axis:
        return self.config.max_Pos
# ---
def job_id(self) -> JobName:
        """Unique job identifier."""
        return self._job_id
# ---
def selected_target_account_index(self, value):
        target = self.target_accounts[value - 1] if value > 0 else None
        self.selected_pane.selected_target = target
        self._selected_target_index = value
        self.import_table.refresh()
# ---
def __init__(self):
        self.thread_data = threading.local()
        self.thread_data.resource_mapping = None
# ---
def __init__(self, artist, title, year=None, material=None):
        super(Sculpture, self).__init__(artist, title, year)
        self.__material = material
# ---
def _poll_ref(self) -> JobStatus:
        ready, _ = ray.wait([self._ref], timeout=0)
        if not ready:
            return JobStatus.RUNNING
        try:
            ray.get(self._ref)
            return JobStatus.SUCCEEDED
        except Exception:
            return JobStatus.FAILED
# ---
def test_llama_param_counts_dont_change_with_seqlen():
    model = LlamaLMHeadModel.init(hax.Axis("v", 2048), _get_llama_config(seq_len=128), key=random.PRNGKey(0))
    model2 = LlamaLMHeadModel.init(hax.Axis("v", 2048), _get_llama_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def Receptionist_Places_Call (self, Number):
        self.Step (Message = "Receptionist places call to " + str (Number) + "...")

        self.Log (Message = "Dialling through receptionist agent...")
        self.Receptionist.dial (Number)
# ---
def register_rpc(self, handlers, module_name):
        # add the internal methods, note that this means they
        # can get clobbbered by subclass versions
        for meth in self.__base_methods:
            handlers["%s.%s" % (module_name, meth)] = self.__base_methods[meth]

        # register our module's handlers
        for name, handler in self.__list_handlers().items():
            handlers["%s.%s" % (module_name, name)] = handler
# ---
def Vocab(self) -> Axis:
        return Axis("vocab", self.grug_config.vocab_size)
# ---
def render_execdepends(thing):
    rendered = []
    for item in thing:
        dep = copy.copy(item)
        dep.setdefault('package_manager', 'apt')
        dep['version'] = ' = '+dep['version'] if 'version' in dep else ''
        rendered.append("{package_manager}: {name}{version}".format(**dep))
    return rendered
# ---
def add_ten(self, item):
        return item + 10
# ---
def empty_page_cache(self, spec: PageTableSpec, *, dtype) -> "KvPageCache":
        return KvPageCache.init(spec, self.config.KVHeads, self.config.HeadSize, dtype=dtype)
# ---
def check_access_rights(self, cr, uid, operation, raise_exception=True):
        #override in order to redirect the check of acces rights on the stock.picking object
        return self.pool.get('stock.picking').check_access_rights(cr, uid, operation, raise_exception=raise_exception)
# ---
def datasource_from_jsonl(urls_or_paths: Sequence[str]) -> ShardedDataSource[dict]:
    return JsonlDataSource(urls_or_paths)
# ---
def testFormatHostname(self):
    """Tests the _FormatHostname function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    hostname_string = test_helper._FormatHostname(
        event, event_data, event_data_stream)
    self.assertEqual(hostname_string, 'ubuntu')
# ---
def local(cls, config: LocalClientConfig | None = None) -> "IrisClient":
        """Create an IrisClient for local execution using real Controller/Worker.

        Args:
            config: Configuration for local execution

        Returns:
            IrisClient wrapping LocalClusterClient
        """
        cfg = config or LocalClientConfig()
        cluster = LocalClusterClient.create(max_workers=cfg.max_workers)
        return cls(cluster)
# ---
def __init__(self, client: "IrisClient", task_name: JobName):
        self._client = client
        self._task_name = task_name
# ---
def fake_pool_set_name_label(self, session, pool_ref, name):
            fake_pool_set_name_label.called = True
# ---
def do_go(self, e):
        self.remote.do_go()
# ---
def testDeleteAttribute(self):
    self.assertEqual((0, 'False\n'), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          bar = 42
        del Foo.bar
        print hasattr(Foo, 'bar')""")))
# ---
def test_mixtral_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def healthy(self) -> bool:
        return not self._failed and not self.is_being_preempted()
# ---
def test_no_elements():
    partial_order = (...,)
    candidates = ()
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ()
# ---
def _interrupt_control_thread(self):
        """
        Signal the control flow thread that it needs to abort processing, likely due to a timeout.
        """
        self._ctrl_thread.proc.kill(exception=OperationInterruptedException, block=False)
# ---
def requires(self):
        return DownloadRITA(year=self.year, month=self.month)
# ---
def show_opencfg_dlg(self):
        # show file dialog
        filename, _ = QFileDialog.getOpenFileName(
            self, self.tr("Open configuration file..."),
            directory=os.path.expanduser("~"),
            filter=self.tr("Json file (*.json);;All files (*.*)")
        )

        # load config file
        if filename:
            self.load_file(filename)
# ---
def test_metric_jit():
    """Metrics work through JIT."""

    @jax.jit
    def fold_metrics_jit(m1, m2):
        return fold(m1, m2)

    m1 = Metric.from_value(10.0, ReductionType.SUM)
    m2 = Metric.from_value(20.0, ReductionType.SUM)
    result = fold_metrics_jit(m1, m2)

    assert jnp.allclose(result.value(), 30.0)
# ---
def testMatMul_Inputs_Empty(self):
    n, k, m = 3, 0, 4
    x = self._randMatrix(n, k, np.float32)
    y = self._randMatrix(k, m, np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def test_is_cloneable_share_goodformat2(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://10.61.222.333:8080/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def intermediate(self, x, static1, *, static2):
            assert static1 is True
            assert static2 is False
            return x + 2 * self.w + static1
# ---
def unregister_endpoint(self, endpoint_id: str) -> None:
        self._remote_client.unregister_endpoint(endpoint_id)
# ---
def test_random_centroid_dimensions():
    """ensure the correct number of dimensions"""
    dimensions = random.randrange(1, 100)
    k = random.randrange(1, 100)
    centroids = kmeans.random_centroids(k, dimensions)
    for centroid in centroids:
        assert len(centroid) == dimensions
# ---
def get_hf_config(self):
        return self.config.to_hf_config(self.Vocab.size)
# ---
def notify(header=None, msg='', duration=2000, sound=None):
    if header is None: header = get_name()
    if sound is None: sound = get_setting('mute_notifications') == 'false'
    icon_path = os.path.join(get_path(), 'icon.png')
    try:
        xbmcgui.Dialog().notification(header, msg, icon_path, duration, sound)
    except:
        builtin = "XBMC.Notification(%s,%s, %s, %s)" % (header, msg, duration, icon_path)
        xbmc.executebuiltin(builtin)
# ---
def test_mem_write_byte_write_off_screen(self):
        self.mda.mem_write_byte(4000, 0xFF)
        self.assertEqual(self.cg.last_blit, None)
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[T_co]:
        raise NotImplementedError
# ---
def throw(**_):
            """Throw an exception to terminate the request"""
            raise Exception()
# ---
def __init__(self, *args, **kwargs):
        super(FarmworkForm, self).__init__(*args, **kwargs)
# ---
def __getitem__(self, name, file_local=False):
        """ Get an option value """
        prefix = 'file-local-options/' if file_local else 'options/'
        return self._get_property(prefix+name)
# ---
def test_legacy_dbapi_error(self):
        engine = engines.testing_engine()
        canary = Mock()

        event.listen(engine, "dbapi_error", canary)

        with engine.connect() as conn:
            try:
                conn.execute("SELECT FOO FROM I_DONT_EXIST")
                assert False
            except tsa.exc.DBAPIError as e:
                eq_(canary.mock_calls[0][1][5], e.orig)
                eq_(canary.mock_calls[0][1][2], "SELECT FOO FROM I_DONT_EXIST")
# ---
def __call__(self, x):
        x = hax.dot(self.w_in, x, axis=self.In)
        x = hnn.relu(x)
        x = hax.dot(self.w_out, x, axis=self.Mid)
        return x
# ---
def date_range(self) -> str:
        return f"{self.start_date}to{self.end_date}"
# ---
def get_vm(self, vm_id: str) -> vm_pb2.VmInfo | None:
        """Get info for a specific VM."""
        ...
# ---
def current_in_milliamps(self, value):
        self.current = value / 1000
# ---
from typing import List


def all_prefixes(string: str) -> List[str]:
    """ Return list of all prefixes from shortest to longest of the input string
    >>> all_prefixes('abc')
    ['a', 'ab', 'abc']
    """
    result = []

    for i in range(len(string)):
        result.append(string[:i+1])
    return result
# ---
def __contains__(self, item):
        return item in self.subqueries
# ---
def test_impl(df):
            C = df.A.str.split(',')
            return C[df.B == 'aa']
# ---
def __init__(self, parent=None, datafolder=None):
        """
        Constructor
        """
        QtGui.QDialog.__init__(self, parent)

        # self.filelist = filelist
        self.datafolder = datafolder

        # labels font
        self.font_labels = QtGui.QFont("Arial", 12, QtGui.QFont.Bold)
        self.font_edits = QtGui.QFont("Arial", 12)
        self.font_buttons = QtGui.QFont("Arial", 10, QtGui.QFont.Bold)


        self.setupUi(self)
        self.exec_()
# ---
def attention_config(self) -> AttentionConfig:
        return AttentionConfig(
            Embed=self.Embed,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            use_bias=self.use_bias,
            upcast_attn=self.upcast_attn,
            attn_backend=self.attn_backend,
            flash_attention_block_size=self.flash_attention_block_size,
            rope=self.rope,
            qk_norm=self.qk_norm,
        )
# ---
def _infer_transpose_keys(model_name: str) -> dict:
    """Infer the transpose keys for a model name, falling back to substring matching."""
    if model_name in _MODEL_TRANSPOSE_KEYS:
        return _MODEL_TRANSPOSE_KEYS[model_name]
    if "Qwen2.5" in model_name:
        return llama_transpose_keys
    raise KeyError(f"No MODEL_TRANSPOSE_KEYS registered for model: {model_name}")
# ---
def registerPlugin(plugin_instance):
    """
    @type plugin_instance: L{amsn2.plugins.developers.aMSNPlugin}
    """
    pass
# ---
def vms(self) -> list[ManagedVm]:
        return self._managed_vms
# ---
def test_load_vortex_empty_file(self, tmp_path):
        """Test loading an empty vortex file."""
        empty_path = tmp_path / "empty.vortex"
        write_vortex_file([], str(empty_path))

        records = list(load_vortex(str(empty_path)))
        assert records == []
# ---
def test_p3_closeness(self):
        c = nx.closeness_centrality(self.P3)
        d = {0: 0.667, 1: 1.000, 2: 0.667}
        for n in sorted(self.P3):
            assert almost_equal(c[n], d[n], places=3)
# ---
def v5p16_scale_group() -> config_pb2.ScaleGroupConfig:
    """Multi-host TPU scale group (v5p-16, 2 VMs per slice)."""
    return config_pb2.ScaleGroupConfig(
        name="tpu-v5p-16",
        min_slices=0,
        max_slices=5,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-16",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def linelisting_to_newline(html: BeautifulSoup):
    # Turn new line listings into new lines
    linelisting = html.findAll("div", {"class": "ltx_listingline"})
    for fn in linelisting:
        fn.append(BeautifulSoup("<br>", "html.parser"))
# ---
def test_masking(self):
    testing_utils.layer_test(
        keras.layers.Masking, kwargs={}, input_shape=(3, 2, 3))
# ---
def getStatus(self,solverId):
    path = "{base}/{solverId}/status".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.get(path,
                       params=self.params,
                       headers=self.headers)
    self.validateReply(req)
    return req.json()
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> Optional[float]:
        return None
# ---
def finish(self):
        logger.info("Finishing wandb run...")
        self._write_replicate_file()
        self.run.finish()
# ---
def terminate_all(self) -> None:
        """Terminate all VM groups in this scale group."""
        with self._vm_groups_lock:
            snapshot = list(self._vm_groups.values())
            self._vm_groups.clear()
        for vm_group in snapshot:
            vm_group.terminate()
# ---
def workspace(tmp_path):
    """Create a test workspace."""
    (tmp_path / "pyproject.toml").write_text("[project]\nname = 'test'\n")
    (tmp_path / "src").mkdir()
    (tmp_path / "src" / "main.py").write_text("print('hello')")
    (tmp_path / "__pycache__").mkdir()
    (tmp_path / "__pycache__" / "main.cpython-312.pyc").write_bytes(b"cached")
    return tmp_path
# ---
def test_unique_str_parallel(self):
        # TODO: test without file
        def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.two.unique() == 'foo').sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
# ---
def build(self, axis: AxisSpec) -> LayerNormBase:
        """Build the normalization layer."""
        raise NotImplementedError
# ---
def model_type(cls) -> Type["Gpt2HyenaModel"]:
        return Gpt2HyenaModel
# ---
def _batch_sizes() -> dict[str, int]:
    return {"130m": 128, "300m": 128, "520m": 128, "1_2b": 256}
# ---
def build(self, ctx: LrScheduleContext):
        return optax.cosine_decay_schedule(ctx.learning_rate, ctx.decay_steps, ctx.min_lr_ratio, self.exponent)
# ---
def _get_address(self):
        self._socket = self.transport.get_extra_info("peername") or (
            None,
            None,
        )
        self._ip = self._socket[0]
        self._port = self._socket[1]
# ---
def schedule_router_postcommit(self, context, router_context):
        # When the hosting device hosts a Neutron router with external
        # connectivity, a "global" router (modeled as a Neutron router) must
        # also run on the hosting device (outside of any VRF) to enable the
        # connectivity.
        current = router_context.current
        if current['gw_port_id'] and current[HOSTING_DEVICE_ATTR] is not None:
            self._conditionally_add_global_router(context.elevated(), current)
# ---
def __init__(self, config: ServerConfig):
        self.config = config

        # Lazily instantiate remote filesystems to avoid triggering cloud
        # auth during local-only development.
        self.fs_cache = {
            None: fsspec.filesystem("local"),
        }
# ---
def intersect_axes(ax1: tuple[AxisSelector, ...], ax2: AxisSelection) -> tuple[AxisSelector, ...]:  # type: ignore
    ...
# ---
def submit(self, request: JobRequest) -> RayJobHandle:
        """Submit a job, routing to TPU/binary/callable based on device type."""
        logger.info("Submitting job: %s", request.name)

        if isinstance(request.resources.device, TpuConfig):
            return self._launch_tpu_job(request)

        if request.entrypoint.binary_entrypoint is not None:
            return self._launch_binary_job(request)

        return self._launch_callable_job(request)
# ---
def remove(self, container_id: str) -> None:
        self._containers.pop(container_id, None)
# ---
def __deepcopy__(self, memo=None):
        # memo does nothing but is required for compatibility with
        # copy.deepcopy
        return self.copy(deep=True)
# ---
def patched__getitem__(proxy_self, keys):
            variable = None
            for var in (self.cf_coord_var, self.cf_bounds_var):
                if proxy_self.variable_name == var.cf_name:
                    return var[keys]
            raise RuntimeError()
# ---
def test_two_network_labels(self):
        CONF.network_label_regex = '^(private|public)$'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(2, len(ip))
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)
# ---
def path(self):
        return self._parsed_url.path.decode("utf-8")
# ---
def output_escape(self, m):
        text = m.group(1)
        return self.renderer.escape(text)
# ---
def __init__(self, root, cwd, badfn=None, gitignorepaths=None):
        super(gitignorematcher, self).__init__(root, cwd, badfn)
        gitignorepaths = gitignorepaths or []
        self._matcher = pathmatcher.gitignorematcher(root, gitignorepaths)
# ---
def wait_for_playback(self):
        """ Waits until playback of the current title is paused or done """
        with self._playback_cond:
            self._playback_cond.wait()
# ---
def save_spendable_output(self):
        self.log.debug("saving spendable output %s" % self.tip.vtx[0])
        self.spendable_outputs.append(self.tip)
# ---
def test_do_executemany_wo_replace(self):
        self._test_do_executemany(False)
# ---
def extract_data(pipeline_response):
            deserialized = self._deserialize("PolicyAssignmentListResult", pipeline_response)
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)
            return deserialized.next_link or None, iter(list_of_elem)
# ---
def read(self, path):
        return False
# ---
def capture(self, tensor, name=None):
    """Adds the given tensor to this graph and returns the captured tensor."""
    if tensor in self._captured:
      # Captured already.
      return self._captured[tensor]
    elif self._capture_by_value:
      return self._add_tensor_and_parents(tensor)
    else:
      return self._capture_tensor_as_extra_input(tensor, name)
# ---
def changeColumnInLines(self):
        """
        Implements function to transform columns in lines to make tests eaiser.
        :return: a reverse matrice
        """
        column = []
        for x in xrange(7):
            col = ''
            for y in xrange(6):
                col += self.grille[y][x]
            column.append(col)
        return column
# ---
def broadcast_msg(server, ns_name, event, *args):
    pkt = dict(type="event",
               name=event,
               args=args,
               endpoint=ns_name)

    for sessid, socket in server.sockets.iteritems():
        socket.send_packet(pkt)
# ---
def without_axes(axis_spec: AxisSpec, to_remove: AxisSelection, allow_mismatched_sizes=False) -> AxisSpec:  # type: ignore
    ...
# ---
def model_type(self) -> Type["ToyLmHeadModel"]:
        return ToyLmHeadModel
# ---
def tagged_eval_sets(self, Pos: Axis) -> list[tuple[AsyncDataset[LmExample], list[str]]]:
        eval_sets = self.validation_sets(Pos)
        tagged = []
        for name, ds in eval_sets.items():
            tags = (self.components[name].tags or []) + [name]
            tagged.append((ds, tags))
        return tagged
# ---
def test_build_resources_gpu_not_supported():
    """Test that GPU raises error."""
    with pytest.raises(ValueError, match="GPU support not yet implemented"):
        build_resources(tpu=None, gpu=2, cpu=None, memory=None)
# ---
def __init__(self, min_value, max_value, step, instance=None,
        can_delete_vote=True, key='', read_only=False,
        template='ratings/star_widget.html', attrs=None):
        super(StarWidget, self).__init__(attrs)
        self.min_value = min_value
        self.max_value = max_value
        self.step = step
        self.instance = instance
        self.can_delete_vote = can_delete_vote
        self.read_only = read_only
        self.template = template
        self.key = key
# ---
def num_cpus(self) -> int:
        return 1
# ---
def _rm_thread(path: str) -> None:
    try:
        fs, _ = fsspec.core.url_to_fs(path)
        fs.rm(path, recursive=True)
    except Exception as e:
        logger.error(f"Failed to delete old checkpoint at {path}: {e}", exc_info=True)
# ---
def parse_fences(self, m):
        self.tokens.append({
            'type': 'code',
            'lang': m.group(2),
            'text': m.group(3),
        })
# ---
def _curriculum_checkpoint_hook(info: levanter.callbacks.StepInfo):
            checkpoint_dir = self.config.trainer.checkpointer.expanded_path(self.config.run_id)
            try:
                future = self._curriculum_actor.save_checkpoint.remote(checkpoint_dir)
                get_default_job_ctx().get(future)
            except Exception as e:
                logger.error(f"Failed to save curriculum checkpoint: {e}")
# ---
def alertPopup(title, msg):
    popup = Popup(title = title,
                      content=Label(text = msg),
                      size_hint=(None, None), size=(dp(600), dp(200)))
    popup.open()
# ---
def on_show():
            """The window was shown.

            This event is triggered when a window is restored after being
            minimised, or after being displayed for the first time.

            :event:
            """
# ---
def add_router_interface_postcommit(self, context, r_port_context):
        pass
# ---
def test_build_event(self):
        evt = event.build_event("evt_name", arg1="val1", arg2=None)
        assert evt.name == "evt_name"
        assert evt.args == {'arg1': "val1", 'arg2': None}
# ---
def idle_count(self) -> int:
        return sum(1 for w in self._workers.values() if w.status == WorkerStatus.IDLE)
# ---
def create_executor(temp_dir: str):
    """Create an Executor that lives in a temporary directory."""
    return Executor(prefix=temp_dir, executor_info_base_path=temp_dir)
# ---
def _get_olmo2_config(use_flash=False, num_kv_heads=4, seq_len=128) -> Olmo2Config:
    return Olmo2Config(
        max_seq_len=seq_len,
        hidden_dim=16,
        intermediate_dim=32,
        num_layers=4,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def fetch_server(self) -> ServerInfo:
        return self._server_info
# ---
def __call__(
        self,
        x: NamedArray,
        xa: Optional[NamedArray] = None,
        attn_mask: Optional[AttentionMask] = None,
        *,
        key=None,
    ) -> NamedArray:
        keys = hax.jax_utils.maybe_rng_split(key, self.Layer.size) if key is not None else None
        x = self.layers.fold(x, xa, attn_mask, key=keys)
        x = self.layer_norm(x)

        return x
# ---
def test_str_get_to_numeric(self):
        def test_impl(df):
            B = df.A.str.split(',')
            C = pd.to_numeric(B.str.get(1), errors='coerce')
            return C

        df = pd.DataFrame({'A': ['AB,12', 'C,321,D']})
        hpat_func = self.jit(locals={'C': types.int64[:]})(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def main():
    """Main entry point."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    cli()
# ---
def initialize(self, io_loop=None):
        super(BlockingResolver, self).initialize(io_loop=io_loop)
# ---
def can_swap_date_fields(self, first, second): # 'day', 'month', 'year'
        return (first, second) in self._swap_possibilities or (second, first) in self._swap_possibilities
# ---
def _docker_container_running(container_name: str) -> bool:
    result = subprocess.run(
        ["docker", "inspect", "-f", "{{.State.Running}}", container_name],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        return False
    output = result.stdout.strip().lower()
    if output == "true":
        return True
    if output == "false":
        return False
    raise RuntimeError(f"Unexpected output from docker inspect: {output!r}")
# ---
def config_from_hf_config(self, hf_config, overrides: Optional[dict] = None) -> LevConfig:
        config = self.LevConfigClass.from_hf_config(hf_config)
        if overrides is not None:
            config = dataclasses.replace(config, **overrides)  # type: ignore
        return config
# ---
def slice_all_ready(slice_info: vm_pb2.SliceInfo) -> bool:
    """Compute all_ready from vms[] in proto."""
    return all(vm.state == vm_pb2.VM_STATE_READY for vm in slice_info.vms)
# ---
def eye(N, M=None, k=0, typecode=None, dtype=None):
    """ eye returns a N-by-M 2-d array where the  k-th diagonal is all ones,
        and everything else is zeros.
    """
    dtype = convtypecode(typecode, dtype)
    if M is None: M = N
    m = np.equal(np.subtract.outer(np.arange(N), np.arange(M)),-k)
    if m.dtype != dtype:
        return m.astype(dtype)
# ---
def job_id(self) -> JobName:
        return self.task_id.parent or self.task_id
# ---
def get_question(self) -> str:
        return self.problem + self.question_suffix()
# ---
def __hash__(self) -> int:
        return hash(self._ms)
# ---
def _run_gcloud(args: list[str], check: bool = False) -> subprocess.CompletedProcess[str]:
    """Run a gcloud command and return the result."""
    return subprocess.run(["gcloud", *args], capture_output=True, text=True, check=check)
# ---
def stub_remove_volume_type_access(context, type_id, project_id):
            raise exception.VolumeTypeAccessNotFound(volume_type_id=type_id,
                                                     project_id=project_id)
# ---
def main(
    bucket_name="marin-data", directory="processed/fineweb/fw-v1.0/", suffix="_processed_html.jsonl.gz", limit=None
):
    counter = 0
    subdirectories = get_subdirectories(bucket_name, directory, suffix)
    for subdir in subdirectories:
        for braceexpand_path in list_files_in_subdir(bucket_name, subdir, suffix):
            print(f"- {braceexpand_path}")
            counter += 1
            if limit and counter >= limit:
                return
# ---
def populate_history(wrapped_optimizer, initial_state, params, num_steps, base_loss, base_grads):
    current_state = initial_state
    for i in range(num_steps):
        # Vary loss and grads slightly to have some variance
        loss = base_loss + jnp.sin(i * 0.1)
        grads = jax.tree_util.tree_map(lambda g: g * (1 + 0.1 * jnp.cos(i * 0.1)), base_grads)
        _, current_state = wrapped_optimizer.update(grads, current_state, params, loss=loss)
    return current_state
# ---
def find_secret(self, key):
        ''' find secret'''
        rval = None
        try:
            rval = self.secrets[key]
        except KeyError as _:
            return None

        return {'key': key, 'value': rval}
# ---
def eval_func_der(self, m, k, r, i):
        """
        Find the derivative of the evaluation function with respect
        to the ith component of the vector r
        """
        return self.data[m][k][i]
# ---
def test_if_body_change():
    source = "if x > 0:\n    return x\n"
    target = "if x > 0:\n    return -x\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[dict]:
        url = self._shard_name_to_url_mapping[shard_name]
        with fsspec.open(url, "r", compression="infer") as f:
            # TODO: would be nice if we could seek faster than this. Can't even skip json parsing
            data = json.load(f)
            return iter(data[row:])
# ---
def trim_zeros(f: NamedArray | ArrayLike, trim: str = "fb") -> NamedArray:
    """Named version of [jax.numpy.trim_zeros][].

    If ``f`` is not a [haliax.NamedArray][], the trimmed coefficient axis is named
    ``degree``.
    """

    (arr,) = unwrap_namedarrays(f)
    result = jnp.trim_zeros(arr, trim=trim)
    axis = _poly_axis_from_input(f, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def without_axes(axis_spec: Sequence[Axis], to_remove: AxisSelection, allow_mismatched_sizes=False) -> tuple[Axis, ...]:  # type: ignore
    ...
# ---
def embed(self, input_ids, *args):
        input_embeds = self.token_embeddings(input_ids)
        return input_embeds
# ---
def test_context_wait(job_context):
    futures = [job_context.run(lambda x: x, i) for i in range(5)]
    ready, pending = job_context.wait(futures, num_returns=2)
    assert len(ready) == 2
    assert len(pending) == 3
# ---
def inline_html(self, html):
        """Rendering span level pure html content.

        :param html: text content of the html snippet.
        """
        if self.options.get('escape'):
            return escape(html)
        return html
# ---
def prod(x, axis=0):
    return _Nprod(x, axis)
# ---
def heartbeat_deadline(self, timeout: Duration) -> Timestamp:
        """Compute the deadline when this worker's heartbeat will expire.

        Args:
            timeout: Heartbeat timeout duration

        Returns:
            Timestamp when the heartbeat will be considered expired
        """
        return self.last_heartbeat.add(timeout)
# ---
def _view_updated(self):
        if self.document.can_restore_from_prefs():
            self.restore_view()
        # XXX Logically, we should call _update_selected_pane() but doing so
        # make tests fail. to investigate.
        self._refresh_target_selection()
        self.view.update_selected_pane()
        self._refresh_swap_list_items()
        self.import_table.refresh()
# ---
def shard_names(self) -> Sequence[str]:
        return ["0"]
# ---
def model_config(self):
        if self.model_name == "llama32":
            return llama_32b_remat
        if self.model_name == "qwen32":
            return qwen3_32b_remat
        raise ValueError(f"Unknown model alias '{self.model_name}'")
# ---
def update_fn(updates, state, params, **extra_args):
        levanter.tracker.jit_log({desc: optax.tree_utils.tree_l2_norm(updates)})
        return updates, None
# ---
def eval_loss(model, *batch, **batch_kwargs):
                model = self.mp.cast_to_compute(model)
                return self.loss_fn(model, *batch, **batch_kwargs, key=None)
# ---
def restoreDefaults(self):
        self.model.restore_defaults()
# ---
def data_size(self):
        # return int(self.offsets[self.num_rows].read().result())
        if self._cached_data_size is not None:
            return self._cached_data_size
        result = int(self.offsets[self.num_rows].read().result())
        if self._cache_metadata:
            self._cached_data_size = result
        return result
# ---
def __invert__(self) -> "NamedArray":  # pragma: no cover
        return haliax.invert(self)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n), 'B': np.arange(n) + 1.0})
            df1 = df[df.A > 5]
            return len(df1.B)
# ---
def __delitem__(self, field):
        delattr(self, field)
# ---
def test_context_put_get(job_context):
    obj = {"key": "value"}
    ref = job_context.put(obj)
    assert job_context.get(ref) == obj
# ---
def visitdir(self, dir):
        return "all"
# ---
def __eq__(self, other):
        """
        Returns true if both objects are equal
        """
        if not isinstance(other, ContributorOrcid):
            return False

        return self.__dict__ == other.__dict__
# ---
def __init__(self):
        pass
# ---
def __add__(self, other: "Duration") -> "Duration":
        return Duration(self._ms + other._ms)
# ---
def exitLonely(self):
        self.ignore(self.lonelyDoneEvent)
        self.lonely.exit()
# ---
def _assert_can_put_with_sharding(array, sharding):
    try:
        jax.device_put(array, sharding)
    except ValueError:
        # assert False, f"Could not put array with shape {array.shape} with sharding {sharding}"
        raise AssertionError(f"Could not put array with shape {array.shape} with sharding {sharding}")
# ---
def get_rest_of_frame_molecule(frame_molecule, selected_molecule):
    # calc the rest
    selector = bridge.Select_AtomGroup(selected_molecule)
    selected = frame_molecule.select(selector)
    rest_molecule = frame_molecule ^ selected

    return rest_molecule
# ---
def __init__(
        self,
        service: ControllerServiceImpl,
        host: str = "0.0.0.0",
        port: int = 8080,
    ):
        self._service = service
        self._host = host
        self._port = port
        self._app = self._create_app()
        self._server: uvicorn.Server | None = None
# ---
def __init__(self, input_ids_key: str, loss_weights_key: str | None):
        self.input_ids_key = input_ids_key
        self.loss_weights_key = loss_weights_key
        self._exemplar = {input_ids_key: np.zeros((0,), dtype=np.int32)}
        if loss_weights_key is not None:
            self._exemplar[loss_weights_key] = np.zeros((0,), dtype=np.float32)
# ---
def _carry_ckpt_name(self):
        return f"BlockSeq[{self.Block}, {self.blocks[0].__class__.__name__}].carry"
# ---
def _str_hash_legacy(s: str) -> str:
    return hashlib.blake2b(s.encode(), digest_size=8).hexdigest()
# ---
def test_linecol_to_offset_first_line():
    source = "hello world\nsecond line\n"
    assert _linecol_to_offset(source, 1, 0) == 0
    assert _linecol_to_offset(source, 1, 6) == 6
# ---
def getlist(self, name, default=None):
        """Return the entire list"""
        return super().get(name, default)
# ---
def count_docs(file_path: str) -> int:
    return len(list(load_file(file_path)))
# ---
def name_to_path(name: str) -> str:
        if protocol is not None:
            return f"{protocol}://{name}"
        return name
# ---
def loadPics(self):
        self.im = loadImage("block.png")
# ---
def test_shard_map_basic():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    def fn(x):
        return x + 1

    sm = hax.shard_map(fn, mesh=mesh, check_rep=False)
    x = hax.ones(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), mesh:
        out = sm(x)
    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array + 1)
# ---
def bind(self, read_server_info=True):
        return self.bound
# ---
def load_state_dict(path):
    """
    Load a model's state dict from a file, bringing all tensors to the CPU first and then converting to numpy.
    This will load using safetensors format
    """
    if safetensors_numpy is None:
        raise ImportError("safetensors_numpy is not installed")
    state_dict = safetensors_numpy.load_file(path)
    return state_dict
# ---
def clean_li(html: BeautifulSoup):
    # Remove the li tags since they repeat the same information (eg 1. 1.)
    tags = html.findAll("span", {"class": "ltx_tag_item"})
    for tag in tags:
        tag.decompose()
    tags = html.findAll("span", {"class": "ltx_tag_listingline"})
    for tag in tags:
        tag.decompose()
# ---
def producer_with_exception():
        raise ValueError("Something went wrong!")
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: Optional[AttentionMask | NamedArray] = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray | tuple[NamedArray, NamedArray | float]:
        del attn_mask, key, pos_ids
        hidden = self.embed_weight.take(self.Vocab, input_ids)
        return hidden, self.aux_loss
# ---
def bank():
    return SubtreeBank.from_corpus(SAMPLE_PROGRAMS)
# ---
def floordiv(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def get_date(self, p_tag):
        """ Given a date tag, return a date object. """
        string = self.tag_value(p_tag)
        result = None

        try:
            result = date_string_to_date(string) if string else None
        except ValueError:
            pass

        return result
# ---
def formfield(self, *args, **kwargs):
		kwargs["form_class"] = JSONFormField
		return super(JSONField, self).formfield(*args, **kwargs)
# ---
def _coerce_to_repo_ref(checkpoint: Union[str, RepoRef]) -> RepoRef:
    """Convert string or RepoRef to RepoRef."""
    if isinstance(checkpoint, str):
        return RepoRef.from_string(checkpoint)
    return checkpoint
# ---
def create_actor_group(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        count: int,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> ActorGroup:
        """Create N instances of an actor, returning a group handle."""
        ...
# ---
def tearDown(self):
        pass
# ---
def __iter__(self):
        return iter(self.examples)
# ---
def decode(self, cipherText, shift):
        a = ord('A')
        decoder = [a + (c - shift if c >= shift else c - shift + 26) for c in range(26)]
        plain = [chr(decoder[ord(c) - a]) for c in cipherText]
        return ''.join(plain)
# ---
def iter_buildable_deps(name):
            """
            instanciates a builder for each image dependency
            does nothing when the image cannot be build
            """
            for dep_name, _ in self.getImage(name).imageDeps():
                try:
                    self.getImage(dep_name)
                    yield dep_name
                except KeyError:
                    continue
# ---
def _hook_flip(self):
        self.update()
        self._window_flip()
# ---
def root(cls, name: str) -> "JobName":
        """Create a root job name (no parent)."""
        return cls((name,))
# ---
def wait(self, futures: list, num_returns: int = 1) -> tuple[list, list]:
        """Wait for futures to complete.

        Args:
            futures: List of futures to wait on
            num_returns: Number of futures to wait for

        Returns:
            Tuple of (ready_futures, pending_futures)
        """
        ...
# ---
def _remove_tpu_lockfile_on_exit_cm():
    try:
        yield
    finally:
        _hacky_remove_tpu_lockfile()
# ---
def KillTask(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def appvms(self):
        ''' Returns a generator containing all domains based on the current
            TemplateVM.
        '''
        for vm in self.app.domains:
            if hasattr(vm, 'template') and vm.template is self:
                yield vm
# ---
def make_upcase(line):
                    return line.strip().upper()
# ---
def heartbeat(self, request: cluster__pb2.HeartbeatRequest, ctx: RequestContext) -> cluster__pb2.HeartbeatResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def get_binary_path(build_dir_path):
    return os.path.join(build_dir_path, 'local', 'bin', 'jerry')
# ---
def get_default_display(self):
        """Get the default display device.

        :deprecated: Use `pyglet.canvas.get_display`.

        :rtype: `Display`
        """
        return pyglet.canvas.get_display()
# ---
def load_json(self, loads=json_loads):
        try:
            self.parsed_json = loads(self.body)
        except Exception:
            if not self.body:
                return None
            raise InvalidUsage("Failed when parsing body as json")

        return self.parsed_json
# ---
def transform_to(self, srs):
        return MultiCoverage([c.transform_to(srs) for c in self.coverages])
# ---
def _init_wandb(config: DedupConfig):
    """Initialize wandb for deduplication tracking."""
    init_wandb(
        run_name=f"{config.mode}",
        tags=[str(config.mode)],
        config={
            "mode": str(config.mode),
            "input_path": config.input_paths,
            "processes": config.processes,
        },
    )
# ---
def tearDown(self):
        self.es_client.indices.delete(index="*", ignore=404)
        self.es_client.indices.delete_template(name="*", ignore=404)
# ---
def polyder(p: NamedArray | ArrayLike, m: int = 1) -> NamedArray:
    """Named version of [jax.numpy.polyder][].

    If ``p`` is not a [haliax.NamedArray][], the differentiated polynomial uses a
    coefficient axis named ``degree``.
    """

    (arr,) = unwrap_namedarrays(p)
    result = jnp.polyder(arr, m=m)
    axis = _poly_axis_from_input(p, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def is_param(value):
    """Determine whether given value is a parameter string."""
    if not isinstance(value, str):
        return False
    return RE_PARAM.match(value)
# ---
def remove_words(self, words):
        """Remove a word or words from the list of words to auto-complete."""
        if isinstance(words, basestring):
            del self._word_freq[words]
            self._word_list.remove(words)
        else:
            for w in words:
                try:
                    del self._word_freq[w]
                    self._word_list.remove(w)
                except KeyError:
                    pass
# ---


def encode_shift(s: str):
    """
    returns encoded string by shifting every character by 5 in the alphabet.
    """
    return "".join([chr(((ord(ch) + 5 - ord("a")) % 26) + ord("a")) for ch in s])


def decode_shift(s: str):
    """
    takes as input string encoded with encode_shift function. Returns decoded string.
    """
    return "".join([chr(((ord(ch) - 5 - ord("a")) % 26) + ord("a")) for ch in s])
# ---
def test_stmt_exception_pickleable_no_dbapi(self):
        self._test_stmt_exception_pickleable(Exception("hello world"))
# ---
def usage():
    print("Usage: %s WORKFLOW_DIR" % sys.argv[0])
# ---
def decode_cmd(tokens: str, tokenizer: str | None):
    """Decode tokens to text."""
    if tokenizer:
        load_tokenizer(tokenizer)
    try:
        token_list = ast.literal_eval(tokens)
        decode_tokens(token_list)
    except Exception as e:
        console.print(f"[red]Error parsing tokens: {e}[/red]")
# ---
def testDeleteNonexistentLocal(self):
    self.assertRaisesRegexp(
        util.ParseError, 'cannot delete nonexistent local',
        _ParseAndVisit, 'def foo():\n  del bar')
# ---
def update(self, new_model: M, step: int) -> "EmaDecaySqrtModelAveraging[M]":
        w = self._raw_weight(step)
        new_tot_w = self.tot_weight + w
        alpha = 0.0 if new_tot_w == 0.0 else w / new_tot_w
        updated = optax.incremental_update(new_model, self.model, alpha)
        return dataclasses.replace(self, model=updated, tot_weight=new_tot_w)
# ---
def py3_info(info):
    # NOTE(boris-42): py33 I hate you.
    info_py3 = copy.deepcopy(info)
    new_name = re.sub("FakeTrace[^.]*", "FakeTracedCls",
                      info_py3["function"]["name"])
    info_py3["function"]["name"] = new_name
    return info_py3
# ---
def encode(self, text, add_special_tokens=True):
            return [ord(c) for c in text]
# ---
def _infer_out_sharding(out_axes: tuple[Axis, ...]):
    mapping = current_thread_local_mapping()
    mesh = _resolve_mesh()

    if mesh is None:
        return None

    pspec = pspec_for_axis(out_axes, mapping)
    if not all_mesh_axes_explicit(mesh, pspec):
        # JAX doesn't like it when you pass out_sharding for non-explicit axes
        return None
    return NamedSharding(mesh, pspec)
# ---
def testTryBareExcept(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        try:
          raise AssertionError
        except:
          pass""")))
# ---
def updateversiondb(cur) :
    db = cherrypy.session['database']
    username = cherrypy.session['username']

    dt = time.strftime("%Y-%m-%d %H:%M:%S")
    try:
        sql = "UPDATE Radio SET radio='%s', genre='%s' WHERE id = 0" % (hostname, dt)
        cur.execute(sql)
    except:
        return
# ---
def pop(self):
        """Return the element at the front of the queue.

        Returns
        -------
        The first element in the queue.

        """
        return self._queue.pop(0)
# ---
def unobserve_property(self, name, handler):
        handlers = self._property_handlers[name]
        handlers.remove(handler)
        if not handlers:
            _mpv_unobserve_property(self._event_handle, hash(name)&0xffffffffffffffff)
# ---
def __init__(self, gan=None, config=None, trainer=None):
      super().__init__(config=config, gan=gan, trainer=trainer)
      self.d_grads = None
      self.g_grads = None
# ---
def f(c, foo):
        return c, foo.my_array
# ---
def get_handler_filename(handler):
    """Shortcut to get the filename from the handler string.

    :param str handler:
      A dot delimited string representing the `<module>.<function name>`.
    """
    module_name, _ = handler.split(".")
    return "{0}.py".format(module_name)
# ---
def kill(self, container_id: str, force: bool = False) -> None: ...
# ---
def __hash__(self):
        """Hash based on the ID (every object is different)."""
        return hash(id(self))
# ---
def validate(candidate):
            return hax.where(self.used_mask["seq", candidate], INVALID, candidate)
# ---
def service_with_autoscaler(state, scheduler, mock_autoscaler):
    """Service with autoscaler enabled for tests."""
    controller_mock = Mock()
    controller_mock.wake = Mock()
    controller_mock.task_schedule_status = scheduler.task_schedule_status
    controller_mock.autoscaler = mock_autoscaler  # Enable autoscaler
    return ControllerServiceImpl(state, controller_mock, bundle_prefix="file:///tmp/iris-test-bundles")
# ---
def loc(self):
        """Attribute for location based indexing. Only supports __getitem__,
        and only when the key is a dict of the form {dim: labels}.
        """
        return _LocIndexer(self)
# ---
def test_equal(self):
        """Test that cmp_version compares a as equal to b"""
        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.4') == 0)
# ---
def axis_indices(self, axis: AxisSelector) -> int | None:
        name = axis_name(axis)
        for i, ax in enumerate(self.axes):
            if ax.name == name:
                return i
        return None
# ---
def compute_reward(self, correct_answer: str, actual_response: str, tokenizer=None) -> float:
        """Compute reward for a response."""
        ...
# ---
def _infer_num_tensorcores() -> int:
    if jax.default_backend() != "tpu":
        return 1
    device_kind = jax.devices()[0].device_kind.lower()
    if "tpu v4" in device_kind or ("tpu v5" in device_kind and "v5e" not in device_kind) or "tpu v7" in device_kind:
        return 2
    return 1
# ---
def cleanup(self) -> None:
        """Cleanup Flight server resources."""
        # shutdown servers in parallel in threads to avoid blocking on shutdown
        for flight_server in self._flight_servers:
            logger.debug(f"Shutting down Arrow Flight server at {flight_server._location}...")
            threading.Thread(target=flight_server.shutdown, daemon=True).start()
# ---
def _is_int(x: float) -> bool:
    try:
        return abs(x - round(x)) <= 1e-7
    except BaseException:
        return False
# ---
def to_proto(self) -> cluster_pb2.Constraint:
        """Convert to protobuf representation."""
        proto = cluster_pb2.Constraint(key=self.key, op=self.op.to_proto())
        if self.value is not None:
            proto.value.CopyFrom(AttributeValue(self.value).to_proto())
        return proto
# ---
def _ViewEventHandler(f):
    f._view = True
    return f
# ---
def test_local_llm_inference():
    config = ModelConfig(
        name="test-llama-200m",
        path="gs://marin-us-east5/gcsfuse_mount/perplexity-models/llama-200m",
        engine_kwargs={"enforce_eager": True, "max_model_len": 1024},
        generation_params={"max_tokens": 16},
    )
    model_name_or_path, config = resolve_model_name_or_path(config)
    run_vllm_inference(model_name_or_path, **config.engine_kwargs)
# ---
def finished(status: JobStatus) -> bool:
        return status in (JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED)
# ---
def fechatimbrado(self):
        return self.__fechatimbrado
# ---
def test_compute_bracket_mask_nested(tokenizer):
    mask = compute_bracket_mask("f([", tokenizer)
    # ] should be allowed (matches open bracket).
    assert float(mask[tokenizer.encode_char("]")]) == 1.0
    # ) should be blocked (wrong bracket type -- innermost is [).
    assert float(mask[tokenizer.encode_char(")")]) == 0.0
# ---
def getOpenVGDBToPhoenixMap(self):
        return OrderedDict(sorted(self.openVGDBToPhoenixMap.items(), key=lambda t: t[0]))
# ---
def tree_map(fn, tree, *rest, is_leaf=None):
    """
    Version of [jax.tree_util.tree_map][] that automatically treats NamedArrays as leaves.
    """
    old_is_leaf = is_leaf
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: old_is_leaf(x) or is_named_array(x)

    return jax.tree.map(fn, tree, *rest, is_leaf=is_leaf)
# ---
def check_answer(self, sample_str: str, ground_truth: str) -> bool:
        """Grade the answer using Tinker's grading."""
        try:
            answer = extract_boxed(sample_str)
        except ValueError:
            return False
        return grade_answer(answer, ground_truth)
# ---
def test_expires_on(self):
        eq_(self.record.expires_on, None)
# ---
def __init__(self, date, precision):
        """Create a period with the given date (a `datetime` object) and
        precision (a string, one of "year", "month", "day", "hour", "minute",
        or "second").
        """
        if precision not in Period.precisions:
            raise ValueError(f'Invalid precision {precision}')
        self.date = date
        self.precision = precision
# ---
def poke(self, context):
        hook = BaseHook.get_connection(self.conn_id).get_hook()

        logging.info('Poking: ' + self.sql)
        records = hook.get_records(self.sql)
        if not records:
            return False
        else:
            if str(records[0][0]) in ('0', '',):
                return False
            else:
                return True
            print(records[0][0])
# ---
def count(self) -> Dataset[int]:
        """Count the total number of items in the dataset.

        Returns:
            Dataset containing a single integer count

        Example:
            >>> ds = Dataset.from_list(range(100)).filter(lambda x: x % 2 == 0)
            >>> count = list(Backend.execute(ds.count()))[0]
            50
        """
        return self.reduce(
            local_reducer=lambda items: sum(1 for _ in items),
            global_reducer=sum,
        )
# ---
def test_ravel():
    H, W, D = hax.make_axes(Height=2, Width=3, Depth=4)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))
    raveled = named1.ravel("Z")

    assert raveled.size == H.size * W.size * D.size
    assert hax.all(hax.equal(raveled, named1.flatten_axes((H, W, D), "Z")))
    assert jnp.all(jnp.equal(raveled.array, jnp.ravel(named1.array)))
# ---
def process_bind_param(self, value, dialect):
                raise nope
# ---
def stop(self) -> None:
        """Stop controller."""
        ...
# ---
def get_absolute_saagie_url(saagie_url):
    if saagie_url.startswith('/'):
        return SAAGIE_ROOT_URL + saagie_url
    return saagie_url
# ---
def num_bytes(model: PyTree):
    # especially with jax.vjp, we get duplicate arrays and want to uniq them
    # NB we need to use object identity here, mostly because of ShapedDtypeStruct
    leaves = {id(x): x for x in jax.tree_util.tree_leaves(model) if is_jax_array_like(x)}
    return sum(x.nbytes for x in leaves.values())
# ---
def test_empty_dataset(backend):
    """Test operations on empty dataset."""
    ds = Dataset.from_list([])
    assert list(Backend.execute(ds, context=backend)) == []
    assert list(Backend.execute(ds.filter(lambda x: True), backend)) == []
    assert list(Backend.execute(ds.map(lambda x: x * 2), backend)) == []
    assert list(Backend.execute(ds.window(10), backend)) == []
# ---
def __next__(self):
        """Iterate through the future's result."""
        if self._iterator is None:
            self._iterator = iter(self.result())
        return next(self._iterator)
# ---
def test_creates_correct_controller_type(self, config_fixture: str, expected_type: type, request):
        """create_controller returns correct controller type based on config."""
        config = request.getfixturevalue(config_fixture)
        controller = create_controller_vm(config)
        assert isinstance(controller, expected_type)
# ---
def case(context):
        """Alter the context."""
        assert context == {"squee": "kapow"}
        context.squee = "boing"
# ---
def test_single_batched_selector():
    B, S, V = Axis("batch", 4), Axis("seq", 3), Axis("vocab", 7)
    x = hax.arange((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    out = x["vocab", idx]
    assert out.axes == (B, S)
    assert jnp.array_equal(out.array, _ref_gather(x, V, idx))
# ---
def preview(c):
    """Build production version of site"""
    c.run('pelican -s publishconf.py')
# ---
def diagnostics(self, *, max_lines: int = 200) -> dict[str, str]:
        if self.vllm_server is None:
            return {}
        return self._backend.diagnostics(self.vllm_server, max_lines=max_lines)
# ---
def format_pwms_new(pwms_new):
    return format_line(prefix='new pwms'.rjust(RJUST), values=pwms_new)
# ---
def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LmDatasetSourceConfigBase:
        humanfriendly_tokens = humanfriendly.format_size(self.num_tokens)[0:-1].replace(" ", "").replace("byte", "")
        out = _patch_source_config(
            self.input_config, self.cache_path, extra_tags=["subsampled", f"subsampled-{humanfriendly_tokens}"]
        )

        return out
# ---
def transition_task(state: ControllerState, task: ControllerTask, new_state: int, *, error: str | None = None) -> None:
    """Transition a task to a new state via handle_event."""
    state.handle_event(
        TaskStateChangedEvent(
            task_id=task.task_id,
            new_state=new_state,
            attempt_id=task.current_attempt_id,
            error=error,
        )
    )
# ---
def generate_messages(database, writes, stream_id, stream_token):
    # writes can be an array and append to the messages, so it can write multiple Write
    # here just write one as example
    messages = [
            firestore_pb2.WriteRequest(database=database, writes = []),
            firestore_pb2.WriteRequest(database=database, writes = [writes],  stream_id = stream_id, stream_token = stream_token)
    ]
    for msg in messages:
            yield msg
# ---
def get_values(self, env):
        """ return a list of the possible values """
        selection = self.selection
        if isinstance(selection, basestring):
            selection = getattr(env[self.model_name], selection)()
        elif callable(selection):
            selection = selection(env[self.model_name])
        return [value for value, _ in selection]
# ---
def __init__(self, data: list[T], is_complete: bool = False):
        super().__init__()
        self.data = data
        self.is_complete = is_complete
        if not is_complete:
            self.complete_promise: Optional[asyncio.Future[None]] = asyncio.Future()
            self.length_updated: Optional[asyncio.Condition] = asyncio.Condition()
        else:
            self.complete_promise = None
            self.length_updated = None
# ---
def test_alibi_attention_compared_to_hf():
    import torch
    from transformers.models.bloom.modeling_bloom import build_alibi_tensor

    L, H = hax.make_axes(L=1, H=16)

    # Returns tensor shaped (batch_size * num_heads, 1, max_seq_len)
    torch_tensor = (
        build_alibi_tensor(torch.ones(1, L.size), H.size, dtype=torch.float32).numpy().reshape(H.size, L.size)
    )

    hax_tensor = np.array(alibi_attention_bias(H, L).array)

    assert np.allclose(torch_tensor, hax_tensor)
# ---
def test_augment_bank_increases_size(bank, rng):
    original_size = bank.total_entries
    augmented = augment_bank(bank, rng, n_renamed=1, n_perturbed=1, synthetic_count=10)
    assert augmented.total_entries > original_size
# ---
def test_get_with_invalid_election_id_non_integer_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': 'hey' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified a non-integer election ID.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def fake_vdi_resize(*args, **kwargs):
            called['resize'] = True
# ---
def __str__(self) -> str:
        """Canonical wire format: '/root/child/grandchild'."""
        return "/" + "/".join(self._parts)
# ---
def assign_task_to_worker(state: ControllerState, task: ControllerTask, worker_id: WorkerId) -> None:
    """Assign a task to a worker via event."""
    state.handle_event(
        TaskAssignedEvent(
            task_id=task.task_id,
            worker_id=worker_id,
        )
    )
# ---
def sync_global_devices(name: str):
    """Creates a barrier across all hosts/devices."""
    h = np.uint32(zlib.crc32(name.encode()))
    assert_equal(h, f"sync_global_devices name mismatch ('{name}')")
# ---
def convert_svg(self, el, text, convert_as_inline):
        # ignore svg elements
        return ""
# ---
def get_lm_head(self) -> hax.NamedArray:
        return self.embeddings.token_embeddings.weight if self.lm_head is None else self.lm_head.weight
# ---
def test_augment_bank_no_duplicates(bank, rng):
    augmented = augment_bank(bank, rng, n_renamed=2, n_perturbed=2, synthetic_count=20)
    for node_type, entries in augmented.entries.items():
        sources = [e.source for e in entries]
        assert len(sources) == len(set(sources)), f"Duplicates in {node_type}"
# ---
def create_jsonl_gz_file(data: list[dict], filepath: str):
    """Helper function to create a JSONL.GZ file"""
    with fsspec.open(filepath, "wt", encoding="utf-8", compression="infer") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")
# ---
def get_mapping(self):
        raise NotImplementedError
# ---
def finalize(self) -> list[PhysicalStage]:
        """Flush remaining ops and return completed stages."""
        self.end_stage()
        return self.stages
# ---
def main(config: FilterDolminoConfig):
    filter_dolmino(config)
# ---
def without_axes(axis_spec: PartialShapeDict, to_remove: AxisSelection, allow_mismatched_sizes=False) -> PartialShapeDict:  # type: ignore
    ...
# ---
def __colors(self):
        return [key for key in self.__dict__.keys() if not key.startswith('_') and key != 'named']
# ---
def _unflatten_bshd(attn_output, q_class, v_class):
    attn_output = attn_output.unflatten_axis("B", q_class["B"])
    attn_output = attn_output.unflatten_axis("S", q_class["S"])
    attn_output = attn_output.unflatten_axis("H", q_class["H"])
    attn_output = attn_output.unflatten_axis("D", v_class["D"])
    return attn_output
# ---
def _watch_stop() -> None:
                    self._stop_event.wait()
                    assert on_stop is not None
                    try:
                        on_stop()
                    except Exception:
                        logger.exception("on_stop callback failed for %s", name or "<unnamed>")
# ---
def reset_input_style(self):
        """Reset regex input line background"""
        if self._input_styled:
            self.inputLine.setStyleSheet(self.styleSheet())
            self._input_styled = False
# ---
def start(self):
        self._start_time = time.time()
        self._n += 1
# ---
def _coerce_to_rr(s: Union[str, RepoRef]) -> RepoRef:
    if isinstance(s, RepoRef):
        return s
    else:
        return RepoRef.from_string(s)
# ---
def checkPause(self):
    pass
# ---
def match(self, item):
        return any(q.match(item) for q in self.subqueries)
# ---
def _cycle_property(self, name, direction='up'):
        self.command('cycle_property', name, direction)
# ---
def test_with_exception(self, mock_start, mock_stop):

        self.assertRaises(ValueError, test_fn_exc)
        expected_info = {
            "function": {
                "name": "osprofiler.tests.unit.test_profiler.test_fn_exc"
            }
        }
        expected_stop_info = {"etype": "ValueError", "message": ""}
        mock_start.assert_called_once_with("foo", info=expected_info)
        mock_stop.assert_called_once_with(info=expected_stop_info)
# ---
def check_valid_bot_type(user_profile: UserProfile, bot_type: int) -> None:
    if bot_type not in user_profile.allowed_bot_types:
        raise JsonableError(_("Invalid bot type"))
# ---
def _custom_setup(self):
        kwargs = {}
        kwargs['netapp_mode'] = 'proxy'
        kwargs['configuration'] = create_configuration()
        self._driver = netapp_nfs.NetAppDirectCmodeNfsDriver(**kwargs)
        self._driver.ssc_enabled = True
        self._driver.configuration.netapp_copyoffload_tool_path = 'cof_path'
# ---
def manipulate(text):
            for key in rules:
                rule = getattr(self.rules, key)
                m = rule.match(text)
                if not m:
                    continue
                getattr(self, 'parse_%s' % key)(m)
                return m
            return False
# ---
def test_capacity_format(self):
        """String formatting for Capacity"""
        c = Capacity(1, 3)
        self.assertEqual(str(c), "R:1.0 W:3.0")
        c = Capacity(0, 0)
        self.assertEqual(str(c), "0")
# ---
def test_plain(self):
        self._test_proxy(_result.ResultProxy)
# ---
def __init__(self, output_path: str, worker_id: str):
        self.output_path = output_path
        self.path = get_status_path(output_path)
        self.worker_id = worker_id
        self._lock_path = self.path + ".lock"
        self.fs = fsspec.core.url_to_fs(self.path, use_listings_cache=False)[0]
# ---
def setup(*args, **kwargs):
    try:
        ctypes.cdll.LoadLibrary(leptlib)
    except Exception as e:
        raise NidabaPluginException(e.message)
# ---
def mpra(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transduction']
    }
# ---
def selkeys(dict_, keys):
            if dict_ is None:
                return None
            return dict((d, dict_[d]) for d in keys if d in dict_)
# ---
def fetch_pipeline_from_server(self):
        """
        Method fetches pipeline from server/cloud
        """
        # TODO
        pass
# ---
def __init__(self, opts):
        self.opts = opts
        if self.opts['transport'] in ('zeromq', 'tcp'):
            self.key = Key(opts)
        else:
            self.key = RaetKey(opts)
# ---
def not_(cls, operand: "PyExpr") -> "PyExpr": ...
# ---
def test_instance_auto_disk_config_passes_fail_safes(self):
        """Should partition if instance is marked as auto_disk_config=True and
        virt-layer specific fail-safe checks pass.
        """
        self.instance_values['auto_disk_config'] = True

        def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4')]
        self.stubs.Set(vm_utils, "_get_partitions",
                       fake_get_partitions)

        self.assertIsPartitionCalled(True)
# ---
def start(self):
        self.fsm.request('Lonely')
# ---
def _assert_coefficients(coeffs: NamedArray, expected: jnp.ndarray, axis: Axis) -> None:
    assert jnp.allclose(coeffs.array, expected)  # type: ignore[union-attr]
    assert coeffs.axes[0] == axis.resize(coeffs.array.shape[0])
# ---
def test_vmap_mapped_kwarg():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Batch, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, Batch)(x=named1)

    expected_jax = jnp.array([named1.sum(Width).array for _ in range(Batch.size)])
    expected_names = (Batch, Depth)

    assert jnp.all(jnp.equal(selected.array, expected_jax))
    assert selected.axes == expected_names
# ---
def test_str_contains_noregex(self):
        def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('BB', regex=False)
            return B.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), 1)
# ---
def _chunk_encode(orig_iter):
        for chunk in orig_iter:
            if not len(chunk):
                continue
            chunk_len = b'%X\r\n' % len(chunk)
            yield chunk_len
            yield chunk
            yield b'\r\n'

        yield b'0\r\n\r\n'
# ---
def test_random_mutation_changes_source(bank):
    source = CORPUS[0]  # fibonacci
    rng = random.Random(42)

    mutation = random_mutation(source, bank, rng=rng)
    assert mutation is not None
    assert mutation.replacement != mutation.original
# ---
def test_find_path_identical_empty():
    source = "x = 1\n"
    path = find_path(source, source)
    assert path == []
# ---
def __init__(self, cluster: LocalClusterClient | RemoteClusterClient, namespace: Namespace = Namespace("")):
        """Initialize IrisClient with a cluster client.

        Prefer using factory methods (local(), remote()) over direct construction.

        Args:
            cluster: Low-level cluster client (LocalClusterClient or RemoteClusterClient)
        """
        self._cluster_client = cluster
        self._namespace = namespace
# ---
def update(self):
        """Records a new data point at the current time.  This method
        is called automatically when the window buffer is flipped.
        """
        from time import time
        t = time()
        self.count += 1
        self.time += t - self.last_time
        self.last_time = t

        if self.time >= self.update_period:
            self.set_fps(self.count / self.update_period)
            self.time %= self.update_period
            self.count = 0
# ---
def create_nano_llama_config() -> LlamaConfig:
    """Create a tiny LlamaConfig for fast testing."""
    return LlamaConfig(
        max_seq_len=64,
        hidden_dim=64,
        intermediate_dim=128,
        num_heads=8,
        num_kv_heads=8,
        num_layers=4,
        tokenizer=DummyTokenizer(),
    )
# ---
def get_lm_head(self) -> NamedArray:
        return hax.named(self.lm_head_fn(self.params), (self.Embed, self.Vocab))
# ---
def gcs_config():
    """Create weight transfer config for GCS checkpoint mode."""
    return WeightTransferConfig(
        mode=WeightTransferMode.GCS_CHECKPOINT,
        checkpoint_dir="gs://marin-eu-west4/rl_testing/llama-1b-math-rl-test-0-35d621/policy_checkpoints",
    )
# ---
def test_wrapped_url(self):
        req = Request("<URL:http://www.python.org>")
        self.assertEqual("www.python.org", req.get_host())
# ---
def accuracy(self):
        """The fractional voltage of the least significant bit. """
        return self._Vref / float(self.get_value_max())
# ---
def format_limits(limits):
    return format_line(prefix='limits'.rjust(RJUST), values=limits)
# ---
def target_account_names(self):
        return [tr('< New Account >')] + [a.name for a in self.target_accounts]
# ---
def example_reading_spec(self):
    data_fields = {
        "image/encoded": tf.FixedLenFeature((), tf.string),
        "image/format": tf.FixedLenFeature((), tf.string),
    }

    data_items_to_decoders = {
        "inputs":
            contrib.slim().tfexample_decoder.Image(
                image_key="image/encoded",
                format_key="image/format",
                channels=self.num_channels),
    }

    return data_fields, data_items_to_decoders
# ---
def visible(self):
        """True if the window is currently visible.  Read-only.

        :type: bool
        """
        return self._visible
# ---
def getSolution(self,solverId):
    path = "{base}/{solverId}/solution".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.get(path,
                       params=self.params,
                       headers=self.headers)
    self.validateReply(req)
    return req.json()
# ---
def validate_variant_based_on_change(self):
		if not self.is_new() and (self.variant_of or (self.has_variants and frappe.get_all("Item", {"variant_of": self.name}))):
			if self.variant_based_on != frappe.db.get_value("Item", self.name, "variant_based_on"):
				frappe.throw(_("Variant Based On cannot be changed"))
# ---
def method4(self, j):
        return j
# ---
def test_fillna_inplace(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            df.A.fillna(5.0, inplace=True)
            return df.A.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
# ---
def QHeadsPerGroup(self) -> Axis:
        """Axis for query heads per group."""
        return Axis("q_heads_per_group", self.q_heads_per_group)
# ---
def intersection(self, bbox, srs):
        bbox = self._bbox_in_coverage_srs(bbox, srs)
        intersection = (
            max(self.bbox[0], bbox[0]),
            max(self.bbox[1], bbox[1]),
            min(self.bbox[2], bbox[2]),
            min(self.bbox[3], bbox[3]),
        )

        if intersection[0] >= intersection[2] or intersection[1] >= intersection[3]:
            return None
        return BBOXCoverage(intersection, self.srs)
# ---
def separator(self, inc_sep):
        ''' setter method for separator '''
        self._separator = inc_sep
# ---
def test_filter_with_expression(backend):
    """Test filter with expression on in-memory data."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"name": "alice", "score": 80},
            {"name": "bob", "score": 60},
            {"name": "charlie", "score": 90},
        ]
    ).filter(col("score") > 70)

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert all(r["score"] > 70 for r in results)
# ---
def _validate_vm_types(config: config_pb2.IrisClusterConfig) -> None:
    """Validate that scale groups have explicit vm_type."""
    for name, sg_config in config.scale_groups.items():
        if sg_config.vm_type == config_pb2.VM_TYPE_UNSPECIFIED:
            raise ValueError(f"Scale group '{name}' must set vm_type to tpu_vm, gce_vm, manual_vm, or local_vm.")
# ---
def terminate(self) -> None:
        if self._terminated:
            return
        self._terminated = True
        for worker in self._workers:
            worker.stop()
        for vm in self._managed_vms:
            self._vm_registry.unregister(vm.info.vm_id)
# ---
def traslado_ieps(self):
        return self.__trieps
# ---
def test_permutation_is_deterministic1(PermutationClass):
    length = 4
    prng_key = jrandom.PRNGKey(1)
    permutation = PermutationClass(length, prng_key)
    indices = jnp.arange(length)
    results = permutation(indices)
    prng_key = jrandom.PRNGKey(1)
    permutation = PermutationClass(length, prng_key)
    results2 = permutation(indices)
    assert jnp.all(results == results2)
# ---
def _getSSCC(self, cr, uid, context=None):
        cr.execute('select id from stock_tracking where create_uid=%s order by id desc limit 1', (uid,))
        res = cr.fetchone()
        return (res and res[0]) or False
# ---
def foo(x):
            return x
# ---
def bank():
    return SubtreeBank.from_corpus(CORPUS)
# ---
def fsspec_exists(file_path):
    """
    Check if a file exists in a fsspec filesystem.

    Args:
        file_path (str): The path of the file

    Returns:
        bool: True if the file exists, False otherwise.
    """

    # Use fsspec to check if the file exists
    fs = fsspec.core.url_to_fs(file_path)[0]
    return fs.exists(file_path)
# ---
def __str__(self):
        """String representation of the Check tree rooted at this node."""

        pass
# ---
def roots(p: NamedArray | ArrayLike) -> NamedArray:
    """Named version of [jax.numpy.roots][].

    If ``p`` is not a [haliax.NamedArray][], the root axis is named ``degree``.
    """

    (arr,) = unwrap_namedarrays(p)
    result = jnp.roots(arr)
    axis = _poly_axis_from_input(p, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def dtype(self):
        """Return the dtype of the underlying reference."""
        return self._ref.dtype
# ---
def test_mistral_gpt2_roundtrip():
    _roundtrip_compare_gpt2_checkpoint("stanford-crfm/expanse-gpt2-small-x777", "checkpoint-60000")
# ---
def test_bench_rust_minhash(benchmark: Any, sample_batch: pa.RecordBatch) -> None:
    batch = sample_batch.slice(length=BENCHMARK_ROWS)
    benchmark(rust_minhash_pipeline, batch)
# ---
def __init__(self, name, description, pluginClass,
				 pluginStartForm, pluginStartMethod,
				 pluginEditForm=None, pluginEditMethod=None):
		self.name=name #No Spaces please...
		self.description=description
		self.plugin=pluginClass
		self.manage_addForm=pluginStartForm
		self.manage_addMethod=pluginStartMethod
		self.manage_editForm=pluginEditForm
		self.manage_editMethod=pluginEditMethod
# ---
def sleep_and_handle_sigterm():
        def handler(signum, frame):
            sys.exit(0)

        signal.signal(signal.SIGTERM, handler)
        while True:
            time.sleep(1)
# ---
def open(read_server_info=True):
        return
# ---
def get_milestone_id():
    """Return the milestone ID for 'Release'"""
    url = f"https://api.github.com/repos/{GITHUB_REPO}/milestones"
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    for m in response.json():
        if m["title"] == MILESTONE_NAME:
            return m["number"]
    raise ValueError(f"Milestone '{MILESTONE_NAME}' not found")
# ---
def upload_photo_id_image(self, img):
        raise NotImplementedError
# ---
def is_elastic(datasource):
    """Detect if given resource uses elastic."""
    return datasource.get('backend') == 'elastic' or datasource.get('search_backend') == 'elastic'
# ---
def test_permutation_rejects_invalid_indices(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    with pytest.raises(IndexError):
        permutation(-1)  # Test negative index
    with pytest.raises(IndexError):
        permutation(length)
# ---
def nonblocking(self) -> "InputName":
        """
        the step will not block on (or attempt to execute) the parent step.

         (Note that if another step depends on the parent step, it will still block on it.)
        """
        return dataclasses.replace(self, block_on_step=False)
# ---
def __rdivmod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.divmod(other, self)
# ---
def number(self):
        try:
            return int(self.num_ctrl.GetValue())
        except ValueError:
            return 0
# ---
def extract_id(self, item):
        raise NotImplementedError
# ---
def get(self):
        self.redirect('/posts/last')
# ---
def get_value(self) -> int:
        return self.value
# ---
def output(self, text, rules=None):
        self.tokens = self.block(text, rules)
        self.tokens.reverse()

        self.inline.setup(self.block.def_links, self.block.def_footnotes)

        out = self.renderer.placeholder()
        while self.pop():
            out += self.tok()
        return out
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)
        if isinstance(pattern, str):
            self.pattern = util.str2bool(pattern)
        self.pattern = int(self.pattern)
# ---
def log_summary(self, metrics: dict[str, Any]):
        self.metrics["summary"] = metrics
# ---
def _build_cmd(self, command: str) -> list[str]:
        return [
            "gcloud",
            "compute",
            "ssh",
            self.vm_name,
            f"--zone={self.zone}",
            f"--project={self.project_id}",
            "--quiet",
            "--command",
            command,
        ]
# ---
def test_propagate_engine_to_connection(self):
        engine = testing_engine("sqlite://",
                        options=dict(execution_options={"foo": "bar"}))
        conn = engine.connect()
        eq_(conn._execution_options, {"foo": "bar"})
# ---
def with_default_renderer_for(self, *syntax_names: str) -> RenderContext:
        renderers = dict(self.renderers)
        for syntax in syntax_names:
            if syntax in DEFAULT_RENDERERS:
                renderers[syntax] = DEFAULT_RENDERERS[syntax]
            else:
                renderers.pop(syntax, None)
        return RenderContext(
            MappingProxyType(renderers), self.postprocessors, self.options, self.env
        )
# ---
def t_TASK(self, t):
        r'((?!\(x\))).+'
        return t
# ---
def post_with_null_comment():
    return make_post(with_comments=False)
# ---
def __call__(self, indices: np.ndarray) -> np.ndarray: ...
# ---
def is_resource_deleted(self, id):
        try:
            self.show_image(id)
        except lib_exc.NotFound:
            return True
        return False
# ---
def return_host(self, host: str) -> None:
        """Return a host to the available pool.

        Called when a VM group is terminated to make the host available again.
        Safe to call multiple times for the same host.
        """
        self._return_hosts([host])
# ---
def fsdp(fn: F, parameter_mapping: ResourceMapping, compute_mapping: ResourceMapping) -> F: ...
# ---
def needAuthForm(self, action):
        """Does this action require an authentication form?"""
        if action not in self.knownActions:
            raise KeyError("unknown action")
        cfg = self.config.get(action, False)
        if cfg == 'auth' or callable(cfg):
            return True
        return False
# ---
def remove(k):
            del self._variables[k]
            self._coord_names.discard(k)
# ---
def test_concat_series_str(self):
        def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1.two, df2.two])
            return (A3 == 'foo').sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def sample_plugin():
    class TestPlugin(BasePlugin):
        @event.event
        def on_test(self):
            pass

    return TestPlugin
# ---
def __init__(self, future: Future):
        self._future = future
        self._iterator: Iterator[Any] | None = None
# ---
def __call__(self, cfg: GrugConfigLike, *, key: PRNGKeyArray) -> PyTree: ...
# ---
def _get_llama_config(use_flash=False, num_kv_heads=4, seq_len=128) -> LlamaConfig:
    return LlamaConfig(
        max_seq_len=seq_len,
        hidden_dim=32,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        attn_backend=AttentionBackend.DEFAULT if use_flash else AttentionBackend.VANILLA,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def _spec_shard_factor(entry, mesh) -> int:
    """Compute product of mesh axis sizes referenced by a PartitionSpec entry."""
    if mesh is None:
        return 1
    if entry is None or entry is PartitionSpec.UNCONSTRAINED:
        return 1
    if isinstance(entry, str):
        return int(mesh.shape.get(entry, 1))
    prod = 1
    for e in entry:
        if e is None or e is PartitionSpec.UNCONSTRAINED:
            continue
        prod *= int(mesh.shape.get(e, 1))
    return prod
# ---
def _original_model(old_model):
        return original_model
# ---
def get_num_train_steps(param_count, batch_size, max_seq_len):
    """Compute the number of steps for Chinchilla optimal training (20x params tokens)."""
    total_tokens = param_count * 20
    tokens_per_step = batch_size * max_seq_len
    return total_tokens // tokens_per_step
# ---
def test_divide(self):
        expr = col("a") / col("b")
        assert expr.evaluate({"a": 10, "b": 4}) == 2.5
# ---
def is_overdue(self):
        """
        Returns True when the due date is in the past and the task has not
        yet been completed.
        """
        return not self.is_completed() and self.days_till_due() < 0
# ---
def deposit(account, amount):
    account['balance'] += amount
    return account['balance']
# ---
def test_mem_write_byte_char_before_attribute(self):
        self.mda.mem_write_byte(3998, 0xFF)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_GREEN, MDA_BLACK))
        self.mda.mem_write_byte(3999, MDA_ATTR_INTENSITY)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def blockquote(node: RenderTreeNode, context: RenderContext) -> str:
    marker = "> "
    with context.indented(len(marker)):
        text = make_render_children(separator="\n\n")(node, context)
        lines = text.splitlines()
        if not lines:
            return ">"
        quoted_lines = (f"{marker}{line}" if line else ">" for line in lines)
        quoted_str = "\n".join(quoted_lines)
        return quoted_str
# ---
def _stop_job(job_id: str) -> None:
    """Stop a running Ray job.

    Note: This requires RAY_ADDRESS to be set, typically via ray_dashboard context manager.

    Args:
        job_id: The job ID or submission ID to stop
    """
    cmd = ["ray", "job", "stop", job_id]
    subprocess.check_output(cmd, text=True, timeout=60)
# ---
def test_getitem_out_of_bounds():
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=2, dtype=jnp.float32)

        data = jnp.array([[1.0, 2.0], [3.0, 4.0]])
        builder.append(data)

        with pytest.raises(IndexError):
            builder[2]
# ---
def remove_figure_captions(html: BeautifulSoup):
    # Remove the figure captions since they are not needed
    captions = html.findAll("figcaption", {"class": "ltx_caption"})
    for caption in captions:
        caption.decompose()
# ---
def __init__(self, *args, **kwargs):
        super(UsuarioForm, self).__init__(*args, **kwargs)
        self.fields['primeiro_telefone'].widget.attrs['class'] = 'telefone'
        self.fields['segundo_telefone'].widget.attrs['class'] = 'telefone'
# ---
def fetch_startup_logs(self, tail_lines: int = 100) -> str | None:
        return "(local controller â€” no startup logs)"
# ---
def tokenizer():
    """Create a tokenizer for testing."""
    return AutoTokenizer.from_pretrained("gpt2")
# ---
def openshift_installed():
        ''' check if openshift is installed '''
        import yum

        yum_base = yum.YumBase()
        if yum_base.rpmdb.searchNevra(name='atomic-openshift'):
            return True

        return False
# ---
def _list_jobs(filters: list[str] | None = None) -> list[dict]:
    """Fetch the list of jobs using the Ray CLI."""
    cmd = ["ray", "list", "jobs", "--detail", "--format=json", "--limit=10000"]
    for f in filters or []:
        cmd.extend(["--filter", f])

    result = subprocess.check_output(cmd, text=True, timeout=60)
    try:
        return json.loads(result)
    except json.JSONDecodeError:
        logger.warning(f"Failed to parse JSON output from ray list jobs: {result}")
        return []
# ---
def to_markdown(html, config: HtmlToMarkdownConfig = HtmlToMarkdownConfig()):
    if isinstance(html, str):
        html = BeautifulSoup(html, "html.parser")
    text = MyMarkdownConverter(config).convert_soup(html)
    # cleanup: replace nbsp as space
    # this isn't quite right if we preserve html in places, but we currently are not doing that
    text = text.replace("\xa0", " ")
    return text
# ---
def is_valid_python(source: str) -> bool:
    try:
        ast.parse(source)
        return True
    except SyntaxError:
        return False
# ---
def max_Pos(self) -> Axis:
        return Axis(name="position", size=self.max_length)
# ---
def eqx_max_pool_mean(x):
        pooled = eqx_max_pool(x)
        return pooled.mean()
# ---
def secrets(self):
        '''secret property getter'''
        if self._secrets is None:
            self._secrets = self.get_secrets()
        return self._secrets
# ---
def CurrentLineAndColumn():
  """Returns the 0-based current line and 0-based current column."""
  # See the comment in CurrentColumn about the calculation for the line and
  # column number
  line, column = vim.current.window.cursor
  line -= 1
  return line, column
# ---
def the_object_name_is_contained_in_container_name(name, container_name):
    ifc = an_ifc_file_exists()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    container = ifcopenshell.util.element.get_container(element)
    if not container:
        assert False, f'Object "{name}" is not in any container'
    assert container.Name == container_name, f'Object "{name}" is in {container}'
# ---
def lerp(
    start: Array,
    end: Array,
    weight: Numeric,
):
    return start + weight * (end - start)
# ---
def __exit__(self, exc_type, exc, tb) -> None:
        self.close()
# ---
def parse_partition_name(self, partition):
        try:
            schema, table_partition = partition.split('.')
            table, partition = table_partition.split('/', 1)
            return schema, table, partition
        except ValueError as e:
            raise ValueError('Could not parse ' + partition)
# ---
def list_tasks(self) -> list[TaskInfo]: ...
# ---
def test___cmp__eq(self):
        self._test__cmp__(
            lambda left, right: left == right,
            (
                False,
                True,
                True,
                False,
                True,
                False,
                not PY3,
                False,
                False,
                False,
            ),
            '=='
        )
# ---
def with_gpu(gpu_type: str = "auto", count: int = 1, **kwargs: Any) -> ResourceConfig:
        device = GpuConfig(variant=gpu_type, count=count)
        return ResourceConfig(device=device, **kwargs)
# ---
def test_select_all(self):
        with config.db.connect() as conn:
            res = conn.execute(
                select([text("*")])
                .select_from(self.tables.square)
                .order_by(self.tables.square.c.id)
            ).fetchall()
            eq_(res, [(1, 10, 100, 40), (10, 42, 1764, 168)])
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            df['moving average'] = df.A.rolling(window=5, center=True).mean()
            return df['moving average'].sum()
# ---
def _pyexpr_to_source(expr: PyExpr) -> str | None:
    """Convert a PyExpr e-graph term back to Python source code.

    Uses string representation parsing since egglog's Python API returns
    structured Expr objects that we can inspect via repr().
    """
    s = repr(expr)
    return _parse_pyexpr_repr(s)
# ---
def get_data(indices):
                return get_local_data_for_leaf(indices, leaf_index)
# ---
def get_radio_pair(self):
		dst = self._get_radio()
		src = self._get_radio()

		dst.neighbor = src.addr
		src.neighbor = dst.addr

		return dst, src
# ---
def getTimeFromTZ(self, tz):
		# Assume sanitized zones - as they're pulled from pytz
		# Let's get the timezone list
		tz_list = FuzzySearch.search(tz, pytz.all_timezones, None, 3)
		if not tz_list[0]['Ratio'] == 1:
			# We didn't find a complete match
			return None
		zone = pytz.timezone(tz_list[0]['Item'])
		zone_now = datetime.datetime.now(zone)
		return { "zone" : tz_list[0]['Item'], "time" : zone_now.strftime("%I:%M %p") }
# ---
def model_type(cls) -> Type["MixtralLMHeadModel"]:
        return MixtralLMHeadModel
# ---
def add_widget(self, widget):
        """Add a widget to the list of widgets to do auto-completion for."""
        if widget in self.widgets:
            return # Widget already added

        if isinstance(widget, TextBox):
            self._add_text_box(widget)
            return

        raise ValueError("Widget type %s not supported." % (type(widget)))
# ---

def add(lst):
    """Given a non-empty list of integers lst. add the even elements that are at odd indices..


    Examples:
        add([4, 2, 6, 7]) ==> 2
    """
    return sum([lst[i] for i in range(1, len(lst), 2) if lst[i]%2 == 0])
# ---
def max_len_per_seq(self) -> int:
        return self.page_size * self.pages_per_seq
# ---
def __init__(self, model_name: str, attribute_name: str, *args, **kwargs):
        self.model_name = model_name
        self.attribute_name = attribute_name
# ---
def _retire_overused_rollouts(self):
        """Remove rollouts that exceeded max_samples usage."""
        if self.max_samples < 0:
            return

        for env_name in self.rollout_storage:
            rollouts = self.rollout_storage[env_name]
            # Keep only rollouts under usage limit
            self.rollout_storage[env_name] = [r for r in rollouts if r.usage_count < self.max_samples]
# ---
def __call__(self, text, rules=None):
        return self.output(text, rules)
# ---
def __init__(self, buffer_size=100):
        self._queue = asyncio.Queue(buffer_size)
# ---
def encode_source(self, source: str) -> list[int]:
        """Encode a Python source string to token IDs.

        Returns a list of base vocabulary token IDs (no special tokens).
        """
        return [self.encode_char(c) for c in source]
# ---
def kernel_name(self):
        return self.json['metadata']['kernelspec']['name']
# ---
def JumpToTab( tab_number ):
  """Jump to Vim tab with corresponding number """
  vim.command( 'silent! tabn {0}'.format( tab_number ) )
# ---
def testFormatTag(self):
    """Tests the _FormatTag function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    tag_string = test_helper._FormatTag(None)
    self.assertEqual(tag_string, '-')

    event_tag = events.EventTag()
    event_tag.AddLabel('one')
    event_tag.AddLabel('two')

    tag_string = test_helper._FormatTag(event_tag)
    self.assertEqual(tag_string, 'one two')
# ---
def _get_shared_executor() -> ThreadPoolExecutor:
    global _shared_executor
    if _shared_executor is None:
        _shared_executor = ThreadPoolExecutor(max_workers=32)
    return _shared_executor
# ---
def poke(self, context):
        '''
        Function that the sensors defined while deriving this class should
        override.
        '''
        raise AirflowException('Override me.')
# ---
def can_retry_preemption(self) -> bool:
        return self.preemption_count < self.max_retries_preemption
# ---
def tokenize_dolmino_subset(name: str, tokenizer: str | None = None) -> ExecutorStep[TokenizeConfig]:
    """Get a specific dolmino split tokenization step."""
    assert name in DOLMINO_DATASETS, f"Split {name} not found in DOLMINO_DATASETS"
    return tokenize_dolmino(tokenizer=tokenizer)[f"dolmino/{name}"]
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "!"
# ---
def test_add_dicts(self):
        """Merge two dicts of values together"""
        a = {
            "a": 1,
            "b": 2,
        }
        b = {
            "a": 3,
            "c": 4,
        }
        ret = add_dicts(a, b)
        self.assertEqual(
            ret,
            {
                "a": 4,
                "b": 2,
                "c": 4,
            },
        )
# ---
def get_stub(self, address: str) -> WorkerClient:
        """Get a worker stub for the given address.

        Args:
            address: Worker address in "host:port" format

        Returns:
            A WorkerClient stub for making RPC calls
        """
        ...
# ---
def wait_for_job(
        self,
        job_id: JobName,
        timeout: float = 300.0,
        poll_interval: float = 2.0,
    ) -> cluster_pb2.JobStatus:
        return self._remote_client.wait_for_job(job_id, timeout=timeout, poll_interval=poll_interval)
# ---
def _use_import_item_table(self, job_id):
        """
            Set the resource and the table to being s3_import_item
        """

        if self.item_resource == None:
            from s3resource import S3Resource
            (prefix, name) = S3ImportJob.ITEM_TABLE_NAME.split("_",1)
            self.item_resource = S3Resource(prefix, name)
        self.resource = self.item_resource
        self.tablename = S3ImportJob.ITEM_TABLE_NAME
        self.table = S3ImportJob.define_item_table()
# ---
def make_stop_wrapper(vm_id, orig_stop):
                    def stop_wrapper():
                        stop_called.append(vm_id)
                        return orig_stop()

                    return stop_wrapper
# ---
def map_jpg():
    return make_map(request, format='jpg')
# ---
def rollout_group_to_dict(group: "RolloutGroup") -> dict[str, Any]:
    """Convert a RolloutGroup to a JSON-serializable dictionary.

    Uses tree mapping to automatically handle all fields including arrays,
    making it robust to schema changes.
    """
    return dataclasses.asdict(
        jax.tree.map(lambda v: _to_list(v) if isinstance(v, list | jax.Array | numpy.ndarray) else v, group)
    )
# ---
def cursorAtSpaceEnd(block):
            cursor = QTextCursor(block)
            cursor.setPosition(block.position() + len(blockIndentation(block)))
            return cursor
# ---
def test_axis_shapes_force_data_when_other_absorber():
    cfg = MeshConfig(axes={"model": -1})
    ici, _ = cfg.axis_shapes(num_devices=4, num_slices=1)
    # data forced to 1 to keep single absorber
    assert ici["data"] == 1
    # model absorbs the rest
    assert ici["model"] == 4
# ---
def _maybe_force_tokenizer_parallelism(tokenizer: PreTrainedTokenizerBase):
    if tokenizer.is_fast and os.getenv("TOKENIZERS_PARALLELISM") is None:
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
# ---
def LineTextInCurrentBuffer( line_number ):
  """ Returns the text on the 1-indexed line (NOT 0-indexed) """
  return vim.current.buffer[ line_number - 1 ]
# ---
def clip(array: NamedOrNumeric, a_min: NamedOrNumeric, a_max: NamedOrNumeric) -> NamedArray:
    """Like jnp.clip, but with named axes. This version currently only accepts the three argument form."""
    (array, a_min, a_max), axes = broadcast_arrays_and_return_axes(array, a_min, a_max)
    array = raw_array_or_scalar(array)
    a_min = raw_array_or_scalar(a_min)
    a_max = raw_array_or_scalar(a_max)

    return NamedArray(jnp.clip(array, a_min, a_max), axes)
# ---
def __init__(self, client: ActorClient, method_name: str):
        self._client = client
        self._method_name = method_name
# ---
def load_store(cls, store, decoder=None):
        """Create a new dataset from the contents of a backends.*DataStore
        object
        """
        variables, attributes = store.load()
        if decoder:
            variables, attributes = decoder(variables, attributes)
        obj = cls(variables, attrs=attributes)
        obj._file_obj = store
        return obj
# ---
def init(Vocab: Axis, config: LlamaConfig, *, key) -> "LlamaEmbedding":
        token_embeddings = hnn.Embedding.init(Vocab, config.Embed, key=key)
        norm = None
        if config.input_embedding_norm:
            norm = config.mk_LayerNorm(config.Embed)
        return LlamaEmbedding(token_embeddings, norm)
# ---
def fn(x):
        x = hax.shard(x)
        visualize_shardings(x)
        return x
# ---
def serve(self):
        if not self.server:
            console.print("[red]No model loaded. Use 'load' command first[/red]")
            return

        console.print("[blue]Starting inference server...[/blue]")
        self.server.serve()
# ---
def __init__(self, minimum_balance):
        BankAccount.__init__(self)
        self.minimum_balance = minimum_balance
# ---
def test_sigv4(self):
        CliRunner().invoke(rubberjack, ['--sigv4-host', 'foo', 'deploy'], catch_exceptions=False)
# ---
def from_ref(ref: ray.ObjectRef, name: str) -> "RayJobInfo":
        return RayJobInfo(ref=ref, submission_id=None, name=name)
# ---
def SearchRecord(x):
	print(x)
	if (x in student_phoneNumber_name) :
		return student_phoneNumber_name[x]

	return False
# ---
def invert(a: A) -> A:
    return wrap_elemwise_unary(jnp.invert, a)
# ---
def __init__(self, vocab_size, **kwargs):
        self._vocab = {i: i for i in range(vocab_size)}
        self._vocab_size = vocab_size
        super().__init__(**kwargs)
# ---
def session(self):
        return self.req.environ.get('beaker.session') if self.req is not None else None
# ---
def test_int(self):
        """Store and retrieve an int"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "num": 1})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["num"], 1)
# ---
def size(self) -> int:
        return len(self._resolve().endpoints)
# ---
def unregister(self, endpoint_id: str) -> None:
        """Unregister a previously registered endpoint.

        Args:
            endpoint_id: ID returned from register()
        """
        ...
# ---
def _maybe_add_ray_verbose(ctx: Context, cmd_args: list[str]) -> list[str]:
    """Add `-v` to Ray CLI commands when cluster.py verbose mode is enabled.

    Most Ray CLI invocations here are of the form `["ray", "<subcommand>", ...]`.
    We insert `-v` after the subcommand (e.g. `ray up -v ...`) since Ray exposes per-subcommand verbose flags.
    """
    if ctx.verbose:
        if len(cmd_args) < 2:
            return cmd_args
        return [*cmd_args[:2], "-v", *cmd_args[2:]]
    return cmd_args
# ---
def _recv(self, addr, bindata, frequency, bandwidth):
		if self.frequency == frequency and self.bandwidth == bandwidth and self.addr == addr:
			self.q.put(bindata)
# ---
def default_combiner(left, right):
                if left is None or right is None:
                    raise ValueError(
                        "Default combiner requires both left and right items (use custom combiner for outer joins)"
                    )
                return {**left, **right}
# ---
def call(*args, **kwargs):
            endpoint = self._pool._get_next_endpoint()
            return self._pool._call_endpoint(endpoint, method_name, args, kwargs)
# ---
def test_full_flexibility():
    partial_order = (...,)
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
# ---
def preprocess_request(req: dict[str, Any]) -> dict[str, Any]:
    """Remove metadata fields from request."""
    return {k: v for k, v in req.items() if not k.startswith("_")}
# ---
def test_einsum_ordered_ellipsis():
    Height = Axis("Height", 2)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)

    m1 = hax.ones((Height, Width, Depth))
    m2 = hax.ones((Depth, Width, Height))

    assert jnp.all(
        jnp.equal(
            einsum("h ... d,d ... h-> ...", m1, m2).array,
            jnp.einsum("ijk,kji->j", m1.array, m2.array),
        )
    )
# ---
def Vocab(self) -> Axis:
        return self._Vocab
# ---
def unembed_active_scale(self):
        """Scaling factor applied when unembedding embeddings."""
        raise NotImplementedError
# ---
def compute_loss(
        m,
        example: AudioTextExample,
        *,
        key=None,
        reduction: Optional[hax.ReductionFunction] = cast(Optional[hax.ReductionFunction], hax.mean),
        reduction_axis: Optional[hax.AxisSelection] = None,
    ) -> jax.numpy.ndarray | hax.NamedArray:
        return m.compute_loss(example, key=key, reduction=reduction, reduction_axis=reduction_axis)
# ---
def unbind(self):
        return True
# ---
def showProfileGUI(self, widget):
        """Show profile Dialog to add a new one"""

        orca_gui_profile.showProfileUI(self)
# ---
def __init__(
        self,
        *,
        callable_bytes: bytes | None = None,
        command: list[str] | None = None,
    ):
        has_callable = callable_bytes is not None
        has_command = command is not None
        if has_callable == has_command:
            raise ValueError("Exactly one of 'callable_bytes' or 'command' must be set")
        self._callable_bytes = callable_bytes
        self.command = command
# ---
def the_object_name_is_not_a_void(name):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(element.VoidsElements):
        assert False, "A void was found"
# ---
def __init__(self, axes: Sequence[Axis]):
        self.axes = tuple(axes)
# ---
def list_services() -> list[ServiceInfo]:
    """List all registered services."""
    register_services()
    return list(SERVICES.values())
# ---
def large_document_dataset():
    """Generate 500 documents: 100 unique content values, each appears 5 times."""
    docs = []
    for content_id in range(100):
        for copy_id in range(5):
            docs.append(
                {
                    "id": content_id * 5 + copy_id,
                    "content": f"document_{content_id}",
                    "value": content_id * 1000 + copy_id,
                }
            )
    return docs
# ---
def safe_name(self) -> str:
        return self.name.replace("/", "-").lower()
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "x": 1, "y": 2, "z": "z1"},
                {"id": 2, "x": 2, "y": 3, "z": "z2"},
                {"id": 3, "x": 3, "y": 4, "z": "z3"},
                {"id": 4, "x": 4, "y": 5, "z": "z4"},
            ],
        )
# ---
def list_endpoints(self, request: cluster__pb2.Controller.ListEndpointsRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListEndpointsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def _compute_hash(self, path: Path) -> str:
        h = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(65536), b""):
                h.update(chunk)
        return h.hexdigest()
# ---
def num_chunks(self) -> int:
        """Total number of chunks across all shards."""
        return len(self.source_items)
# ---
def glu(x: NamedArray, axis: Axis) -> NamedArray:
    axis_index = x.axes.index(axis)
    return NamedArray(jnn.glu(x.array, axis_index), x.axes)
# ---
def stop(self) -> None: ...
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)

        parts = pattern.split('..', 1)
        if len(parts) == 1:
            # No range.
            self.point = self._convert(parts[0])
            self.rangemin = None
            self.rangemax = None
        else:
            # One- or two-sided range.
            self.point = None
            self.rangemin = self._convert(parts[0])
            self.rangemax = self._convert(parts[1])
# ---
def temp_bundle_dir():
    """Create a temporary directory for bundle storage testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)
# ---
def microbatch_size(self) -> int | None:
        if self.per_device_parallelism < 0:
            return None
        return self.per_device_parallelism * self.data_axis_size
# ---
def test_shard_map_decorator_usage():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    @hax.shard_map(mesh=mesh, check_rep=False)
    def fn(x):
        return x + 5

    x = hax.ones(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), mesh:
        out = fn(x)

    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array + 5)
# ---
def __lt__(self, other):
        if other is None:
            return False
        return self.id < other.id
# ---
def close(self): pass
# ---
def getUserIdsWorker(login, pwd, qin, qout):
	while True:
		url = qin.get()
		if url == None:
			break
		qin.task_done()
		try:
			dom = getAtomFeed(url, login, pwd)
		except:
			continue
		userIds = dom.getElementsByTagName('snx:userid')
		for index, item, in enumerate(userIds):
			qout.put(item.firstChild.data)
# ---
def test_write_vortex_basic(self, tmp_path):
        """Test basic vortex file writing."""
        records = [{"id": i, "value": i * 2} for i in range(10)]
        output_path = tmp_path / "output.vortex"

        result = write_vortex_file(records, str(output_path))

        assert result["count"] == 10
        assert output_path.exists()

        # Verify roundtrip
        loaded = list(load_vortex(str(output_path)))
        assert loaded == records
# ---
def __init__(
            self,
            poke_interval=60,
            timeout=60*60*24*7,
            soft_fail=False,
            *args, **kwargs):
        super(BaseSensorOperator, self).__init__(*args, **kwargs)
        self.poke_interval = poke_interval
        self.soft_fail = soft_fail
        self.timeout = timeout
# ---
def close_db(error):
    """Closes connection with Mongo client"""
    if hasattr(g, 'mongo_client'):
        g.mongo_client.close()
# ---
def __call__(
        self,
        params: PyTree,
        tokens: jax.Array,
        cfg: GrugConfigLike,
        *,
        mask: AttentionMask | jax.Array | None = None,
    ) -> jax.Array: ...
# ---
def handler(signum, frame):
            sys.exit(0)
# ---
def photoloop():
	count = 0
	while (count < 9):
		sleep(0.5)
		image1 =  newphoto()
		if lastfilter is not "none":
			dofilter(lastfilter,image1)
		count = count + 1
# ---
def _getN(self, n):
        n -= 1
        p = self.head
        while n:
            p = p.next
            n -= 1
        return p
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetWeekdayRange(0), (4, 11))
    self.assertEqual(self.schedule.GetWeekdayRange(1), (12, 19))
    self.assertEqual(self.schedule.GetWeekdayRange(2), (20, 27))
    self.assertEqual(self.schedule.GetWeekdayRange(3), (28, 37))
    self.assertEqual(self.schedule.GetWeekdayRange(4), (38, 47))
    self.assertEqual(self.schedule.GetWeekdayRange(5), (48, 57))
# ---
def convert_to_cache(self, value, record, validate=True):
        return bool(value)
# ---
def test_urlwith_fragment(self):
        req = Request("http://www.python.org/?qs=query#fragment=true")
        self.assertEqual("/?qs=query", req.get_selector())
        req = Request("http://www.python.org/#fun=true")
        self.assertEqual("/", req.get_selector())
# ---
def test_is_position_token_false_for_base(tok):
    base_token = tok.encode_char("a")
    assert not tok.is_position_token(base_token)
# ---
def test_moe_linear_out_first_property():
    E, In, Out = hax.make_axes(E=2, In=4, Out=3)
    moe = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(0), out_first=True)
    assert moe.out_first
    assert moe.weight.axes[:3] == (E, Out, In)

    moe2 = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(1), out_first=False)
    assert not moe2.out_first
    assert moe2.weight.axes[:3] == (E, In, Out)
# ---
def add_dims_to_spec(_, qss, sds):
                if partition_grads_into_blocks:
                    qss = jax.tree.map(lambda qs: PartitionSpec(*((None,) + qs)), qss)
                if sds is not None:
                    qss = jax.tree.map(lambda qs: PartitionSpec(*(sds + qs)), qss)
                return qss
# ---
def test_get_stats_invalid_container(docker_runtime):
    """Test that get_stats returns available=False for invalid container ID."""
    invalid_container_id = "nonexistent_container_12345"

    stats = docker_runtime.get_stats(invalid_container_id)

    assert stats.available is False
# ---
def _normalize(name):
    '''Transform "Firstname [Middlenames] Lastname" into
    "Lastname, Firstname [Middlenames]".'''
    split = name.split()
    if len(split) == 1:
        return name
    return split[-1] + ', ' + ' '.join(name[0:-1])
# ---
def test_quantile_parallel_float_nan(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float32)})
            df.A[0:100] = np.nan
            df.A[200:331] = np.nan
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def _make_serializable(obj):
            """Recursively convert non-serializable objects (like Enums) to serializable forms."""
            if isinstance(obj, Enum):
                return obj.name
            elif isinstance(obj, dict):
                return {k: _make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [_make_serializable(v) for v in obj]
            return obj
# ---
def __exit__(self, exc_type, exc, tb):
        jax_config.abstract_mesh_context_manager.set_local(self._prev)
        return False
# ---
def output_footnote(self):
        self.inline._in_footnote = True
        body = self.renderer.placeholder()
        key = self.token['key']
        while self.pop()['type'] != 'footnote_end':
            body += self.tok()
        self.footnotes.append({'key': key, 'text': body})
        self.inline._in_footnote = False
        return self.renderer.placeholder()
# ---
def setup(self):
        IfcStore.purge()
        bpy.ops.wm.read_homefile(app_template="")
        if bpy.data.objects:
            bpy.data.batch_remove(bpy.data.objects)
            bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)
        blenderbim.bim.handler.setDefaultProperties(None)
# ---
def num_devices_per_slice(self):
        """number of devices within a slice"""
        return jax.device_count() // self.num_slices
# ---
def drain_actor_pool(self) -> None:
        logger.info(f"{self.get_actor_pool_name()} actor pool members draining.")
        self._remove_members_from_actor_pool(0)
        logger.info(f"{self.get_actor_pool_name()} actor pool drained.")
# ---
def SetLocationList( diagnostics ):
  """Populate the location list with diagnostics. Diagnostics should be in
  qflist format; see ":h setqflist" for details."""
  vim.eval( 'setloclist( 0, {0} )'.format( json.dumps( diagnostics ) ) )
# ---
def setUp(self):
        self._mock_multiplexer = mock.create_autospec(
            plugin_event_multiplexer.EventMultiplexer
        )
        self._mock_tb_context = base_plugin.TBContext(
            multiplexer=self._mock_multiplexer
        )
# ---
def LoadWF(self, waveform, fs):
        self.s = waveform
        self.fs = fs
        self.sLength, self.nChans = self.s.shape
# ---
def get_cluster(self, word):
        """
        Returns the cluster number for a word in the vocabulary
        """
        idx = self.ix(word)
        return self.clusters[idx]
# ---
def std(x, axis=0):
    N = asarray(x).shape[axis]
    return _Nstd(x, axis)*sqrt(N/(N-1.))
# ---
def bitwise_xor(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_xor](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_xor.html)
    """
    return jnp.bitwise_xor(x1, x2)
# ---
def init_fn(params):
        return None
# ---
def embed(self, input_ids, *, key):
        input_embeds = self.token_embeddings(input_ids)
        input_Pos = input_ids.resolve_axis("position")
        position_embeds = self.position_embeddings.embed(hax.arange(input_Pos))
        x = input_embeds + position_embeds

        return x
# ---
def mutagen(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'mutagenesis',
        'purpose': 'repression',
        'method': 'mutagen treatment'
    }
# ---
def main(cfg: DownloadConfig) -> None:
    """Download HuggingFace dataset."""
    download_hf(cfg)
# ---
def __init__(self, fs: AbstractFileSystem):
        super().__init__()
        self._fs = fs
        import concurrent.futures

        self._executor = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENT_CHUNKS)
# ---
def __call__(self, x, *, key):
            return x + self.array + self.static + hax.random.normal(key, x.axes)
# ---
def checkWall(self, wall):
        if wall.state == "hidden":
            if (self.x >= wall.x - self.d and
                    (self.x + 32 <= wall.x + 32 + self.d)):
                return False
# ---
def validate_with_braceexpand(braceexpand_paths, paths):
    be_paths = []
    for be_path in braceexpand_paths:
        be_paths.extend(list(braceexpand(be_path)))

    if set(be_paths) != set(paths):
        print("Braceexpand paths are not equal to the original paths")
        print("Braceexpand paths:")
        print(be_paths)
        print("Original paths:")
        print(paths)
# ---
def addDirectory( self, path, force = False, rpc = '', url = '', timeout = None ):
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.addDirectory( path, force )
# ---
def test_spawn_empty_dns(self):
        """Test spawning with an empty dns list"""
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_VHD, None, None,
                         os_type="linux", architecture="x86-64",
                         empty_dns=True)
        self.check_vm_params_for_linux()
# ---
def unique_counts(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]:
    """Shortcut for :func:`unique` that also returns counts."""

    values, counts = typing.cast(
        tuple[NamedArray, NamedArray],
        unique(
            array,
            Unique,
            return_counts=True,
            axis=axis,
            fill_value=fill_value,
        ),
    )
    return values, counts
# ---
def setUp(self):
        with mock.patch(
            'airflow.providers.google.cloud.hooks.vision.CloudVisionHook.__init__',
            new=mock_base_gcp_hook_default_project_id,
        ):
            self.hook = CloudVisionHook(gcp_conn_id='test')
# ---
def col(name: str) -> ColumnExpr:
    """Create a column reference expression.

    Example:
        >>> col("score") > 0.5
        (col('score') > lit(0.5))
    """
    return ColumnExpr(name)
# ---
def last_two_digits(year):
    return year - ((year // 100) * 100)
# ---
def _default_destination_address(self, cr, uid, context=None):
        user = self.pool.get('res.users').browse(cr, uid, uid, context=context)
        return user.company_id.partner_id.id
# ---
def update(self, request, *args, **kwargs):
        raise MethodNotAllowed(self.action)
# ---
def is_finite(self) -> bool:
        return self.dataset.has_len()
# ---
def check_status(self):
        if self._process and self._process.poll() is not None:
            print(f"Remote process exited with code {self._process.returncode}")
# ---
def slice(
    array: NamedArray,
    axis: AxisSelector,
    new_axis: AxisSelector | None = None,
    start: int = 0,
    length: int | None = None,
) -> NamedArray:
    pass
# ---
def test_actor_options_default_preemptible():
    from fray.v2.ray_backend.backend import _actor_ray_options

    options = _actor_ray_options(ResourceConfig())
    assert options["num_cpus"] == 1
    assert "resources" not in options
# ---
def device_username(self):
        if self._values['device_username'] is None:
            return None
        return self._values['device_username']
# ---
def connect(self):
        # Open either an SSL or regular NNTP connection.
        try:
            if ( self.ssl ):
                self.connection = NNTP_SSL(self.server, self.port, self.username, self.password, False, True, timeout=15)
            else:
                self.connection = NNTP(self.server, self.port, self.username, self.password, False, True, timeout=15)
        except:
            pass

        if ( self.connection ): return True
        return False
# ---
def kill(self, container_id: str, force: bool = False) -> None:
        del force  # Local containers don't distinguish force vs graceful
        if container_id in self._containers:
            self._containers[container_id].kill()
# ---
def authenticated_userid(self, request):
		return self.match(request).authenticated_userid(request)
# ---
def reload_model(self, model: LmHeadModel | None, state_dict: dict) -> None:
        # Serialize numpy arrays to (bytes, dtype, shape) tuples to survive RPC serialization.
        # vLLM's collective_rpc can corrupt numpy arrays during pickling.
        serialized_state_dict = serialize_state_dict_for_rpc(state_dict)
        self.llm.update_weights(serialized_state_dict, self.model_name)
        self.llm.reset_prefix_cache()
# ---
def __init__(self, node, emailMessage, references=list(), children=dict(), slotted=bool("false")):
        self.node = node
        self.children = dict(children)
        self.references = references[:]
        self.slotted = slotted
        self.emailMessage = emailMessage
# ---
def _generate_permutation_params(self, rng):
        length = self.length

        if length == 1:
            return 1, 0

        while True:
            a = rng.integers(1, length)
            if np.gcd(a, length) == 1:
                break

        b = rng.integers(0, length)  # b can be in [0, length-1]
        return a, b
# ---
def test_job_expands_to_correct_number_of_tasks(make_job_request):
    """expand_job_to_tasks creates correct number of tasks based on replicas."""
    request = make_job_request()
    request.replicas = 3
    job = ControllerJob(job_id=JobName.root("test-job"), request=request)

    tasks = expand_job_to_tasks(job)

    assert len(tasks) == 3
    for i, task in enumerate(tasks):
        assert task.task_index == i
        assert task.job_id == job.job_id
# ---
def exists_volume_mount(self, volume_mount):
        ''' return whether a volume mount exists '''
        exist_volume_mounts = self.get_volume_mounts()

        if not exist_volume_mounts:
            return False

        volume_mount_found = False
        for exist_volume_mount in exist_volume_mounts:
            if exist_volume_mount['name'] == volume_mount['name']:
                volume_mount_found = True
                break

        return volume_mount_found
# ---
def _wait_for_process(self):
        if self._proc.returncode is not None:
            return
        logging.debug("Job %r waiting for virt-v2v process", self._id)
        if not self._proc.wait(timeout=self.PROC_WAIT_TIMEOUT):
            raise V2VProcessError("Job %r timeout waiting for process pid=%s",
                                  self._id, self._proc.pids)
# ---
def __and__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_and(self, other)
# ---
def test_fold_doesnt_reduce_scalars():
    Height = Axis("Height", 10)
    named1 = hax.random.uniform(PRNGKey(0), (Height,))

    acc = hax.zeros((Height,))

    total = hax.fold(lambda x, z, y: x + z * y, Height)(acc, 4.0, named1)

    assert jnp.all(jnp.isclose(total.rearrange(acc.axes).array, jnp.sum(named1.array * 4.0)))
# ---
def remain_alnum(text):
    """Remain digits and English letters of a string.
    """
    return ''.join(c for c in text if c.isalnum()
                                   and ord(' ') <= ord(c) <= ord('z'))
# ---
def wrap_elemwise_unary(f, a, *args, **kwargs):
    if isinstance(a, NamedArray):
        return NamedArray(f(a.array, *args, **kwargs), a.axes)
    else:
        return f(a, *args, **kwargs)
# ---
def current_tracker() -> "Tracker": ...
# ---
def patch_sqlalchemy():
    mysqldb_hooks.install_patches()
    try:
        yield
    finally:
        mysqldb_hooks.reset_patches()
# ---
def get_version(module, git_path, dest, ref="HEAD"):
    ''' samples the version of the git repo '''

    cmd = "%s rev-parse %s" % (git_path, ref)
    rc, stdout, stderr = module.run_command(cmd, cwd=dest)
    sha = to_native(stdout).rstrip('\n')
    return sha
# ---
def update_router_precommit(self, context, router_context):
        pass
# ---
def _signal_handler(signum, frame):
        raise KeyboardInterrupt
# ---
def __init__(self, df, lmbda = 0):
        pass
# ---
def test_intermingling(self):
        with self.getcapture() as cap:
            oswritebytes(1, "1")
            sys.stdout.write(str(2))
            sys.stdout.flush()
            oswritebytes(1, "3")
            oswritebytes(2, "a")
            sys.stderr.write("b")
            sys.stderr.flush()
            oswritebytes(2, "c")
            out, err = cap.readouterr()
        assert out == "123"
        assert err == "abc"
# ---
def is_command(self) -> bool:
        return self.command is not None
# ---
def test_limit_complete(self):
        """A limit with item_capacity = 0 is 'complete'"""
        limit = Limit(item_limit=0)
        self.assertTrue(limit.complete)
# ---
def heartbeat(self) -> str:
        return f"TPU allocation '{self.tpu_name}' active for {self.username}"
# ---
def _shard_filename(output_path: str, shard_idx: int) -> str:
    return os.path.join(output_path, f"shard_{shard_idx:05d}.jsonl.gz")
# ---
def delete_pki_dir(self):
        '''
        Delete the private key directory
        '''
        path = self.opts['pki_dir']
        if os.path.exists(path):
            shutil.rmtree(path)
# ---
def test_impl(A):
            df = pd.DataFrame({'A': A})
            return df.A.quantile(.25)
# ---
def clean_setup_provider(request, provider):
    BaseProvider.clear_providers()
    setup_or_skip(request, provider)
    yield
    BaseProvider.clear_providers()
# ---
def format_ambients(ambients):
    return format_line(prefix='ambients'.rjust(RJUST), values=ambients)
# ---
def permute_sharded(x_flat_: Array, topk_idx_flat_: Array):
            sort_idx_ = jnp.argsort(topk_idx_flat_, axis=-1)
            x_repeat_sort_ = jnp.take(x_flat_, sort_idx_ // self.config.num_experts_per_tok, axis=0)
            group_sizes_ = jnp.bincount(topk_idx_flat_, length=self.config.n_routed_experts)

            return x_repeat_sort_, group_sizes_, sort_idx_
# ---
def testMatMul_OutEmpty_B(self):
    n, k, m = 3, 8, 0
    x = self._randMatrix(n, k, np.float32)
    y = self._randMatrix(k, m, np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def output_newline(self):
        return self.renderer.newline()
# ---
def default_window_title(self):
		"""ã‚¹ã‚ºãƒ¡ãƒãƒã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¿ã‚¤ãƒˆãƒ«(ãƒ©ãƒ™ãƒ«)ã‚’å¿œç­”ã™ã‚‹ã€‚"""
		if TRACE: print(__name__), self.default_window_title.__doc__

		return "Wasp"
# ---
def get(self):
        self.handle_request('GET')
# ---
def from_ms(cls, milliseconds: int) -> "Duration":
        """Create duration from milliseconds."""
        return cls(milliseconds)
# ---
def read(self, oldstr):
        self.time, newstr = get_variable_length_number(oldstr)
        return self.time, newstr
# ---
def get_job(self, job_id: JobName) -> ControllerJob | None:
        with self._lock:
            return self._jobs.get(job_id)
# ---
def rec_fn(x, *rest):
        if isinstance(x, haliax.nn.Stacked):
            new_inner = haliax.vmap(mapped_fn, x.Block)(x.stacked, *[r.stacked for r in rest])
            return dataclasses.replace(x, stacked=new_inner)  # type: ignore
        else:
            return fn(x, *rest)
# ---
def _assert_all_in_dataset(self, names, virtual_okay=False):
        bad_names = set(names) - set(self._variables)
        if virtual_okay:
            bad_names -= self.virtual_variables
        if bad_names:
            raise ValueError('One or more of the specified variables '
                             'cannot be found in this dataset')
# ---
def activate(self, callingWindow, fullContext, mainItem, i):
        fitID = self.mainFrame.getActiveFit()
        Fit.getInstance().setAsPattern(fitID, mainItem)
        wx.PostEvent(self.mainFrame, GE.FitChanged(fitIDs=(fitID,)))
# ---
def _init_weight(key: PRNGKeyArray, shape: tuple[int, ...], std: float) -> Float[Array, "..."]:
    return std * random.truncated_normal(key, -3, 3, shape)
# ---
def __init__(self, prev):
        self.prev = prev  # ContentOfGroup or CharClass
        self.q = WrappedCharClass()

        # forward function
        self.add = self.q.add

        self.next_is_range = False
        self.empty = True
        self.can_mutate = True
# ---
def weights():
    return {"ds1": 0.5, "ds2": 0.3, "ds3": 0.2}
# ---
def test_str_split(self):
        def test_impl(df):
            return df.A.str.split(',')

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(hpat_func(df), test_impl(df), check_names=False)
# ---
def trace_with_result_func(a, i=10):
    return (a, i)
# ---
def allow_more(self, current_count: int) -> bool:
        """Return True if we should store another sample for the benchmark."""
        if not self.should_log():
            return False
        if self.log_all:
            return True
        assert self.max_samples_per_benchmark is not None
        return current_count < self.max_samples_per_benchmark
# ---
def test_to_obj_list():
    msg = '["hoge", "hogi", {"atr1": "val2", "atr2": 1.0}]'
    bb = jps.utils.to_obj(msg)
    assert len(bb) == 2
    assert bb[0] == 'hoge'
    assert bb[1] == 'hogi'
    assert bb[2].atr1 == 'val2'
    assert bb[2].atr2 == 1.0
# ---
def setMaterial(self, material):
        self.__material = material
# ---
def get_size(self):
        """Return the current size of the window.

        The window size does not include the border or title bar.

        :rtype: (int, int)
        :return: The width and height of the window, in pixels.
        """
        raise NotImplementedError('abstract')
# ---
def display_name(self):
        """
        Find the most appropriate display name for a user: look for a "display_name", then
        a "real_name", and finally fall back to the always-present "name".
        """
        for k in self._NAME_KEYS:
            if self._raw.get(k):
                return self._raw[k]
            if "profile" in self._raw and self._raw["profile"].get(k):
                return self._raw["profile"][k]
        return self._raw["name"]
# ---
def init_and_merge(state, *args, **kwargs):
        init_state = init_fn(*args, **kwargs)
        # remove all ShapeDTypeStructs from the state
        state = equinox.filter(state, lambda x: not isinstance(x, jax.ShapeDtypeStruct))
        return equinox.combine(state, init_state)
# ---
def choosers_dm(choosers, spec):
    return eval_variables(spec.index, choosers)
# ---
def test_accelerator_type_name_handles_unknown():
    """Unknown accelerator types are marked clearly."""
    assert accelerator_type_name(999).startswith("UNKNOWN(")
# ---
def to_arrow(self, message):
        raise NotImplementedError("Not implemented.")
# ---
def decorator(fn):
        if split not in SNAPSHOT_PATHS:
            raise ValueError(f"Invalid split: {split}")

        input_path = SNAPSHOT_PATHS[split]["inputs"]

        files = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(input_path) if f.endswith(ext)]
        return pytest.mark.parametrize("input_name", files)(fn)
# ---
def rpc_post(client: TestClient, method: str, body: dict | None = None):
    """Helper to call RPC endpoint and return JSON response."""
    resp = client.post(
        f"/iris.cluster.ControllerService/{method}",
        json=body or {},
        headers={"Content-Type": "application/json"},
    )
    assert resp.status_code == 200, f"RPC {method} failed: {resp.text}"
    return resp.json()
# ---
def logical_or(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logical_or(x1, x2)
# ---
def _checkpoint_leaf(x):
        if is_jax_array_like(x):
            return checkpoint_name(x, name)
        else:
            return x
# ---
def get_file_details_dir(filename):
  return filename[filename.rindex("/")+1:]
# ---
def test_subscribe(self):
        """Can subscribe and unsubscribe from hooks"""
        hook = lambda: None
        self.dynamo.subscribe("precall", hook)
        self.assertEqual(len(self.dynamo._hooks["precall"]), 1)
        self.dynamo.unsubscribe("precall", hook)
        self.assertEqual(len(self.dynamo._hooks["precall"]), 0)
# ---
def __init__(self, info: vm_pb2.VmInfo):
        # Don't call super().__init__ - just set the minimal attributes
        self.info = info
        self._log_lines: list[str] = []
# ---
def get_windows(self):
            """Get the windows currently attached to this display.

            :rtype: sequence of `Window`
            """
            raise NotImplementedError('deprecated')
# ---
def create_test_rollout_storage_config() -> RolloutStorageConfig:
    """Create in-memory storage config for testing."""
    from marin.rl.rollout_storage import StorageType

    test_id = uuid.uuid4().hex[:8]
    return RolloutStorageConfig(storage_type=StorageType.IN_MEMORY, queue_name=f"test_{test_id}")
# ---
def open_shard_at_row(self, shard_name: str, row: int):
        return self.docs[int(shard_name)][row:]
# ---
def _get_task(self) -> PendingTask | None:
        """Try to get a task from the queue."""
        try:
            return self._task_queue.get(timeout=0.5)
        except Empty:
            return None
# ---
def is_status_updated(self, process, previous_status):
        if process.updated_pending_status:
            return True
        if process.status != previous_status['status']:
            return True
        if (process.connection_status !=
                previous_status['ipsec_site_connections']):
            return True
# ---
def __init__(self, transforms, num_cpus, num_gpus, resources):
        self.transforms = transforms
        self._num_cpus = num_cpus
        self._num_gpus = num_gpus
        self._resources = resources
# ---
def _description_domain(self, env):
        return self.domain(env[self.model_name]) if callable(self.domain) else self.domain
# ---
def address(self) -> str:
        return "fake-host"
# ---
def dict_to_querystring(objs):
    return "&".join(["%s=%s" % (k, v)
                     for k, v in objs.items()
                     if v is not None])
# ---
def register_options_page(page_class):
    _pages.register(page_class.__module__, page_class)
# ---
def parse_key(key, sep='.'):
        '''parse the key allowing the appropriate separator'''
        common_separators = list(Yedit.com_sep - set([sep]))
        return re.findall(Yedit.re_key.format(''.join(common_separators)), key)
# ---
def _make_dataset(key: jax.Array, *, n_points: int = 2048) -> tuple[NamedArray, NamedArray]:
    data_axis = Axis("data", n_points)
    feature_axis = Axis("in", 2)
    out_axis = Axis("out", 1)

    xy = jrandom.uniform(key, (n_points, 2), minval=-1.0, maxval=1.0)
    inputs = hax.named(xy, (data_axis, feature_axis))
    targets = hax.named(xy[:, :1], (data_axis, out_axis))
    return inputs, targets
# ---
def _set_tunnel(self, host, port=None, headers=None):
        self._tunnel_host = host
        self._tunnel_port = port
        if headers:
            self._tunnel_headers = headers
        else:
            self._tunnel_headers.clear()
# ---
def lambda_fn(x):
      return math_ops.matmul(x[0], x[1])
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.std()
# ---
def test_node_source_span():
    source = "x = 1 + 2\n"
    tree = ast.parse(source)
    # The BinOp node should span "1 + 2".
    assign = tree.body[0]
    binop = assign.value
    span = _node_source_span(source, binop)
    assert span is not None
    start, end = span
    assert source[start:end] == "1 + 2"
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        """Map model parameter names to HF parameter names"""
        return {
            "transformer": "model",
            "embeddings": "model",
            "lm_head": "lm_head",
        }
# ---
def selu(a: A) -> A:
    return wrap_elemwise_unary(jnn.selu, a)
# ---
def GetVimCommand( user_command, default = 'edit' ):
  vim_command = BUFFER_COMMAND_MAP.get( user_command, default )
  if vim_command == 'edit' and not BufferIsUsable( vim.current.buffer ):
    vim_command = 'split'
  return vim_command
# ---
def test_brackets_balanced_simple():
    assert brackets_balanced("(a + b)")
    assert brackets_balanced("[1, 2, 3]")
    assert brackets_balanced("{x: y}")
# ---
def test_find_span_end_expression():
    source = "x = 1 + 2\n"
    # The BinOp "1 + 2" starts at offset 4.
    end = _find_span_end(source, 4)
    assert end is not None
    assert source[4:end] == "1 + 2"
# ---
def test_empty_set_against_string(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.z.in_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def set_process_stats_callback(stats_cb):
    """ Sets a callback function (hook) to push stats after a process operation call. """
    global stats_callback
    if stats_cb is None:
        pass
    elif stats_callback:
        log.warn("Stats callback already defined")
    stats_callback = stats_cb
# ---
def fn(config: MyConfig):
        append_log(log, config)
        time.sleep(run_time)
# ---
def normalize_answer(answer: str | None) -> str | None:
    if answer is None:
        return None
    answer = answer.strip()
    try:
        # Remove enclosing `\text{}`.
        m = re.search("^\\\\text\\{(?P<text>.+?)\\}$", answer)
        if m is not None:
            answer = m.group("text").strip()
        return _strip_string(str(answer))
    except Exception:
        return answer
# ---
def _handle_key_binding_message(self, binding_name, key_state, key_name):
        self._key_binding_handlers[binding_name](key_state, key_name)
# ---
def start_tls(self, read_server_info=True):
        self.start_tls_called = True
# ---
def StdCapture(out=True, err=True, in_=True):
    return capture.MultiCapture(out, err, in_, Capture=capture.SysCapture)
# ---
def getheaders(self, name):
        return list(self.values())
# ---
def observer(val):
            if cond(val):
                sema.release()
# ---
def _power_on(driver_info):
    """Turn the power ON for this node.

    :param driver_info: the ipmitool parameters for accessing a node.
    :returns: one of ironic.common.states POWER_ON or ERROR.
    :raises: IPMIFailure on an error from ipmitool (from _power_status call).

    """
    return _set_and_wait(states.POWER_ON, driver_info)
# ---
def fold_fun(acc, z, x):
        return acc + z * x
# ---
def test_plain_ndarray_selector():
    B, V = Axis("batch", 3), Axis("vocab", 5)
    x = hax.arange((B, V))
    idx = jnp.array([0, 2, 4], dtype=jnp.int32)
    out = x["vocab", idx]
    assert out.axes == (B,)
    assert jnp.array_equal(out.array, x.array[jnp.arange(3), idx])
# ---
def _UNSPECIFIED():
    raise ValueError("unspecified")
# ---
def comment_urlview(self):
        data = self.get_selected_item()
        comment = data.get('body') or data.get('text') or data.get('url_full')
        if comment:
            self.term.open_urlview(comment)
        else:
            self.term.flash()
# ---
def __init__(self, bundle_path: Path):
        self._bundle_path = bundle_path
# ---
def unregister():
    global imported_mods
    print(imported_mods)
    for mod in imported_mods.values():
        mod.unregister()
    imported_mods = {}
# ---
def __init__(self, base_controller: Controller, sim: LungEnv, pid_K=[0.0, 0.0], decay=0.1, **kwargs):
        self.base_controller = base_controller
        self.sim = sim
        self.I = 0.0
        self.K = pid_K
        self.decay = decay

        self.reset()
# ---
def requires(self):
        return RawData()
# ---
def test_ref_get_with_invalid_election_id_non_existent_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': '69' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified an ID for a non-existent election.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def test_initial_state(self):
        self.assertEqual(self.mda.control_reg, 0x00)
        self.assertEqual(self.mda.control_reg, 0x00)
        self.assertEqual(self.mda.screen, None)
        self.assertEqual(self.mda.char_generator, self.cg)
        self.assertEqual(len(self.mda.video_ram), 4096)
# ---
def _wait_ref(self, timeout: float | None, raise_on_failure: bool) -> JobStatus:
        try:
            ray.get(self._ref, timeout=timeout)
        except Exception:
            if raise_on_failure:
                raise
        return self.status()
# ---
def __eq__(self, other: object) -> bool:
        if not isinstance(other, Timestamp):
            return NotImplemented
        return self._epoch_ms == other._epoch_ms
# ---
def is_local(self) -> bool:
        return self._config.controller.WhichOneof("controller") == "local"
# ---
def _get_provider_uuid_by_host(self, host):
        # We have to temporarily mutate to 2.53 to get the hypervisor UUID.
        with utils.temporary_mutation(self.admin_api, microversion='2.53'):
            return super(ComputeStatusFilterTest211,
                         self)._get_provider_uuid_by_host(host)
# ---
def pids(self):
        return [p.pid for p in self._procs]
# ---
def _pipeline() -> int:
        docs = pq.read_table(small_parquet_path).to_pylist()
        return len([dupekit.process_dicts_loop(doc) for doc in docs])
# ---
def signin():
    # Retorna a user data
    user_info = User.objects(email=request.form['email'], password=sha1(
        request.form['password']).hexdigest())
    if user_info.count():
        return JSON(token=user_info.get().token, roles=user_info.get().roles)
    else:
        return JSON(message='User not found')
# ---
def maybe_rng_split(key: PRNGKeyArray | None, num: int = 2):
    """Splits a random key into multiple random keys. If the key is None, then it replicates the None. Also handles
    num == 1 case"""
    if key is None:
        return [None] * num
    elif num == 1:
        return jnp.reshape(key, (1,) + key.shape)
    else:
        return jrandom.split(key, num)
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if dir == "":
            dir = self._path
        else:
            dir = self._path + "/" + dir
        return self._matcher.visitdir(dir)
# ---
def size(self) -> int:
        return self.byte_end - self.byte_start
# ---
def make_table(self):
        """Convenience method for making a table"""
        hash_key = DynamoKey("id")
        self.dynamo.create_table("foobar", hash_key=hash_key)
# ---
def frame_step(self):
        self.command('frame_step')
# ---
def test_from_not_handler(self):
        def func():
            pass

        with pytest.raises(ValueError) as excinfo:
            event.HandlerInstance.from_handler(func)
        excinfo.match(r"Event handler must be decorated with `@event`")
# ---
def format_move_dict_to_root(obj, field):
    for attr in obj[field]:
        obj["%s/%s" % (field, attr)] = obj[field][attr]
    del obj[field]
# ---
def on_begin_job_fetch(self, max):
        self.main_view.show_job_fetch_progress_dialog(max)
# ---
def test_capsyscapfdbinary(self, testdir):
        p = testdir.makepyfile("""
            def test_one(capsys, capfdbinary):
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*ERROR*setup*test_one*",
            "E*capfdbinary*capsys*same*time*",
            "*1 error*"])
# ---
def to_t(arr: jnp.ndarray):
            return torch.from_numpy(np.array(arr))
# ---
def step_impl(context, username, password, email, first_name, last_name):
    context.base_user = User(username=username, email=email, password=password, first_name=first_name,
                        last_name=last_name)
# ---
def get_cpu_millicores(self) -> int | None:
        if not self.resources or not self.resources.cpu:
            return None
        return self.resources.cpu * 1000
# ---
def _get_fs_and_plain_path(path, fs=None):
    if fs is None:
        fs, _, (path_to_open,) = fsspec.get_fs_token_paths(str(path))
    else:
        path_to_open = path
    return fs, path_to_open
# ---
def generate_data(config: GenerateDataConfig):
    """Generate numbers from 0 to `n` - 1 and write them to `output_path`."""
    numbers = list(range(config.n))

    # Write to file
    numbers_path = os.path.join(config.output_path, "numbers.json")
    with fsspec.open(numbers_path, "w") as f:
        json.dump(numbers, f)
# ---
def _concatenate(x):
    if isinstance(x[0], np.ndarray):
        return np.concatenate(x)
    return jnp.concatenate(x)
# ---
def active_at_datetime(self, deadline):
        """Check whether the verification was active at a particular datetime.

        Arguments:
            deadline (datetime): The date at which the verification was active
                (created before and expiration datetime is after today).

        Returns:
            bool

        """
        return (
            self.created_at < deadline and
            self.expiration_datetime > now()
        )
# ---
def trace(
        self, axis1: AxisSelector, axis2: AxisSelector, offset=0, dtype=None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.trace(self, offset=offset, axis1=axis1, axis2=axis2, dtype=dtype)
# ---
def list_jobs(self) -> list[cluster_pb2.JobStatus]:
        return self._remote_client.list_jobs()
# ---
def jnp_to_python(a: jnp.ndarray):
    if isinstance(a, (float, int)):
        return float(a)
    elif a.shape == () or a.shape == (1,):
        return a.item()
    else:
        return a.tolist()
# ---
def test_from_handler(self):
        @event.event
        def handler():
            pass

        h_inst = event.HandlerInstance.from_handler(handler)
        assert h_inst.info is handler._h_info
        assert h_inst.enabled
        assert h_inst.handler is handler._h_info.handler
# ---
def __init__(self, port_range: tuple[int, int] = (30000, 40000)):
        self._range = port_range
        self._allocated: set[int] = set()
        self._lock = threading.Lock()
# ---
def _parse_range(data):
    """Parses the format range properties - min, max."""
    input_type = {}
    try:
        input_type['min'] = data['range'][0]
    except (KeyError, TypeError):  # set default value
        input_type['min'] = float('-inf')
    try:
        input_type['max'] = data['range'][1]
    except (KeyError, TypeError):  # set default value
        input_type['max'] = float('inf')
    return input_type
# ---
def _product_get(self, cr, uid, id, product_ids=False, context=None, states=None):
        """
        @param product_ids:
        @param states:
        @return:
        """
        if states is None:
            states = ['done']
        ids = id and [id] or []
        return self._product_get_multi_location(cr, uid, ids, product_ids, context=context, states=states)
# ---
def _list_vdis(self):
        url = FLAGS.xenapi_connection_url
        username = FLAGS.xenapi_connection_username
        password = FLAGS.xenapi_connection_password
        session = xenapi_conn.XenAPISession(url, username, password)
        return session.call_xenapi('VDI.get_all')
# ---
def __init__(self, main_model):
        self.main_view = None
        self.main_model = main_model

        self.main_model.begin_job_fetch.connect(self.on_begin_job_fetch)
        self.main_model.update_job_fetch_progress.connect(self.on_job_fetch_update)
        self.main_model.fetched_job.connect(self.on_fetched_job)
# ---
def query(self, name):
        ret = []
        obj = getattr(self.ndb.query, name)
        for line in obj():
            ret.append(line)
        return bottle.template('{{!ret}}', ret=json.dumps(ret))
# ---
def date(self, val):
        self.opt_meta['date'] = _EpubDate(val)
# ---
def create_floatingip_postcommit(self, context, fip_context):
        pass
# ---
def _ensure_batched(x):
        if len(x) == 0:
            return list(x)
        elif isinstance(x[0], Sequence) or isinstance(x[0], np.ndarray):
            return list(x)
        else:
            return [x]
# ---
def __repr__(self):
        return ['NONE', 'SHUTDOWN', 'LOG_MESSAGE', 'GET_PROPERTY_REPLY', 'SET_PROPERTY_REPLY', 'COMMAND_REPLY',
                'START_FILE', 'END_FILE', 'FILE_LOADED', 'TRACKS_CHANGED', 'TRACK_SWITCHED', 'IDLE', 'PAUSE', 'UNPAUSE',
                'TICK', 'SCRIPT_INPUT_DISPATCH', 'CLIENT_MESSAGE', 'VIDEO_RECONFIG', 'AUDIO_RECONFIG',
                'METADATA_UPDATE', 'SEEK', 'PLAYBACK_RESTART', 'PROPERTY_CHANGE', 'CHAPTER_CHANGE'][self.value]
# ---
def onchange_partner_in(self, cr, uid, ids, partner_id=None, context=None):
        return {}
# ---
def status(self, job_id: JobName) -> cluster_pb2.JobStatus:
        """Get job status.

        Args:
            job_id: Job ID to query

        Returns:
            JobStatus proto with current state
        """
        return self._cluster_client.get_job_status(job_id)
# ---
def get_team_name(code):
    return team_mapping[code]
# ---
def test_column_list_select2(self):
        # make sure SDC copies the columns like Pandas does
        def test_impl(df):
            df2 = df[['A']]
            df2['A'] += 10
            return df2.A, df.A

        hpat_func = self.jit(test_impl)
        n = 11
        df = pd.DataFrame(
            {'A': np.arange(n), 'B': np.ones(n), 'C': np.random.ranf(n)})
        np.testing.assert_array_equal(hpat_func(df.copy())[1], test_impl(df)[1])
# ---
def elu(a: A) -> A:
    return wrap_elemwise_unary(jnn.elu, a)
# ---
def vm_ops(self) -> PlatformOps:
        return _GcpPlatformOps(self._platform, self._label_prefix)
# ---
def build_push(source_tag: str, region: tuple[str, ...], project: str, image_name: str, version: str):
    """Push a local Docker image to GCP Artifact Registry."""
    _push_to_registries(
        source_tag,
        region,
        project,
        image_name=image_name,
        version=version,
    )
# ---
def __eq__(self, other):
        if not isinstance(other, BBOXCoverage):
            return NotImplemented

        if self.srs != other.srs:
            return False

        if self.bbox != other.bbox:
            return False

        return True
# ---
def addFile( self, lfn, force = False, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndicts = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.addFile( lfndicts, force )
# ---
def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take("Width", 2)
# ---
def test_remote_main_non_empty_pool(self):
        """Ensure AggregateError is raised if removing the main."""
        aggregate = self._aggregate_setup(aggr_state=aggregate_states.ACTIVE,
                                          hosts=['host', 'host2'],
                                          metadata=self.fake_metadata)
        self.assertRaises(exception.InvalidAggregateAction,
                          self.conn._pool.remove_from_aggregate,
                          self.context, aggregate, "host")
# ---
def custom_model_info(repo_id, *args, **kwargs) -> ModelInfo:
            if _is_url_like(repo_id):
                # `tags=None` makes is_base_mistral return False, skipping the problematic code path
                return ModelInfo(id="monkeypatched", tags=None)
            return original_model_info(repo_id, *args, **kwargs)
# ---
def refresh(self, force=False):
        self.refresh_cnt += 1
        if self.refresh_cnt >= 60000:
            self.refresh_cnt = 0
        redraw_required = False
        for wid in self.widgets:
            if (self.refresh_cnt % wid.metric.refresh_rate == 0) or force:
                wid.refresh()
                redraw_required = True
        if redraw_required:
            self.queue_draw()
        return True
# ---
def list_iris_containers(self, all_states: bool = True) -> list[str]:
        """List all containers with iris.managed=true label."""
        cmd = ["docker", "ps", "-q", "--filter", "label=iris.managed=true"]
        if all_states:
            cmd.append("-a")
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        if result.returncode != 0:
            return []
        return [cid for cid in result.stdout.strip().split("\n") if cid]
# ---
def this_output_path(name: str | None = None):
    return OutputName(name=name)
# ---
def live_checker(text):
            regex_error.setStyleSheet("")
            regex_error.setText("")
            try:
                check()
            except OptionsCheckError as e:
                regex_error.setStyleSheet(self.STYLESHEET_ERROR)
                regex_error.setText(e.info)
# ---
def __enter__(self):
        """Start batch generation thread."""
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        return self
# ---
def matchfn(self, f):
        return self._m1(f) and self._m2(f)
# ---
def as_sync_dataset(self) -> "SyncDataset[T_co]":
        return self
# ---
def sos_token_id(self) -> int:
        return 1
# ---
def get_y_from_x(self, x):
        if self.b == 0:
            return 0.0

        return 1.0 * (-self.c - self.a * x) / self.b
# ---
def _state_dict_key_map(self):
        # This tells the serialization logic how to map attribute names to state dict keys
        # We want the wrapper's parameters to be at the top level.
        # The Gpt2LMHeadModel's parameters will be nested under "model" if we follow its own mapping.
        return {
            "model": "model",
            "an_int_param": "an_int_param",
            "a_bool_buffer": "a_bool_buffer",
            "a_float_param": "a_float_param",
        }
# ---
def point_in_poly(x,y,poly):

    n = len(poly)
    inside = False

    p1x,p1y = poly[0]
    for i in range(n+1):
        p2x,p2y = poly[i % n]
        if y > min(p1y,p2y):
            if y <= max(p1y,p2y):
                if x <= max(p1x,p2x):
                    if p1y != p2y:
                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x
                    if p1x == p2x or x <= xints:
                        inside = not inside
        p1x,p1y = p2x,p2y

    return inside
# ---
def strerror(errno_value):
    """
    The string representation of a particular errno
    """
    return "[Errno {}] Invalid argument".format(errno_value)
# ---
def top_right_corner3d(self):
        return self.edge_points3d[1]
# ---
def test_get_resources_list(self):

        response = requests.get(
            self.get_endpoint(TEMPL_V1_COLLECTION_ENDPOINT))

        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.json(), ["vms"])
# ---
def __init__(self, subquery):
        self.subquery = subquery
# ---
def key_str_all(self):
        '''
        Return all managed key strings
        '''
        ret = {}
        for status, keys in six.iteritems(self.list_keys()):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                path = os.path.join(self.opts['pki_dir'], status, key)
                with salt.utils.fopen(path, 'r') as fp_:
                    ret[status][key] = fp_.read()
        return ret
# ---
def KVHeads(self) -> Axis:
        return Axis("kv_head", self.num_kv_heads)
# ---
def get_block_by_number(self, block_number) -> Optional[Block]:
        with self.lock:
            return self._state.get_block_by_number(block_number)
# ---
def _labels_one_hot_emulated(
    labels_adjusted: jax.Array,
    num_classes: int,
    dtype: jnp.dtype,
) -> jax.Array:
    labels_adjusted = labels_adjusted.astype(jnp.int32)
    in_block = (labels_adjusted >= 0) & (labels_adjusted < num_classes)
    safe_labels = jnp.where(in_block, labels_adjusted, -1)
    cols = jnp.arange(num_classes, dtype=labels_adjusted.dtype)[None, :]
    return (cols == safe_labels[:, None]).astype(dtype)
# ---
def __call__(self, indices: int) -> int: ...
# ---
def find(self):
        return self._find()
# ---
def tearDown(self):
        super(BaseResourceTestCase, self).tearDown()
        self._service.stop()
        self.session.execute("DROP TABLE IF EXISTS vms;", None)
# ---
def delete(user_fn):
        user_fn = _strip_wrapped_partial(user_fn)
        if type(user_fn) is types.FunctionType:  # noqa: E721
            try:
                del cache[user_fn]
            except KeyError:
                warnings.warn(f"Could not delete cache for function {user_fn}. Has it already been deleted?")
        else:
            warnings.warn("Could not delete non-function from cache.")
# ---
def test_unicode_options(self):
        fmt = HtmlFormatter(title=u'FÃ¶Ã¶',
                            cssclass=u'bÃ¤r',
                            cssstyles=u'div:before { content: \'bÃ¤z\' }',
                            encoding='utf-8')
        handle, pathname = tempfile.mkstemp('.html')
        tfile = os.fdopen(handle, 'w+b')
        fmt.format(tokensource, tfile)
        tfile.close()
# ---
def add_cookie_header(self, request):
        self.ach_req = request
# ---
def _update_chainstate(self, block: Block, batch):
        self._last_block = block
        self._update_block_number_mapping(block, batch)
        self.tx_pool.remove_tx_in_block_from_pool(block)
        self._state.update_mainchain_height(block.block_number, batch)
        self._state.update_tx_metadata(block, batch)
# ---
def status(self) -> JobStatus:
        iris_status = self._job.status()
        return map_iris_job_state(iris_status.state)
# ---
def test_filter_ips_white_list(self):
        CONF.network_label_regex = '.*'
        CONF.ip_regex = '^(15.|123.)'
        CONF.black_list_regex = '^10.123.123.*'
        ip = self.instance.get_visible_ip_addresses()
        ip = filter_ips(
            ip, CONF.ip_regex, CONF.black_list_regex)
        self.assertEqual(2, len(ip))
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)
# ---
def load_config() -> CliConfig:
    if os.path.exists(".levanter.yaml"):
        d = yaml.load(open(".levanter.yaml", "r"), Loader=yaml.SafeLoader)
    elif os.path.exists(".config"):
        warnings.warn("Using deprecated .config file. Please rename to .levanter.yaml")
        d = yaml.load(open(".config", "r"), Loader=yaml.SafeLoader)
    else:
        d = {}

    return draccus.decode(CliConfig, d)
# ---
def get_actor_pool_name(self) -> str:
        return f"{self._tpu_type} slices"
# ---
def path(self) -> str:
        """Returns the URL path to mount the application to when serving multiple applications."""
        return "/iris.cluster.WorkerService"
# ---
def compute(model, input):
            model_output = model(input, attn_mask=attn_mask)
            return model_output
# ---
def get_reference_answer(self) -> str:
        return self.answer
# ---
def zip_browsers_logs():
    if platform.system() == "Windows":
        cmd = "Compress-Archive browserlogs browserlogs-$(date +%Y-%m-%d-%H%M).zip"
        subprocess.call(["powershell.exe", cmd])

    elif platform.system() == "Linux" or platform.system() == "Darwin":
        cmd = "zip -vr browserlogs-$(date +%Y-%m-%d-%H%M).zip" + " browserlogs/"
        cr.run_command(cmd)
# ---
def _get_dashboard_address(self) -> str:
        if ray.is_initialized():
            try:
                ctx = ray.get_runtime_context()
                gcs_address = getattr(ctx, "gcs_address", None)
                if gcs_address is not None:
                    return gcs_address
            except Exception:
                pass
        return self._address
# ---
def map_batches(self, fn: MapFunction[Sequence[U]], *extra_args, **extra_kwargs) -> "BatchMappedAsyncDataset[U]":
        return BatchMappedAsyncDataset(self, fn, *extra_args, **extra_kwargs)
# ---
def exitTransitionToCostume(self):
        pass
# ---
def test_lambda_config_serialization(self):
    # Test serialization with output_shape and output_shape_type
    layer = keras.layers.Lambda(lambda x: x + 1, output_shape=(1, 1))
    layer(keras.backend.variable(np.ones((1, 1))))
    config = layer.get_config()
    layer = keras.layers.deserialize({
        'class_name': 'Lambda',
        'config': config
    })
    layer = keras.layers.Lambda.from_config(config)
# ---
def error():
    """ Display errors. """
    return render_template('error.html')
# ---
def add_price(self, price_list=None):
		'''Add a new price'''
		if not price_list:
			price_list = (frappe.db.get_single_value('Selling Settings', 'selling_price_list')
						or frappe.db.get_value('Price List', _('Standard Selling')))
		if price_list:
			item_price = frappe.get_doc({
				"doctype": "Item Price",
				"price_list": price_list,
				"item_code": self.name,
				"currency": erpnext.get_default_currency(),
				"price_list_rate": self.standard_rate
			})
			item_price.insert()
# ---
def __rrshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.right_shift(other, self)
# ---
def layers(self) -> Sequence[Olmo3DecoderLayer]:
        return cast(Sequence[Olmo3DecoderLayer], self._layers.unstacked())
# ---
def init(config: Olmo2Config, *, key) -> "Olmo2Transformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, Olmo2DecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return Olmo2Transformer(config, layers, ln_f)
# ---
def allocator():
    """Create PortAllocator with small range for testing."""
    return PortAllocator(port_range=(40000, 40100))
# ---
def __init__(self, selection=None, string=None, **kwargs):
        if callable(selection):
            from openerp import api
            selection = api.expected(api.model, selection)
        super(Selection, self).__init__(selection=selection, string=string, **kwargs)
# ---
def test_create_actor_synchronous_call(client: LocalClient):
    actor = client.create_actor(Counter, start=10, name="counter")
    assert actor.get() == 10
    actor.increment(3)
    assert actor.get() == 13
# ---
def get_right_shard(self, op_index: int) -> Any:
        """Get right shard for join at given op index.

        Raises:
            ValueError: If no right shard is provided for the join
        """
        shards = self.aux_shards.get(op_index, [])
        if len(shards) != 1:
            raise ValueError(f"Expected exactly 1 right shard for join at op index {op_index}, got {len(shards)}")
        return shards[0]
# ---
def start(self):
        """Start background streaming thread."""
        self._stop_event.clear()
        if self._mode == "controller":
            self._thread = threading.Thread(target=self._stream_controller, daemon=True)
        else:
            self._thread = threading.Thread(target=self._discover_and_stream_workers, daemon=True)
        self._thread.start()
# ---
def __len__(self) -> int:
        return self.input_ids.axis_size("batch")
# ---
def get_queryset(self):
        """
        Filter providers by current user
        """
        user = self.request.user
        if (type(user) == AnonymousUser):
            return Identity.objects.none()

        identities = user.current_identities()
        return identities
# ---


def monotonic(l: list):
    """Return True is list elements are monotonically increasing or decreasing.
    >>> monotonic([1, 2, 4, 20])
    True
    >>> monotonic([1, 20, 4, 10])
    False
    >>> monotonic([4, 1, 0, -10])
    True
    """
    if l == sorted(l) or l == sorted(l, reverse=True):
        return True
    return False
# ---
def exists_volume(self, volume):
        ''' return whether a volume exists '''
        exist_volumes = self.get_volumes()

        volume_found = False
        for exist_volume in exist_volumes:
            if exist_volume['name'] == volume['name']:
                volume_found = True
                break

        return volume_found
# ---
def alive(self):
        return not self.closed.is_set()
# ---
def __init__(self, coverages):
        self.coverages = coverages
        self.bbox = self.extent.bbox
# ---
def find_closest_divisible_int_to_sqrt(n: int) -> int:
    """
    Find the closest integer to the square root of n (less than or equal to sqrt(n)) that divides n.
    """
    assert n > 0, f"Expected n > 0, got {n}"
    for i in range(int(n**0.5), 0, -1):
        if n % i == 0:
            return i

    return 1
# ---
def create_test_entrypoint():
    """Create a simple test entrypoint."""
    from dataclasses import dataclass

    @dataclass
    class Entrypoint:
        callable: object
        args: tuple = ()
        kwargs: dict | None = None

        def __post_init__(self):
            if self.kwargs is None:
                self.kwargs = {}

    def test_fn():
        print("Hello from test")

    return Entrypoint(callable=test_fn)
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)  # First moment
        return ScaleByScionState(momentum_buffer=momentum_buffer)
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        if self.server_thread:
            self.server_thread.shutdown()
        return False
# ---
def list_instances(self, project: str, zone: str) -> list[dict]:
        return self._instances
# ---
def params(model_cfg):
    return init_edit_params(model_cfg, key=jax.random.PRNGKey(0))
# ---
def test_get_extra_attributes_parsing(monkeypatch, env_value, expected):
    """Test parsing of IRIS_WORKER_ATTRIBUTES environment variable."""
    monkeypatch.setenv("IRIS_WORKER_ATTRIBUTES", env_value)
    result = _get_extra_attributes()
    assert result == expected
# ---
def test_count_subtraction(self):
        """Count subtraction"""
        count = Count(4, 2)
        self.assertEqual(count - 2, 2)
# ---
def _convert_disk_format(format):
    # TODO: move to volume format when storage/volume.py
    #       will be accessible for /lib/vdsm/v2v.py
    if format == 'qcow2':
        return 'COW'
    elif format == 'raw':
        return 'RAW'
    raise KeyError
# ---
def init(Layers, *, key):
            stack = hax.nn.Stacked.init(Layers, Module)(
                layer_idx=hax.arange(Layers), key=jax.random.split(key, Layers.size)
            )
            return Model(layers=stack)
# ---
def reset(self):
        return GenState(
            cache=self.cache.reset(),
            decode_state=self.decode_state.reset(),
        )
# ---
def GetLogStreamAsString(region, stream_name, log_group):
  """Returns the messages of the log stream as a string."""
  log_lines = []
  token = None
  events = []
  while token is None or events:
    response = GetLogs(region, stream_name, log_group, token)
    events = response['events']
    token = response['nextForwardToken']
    for event in events:
      log_lines.append(event['message'])
  return '\n'.join(log_lines)
# ---
def _span(values: Iterable[float]) -> float:
    seq = list(values)
    return max(seq) - min(seq)
# ---
def main_basename(path):
    r"""Return a main name of a basename of a given file path.

    Example
    -------
    >>> main_basename('c:\code\langconv\MsgID.h')
    'MsgID.h'
    """
    base = os.path.basename(path)
    base_main, _base_ext = os.path.splitext(base)
    return base_main
# ---
def get_destroy_serializer(self):
        return serializers.Dict(
            InstanceIds=serializers.ListOfOne(serializers.Property("InstanceId"))
        )
# ---
def test_collect_workdir_size_mb_with_temp_directory(tmp_path):
    """Test workdir size calculation with a temporary directory."""
    (tmp_path / "file1.txt").write_text("x" * 1024 * 100)  # 100 KB
    (tmp_path / "file2.txt").write_text("y" * 1024 * 100)  # 100 KB

    size_mb = collect_workdir_size_mb(tmp_path)

    assert size_mb >= 1
# ---
def _copy_shard(info: dict):
        asyncio.run(
            _extend_cache_with_other_cache(
                output_path, info["path"], exemplar, info["data_offset_tree"], info["row_offset"]
            )
        )
# ---
def out_qdq(compute_dtype, out, scale, amax_history):
    return out
# ---
def testDeleteGlobal(self):
    self.assertEqual((0, 'False\n'), _GrumpRun(textwrap.dedent("""\
        foo = 42
        del foo
        print 'foo' in globals()""")))
# ---
def resolve_model_name_or_path(model: ModelConfig) -> tuple[str, ModelConfig]:
    """Resolve the `model` argument to pass to vLLM."""
    model = _maybe_enable_streaming(model)
    model_name_or_path = model.path if model.path is not None else model.name
    return model_name_or_path, model
# ---
def slice_dataset(self, start_index: Optional[int] = None, end_index: Optional[int] = None):
        """
        Slices the dataset from `start_index` to `end_index`.
        """
        return SlicedAsyncDataset(self, start_index, end_index)
# ---
def test_find_path_simple():
    source = "x = 1 + 2\n"
    target = "x = 3 + 4\n"
    path = find_path(source, target)
    assert len(path) >= 1

    # Replaying the path should produce valid Python.
    current = source
    for mutation in path:
        current = mutation.apply(current)
    ast.parse(current)
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - stop worker and wait for completion."""
        self.stop()
        self.join(timeout=5)

        try:
            status, error = self.result_queue.get_nowait()
            if status == "error":
                raise RuntimeError(f"{self.__class__.__name__} failed") from error
        except queue.Empty:
            pass

        return False
# ---
def last_block(self) -> Block:
        with self.lock:
            return self._last_block
# ---
def try_import_module(module):
        try:
            __import__(module)
        except ImportError:
            return False
        else:
            return True
# ---
def _np(x):
    return np.array(x.detach().cpu().numpy())
# ---
def neq(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def speaker_sequence(self, speaker_id, n):
        """Random walk through transitions matrix to produce a sequence of speaker ids"""
        seq = []
        for i in range(n):
            seq.append(speaker_id)
            speaker_id = self.after(speaker_id)
        return seq
# ---
def content_type(self):
        return self.headers.get("Content-Type", DEFAULT_HTTP_CONTENT_TYPE)
# ---
def make_depends(deps):
            return tuple(deps(recs) if callable(deps) else deps)
# ---
def simplify_capture(capture):
        if capture == Ellipsis:
            return Ellipsis
        elif (capture.binding == capture.axes[0] or capture.binding is None) and len(capture.axes) == 1:
            return capture.axes[0]
        elif capture.binding is None:
            return capture.axes
        else:
            return {capture.binding: capture.axes}
# ---
def should_poll(self):
        """Return the polling state."""
        return False
# ---
def mark_exact_dups_paragraphs(batches: Iterator[pa.RecordBatch]) -> Iterator[pa.RecordBatch]:
        """Mark duplicate paragraphs in a single record using exact hash matching."""

        dup_map = _load_dupe_map_shard(duplicate_key_shards)

        for batch in batches:
            yield dupekit.mark_paragraph_duplicates(
                batch,
                dup_map,
                attribute_name=str(DedupMode.EXACT_PARAGRAPH),
                algorithm=dupekit.HashAlgorithm.Xxh3_128,
            )
# ---
def get_env_var(self, key):
        '''return a environment variables '''
        results = self.get(DeploymentConfig.env_path) or []
        if not results:
            return None

        for env_var in results:
            if env_var['name'] == key:
                return env_var

        return None
# ---
def func(self, *args, **kwargs):
            if self.stopped():
                raise RuntimeError("stopped!")
            return self._func(*args, **kwargs)
# ---
def setUp(self):
        super().setUp()
        self.client.force_authenticate(user=self.user)
# ---
def __init__(
        self,
        service: WorkerServiceImpl,
        host: str = "0.0.0.0",
        port: int = 8080,
    ):
        self._service = service
        self._host = host
        self._port = port
        self._app = self._create_app()
        self._server: uvicorn.Server | None = None
# ---
def extract_braced_content(text: str, start: int = 0) -> str | None:
    """Extract content from {content} starting at position start."""
    if start >= len(text) or text[start] != "{":
        return None
    end = find_matching_brace(text, start)
    return text[start + 1 : end] if end else None
# ---
def on_activate():
            """The window was activated.

            This event can be triggered by clicking on the title bar, bringing
            it to the foreground; or by some platform-specific method.

            When a window is "active" it has the keyboard focus.

            :event:
            """
# ---
def __init__(self, fallback):
        self._fallback = fallback
# ---
def _find_base_path(input_path: str | list[str], input_files: list[str]) -> str:
    # Determine base path for rebasing
    base_path = input_path[0] if isinstance(input_path, list) else input_path
    if base_path in input_files:
        # NOTE: if the base_path is in the input_files, means it's a specific file, so rebase to its directory
        base_path = os.path.dirname(base_path)
    return base_path
# ---
def name_or_path(self):
        return self.tokenizer.name_or_path
# ---
def tokenize_prompt(self, prompt):
            return np.array([ord(c) for c in prompt], dtype=np.int32)
# ---
def get(self) -> int:
        return self._value
# ---
def _parse_avro_schema(self):
        """Extract and parse Avro schema from a read session."""
        if self._avro_schema_json:
            return

        self._avro_schema_json = json.loads(self._first_message.avro_schema.schema)
        self._column_names = tuple(
            (field["name"] for field in self._avro_schema_json["fields"])
        )
        self._first_message = None
# ---
def __init__(self, max_threads: int = 8):
        self._executor = ThreadPoolExecutor(max_workers=max_threads)
        self._jobs: list[LocalJobHandle] = []
# ---
def __init__(self, *args, **kwargs):
        super(EspecialidadeMedicoFilterSet, self).__init__(*args, **kwargs)

        row1 = to_row([('especialidade', 12)])

        self.form.helper = FormHelper()
        self.form.helper.form_method = 'GET'
        self.form.helper.layout = Layout(
            Fieldset(_('Pesquisar MÃ©dico'),
                     row1, form_actions(save_label='Filtrar'))
        )
# ---
def _match_less_than_or_equal(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) <= str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def _parse_rows(self):
        """Parse rows from the message only once."""
        if self._iter_rows is not None:
            return

        rows = self._stream_parser.to_rows(self._message)
        self._iter_rows = iter(rows)
# ---
def __call__(self, x: NamedArray, gate: NamedArray) -> NamedArray:
        in_dtype = x.dtype
        x32 = x.astype(jnp.float32)
        var = hax.mean(hax.square(x32), axis=self.axis)
        inv = hax.rsqrt(var + jnp.asarray(self.eps, dtype=jnp.float32))
        y = (x32 * inv).astype(in_dtype)  # RMSNorm (from haliax/nn/normalization.py)
        y = self.weight * y  # learned scale
        gated = y * hnn.silu(gate)  # GDN's output gate
        return gated.astype(in_dtype)
# ---
def lugar(self):
        return self.__lugar
# ---
def hr(node: RenderTreeNode, context: RenderContext) -> str:
    thematic_break_width = 70
    return "_" * thematic_break_width
# ---
def __init__(self, futures: list[tuple[ResolvedEndpoint, Future]]):
        self._futures = futures
# ---
def test_bundle_download_intermittent(cluster):
    """Bundle download fails intermittently, task retries handle it."""
    _url, client = cluster
    enable_chaos(
        "worker.bundle_download", failure_rate=0.5, max_failures=2, error=RuntimeError("chaos: download failed")
    )
    job = submit(client, _quick, "bundle-fail", max_retries_failure=3)
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def GetVariableValue( variable ):
  return vim.eval( variable )
# ---
def _get_username(self, section=None):
        try:
            names = env.config_object.get_list(section, env.config_object.USERNAME)
            username = names[0]
        except:
            print ('You must first set up a database server on this machine, '
                   'and create a database user')
            raise
        return username
# ---
def secrets(self):
        '''secret property setter'''
        if self._secrets is None:
            self._secrets = self.get_secrets()
        return self._secrets
# ---
def test_multiple_statement_differences():
    source = "a = 1\nb = 2\n"
    target = "a = 10\nb = 20\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 2
# ---
def matchfn(self, f):
        return bool(self.m1(f)) ^ bool(self.m2(f))
# ---
def test_iadd(self, evt):
        rval = event.ReturnValue(append_events=[evt])
        rval2 = event.ReturnValue(eat=True, append_events=[evt])
        rset = event.ResultSet()

        rset += rval
        rset += rval2
        rset += None
        assert rset.eat
        assert rset.append_events == [evt, evt]
# ---
def slow_create(tags=None):
            time.sleep(0.2)  # Simulate slow VM creation
            return original_create(tags)
# ---
def map(
        self,
        fn: Callable[[Any], T],
        items: Sequence[Any],
    ) -> list[WorkerFuture[T]]:
        """Map a function over items in parallel.

        Args:
            fn: Function to apply to each item
            items: Items to process

        Returns:
            List of futures, one per item
        """
        return [self.submit(fn, item) for item in items]
# ---
def __repr__(self):
        return '<BBOXCoverage %r/%r>' % (self.extent.llbbox, self.bbox)
# ---
def concat_axes(a1: Sequence[Axis], a2: AxisSpec) -> tuple[Axis, ...]:
    pass
# ---
def __init__(self, Pos: hax.Axis, max_pack_size: int, pad_token: int):
        self.Pos = Pos
        self._ids: list[int] = []
        self._segment_ids: list[int] = []
        self._loss_weight: list[float] = []
        self.num_segments = 0
        self.pad_token = pad_token
        self.max_pack_size = max_pack_size
        assert pad_token is not None, "pad_token must be set"
# ---
def __init__(self, balance=0):
        self.balance = balance
# ---
def author(self, val):
        if isinstance(val, Author) or isinstance(val, str):
            authors = [val]
        else:
            authors = val
        for aut in authors:
            try:
                self._authors.append(Author('' + aut))
            except TypeError:
                # aut is not a string, so it should be an Author object
                self._authors.append(aut)
# ---
def is_finite(self) -> bool:
        if self.stop_strategy == StopStrategy.RESTART_STRATEGY:
            return False

        return True
# ---
def resolve_axis(self, axes: AxisSelection) -> AxisSpec: ...
# ---
def get(self, ref: Any) -> Any:
        """Get result, unwrapping _ImmediateFuture if needed."""
        if isinstance(ref, _ImmediateFuture):
            return ref.result()
        return ref
# ---
def total_trainable_params(self, vocab_size: int) -> int:
        """Return total trainable parameter count for this model configuration."""
        ...
# ---
def get_template_path(config_name: str, infra_path: str = "infra") -> str:
    """Get the template path for a given config."""
    template_name = CONFIGS[config_name].get("TEMPLATE", "cluster")
    template_filename = f"marin-{template_name}-template.yaml"
    template_path = os.path.join(infra_path, template_filename)

    if not os.path.exists(template_path):
        raise FileNotFoundError(f"Template {template_filename} not found in {infra_path}")

    return template_path
# ---
def setup(self):
        self.add_copy_specs([
            "/proc/sysvipc/msg",
            "/proc/sysvipc/sem",
            "/proc/sysvipc/shm"
        ])
        self.add_cmd_output("ipcs")
# ---
def test_deadline_uses_monotonic_time():
    """Deadline uses monotonic time (immune to clock changes)."""
    # Create a deadline 100ms in the future
    deadline = Deadline.from_ms(100)

    # Should not be expired immediately
    assert not deadline.expired()

    # Sleep for 50ms - still not expired
    time.sleep(0.05)
    assert not deadline.expired()

    # Sleep for another 60ms - should be expired now (total 110ms)
    time.sleep(0.06)
    assert deadline.expired()
# ---
def delete(self, match):
        self._call_all('delete', match)
# ---
def select_leaf(leaf):
            if isinstance(leaf, haliax.NamedArray):
                if haliax.selects_axis(leaf.axes, self.Block):
                    return leaf[self.Block, index]
                else:
                    return leaf
            if is_jax_or_hax_array_like(leaf):
                if getattr(leaf, "shape", ()) and leaf.shape[0] == self.Block.size:
                    return leaf[index]
                return leaf
            return leaf
# ---
def test_addition_commutativity():
    variants = generate_expression_variants("a + b")
    assert "b + a" in variants
# ---
def test_fft_freq_and_shift():
    N = Axis("n", 8)
    x = hax.arange(N)

    f = hax.fftfreq(N)
    assert f.axes == (N,)
    assert jnp.allclose(f.array, jfft.fftfreq(8))

    rf = hax.rfftfreq(N)
    assert rf.axes[0].size == 5
    assert jnp.allclose(rf.array, jfft.rfftfreq(8))

    shifted = hax.fftshift(x)
    assert jnp.allclose(shifted.array, jfft.fftshift(x.array))
    unshifted = hax.ifftshift(shifted)
    assert jnp.allclose(unshifted.array, x.array)
# ---
def _expand_or_placeholder(url):
        expanded = list(expand_glob(url))
        return expanded if expanded else [url]
# ---
def saveWindowState(self):
        """
        Saves the main window state (position, size, toolbar positions)
        """
        mwState = self.saveState().toBase64()
        mwGeom  = self.saveGeometry().toBase64()
        cbpos.config['mainwindow', 'state'] = unicode(mwState)
        cbpos.config['mainwindow', 'geometry'] = unicode(mwGeom)
        cbpos.config.save()
# ---
def template_trim(filt_q, trim_q):
    return "filt_q: %d, trim_q: %d" % (filt_q, trim_q), ["--filter_qual", str(filt_q), "--trim_qual", str(trim_q)]
# ---
def scopes_set(self, scopes):
        self._scopes = scopes
        if self.req is not None:
            self.req.environ.setdefault('wenoit_etalage', {})['_scopes'] = scopes
# ---
def prng_key_for(self, slot_id: int, pos_id: int) -> jaxtyping.PRNGKeyArray:
        """
        Get the PRNG key for the given slot ID and position.
        This is used to sample new tokens for the given slot ID and position.
        """
        per_pos_key = self.prng_keys[ensure_scalar(slot_id)]
        return jax.random.fold_in(per_pos_key, ensure_scalar(pos_id))
# ---
def from_id(cls, notebook, platform_id, job_id):
        return SaagieJob(
            notebook,
            requests.get(JOB_URL_PATTERN % (platform_id, job_id), auth=SAAGIE_BASIC_AUTH_TOKEN).json())
# ---
def is_callable(self) -> bool:
        return self._callable_bytes is not None
# ---
def https_open(self, req):
        return self.do_open(self.httpconn, req)
# ---
def test_grids():
    L = 10
    thetas, phis = standard_grid(L)

    # Can't really test much here
    assert thetas.size == L
    assert phis.size == L**2

    grid = get_cartesian_grid(thetas, phis)
    assert grid.shape == (L**2, 3)
# ---
def unembed(self, x: NamedArray):
        return self.token_embeddings.unembed(x)
# ---
def check_answer(self, sample_str: str) -> bool:
        try:
            answer = extract_boxed(sample_str)
        except ValueError:
            return False
        return safe_grade(answer, self.answer, self.grader, self.timeout)
# ---
def blank_req(self, path, environ = None, base_url = None, headers = None, POST = None, **kw):
        env = environ.copy() if environ else {}
        etalage_env = env.setdefault('etalage', {})
        for key in self.env_keys:
            value = getattr(self, key)
            if value is not None:
                etalage_env[key] = value
        return webob.Request.blank(path, environ = env, base_url = base_url, headers = headers, POST = POST, **kw)
# ---
def tearDown(self):
        self.db_info.delete()
        self.backup.delete()
        self.datastore.delete()
        self.datastore_version.delete()
        models.create_nova_client = self.orig_client
        task_api.API(self.context).create_instance = self.orig_api
        models.run_with_quotas = self.run_with_quotas
        backup_models.DBBackup.check_swift_object_exist = self.check
        self.backup.delete()
        self.db_info.delete()
        super(CreateInstanceTest, self).tearDown()
# ---
def _align_variables(variables, join='outer'):
    """Align all DataArrays in the provided dict, leaving other values alone.
    """
    alignable = [k for k, v in variables.items() if hasattr(v, 'indexes')]
    aligned = align(*[variables[a] for a in alignable],
                    join=join, copy=False)
    new_variables = OrderedDict(variables)
    new_variables.update(zip(alignable, aligned))
    return new_variables
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        del name, type
        logger.warning("Trackio does not currently support artifacts. Skipping upload for %s", artifact_path)
# ---
def seq_lens(self) -> ht.i32[NamedArray, "seq"]:  # type: ignore[name-defined]
        """Current logical length for each active sequence."""
        return self.sequences.seq_lens
# ---
def testRegression(self):
        envelope = range(22050)
        envelope.reverse()
        envelope = range(22050) + envelope

        gen = VectorInput(envelope)
        tcToTotal = sTCToTotal()
        p = Pool()

        gen.data >> tcToTotal.envelope
        tcToTotal.TCToTotal >> (p, 'lowlevel.tctototal')

        run(gen)

        self.assertAlmostEqual(p['lowlevel.tctototal'],
                               TCToTotal()(envelope))
# ---
def _make(name: str = "test-job") -> cluster_pb2.Controller.LaunchJobRequest:
        job_name = JobName.root(name)
        return cluster_pb2.Controller.LaunchJobRequest(
            name=job_name.to_wire(),
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=1024**3),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )
# ---
def __init__(self, dataset: AsyncDataset[T_co]):
        self.dataset = dataset
# ---
def validate_stock_for_has_batch_and_has_serial(self):
		if self.stock_ledger_created():
			for value in ["has_batch_no", "has_serial_no"]:
				if frappe.db.get_value("Item", self.name, value) != self.get_value(value):
					frappe.throw(_("Cannot change {0} as Stock Transaction for Item {1} exist.".format(value, self.name)))
# ---
def __init__(self, field):
		self.field = field
# ---
def wait_async_x_writes():
        x_write_future.wait()
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("data", String(50)),
        )
# ---
def TextAfterCursor():
  """Returns the text after CurrentColumn."""
  return ToUnicode( vim.current.line[ CurrentColumn(): ] )
# ---
def remove(url, *, recursive=False, **kwargs):
    """Remove a file from a remote filesystem."""
    # TODO: better to use a STS deletion policy or job for this one.
    fs, path = fsspec.core.url_to_fs(url, **kwargs)

    fs.rm(path, recursive=recursive)
# ---
def _is_trivial_index(idx: Any) -> bool:
    return isinstance(idx, slice) and idx.start is None and idx.stop is None and idx.step is None
# ---
def try_load_hf(model_id):
        try:
            AutoConfig.from_pretrained(model_id)
        except Exception:
            return False
        else:
            return True
# ---
def kill_task(self, task_id: str, term_timeout_ms: int = 5000) -> bool: ...
# ---
def feature_encoders(self, data_dir):
    del data_dir
    return {
        "inputs": text_encoder.ImageEncoder(channels=self.num_channels),
        "targets": text_encoder.ClassLabelEncoder(self.class_labels)
    }
# ---
def test_rename_variables_preserves_structure(rng):
    source = "def foo(x):\n    return x + 1"
    result = rename_variables(source, rng)
    assert result is not None
    # Should still have a function with a return and addition.
    tree = ast.parse(result)
    func = tree.body[0]
    assert isinstance(func, ast.FunctionDef)
    ret = func.body[0]
    assert isinstance(ret, ast.Return)
    assert isinstance(ret.value, ast.BinOp)
    assert isinstance(ret.value.op, ast.Add)
# ---
def get_tracker(name: Literal["wandb"]) -> WandbTracker: ...
# ---
def init(In, Out, key):
            blocks = hax.nn.Stacked.init(Layer, Block)(In, Out, key=jax.random.split(key, Layer.size))
            return Tformer(blocks)
# ---
def _map_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield fn(item)
# ---
def _default_cache_key():
    assert False
# ---
def get(self):
        field, record, value = self.args
        # in order to read the current field's value, remove self from cache
        del record._cache[field]
        # read the current field's value, and update it in cache only
        record._cache[field] = new_value = record[field.name] | value
        return new_value
# ---
def _ParseAndVisit(source):
  mod = pythonparser.parse(source)
  _, future_features = imputil.parse_future_features(mod)
  importer = imputil.Importer(None, 'foo', 'foo.py', False)
  b = block.ModuleBlock(importer, '__main__', '<test>',
                        source, future_features)
  visitor = stmt.StatementVisitor(b)
  visitor.visit(mod)
  return visitor
# ---
def Auth(self): # I really don't know how to call this.
        return self.__data['cls_auth']
# ---
def test_column_var(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.var()

        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(), test_impl())
# ---
def col_clause(self):
        return self.field + " = ?", [self.buf_pattern]
# ---
def _make_and_expr(self, check1, _and, check2):
        """Create an 'and_expr'.

        Join two checks by the 'and' operator.
        """

        return [('and_expr', AndCheck([check1, check2]))]
# ---
def inject_profiler():
        return dict(profiler_includes=templatetags.profiler_includes())
# ---
def checksum(sscc):
        salt = '31' * 8 + '3'
        sum = 0
        for sscc_part, salt_part in zip(sscc, salt):
            sum += int(sscc_part) * int(salt_part)
        return (10 - (sum % 10)) % 10
# ---
def shard_map(
    *,
    in_specs: Any = None,
    out_specs: Any = None,
    mesh: Mesh | None = None,
    axis_mapping: ResourceMapping | None = None,
    check_rep: bool = False,
    **kwargs,
) -> typing.Callable[[Callable[Args, R]], Callable[Args, R]]: ...
# ---
def identical(self, other):
        """Like equals, but also checks all dataset attributes and the
        attributes on all variables and coordinates.

        See Also
        --------
        Dataset.broadcast_equals
        Dataset.equals
        """
        try:
            return (utils.dict_equiv(self.attrs, other.attrs)
                    and self._all_compat(other, 'identical'))
        except (TypeError, AttributeError):
            return False
# ---
def test_impl(df):
            return df.A.str.replace('AB', 'EE', regex=False)
# ---
def _add_tiny(x):
    return x + jnp.finfo(x.dtype).tiny
# ---
def cleanup(self) -> None:
        """No cleanup needed for GCS checkpoints."""
        pass
# ---
def test_ring_buffer_query_limit(ring_buffer):
    """Query respects limit parameter, returning most recent records."""
    for i in range(10):
        ring_buffer.append(BufferedLogRecord(float(i), "INFO", "test", f"msg-{i}"))
    results = ring_buffer.query(limit=3)
    assert len(results) == 3
    assert results[0].message == "msg-7"
# ---
def test_actor_exception_propagation():
    """Test that exceptions from actor methods propagate to the client."""
    server = ActorServer(host="127.0.0.1")
    server.register("calc", Calculator())
    port = server.serve_background()

    try:
        resolver = FixedResolver({"calc": f"http://127.0.0.1:{port}"})
        client = ActorClient(resolver, "calc")
        with pytest.raises(ZeroDivisionError):
            client.divide(1, 0)
    finally:
        server.stop()
# ---
def __init__(
        self,
        project: str,
        zone: str,
        api: GcsApi | None = None,
    ):
        self._project = project
        self._zone = zone
        self._api = api or RealGcsApi()
# ---
def test_latex(self):
        body, mime_type = self._run_handler(
            EXPERIMENT, SESSION_GROUPS, download_data.OutputFormat.LATEX
        )
        self.assertEqual("application/x-latex", mime_type)
        self.assertEqual(EXPECTED_LATEX, body)
# ---
def step(state: TrainingState, batch: dict[str, jax.Array]):
        (_loss, metrics), grads = jax.value_and_grad(loss_and_metrics, has_aux=True)(state.params, batch)
        updates, new_opt_state = optimizer.update(grads, state.opt_state, state.params)
        new_params = optax.apply_updates(state.params, updates)
        new_state = replace(state, step=state.step + 1, params=new_params, opt_state=new_opt_state)
        return new_state, metrics
# ---
def get_extra_inputs():
  """Returns the captured input tensors by the function.

  Returns:
    If the default graph is being used to define a function, the
    returned list of tensors are those accessed inside the function body
    but defined outside the function body so far. Otherwise, returns an
    empty list.
  """
  g = ops.get_default_graph()
  if isinstance(g, _FuncGraph):
    return g.extra_inputs
  else:
    return []
# ---
def send_endorsement_notification(endorsement):
    subject = _("%s has endorsed you on Villages.cc") % endorsement.endorser
    send_notification(subject, endorsement.endorser, endorsement.recipient,
                      'endorsement_notification_email.txt',
                      {'endorsement': endorsement})
# ---
def __init__(self, doc_cache: TreeCache[dict], seq_len: int):
        super().__init__()
        self.doc_cache = doc_cache
        self.seq_len = seq_len
        self._store: TreeStore | None = doc_cache.store
        self._cached_len: int | None = None
# ---
def created_at_ms(self) -> int:
        """Timestamp when this VM group was created (milliseconds since epoch)."""
        return self._created_at.epoch_ms()
# ---
def config(self) -> GrugConfigLike:
        return self.grug_config
# ---
def query_string(self):
        if self._parsed_url.query:
            return self._parsed_url.query.decode("utf-8")
        else:
            return ""
# ---
def __init__(
        self,
        state: WorkerState,
        task_queue: "Queue[PendingTask]",
        resolver: Resolver,
        timeout: float,
    ):
        self.state = state
        self._task_queue = task_queue
        self._resolver = resolver
        self._timeout = timeout
        self._discover_backoff = ExponentialBackoff(initial=0.05, maximum=1.0)
        self._actor_client: ActorClient | None = None
        self._stop_event: threading.Event | None = None
# ---
def to_proto(self) -> cluster_pb2.CoschedulingConfig:
        """Convert to protobuf representation."""
        return cluster_pb2.CoschedulingConfig(group_by=self.group_by)
# ---
def loss_fn(model, data):
        m = jax.vmap(model)
        return jnp.mean(jnp.square(m(data)))
# ---
def get_all_coeffs():
    """Get all available calibration coefficients for the satellites."""
    coeffs = {}

    for platform in URLS:
        if platform not in coeffs:
            coeffs[platform] = {}
        for chan in URLS[platform].keys():
            url = URLS[platform][chan]
            print(url)
            page = get_page(url)
            coeffs[platform][chan] = get_coeffs(page)

    return coeffs
# ---
def from_string(value):
        """ Convert an ORM ``value`` into a :class:`date` value. """
        if not value:
            return None
        value = value[:DATE_LENGTH]
        return datetime.strptime(value, DATE_FORMAT).date()
# ---
def render(self, text):
        """Render the Markdown text.

        :param text: markdown formatted text content.
        """
        return self.parse(text)
# ---
def compute_reward(generated_tokens: np.ndarray, expected_tokens: np.ndarray) -> float:
    """
    Compute reward based on exact token ID match.

    Args:
        generated_tokens: Array of generated token IDs
        expected_tokens: Array of expected token IDs (e.g., tokenized "Paris")

    Returns:
        1.0 if expected_tokens appear consecutively in generated_tokens, else 0.0
    """
    if np.array_equal(generated_tokens, expected_tokens):
        return 1.0
    else:
        return 0.0
# ---
def reader(self) -> "InMemoryRolloutReader":
        """Create a reader for this queue."""
        return InMemoryRolloutReader(self)
# ---
def test_glob (self):
    import glob
    pattern = os.path.join (utils.TEST_ROOT, "*")
    self.assertEquals (list (fs.glob (pattern)), glob.glob (pattern))
# ---
def wait_ready(self, count: int | None = None, timeout: float = 300.0) -> list[ActorHandle]:
        """Return ready actor handles. Local actors are ready immediately."""
        if count is None:
            count = len(self._handles)
        self._yielded = True
        return self._handles[:count]
# ---
def __init__(self, subqueries=()):
        self.subqueries = subqueries
# ---
def valida_igualdade(self, texto1, texto2, msg):
        if texto1 != texto2:
            raise ValidationError(msg)
        return True
# ---
def __iter__(self):
        """Iterator for each row in all pages."""
        for page in self.pages:
            for row in page:
                yield row
# ---
def loss_fn(x_in, w_in, y_in):
        return linear_softmax_cross_entropy_loss(
            x_in,
            y_in,
            w_in,
            reduction="mean",
            implementation="mosaic_tpu",
        )
# ---
def model_params(self) -> M:
        return self.model
# ---
def test_scheme():

    # does not raise NotImplementedError
    UrlPath('/dev/null').touch()
# ---
def arcsinh(a: A) -> A:
    return wrap_elemwise_unary(jnp.arcsinh, a)
# ---
def print_json_field(label, json_value):
    print_field(label, json.dumps(json_value, ensure_ascii=False))
# ---
def _patch_up_reduce_fn(reduce_fn):
    if reduce_fn is haliax.max:
        reduce_fn = jax.lax.max
    elif reduce_fn is haliax.min:
        reduce_fn = jax.lax.min
    elif reduce_fn is haliax.sum:
        reduce_fn = jax.lax.add
    elif reduce_fn is haliax.prod:
        reduce_fn = jax.lax.mul

    return reduce_fn
# ---
def wait_for_interrupt(self):
        """Wait for Ctrl+C, keeping the cluster running."""
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            pass
# ---
def get_ion_actor_id(process):
    """Given an ION process, return the ion-actor-id from the context, if set and present"""
    ion_actor_id = None
    if process:
        ctx = process.get_context()
        ion_actor_id = ctx.get(MSG_HEADER_ACTOR, None) if ctx else None
    return ion_actor_id
# ---
def abort_job(job_id):
    try:
        job = _get_job(job_id)
        job.abort()
    except ClientError as e:
        logging.info('Cannot abort job, error: %s', e)
        return errCode[e.err_name]
    return {'status': doneCode}
# ---
def matchfn(self, f):
        return self._m1(f) and (not self._m2(f) or self._m1.exact(f))
# ---
def start_container(self, container_id: str) -> None:
        result = subprocess.run(
            ["docker", "start", container_id],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode != 0:
            raise RuntimeError(f"Failed to start container: {result.stderr}")
# ---
def moneda(self):
        return self.__moneda
# ---
def __str__(self):
        return self.get_url()
# ---
def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self._stop_tokens = None
        self.max_tokens = 512
# ---
def get_match(self):
        match = Forseti.Match()
        self._set_match_panel(match, 0, 0)
        self._set_match_panel(match, 1, 2)
        self._set_match_panel(match, 2, 1)
        self._set_match_panel(match, 3, 3)
        try:
            match.match_number = int(self.match_num_ctrl.GetValue())
        except ValueError:
            match.match_number = random.getrandbits(31)
        return match
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        return YarnRotaryEmbeddings.init(HeadSize, self)
# ---
def init(axis: Axis, eps: float = 1e-6) -> "GatedRmsNorm":
        return GatedRmsNorm(axis=axis, weight=hax.ones(axis), eps=eps)
# ---
def set_time(self, match):
        self.time_text.SetLabel(format_time(match.game_time_so_far))
        self.stage_text.SetLabel(match.stage_name)
# ---
def max_Pos(self) -> Axis:
        return Axis("position", self.max_seq_len)
# ---
def _make_state(step, key, depth=3):
    model = MLP(in_size=2, out_size=1, width_size=2, depth=depth, key=key)
    optim = optax.adam(1e-4)
    opt_state = optim.init(arrays_only(model))

    return TrainerState(step, model, optim, opt_state, key, is_trainable=True, mp=None, model_averaging=None)
# ---
def _autoscaling_monitor_loop(self):
        """Start a background thread that monitors the load and scales the number of actors accordingly."""
        while not self._monitor_stop_event.is_set():
            try:
                time.sleep(self.scale_check_interval)
                self._check_and_scale()
            except Exception:
                # Best-effort; do not crash the monitor
                pass
# ---
def max(self, axis: AxisSelection | None = None, *, where=None) -> "NamedArray":  # pragma: no cover
        return haliax.max(self, axis=axis, where=where)
# ---
def get_extra_args():
  """Returns the corresponding function arguments for the captured inputs.

  Returns:
    If the default graph is being used to define a function, the
    returned list of place holders are those used inside the function
    body corresponding those returned by get_extra_inputs(). Otherwise,
    returns an empty list.
  """
  g = ops.get_default_graph()
  if isinstance(g, _FuncGraph):
    return g.extra_args
  else:
    return []
# ---
def isclose(a: NamedArray, b: NamedArray, rtol=1e-05, atol=1e-08, equal_nan=False) -> NamedArray:
    """Returns a boolean array where two arrays are element-wise equal within a tolerance."""
    a, b = broadcast_arrays(a, b)
    # TODO: numpy supports an array atol and rtol, but we don't yet
    return NamedArray(jnp.isclose(a.array, b.array, rtol=rtol, atol=atol, equal_nan=equal_nan), a.axes)
# ---
def simple_loss_fn(model, batch, key=None):
    """Loss function returning scalar only."""
    return hax.sum(batch * model.weight)
# ---
def _write_yaml_to_memory(yaml: str, path: str = "memory://test.yaml"):
    with fsspec.open(path, "w") as f:
        f.write(yaml)
    return path
# ---
def __post_init__(self):
        object.__setattr__(self, "caches", tuple(self.caches))
# ---
def test_clean_wiki_html():
    cleaned = clean_wiki_html(SAMPLE_WIKIPEDIA_HTML, remove_reference_section=True)
    soup = BeautifulSoup(cleaned, "html.parser")

    # References should be removed
    assert soup.find("div", {"class": "reflist"}) is None
    assert soup.find("h2", {"id": "References"}) is None

    # Infobox should be moved
    assert soup.find("h2", string="InfoBox") is not None

    # Equations should be unwrapped
    assert "$E = mc^2$" in str(cleaned)
# ---
def max_slices(self) -> int:
        """Maximum number of VM groups allowed."""
        return self._config.max_slices
# ---
def __init__(self, remote, *args, **kwargs):
        super(MatchControl, self).__init__(*args, **kwargs)
        self.remote = remote
        self.InitUI()
# ---
def _compatible_block(shard_len: int, max_block: int) -> int:
        """Pick largest block <= max_block that divides shard_len; prefer multiples of 128."""
        if shard_len <= 0:
            return max_block
        cap = min(max_block, shard_len)
        for step in (128, 1):
            candidate = cap - (cap % step)
            while candidate >= step:
                if shard_len % candidate == 0:
                    return candidate
                candidate -= step
        return 1
# ---
def python_mark_exact_dups_documents(
    batch: pa.RecordBatch,
    text_col: str,
    id_col: str,
    dup_map: dict[str, Any],
    attribute_name: str,
) -> list[dict[str, Any]]:
    results = []
    for record in batch.to_pylist():
        text, record_id = record[text_col], record[id_col]
        h = _str_hash_legacy(text)
        is_dup = h in dup_map and dup_map[h]["canonical"] != record_id
        results.append({"id": record_id, "attributes": {attribute_name: is_dup}})
    return results
# ---
def test_composed_multiple(self):
        table = self.tables.some_table
        lx = (table.c.x + table.c.y).label("lx")
        ly = (func.lower(table.c.q) + table.c.p).label("ly")
        self._assert_result(
            select([lx, ly]).order_by(lx, ly.desc()),
            [(3, util.u("q1p3")), (5, util.u("q2p2")), (7, util.u("q3p1"))],
        )
# ---
def add(self, other):
        assert isinstance(self.ast, ast.CharClass)
        self.ast.elems = self.ast.elems + (other,)
# ---
def p_error(self, p):
        if p:
            raise SyntaxError(
                "Character '%s' at line %d" % (p.value[0], p.lineno)
            )
        else:
            raise SyntaxError("SyntaxError at EOF")
# ---
def noop():
        pass
# ---
def pages_per_seq(self) -> int:
        return self.page_indices.axis_size("page")
# ---
def pytest_addoption(parser):
    """Add CLI options related to Testimony token based mark collection"""
    parser.addoption(
        '--importance',
        help='Comma separated list of importance levels to include in test collection',
    )
    parser.addoption(
        '--component',
        help='Comma separated list of component names to include in test collection',
    )
    parser.addoption(
        '--assignee',
        help='Comma separated list of assignees to include in test collection',
    )
# ---
def __del__(self):
        print("Updated " + self.tableName + " table.")
# ---
def __str__(self):
        return f"{self.name}=={self.version}" if self.version else self.name
# ---
def _reinit_embed_vectors(Embed, new_vocab, embeddings_matrix, ids_to_reinit: Iterable[int], key):
    # Match the existing embedding statistics to avoid abrupt scale shifts.
    mu = hax.mean(embeddings_matrix, axis="vocab")
    std = hax.std(embeddings_matrix, axis="vocab")
    reinited = hax.random.truncated_normal(key, (new_vocab, Embed), -3, 3) * std + mu
    new_weight = embeddings_matrix.at["vocab", ids_to_reinit].set(reinited)
    return new_weight
# ---
def cleanup_log(path: str):
    os.unlink(path)
# ---
def main(args: RayCachedLMDatasetConfig):
    """Caches two different kinds of datasets. It can cache a dataset from a list of urls, or a dataset from a hf dataset"""
    init_logging(".", "cache_dataset.log")
    args.initialize()

    for split in ["train", "validation"]:
        print(f"Caching {split} for all components.")
        # build_caches will build or load as needed
        args.build_caches(split)
        print(f"Finished caching {split}.")
# ---
def minimum(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.minimum(x1, x2)
# ---
def get_bucket_location(bucket_name_or_path):
    """Get the GCS bucket's location."""
    if bucket_name_or_path.startswith("gs://"):
        bucket_name = split_gcs_path(bucket_name_or_path)[0]
    else:
        bucket_name = bucket_name_or_path

    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    # this returns upper case regions, which isn't consistent with the rest of the codebase
    return bucket.location.lower()
# ---
def fake_vdi_resize(*args, **kwargs):
            self.called = True
# ---
def evaluate(self, record: dict) -> Any:
        parent_val = self.parent.evaluate(record)
        if isinstance(parent_val, dict):
            return parent_val.get(self.field)
        return None
# ---
def setUp(self):

        self.data = MockData()
# ---
def model_type(self) -> type[GrugWrapper]:
        return GrugWrapper
# ---
def run_ansible(params):
        '''run the idempotent ansible code'''
        oc_version = OCVersion(params['kubeconfig'], params['debug'])

        if params['state'] == 'list':

            #pylint: disable=protected-access
            result = oc_version.get()
            return {'state': params['state'],
                    'results': result,
                    'changed': False}
# ---
def set_after_delay():
        time.sleep(0.05)
        flag.set()
# ---
def init(config: ApertusConfig, *, key) -> "ApertusTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, ApertusDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return ApertusTransformer(config, layers, ln_f)
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            B = df.A.fillna(5.0)
            return B.sum()
# ---
def autolink(self, link, is_email=False):
        """Rendering a given link or email address.

        :param link: link content or email address.
        :param is_email: whether this is an email or not.
        """
        text = link = escape(link)
        if is_email:
            link = 'mailto:%s' % link
        return '<a href="%s">%s</a>' % (link, text)
# ---
def pop(self):
        """Get the element with the highest priority.

        Get the element with the highest priority (i.e., smallest value).

        Returns
        -------
        The element with the highest priority.

        """
        return heapq.heappop(self._queue)[1]
# ---
def main():
    load_data('enron_mail_clean.tar.gz')
    import pdb
    pdb.set_trace()
# ---
def mkdir(self, path):
        "Make relative directory in workflowdir"
        path = os.path.join(self.workflowdir, path)
        if not os.path.exists(path):
            os.makedirs(path)
# ---
def attrs(self):
        """Dictionary of global attributes on this dataset
        """
        if self._attrs is None:
            self._attrs = OrderedDict()
        return self._attrs
# ---
def string_match(cls, pattern, value):
        """Determine whether the value matches the pattern. Both
        arguments are strings. Subclasses implement this method.
        """
        raise NotImplementedError()
# ---
def _health_check() -> dict:
    """Health check endpoint."""
    return {"status": "healthy", "service": "levanter-inference"}
# ---
def __call__(self, environ, start_response):
        if environ['PATH_INFO'].startswith('/socket.io'):
            socketio_manage(environ, {'': QueueStatusHandler})
# ---
def subtract(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.subtract(x1, x2)
# ---
def count_unknown_letters_in_expr(expr: str):
    expr = expr.replace("sqrt", "")
    expr = expr.replace("frac", "")
    letters_in_expr = set([x for x in expr if x.isalpha()])
    return len(letters_in_expr)
# ---
def filter_checkpoint(
    fun: Callable,
    *,
    prevent_cse: bool = True,
    policy: Callable[..., bool] | None = None,
):
    """As `jax.checkpoint`, but allows any Python object as inputs and outputs"""

    warnings.warn(
        "filter_checkpoint is deprecated, use eqx.filter_checkpoint instead",
        DeprecationWarning,
    )

    return eqx.filter_checkpoint(fun, prevent_cse=prevent_cse, policy=policy)
# ---
def test_variant_preserved_in_display():
    """Variant information is preserved when formatting for display."""
    variant = "v5litepod-16"
    result = format_accelerator_display(config_pb2.ACCELERATOR_TYPE_TPU, variant)
    assert variant in result, f"Variant '{variant}' must be visible in display: {result}"
# ---
def determine_domain(self, records, operator, value):
        """ Return a domain representing a condition on ``self``. """
        if self.search:
            return self.search(records, operator, value)
        else:
            return [(self.name, operator, value)]
# ---
from typing import List, Tuple


def sum_product(numbers: List[int]) -> Tuple[int, int]:
    """ For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    >>> sum_product([])
    (0, 1)
    >>> sum_product([1, 2, 3, 4])
    (10, 24)
    """
    sum_value = 0
    prod_value = 1

    for n in numbers:
        sum_value += n
        prod_value *= n
    return sum_value, prod_value
# ---
def StdCaptureFD(out=True, err=True, in_=True):
    return capture.MultiCapture(out, err, in_, Capture=capture.FDCapture)
# ---
def __init__(self, ebox_data, sensor_type, name):
        """Initialize the sensor."""
        self.client_name = name
        self.type = sensor_type
        self._name = SENSOR_TYPES[sensor_type][0]
        self._unit_of_measurement = SENSOR_TYPES[sensor_type][1]
        self._icon = SENSOR_TYPES[sensor_type][2]
        self.ebox_data = ebox_data
        self._state = None
# ---
def test_construct_image_url_direct(self):
        drv = self._driver
        img_loc = ("nfs://host/path/image-id", None)
        location = drv._construct_image_nfs_url(img_loc)
        if location != "nfs://host/path/image-id":
            self.fail("Unexpected direct url.")
# ---
def on_upper_bound_changed(self):
        self.ui.spinBoxLowerBound.setMaximum(self.ui.spinBoxUpperBound.value() - 1)
        self.ui.spinBoxBoundaryNumber.setMaximum(math.ceil((self.ui.spinBoxUpperBound.value()
                                                            - self.ui.spinBoxLowerBound.value()) / 2))
# ---
def to_hf_config(self, vocab_size: int, config_overrides: Optional[dict] = None) -> HfConfig:
        pass
# ---
def pallas_tpu_impl_batched(x_batched: jax.Array) -> jax.Array:
    """TPU/Pallas implementation placeholder.

    Replace this body with a real `jax.experimental.pallas` kernel. Keep the signature stable and match the reference.
    """

    return reference_impl_batched(x_batched)
# ---
def test_get_secret_key(self):
        first_key = csrf._get_secret_key()
        self.assertEqual(len(first_key), 32)
        second_key = csrf._get_secret_key()
        self.assertEqual(first_key, second_key)
# ---
def test_buffered_column_result_proxy(self):
        self._test_proxy(_result.BufferedColumnResultProxy)
# ---
def base_field(self):
        """ Return the base field of an inherited field, or ``self``. """
        return self.related_field.base_field if self.inherited else self
# ---
def cli(ctx, dry_run, executor_info_base_path, prefix):
    """Manage pretraining datasets: download, tokenize, and list available datasets."""
    # These arguments are passed by the test framework but used by executor_main, not this CLI
    if ctx.invoked_subcommand is None:
        ctx.invoke(_list)
# ---
def _find_slice_for_worker(self, vm_address: str) -> tuple[str | None, ScalingGroup | None]:
        """Find the slice and group containing a worker by VM address."""
        for group in self._groups.values():
            for slice_obj in group.vm_groups():
                for vm in slice_obj.vms():
                    if vm.info.address == vm_address:
                        return slice_obj.slice_id, group
        return None, None
# ---
def result(self) -> "TreeCache":
        if not self._is_closed:
            raise RuntimeError("Cannot get result until TreeCacheWriter is closed")
        return TreeCache.load(self.cache_dir, self._exemplar, self.metadata)
# ---
def _build_query_string(q, default_field=None):
    """
    Builds "query_string" object from 'q'.

    :param: q of type String
    :param: default_field
    :return: dictionary object.
    """

    query_string = {'query_string': {'query': q, 'default_operator': 'AND'}}
    query_string['query_string'].update({'lenient': False} if default_field else {'default_field': default_field})

    return query_string
# ---
def baseline_func_phase(self,z_data,f_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		returns the baseline as a function
		the points in between the datapoints are computed by cubic interpolation
		'''
		return interp1d(f_data, self._baseline_als(np.angle(z_data),lam,p,niter=niter), kind='cubic')
# ---
def resolve_axis(axis_spec: AxisSpec, axis_selection: AxisSelector) -> Axis: ...
# ---
def get_runtime_env(self) -> dict:
        """
        Returns the runtime environment to run the evaluator on the Ray cluster.
        """
        return build_runtime_env_for_packages(
            extra=["eval", "tpu"],
            pip_packages=["statsmodels==0.14.4"],
            env_vars={
                "TOKENIZERS_PARALLELISM": "false",
                "HF_DATASETS_TRUST_REMOTE_CODE": "1",
                "HF_ALLOW_CODE_EVAL": "1",
            },
        )
# ---

def choose_num(x, y):
    """This function takes two positive numbers x and y and returns the
    biggest even integer number that is in the range [x, y] inclusive. If
    there's no such number, then the function should return -1.

    For example:
    choose_num(12, 15) = 14
    choose_num(13, 12) = -1
    """
    if x > y:
        return -1
    if y % 2 == 0:
        return y
    if x == y:
        return -1
    return y - 1
# ---
def test_index_with_tracer():
    H, W, D = hax.make_axes(H=20, W=30, D=40)
    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    @jax.jit
    def f(idx):
        return named1["H", idx]

    idx = jnp.array([1, 2, 3])
    assert jnp.all(jnp.equal(f(idx).array, named1.array[1:4, :, :]))

    idx = jnp.array(0)
    assert jnp.all(jnp.equal(f(idx).array, named1.array[0, :, :]))
# ---
def predict(self, documents: list[str]):
        raise NotImplementedError
# ---
def print_ls_l_header():
    print(get_ls_l_header())
# ---
def __init__(self, objs: Iterable[T] = ()):
        self._index_to_obj: list[T] = []
        self._obj_to_index: dict[T, int] = {}
        for obj in objs:
            self.append(obj)
# ---
def copy_process_status(self, process):
        return {
            'id': process.vpnservice['id'],
            'status': process.status,
            'updated_pending_status': process.updated_pending_status,
            'ipsec_site_connections': copy.deepcopy(process.connection_status)
        }
# ---
def test_cache_middleware(self):
        req = Request.blank('/something', environ={'REQUEST_METHOD': 'GET'})
        resp = self.app(req.environ, start_response)
        self.assertTrue('swift.cache' in resp)
        self.assertTrue(isinstance(resp['swift.cache'], MemcacheRing))
# ---
def __getitem__(self, idx):
        return self._calls[idx]
# ---
def time(self):
        """Returns current virtual time."""
        with self._lock:
            return self._current_time
# ---
def deterministic_hash(obj: object) -> int:
    """Compute a deterministic hash for an object."""
    s = msgspec.msgpack.encode(obj, order="deterministic")
    return zlib.adler32(s)
# ---
def __nonzero__(self):
        return self.__bool__()
# ---
def to_fn(self) -> ActivationFunction:
        if self is ActivationFunctionEnum.xielu:
            raise ValueError("xielu is parameterized; use XIELUActivation directly.")
        return TO_FN[self]
# ---
def candidates_for_budget(
        self,
        budget: float,
        seq_len: int = DEFAULT_SEQ_LEN,
    ) -> Iterator[CandidateConfig]:
        """Yield valid candidate training configs for the given FLOP budget.

        This is the main entry point for generating training configurations.
        Implementations should iterate over model architectures and yield
        complete CandidateConfig objects with model, optimizer, batch size,
        and training steps all configured.
        """
        ...
# ---
def get_stop_tokens(tokenizer_name: str):
    """Get stop tokens from tokenizer."""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    return [tokenizer.eos_token_id]
# ---
def resource_spec():
    return cluster_pb2.ResourceSpecProto(cpu=4, memory_bytes=8 * 1024**3, disk_bytes=100 * 1024**3)
# ---
def increment(self) -> None:
        self._count += 1
# ---
def test_less_than(self):
        expr = col("score") < 100
        assert expr.evaluate({"score": 50}) is True
        assert expr.evaluate({"score": 100}) is False
        assert expr.evaluate({"score": 150}) is False
# ---
def test_filter_passing_sorted_by_model_score():
    c1 = _make_candidate("def f(x):\n    return x\n", score=-1.0)
    c2 = _make_candidate("def f(x):\n    return x\n", score=-0.5)
    tests = ["assert f(1) == 1"]
    passing = filter_passing([c1, c2], tests)
    assert len(passing) == 2
    # Higher model score first.
    assert passing[0].score >= passing[1].score
# ---
def delete_previous_logs():
    cmd = 'rm -rf logs/*'
    cr.run_command(cmd)
# ---
def __init__(self, config: InferenceServerConfig, inference_context: InferenceContext, app: FastAPI):
        """Initialize the inference server with pre-built components.

        Use InferenceServer.create() to build a new server instance.
        """
        self.config = config
        self.inference_context = inference_context
        self.app = app
        self._server = None
# ---
def configurable_base(cls):
        return Resolver
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.subqueries == other.subqueries
# ---
def test_unparseable_input_returns_empty():
    assert generate_expression_variants("if x:") == []
# ---
def test_vmap_multiple_axes():
    Batch1 = Axis("Batch1", 4)
    Batch2 = Axis("Batch2", 3)
    Width = Axis("Width", 2)
    Depth = Axis("Depth", 5)

    named = hax.random.uniform(PRNGKey(0), (Batch1, Batch2, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, (Batch1, Batch2))(named)

    expected = jnp.sum(named.array, axis=2)

    assert jnp.allclose(selected.array, expected)
    assert selected.axes == (Batch1, Batch2, Depth)
# ---
def stop(self):
        pass
# ---
def current_attempt(self) -> ControllerTaskAttempt | None:
        """The most recent attempt, or None if no attempts yet."""
        return self.attempts[-1] if self.attempts else None
# ---
def log10(a: A) -> A:
    return wrap_elemwise_unary(jnp.log10, a)
# ---
def test_scan_reports_mismatched_unnamed_array():
    Height = Axis("Height", 2)

    def f(c, x, y):
        return c, x + y

    good = jnp.zeros((Height.size, 3))
    bad = jnp.zeros((Height.size - 1, 3))

    with pytest.raises(ValueError) as e:
        hax.scan(f, Height)(0, good, y=bad)

    assert "y has leading dimension" in str(e.value)
# ---
def execute(conn, clauseelement, multiparams,
                                                    params ):
            stmts.append((str(clauseelement), params, multiparams))
# ---
def text_renderer(node: RenderTreeNode, context: RenderContext) -> str:
        return node.content
# ---
def get_full_name(self):
        return self.nickname
# ---
def test_zero_duration():
    """Zero duration is handled correctly."""
    zero = Duration.from_ms(0)

    assert zero.to_ms() == 0
    assert zero.to_seconds() == 0.0

    # Proto roundtrip
    proto = zero.to_proto()
    restored = Duration.from_proto(proto)
    assert restored.to_ms() == 0
# ---
def test_filter_ips_black_list(self):
        CONF.network_label_regex = '.*'
        CONF.ip_regex = '.*'
        CONF.black_list_regex = '^10.123.123.*'
        ip = self.instance.get_visible_ip_addresses()
        ip = filter_ips(
            ip, CONF.ip_regex, CONF.black_list_regex)
        self.assertEqual(2, len(ip))
        self.assertTrue('10.123.123.123' not in ip)
# ---
def is_inside_jit():
    """Returns True if we're currently inside a jit"""
    return isinstance(jnp.zeros(()), jax.core.Tracer)
# ---
def wake(self) -> None:
        """Signal the controller loop to run immediately.

        Called when events occur that may make scheduling possible:
        - New job submitted
        - New worker registered
        - Task finished (freeing capacity)
        """
        self._wake_event.set()
# ---
def tree_flatten(tree, is_leaf=None):
    """
    Version of [jax.tree_util.tree_flatten][] that automatically treats NamedArrays as leaves.
    """
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: is_leaf(x) or is_named_array(x)

    return jax.tree_util.tree_flatten(tree, is_leaf=is_leaf)
# ---
def test_brackets_balanced_nested():
    assert brackets_balanced("f(g[h({x})])")
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetGroupList(0), ['011', '012', '013', '014', '015', '016', '017', '018', '019'])
    self.assertEqual(self.schedule.GetGroupList(1), ['021', '022', '023', '024', '025', '026', '028'])
    self.assertEqual(self.schedule.GetGroupList(3), ['041', '042'])
    self.assertEqual(self.schedule.GetGroupList(8), ['0111', '0112', '0113', '0114'])
# ---
def update(self, resource, id_, updates):
        args = self._es_args(resource, refresh=True)
        return self.es.update(id=id_, body={'doc': updates}, **args)
# ---
def testIfElse(self):
    self.assertEqual((0, 'foo\nbar\n'), _GrumpRun(textwrap.dedent("""\
        if True:
          print 'foo'
        else:
          print 'bar'
        if False:
          print 'foo'
        else:
          print 'bar'""")))
# ---
def test_special_token_ids_are_distinct(tok):
    ids = {tok.pad_token_id, tok.sos_token_id, tok.eos_token_id}
    assert len(ids) == 3
# ---
def _create_thread(self, target=None, **kwargs):
        return IonProcessThread(target=target, heartbeat_secs=self.heartbeat_secs, **kwargs)
# ---
def test_dialect_engine_construction_options(self):
        dialect = Mock()
        engine = Engine(Mock(), dialect, Mock(),
                                execution_options={"foo": "bar"})
        eq_(
            dialect.set_engine_execution_options.mock_calls,
            [call(engine, {"foo": "bar"})]
        )
# ---
def _get_num_train_steps(param_count: int, batch_size: int, max_seq_len: int, tpp: int = 20) -> int:
    total_tokens = param_count * tpp
    return max(1, total_tokens // (batch_size * max_seq_len))
# ---
def unique_values(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> NamedArray:
    """Shortcut for :func:`unique` that returns only unique values."""

    return typing.cast(
        NamedArray,
        unique(
            array,
            Unique,
            axis=axis,
            fill_value=fill_value,
        ),
    )
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "x": 1, "y": 2, "q": "q1", "p": "p3"},
                {"id": 2, "x": 2, "y": 3, "q": "q2", "p": "p2"},
                {"id": 3, "x": 3, "y": 4, "q": "q3", "p": "p1"},
            ],
        )
# ---
def _trans_fn(self, is_transaction=False):
        def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
        return go
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n), 'B': np.random.ranf(n)})
            Ac = df.A.rolling(3).sum()
            return Ac.sum()
# ---
def list_tasks(self) -> list[TaskInfo]:
        """List all tasks.

        Returns TaskInfo views (implemented by TaskAttempt) to decouple callers
        from execution internals.
        """
        return list(self._tasks.values())
# ---
def test_empty_iteration(max_capacity):
    # Create a BackgroundIterable instance with an empty producer function
    background_iterable = BackgroundIterable(lambda: iter([]), max_capacity=max_capacity)

    # Convert the iterator to a list for comparison
    data = list(background_iterable)

    # Assert that the produced data is empty
    assert data == []
# ---
def __call__(self, x):
            return self.down_proj(self.up_proj(x))
# ---
def _ensure_encoding(possibly_bytes):
    return possibly_bytes.decode('utf-8') if type(possibly_bytes) is bytes else possibly_bytes
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("data", String(100)),
        )
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "RotaryEmbeddingsConfig":
        return DefaultRotaryEmbeddingsConfig(theta=rope_theta, factor=config.get("factor", 1.0))
# ---
def style_css(self, val):
        self._style_css = '' + val
# ---
def run_sh(cmd: str, **kwargs) -> subprocess.CompletedProcess:
    """Run command from string with logging."""
    return run(shlex.split(cmd), **kwargs)
# ---
def test_logistic():
    check_gen_is_equal(lambda k, s: jax.random.logistic(k, s), lambda k, s: hax.random.logistic(k, s))
# ---
def output_exemplar(self):
        return BatchEncoding({})
# ---
def test_copy_experiment_wrong_course(self):
        """ Tests that copy_experiment fails if experiment is different coruse """
        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)
        url = reverse("ab_testing_tool_copy_experiment", args=(experiment.id,))
        response = self.client.post(url, follow=True)
        self.assertError(response, UNAUTHORIZED_ACCESS)
# ---
def is_slotted(self):
        return self.slotted
# ---
def test_with_args(self, mock_start, mock_stop):
        self.assertEqual(1, traced_func(1))
        expected_info = {
            "info": "some_info",
            "function": {
                "name": "osprofiler.tests.unit.test_profiler.traced_func",
                "args": str((1,)),
                "kwargs": str({})
            }
        }
        mock_start.assert_called_once_with("function", info=expected_info)
        mock_stop.assert_called_once_with()
# ---
def _Create(self):
    """Create the log group."""
    create_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'create-log-group',
        '--log-group-name', self.name
    ]
    vm_util.IssueCommand(create_cmd)
# ---
def __init__(self, directory=None):
        if directory is None:
                directory = []
        import copy
        self.directory = copy.deepcopy(directory)
        self.bound = False
        self.start_tls_called = False
        self.extend = self.Extend(self)

        self.operation = {
                    "!" : self._search_not,
                    "&" : self._search_and,
                    "|" : self._search_or,
            }
# ---
def init_fn(params):
        # Initialize mean_square and momentum buffers to zero trees
        mean_square = otu.tree_zeros_like(params)
        momentum_buf = otu.tree_zeros_like(params)
        return ScaleByRMSPropMomState(
            count=jnp.zeros([], jnp.int32),
            mean_square=mean_square,
            momentum=momentum_buf,
        )
# ---
def add(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.add](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.add.html)
    """
    return jnp.add(x1, x2)
# ---
def cli():
    """TPU CI Infrastructure Management - Manage preemptible TPU VMs for GitHub Actions CI."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        stream=sys.stderr,
    )
# ---
def __init__(self, urls):
        self.urls = urls
        # Force materialization early so duplicate shard names surface immediately.
        _ = self._shard_name_to_url_mapping
# ---
def test_get_lines_with_long_string():
        assert len(get_lines("a"*(2*LINEWIDTH-1))) == 2
# ---
def user_linked_in_url(self):
        return self._get_profile().linked_in_url
# ---
def synthetic_batch_iterator(
    *,
    rng: jax.Array,
    batch_size: int,
    seq_len: int,
    vocab_size: int,
) -> Iterator[dict[str, jax.Array]]:
    """Infinite generator of random token/label pairs."""

    def _step(key: jax.Array) -> dict[str, jax.Array]:
        tokens = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)
        return {"tokens": tokens[:, :-1], "labels": tokens[:, 1:]}

    while True:
        rng, key = jax.random.split(rng)
        yield _step(key)
# ---
def serve():
    if os.environ.get("DEV") == "true":
        return proxy_to_dev_server("")
    return send_from_directory(app.static_folder, "index.html")
# ---
def add_ms(self, milliseconds: int) -> "Timestamp":
        """Return new timestamp offset by milliseconds (may be negative)."""
        return Timestamp(self._epoch_ms + milliseconds)
# ---
def vm(ctx):
    """VM management commands (via controller RPC)."""
    pass
# ---
def _make_unique_job_id(client, run_id):
    job_id = run_id
    try:
        while client.get_job_status(job_id) is not None:
            job_id = f"{run_id}-{time.time_ns()}"
    except Exception as e:  # noqa
        if "does not exist" in str(e):
            pass
        else:
            raise
    return job_id
# ---
def _filter_weights_to_components(weights: dict[str, float]) -> dict[str, float]:
    """Return a copy of weights keeping only keys present in components.

    This avoids passing weights for datasets that aren't included in the mixture,
    which Levanter correctly rejects.
    """
    allowed = set(components.keys())
    return {k: v for k, v in weights.items() if k in allowed}
# ---
def address(self):
        """Get the full address the server is running on."""
        for server in self._server.servers:
            for sock in server.sockets:
                addr = sock.getsockname()
                host, port = addr[0], addr[1]
                # handle weird ipv6 localhost address which confuses clients
                if host == "::1" or host == ":1":
                    host = "localhost"
                return f"{host}:{port}"
        return None
# ---
def __exit__(self, *args):
        problems = []
        for cmanager in reversed(self._cmanagers):
            try:
                cmanager.__exit__(*args)
            except Exception as e:
                problems.append(e)

        self._cmanagers = []

        if len(problems) > 0:
            raise RuntimeError("Exception(s) occurred while exiting trainer", problems) from problems[0]
# ---
def zeros_like(total: Arrayish, per_tag: Arrayish) -> "_EvalRunningMeans":
        z = RunningMean.zeros_like(total)
        per_tag = RunningMean.zeros_like(per_tag)
        return _EvalRunningMeans(z, per_tag, z, per_tag)
# ---
def unregister_old(bl_id):
    global imported_mods
    mod = imported_mods.get(bl_id)
    if mod:
        #print("Unloaded old node type {}".format(bl_id))
        mod.unregister()
        del imported_mods[bl_id]
# ---
def test_update_overflow_error():
    # Overflow: scalar start + update too large for axis â†’ ValueError
    Seq = hax.Axis("seq", 4)
    arr = hax.zeros((Seq,), dtype=int)
    # update of length 3 starting at pos=2 would run off the end (2+3 > 4)
    upd = hax.arange((hax.Axis("seq", 3),), dtype=int)

    with pytest.raises(ValueError):
        updated_slice(arr, {"seq": 2}, upd)
# ---
def is_good(n):

    return 1 + ((int(n) - 1) % 9) == 9
# ---
def __delete__(self, instance):
		del(instance.__dict__[self.field.name])
		setattr(instance, self.field.attname, json.dumps(None))
# ---
def _succeeded_job():
    print("Snapshot succeeded job completed.")
    return "ok"
# ---
def open_file_like(self, file_like):
        '''Assign a file-like object, such as those provided by StringIO, as an open file object.
        >>> from io import StringIO
        >>> fileLikeOpen = StringIO()
        >>> mf = MidiFile()
        >>> mf.open_file_like(fileLikeOpen)
        >>> mf.close()
        '''
        self.file = file_like
# ---
def test_ckpt_path_with_trailing_slash():
    path = "checkpoints/llama-8b-tootsie-phase2/checkpoints/step-730000/"
    assert ckpt_path_to_step_name(path) == "llama-8b-tootsie-phase2-730000"
# ---
def name(self):
        """Return the name of the sensor."""
        return f"{self.client_name} {self._name}"
# ---
def truncate_model_name(model_name: str, max_length: int = 62) -> str:
    """Truncate model name to max_length if it exceeds that length."""
    return model_name[:max_length] if len(model_name) > max_length else model_name
# ---
def testImport(self):
    self.assertEqual((0, "<type 'dict'>\n"), _GrumpRun(textwrap.dedent("""\
        import sys
        print type(sys.modules)""")))
# ---
def __init__(self, shortname, loader):
        # Not preloaded
        # loaders must produce dictionaries (or an appropriate iterable)
        # with the required keys.
        # The reason for this is that code for certain servers need not be loaded
        # if it's not going to be used at all
        # It also prevents import loop collisions.
        global __ServerImplementationDict
        self.__data = ServerImplementationDict(loader)
        self.__shortname = shortname
# ---
def test_evaluate_integer(self):
        expr = lit(42)
        assert expr.evaluate({}) == 42
# ---
def __repr__(self):
        return '<{0.__class__.__name__!s} {0.verb}>'.format(self)
# ---
def test_original_excluded():
    variants = generate_expression_variants("a + b")
    assert "a + b" not in variants
# ---
def eval_batch_size(self):
        return self.per_device_eval_parallelism * self.data_axis_size
# ---
def unwrap_metrics(pytree):
    """
    Walk a pytree and extract .value() from all Metric objects.
    """

    def _unwrap(x):
        if isinstance(x, Metric):
            return x.value()
        return x

    return jax.tree_util.tree_map(_unwrap, pytree, is_leaf=lambda x: isinstance(x, Metric))
# ---
def test_run_task_with_ports(worker):
    """Test run_task allocates ports correctly."""
    task_id = JobName.root("job-with-ports").task(0).to_wire()
    request = create_run_task_request(task_id=task_id, ports=["http", "grpc"])
    worker.submit_task(request)

    # Verify ports were allocated
    task = worker.get_task(task_id)
    assert len(task.ports) == 2
    assert "http" in task.ports
    assert "grpc" in task.ports
# ---
def __repr__(self):
        return "<includematcher includes=%r>" % self._pats
# ---
def __del__(self):
        if hasattr(self, "_batches") and hasattr(self._batches, "stop"):
            self._batches.stop()
# ---
def intermediate(self, x, y, z, *, static1, static2):
            assert static1 is True
            assert static2 is False
            return x + 2 * self.w + y + z
# ---
def test_add_literal(self):
        expr = col("a") + 10
        assert expr.evaluate({"a": 5}) == 15
# ---
def chip_count(self) -> int:
        return self.device.chip_count()
# ---
def _initialize_jax_config(self):
        for key, value in self.jax_config.items():
            jax.config.update(key, value)

        if self.jax_compilation_cache_dir is not None:
            jax.config.update("jax_compilation_cache_dir", self.jax_compilation_cache_dir)
# ---
def _join_list_field(value: Any) -> str | None:
    if isinstance(value, list):
        text_items = [str(item) for item in value if item is not None]
        if text_items:
            return "\n".join(text_items)
    return None
# ---
def _delete_vm(name: str, zone: str, project: str) -> bool:
    result = _run_gcloud(["compute", "instances", "delete", name, f"--project={project}", f"--zone={zone}", "--quiet"])
    if result.returncode != 0:
        error = result.stderr.strip()
        if "not found" in error.lower():
            click.echo(f"  VM {name} already deleted")
            return True
        click.echo(f"  Failed to delete VM {name}: {error}", err=True)
        return False
    return True
# ---
def cleanup(self) -> None:
        """Cleanup transfer server and thread pool."""
        logger.info("Cleaning up JAX transfer server")
        self.executor.shutdown(wait=True)
        self.transfer_server = None
# ---
def LineAndColumnAfterLastNonWhitespace():
  line, column = CurrentLineAndColumn()
  line_value = vim.current.line[ :column ].rstrip()
  while not line_value:
    line = line - 1
    if line == -1:
      return None
    line_value = vim.current.buffer[ line ].rstrip()
  return line, len( line_value )
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        return self.device_flops(dtype) * self.chip_count()
# ---
def test_create_coinbase(self):
        height = 20
        coinbase_tx = create_coinbase(height=height)
        assert_equal(CScriptNum.decode(coinbase_tx.vin[0].scriptSig), height)
# ---
def _return_value():
    return 42
# ---
def test_cluster(use_docker, docker_cleanup_scope):
    """Provide a running test cluster for E2E tests."""
    with E2ECluster(use_docker=use_docker) as cluster:
        yield cluster
# ---
def test_column_map_arg(self):
        def test_impl(df):
            df['B'] = df.A.map(lambda a: 2 * a)
            return

        n = 121
        df1 = pd.DataFrame({'A': np.arange(n)})
        df2 = pd.DataFrame({'A': np.arange(n)})
        hpat_func = self.jit(test_impl)
        hpat_func(df1)
        self.assertTrue(hasattr(df1, 'B'))
        test_impl(df2)
        np.testing.assert_equal(df1.B.values, df2.B.values)
# ---
def expired(self) -> bool:
        """Check if deadline has passed."""
        return time.monotonic() >= self._deadline
# ---
def reset_fray_context():
    """Reset fray context between tests for isolation."""
    _job_context.set(None)
    yield
    _job_context.set(None)
# ---
def __call__(self, input_ids: NamedArray, *, key: PRNGKeyArray | None = None):
        """Alias for `embed`. key is ignored."""
        return self.embed(input_ids)
# ---
def yield_1(it):
        yield from [1]
# ---
def test_and_short_circuit(self):
        expr = (col("a") > 0) & (col("b") > 0)
        # If a <= 0, b is not evaluated (short-circuit)
        assert expr.evaluate({"a": -1, "b": 1}) is False
# ---
def _tree_byte_size(tree) -> int:
    """Return the per-device number of bytes represented by ``tree``."""

    return sharded_tree_size(tree)
# ---
def _bos_tokens(self) -> list[int]:
        return self.tokenizer.encode("<|begin_of_text|>", add_special_tokens=False)
# ---
def test_init_params_shapes(params, tiny_cfg):
    assert params.token_embed.shape == (tiny_cfg.vocab_size, tiny_cfg.hidden_dim)
    assert params.output_proj.shape == (tiny_cfg.hidden_dim, tiny_cfg.vocab_size)
    assert params.final_norm.shape == (tiny_cfg.hidden_dim,)
    assert len(params.blocks) == tiny_cfg.num_layers
# ---
def __init__(
        self,
        host_alias: str,
        command_str: str,
        sync_path: str,
        env_dict: dict[str, str] | None = None,
    ):
        self._lock = threading.Lock()
        self._process: subprocess.Popen | None = None
        self._command_str = command_str
        self._host_alias = host_alias
        self._sync_path = sync_path
        self._env_dict = env_dict or {}
# ---
def from_seconds(cls, timeout_seconds: float) -> "Deadline":
        """Create deadline from seconds in the future."""
        return cls(time.monotonic() + timeout_seconds)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.quantile(.25)
# ---
def from_hf_config(cls, hf_config: HfConfig):
        pass
# ---
def _l2norm(x: NamedArray, axis: hax.AxisSelector, eps: float = 1e-6) -> NamedArray:
    """L2-normalize x along a named axis.

    Args:
        x: NamedArray of any shape.
        axis: the single axis to normalize along (e.g., the head dimension Dk).
    """
    x32 = x.astype(jnp.float32)
    inv = hax.rsqrt(hax.sum(hax.square(x32), axis=axis) + jnp.asarray(eps, dtype=jnp.float32))
    return (x32 * inv).astype(x.dtype)
# ---
def test_select_columns(self):
        with config.db.connect() as conn:
            res = conn.execute(
                select(
                    [self.tables.square.c.area, self.tables.square.c.perimeter]
                )
                .select_from(self.tables.square)
                .order_by(self.tables.square.c.id)
            ).fetchall()
            eq_(res, [(100, 40), (1764, 168)])
# ---
def alive_threads(self) -> list[ManagedThread]:
        """Return threads that are still alive, including those in child containers."""
        with self._lock:
            threads = list(self._threads)
            children = list(self._children)

        alive = [t for t in threads if t.is_alive]
        for child in children:
            alive.extend(child.alive_threads())
        return alive
# ---
def is_found(hit):
            if 'exists' in hit:
                hit['found'] = hit['exists']
            return hit.get('found', False)
# ---
def num_gpus(self) -> int:
        return 0
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.field == other.field and \
            self.ascending == other.ascending
# ---
def test_stop(self):
        p = profiler.init("secret", base_id="1", parent_id="2")
        p.stop = mock.MagicMock()
        profiler.stop(info="info")
        p.stop.assert_called_once_with(info="info")
# ---
def __init__(self, exemplar):
        self.exemplar = exemplar
# ---
def init(cls, Vocab: Axis, config: Gpt2Config, *, key) -> "Gpt2LMHeadModel":
        k_t, k_embeddings = jrandom.split(key, 2)
        transformer = Gpt2Transformer.init(config, key=k_t)
        embeddings = Gpt2Embeddings.init(Vocab, config, key=k_embeddings)

        return Gpt2LMHeadModel(transformer, embeddings)
# ---
def wait(self, timeout: float = 60.0) -> str | None:
        """Wait for completion and return result."""
        self._done.wait(timeout=timeout)
        return self.result
# ---
def make_job_request():
    """Create a minimal LaunchJobRequest for testing."""

    def _make(name: str = "test-job") -> cluster_pb2.Controller.LaunchJobRequest:
        return cluster_pb2.Controller.LaunchJobRequest(
            name=name,
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=1024**3),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )

    return _make
# ---
def from_state_dict(cls, config: GatedDeltaNetConfig, state: dict[str, jnp.ndarray], *, key) -> "GatedDeltaNet":
        """
        Build a fresh layer from config + state dict.
        """
        layer = cls.init(config, key=key)
        return layer.load_state_dict(state)
# ---
def default_view_class(self):
		"""ã†ã•ãŽã®ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ“ãƒ¥ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã‚’å¿œç­”ã™ã‚‹ã€‚"""
		if TRACE: print(__name__), self.default_view_class.__doc__

		return BunnyView
# ---
def _make(name: str = "test-job") -> cluster_pb2.Controller.LaunchJobRequest:
        return cluster_pb2.Controller.LaunchJobRequest(
            name=name,
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=1024**3),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )
# ---
def remove_duplicates(self):
        if self.ui.chkBRemoveDuplicates.isChecked():
            for lbl in self.message.message_type:
                seq = lbl.fuzz_values[:]
                seen = set()
                add_seen = seen.add
                lbl.fuzz_values = [l for l in seq if not (l in seen or add_seen(l))]
# ---
def frame_back_step(self):
        self.command('frame_back_step')
# ---
def test_wait_create_table(self):
        """Create table shall wait for the table to come online."""
        tablename = "foobar_wait"
        hash_key = DynamoKey("id")
        self.dynamo.create_table(tablename, hash_key=hash_key, wait=True)
        self.assertIsNotNone(self.dynamo.describe_table(tablename))
# ---
def eval_data(self) -> Iterator[MockEnvExample]:
        """Stream evaluation data."""
        for example in self.eval_examples:
            yield MockEnvExample(
                raw_prompt=example["prompt"],
                raw_answer=example["answer"],
                processed_prompt=example["prompt"],
                processed_answer=example["answer"],
                metadata={"task_type": self.task_type},
            )
# ---
def setup_clients(cls):
        super(MigrationsAdminTest, cls).setup_clients()
        cls.client = cls.os_admin.migrations_client
# ---
def __rlshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.left_shift(other, self)
# ---
def leaky_relu(a: A) -> A:
    return wrap_elemwise_unary(jnn.leaky_relu, a)
# ---
def _fromUtf8(s):
        return s
# ---
def main(pegasus_share_dir):
    if len(sys.argv) != 2:
        usage()
        exit(1)

    if "-h" in sys.argv:
        usage()
        exit(1)

    workflowdir = sys.argv[1]
    if os.path.exists(workflowdir):
        print("ERROR: WORKFLOW_DIR '%s' already exists" % workflowdir)
        exit(1)

    workflowdir = os.path.abspath(workflowdir)
    sharedir = os.path.join(pegasus_share_dir, "init")
    w = Workflow(workflowdir, sharedir)
    w.configure()
    w.generate()
# ---
def _read_parquet(path: Path) -> list[dict]:
    """Read records from a parquet file."""
    import pyarrow.parquet as pq

    table = pq.read_table(path)
    return table.to_pylist()
# ---
def add_hook(self, *, every: int = 1): ...
# ---
def convert_to_export(self, value, env):
        if value or value == 0:
            return value if env.context.get('export_raw_data') else ustr(value)
        return ''
# ---
def get_message(self):
        xpath = '//div[@class="bienvenueMdp"]/following-sibling::div'
        return '%s%s' % (CleanText(xpath + '/strong')(self.doc), CleanText(xpath, children=False)(self.doc))
# ---
def buy(self, owner, data,
            size, price=None, plimit=None,
            exectype=None, valid=None):

        order = BuyOrder(owner=owner, data=data,
                         size=size, price=price, pricelimit=plimit,
                         exectype=exectype, valid=valid)

        return self.submit(order)
# ---
def init_fn(key):
        return hax.nn.MLP.init(In, Out, 2, 3, key=key)
# ---
def max_seq_len(self) -> int:
        return self.core.max_seq_len
# ---
def get_result_proxy(self):
                return cls(self)
# ---
def scan_checkpoint_policy_decode(policy: str | dict | bool):
        if not isinstance(policy, dict):
            return ScanCheckpointPolicy.from_bool_or_str(policy)

        from draccus.parsers.decoding import decode_dataclass

        return decode_dataclass(ScanCheckpointPolicy, policy)
# ---
def push(self, item):
        """Push an element on the priority queue.

        The element is pushed on the priority queue according
        to its priority.

        Parameters
        ----------
        item :
            The element to push on the queue.

        """
        heapq.heappush(self._queue, (self.func(item), item))
# ---
def rms_norm(x: Float[Array, "... D"], weight: Float[Array, "D"], eps: float) -> Float[Array, "... D"]:
    """RMS normalization.

    Kept local rather than delegating to Grug's version because Grug's
    calls unshard(weight) which requires a JAX mesh context.
    """
    dtype = x.dtype
    x = x.astype(jnp.float32)
    variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
    normed = x * jax.lax.rsqrt(variance + eps)
    out = normed * weight
    return out.astype(dtype)
# ---
def worker_metadata():
    return cluster_pb2.WorkerMetadata(
        hostname="test-host",
        ip_address="192.168.1.1",
        cpu_count=8,
        memory_bytes=16 * 1024**3,
        disk_bytes=100 * 1024**3,
    )
# ---
def run_buf_generate(root_dir: Path) -> None:
    """Run buf generate using npx."""
    print("Running npx buf generate...")
    result = subprocess.run(
        ["npx", "--yes", "@bufbuild/buf", "generate"],
        cwd=root_dir,
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        print(f"Error running buf generate:\n{result.stderr}", file=sys.stderr)
        sys.exit(1)

    print("âœ“ buf generate completed successfully")
# ---
def get_bandwidth_range(self):
		return self.bandwidth_range
# ---
def _balance_Q(Q):
                        norms = jnp.array([jnp.max(jnp.abs(q)) for q in Q], dtype=jnp.float32)
                        gmean = jnp.exp(jnp.mean(jnp.log(norms)))
                        to_mul = gmean / norms
                        return [q * x.astype(q.dtype) for q, x in zip(Q, to_mul)]
# ---
def __post_init__(self):
            if self.kwargs is None:
                self.kwargs = {}
# ---
def test_column_map(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df['B'] = df.A.map(lambda a: 2 * a)
            return df.B.sum()

        n = 121
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# ---
def on_mouse_enter(x, y):
            """The mouse was moved into the window.

            This event will not be trigged if the mouse is currently being
            dragged.

            :Parameters:
                `x` : int
                    Distance in pixels from the left edge of the window.
                `y` : int
                    Distance in pixels from the bottom edge of the window.

            :event:
            """
# ---
def test_trivial(self):
        assert isinstance(self.db, smadata2.db.base.BaseDatabase)
# ---
def test_summary(bank):
    summary = bank.summary()
    assert "SubtreeBank" in summary
    assert "entries" in summary
# ---
def remove_matching_braces(s, brace_open="{", brace_close="}"):
    """Remove content up to and including the first matching brace pair."""
    end_pos = find_matching_brace(s, 0, brace_open, brace_close)
    return s[: end_pos + 1] if end_pos is not None else s
# ---
def load_data(self):  # pragma: no cover
        warnings.warn('the Dataset method `load_data` has been deprecated; '
                      'use `load` instead',
                      FutureWarning, stacklevel=2)
        return self.load()
# ---
def test_logical_and(self):
        expr = (col("score") > 0.5) & (col("category") == "A")
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def round_axis_for_partitioning(axis: Axis, mapping: ResourceMapping | None = None) -> Axis:
    """Round an axis so that it's divisible by the size of the partition it's on"""
    size = physical_axis_size(axis, mapping)
    if size is None:
        return axis
    else:
        new_size = (axis.size + size - 1) // size * size
        return Axis(axis.name, new_size)
# ---
def assert_diff_version(**kwargs):
            output_path = get_output_path(**(defaults | kwargs))
            assert output_path != default_output_path
# ---
def _compute_learning_rate(self, batch_size: int, hidden_dim: int) -> float:
        """Compute learning rate from batch size and hidden dim."""
        return (self.lr_constant * math.sqrt(batch_size)) / hidden_dim
# ---
def custom_validate_repo_id(repo_id):
            if _is_url_like(repo_id):
                return
            return original_validate_repo_id(repo_id)
# ---
def _index_of_batch_axis(array, default):
        if isinstance(array, NamedArray):
            return array.axis_indices(axis)
        elif callable(default):
            return default(array)
        else:
            return default
# ---
def compute(model, input_ids):
            return model(input_ids, attn_mask=attn_mask)
# ---
def controller_address(self) -> str:
        """Get controller address from bootstrap config, if set.

        Returns:
            Controller address string, or empty string if not configured
        """
        # TODO: Derive controller address from controller.manual/local when unset.
        bootstrap = self._proto.defaults.bootstrap
        if bootstrap.HasField("controller_address"):
            return bootstrap.controller_address
        return ""
# ---
def newFinal(self, name):
    pass
# ---
def the_object_name_is_not_voided(name):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(element.HasOpenings):
        assert False, "An opening was found"
# ---
def explicit(mask: NamedArray) -> "AttentionMask":
        return AttentionMask(is_causal=False, causal_offset=None, explicit_mask=mask)
# ---
def evaluation_interval_seconds(self) -> float:
        """Configured evaluation interval in seconds."""
        return self._evaluation_interval.to_seconds()
# ---
def show_image(self, image_id):
        """Returns the details of a single image."""
        resp, body = self.get("images/%s" % str(image_id))
        self.expected_success(200, resp.status)
        body = json.loads(body)
        self.validate_response(schema.get_image, resp, body)
        return service_client.ResponseBody(resp, body['image'])
# ---
def getTransformation( self, transName, extraParams = False, rpc = '', url = '', timeout = None ):
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.getTransformation( transName, extraParams )
# ---
def argmin(self, axis: AxisSelector | None) -> "NamedArray":  # pragma: no cover
        return haliax.argmin(self, axis=axis)
# ---
def k(self):
        html = "<h2>Stopping</h2>"
        killall()
        return html
# ---
def _add_missing_coords_inplace(self):
        """Add missing coordinates to self._variables
        """
        for dim, size in iteritems(self.dims):
            if dim not in self._variables:
                # This is equivalent to np.arange(size), but
                # waits to create the array until its actually accessed.
                data = indexing.LazyIntegerRange(size)
                coord = Coordinate(dim, data)
                self._variables[dim] = coord
# ---
def fail():
                raise ValueError("intentional error")
# ---
def __getattribute__(self, name):
        try:
            return object.__getattribute__(self, name)
        except AttributeError:
            parent = object.__getattribute__(self, '_parent')
            if parent is None:
                default_values = object.__getattribute__(self, 'default_values')
                if name in default_values:
                    return default_values[name]
                raise
            return getattr(parent, name)
# ---
def _list_to_dict(list_, key_label='key'):
    """
    Transforms a list of dictionaries into a dictionary of dictionaries.

    Original dictionaries are assigned key specified in each of them
    by key_label.
    """
    dict_ = {}
    for item in list_:
        dict_[item[key_label]] = item
    return dict_
# ---
def parse_query(query):
    q = {'mode': 'main'}
    if query.startswith('?'): query = query[1:]
    queries = urlparse.parse_qs(query)
    for key in queries:
        if len(queries[key]) == 1:
            q[key] = queries[key][0]
        else:
            q[key] = queries[key]
    return q
# ---
def _validate_hello():
        print("Reload validation job OK")
        return 42
# ---
def resetfirst():
            nonlocal first
            first = True
# ---
def _dtype_name(dtype: Optional[jnp.dtype]) -> Optional[str]:
    if dtype is None:
        return None
    return jnp.dtype(dtype).name
# ---
def query(self, *, prefix: str | None = None, limit: int = 200) -> list[BufferedLogRecord]:
        with self._lock:
            items = list(self._buffer)
        if prefix:
            items = [r for r in items if r.logger_name.startswith(prefix)]
        return items[-limit:]
# ---
def _identity_loss_weight(loss_weight: np.ndarray) -> np.ndarray:
    return loss_weight
# ---
def _parse_fastavro(self):
        """Convert parsed Avro schema to fastavro format."""
        self._parse_avro_schema()
        self._fastavro_schema = fastavro.parse_schema(self._avro_schema_json)
# ---
def testAssignAttribute(self):
    self.assertEqual((0, '123\n'), _GrumpRun(textwrap.dedent("""\
        e = Exception()
        e.foo = 123
        print e.foo""")))
# ---
def test_or_one_true(self):
        expr = (col("a") > 0) | (col("b") > 0)
        assert expr.evaluate({"a": 1, "b": -1}) is True
        assert expr.evaluate({"a": -1, "b": 1}) is True
# ---
def _copy_target_attributes(self):
    """Override `_copy_target_attributes` to exclude `provides`."""
    return [a for a in super(GoThriftGen, self)._copy_target_attributes if a != 'provides']
# ---
def anypats(self):
        """None of .always(), .isexact(), and .prefix() is true --
        optimizations will be difficult."""
        return not self.always() and not self.isexact() and not self.prefix()
# ---
def test_csv(self):
        body, mime_type = self._run_handler(
            EXPERIMENT, SESSION_GROUPS, download_data.OutputFormat.CSV
        )
        self.assertEqual("text/csv", mime_type)
        self.assertEqual(EXPECTED_CSV, body)
# ---
def get_root_input_type_from_json(data):
    """Return the root input type from JSON formatted string."""
    return parse_format(json.loads(data))
# ---
def _tokenize_batches(
    *, ctx: JobContext, tokenizer_ref: Any, config: TokenizeConfig | HfTokenizeConfig, batches: Iterator[dict]
) -> Iterator[dict]:
    """Tokenize a list of batches using the specified tokenizer and format."""
    tokenizer: transformers.PreTrainedTokenizer = ctx.get(tokenizer_ref)
    batch_processor = preprocessor_for_format(config.format, tokenizer)

    for batch in batches:
        yield from batch_processor(batch)
# ---
def eval_data(self) -> Iterator[DataExample]:
        test_dataset = _get_hendrycks_math_test()

        for idx, item in enumerate(test_dataset):
            raw_prompt = item["problem"]
            raw_answer = item["solution"]
            example_id = f"test_{idx}"
            yield self.clean_example(raw_prompt, raw_answer, example_id)
# ---
def empty_queue_space(self) -> jnp.ndarray:
        """How many tokens can be enqueued in the queue."""
        return self.queued_tokens.axis_size("position") - self.num_queued_tokens
# ---
def __getitem__(self, index: int) -> T_co:
        return self.get_batch([index])[0]
# ---
def run_aggregate_total(config: AggregateConfig) -> str:
    logger.info(f"Starting train-test overlap aggregation with config: {config}")
    aggregate_total(config)
    logger.info(f"Aggregation completed! Results written to {config.output_path}")
    return config.output_path
# ---
def _interleave_kv(new_k, new_v):
    # [T, H, D] x2 -> [T, 2H, D] with (k0,v0,k1,v1,...) along heads
    T, H, D = new_k.shape
    return jnp.stack([new_k, new_v], axis=2).reshape(T, 2 * H, D)
# ---
def test_llama_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def get_currency_id(self, cr, uid, picking):
        return False
# ---
def head_size(self) -> int:
        if self.head_dim is not None:
            return self.head_dim
        return self.Embed.size // self.num_heads
# ---
def terminate_job(self, job_id: JobName) -> None:
        self._remote_client.terminate_job(job_id)
# ---
def connect_ftp(self, user, passwd, host, port, dirs,
                            timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
                self.user, self.passwd = user, passwd
                self.host, self.port = host, port
                self.dirs = dirs
                self.ftpwrapper = MockFTPWrapper(self.data)
                return self.ftpwrapper
# ---
def _detect_nvidia_gpu_environment() -> bool:
    for key in ("NVIDIA_VISIBLE_DEVICES", "CUDA_VISIBLE_DEVICES"):
        value = os.environ.get(key)
        if not value:
            continue
        if value:
            return True
    return bool(glob.glob("/dev/nvidia[0-9]*"))
# ---
def __init__(self, default_factory=lambda: None):
            self.__factory = default_factory
# ---
def reset(self) -> None:
        """Free all local sequence slots and reset to the initial `DecodeState`.

        Keeps the KV cache memory allocated. Reuses current `PageTable` object with pages freed.
        """
        self.gen_state = eqx.filter_jit(self.gen_state.reset, donate="all")()
        self.free_slots = list(range(int(self.gen_state.decode_state.max_seqs)))
        self.local_map.clear()
        self.sequences.clear()
        self.results = {}
# ---
def files(self):
        if self.parsed_files is None:
            self.form  # compute form to get files

        return self.parsed_files
# ---
def do_init(self, e):
        self.remote.do_init(self.get_match())
# ---
def deg2rad(a: A) -> A:
    return wrap_elemwise_unary(jnp.deg2rad, a)
# ---
def __create_action(self, main_root):
		action_dir = os.path.join(main_root, 'action')
		safe_mkdir(action_dir)

		actions = set()
		for unit, param in self.__task.data['workflow'].items():
			actions.add(param['type'])
		self.__action_mgr.create_actions(actions, action_dir)
# ---
def test_simple_many(self, tmpfile):
        for i in range(10):
            self.test_simple(tmpfile)
# ---
def config_dump(self):
        path = bottle.request.body.getvalue().decode('utf-8')
        self.config.dump(path)
# ---
def test_transaction_tlocal_engine_ctx_rollback(self):
        fn = self._trans_rollback_fn()
        engine = engines.testing_engine(options=dict(
                                strategy='threadlocal',
                                pool=testing.db.pool))
        ctx = engine.begin()
        assert_raises_message(
            Exception,
            "breakage",
            testing.run_as_contextmanager, ctx, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def __unicode__(self):
		return nom_estado
# ---
def _quick():
    return 1
# ---
def tracker1(name):
            def go(*args, **kw):
                canary1.append(name)
            return go
# ---
def _():
        counts_ref[...] = jnp.zeros_like(counts_ref)
# ---
def __enter__(self):
        gui, backend = self.shell.enable_matplotlib(self.new_backend)
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            df.A.fillna(5.0, inplace=True)
            return df.A.sum()
# ---
def axis_name(ax: Sequence[AxisSelector]) -> tuple[str, ...]:  # type: ignore
    ...
# ---
def accelerator_descriptor(config: ResourceConfig) -> str | None:
    """Get accelerator type string (e.g., 'v4-8', 'H100') for logging/tracking."""
    if isinstance(config.device, TpuConfig):
        return config.device.variant
    elif isinstance(config.device, GpuConfig):
        return config.device.variant
    return None
# ---
def __eq__(self, other):
        """Equality."""
        return isinstance(other, EventType) and other.verb == self.verb
# ---
def cleanup(self):
        """Clean up venv and all tracked processes."""
        logger.info(f"Cleaning up job {self.job_id}")
        if self.process_env is not None:
            self.process_env.__exit__(None, None, None)
            self.process_env = None
# ---
def resize_embeddings(self, new_size: int, key: Optional[PRNGKeyArray] = None):
        new_token_embeddings = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, Vocab=self.Vocab.resize(new_size), token_embeddings=new_token_embeddings)
# ---
def add(self, other):
        if self.is_alt:
            last_alt = self.alt.parts[-1] + (other,)
            self.alt.parts = self.alt.parts[:-1] + (last_alt,)
        else:
            self.group.seq = self.group.seq + (other,)
# ---
def decode(string, backend=None, keys=False):
    """Convert a JSON string into a Python object.

    The keyword argument 'keys' defaults to False.
    If set to True then jsonpickle will decode non-string dictionary keys
    into python objects via the jsonpickle protocol.

    >>> str(decode('"my string"'))
    'my string'
    >>> decode('36')
    36
    """
    if backend is None:
        backend = json
    return unpickler.decode(string, backend=backend, keys=keys)
# ---
def test_scatter_set():
    B, V = Axis("batch", 2), Axis("vocab", 6)
    x = hax.zeros((B, V))
    idx = hax.named(jnp.array([1, 4]), B)
    val = hax.ones(B) * 9
    y = x.at[{V: idx}].set(val)
    ref = jnp.zeros((2, 6)).at[jnp.arange(2), idx.array].set(9)
    assert jnp.array_equal(y.array, ref)
# ---
def __init__(self):
            super().__init__()
            self.m = M()
            self.c = self.m.a
# ---
def iter_containing(self, name):
        ctx_dict = object.__getattribute__(self, '__dict__')
        if name in ctx_dict:
            yield self
        parent = ctx_dict.get('_parent')
        if parent is not None:
            for ancestor in parent.iter_containing(name):
                yield ancestor
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"token_embeddings": "model.embed_tokens"}
# ---
def _prefix(kindpats):
    """Whether all the patterns match a prefix (i.e. recursively)"""
    for kind, pat, source in kindpats:
        if kind not in ("path", "relpath"):
            return False
    return True
# ---
def test_slice_nd_shorthand_syntax():
    # syntax like arr["X", 0:10, "Y", 0:10] is supported

    H, W, D = hax.make_axes(H=10, W=20, D=30)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    assert jnp.all(jnp.equal(named1["H", 0:10, "D", 0:10].array, named1.array[0:10, :, 0:10]))
# ---
def format_path( str ):
	while( str.find( '//' ) != -1 ):
		str = str.replace( '//', '/' )
	return str
# ---
def manual_scale_group() -> config_pb2.ScaleGroupConfig:
    """Scale group config for manual hosts."""
    return config_pb2.ScaleGroupConfig(
        name="manual-hosts",
        min_slices=0,
        max_slices=3,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_CPU,
        runtime_version="manual",
        zones=["manual"],
    )
# ---
def extent(self):
        from mapproxy.layer import MapExtent

        return MapExtent(self.bbox, self.srs)
# ---
def _remove_spans(text: str, spans: list[list[int]]) -> str:
        """
        Return `text` with `spans` removed.
        Example: text = "hello", spans = [[1, 4]], returns "ho"
        """
        # Sort spans in reverse order to avoid index shifting
        sorted_spans = sorted(spans, key=lambda x: x[1], reverse=True)
        for start, end, _ in sorted_spans:
            text = text[:start] + text[end:]

        return text
# ---
def testForContinue(self):
    self.assertEqual((0, '1\n2\n3\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1, 2, 3):
          print i
          continue
          raise AssertionError""")))
# ---
def _sleep_then_succeed():
    time.sleep(0.1)
# ---
def clone_sources(self) -> ht.i32[NamedArray, "seq"]:  # type: ignore[name-defined]
        """Mapping from clone targets to their parent sequences."""
        return self.sequences.clone_sources
# ---
def _handle_par_h(self):
        'Check if there is a waiting paragraph heading and handle it.'
        try:
            self._heading(self.par_h)
        except AttributeError:
            pass
# ---
def fn(x):
        return x - 1
# ---
def broadcast_axis(a: NamedArray, axis: AxisSpec) -> NamedArray:
    """
    Broadcasts `a`, ensuring that it has all the axes in `axis`.
     `broadcast_axis` is an alias for `broadcast_to(a, axis, enforce_no_extra_axes=False, ensure_order=True)`

     You typically use this function when you want to broadcast an array to a common set of axes.
    """
    if isinstance(axis, Axis) and axis in a.axes:
        return a

    return broadcast_to(a, axis, enforce_no_extra_axes=False, ensure_order=True)
# ---
def copy_specification_from_item_group(self):
		self.set("website_specifications", [])
		if self.item_group:
			for label, desc in frappe.db.get_values("Item Website Specification",
										   {"parent": self.item_group}, ["label", "description"]):
				row = self.append("website_specifications")
				row.label = label
				row.description = desc
# ---
def __init__(self, endpoint_name: str):
        self._endpoint_name = endpoint_name
        self._client: Any = None
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: Optional[str] = None
    ) -> HFCheckpointConverter["MixtralConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfMixtralConfig,
        )
# ---
def __call__(self, x):
            return self.second(self.first(x))
# ---
def test_evaluate_string(self):
        expr = lit("hello")
        assert expr.evaluate({"anything": "ignored"}) == "hello"
# ---
def __repr__(self) -> str:  # pragma: no cover - trivial
        return self.name
# ---
def __init__(self):
        self._containers: dict[str, _LocalContainer] = {}
# ---
def _pacify_named_arrays(leaf):
    if isinstance(leaf, NamedArray):
        return _PassiveNamedArray(leaf.array, leaf.axes)
    elif isinstance(leaf, _PassiveNamedArray):
        assert False, "PassiveNamedArray should not be present in the tree"
    else:
        return leaf
# ---
def size(self) -> int:
        return len(self.queue)
# ---
def _delete(self, resource, name=None, selector=None):
        '''call oc delete on a resource'''
        cmd = ['delete', resource]
        if selector is not None:
            cmd.append('--selector={}'.format(selector))
        elif name is not None:
            cmd.append(name)
        else:
            raise OpenShiftCLIError('Either name or selector is required when calling delete.')

        return self.openshift_cmd(cmd)
# ---
def shutdown(self, wait: bool = True) -> None:
        self._executor.shutdown(wait=wait)
# ---
def ndim(self) -> int:
        """Number of axes in the current view."""
        return len(self.axes)
# ---
def testFunctionDecoratorWithArg(self):
    self.assertEqual((0, '<b id=red>foo</b>\n'), _GrumpRun(textwrap.dedent("""\
        def tag(name):
          def bold(fn):
            return lambda: '<b id=' + name + '>' + fn() + '</b>'
          return bold
        @tag('red')
        def foo():
          return 'foo'
        print foo()""")))
# ---
def html_inline(node: RenderTreeNode, context: RenderContext) -> str:
    return node.content
# ---
def set_slotted(self, slotted):
        self.slotted = slotted
# ---
def count_all(self):
        """Tells how many events have been counted globally

        Returns:
            int

        """
        return self.events_count
# ---
def _run_test_case(self, test_case):
        correct_output = self._solved_output.format(
            test_case.expected1,
            test_case.expected2
        )
        test_output = self.get_puzzle_solution(test_case.input)
        if correct_output == test_output:
            print("Test passed for input '{0}'".format(test_case.input))
        else:
            print("Test failed for input '{0}'".format(test_case.input))
            print(test_output)
# ---
def num_gpus(self):
        return self._num_gpus
# ---
def config():
    # Later: only send the necessary parts of config
    return jsonify(asdict(server.config))
# ---
def testOneValue(self):
        gen = VectorInput([1])
        tcToTotal = sTCToTotal()
        p = Pool()

        gen.data >> tcToTotal.envelope
        tcToTotal.TCToTotal >> (p, 'lowlevel.tctototal')

        self.assertRaises(RuntimeError, lambda: run(gen))
# ---
def __init__(self, payload: bytes):
        super().__init__()
        self._payload = payload
# ---
def output_block_quote(self):
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'block_quote_end':
            body += self.tok()
        return self.renderer.block_quote(body)
# ---
def service(self):
        ''' service property '''
        return self.svc
# ---
def test_find_path_converges_to_target():
    source = "return a + b\n"
    target = "return a * b\n"
    path = find_path(source, target)
    assert len(path) >= 1

    current = source
    for mutation in path:
        current = mutation.apply(current)

    # The result should structurally match the target.
    assert ast.dump(ast.parse(current)) == ast.dump(ast.parse(target))
# ---
def current_label_index(self):
        return self.ui.comboBoxFuzzingLabel.currentIndex()
# ---
def _strip_wrapped_partial(fun):
    if hasattr(fun, "__wrapped__"):  # ft.wraps
        return _strip_wrapped_partial(fun.__wrapped__)
    if isinstance(fun, ft.partial):
        return _strip_wrapped_partial(fun.func)
    return fun
# ---
def fake_clear_pool(id):
            fake_clear_pool.called = True
# ---
def _get_l3_plugin(self):
        return directory.get_plugin(constants.L3)
# ---
def index():
    """Landing page for SciNet"""
    return render_template("index.html")
# ---
def full(shape: AxisSpec, fill_value: T, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to `fill_value`"""
    if isinstance(shape, Axis):
        r = NamedArray(jnp.full(shape=shape.size, fill_value=fill_value, dtype=dtype), (shape,))
    else:
        x_shape = to_jax_shape(shape)
        r = NamedArray(jnp.full(shape=x_shape, fill_value=fill_value, dtype=dtype), shape)

    return auto_sharded(r)
# ---
def __init__(self):
        self.vocab_size = len(self.TOKENS)
        self.TOKENS.sort(key=len, reverse=True)  # Sort by length for greedy matching
        self.pad_token_id = self.TOKENS.index("<pad>")
        self.eos_token = "</s>"
        self.bos_token = "<s>"
# ---
def main():
    """Entry point that runs the async main loop."""
    asyncio.run(main_async())
# ---
def _is_mean_reduction(reduction):
    # Mean reductions need special handling for weighted reductions
    return reduction is hax.mean or reduction is jnp.mean
# ---
def test_linecol_to_offset_second_line():
    source = "hello world\nsecond line\n"
    assert _linecol_to_offset(source, 2, 0) == 12
    assert _linecol_to_offset(source, 2, 7) == 19
# ---
def test_filter_expression_logical_or(backend):
    """Test filter with logical OR expression."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"a": 1, "b": 2},
            {"a": -1, "b": 3},
            {"a": 2, "b": -1},
            {"a": -1, "b": -1},
        ]
    ).filter((col("a") > 0) | (col("b") > 0))

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 3
# ---
def __call__(self, host, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
        self.host = host
        self.timeout = timeout
        return self
# ---
def baz(x: Float["b"]):  # type: ignore  # noqa: F722
        pass
# ---
def test_basic_rearrange_transpose():
    assert einops_rearrange(z, "b d h w c -> b h w d c").axes == (B, H, W, D, C)
    # make sure the values are right too
    z_t = z.array.transpose((0, 2, 3, 1, 4))
    assert (einops_rearrange(z, "b d h w c -> b h w d c").array == z_t).all()
# ---
def wake(self) -> None: ...
# ---
def setup():
            """Create some temporary files."""
            context.temp_dir = tempfile.mkdtemp()
            context.filenames = ["file_%03d" % i for i in range(files)]
            for filename in context.filenames:
                with open(os.path.join(context.temp_dir, filename), "w") as f:
                    f.write("This is the file %r.\n" % filename)
# ---
def generation_kwargs(self):
        """Get the generation kwargs from the worker."""
        return self.leader._generation_kwargs
# ---
def extract_features(text: str, ngram_config: NGramConfig | None) -> Iterator[str]:
    """
    Extract features (paragraphs or n-grams) from text.
    """
    paragraphs = text.split("\n")

    for para in paragraphs:
        if ngram_config:
            yield from extract_ngrams(para, ngram_config.ngram_length, ngram_config.stride)
        else:
            # Exact paragraph matching
            yield para
# ---
def _process_link(self, m, link, title=None):
        line = m.group(0)
        text = m.group(1)
        if line[0] == '!':
            return self.renderer.image(link, title, text)

        self._in_link = True
        text = self.output(text)
        self._in_link = False
        return self.renderer.link(link, title, text)
# ---
def _infer_core_grid(b_dim: int, block_sizes: BlockSizes) -> tuple[int, int]:
    num_cores = _infer_num_tensorcores()
    if num_cores > 1:
        if b_dim % num_cores != 0:
            num_cores = 1
        else:
            b_per_core = b_dim // num_cores
            if b_per_core % block_sizes.b_block_size != 0:
                num_cores = 1
    num_b_blocks_per_core = b_dim // (num_cores * block_sizes.b_block_size)
    return num_cores, num_b_blocks_per_core
# ---
def hf_checkpoint_converter(self, ref_checkpoint: str | None = None) -> HFCheckpointConverter["ParallelLlamaConfig"]:
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfLlamaConfig,
        )
# ---
def tril(array: NamedArray, axis1: Axis, axis2: Axis, k=0) -> NamedArray:
    """Compute the lower triangular part of an array along two named axes."""
    array = array.rearrange((..., axis1, axis2))

    inner = jnp.tril(array.array, k=k)
    return NamedArray(inner, array.axes)
# ---
def decode_tokens(token_list: list[int]):
    """Helper to decode tokens."""
    decoded = current_tokenizer.decode(token_list)
    console.print(f"[green]Decoded:[/green] {decoded!r}")
# ---
def direct_fn(pred):
        pred_embeddings, pred_lm_head = pred
        logits = hax.dot(pred_embeddings, pred_lm_head, axis="embed")
        target_y = hax.nn.one_hot(true_ids, Vocab, dtype=pred_embeddings.dtype)
        loss, logz = cross_entropy_loss_and_log_normalizers(logits, Vocab, target_y)
        return (loss + 0.5 * (logz**2)).mean().scalar()
# ---
def start(self) -> None:
        app = self._create_app()
        self.server = make_server("localhost", self.proxy_port, app, threaded=True)
        self.thread = threading.Thread(target=self.server.serve_forever, daemon=True)
        self.thread.start()
        logger.info("Started Ray dashboard proxy on http://localhost:%d", self.proxy_port)
# ---
def __module_description(self):
        return self.description
# ---
def testAugAssignPow(self):
    self.assertEqual((0, '64\n'), _GrumpRun(textwrap.dedent("""\
        foo = 8
        foo **= 2
        print foo""")))
# ---
def from_hours(cls, hours: int) -> "Duration":
        """Create duration from hours."""
        return cls(hours * 60 * 60 * 1000)
# ---
def template_flash(stitches, do_outies):
    return "stitches: %s, outies: %s" % (stitches, do_outies), ["--flash", str(stitches), "--allow_outies", str(do_outies)]
# ---
def parse_args():
    parser = argparse.ArgumentParser(description='easy 249')
    parser.add_argument('stock_prices', action='store', nargs='+',
                        help='prices of a given stock')
    return parser.parse_args()
# ---
def __call__(self, target, creds, enforcer):
        """Check http: rules by calling to a remote server.

        This example implementation simply verifies that the response
        is exactly 'True'.
        """

        url = ('http:' + self.match) % target
        data = {'target': jsonutils.dumps(target),
                'credentials': jsonutils.dumps(creds)}
        post_data = urllib.urlencode(data)
        f = urllib2.urlopen(url, post_data)
        return f.read() == "True"
# ---
def resiliparse_kwargs(self) -> dict:
        exclude = {"markdownify_config", "prepend_title"}
        return {f.name: getattr(self, f.name) for f in fields(self) if f.name not in exclude}
# ---
def __ge__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "ge")
# ---
def normal(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.normal(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def convert_to_levanter_task_config(tasks: Sequence[EvalTaskConfig]) -> list[TaskConfig]:
    """
    Convert a list of EvalTaskConfig to a list of TaskConfig that Levanter's eval_harness expects.
    """
    return [
        TaskConfig(
            task=task.name,
            num_fewshot=task.num_fewshot,
            task_alias=task.task_alias,
        )
        for task in tasks
    ]
# ---
def to_state_dict(self, prefix: str | None = None) -> StateDict:
        return default_eqx_module_to_state_dict(self, prefix)
# ---
def heartbeat(
        self,
        request: cluster_pb2.HeartbeatRequest,
    ) -> cluster_pb2.HeartbeatResponse: ...
# ---
def _vm_detail_page(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("VM Detail", "/static/controller/vm-detail.js"))
# ---
def start(self) -> str:
        """Start the controller. Returns the controller address.

        For GCP: creates a GCE VM, bootstraps, returns internal IP.
        For local: starts in-process Controller, returns localhost URL.
        """
        self._controller = create_controller_vm(self._config, threads=self._threads)
        address = self._controller.start()
        logger.info("Controller started at %s (local=%s)", address, self.is_local)
        return address
# ---
def test_default_return_capacity(self):
        """When default_return_capacity=True, always return capacity"""
        self.dynamo.default_return_capacity = True
        with patch.object(self.dynamo, "call") as call:
            call().get.return_value = None
            rs = self.dynamo.scan("foobar")
            list(rs)
        call.assert_called_with(
            "scan",
            TableName="foobar",
            ReturnConsumedCapacity="INDEXES",
            ConsistentRead=False,
        )
# ---
def __init__(self, platform: config_pb2.GcpPlatformConfig, label_prefix: str):
        self._platform = platform
        self._label_prefix = label_prefix
# ---
def GET(self):

        return render.hello_form()
# ---
def make_state(key):
            model = Gpt2Mlp.init(Embed, Intermediate, jax.nn.relu, key=key)
            optim = optax.adam(1e-4)
            opt_state = optim.init(arrays_only(model))

            return arrays_only(model), arrays_only(opt_state), key
# ---
def test_take_with_filter_and_map(backend):
    """Test take fuses with other operations."""
    ds = (
        Dataset.from_list([list(range(20))])
        .flat_map(lambda x: x)
        .filter(lambda x: x % 2 == 0)  # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
        .take_per_shard(5)  # [0, 2, 4, 6, 8]
        .map(lambda x: x * 2)  # [0, 4, 8, 12, 16]
    )
    result = list(Backend.execute(ds, context=backend))
    assert result == [0, 4, 8, 12, 16]
# ---
def test_arange():
    (H,) = hax.make_axes(H=10)

    assert jnp.all(jnp.equal(hax.arange(H).array, jnp.arange(10)))
    assert hax.arange(H).axes == (H,)

    # test stride
    assert jnp.all(jnp.equal(hax.arange(H, step=2).array, jnp.arange(0, 20, 2)))

    # test start and stride
    assert jnp.all(jnp.equal(hax.arange(H, start=2, step=2).array, jnp.arange(2, 22, 2)))
# ---
def test_run_blocking():
    with TemporaryVenv() as venv:
        result = venv.run([venv.python_path, "--version"], capture_output=True, text=True)
        assert result.returncode == 0
        assert "Python" in result.stdout
# ---
def create(*args, **kw):
        return create_environment(*args, **kw)
# ---
def unit_of_measurement(self):
        """Return the unit of measurement of this entity, if any."""
        return self._unit_of_measurement
# ---
def __call__(self, x: NamedArray, *, key=None):
        k1, k2 = haliax.jax_utils.maybe_rng_split(key, 2)
        x = self.c_fc(x, key=k1)
        x = self.act(x)
        x = self.c_proj(x, key=k2)
        return x
# ---
def delete_all(self):
        self._call_all('delete_all')
# ---
def inspect(self, container_id: str) -> ContainerStatus:
        c = self._containers.get(container_id)
        if not c:
            return ContainerStatus(running=False, exit_code=1, error="container not found")
        return ContainerStatus(
            running=c._running,
            exit_code=c._exit_code,
            error=c._error,
        )
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        k_gate, k_up, k_down = maybe_rng_split(key, 3)
        hidden_states = self.gate_proj(x, key=k_gate)
        hidden_states = self.act(hidden_states)
        hidden_states = hidden_states * self.up_proj(x, key=k_up)
        outputs = self.down_proj(hidden_states, key=k_down)
        return outputs
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Job Percentage", device_id)
# ---
def get_name():
    return addon.getAddonInfo('name')
# ---
def test_evaluate_existing_column(self):
        expr = col("name")
        assert expr.evaluate({"name": "alice"}) == "alice"
# ---
def use_test_mesh(tensor_parallelism: int = 1, *, mesh: Optional[Mesh] = None, skip_ok: bool = False):
    """Context manager that activates a data/model Mesh and sets haliax's global mesh."""
    if mesh is None:
        mesh = create_test_mesh(tensor_parallelism, skip_ok=skip_ok)
    with set_mesh(mesh):
        yield mesh
# ---
def tan(a: A) -> A:
    return wrap_elemwise_unary(jnp.tan, a)
# ---
def __abs__(self) -> "NamedArray":  # pragma: no cover
        return haliax.absolute(self)
# ---
def _remove_job(job_id):
    with _lock:
        if job_id not in _jobs:
            raise NoSuchJob("No such job %r" % job_id)
        del _jobs[job_id]
# ---
def _loss_fn(params: TinyMLP, x: NamedArray, y: NamedArray) -> jax.Array:
    preds = params(x)
    diff = preds - y
    return hax.mean(diff * diff).scalar()
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        keys = maybe_rng_split(key, len(self.layers))
        for layer, k in zip(self.layers[:-1], keys):
            x = self.activation(layer(x, key=k))
        return self.layers[-1](x, key=keys[-1])
# ---
def from_text(cls, fname):
        vocab = np.genfromtxt(fname, dtype=str, delimiter=" ", usecols=0)
        clusters = np.genfromtxt(fname, dtype=int, delimiter=" ", usecols=1)
        return cls(vocab=vocab, clusters=clusters)
# ---
def test_random_data(self):
        data = MockData()
        a_set = data.get_random_elements(10)
        self.assertTrue(len(a_set) == 10, "the data should have 10 elements!")
# ---
def pending_count(self) -> int:
        """Number of pending nodes."""
        return len(self.pending_nodes)
# ---
def _distributed_work_job():
    """Coscheduled job that uses job context."""
    from iris.cluster.client import get_job_info

    info = get_job_info()
    if info is None:
        raise RuntimeError("Not running in an Iris job context")
    print(f"Task {info.task_index} of {info.num_tasks} on worker {info.worker_id}")
    return f"Task {info.task_index} done"
# ---
def get_next_file(self):
    to_return = self.files[self.idx]

    self.idx += 1
    self.idx %= len(self.files)

    return PARAMS["PATH"]+to_return
# ---
def download_artifact(artifact: "wandb.sdk.Artifact", root: Optional[Path]) -> Path:
    if root is None:
        root = Path(tempfile.mkdtemp(prefix="wandb-profile-"))
    else:
        root.mkdir(parents=True, exist_ok=True)
    download_path = Path(artifact.download(root=str(root)))
    return download_path
# ---
def update_env_var(self, key, value):
        '''place an env in the env var list'''

        env_vars_array = self.get_env_vars()
        idx = None
        for env_idx, env_var in enumerate(env_vars_array):
            if env_var['name'] == key:
                idx = env_idx
                break

        if idx:
            env_vars_array[idx]['value'] = value
        else:
            self.add_env_value(key, value)

        return True
# ---
def fake_create_vbd(cls, session, vm_ref, vdi_ref, userdevice,
                            vbd_type='disk', read_only=False, bootable=True):
            pass
# ---
def get_preference_adapter(source: str) -> PreferenceTransformAdapter | None:
    return preference_transform_templates.get(source)
# ---
def __rmod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.mod(other, self)
# ---
def task(self, index: int) -> "JobName":
        """Create a task name for this job.

        Tasks are job names with a numeric suffix.

        Example:
            JobName.from_string("/my-job").task(0) -> JobName(("my-job", "0"))
        """
        return JobName((*self._parts, str(index)))
# ---
def col_clause(self):
        search = (self.pattern
                  .replace('\\', '\\\\')
                  .replace('%', '\\%')
                  .replace('_', '\\_'))
        clause = self.field + " like ? escape '\\'"
        subvals = [search]
        return clause, subvals
# ---
def error(self, proto, *args):
        self.proto, self.args = proto, args
# ---
def __init__(self, uri, username, password, vminfo, vmid, irs):
        super(KVMCommand, self).__init__(vminfo, vmid, irs)
        self._uri = uri
        self._username = username
        self._password = password
# ---
def axes(self) -> tuple[Axis, ...]:
        """Axes visible from this view after applying staged selectors."""
        return _axes_after_prefix(self._axes, self._prefix)
# ---
def setGlobalForceParameter(force, key, value):
  for i in range(force.getNumGlobalParameters()):
    if force.getGlobalParameterName(i)==key:
      print('setting force parameter', key, '=', value)
      force.setGlobalParameterDefaultValue(i, value);
# ---
def execute(self):
        with self._volumes(), self._ssh_agent:
            yield self._start_helper()
# ---
def create(self, rsc, **kw):
        if isinstance(rsc, string_types):
            cls = getattr(resource, rsc.capitalize())
            rsc = cls(kw, self)
        return rsc.save()
# ---
def get_client(
    client,
    profile_name,
    aws_access_key_id,
    aws_secret_access_key,
    region=None,
):
    """Shortcut for getting an initialized instance of the boto3 client."""

    boto3.setup_default_session(
        profile_name=profile_name,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=region,
    )
    return boto3.client(client)
# ---
def test_str_split_box_df(self):
        def test_impl(df):
            return pd.DataFrame({'B': df.A.str.split(',')})

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df).B, test_impl(df).B, check_names=False)
# ---
def test_shard_cache_crashes_if_processor_throws():
    class ThrowingProcessor(SimpleProcessor):
        def __call__(self, batch: Sequence[Sequence[int]]):
            raise RuntimeError("exc")

    with tempfile.TemporaryDirectory(ignore_cleanup_errors=True) as tmpdir:
        with pytest.raises(RuntimeError):
            build_or_load_cache(tmpdir, SimpleShardSource(), ThrowingProcessor())
# ---
def test_from_files_empty_glob_ok(tmp_path):
    """Test from_files with empty_glob_ok=True."""
    input_dir = tmp_path / "input"
    input_dir.mkdir()

    # No error when empty_glob_ok=True
    ds = Dataset.from_files(f"{input_dir}/*.txt", empty_glob_ok=True)
    files = list(ds.source)
    assert len(files) == 0
# ---
def _replace(self, fname, force=False):
        '''replace the current object with oc replace'''
        # We are removing the 'resourceVersion' to handle
        # a race condition when modifying oc objects
        yed = Yedit(fname)
        results = yed.delete('metadata.resourceVersion')
        if results[0]:
            yed.write()

        cmd = ['replace', '-f', fname]
        if force:
            cmd.append('--force')
        return self.openshift_cmd(cmd)
# ---
def __init__(self, fn: Callable[[StepInfo[S]], Any]):
        self.fn = fn
# ---
def __init__(self, job_id: str, future: Future[None]):
        self._job_id = job_id
        self._future = future
        self._terminated = threading.Event()
# ---
def an_ifc_file_does_not_exist():
    ifc = IfcStore.get_file()
    if ifc:
        assert False, "An IFC is available"
# ---
def format_num_tokens(self) -> str:
        """Format total number of tokens in B/M/K notation."""
        tokens = self.total_tokens
        if tokens >= 1_000_000_000:
            return f"{tokens / 1_000_000_000:.1f}B"
        elif tokens >= 10_000_000:
            return f"{int(tokens / 1_000_000)}M"
        elif tokens >= 1_000_000:
            return f"{tokens / 1_000_000:.1f}M"
        elif tokens >= 1_000:
            return f"{tokens / 1_000:.1f}K"
        return str(tokens)
# ---
def isRequired(self, key):
        """ @rtype: bool """
        return SummaryKeyMatcher.cNamespace().is_required(self, key)
# ---
def _sleep_time(iter):
    """Return the time-to-sleep for the n'th iteration of a retry loop.

    This implementation increases exponentially.

    :param iter: iteration number
    :returns: number of seconds to sleep

    """
    if iter <= 1:
        return 1
    return iter ** 2
# ---
def main(config: Config):
        assert config.project == "test"
        assert config.x == 2
# ---
def varied_metrics_loss_fn(model, batch, key=None):
    """Loss function with different metric types for aggregation testing."""
    loss = hax.sum(batch * model.weight)
    metrics = {
        "accuracy": jnp.array(0.95),
        "num_tokens": jnp.array(128.0),
        "max_logit": jnp.array(5.0),
    }
    return loss, metrics
# ---
def t_DONE(self, t):
        r'(\(x\))'
        return t
# ---
def test_capacity_math(self):
        """Capacity addition and equality"""
        cap = Capacity(2, 4)
        s = set([cap])
        self.assertIn(Capacity(2, 4), s)
        self.assertNotEqual(Capacity(1, 4), cap)
        self.assertEqual(Capacity(1, 1) + Capacity(2, 2), Capacity(3, 3))
# ---
def _interpret(self):
        if not self.min:
            return

        try:
            count = int("".join(self.min))
        except ValueError:
            assert False, "internal error: cannot convert to number minimum of repetition"
        self.between.min = count
# ---
def gelu(a: A, approximate: bool = True) -> A:
    return wrap_elemwise_unary(jnn.gelu, a, approximate=approximate)
# ---
def lang(self):
        '''Language of the ebook. (mandatory)

        The language must be given as a lower-case two-letter code, optionally
        followed by a "-" and an upper-case two-letter country code.
        (e.g., "en", "en-US", "en-UK", "de", "de-DE", "de-AT")

        If this property is left unset, it defaults to "en".'''
        try:
            return self._lang
        except AttributeError:
            self.lang = 'en'
            return self._lang
# ---
def noop():
    pass
# ---
def __setstate__(self, state: dict) -> None:
        self._name = state["name"]
        self._count = state["count"]
        self._job_id = state["job_id"]
        self._handles = []
        self._discovered_names = set()
# ---
def bsearch(nums, left, right, res, i, j, target):
    while left <= right:
        middle = (left + right) // 2
        candidate = nums[i] + nums[j] + nums[middle]
        if res is None or abs(candidate - target) < abs(res - target):
            res = candidate
        if candidate == target:
            return res
        elif candidate > target:
            right = middle - 1
        else:
            left = middle + 1
    return res
# ---
def loss_fn(model, completions=completions, advantages=advantages, key=key):
                return compute_policy_gradient_loss(model, prompt_named, completions, advantages, key)
# ---
def poke(self, context):
        logging.info('Poking for prefix : {self.prefix}\n'
                     'in bucket s3://{self.bucket_name}'.format(**locals()))
        import airflow.hooks.S3_hook
        hook = airflow.hooks.S3_hook.S3Hook(s3_conn_id=self.s3_conn_id)
        return hook.check_for_prefix(
            prefix=self.prefix,
            delimiter=self.delimiter,
            bucket_name=self.bucket_name)
# ---
def setUp(self):
        super(TestReviewXBlock, self).setUp()

        for idx, student in enumerate(self.STUDENTS):
            username = 'u{}'.format(idx)
            self.create_account(username, student['email'], student['password'])
            self.activate_user(student['email'])

        self.staff_user = GlobalStaffFactory()
# ---
def __add__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.add(self, other)
# ---
def _invert_amounts(self, apply_to_all):
        if apply_to_all:
            panes = self.panes
        else:
            panes = [self.selected_pane]
        for pane in panes:
            entries = self.loader.accounts.entries_for_account(pane.account)
            txns = dedupe(e.transaction for e in entries)
            for txn in txns:
                for split in txn.splits:
                    split.amount = -split.amount
        self.import_table.refresh()
# ---
def _getPoint(self, point):
        """
        Converts a Point type into an Excellon coordinate
        """
        return "X%.6fY%.6f\n" % (point.x, -point.y)
# ---
def figure(self):
        rand = randint(0, FIG_LEN)
        return self.__figures[rand % FIG_LEN]
# ---
def test_regions_passed_through(self):
        resources = ResourceConfig(regions=["us-central1", "us-east1"])
        spec = convert_resources(resources)
        assert list(spec.regions) == ["us-central1", "us-east1"]
# ---
def test_laplace():
    check_gen_is_equal(lambda k, s: jax.random.laplace(k, s), lambda k, s: hax.random.laplace(k, s))
# ---
def __init__(
        self,
        cluster: LocalClusterClient | RemoteClusterClient,
        namespace: Namespace,
        job_id: JobName,
    ):
        self._cluster = cluster
        self._namespace = namespace
        self._job_id = job_id
# ---
def loadSelectedProfile(self):
        """Load selected profile"""

        activeProfile = self.getComboBoxList(self.profilesCombo)
        self.loadProfile(activeProfile)
# ---
def test_run_blocking_no_check():
    with TemporaryVenv() as venv:
        result = venv.run([venv.python_path, "-c", "import sys; sys.exit(1)"], check=False)
        assert result.returncode == 1
# ---
def axis_indices(self, axis: AxisSelection) -> tuple[int | None, ...]: ...
# ---
def main(config: TokenizeConfig):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    tokenize(config)
# ---
def all_keys(self):
        '''
        Merge managed keys with local keys
        '''
        keys = self.list_keys()
        keys.update(self.local_keys())
        return keys
# ---
def output_url(self, m):
        link = m.group(1)
        if self._in_link:
            return self.renderer.text(link)
        return self.renderer.autolink(link, False)
# ---
def get_test_client(nowait=False):
    client = get_es_connection('default')

    # wait for yellow status
    for _ in range(1 if nowait else 5):
        try:
            client.cluster.health(wait_for_status="yellow")
            return client
        except ConnectionError:
            time.sleep(0.1)
    else:
        # timeout
        raise SkipTest("Elasticsearch failed to start")
# ---
def init(cls, Vocab: Axis, config: LlamaConfig, *, key) -> "LlamaLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = LlamaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)

        return LlamaLMHeadModel(transformer, embeddings, lm_head)
# ---
def reducer(*tokens):
    """Decorator for reduction methods.

    Arguments are a sequence of tokens, in order, which should trigger running
    this reduction method.
    """

    def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func

    return decorator
# ---
def broadcast(*args, **kwargs) -> BroadcastFuture:
            result = self._pool._resolve()
            futures = []
            for endpoint in result.endpoints:
                future = self._pool._executor.submit(
                    self._pool._call_endpoint,
                    endpoint,
                    method_name,
                    args,
                    kwargs,
                )
                futures.append((endpoint, future))
            return BroadcastFuture(futures)
# ---
def EvalBatch(self):
        return Axis(self.batch_axis_name, self.eval_batch_size)
# ---
def test_exception_wrapping_dbapi(self):
        conn = testing.db.connect()
        for _c in testing.db, conn:
            assert_raises_message(
                tsa.exc.DBAPIError,
                r"not_a_valid_statement",
                _c.execute, 'not_a_valid_statement'
            )
# ---
def cumprod(self, axis: AxisSelector, *, dtype=None) -> "NamedArray":  # pragma: no cover
        return haliax.cumprod(self, axis=axis, dtype=dtype)
# ---
def _all_dbs_loaded(self):
        if self.client and self._core and self.lc:
            return True
        else:
            print("Load connection to Mongo and eQuilibrator local cache.")
            return False
# ---
def simple_jax_workload():
    return simple_jax_fn()
# ---
def init_logits():
        xw_scratch_ref[...] = jax.lax.dot_general(
            x_ref[...],
            w_ref[...],
            (((1,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
# ---
def to_rows(self, message):
        raise NotImplementedError("Not implemented.")
# ---
def to_dataframe(self, message, dtypes=None):
        record_batch = self._parse_arrow_message(message)

        if dtypes is None:
            dtypes = {}

        df = record_batch.to_pandas()

        for column in dtypes:
            df[column] = pandas.Series(df[column], dtype=dtypes[column])

        return df
# ---
def apply_chat_template(self, messages, tokenize, add_generation_prompt):
            # Simple: just return tokens for the user message content
            return [ord(c) for c in messages[0]["content"]]
# ---
def special_tokens_injection_check(marin_tokenizer: PreTrainedTokenizer):
    """Test that special tokens are correctly replaced."""
    for token_id, token_str in MARIN_CUSTOM_SPECIAL_TOKENS.items():
        assert marin_tokenizer.decode(token_id) == token_str
        assert marin_tokenizer.convert_tokens_to_ids([token_str]) == [token_id]
# ---
def helpButtonClicked(self, widget):
        """Signal handler for the "clicked" signal for the helpButton
           GtkButton widget. The user has clicked the Help button.

        Arguments:
        - widget: the component that generated the signal.
        """

        orca.helpForOrca(page="preferences")
# ---
def test_hash(self):
        @event.event
        def handler():
            pass

        h_inst = event.HandlerInstance.from_handler(handler)
        h_inst2 = event.HandlerInstance.from_handler(handler)
        assert h_inst is not h_inst2
        assert hash(h_inst) == hash(h_inst2)
        assert h_inst != h_inst2
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> float | None:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=True,
        )
# ---
def visitdir(self, dir):
        if self(dir):
            return "all"
        return self._matcher.visitdir(dir)
# ---
def Layers(self) -> Axis:
        """Alias for :attr:`Block` used by some downstream code."""

        return self.Block
# ---
def value_match(cls, pattern, value):
        """Determine whether the value matches the pattern. The value
        may have any type.
        """
        return cls.string_match(pattern, util.as_string(value))
# ---
def _GetNumNonVisibleFiles( file_list ):
  """Returns the number of file in the iterable list of files |file_list| which
  are not curerntly open in visible windows"""
  return len(
      [ f for f in file_list
        if not BufferIsVisible( GetBufferNumberForFilename( f, False ) ) ] )
# ---
def _create_lm_example(tokens):
            tokens = hax.named(tokens, self.Pos)
            example = LmExample.causal(
                tokens=tokens,
                eos_id=eos_id,
                block_cross_document_attention=block_cross_document_attention,
            )

            example = jax.lax.with_sharding_constraint(example, sharding)

            return example
# ---
def done(self) -> bool:
        return self._future.done()
# ---
def _MakeModuleBlock():
  return block.ModuleBlock(None, '__main__', '<test>', '',
                           imputil.FutureFeatures())
# ---
def build_controller_image(
    tag: str,
    push: bool,
    dockerfile: str | None,
    context: str | None,
    platform: str,
    region: tuple[str, ...],
    project: str,
):
    """Build Docker image for Iris controller."""
    _build_image("controller", tag, push, dockerfile, context, platform, region, project)
# ---
def register_event_callback(self, callback):
        self._event_callbacks.append(callback)
# ---
def scheduler(state):
    return Scheduler(state)
# ---
def go(conn, cursor, statement, parameters, context, executemany):
                canary.append((statement, context))
# ---
def start():
    global backProc
    backProc = multiprocessing.Process(target=testFun, args=(), daemon=True)
    backProc.start()
    return 'started: ' + str(backProc.pid)
# ---
def message(self, *args, **kwargs): # real signature unknown
        pass
# ---
def module_org_with_manifest():
    org = entities.Organization().create()
    manifests.upload_manifest_locked(org.id)
    return org
# ---
def test_reset_on_high_resolution_enable(self):
        self.assertEqual(self.reset_count, 0)

        self.mda.io_write_byte(0x3B8, 0x01)
        self.assertEqual(self.reset_count, 1)

        # Second write shouldn't call reset again.
        self.mda.io_write_byte(0x3B8, 0x01)
        self.assertEqual(self.reset_count, 1)
# ---
def test_metric_pytree():
    """Metrics are JAX pytrees."""
    m = Metric(_value=30.0, _count=2.0, reduction=ReductionType.MEAN)
    flat, treedef = jax.tree_util.tree_flatten(m)
    reconstructed = jax.tree_util.tree_unflatten(treedef, flat)

    assert jnp.allclose(reconstructed.value(), 15.0)  # 30 / 2 = 15
    assert reconstructed.reduction == ReductionType.MEAN
# ---
def test_actor_group_shutdown(client: LocalClient):
    group = client.create_actor_group(Counter, name="counters", count=2)
    handles = group.wait_ready()
    assert len(handles) == 2
    group.shutdown()
# ---
def get_children(self):
        return self.children
# ---


def get_positive(l: list):
    """Return only positive numbers in the list.
    >>> get_positive([-1, 2, -4, 5, 6])
    [2, 5, 6]
    >>> get_positive([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])
    [5, 3, 2, 3, 9, 123, 1]
    """
    return [e for e in l if e > 0]
# ---
def list_endpoints(self, prefix: str) -> list[cluster_pb2.Controller.Endpoint]:
        return self._remote_client.list_endpoints(prefix)
# ---
def test_create_job_ctx_defaults_to_ray_when_initialized():
    """Test that create_job_ctx returns ray context when Ray is initialized."""
    from fray.job import RayContext

    if ray.is_initialized():
        ray.shutdown()

    try:
        ray.init(ignore_reinit_error=True)
        ctx = create_job_ctx()
        assert isinstance(ctx, RayContext)
    finally:
        ray.shutdown()
# ---
def __init__(
        self,
        config: config_pb2.IrisClusterConfig,
        threads: ThreadContainer | None = None,
    ):
        self._config = config
        self._threads = threads
        self._controller: _InProcessController | None = None
        self._temp_dir: tempfile.TemporaryDirectory | None = None
        self._autoscaler: Autoscaler | None = None
# ---
def get_sample_outputs(self) -> dict[str, list[dict]]:
        """
        Get all stored sample outputs.

        Returns:
            Dictionary mapping task names to lists of sample outputs
        """
        return self.sample_outputs
# ---
def dofilter (theimage,thefilter):
   lastfilter = thefilter
   global image1
   image1 =  image1.filter(thefilter)
   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def test_dslice_with_selector():
    B, S, V = Axis("batch", 2), Axis("seq", 5), Axis("vocab", 10)
    x = hax.arange((B, S, V))
    idx = (hax.arange((B, S), dtype=jnp.int32) + 2) % 4
    shard = V.resize(4)
    x_shard = x["vocab", dslice(0, shard)]
    out = x_shard["vocab", idx]
    assert out.axes == (B, S)
    ref = x.array[:, :, :4][jnp.arange(B.size)[:, None], jnp.arange(S.size)[None, :], idx.array]
    assert jnp.array_equal(out.array, ref)
# ---
def internal_shortname(self):
        return self.__data['str_shortname']
# ---
def should_display_status_to_user(self):
        """Whether or not the status from this attempt should be displayed to the user."""
        return True
# ---
def submit(client, fn, name, *args, **kw):
    return client.submit(
        entrypoint=Entrypoint.from_callable(fn, *args),
        name=name,
        resources=ResourceSpec(cpu=1, memory="1g"),
        environment=EnvironmentSpec(),
        **kw,
    )
# ---
def __call__(self, batch: List[T]) -> PyTree:
        return jtu.tree_map(
            lambda _, *xs: PreparedBatch.from_batch([np.asarray(x) for x in xs]),
            self.exemplar,
            *batch,
            is_leaf=heuristic_is_leaf,
        )
# ---
def __repr__(self) -> str:
        return f"JobName({str(self)!r})"
# ---
def test_completely_different_programs():
    source = "x = 1\n"
    target = "y = 'hello'\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1
# ---
def url(self) -> str:
        return f"http://{self._config.host}:{self._config.port}"
# ---
def test_compute_next_token_loss_reduction_returns_scalar():
    Vocab = Axis("vocab", 32)
    cfg = ToyLmConfig(max_seq_len=8, embed_dim=16)
    model = ToyLmHeadModel.init(Vocab, cfg, key=jax.random.PRNGKey(0))

    Batch = Axis("batch", 4)
    Pos = cfg.max_Pos.resize(8)
    example = _toy_example(Batch, Pos, Vocab, key=jax.random.PRNGKey(1))

    loss = model.compute_next_token_loss(example)
    assert loss.axes == ()
    assert jnp.shape(loss.array) == ()
# ---
def v(self, vol=""):
        html = ""
        if vol == "" or vol == None :
           html += "Error"
        v = volume(vol)

        html += "<h6>%s (%s) </h6>" % (v, vol)
        return html
# ---
def should_exclude(file_path: pathlib.Path) -> bool:
    return matches_pattern(file_path, EXCLUDE_PATTERNS)
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids)
        x = self.norm(x)
        return x
# ---
def tearDown(self):
        self.session.execute("DROP TABLE IF EXISTS ports;", None)
        super(TestNestedResourceTestCase, self).tearDown()
# ---
def to_snake_case(name):
  intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\1_\2', name)
  insecure = re.sub('([a-z])([A-Z])', r'\1_\2', intermediate).lower()
  # If the class is private the name starts with "_" which is not secure
  # for creating scopes. We prefix the name with "private" in this case.
  if insecure[0] != '_':
    return insecure
  return 'private' + insecure
# ---
def __repr__dict__(self):
        return {'id':self.id,
                'analysis_id':self.analysis_id,
            'experiment_id':self.experiment_id,
            'sample_name_abbreviation':self.sample_name_abbreviation,
            'sample_name':self.sample_name,
            'time_point':self.time_point,
            'analysis_type':self.analysis_type,
            'used_':self.used_,
            'comment_':self.comment_}
# ---
def __init__(self, comodel_name=None, string=None, **kwargs):
        super(Many2one, self).__init__(comodel_name=comodel_name, string=string, **kwargs)
# ---
def log(a: A) -> A:
    return wrap_elemwise_unary(jnp.log, a)
# ---
def test_add_project_access_with_no_admin_user(self):
        req = fakes.HTTPRequest.blank('/v2/%s/types/%s/action' % (
            fake.PROJECT_ID, fake.VOLUME_TYPE3_ID),
            use_admin_context=False)
        body = {'addProjectAccess': {'project': PROJ2_UUID}}
        self.assertRaises(exception.PolicyNotAuthorized,
                          self.type_action_controller._addProjectAccess,
                          req, fake.VOLUME_TYPE3_ID, body)
# ---
def get_default_job_ctx() -> JobContext:
    """Get the current default job context, creating one if unset."""
    ctx = _job_context.get()
    if ctx is None:
        ctx = create_job_ctx(context_type="auto")
    return ctx
# ---
def _test_encryption(self, message):
        enc = self.alice.encrypt(message)
        self.assertFalse(enc.endswith('\n'))
        dec = self.bob.decrypt(enc)
        self.assertEquals(dec, message)
# ---
def _update(self, records, value):
        """ Update the cached value of ``self`` for ``records`` with ``value``. """
        records._cache[self] = value
# ---
def __init__(self, module, message):
        """
        SearchException constructor.

        Args:
            module (str): name of module/class that's raising exception
            message (str): exception message to be displayed

        Usage:
            raise SearchException("Test", "this is an error")

        """
        message = "{0} - {1}".format(module, message)
        Exception.__init__(self, message)
# ---
def connected(self):
        return self._connected
# ---
def modis_diff(domain, b, threshold=None):
    '''Compute (b2-b1) < threshold, a simple water detection index.

       This method may be all that is needed in cases where the threshold can be hand tuned.
    '''
    if threshold == None: # If no threshold value passed in, load it based on the data set.
        threshold = float(domain.algorithm_params['modis_diff_threshold'])
    return get_diff(b).lte(threshold)
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        return ScaleByAdoptState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)
# ---
def __repr__(self):
        return_str = "<MidiFile %d tracks\n" % len(self.tracks)
        for track in self.tracks:
            return_str = return_str + "  " + track.__repr__() + "\n"
        return return_str + ">"
# ---
def _get_hendrycks_math_test() -> Dataset:
    test_dataset = load_dataset("HuggingFaceH4/MATH-500", name="default", split="test")
    return cast(Dataset, test_dataset)
# ---
def HeadSize(self) -> Axis:
        return Axis("head_size", self.head_size)
# ---
def _use_upload_table(self):
        """
            Set the resource and the table to being s3_import_upload
        """

        if self.upload_resource == None:
            from s3resource import S3Resource
            (prefix, name) = self.UPLOAD_TABLE_NAME.split("_",1)
            self.upload_resource = S3Resource(prefix, name)
        self.resource = self.upload_resource
        self.table = self.upload_table
        self.tablename = self.upload_tablename
# ---
def test_from_iterable(backend):
    """Test creating dataset from iterable."""
    ds = Dataset.from_iterable(range(5))
    assert list(Backend.execute(ds, context=backend)) == [0, 1, 2, 3, 4]
# ---
def worker_id(self) -> WorkerId | None:
        """Worker from the most recent attempt, if any.

        Returns the worker ID from the last attempt regardless of whether
        the attempt is terminal. This is used for reporting which worker
        ran (or is running) the task.
        """
        if self.attempts:
            return self.attempts[-1].worker_id
        return None
# ---
def _try_convert_int(val, default):
    # handles a few cases we see in the wild: the number, "number", 'number', number;
    # punting on percent and fraction
    try:
        return int(val)
    except ValueError:
        val = val.strip().replace('"', "").replace("'", "").replace(";", "").replace(",", "")
        try:
            return int(val)
        except ValueError:
            return default
# ---
def mpra_1(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transduction'],
        'introduced_elements': 'synthesized DNA',
        'modified_site_nonspecific': 'random'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def get_random_stacks(self, num_samples, stack_size):

        start_indices = random.sample(range(len(self.examplers)), num_samples)
        return [self.get_stack(start_index, stack_size) for start_index in start_indices]
# ---
def test_scale_up_passes_tags_to_manager(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """scale_up() passes tags to the VmManager."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        group.scale_up(tags={"env": "prod", "team": "ml"})

        manager.create_vm_group.assert_called_once_with({"env": "prod", "team": "ml"})
# ---
def test_as_remote_kwargs_with_env_vars():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=CpuConfig())
    kwargs = as_remote_kwargs(config, env_vars={"FOO": "bar"})
    assert kwargs["runtime_env"] == {"env_vars": {"FOO": "bar"}}
# ---
def register_transfer_server(self, transfer_server_address: str):
        """
        Register the actual JAX transfer server address with the coordinator.
        Called by the server when it starts up.
        """
        self.transfer_server_address = transfer_server_address
# ---
def _ensure_is_array(x):
        if isinstance(x, (int, float, bool, complex)):
            return jnp.array(x)
        else:
            return x
# ---
def cluster(ctx):
    """Cluster management commands."""
    parent_obj = ctx.obj or {}
    ctx.ensure_object(dict)
    ctx.obj.update(parent_obj)
# ---
def to_rows(self, message):
        record_batch = self._parse_arrow_message(message)

        # Iterate through each column simultaneously, and make a dict from the
        # row values
        for row in zip(*record_batch.columns):
            yield dict(zip(self._column_names, row))
# ---
def loss_fn(x):
        y = model(x)
        return hax.sum(y.slice(Pos, start=loss_pos, length=1)).array
# ---
def test_pickle_roundtrip_preserves_name(self):
        handle = IrisActorHandle("my-actor")
        data = pickle.dumps(handle)
        restored = pickle.loads(data)
        assert restored._endpoint_name == "my-actor"
        assert restored._client is None
# ---
def __post_init__(self):
        if len(self.configs) == 0:
            raise ValueError("At least one dataset must be provided")

        if set(self.configs.keys()) != set(self.train_weights.keys()):
            raise ValueError(
                f"The keys in configs and weights must be the same;got {self.configs.keys()} and"
                f" {self.train_weights.keys()}"
            )
# ---
def _fn_fwd(x: jax.Array, labels: jax.Array, w: jax.Array):
        loss, lse = _forward(x, labels, w)
        return (loss, lse), (x, labels, w, lse)
# ---
def test_decimal(self):
        """Store and retrieve a Decimal"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "num": Decimal("1.1")})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["num"], Decimal("1.1"))
# ---
def testAssignName(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        foo = 'bar'
        print foo""")))
# ---
def mask_to_bias(mask: NamedArray, mask_value: float = -1e9) -> NamedArray:
    return mask * mask_value
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "LmHeadModel[LmConfigT]":
        """
        Resizes the vocabulary of the model. Key may be provided to use random initialization, otherwise, there
        should be some deterministic initialization of any new parameters.
        """
        pass
# ---
def test_detect_text_with_error_response(self, annotator_client_mock):
        # Given
        detect_text_method = annotator_client_mock.text_detection
        detect_text_method.return_value = AnnotateImageResponse(
            error={"code": 3, "message": "test error message"}
        )

        # When
        with pytest.raises(AirflowException) as ctx:
            self.hook.text_detection(image=DETECT_TEST_IMAGE)

        err = ctx.value
        assert "test error message" in str(err)
# ---
def current_tracker(tracker: "Tracker") -> typing.ContextManager:
    """Returns a context manager for setting the global tracker"""
    ...
# ---
def output_exemplar(self):
        return {"test": np.array([0], dtype=np.int64)}
# ---
def sync_and_track():
            result = _sync_weights_original()
            if result:
                self.weight_transfers += 1
            return result
# ---
def calculate_alpha(self):
        """
        http://en.wikipedia.org/wiki/Alpha_(investment)
        """
        return alpha(self.algorithm_period_returns,
                     self.treasury_period_return,
                     self.benchmark_period_returns,
                     self.beta)
# ---
def port(self):
        if not hasattr(self, "_socket"):
            self._get_address()
        return self._port
# ---
def _do_block(carry, block, *extra_args, **extra_kwargs):
        return block(carry, *extra_args, **extra_kwargs)
# ---
def do_go(self):
        self.do_time_ctrl('start')
# ---
def _create_instance_ref(self):
        return db.instance_create(self.context,
                                  {'user_id': self.user_id,
                                   'project_id': self.project_id,
                                   'instance_type_id': 1})
# ---
def compute_axis_mapping(self) -> ResourceMapping:
        """Mapping from logical axis to physical axis for compute."""
        return self.mesh.resolved_compute_mapping
# ---
def concat_axes(a1: ShapeDict, a2: AxisSpec) -> ShapeDict:
    pass
# ---
def editingKey(self, cell, editable, path, treeModel):
        """Starts user input of a Key for a selected key binding"""

        self._presentMessage(messages.KB_ENTER_NEW_KEY)
        orca_state.capturingKeys = True
        editable.connect('key-press-event', self.kbKeyPressed)
        return
# ---
def testFormatDisplayName(self):
    """Tests the _FormatDisplayName function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    display_name_string = test_helper._FormatDisplayName(
        event, event_data, event_data_stream)
    self.assertEqual(display_name_string, 'FAKE:log/syslog.1')
# ---
def bump_seq_len_to_next_page(self, seq_id: int) -> "SequenceTable":
        cur = self.seq_lens["seq", seq_id]
        size = jnp.array(self.page_size, dtype=jnp.int32)
        next_page = ((cur + size - 1) // size) * size
        new_seq_lens = self.seq_lens.at["seq", seq_id].set(next_page)
        return dataclasses.replace(self, seq_lens=new_seq_lens)
# ---
def _restart_db_server(self, db_version):
        sudo('svcadm restart postgresql')
# ---
def _get_startup(self):
        if not self.startup:
            self._get_member()
            if self.team_member:
                self.startup = self.team_member.startup
        return self.startup
# ---
def pop(self):
        """Pop an element from the queue."""
        raise NotImplementedError
# ---
def extract_run_name_from_path(path: str) -> str:
    """Extract run name (last component) from a checkpoint path.

    E.g., 'gs://bucket/checkpoints/my-run-abc123' -> 'my-run-abc123'
    """
    return os.path.basename(path.rstrip("/"))
# ---
def test_repr_handles_partials():
    """__repr__ should unwrap functools.partial"""
    assert repr(MapOp(partial(int, base=2))) == "MapOp(fn=int)"

    def my_base(n: str, base: int = 10) -> int:
        return int(n, base)

    op = MapOp(partial(my_base, base=2))
    assert repr(op) == "MapOp(fn=test_repr_handles_partials.<locals>.my_base)"
# ---
def make_inputs(batch):
        key = jax.random.PRNGKey(batch)
        key_x, key_w, key_y = jax.random.split(key, 3)
        x = jax.random.normal(key_x, (batch, hidden), dtype=jnp.float32)
        w = jax.random.normal(key_w, (hidden, vocab), dtype=jnp.float32)
        y = jax.random.randint(key_y, (batch,), 0, vocab, dtype=jnp.int32)
        return x, w, y
# ---
def test_read_dataset_streaming_with_columns(self, sample_data, tmpdir, ext, create_fn):
        """Test streaming reading with column selection for both formats"""
        input_file = os.path.join(tmpdir, f"test_input.{ext}")
        create_fn(sample_data, input_file)

        rows = list(read_dataset_streaming(input_file, columns=["id"]))

        assert len(rows) == len(sample_data)
        assert "id" in rows[0]
        assert "text" not in rows[0]
# ---
def list_actors(self, request: actor__pb2.ListActorsRequest, ctx: RequestContext) -> actor__pb2.ListActorsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def synthetic_target_dir(self, target, target_workdir):
    all_sources = list(target.sources_relative_to_buildroot())
    source = all_sources[0]
    namespace = self._get_go_namespace(source)
    return os.path.join(target_workdir, 'src', 'go', namespace.replace(".", os.path.sep))
# ---
def named_call(f: F, name: str | None = None) -> F: ...
# ---
def test_entrypoint_params_tpu():
    from fray.v2.ray_backend.backend import get_entrypoint_params

    request = JobRequest(
        name="tpu-job",
        entrypoint=Entrypoint.from_binary("train", []),
        resources=ResourceConfig(device=TpuConfig(variant="v4-8")),
    )
    params = get_entrypoint_params(request)
    assert "entrypoint_resources" in params
    assert params["entrypoint_resources"]["TPU-v4-8-head"] == 1.0
    assert params["entrypoint_resources"]["TPU"] == 4.0
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetHoursRanges(0), [(4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11)])
    self.assertEqual(self.schedule.GetHoursRanges(3), [(28, 30), (30, 31), (31, 32), (32, 34), (34, 35), (35, 36), (36, 37)])
    self.assertEqual(self.schedule.GetHoursRanges(5), [(48, 49), (49, 50), (50, 52), (52, 53), (53, 54), (54, 56), (56, 57)])
# ---
def test_synthetic_subtrees_nonempty(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=10)
    assert len(entries) > 0
# ---
def test(b):
    if not b:
        raise RuntimeError('test assertion failed')
# ---
def test___cmp__le(self):
        self._test__cmp__(
            lambda left, right: left <= right,
            (
                False,
                True,
                True,
                True,
                True,
                TypeError if PY3 else False,
                TypeError if PY3 else True,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
            ),
            '<='
        )
# ---
def __init__(self, what, expected, detail=None):
        message = f"'{what}' is not {expected}"
        if detail:
            message = f"{message}: {detail}"
        super().__init__(message)
# ---
def _run() -> int:
        return len(dupekit.process_native(small_parquet_path))
# ---
def test_error_attribute_issue555(testdir):
    testdir.makepyfile("""
        import sys
        def test_capattr():
            assert sys.stdout.errors == "strict"
            assert sys.stderr.errors == "strict"
    """)
    reprec = testdir.inline_run()
    reprec.assertoutcome(passed=1)
# ---
def __len__(self):
        return SummaryKeyMatcher.cNamespace().size(self)
# ---
def ready_count(self) -> int:
        """All local actors are ready immediately after creation."""
        return len(self._handles)
# ---
def __init__(self, dataset: AsyncDataset[T_co], key: PRNGKeyArray, perm_type: PermType = "feistel"):
        super().__init__()
        self.dataset = dataset
        self.key = key
        self._permutation: Optional[Permutation] = None
        self._perm_type = perm_type
# ---
def schedule(step):
            tokens_trained = step * self.batch_size * self.seq_length
            return jnp.minimum(ctx.learning_rate, self.batch_size * self.a * tokens_trained**self.b)
# ---
def key_iterator(key: PRNGKeyArray | int):
    if isinstance(key, int):
        key = jax.random.PRNGKey(key)
    while True:
        key, subkey = jax.random.split(key)
        yield subkey
# ---
def accept_handler(fd, events):
        while True:
            try:
                connection, address = sock.accept()
            except socket.error as e:
                if e.args[0] in (errno.EWOULDBLOCK, errno.EAGAIN):
                    return
                raise
            callback(connection, address)
# ---
def checkinfo2(content):
    content[1] = content[1].decode('gbk')
    key = content[1].encode('utf-8')
    if key == 'èŠ‚æ“':
        return 'è¿™ç§ä¸œè¥¿æ—©å°±æ²¡æœ‰äº†'


    result = animation(key)    #æœåŠ¨æ¼«
    return result
# ---
def update_stats(self, stats, delta=1, sampleRate=1):
        """
        Updates one or more stats counters by arbitrary amounts
        """
        if not self.enabled or self.addr is None:
            return

        if type(stats) is not list:
            stats = [stats]
        data = {}
        for stat in stats:
            data["%s%s" % (self.prefix, stat)] = "%s|c" % delta

        self.send(data, sampleRate)
# ---
def __init__(self, prev):
        self.prev = prev  # ContentOfGroup or CharClass
        self.pattern = ast.PatternChar()
        self.pattern.type = ast.PatternChar.Unicode

        self.prev.add(self.pattern)
# ---
def __init__(self, tableName, tableColumns=[], coreInfo={}):
        self.tableName = tableName
        self.columnsDict = OrderedDict(tableColumns)
        self.dbFile = os.path.join(os.getcwd().replace("python", "metadata"), "libretro.sqlite")
        self.dbFileExists = os.path.isfile(self.dbFile)
        self.coreInfo = coreInfo
# ---
def VHeadDim(self) -> Axis:
        return Axis("v_head_dim", self.head_v_dim)
# ---
def test_metric_fold_associativity(reduction):
    """fold is associative for all reduction types."""
    m1 = Metric.from_value(10.0, reduction)
    m2 = Metric.from_value(20.0, reduction)
    m3 = Metric.from_value(30.0, reduction)

    # (m1 + m2) + m3
    result1 = fold(fold(m1, m2), m3)

    # m1 + (m2 + m3)
    result2 = fold(m1, fold(m2, m3))

    assert jnp.allclose(result1.value(), result2.value())
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        """Convert to proto for RPC APIs."""
        ...
# ---
def _refresh_swap_list_items(self):
        if not self.panes:
            return
        items = []
        basefmt = self.selected_pane.parsing_date_format
        for first, second in [(DAY, MONTH), (MONTH, YEAR), (DAY, YEAR)]:
            swapped = swap_format_elements(basefmt, first, second)
            items.append("{} --> {}".format(basefmt.iso_format, swapped.iso_format))
        self.swap_type_list[:3] = items
# ---
def decorator(f):
        f.http_route = path
        f.http_method = method
        return f
# ---
def test_actor_method_with_kwargs(client: LocalClient):
    actor = client.create_actor(Adder, name="adder")
    assert actor.add.remote(a=3, b=4).result() == 7
    assert actor.add(a=10, b=20) == 30
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"transformer": None}
# ---
def floor_divide(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.floor_divide](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.floor_divide.html)
    """
    return jnp.floor_divide(x1, x2)
# ---
def handle500(error):
    return '<H1>Oops, its broken:&nbsp;{}<BR>'.format(error)
# ---
def __init__(self):
        super(TestAppModelSimple, self).__init__()
        self.my_key = ""
        self.stringField = ""
# ---
def _seq_params_from_work(work: PrefillWork, idx: int) -> SeqDecodingParams:
    def select(x):
        if isinstance(x, NamedArray):
            return x["seq", idx]
        elif is_jax_array_like(x):
            return x[idx]
        else:
            raise TypeError(f"Unexpected type in seq_params: {type(x)}")

    return hax.tree_util.tree_map(select, work.seq_params)
# ---
def intermediate(self, x, scalar):
            return x + 2 * self.w + scalar
# ---
def build_test_opener(*handler_instances):
    opener = OpenerDirector()
    for h in handler_instances:
        opener.add_handler(h)
    return opener
# ---
def test_capturing_readouterr(self):
        with self.getcapture() as cap:
            print("hello world")
            sys.stderr.write("hello error\n")
            out, err = cap.readouterr()
            assert out == "hello world\n"
            assert err == "hello error\n"
            sys.stderr.write("error2")
            out, err = cap.readouterr()
        assert err == "error2"
# ---
def _es_args(self, resource, refresh=None):
        """Get index and doctype args."""
        datasource = self._datasource(resource)
        args = {
            'index': self.index,
            'doc_type': datasource[0],
            }
        if refresh:
            args['refresh'] = refresh
        return args
# ---
def test_updated_on(self):
        eq_(self.record.updated_on, None)
# ---
def update_weights(self, new_state_dict: dict, model_name: str):
        """Synchronous weight update."""
        return self.bridge.run(self._update_weights_async(new_state_dict, model_name))
# ---
def __next__(self):
        """
        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.
        :return: The current object of the iterator.
        """
        if self.index >= self.list_length:
            raise StopIteration
        else:
            self.index += 1
            self.update(task_id=self.task_id,
                        progress=self.index)

            return self.list[self.index - 1]
# ---
def initialize(self, resolver, mapping):
        self.resolver = resolver
        self.mapping = mapping
# ---
def should_log(self) -> bool:
        """Return True if any sample logging should occur."""
        if self.log_all:
            return True
        return self.max_samples_per_benchmark is not None and self.max_samples_per_benchmark > 0
# ---
def __repr__(self) -> str:
        return f"{self.parent}[{self.field!r}]"
# ---
def invert():
   global image1
   image1= ImageChops.invert(image1)

   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def _last_or_fail(self, psm: PSM):
        if self.parent.g.group.seq:
            return self.parent.g.group.seq[-1]
        else:
            psm.error = "nothing to repeat"
# ---
def Offer_To_Pick_Up_Call (self, Call_Flow_Control, Call_ID):
        self.Step (Message = "Client offers to answer call...")

        try:
            Call_Flow_Control.PickupCall (call_id = Call_ID)
        except:
            self.Log (Message = "Pick-up call returned an error of some kind.")
# ---
def exception(self) -> BaseException | None:
        return self._exception
# ---
def compute(input):
            return hax.nn.softmax(model(input, key=None, attn_mask=attn_mask), axis=model.Vocab)
# ---
def connection_made(self, transport):
        """Called when reader thread is started"""
        self.port = transport.serial.port
        logger.debug('connection_made: `%s` `%s`', self.port, transport)
        self.transport = transport
        self.connected.set()
        self.disconnected.clear()
# ---
def init(cls, Vocab: Axis, config: QwenConfig, *, key) -> "QwenLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = QwenTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)

        return QwenLMHeadModel(transformer, embeddings, lm_head)
# ---
def test_ckpt_path_with_input_name_trailing_slash():
    input_name = make_input_name("checkpoints/step-555000/", "myrun")
    assert ckpt_path_to_step_name(input_name) == "myrun-555000"
# ---
def merge(self) -> hax.NamedArray:
        return hax.dot(self.lora_A.weight, self.lora_B.weight, axis=LORA_R) * self.scale
# ---
def test_axis_shapes_force_replica_dcn_when_other_absorber():
    cfg = MeshConfig(dcn_axes={"other_dcn": -1})
    _, dcn = cfg.axis_shapes(num_devices=8, num_slices=2)
    # replica_dcn forced to 1 to leave only one absorber
    assert dcn["replica_dcn"] == 1
    assert dcn["other_dcn"] == 2
# ---
def update_header(file_path, year, ci):
    config = _get_config(file_path, year)
    ext = file_path.rsplit('.', 1)[-1]
    if ext not in SUPPORTED_FILES or not os.path.isfile(file_path):
        return False
    if os.path.basename(file_path)[0] == '.':
        return False
    return _update_header(file_path, config, SUBSTRING, SUPPORTED_FILES[ext]['regex'],
                          SUPPORTED_FILES[ext]['format'], ci)
# ---
def testAssertNoMsg(self):
    self.assertEqual((0, 'AssertionError()\n'), _GrumpRun(textwrap.dedent("""\
        try:
          assert False
        except AssertionError as e:
          print repr(e)""")))
# ---
def _get_extension(file_path: str) -> str:
    for ext in sorted(SUPPORTED_EXTENSIONS, key=len, reverse=True):
        if file_path.endswith(ext):
            return ext
    raise ValueError(f"Unsupported extension: {file_path}.")
# ---
def test_map(backend):
    """Test map operation with all backends."""
    ds = Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x * 2)
    result = list(Backend.execute(ds, context=backend))
    assert sorted(result) == [2, 4, 6, 8, 10]
# ---
def step_impl(context):
    context.user_service.save(context.base_user)
# ---
def product(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.product(self, axis=axis, dtype=dtype, where=where)
# ---
def test_max_entries_per_type():
    """Cap on entries per type should be respected."""
    # Generate a lot of unique programs.
    programs = [f"def f{i}(x):\n    return x + {i}\n" for i in range(200)]
    bank = SubtreeBank.from_corpus(programs, max_entries_per_type=50)
    for entries in bank.entries.values():
        assert len(entries) <= 50
# ---
def is_ray_initialized() -> bool:
    """Check if Ray is initialized without requiring callers to import Ray."""
    return ray.is_initialized()
# ---
def test_empty_rollouts_raises_error():
    """Test that empty rollout list raises ValueError."""
    with pytest.raises(ValueError, match="Cannot create batch from empty rollout list"):
        train_batch.create_training_batch_from_rollouts([], max_tokens=16, pad_token_id=0)
# ---
def __sub__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.subtract(self, other)
# ---
def load_item(self, row):
        """
            Load an item from the item table (counterpart to add_item
            when restoring a job from the database)
        """

        item = S3ImportItem(self)
        if not item.restore(row):
            self.error = item.error
            if item.load_parent is None:
                self.error_tree.append(deepcopy(item.element))
        # Update lookup lists
        item_id = item.item_id
        self.items[item_id] = item
        return item_id
# ---
def test_prefix(self):
        import functools
        other_event_deco = functools.partial(event.event, _prefix="__test_")

        @other_event_deco
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.event_name == '__test_test'
# ---
def mk_listbox(frame, side='top', sbars='y', sel_mode=tkinter.EXTENDED):
    BORDER = 0
    COLOR = 'grey'

    listbox_frame = tkinter.Frame(frame, bg=COLOR, bd=BORDER)
    listbox_frame.pack(side=side, fill='both', expand=True)

    listbox = tkinter.Listbox(listbox_frame, selectmode=sel_mode)
    mk_scrollable_area(listbox, listbox_frame, sbars)
    return listbox
# ---
def to_wire(self) -> str:
        """Serialize to wire format for RPC/env vars."""
        return str(self)
# ---
def stopRequested(self):
    return False
# ---
def test_lambda_output_shape_autocalculate_multiple_inputs(self):

    def lambda_fn(x):
      return math_ops.matmul(x[0], x[1])

    l = keras.layers.Lambda(lambda_fn)
    output_shape = l.compute_output_shape([(10, 10), (10, 20)])
    self.assertAllEqual((10, 20), output_shape)
# ---
def search_query_builder(query):
    parsed_query = expr.parseString(query)[0]
    return _parsed_query2dict(parsed_query)
# ---
def _pipeline() -> int:
        if batch_size is None:
            batches = [in_memory_table.to_pylist()]
        else:
            batches = (b.to_pylist() for b in in_memory_table.to_batches(max_chunksize=batch_size))
        return sum(len(dupekit.process_dicts_batch(batch)) for batch in batches)
# ---
def current_date_time():
    # Get the current local time and format as MM-DD-YYYY-HH-MM-SS
    formatted_time = time.strftime("%m-%d-%Y-%H-%M-%S", time.localtime())

    return formatted_time
# ---
def convert_to_cache(self, value, record, validate=True):
        # apply rounding here, otherwise value in cache may be wrong!
        value = float(value or 0.0)
        digits = self.digits
        return float_round(value, precision_digits=digits[1]) if digits else value
# ---
def notfirst():
            nonlocal first
            if first:
                first = False
                return True
            return False
# ---
def test_check_share_in_use_no_conn(self):
        drv = self._driver
        share = drv._check_share_in_use(None, '/dir')
        if share:
            self.fail('Unexpected share detected.')
# ---
def hard_sigmoid(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_sigmoid, a)
# ---
def test_pickle_drops_client(self):
        """Client is transient state â€” pickle should not carry it."""
        handle = IrisActorHandle("my-actor")
        # Manually set client to simulate resolved state
        handle._client = "fake-client"
        data = pickle.dumps(handle)
        restored = pickle.loads(data)
        assert restored._client is None
# ---
def template_input(input):
    input = os.path.abspath(input)
    # input, input_cmd
    return "input\t{}".format(input), ["--input", input]
# ---
def test_basic_order():
    partial_order = ("apple", ..., "banana")
    candidates = ("banana", "apple", "cherry")
    expected_output = ("apple", "cherry", "banana")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert actual_output == expected_output
    assert_partial_order_respected(partial_order, actual_output)
# ---
def join_protocol(path):
        return f"{protocol}://{path}" if protocol else path
# ---
def _add_graphics(root, params):
    e = root.find('./devices/graphics/[@type]')
    if e is not None:
        params['graphics'] = e.get('type')
# ---
def setUp(self):
        pass
# ---
def test_job_name_require_task_errors_on_non_task():
    with pytest.raises(ValueError):
        JobName.from_string("/root/child").require_task()
# ---
def __call__(cls, verb, *args):
        if verb not in cls.verbs:
            cls.verbs[verb] = super(_VerbSingleton, cls).__call__(verb, *args)
        return cls.verbs[verb]
# ---
def transition_to(
        self,
        state: TaskState,
        *,
        message: str = "",
        error: str | None = None,
        exit_code: int | None = None,
    ) -> None:
        self.status = state
        self.status_message = message
        if is_task_finished(state):
            self.finished_at = Timestamp.now()
            if error:
                self.error = error
            if exit_code is not None:
                self.exit_code = exit_code
# ---
def default_get(self, cr, uid, fields_list, context=None):
        # merge defaults from stock.picking with possible defaults defined on stock.picking.in
        defaults = self.pool['stock.picking'].default_get(cr, uid, fields_list, context=context)
        in_defaults = super(stock_picking_in, self).default_get(cr, uid, fields_list, context=context)
        defaults.update(in_defaults)
        return defaults
# ---
def add_model(self, model):
        assert isinstance(model, Polyhedron)
        self.models.append(model)
# ---
def set_match(self, match):
        self._set_panel_match(match, 0, 0)
        self._set_panel_match(match, 1, 2)
        self._set_panel_match(match, 2, 1)
        self._set_panel_match(match, 3, 3)
        self.match_num_ctrl.SetValue(str(match.match_number))
# ---
def test_nunique_str(self):
        def test_impl(n):
            df = pd.DataFrame({'A': ['aa', 'bb', 'aa', 'cc', 'cc']})
            return df.A.nunique()

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        # test compile again for overload related issues
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# ---
def add_secret(self, key, value):
        ''' add a secret '''
        if self.secrets:
            self.secrets[key] = value
        else:
            self.put(Secret.secret_path, {key: value})

        return True
# ---
def poll(self, job_id: JobId) -> JobInfo:
        """Get current status of a job without blocking.

        Args:
            job_id: Job identifier

        Returns:
            Current job information including status

        Raises:
            KeyError: If job_id is not found
        """
        ...
# ---
def test_to_from_state_dict():
    a = jnp.array([1, 2])
    b = jnp.array([3, 4])
    m = M(a, b)

    state_dict = to_state_dict(m)
    assert state_dict == {"a": a, "b": b}

    m2 = M(jnp.array([0, 0]), jnp.array([0, 0]))
    m2 = from_state_dict(m2, state_dict)
    assert jnp.all(m2.a == a)
    assert jnp.all(m2.b == b)
# ---
def test_flatten(self):
    testing_utils.layer_test(
        keras.layers.Flatten, kwargs={}, input_shape=(3, 2, 4))

    # Test channels_first
    inputs = np.random.random((10, 3, 5, 5)).astype('float32')
    outputs = testing_utils.layer_test(
        keras.layers.Flatten,
        kwargs={'data_format': 'channels_first'},
        input_data=inputs)
    target_outputs = np.reshape(
        np.transpose(inputs, (0, 2, 3, 1)), (-1, 5 * 5 * 3))
    self.assertAllClose(outputs, target_outputs)
# ---
def nanmean(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanmean, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def __init__(self, client, config, serializer, deserializer):
        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer
        self._config = config
# ---
def test_str_split_filter(self):
        def test_impl(df):
            B = df.A.str.split(',')
            df2 = pd.DataFrame({'B': B})
            return df2[df2.B.str.len() > 1]

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D', 'G', '', 'g,f']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_frame_equal(
            hpat_func(df), test_impl(df).reset_index(drop=True))
# ---
def go(conn, *args, **kw):
                canary.append(name)
# ---
def __init__(
        self,
        *,
        env: dict[str, str] | None = None,
        cwd: str | None = None,
    ):
        """Initialize job group.

        Args:
            env: Base environment dict for commands. If None, uses os.environ.
            cwd: Working directory for commands. If None, uses current directory.
        """
        self._base_env = env
        self._cwd = cwd
        self._processes: list[subprocess.Popen] = []
        self._entered = False
# ---
def test_dothefiltering(self):
        self.assertTrue("foo.doc" not in
                        tecautils.filterImages(self.files_list,
                                               self.conf))
        self.assertTrue("yukinon.jpg" in
                        tecautils.filterImages(self.files_list,
                                               self.conf))
# ---
def get_verb(cls, verb):
        return cls.verbs.get(verb)
# ---
def rms_norm(x: Float[Array, "... D"], weight: Float[Array, "D"], eps: float) -> Float[Array, "... D"]:
    weight = unshard(weight)
    dtype = x.dtype
    x = x.astype(jnp.float32)
    variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
    normed = x * jax.lax.rsqrt(variance + eps)
    out = normed * weight
    return out.astype(dtype)
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "ASRMixin":
        """
        Resizes the vocabulary of the ASR Output space. Key may be provided to use random initialization, otherwise,
        there should be some deterministic initialization of any new parameters.
        """
        pass
# ---
def port(self):
        """Get the port the server is running on."""
        if self.config.port > 0:
            return self.config.port

        # query the uvicorn server socket list for the port
        for server in self._server.servers:
            for sock in server.sockets:
                addr = sock.getsockname()
                return addr[1]

        return None
# ---
def vmap_via(self, fn: Callable[[M], OutputT_co]) -> Callable[[], OutputT_co]: ...
# ---
def create_test_inference_server_config(model_config: LlamaConfig, output_dir: str | Path):
    return InferenceServerConfig(
        trainer=create_nano_trainer_config(output_dir),
        tokenizer=DummyTokenizer(),
        service=InferenceEngineConfig(
            max_seqs=8,
            page_size=8,
            max_seq_len=64,
            max_queued_tokens=8,
        ),
        temperature=1.0,
        port=find_open_port(),
    )
# ---
def test_olmo3_param_counts_dont_change_with_seqlen():
    """Test that parameter counts are independent of sequence length."""
    from levanter.models.olmo3 import Olmo3LMHeadModel

    model = Olmo3LMHeadModel.init(hax.Axis("v", 2048), _get_olmo3_config(seq_len=128), key=random.PRNGKey(0))
    model2 = Olmo3LMHeadModel.init(hax.Axis("v", 2048), _get_olmo3_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def axis_size(ax: AxisSpec) -> int:
    """
    Returns the size of the axis or the product of the sizes of the axes in the axis spec
    """

    if isinstance(ax, Axis):
        return ax.size
    elif isinstance(ax, Mapping):
        return prod(ax.values())
    else:
        return prod(axis.size for axis in ax)
# ---
def write_batch(self, batch: RolloutBatch) -> None:
        """Write a batch.

        Args:
            batch: RolloutBatch to write.
        """
        pass
# ---
def get_tracker(name: Literal["trackio"]) -> TrackioTracker: ...
# ---
def relu6(a: A) -> A:
    return wrap_elemwise_unary(jnn.relu6, a)
# ---
def run(self):
        now = datetime.datetime.now()
        secondsInCurrentDay = now.hour * 3600 + now.minute * 60 + now.second
        if secondsInCurrentDay < self._timeOfDay:
            sleepDuration = self._timeOfDay - secondsInCurrentDay
        else:
            sleepDuration = self._timeOfDay + 3600 * 24 - secondsInCurrentDay
        logging.getLogger("main").info("sleeping for " + str(sleepDuration) + " seconds")
        self._waitIfNotStopped(sleepDuration)
        self._program.run()
# ---
def test_wait_delete_table(self):
        """Delete table shall wait for the table to go offline."""
        tablename = "foobar_wait"
        hash_key = DynamoKey("id")
        self.dynamo.create_table(tablename, hash_key=hash_key, wait=True)
        result = self.dynamo.delete_table(tablename, wait=True)
        self.assertTrue(result)
# ---
def config_from_hf_checkpoint(self, ref: Optional[Union[str, RepoRef]] = None) -> LevConfig:
        config = self.hf_config_from_hf_checkpoint(ref)
        return self.config_from_hf_config(config)
# ---
def volume(vol) :
    global player
    if player == 'omxplayer':
        return volume_omxplayer(vol)
    else:
        return volume_alsa(vol)
# ---
def log_norm_passthrough(desc: str) -> GradientTransformation:
    """
    Creates a gradient transformation that logs the L2 norm of the updates
    and returns the updates unchanged.
    """

    def init_fn(params):
        return None

    def update_fn(updates, state, params, **extra_args):
        levanter.tracker.jit_log({desc: optax.tree_utils.tree_l2_norm(updates)})
        return updates, None

    return GradientTransformationExtraArgs(init_fn, update_fn)
# ---
def create_vm_group_side_effect(_tags: dict[str, str] | None = None) -> MagicMock:
        manager._create_count += 1
        slice_id = f"new-slice-{manager._create_count}"
        return make_mock_slice(slice_id)
# ---
def _identity(_):
            return cache
# ---
def list_slices(self, group_config: config_pb2.ScaleGroupConfig) -> list[str]:
        return []
# ---
def batch_size(self):
        return self._batch_size
# ---
def t_error(self, t):
        raise SyntaxError(
            "Illegal character: '%s' at Line %d" % (t.value[0], t.lineno)
        )
# ---
def _ensure_int_is_array(x):
    # who tf decided that bools are ints
    if isinstance(x, int) and not isinstance(x, bool):
        return jnp.array(x)
    else:
        return x
# ---
def test_works_after_dispose(self):
        eng = create_engine(testing.db.url)
        for i in range(3):
            eq_(eng.scalar(select([1])), 1)
            eng.dispose()
# ---
def __enter__(self):
        """\
        Enter context handler. May raise RuntimeError in case the connection
        could not be created.
        """
        self.start()
        # Wait for protocol to connect.
        event = OrEvent(self.connected, self.closed)
        event.wait(self.default_timeout_s)
        return self
# ---
def test_parse_empty_json_template(self):
        tmpl_str = '{}'
        msg = 'Template format version not found'
        self._parse_template(tmpl_str, msg)
# ---
def chat(self, message: Optional[str] = None):
        """Chat with the model."""
        if not self.server:
            console.print("[red]No model loaded. Use 'load' command first[/red]")
            return

        if message:
            messages = [ChatMessage(role="user", content=message)]
            self._run_chat_completion(messages)
        else:
            self._run_chat_session()
# ---
def detail_export(self, request, pk=None):
        serializer = ConditionExportSerializer(self.get_object())
        xml = ConditionRenderer().render([serializer.data])
        return XMLResponse(xml, name=self.get_object().key)
# ---
def _dummy_step_info(step):
    return StepInfo(
        state=TrainerState(
            # + 1 b/c step here is next step
            step=step + 1,
            model=None,
            optimizer=None,  # type: ignore
            opt_state=None,
            training_key=jax.random.PRNGKey(0),
            is_trainable=True,
            mp=None,
            model_averaging=None,
        ),
        loss=0.0,
        step_duration=0.0,
    )
# ---
def test_optimize_with_take():
    """Take should be fused with map and filter."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            TakePerShardOp(10),
            FilterOp(lambda x: x > 5),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    fused_op = plan.stages[0].operations[0]
    assert isinstance(fused_op, Map)
# ---
def _related_domain(self):
        if callable(self.domain):
            # will be called with another model than self's
            return lambda recs: self.domain(recs.env[self.model_name])
        else:
            # maybe not correct if domain is a string...
            return self.domain
# ---
def to_int(val):
  return int(val)
# ---
def _get_vm_status(vm_name: str, zone: str, project: str) -> dict | None:
    """Get detailed status of a VM."""
    result = _run_gcloud(
        [
            "compute",
            "instances",
            "describe",
            vm_name,
            f"--project={project}",
            f"--zone={zone}",
            "--format=json",
        ]
    )
    if result.returncode != 0:
        return None
    return json.loads(result.stdout)
# ---
def list_item(node: RenderTreeNode, context: RenderContext) -> str:
    """Return one list item as string.

    This returns just the content. List item markers and indentation are
    added in `bullet_list` and `ordered_list` renderers.
    """
    block_separator = "\n" if is_tight_list_item(node) else "\n\n"
    text = make_render_children(block_separator)(node, context)

    if not text.strip():
        return ""
    return text
# ---
def train_cfg(model_cfg):
    return EditTrainingConfig(
        model=model_cfg,
        max_seq_len=MAX_SEQ_LEN,
        total_steps=3,
        batch_size=2,
        warmup_steps=1,
        log_interval=1,
    )
# ---
def handler():
            pass
# ---
def fetch_task_logs(
        self,
        task_id: JobName,
        start: "Timestamp | None" = None,
        max_lines: int = 0,
    ) -> list[cluster_pb2.Worker.LogEntry]:
        return self._remote_client.fetch_task_logs(task_id, start, max_lines)
# ---
def test_boolean_op_swap():
    variants = generate_expression_variants("a and b")
    assert "a or b" in variants
# ---
def build(self, Vocab: Axis, *, key: PRNGKeyArray) -> GrugWrapper:
        grug_cfg = GrugModelConfig(
            vocab_size=Vocab.size,
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            max_seq_len=self.max_seq_len,
        )
        return GrugWrapper.init(Vocab, grug_cfg, key=key)
# ---
def test_mem_read_word_just_past_the_end(self):
        self.mda.video_ram[3998] = 0x12
        self.mda.video_ram[3999] = 0x34
        self.assertEqual(self.mda.mem_read_word(3999), 0x0034)
# ---
def huber(residual):
        abs_r = jnp.abs(residual)
        quad = 0.5 * residual**2
        linear = delta * (abs_r - 0.5 * delta)
        return jnp.where(abs_r <= delta, quad, linear)
# ---
def __add__(self, other: "PyExpr") -> "PyExpr": ...
# ---
def compute_cats_reward(response: str) -> float:
    """Compute reward for cat-themed responses using MoarCatsTask logic."""
    num_cats = response.lower().count("cat")
    love_cats = response.lower().count("love cats")
    return (num_cats + (10 * love_cats)) / (1 + len(response))
# ---

def is_equal_to_sum_even(n):
    """Evaluate whether the given number n can be written as the sum of exactly 4 positive even numbers
    Example
    is_equal_to_sum_even(4) == False
    is_equal_to_sum_even(6) == False
    is_equal_to_sum_even(8) == True
    """
    return n%2 == 0 and n >= 8
# ---
def cached_token_count(cache_path: str, field: str = "input_ids") -> int:
    """Return the total number of tokens stored in a finished TreeCache."""
    cache = TreeCache.load(cache_path, {field: np.zeros((0,), dtype=np.int32)})
    return cache.store.tree[field].data_size
# ---
def metadata(self) -> Dict[str, Any]:
        return {}
# ---
def slice_is_terminal(slice_info: vm_pb2.SliceInfo) -> bool:
    """Compute is_terminal from vms[] in proto."""
    terminal = {
        vm_pb2.VM_STATE_READY,
        vm_pb2.VM_STATE_FAILED,
        vm_pb2.VM_STATE_TERMINATED,
        vm_pb2.VM_STATE_PREEMPTED,
    }
    return all(vm.state in terminal for vm in slice_info.vms)
# ---
def generate_hash_from_messages(messages: list[dict[str, str]]) -> str:
    """Generate a hash from a list of messages.

    Args:
        messages (List[Dict[str, str]]): A list of messages.

    Returns:
        str: A hash of the messages.
    """
    return hashlib.sha256(str(messages).encode()).hexdigest()
# ---
def has_local_mods(module, git_path, dest, bare):
    if bare:
        return False

    cmd = "%s status --porcelain" % (git_path)
    rc, stdout, stderr = module.run_command(cmd, cwd=dest)
    lines = stdout.splitlines()
    lines = list(filter(lambda c: not re.search('^\\?\\?.*$', c), lines))

    return len(lines) > 0
# ---
def test_ckpt_path_invalid_string_path():
    path = "checkpoints/llama/checkpoints/not-a-step-name"
    with pytest.raises(ValueError, match="Invalid path"):
        ckpt_path_to_step_name(path)
# ---
def test_sentinel_file_reset(tmp_path: Path) -> None:
    """Test SentinelFile.reset removes the file."""
    sentinel = SentinelFile(str(tmp_path / "sentinel.txt"))

    # Signal then reset
    sentinel.signal()
    assert sentinel.is_set()

    sentinel.reset()
    assert not sentinel.is_set()

    # Reset is idempotent
    sentinel.reset()
    assert not sentinel.is_set()
# ---
def __init__(self, cluster: Cluster, status_file: StatusFile):
        self.cluster = cluster
        self._status_file = status_file
        self._job_id: JobId | None = None
        self._heartbeat_thread: Thread | None = None
        self._stop_event = Event()
# ---
def test_constant_schedule():
    optimizer = AdamConfig(
        learning_rate=1e-3,
        weight_decay=0.0,
        warmup=0.0,
        min_lr_ratio=1.0,  # No decay
        lr_schedule="constant",
        cycles=None,
    )

    sched_fn = optimizer.lr_scheduler(1000)

    assert sched_fn(0) == 1e-3
    assert sched_fn(500) == 1e-3
    assert sched_fn(999) == 1e-3
# ---
def backwards(self, orm):
        # Deleting model 'ArticleComment'
        db.delete_table('cms_articlecomment')
# ---
def init(Vocab: Axis, config: Olmo2Config, *, key) -> "Olmo2Embedding":
        return Olmo2Embedding(Vocab, hnn.Embedding.init(Vocab, config.Embed, key=key))
# ---
def _get_blob_reference(self):
        return self.get_resource_name(TEST_BLOB_PREFIX)
# ---
def batch_completions(self, prompts, temperature, n, max_tokens=None, stop=None, system_prompt=None, top_k=None):
        """Return mock completions for each prompt."""
        return [create_mock_chat_completion(self.tokenizer) for prompt in prompts]
# ---
def config(self):
        """A GL config describing the context of this window.  Read-only.

        :type: `pyglet.gl.Config`
        """
        return self._config
# ---
def add(self,accum,item):
        return accum + item
# ---
def delete_floatingip_postcommit(self, context, fip_context):
        pass
# ---
def address(self) -> str:
        return self._address or self.vm_id
# ---
def __init__(self, config: ReplayConfig):
        self.config = config
# ---
def peek(self):
        if self.tokens:
            return self.tokens[-1]
        return None
# ---
def test_no_trivially_short_entries(bank):
    """Entries shorter than 5 chars should be filtered out."""
    for entries in bank.entries.values():
        for entry in entries:
            assert len(entry.source) >= 5, f"Too short: {entry.source!r}"
# ---
def result(self) -> Any:
        """Get the underlying result from the future."""
        return self._future.result()
# ---
def items(self, section_name):
            if section_name != section:
                raise NoSectionError(section_name)
            return {
                'memcache_servers': memcache_servers,
                'memcache_serialization_support':
                memcache_serialization_support,
                'memcache_max_connections': memcache_max_connections,
            }
# ---
def process_loglikelihood(self, packed_request):
        out = self._jit_loglikelihood(self.model, packed_request)
        return out
# ---
def _get_vocab_size(pretraining_data):
    tokenizer = unwrap_versioned_value(pretraining_data.tokenizer)
    vocab_size = _cached_load_tokenizer(tokenizer).vocab_size
    return vocab_size
# ---
def _write_jsonl(path: Path, rows: list[dict]) -> None:
    with path.open("w", encoding="utf-8") as handle:
        for row in rows:
            handle.write(json.dumps(row) + "\n")
# ---
def start(self) -> None:
        self._thread.start()
# ---
def __send(self, data, flags=0):
        try:
            return self.__iowait(self._connection.send, data, flags)
        except OpenSSL.SSL.SysCallError as e:
            if e[0] == -1 and not data:
                # errors when writing empty strings are expected and can be ignored
                return 0
            raise
# ---
def test___cmp__lt(self):
        self._test__cmp__(
            lambda left, right: left < right,
            (
                False,
                False,
                False,
                True,
                False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
            ),
            '<'
        )
# ---
def init():
        if not hasattr(DefaultDotGeneralOp, "_instance"):
            DefaultDotGeneralOp._instance = DefaultDotGeneralOp()

        return DefaultDotGeneralOp._instance
# ---
def _get_first_model_id(server_url: str) -> str:
    response = requests.get(f"{server_url}/models", timeout=30)
    response.raise_for_status()
    payload = response.json()
    data = payload.get("data", [])
    if not data:
        raise RuntimeError(f"No models returned from {server_url}/models: {str(payload)[:2000]}")
    model_id = data[0].get("id")
    if not model_id:
        raise RuntimeError(f"Missing model id in {server_url}/models response: {str(payload)[:2000]}")
    return str(model_id)
# ---
def hf_config_from_hf_checkpoint(self, ref: Optional[Union[str, RepoRef]] = None) -> HfConfig:
        path, rev = self._get_ref(ref)

        with _patch_hf_hub_download():
            config = AutoConfig.from_pretrained(path, revision=rev, trust_remote_code=self.trust_remote_code)
        return config
# ---
def __init__(self, main_controller, word_list=[], comp_len=DEFAULT_COMPLETION_LENGTH):
        """Constructor.

            @type  word_list: iterable
            @param word_list: A list of words that should be auto-completed."""
        self.main_controller = main_controller
        assert isinstance(word_list, list)
        self.comp_len = comp_len
        self._word_list = []
        self._word_freq = defaultdict(lambda: 0)
        self.add_words(word_list)
        self.widgets = set()
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: AttentionMask | NamedArray | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        x = self.embeddings.embed(input_ids)
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def __init__(
        self, plotly_name="minexponent", parent_name="choropleth.colorbar", **kwargs
    ):
        super(MinexponentValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            edit_type=kwargs.pop("edit_type", "colorbars"),
            min=kwargs.pop("min", 0),
            **kwargs
        )
# ---
def test_hf_gradient():
    _compare_gpt2_checkpoint_gradients("gpt2", None)
# ---
def test_slice_nd_array_present_dims():
    # tests slicing with arrays that are already present in the named array, which is sometimes ok
    H, W, D = hax.make_axes(H=10, W=20, D=30)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    index1 = hax.random.randint(PRNGKey(0), (H,), 0, H.size)

    # this is ok, since the H would be eliminated anyway
    assert jnp.all(jnp.equal(named1[{"H": index1}].array, named1.array[index1.array, :, :]))
# ---
def mk_treeview(frame, side='top', sbars='y'):
    BORDER = 0
    COLOR = 'grey'

    treeview_frame = tkinter.Frame(frame, bg=COLOR, bd=BORDER)
    treeview_frame.pack(side=side, fill='both', expand=True)

    treeview = tkinter.ttk.Treeview(treeview_frame)
    mk_scrollable_area(treeview, treeview_frame, sbars)

    return treeview
# ---
def test_reductions_produce_scalar_named_arrays_when_None_axis():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)

    named1 = hax.random.uniform(PRNGKey(0), (Height, Width))

    assert isinstance(hax.mean(named1, axis=None), NamedArray)

    # But if we specify axes, we always get a NamedArray, even if it's a scalar
    assert isinstance(hax.mean(named1, axis=("Height", "Width")), NamedArray)
    assert hax.mean(named1, axis=("Height", "Width")).axes == ()
# ---
def _parse_progress(self, chunk):
        m = self.DISK_PROGRESS_RE.match(chunk)
        if m is None:
            return None
        try:
            return int(m.group(1))
        except ValueError:
            raise OutputParserError('error parsing progress regex: %r'
                                    % m.groups)
# ---
def __init__(self, code, msg, headers, data, url=None):
        io.StringIO.__init__(self, data)
        self.code, self.msg, self.headers, self.url = code, msg, headers, url
# ---
def visit_math(self, element):
        return self._visit_children(element)
# ---
def testSmallMaxRange(self):
        self.assertConfigureFails(HPCP(), {'maxFrequency':1199, 'splitFrequency':1000})
# ---
def test_broadcast_to():
    H, W, D = hax.make_axes(H=10, W=20, D=30)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    assert jnp.all(jnp.equal(hax.broadcast_to(named1, (H, W, D)).array, named1.array))

    ZZ = Axis("ZZ", 5)

    assert jnp.all(jnp.equal(hax.broadcast_to(named1, (H, W, D, ZZ)).array, named1.array.reshape(10, 20, 30, 1)))
# ---
def is_nested_node(el):
            return el and el.name in ["ol", "ul", "li", "table", "thead", "tbody", "tfoot", "tr", "td", "th"]
# ---
def __len__(self):
        return len(self.store)
# ---
def test_parse_invalid_yaml_and_json_template(self):
        tmpl_str = '{test'
        msg = 'line 1, column 1'
        self._parse_template(tmpl_str, msg)
# ---
def __init__(self):
                OpenerDirector.__init__(self)
                self.recorded = []
# ---
def default(self, o):
        # We can probably get rid of this if we require python 3.11
        # and change ActivationFunctionEnum to a StrEnum
        if isinstance(o, ActivationFunctionEnum):
            return o.name
        return super().default(o)
# ---
def testFormatTimeZone(self):
    """Tests the _FormatTimeZone function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    zone_string = test_helper._FormatTimeZone(
        event, event_data, event_data_stream)
    self.assertEqual(zone_string, 'UTC')
# ---
def test_empty_set_against_integer_negation(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.notin_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [(1,), (2,), (3,), (4,)], params={"q": []})
# ---
def egcd(a, b):
    if a == 0:
        return (0, 1)
    else:
        y, x = egcd(b % a, a)
        return (x - (b // a) * y, y)
# ---
def bmarks():
    return_data = get_import_bm()
    return return_data
# ---
def read_remote(self, minion_id, status=ACC):
        '''
        Read in a remote key of status
        '''
        path = os.path.join(self.opts['pki_dir'], status, minion_id)
        if not os.path.isfile(path):
            return {}
        with salt.utils.fopen(path, 'rb') as fp_:
            return self.serial.loads(fp_.read())
# ---
def fsdp(parameter_mapping: ResourceMapping, compute_mapping: ResourceMapping) -> typing.Callable[[F], F]: ...
# ---
def check_dashboard(cluster_name: str, dashboard_port: int) -> tuple[str, bool]:
        """Check if a dashboard is accessible."""
        try:
            response = requests.get(f"http://localhost:{dashboard_port}/api/version", timeout=3)
            return (cluster_name, response.status_code == 200)
        except (requests.ConnectionError, requests.Timeout):
            return (cluster_name, False)
# ---
def __init__(self, *a, **k):
        super(SessionRecordingComponent, self).__init__(*a, **a)
        self.set_trigger_recording_on_release(not (self._record_button.is_pressed))
# ---
def test_spawn_vhd_glance_windows(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_VHD, None, None,
                         os_type="windows", architecture="i386")
        self.check_vm_params_for_windows()
# ---
def config_options(self):
        ''' return config options '''
        return self._options
# ---
def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):
        super(LW_AveragePooling2D, self).__init__(pool_size, strides, padding, data_format)
# ---
def all(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.all](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.all.html#jax.numpy.all).
    """
    return wrap_reduction_call(jnp.all, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def __post_init__(self):
        super().__post_init__()
        assert (
            self.num_experts_per_tok <= self.n_routed_experts
        ), f"num_experts_per_tok={self.num_experts_per_tok} greater than by n_routed_experts={self.n_routed_experts}."
# ---
def get_by_id(self, id):
        Util.validate_type(id, "str")
        return self._get_by_id(id)
# ---
def teardown():
    """Destroy all TPU CI infrastructure."""
    delete_controller_vm()
    delete_all_tpu_vms()
# ---
def poke(self, context):
        import airflow.hooks.hdfs_hook
        sb = airflow.hooks.hdfs_hook.HDFSHook(self.hdfs_conn_id).get_conn()
        logging.getLogger("snakebite").setLevel(logging.WARNING)
        logging.info(
            'Poking for file {self.filepath} '.format(**locals()))
        try:
            files = [f for f in sb.ls([self.filepath])]
        except:
            return False
        return True
# ---
def current_in_milliamps(self):
        return self.current * 1000
# ---
def num_gpus(self) -> int:
        return self.bt.num_gpus
# ---
def effective_principals(self, request):
		return self.match(request).effective_principals(request)
# ---
def _is_simple_field(field: FieldDescriptor) -> bool:
    """Check if a protobuf field is a simple scalar type that maps to Click."""
    if field.label == FieldDescriptor.LABEL_REPEATED:
        return False
    if field.message_type is not None:
        return False
    return field.type in PROTO_TYPE_TO_CLICK
# ---
def terminate(ctx, job_id):
    """Terminate a job."""
    cluster = ctx.obj["cluster"]
    cluster.terminate(job_id)
    click.echo(f"Terminated: {job_id}")
# ---
def discover_latest(self) -> bool:
        return self.checkpoint_step is None
# ---
def stop_job(job_id: str) -> None:
    """Stop a running Ray job.

    Note: This requires RAY_ADDRESS to be set, typically via ray_dashboard context manager.

    Args:
        job_id: The job ID or submission ID to stop
    """
    cmd = ["ray", "job", "stop", job_id]
    run_ray_command(cmd, timeout=60, capture_output=False)
# ---
def json(self):
        if self.parsed_json is None:
            self.load_json()

        return self.parsed_json
# ---
def encode(self, text, add_special_tokens=True):
        if add_special_tokens:
            text = f"{self.bos_token} {text} {self.eos_token}"

        tokens = []
        while text:
            for token in self.TOKENS:
                if text.startswith(token):
                    tokens.append(self.TOKENS.index(token))
                    text = text[len(token) :]
                    break
            else:
                raise ValueError(f"Unknown token in text: '{text}'")

        return tokens
# ---
def i_set_prop_to_value(prop, value):
    try:
        eval(f"bpy.context.{prop}")
    except:
        assert False, "Property does not exist"
    try:
        exec(f'bpy.context.{prop} = "{value}"')
    except:
        exec(f"bpy.context.{prop} = {value}")
# ---
def _receive_once():
            try:
                return self._transfer_client.receive_weights(self._policy_model)
            except Exception:
                logger.exception("Weight transfer client failed while receiving weights.")
                return None
# ---
def free_pages(self, seq_id: int) -> "DecodeState":
        sequences, page_table = self.sequences.free_pages(self.page_table, seq_id)
        return dataclasses.replace(self, sequences=sequences, page_table=page_table)
# ---
def setYear(self, year):
        self.__year = year
# ---
def __init__(self, code, headers):
        self.code = code
        self.headers = headers
        self.reset()
# ---
def __call__(self, *args, **kwargs):
    input_types = []
    args = list(args)
    for (i, x) in enumerate(args):
      x = ops.convert_to_tensor(x)
      if not isinstance(x, ops.Tensor):
        raise ValueError("Expect a Tensor but get ", x)
      input_types.append(x.dtype)
      args[i] = x
    return self.instantiate(input_types)(*args, **kwargs)
# ---
def test_basic_identity():
    assert einops_rearrange(z, "b d h w c -> b d h w c").axes == (B, D, H, W, C)
    assert (einops_rearrange(z, "b d h w c -> b d h w c").array == z.array).all()
# ---
def ready_slice_count(self) -> int:
        """Count of VM groups where all VMs are ready."""
        with self._vm_groups_lock:
            snapshot = list(self._vm_groups.values())
        return sum(1 for g in snapshot if g.status().all_ready)
# ---
def get(self, key):
        ''' get a specified key'''
        try:
            entry = Yedit.get_entry(self.yaml_dict, key, self.separator)
        except KeyError:
            entry = None

        return entry
# ---
def list_endpoints_by_prefix(self, prefix: str) -> list[ControllerEndpoint]:
        """List endpoints matching a name prefix for non-terminal jobs."""
        with self._lock:
            return self._visible_endpoints(lambda ep: ep.name.startswith(prefix))
# ---
def calculate_period_returns(self, returns):
        period_returns = (1. + returns).prod() - 1
        return period_returns
# ---
def metadata(self):
        return {}
# ---
def _checked_request(url):
    try:
        response = requests.get(url, headers={"Metadata-Flavor": "Google"})
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException:
        logger.exception(f"Could not get {url} from metadata server. Is this a TPU VM?", exc_info=True)
        raise
# ---
def job_id(self) -> str:
        return self._job_id
# ---
def id(self):
        return self._id
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.stuff.insert(),
            [
                {"id": 1, "data": "some data"},
                {"id": 2, "data": "some data"},
                {"id": 3, "data": "some data"},
                {"id": 4, "data": "some other data"},
            ],
        )
# ---
def cross_entropy_loss(
    logits: NamedArray,
    Label: AxisSelector,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: None = None,
) -> jnp.ndarray | NamedArray: ...
# ---
def union_axes(a1: AxisSelection, a2: AxisSelection) -> AxisSelection: ...
# ---
def _PostCreate(self):
    """Set the retention policy."""
    put_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'put-retention-policy',
        '--log-group-name', self.name,
        '--retention-in-days', str(self.retention_in_days)
    ]
    vm_util.IssueCommand(put_cmd)
# ---
def __init__(self, rules=None, **kwargs):
        self.tokens = []
        self.def_links = {}
        self.def_footnotes = {}

        if not rules:
            rules = self.grammar_class()

        self.rules = rules
# ---
def __len__(self):
        return len(self._variables)
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        object_ref = self._ray_method.remote(*args, **kwargs)
        return RayActorFuture(object_ref)
# ---
def _kill_agent(self):
        rc, out, err = execCmd([_SSH_AGENT.cmd, '-k'],
                               env={'SSH_AGENT_PID': self._agent_pid})
        if rc != 0:
            logging.error('Error killing ssh-agent (PID=%r), exit code: %r'
                          ', out: %r, err: %r' %
                          (self._agent_pid, rc, out, err))
# ---
def serve(c):
    """Serve site at http://localhost:8000/"""

    class AddressReuseTCPServer(RootedHTTPServer):
        allow_reuse_address = True

    server = AddressReuseTCPServer(
        CONFIG['deploy_path'],
        ('', CONFIG['port']),
        ComplexHTTPRequestHandler)

    sys.stderr.write('Serving on port {port} ...\n'.format(**CONFIG))
    server.serve_forever()
# ---
def transform_abstract(html: BeautifulSoup):
    # Transform the abstract from h6 to h2
    abstract = html.findAll("h6", {"class": "ltx_title_abstract"})
    for ab in abstract:
        ab.name = "h2"
    return html
# ---


def below_threshold(l: list, t: int):
    """Return True if all numbers in the list l are below threshold t.
    >>> below_threshold([1, 2, 4, 10], 100)
    True
    >>> below_threshold([1, 20, 4, 10], 5)
    False
    """
    for e in l:
        if e >= t:
            return False
    return True
# ---
def subsample_fastqs(path_fastqs, num_files=10, num_sequences=1000):
    for i, path_fastq in enumerate(path_fastqs):
        if i >= num_files:
            return
        with open(path_fastq) as fastq_inf:
            fastq_gen = read_fastq(fastq_inf)
            yield limit_fastq(fastq_gen, num_sequences=num_sequences)
# ---
def __str__(self):
        year = ""
        if self.__year is not None:
            year = " in {0}".format(self.__year)
        return "{0} by {1}{2}".format(self.__title, self.__artist, year)
# ---
def __is_public_valid_method(self,attr):
        if inspect.ismethod(getattr(self, attr)) and attr[0] != '_' and\
                attr != 'register_rpc' and attr!='register_method_args':
                    return True
        return False
# ---
def test_member_route_requires_login(self):
        # Ensure member route requres logged in user.
        response = self.client.get("/members", follow_redirects=True)
        self.assertIn(b"Please log in to access this page", response.data)
# ---
def test_repr(self):
        expr = col("a") + col("b")
        assert repr(expr) == "(col('a') + col('b'))"
# ---
def the_object_name_does_not_exist(name):
    assert bpy.data.objects.get(name) is None, "Object exists"
# ---
def __init__(self):
        self.reused_arrays: list[tuple[ArrayLike, list[str]]] = []
        self.static_arrays: list[str] = []
# ---
def indented(self, width: int) -> Generator[None, None, None]:
        self.env["indent_width"] += width
        try:
            yield
        finally:
            self.env["indent_width"] -= width
# ---
def coscheduling_group_by(self) -> str | None:
        """The attribute key used to group workers for coscheduling, or None."""
        if self.is_coscheduled:
            return self.request.coscheduling.group_by
        return None
# ---
def __setattr__(self, name, value):
        """ Set slot or non-slot field attribute. """
        try:
            object.__setattr__(self, name, value)
        except AttributeError:
            if self._attrs:
                self._attrs[name] = value
            else:
                self._attrs = {name: value}
# ---
def test_running(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_RUNNING) == JobStatus.RUNNING
# ---
def _find_all_recursive(bundle_path: Path, pattern: str) -> list[Path]:
    return list(bundle_path.rglob(pattern))
# ---
def test_retry_budget_exact(cluster):
    """Task fails exactly N-1 times, succeeds on last attempt."""
    _url, client = cluster
    enable_chaos("worker.create_container", failure_rate=1.0, max_failures=2, error=RuntimeError("chaos: transient"))
    job = submit(client, _quick, "exact-retry", max_retries_failure=2)
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def _convert_id_to_token(self, index: int) -> str:
        return str(index)
# ---
def __init__(self, handle: IrisActorHandle, method_name: str):
        self._handle = handle
        self._method = method_name
# ---
def qdq_and_return(x, q_dtype, scale, amax_history, compute_dtype):
    dtype_max = get_fp8_max(q_dtype, jnp.float32)
    amax_from_history = jnp.max(amax_history, axis=0)
    new_scale = compute_scale(amax_from_history, scale, dtype_max)

    qx = quantize_dequantize(x, q_dtype, new_scale, compute_dtype)

    new_history = compute_amax_history(x, amax_history)

    return qx, new_scale, new_history
# ---
def __init__(self):
        self.flat_map_count = 0
        self.map_count = 0
        self.processed_ids = []
# ---
def reset(self) -> None:
        self._attempt = 0
# ---
def gte(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def build(self, ctx: LrScheduleContext):
        return _inv_decay_schedule(ctx.learning_rate, ctx.min_lr, ctx.decay_steps)
# ---
def act(self, s):
        if random.random() < self.explore_rate:
            return random.randint(0, NUM_ACTIONS - 1)
        else:

            return np.argmax(self.brain.predict(s))
# ---
def can_pack(self, ids: list[int]) -> bool:
        return len(ids) + len(self._ids) <= self.Pos.size and self.num_segments < self.max_pack_size
# ---
def on_step(self, info: StepInfo[S], force: bool = False):
        self.fn(info)
# ---
def infer_parquet_schema(record: dict[str, Any] | Any):
    """Infer PyArrow schema from a dictionary record."""
    import pyarrow as pa

    if is_dataclass(record):
        record = asdict(record)

    fields = []
    for key, value in record.items():
        fields.append((key, infer_parquet_type(value)))

    return pa.schema(fields)
# ---
def mean(self) -> Scalar:
        return self.sum / self.num
# ---
def write(self):
        '''
        Write MIDI data as a file to the file opened with `.open()`.
        '''
        self.file.write(self.writestr())
# ---
def num_cpus(self) -> int:
        """The number of CPUs this processor needs to run."""
        return self.bt.num_cpus
# ---
def read_all_jsonl_gz():
    """Fixture to read all JSONL gzipped files from a directory."""

    def _read_all(directory: Path, pattern: str = "*.jsonl.gz") -> list[dict]:
        records = []
        for file_path in sorted(directory.glob(pattern)):
            with gzip.open(file_path, "rt", encoding="utf-8") as handle:
                for line in handle:
                    if line.strip():
                        records.append(json.loads(line))
        return records

    return _read_all
# ---
def test_decay_phase():
    ma = EmaDecaySqrtModelAveraging(model=_dummy(0.0), beta=0.0, switch_step=0, decay_steps=4)
    ma = ma.update(_dummy(1.0), step=0)
    ma_end = ma.update(_dummy(10.0), step=4)
    assert jnp.allclose(ma_end.model["p"], 1.0)
    assert jnp.isclose(ma_end.tot_weight, 1.0)
# ---
def __init__(self):
        self.wake = Mock()
        self.kill_tasks_on_workers = Mock()
# ---
def is_pallas_dslice(x: object) -> bool:
    return isinstance(x, Slice)
# ---
def test_contains(self):
        prio_set_list = event._PrioritizedSetList()
        obj = object()

        prio_set_list.add(0, obj)
        assert obj in prio_set_list
# ---
def test_reinitialize_some_tokens_invalid_tokens(local_gpt2_tokenizer):
    # Test with tokens not in vocabulary
    tokenizer = local_gpt2_tokenizer
    model = None

    with pytest.raises(ValueError, match="One or more tokens are not in the tokenizer vocabulary"):
        reinitialize_some_tokens(model, tokenizer, ["<mklamnfljkaf>"], jax.random.PRNGKey(0))
# ---
def submit_job(ctx, entrypoint, working_dir, runtime_env):
    """Submit a Ray job."""
    runtime_env_dict = json.loads(runtime_env) if runtime_env else None

    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        job_id = _submit_job(entrypoint, working_dir, runtime_env_dict)
        print(f"Job submitted with ID: {job_id}")
# ---
def dummy_fn():
      # pylint: disable=pointless-statement
      value
# ---
def __init__(self):
        self.message = "Downloading.."
        self.total_bytes = 0
        self.current_bytes = 0
        self.completed = False
        self.error_occured = False
        self.start_time = 0
        self.file_name = ""
        self.kbps = 0
        self.assembly = False
        self.assembly_percent = 0
# ---
def _get_key_str(self, minion_id, status):
        '''
        Return the key string in the form of:

        pub: <pub>
        verify: <verify>
        '''
        path = os.path.join(self.opts['pki_dir'], status, minion_id)
        with salt.utils.fopen(path, 'r') as fp_:
            keydata = self.serial.loads(fp_.read())
            return 'pub: {0}\nverify: {1}'.format(
                    keydata['pub'],
                    keydata['verify'])
# ---
def refund(self, params=None):
        if params is None:
            params = dict()
        if hasattr(self, 'lineitem_id'):
            params['lineitem_id'] = self.lineitem_id
            url = 'sales/refund_lineitem'
        elif hasattr(self, 'invoice_id'):
            params['invoice_id'] = self.invoice_id
            url = 'sales/refund_invoice'
        else:
            params['sale_id'] = self.sale_id
            url = 'sales/refund_invoice'
        return Sale(Api.call(url, params))
# ---
def Caller_Hears_Dialtone (self):
        self.Step (Message = "Caller hears dial-tone...")

        self.Log (Message = "Caller agent waits for dial-tone...")
        self.Caller.sip_phone.Wait_For_Dialtone ()
# ---
def lte(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def _get_fixed_ips_subnets(self, context, gw_port):
        nw = self._core_plugin.get_network(context, gw_port['network_id'])
        subnets = [{'subnet_id': s} for s in nw['subnets']]
        return subnets
# ---
def is_root(self) -> bool:
        """True if this is a root job (no parent)."""
        return len(self._parts) == 1
# ---
def is_slow(self):
        for sort in self.sorts:
            if sort.is_slow():
                return True
        return False
# ---
def fn(config: Dataclass | None):
        append_log(log, config)
# ---
def redraw(self, _, ctx):
        ctx.set_antialias(cairo.ANTIALIAS_SUBPIXEL)
        for wid in self.widgets:
            wid.redraw(ctx)
# ---
def take(self, axis: AxisSelector, index: int | "NamedArray") -> "NamedArray":  # pragma: no cover
        return haliax.take(self, axis=axis, index=index)
# ---
def test_spatial_dropout_3d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout3D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4, 4, 5))

    testing_utils.layer_test(
        keras.layers.SpatialDropout3D,
        kwargs={'rate': 0.5, 'data_format': 'channels_first'},
        input_shape=(2, 3, 4, 4, 5))
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        """Map from Levanter model names to HF."""
        return {"transformer": "model", "embeddings": None}
# ---
def matchfn(self, f):
        return False
# ---
def nested_scan_outer_block(nested_remat, axis_size):
        outer_block_size: int | None
        if nested_remat is True:
            outer_block_size = find_closest_divisible_int_to_sqrt(axis_size)
        elif nested_remat is False:
            outer_block_size = None
        else:
            outer_block_size = nested_remat
        return outer_block_size
# ---
def _mesh_context():
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape((len(devices),))
    mesh = jax.sharding.Mesh(mesh_devices, axis_names=("data",))
    with hax.partitioning.set_mesh(mesh):
        yield
# ---
def test_encrypt_really_long_message(self):
        self._test_encryption(''.join(['abcd' for i in xrange(1024)]))
# ---
def seq_to_names(self, sequence):
        return([self.speakers[id] for id in sequence])
# ---
def test_read_string_table(datadir):
    datadir.join('reader').chdir()
    src = 'sharedStrings.xml'
    with open(src) as content:
        assert read_string_table(content.read()) == [
            'This is cell A1 in Sheet 1', 'This is cell G5']
# ---
def jit_grad_accum(mlp, x):
        grad_fn = eqx.filter_value_and_grad(loss_fn, has_aux=True)
        grad_fn = microbatched(grad_fn, Batch, parallelism, axis_mapping, axis_mapping)
        (acc_v, acc_aux), acc_g = grad_fn(
            mlp,
            x,
        )
        return acc_v, acc_g
# ---
def __del__(self):
        if self.handle:
            self.terminate()
# ---
def with_sliding_window(self, sliding_window: int | None) -> "AttentionMask":
        """Return a copy of this mask with ``sliding_window`` applied."""
        return AttentionMask(
            is_causal=self.is_causal,
            causal_offset=self.causal_offset,
            explicit_mask=self.explicit_mask,
            segment_ids=self.segment_ids,
            sliding_window=sliding_window,
        )
# ---
def broadcast_qs(_, ps, q, s):
                stack_n = ps[0]
                if partition_grads_into_blocks:
                    # add leading dim for stacked partitions
                    q = jax.tree.map(lambda x: jnp.repeat(jnp.expand_dims(x, 0), stack_n, axis=0), q)
                if s > 0:
                    # add leading dim if we're scanning this layer
                    q = jax.tree.map(lambda d: jnp.repeat(jnp.expand_dims(d, 0), s, axis=0), q)
                return q
# ---
def mock_optimizer_transform() -> optax.GradientTransformation:
    def init_fn(params):
        return {"count": jnp.array(0, dtype=jnp.int32)}

    def update_fn(updates, state, params=None):
        new_state = {"count": state["count"] + 1}
        # simple transformation: negate gradients
        transformed_updates = jax.tree_util.tree_map(lambda g: -g, updates)
        return transformed_updates, new_state

    return optax.GradientTransformation(init_fn, update_fn)
# ---
def task_index(self) -> int:
        """0-indexed task number within the job."""
        return self.task_id.require_task()[1]
# ---
def hash_func(obj: Any) -> int:
    """Convert arbitrary objects to i128 integers for hashing."""
    h = sha256(dumps(obj)).digest()
    return int.from_bytes(h[:16], "big", signed=True)
# ---
def __init__(self, parent: ContentOfGroup):
        self.parent = parent
        self.between = ast.Between()
        self.min = []
# ---
def _init_core(core_cfg: GrugModelConfig, key: PRNGKeyArray):
    from levanter.grug.model import init_parameters as grug_init

    return grug_init(core_cfg, key=key)
# ---
def normalize_by_func(self,f_data,z_data,func):
		return z_data/func(f_data)
# ---
def is_note_on(self):
        '''
        return a boolean if this is a NOTE_ON message and velocity is not zero_

        >>> mt = MidiTrack(1)
        >>> me1 = MidiEvent(mt)
        >>> me1.type_ = "NOTE_ON"
        >>> me1.velocity = 120
        >>> me1.is_note_on()
        True
        >>> me1.is_note_off()
        False
        '''
        return self.type_ == "NOTE_ON" and self.velocity != 0
# ---
def _run_prefill(
    gen_state: GenState,
    model: LmHeadModel,
    sampler: Sampler,
    work: PrefillWork,
    max_seqs_in_prefill: int,
) -> tuple[GenState, _DecodeOutputs]:
    gen_state = _apply_prefill_work(gen_state, work)
    return _prefill_kernel(gen_state, model, sampler, work.queue, max_seqs_in_prefill)
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "WhisperDecoder":
        new_embeddings = self.embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, embeddings=new_embeddings)
# ---
def index(self):
        html = header
        (_1, _2, id) = getradio('0')
        (radio, genre, url) = getradio(id)

        if id != 0:
            html += '''<h3><a href="#" onClick="fplayradio('%s')"> ''' % id
            html += '''Play Last Radio %s <span class="glyphicon glyphicon-play"></span></a></h3>''' % radio
        html += getfooter()
        return html
# ---
def close(self) -> None:
        if self.vllm_server is not None:
            self._backend.stop(self.vllm_server)
            self.vllm_server = None
# ---
def is_active(self):
        """
        Returns True when the start date is today or in the past and the
        task has not yet been completed.
        """
        start = self.start_date()
        return not self.is_completed() and (not start or start <= date.today())
# ---
def test_current_pixel_wraps_bottom(self):
        self.mda.current_pixel = [719, 349]
        self.mda.io_read_byte(0x3BA)
        self.assertEqual(self.mda.current_pixel, [0, 0])
# ---
def the_processor(self) -> ProcessorMixin:
        return load_processor(self.processor)
# ---
def __unicode__(self):
        return "%s: %s" %(self.TIPOS[self.tipo][1],self.msg)
# ---
def reparam(self) -> AbstractLinearReparam:
        return self._reparam_cls(self.In, self.Out)
# ---
def to_task_spec(self) -> list[str | dict]:
        """
        Convert task specifications to a list of dictionaries or strings.

        Returns:
            List of task specifications, with TaskConfig objects converted to dictionaries
        """
        return [task.to_dict() if isinstance(task, TaskConfig) else task for task in self.task_spec]
# ---
def _chips_per_slice(tpu_type: str) -> tuple[int, int]:
    config = TPU_CONFIG_BY_NAME.get(tpu_type)
    if config is None:
        raise ValueError(f"Unknown TPU type: {tpu_type}")

    try:
        _, suffix = tpu_type.split("-", maxsplit=1)
        reported_per_slice = int(suffix)
    except (ValueError, TypeError) as exc:  # pragma: no cover - defensive
        raise ValueError(f"Unexpected TPU type format: {tpu_type}") from exc

    return config.chip_count, reported_per_slice
# ---
def teardown(self):
        self.drain_actor_pool()
        self._slice_info = None
# ---
def connection_lost(self, exception):
        """\
        Called when the serial port is closed or the reader loop terminated
        otherwise.
        """
        if isinstance(exception, Exception):
            logger.debug('Connection to port `%s` lost: %s', self.port,
                         exception)
        else:
            logger.debug('Connection to port `%s` closed', self.port)
        self.connected.clear()
        self.disconnected.set()
# ---
def test_current_pixel_wraps_right(self):
        self.mda.current_pixel = [719, 0]
        self.mda.io_read_byte(0x3BA)
        self.assertEqual(self.mda.current_pixel, [0, 1])
# ---
def _engine_fixture(self):
        buf = util.StringIO()
        def dump(sql, *multiparams, **params):
            buf.write(util.text_type(sql.compile(dialect=engine.dialect)))
        engine = create_engine('postgresql://', strategy='mock', executor=dump)
        return engine, buf
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "google/gemma-2b"
    ) -> HFCheckpointConverter["GemmaConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self,
            reference_checkpoint=ref_checkpoint,
            trust_remote_code=True,
            HfConfigClass=HfGemmaConfig,
        )
# ---
def readline(self, count=None): pass
# ---
def isexact(self):
        return self._m1.isexact() or self._m2.isexact()
# ---
def test_count_multiplication(self):
        """Count multiplication"""
        count = Count(4, 2)
        self.assertEqual(2 * count, 8)
# ---
def create_user(self, email=None, password=None, **extra_fields):
        return self._create_user(email, password, False, False,
                                 **extra_fields)
# ---
def pages_and_slots(self):
        token_dests = self.new_token_dests

        t_pages = hax.where(is_valid(token_dests), token_dests // self.page_size, INVALID)
        t_slots = hax.where(is_valid(token_dests), token_dests % self.page_size, INVALID)

        return t_pages, t_slots
# ---
def _get_comment_invoice(self, cr, uid, picking):
        """
        @return: comment string for invoice
        """
        return picking.note or ''
# ---
def __call__(self, env, start_response):
        return env
# ---
def posts():
    return [make_post() for _ in range(3)]
# ---
def _read_ovf(job_id):
    file_name = os.path.join(_V2V_DIR, "%s.ovf" % job_id)
    try:
        with open(file_name, 'r') as f:
            return f.read()
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
        raise NoSuchOvf("No such ovf %r" % file_name)
# ---
def __sub__(self, other: "PyExpr") -> "PyExpr": ...
# ---
def _rebuild_named_array(like, array):
        if is_named_array(array):
            return array

        if is_named_array(like):
            return hax.NamedArray(array, like.axes)
        else:
            return array
# ---
def description(self):
        return self._description
# ---
def ship(self, params=None):
        if params is None:
            params = dict()
        params['sale_id'] = self.sale_id
        return Sale(Api.call('sales/mark_shipped', params))
# ---
def _simplify_captures(expr):
    def simplify_capture(capture):
        if capture == Ellipsis:
            return Ellipsis
        elif (capture.binding == capture.axes[0] or capture.binding is None) and len(capture.axes) == 1:
            return capture.axes[0]
        elif capture.binding is None:
            return capture.axes
        else:
            return {capture.binding: capture.axes}

    return [simplify_capture(capture) for capture in expr.captures]
# ---
def tearDown(self):
        crypto._fernet = None
# ---
def tunnel(
        self,
        controller_address: str,
        local_port: int | None = None,
        timeout: float | None = None,
        tunnel_logger: logging.Logger | None = None,
    ) -> AbstractContextManager[str]:
        """Return direct connection for manual platform (no tunnel needed)."""
        return nullcontext(controller_address)
# ---
def replace(self, idx: int, value: PageCacheT) -> "ListCache[PageCacheT]":
        caches = list(self.caches)
        caches[idx] = value
        return ListCache(tuple(caches))
# ---
def main():
    for name, (input_ext, transform_fn) in DATASETS.items():
        regenerate_dataset(name, input_ext, transform_fn)
# ---
def get_inherited(self, name, default = UnboundLocalError, depth = 1):
        ctx = self.get_containing(name, depth = depth)
        if ctx is None:
            if default is UnboundLocalError:
                raise AttributeError('Attribute %s not found in %s' % (name, self))
            return default
        return object.__getattribute__(ctx, name)
# ---
def Embed(self) -> Axis:
        return Axis(name="embed", size=self.hidden_dim)
# ---
def name(self) -> str:
        """Name of this scale group."""
        return self._config.name
# ---
def __init__(self, tracker, time, direction, opcode, data):
        print(str(type(self)).split('.')[3]+'('+str(len(data))+'): '+ str(data.get_array_hex(1))[1:-1])
# ---
def resolve(self, host, port, *args, **kwargs):
        if (host, port) in self.mapping:
            host, port = self.mapping[(host, port)]
        elif host in self.mapping:
            host = self.mapping[host]
        return self.resolver.resolve(host, port, *args, **kwargs)
# ---
def find_date_and_format(string):
            for ord, format in enumerate(cls.date_formats):
                for format_option in format:
                    try:
                        date = datetime.strptime(string, format_option)
                        return date, ord
                    except ValueError:
                        # Parsing failed.
                        pass
            return (None, None)
# ---

def solution(lst):
    """Given a non-empty list of integers, return the sum of all of the odd elements that are in even positions.


    Examples
    solution([5, 8, 7, 1]) ==> 12
    solution([3, 3, 3, 3, 3]) ==> 9
    solution([30, 13, 24, 321]) ==>0
    """
    return sum([x for idx, x in enumerate(lst) if idx%2==0 and x%2==1])
# ---
def is_in_preformatted(el):
            return el.name == "pre" or el.find_parent("pre")
# ---
def get_updated_line_contents(updates=None):
    test_contents = copy.deepcopy(test_line_contents)
    if updates is not None:
        test_contents.update(updates)
    return test_contents
# ---
def query(self, *, prefix: str | None = None, limit: int = 200) -> list[BufferedLogRecord]: ...
# ---
def require_task(self) -> tuple["JobName", int]:
        """Return (parent_job, task_index) for task names.

        Raises:
            ValueError: If this name is not a task or has no parent.
        """
        task_index = self.task_index
        if task_index is None:
            raise ValueError(f"JobName is not a task: {self}")
        if self.parent is None:
            raise ValueError(f"Task has no parent job: {self}")
        return (self.parent, task_index)
# ---
def __init__(self, rules):
        """Initialize the 'or' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules
# ---
def test_check_health_returns_healthy_on_success():
    """check_health returns healthy result when curl succeeds."""
    conn = MagicMock()
    conn.run.return_value = MagicMock(returncode=0, stdout="OK")
    result = check_health(conn, port=10001)
    assert result.healthy is True
# ---
def message_data(self):
        if self.proto_view == 0:
            return self.message.plain_bits_str
        elif self.proto_view == 1:
            return self.message.plain_hex_str
        elif self.proto_view == 2:
            return self.message.plain_ascii_str
        else:
            return None
# ---
def evaluate(self, record: dict) -> bool:
        left_val = self.left.evaluate(record)
        right_val = self.right.evaluate(record)
        return _COMPARE_OPS[self.op](left_val, right_val)
# ---
def test_evaluate_missing_column(self):
        expr = col("name")
        assert expr.evaluate({"other": "bob"}) is None
# ---
def _generate(cls, create, attrs):
        note = super()._generate(create, attrs)
        note.pubdate = datetime.now()
        note.save()
        note.related_content.last_note = note
        note.related_content.save()
        return note
# ---
def exists(self):
        if self.want.member_id is None:
            return False
        uri = 'https://{0}:{1}/mgmt/cm/device/licensing/pool/regkey/licenses/{2}/offerings/{3}/members/{4}'.format(
            self.client.provider['server'],
            self.client.provider['server_port'],
            self.want.pool_id,
            self.want.key,
            self.want.member_id
        )
        resp = self.client.api.get(uri)
        if resp.status == 200:
            return True
        return False
# ---
def shutdown(self) -> None:
        self._inference_server.shutdown()
# ---
def address(self) -> str:
        """Target address (IP, hostname, or VM identifier)."""
        ...
# ---
def logical_not(a: A) -> A:
    return wrap_elemwise_unary(jnp.logical_not, a)
# ---
def get_transfer_info(self) -> tuple[int | None, str | None]:
        """
        Returns the latest weight ID and transfer server address without blocking.
        Returns (None, None) if no weights are available or server not registered.
        """
        return self._latest_weight_id, self.transfer_server_address
# ---
def uri(self, uri):
        """
        Sets the uri of this ContributorOrcid.

        :param uri: The uri of this ContributorOrcid.
        :type: str
        """

        self._uri = uri
# ---
def __init__(self, region, name, retention_in_days=7):
    super(LogGroup, self).__init__()
    self.region = region
    self.name = name
    self.retention_in_days = retention_in_days
# ---
def path_from_key_path(key_path):
        return "/".join(key_path.split("."))
# ---
def f(x: NamedArray, *args, **kwargs):
        unnamed_x = x.array
        hax_out = hax_mod(x, *args, **kwargs)  # type: ignore
        eqx_out = eqx_mod(unnamed_x, *args, **kwargs)  # type: ignore

        assert jnp.allclose(hax_out.array, eqx_out)
        return hax_out
# ---
def fake_raise(*args, **kwargs):
            raise exception.MigrationError(reason='test failure')
# ---
def as_sync_dataset(self):
        return SyncifiedDataset(self)
# ---
def is_supported(self):
        return not self.capsule_types.isdisjoint(self.SUPPORTED_CAPSULE_TYPES)
# ---
def create_zstd_compressed_jsonl(records: list[dict]) -> bytes:
    jsonl_content = "\n".join(json.dumps(record) for record in records) + "\n"
    jsonl_bytes = jsonl_content.encode("utf-8")
    cctx = zstd.ZstdCompressor()
    return cctx.compress(jsonl_bytes)
# ---
def isexact(self):
        return self._m1.isexact()
# ---
def tearDown(self):
        super(BaseSystemTest, self).tearDown()
        for tablename in self.dynamo.list_tables():
            self.dynamo.delete_table(tablename)
        self.dynamo.clear_hooks()
# ---
def execute(self, context):
        started_at = datetime.now()
        while not self.poke(context):
            if (datetime.now() - started_at).total_seconds() > self.timeout:
                if self.soft_fail:
                    raise AirflowSkipException('Snap. Time is OUT.')
                else:
                    raise AirflowSensorTimeout('Snap. Time is OUT.')
            sleep(self.poke_interval)
        logging.info("Success criteria met. Exiting.")
# ---
def execute(self):
        with self._volumes():
            yield self._start_helper()
# ---
def create_paths_file(paths: list[str]) -> bytes:
    content = "\n".join(paths) + "\n"
    return gzip.compress(content.encode("utf-8"))
# ---
def _router_name(self, router_id):
        return N_ROUTER_PREFIX + router_id
# ---
def __repr__(self):
        return "{0.__class__.__name__}({0.subqueries!r})".format(self)
# ---
def closeEvent(self, event: QCloseEvent):
        settings.write("{}/geometry".format(self.__class__.__name__), self.saveGeometry())
        super().closeEvent(event)
# ---
def dec(*args, **kwargs):
            try:
                return self.view(*args, **kwargs)
            except PermissionRequired as e:
                kwargs['_perm'] = e.perm
                kwargs['_view'] = self.view
                return self.error_view(*args, **kwargs)
# ---
def test_accelerator_types_are_distinguishable():
    """Different accelerator types produce different display names."""
    types = [
        config_pb2.ACCELERATOR_TYPE_UNSPECIFIED,
        config_pb2.ACCELERATOR_TYPE_CPU,
        config_pb2.ACCELERATOR_TYPE_GPU,
        config_pb2.ACCELERATOR_TYPE_TPU,
    ]
    friendly_names = [accelerator_type_friendly(t) for t in types]
    assert len(friendly_names) == len(set(friendly_names)), "All accelerator types must have unique display names"
# ---
def append(self, row):
		"""
		Append a row at the end of the file.
		If the row does not have an id, one is automatically assigned.
		"""
		i = len(self) + 1 # FIXME this wont work properly in incomplete files
		if "_id" not in row:
			row["_id"] = i
		self[i] = row
# ---
def shape(self) -> Mapping[str, int]:
        """Mapping from axis name to size for the current view."""
        return {ax.name: ax.size for ax in self.axes}
# ---
def __getattr__(self, name):
        """Names of resource classes are accepted and resolved as dynamic attribute names.

        This allows convenient retrieval of resources as api.<resource-class>(id=<id>),
        or api.<resource-class>s(q='x').
        """
        return GET(self, name)
# ---
def convert_to_export(self, value, env):
        if value or value == 0.0:
            return value if env.context.get('export_raw_data') else ustr(value)
        return ''
# ---
def test_greater_than(self):
        expr = col("score") > 100
        assert expr.evaluate({"score": 50}) is False
        assert expr.evaluate({"score": 100}) is False
        assert expr.evaluate({"score": 150}) is True
# ---
def canonicalMachineName(machine=''):
    aliases = {'nstxu': ['nstx', 'nstxu', 'nstx-u'],
               'diiid': ['diiid', 'diii-d', 'd3d'],
               'cmod': ['cmod', 'c-mod']}
    for key, value in aliases.items():
        if machine.lower() in value:
            return key
    # invalid machine name
    raise FdpError('"{}" is not a valid machine name\n'.format(machine))
# ---
def setUp(self):
        super(JsonYamlResolvedCompareTest, self).setUp()
        self.longMessage = True
        self.maxDiff = None
# ---
def message_unsubscribe(self, *args, **kwargs):
        """Send the unsubscribe action on stock.picking model to match with subscribe"""
        return self.pool.get('stock.picking').message_unsubscribe(*args, **kwargs)
# ---
def _batchify_ctor(ctor, batch_dims):
        # this is gross but it basically just vmaps the ctor over each batch dimension
        return functools.reduce(lambda ctor, batch_axis: vmap(ctor, batch_axis), reversed(batch_dims), ctor)
# ---
def __mul__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.multiply(self, other)
# ---
def test_permutation_with_array_returns_correct_values(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    indices = jnp.arange(length)
    results = permutation(indices)
    assert isinstance(results, numpy.ndarray)
    assert len(results) == length
    assert jnp.sum(results == indices) <= 2
# ---
def _unwrap(x):
        if isinstance(x, Metric):
            return x.value()
        return x
# ---
def create_test_rollout_stats(episode_reward: float, lesson_id: str = "test") -> RolloutStats:
    """Helper to create rollout stats for testing."""
    return RolloutStats(
        lesson_id=lesson_id, episode_reward=episode_reward, env_example_id="test_example", temperature=1.0, top_k=8
    )
# ---
def stop(self, timeout: Duration = Duration.from_seconds(10.0)) -> None:
        """Signal VM thread to stop and wait for it to exit."""
        self._threads.stop(timeout=timeout)
# ---
def test_fixed_window_with_elastic_expiry_cluster(self):
        storage = MemcachedStorage("memcached://localhost:22122,localhost:22123")
        limiter = FixedWindowElasticExpiryRateLimiter(storage)
        per_sec = RateLimitItemPerSecond(2, 2)

        assert limiter.hit(per_sec)
        time.sleep(1)
        assert limiter.hit(per_sec)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert limiter.test(per_sec)
# ---
def test_log():
    with tempfile.TemporaryDirectory() as tmpdir:
        with SummaryWriter(logdir=tmpdir) as writer:
            tracker = TensorboardTracker(writer)
            tracker.log({"float": 2.0}, step=0)
            tracker.log({"str": "test"}, step=0)
            tracker.log({"scalar_jax_array": jnp.array(3.0)}, step=0)
            tracker.log({"scalar_np_array": np.array(3.0)}, step=0)
            tracker.log({"histogram": Histogram.from_array(jnp.array([1.0, 2.0, 3.0]))}, step=0)
# ---
def setUp(self):
        self.db = self.opendb()
        self.sample_data()
# ---
def test_fold_str_args():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    acc = hax.zeros((Height, Width))

    total = hax.fold(lambda x, y: x + y, "Depth")(acc, named1)

    assert jnp.all(jnp.isclose(total.rearrange(acc.axes).array, jnp.sum(named1.array, axis=2)))
# ---
def status(self) -> ControllerStatus:
        """Get controller status from GCP."""
        address = self.discover()
        if not address:
            return ControllerStatus(running=False, address=None, healthy=False, vm_name=None)

        vm_name = self._find_controller_vm_name()
        healthy = _check_health_rpc(address)

        return ControllerStatus(
            running=True,
            address=address,
            healthy=healthy,
            vm_name=vm_name,
        )
# ---
def broadcast_arrays_and_return_axes(
    *arrays: NamedOrNumeric | None, require_subset: bool = True, ensure_order: bool = True
) -> tuple[tuple[NamedOrNumeric | None, ...], tuple[Axis, ...]]: ...
# ---
def _new_model(old_model):
        return jax.tree_util.tree_map(lambda x: x * 0, old_model)
# ---
def __post_init__(self):
        if self.ports is None:
            self.ports = {}
# ---
def task_schedule_status(self, task: ControllerTask, context: SchedulingContext) -> TaskScheduleResult:
        """Get the current scheduling status of a task (for dashboard display)."""
        ...
# ---
def update_fn(updates, state, params=None):
        buf = state.momentum_buffer
        buf = jax.tree.map(
            lambda m, g: None if g is None else momentum * m + (1 - momentum) * g,
            buf,
            updates,
            is_leaf=lambda x: x is None,
        )

        updates = jax.tree_map(lambda u: None if u is None else jnp.sign(u), buf, is_leaf=lambda x: x is None)

        return updates, ScaleByScionState(momentum_buffer=buf)
# ---
def matchfn(self, f):
        for match in self._matchers:
            if match(f):
                return True
        return False
# ---
def decode(self, input_ids, kv_cache, batch_info, pos_ids):
        # Produce logits that prefer `eos` for every sampled position
        Pos = input_ids.resolve_axis("position")
        Vocab = self.Vocab
        # One-hot on vocab axis for eos token, broadcast over positions
        logits = hax.nn.one_hot(self.eos, Vocab, dtype=jnp.float32)
        logits = logits.broadcast_axis(Pos)
        return logits, kv_cache
# ---
def __init__(self, job, run_data):
        self.job = job
        self.id = run_data['id']
        self.status = run_data['status']
        self.stderr = run_data.get('logs_err', '')
        self.stdout = run_data.get('logs_out', '')
# ---
def partition(self, tensor):
        """Partition tensor into blocks."""

        assert tensor.shape == self._shape
        tensors = [tensor]
        for i, indices in self._splits:
            tensors_local = []
            for t in tensors:
                tensors_local.extend(jnp.split(t, indices_or_sections=indices, axis=i))
            tensors = tensors_local
        return tuple(tensors)
# ---
def __is_nice_string_using_old_rules(self, string):
        return (self.__regex_naughty.search(string) is None
            and len(self.__regex_vowels.findall(string)) > 2
            and self.__regex_double_char.search(string))
# ---
def conceptos(self):
        return self.__conceptos
# ---
def phoenixToOpenVGDB(self, phoenixID):
        ret = ""
        try:
            ret = self.phoenixToOpenVGDBMap[phoenixID]
        except KeyError:
            ret = ""
        return ret
# ---
def display_error(self, error):
        dialog = QtWidgets.QMessageBox(QtWidgets.QMessageBox.Warning, error.title, error.info, QtWidgets.QMessageBox.Ok, self)
        dialog.exec_()
# ---
def get_index(self, obj: T) -> int:
        return self._obj_to_index[obj]
# ---
def test_pd_DataFrame_from_series_par(self):
        def test_impl(n):
            S1 = pd.Series(np.ones(n))
            S2 = pd.Series(np.random.ranf(n))
            df = pd.DataFrame({'A': S1, 'B': S2})
            return df.A.sum()

        hpat_func = self.jit(test_impl)
        n = 11
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
        self.assertEqual(count_parfor_OneDs(), 1)
# ---
def test_make_choices_only_one():
    probs = pd.DataFrame(
        [[1, 0, 0], [0, 1, 0]], columns=['a', 'b', 'c'], index=['x', 'y'])
    choices = mnl.make_choices(probs)

    pdt.assert_series_equal(
        choices,
        pd.Series([0, 1], index=['x', 'y']))
# ---
def _call_splash_attention(q_, k_, v_, seg_ids, sinks_):
            return jax.vmap(
                lambda q_b, k_b, v_b, si: kernel(q_b, k_b, v_b, segment_ids=si, sinks=sinks_),
                in_axes=(0, 0, 0, 0),
            )(q_, k_, v_, seg_ids)
# ---
def __call__(self, target, cred):
        """Check the policy.

        Returns the logical inverse of the wrapped check.
        """

        return not self.rule(target, cred)
# ---
def check_valid_interface_type(interface_type: Optional[int]) -> None:
    if interface_type not in Service.ALLOWED_INTERFACE_TYPES:
        raise JsonableError(_("Invalid interface type"))
# ---
def num_classes(self):
    raise NotImplementedError()
# ---
def test_just_err_capture(self):
        with self.getcapture(out=False, err=True) as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            out, err = cap.readouterr()
        assert err == "world"
        assert not out
# ---
def push(self, item: T_co) -> None:
        """Add an item to the queue."""
        ...
# ---
def test_transaction_engine_ctx_commit(self):
        fn = self._trans_fn()
        ctx = testing.db.begin()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def done(self, lease: Lease[T_co]) -> None:
        if lease.lease_id in self.leases:
            del self.leases[lease.lease_id]
# ---
def _resolve_zones(
    group_config: config_pb2.ScaleGroupConfig,
    platform: config_pb2.GcpPlatformConfig,
) -> list[str]:
    if group_config.zones:
        return list(group_config.zones)
    if platform.default_zones:
        return list(platform.default_zones)
    if platform.zone:
        return [platform.zone]
    raise ValueError(f"No zones configured for scale group {group_config.name}")
# ---
def test_remove_from_empty_aggregate(self):
        values = {"name": 'fake_aggregate',
                  "availability_zone": 'fake_zone'}
        result = db.aggregate_create(self.context, values)
        self.assertRaises(exception.AggregateError,
                          self.conn._pool.remove_from_aggregate,
                          None, result, "test_host")
# ---
def _round_to_nearest_multiple(x: int, multiple: int) -> int:
    return ((x + multiple - 1) // multiple) * multiple
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "x": 1, "y": 2},
                {"id": 2, "x": 2, "y": 3},
                {"id": 3, "x": 3, "y": 4},
                {"id": 4, "x": 4, "y": 5},
            ],
        )
# ---
def setRow(self, key, **values):
		self.__setitem__(key, DBRow(self, columns=values))
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.setStyleSheet(self.STYLESHEET)
# ---
def terminate(self, job_id: JobId) -> None:
        """Terminate a running job.

        Attempts graceful termination first, then forceful kill if needed.

        Args:
            job_id: Job identifier

        Raises:
            KeyError: If job_id is not found
        """
        ...
# ---

def next_smallest(lst):
    """
    You are given a list of integers.
    Write a function next_smallest() that returns the 2nd smallest element of the list.
    Return None if there is no such element.

    next_smallest([1, 2, 3, 4, 5]) == 2
    next_smallest([5, 1, 4, 3, 2]) == 2
    next_smallest([]) == None
    next_smallest([1, 1]) == None
    """
    lst = sorted(set(lst))
    return None if len(lst) < 2 else lst[1]
# ---
def test_copy_experiment_inavlid_id(self):
        """ Tests that copy_experiment fails with bad experiment_id """
        url = reverse("ab_testing_tool_copy_experiment", args=(12345,))
        response = self.client.post(url, follow=True)
        self.assertEquals(response.status_code, 404)
# ---
def test_parse_paren_groups():
    lhs, rhs = parse_rearrangement("a (b c) d -> b c a d")
    assert lhs.is_ordered
    assert _simplify_captures(lhs) == ["a", ("b", "c"), "d"]
    assert rhs.is_ordered
    assert _simplify_captures(rhs) == ["b", "c", "a", "d"]

    lhs, rhs = parse_rearrangement("a (b: c) (d: e f) -> b c a d")
    assert lhs.is_ordered
    assert _simplify_captures(lhs) == ["a", {"b": ("c",)}, {"d": ("e", "f")}]
# ---
def strip_lines(self):
        """
        Remove excessive number of lines. This deletes the oldest half.
        """
        if (self.num_lines > MAX_NUM_STORED_LINES):
            for i in range(MAX_NUM_STORED_LINES // 2):
                self.lines.pop(i)
# ---
def namespace(self) -> Namespace:
        """Namespace derived from the root job ID.

        All jobs in a hierarchy share the same namespace, enabling actors
        to be discovered across the job tree.
        """
        if self.job_id is None:
            raise RuntimeError("No job id available - ensure IrisContext is initialized from a job")
        return Namespace.from_job_id(self.job_id)
# ---
def __init__(self, fmt):
                self.file_format = fmt
# ---
def __init__( self, **kwargs ):

    Client.__init__( self, **kwargs )
    opsH = Operations()
    self.maxResetCounter = opsH.getValue( 'Productions/ProductionFilesMaxResetCounter', 10 )

    self.setServer( 'Transformation/TransformationManager' )
# ---
def axis_spec_to_shape_dict(axis_spec: AxisSpec) -> dict[str, int]:  # type: ignore
    ...
# ---
def service(self, config):
        ''' setter for service property '''
        self.svc = config
# ---


def greatest_common_divisor(a: int, b: int) -> int:
    """ Return a greatest common divisor of two integers a and b
    >>> greatest_common_divisor(3, 5)
    1
    >>> greatest_common_divisor(25, 15)
    5
    """
    while b:
        a, b = b, a % b
    return a
# ---
def list_iris_containers(self, all_states: bool = True) -> list[str]: ...
# ---
def available_host_count(self) -> int:
        """Number of hosts available for new VM groups."""
        return len(self._available_hosts)
# ---
def test_data(request):
    data, choosers, spec, probabilities = request.param
    return {
        'data': data,
        'choosers': choosers,
        'spec': spec,
        'probabilities': probabilities
    }
# ---
def get_layer_types(self) -> Sequence[str]:
        if self.layer_types is not None:
            if len(self.layer_types) != self.num_layers:
                raise ValueError("layer_types must match num_layers")
            return list(self.layer_types)
        return ["sliding_attention" if (i + 1) % 4 != 0 else "full_attention" for i in range(self.num_layers)]
# ---
def requires(self):
        yield DownloadRITACatalogs()
        yield DownloadRITAData()
# ---
def make_message(self, record):
        tmplt = get_template('mailing/instructor_activity.txt')
        return tmplt.render(context=record)
# ---
def visit_msubsup(self, element):
        children = self._get_clean_children(element)
        if len(children) == 3:
            base = self._visit(children[0])
            sub = self._visit(children[1])
            sup = self._visit(children[2])
            return BracedNode(f"{{{base}}}_{{{sub}}}^{{{sup}}}")
        return TextNode("")
# ---
def regenerate(c):
    """Automatically regenerate site upon file modification"""
    c.run('pelican -r -s pelicanconf.py')
# ---
def simple_process(processor, source):
    result = []
    for shard_name in source.shard_names:
        for batch in source.open_shard(shard_name):
            result.append(processor([batch])[0])

    return result
# ---
def log_summary(self, metrics: Mapping[str, Any]):
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "summary",
                "metrics": metrics,
            }
        )
        self.logger.info(json.dumps(record))
        self._summary_metrics.update(_flatten(metrics))
# ---
def _terminate_process(process: subprocess.Popen) -> None:
    try:
        if sys.platform == "win32":
            process.kill()
        else:
            os.killpg(process.pid, signal.SIGKILL)
    except Exception as e:
        logger.info(f"Failed to terminate process {process.pid} -- already terminated? {e}")
# ---
def scheme(self):
        if (
            self.app.websocket_enabled
            and self.headers.get("upgrade") == "websocket"
        ):
            scheme = "ws"
        else:
            scheme = "http"

        if self.transport.get_extra_info("sslcontext"):
            scheme += "s"

        return scheme
# ---
def set_mouse_visible(self, visible=True):
        """Show or hide the mouse cursor.

        The mouse cursor will only be hidden while it is positioned within
        this window.  Mouse events will still be processed as usual.

        :Parameters:
            `visible` : bool
                If True, the mouse cursor will be visible, otherwise it
                will be hidden.

        """
        self._mouse_visible = visible
        self.set_mouse_platform_visible()
# ---
def init(config: LlamaConfig, *, key) -> "LlamaTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, LlamaDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return LlamaTransformer(config, layers, ln_f)
# ---
def get_user_id(user: UserProfile) -> int:
    return user.id
# ---
def __init__(self,domain='gw.api.taobao.com',port=80):
		RestApi.__init__(self,domain, port)
		self.user_nick = None
# ---
def inferred_head_dim(self) -> int:
        if self.head_dim is not None:
            return self.head_dim
        if self.hidden_dim % self.num_heads != 0:
            raise ValueError(
                f"hidden_dim={self.hidden_dim} is not divisible by num_heads={self.num_heads}; set head_dim explicitly"
            )
        return self.hidden_dim // self.num_heads
# ---
def after_execute(conn, clauseelement, multiparams, params, result):
            assert isinstance(multiparams, (list, tuple))
            assert isinstance(params, dict)
# ---
def save_file(self):
        if self.filename is not None:
            config_string = self.rootnode.dump()
            with open(self.filename, 'w') as f:
                f.write(config_string)
            self.dirty = False
        else:
            self.show_savecfg_dlg()
# ---
def finish_revert_migration(self, instance):
                self.finish_revert_migration_called = True
# ---
def genetic_modification_10(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'nucleic_acid_delivery_method': ['transduction'],
        'introduced_elements': 'gRNAs and CRISPR machinery',
    }
# ---
def get_slice(indices):
        arr = array_or_slice[indices]
        if dtype is not None:
            arr = arr.astype(dtype)

        return arr
# ---
def set_sort(query, sort):
    query['sort'] = []
    for (key, sortdir) in sort:
        sort_dict = dict([(key, 'asc' if sortdir > 0 else 'desc')])
        query['sort'].append(sort_dict)
# ---
def __exit__(self, *args):
        if self._rpc_client:
            self._rpc_client = None
        if self._controller_client:
            self._controller_client.close()
        if self._manager:
            self._manager.stop()
        else:
            # Docker path cleanup
            for worker in self._workers:
                worker.stop()
            if self._controller:
                self._controller.stop()
            if self._temp_dir:
                self._temp_dir.cleanup()
# ---
def delete(self, path):
        ''' remove path from a dict'''
        try:
            entry = Yedit.get_entry(self.yaml_dict, path, self.separator)
        except KeyError:
            entry = None

        if entry is None:
            return (False, self.yaml_dict)

        result = Yedit.remove_entry(self.yaml_dict, path, self.separator)
        if not result:
            return (False, self.yaml_dict)

        return (True, self.yaml_dict)
# ---
def do_put(self, context, descriptor, reader, writer):
        pass
# ---
def versioned(value: T_co) -> VersionedValue[T_co]:
    if isinstance(value, VersionedValue):
        raise ValueError("Can't nest VersionedValue")
    elif isinstance(value, InputName):
        # TODO: We have also run into Versioned([InputName(...), ...])
        raise ValueError("Can't version an InputName")

    return VersionedValue(value)
# ---
def _merge_chunk_streams(exec_ctx, futures: list):
    active = {id(f): f for f in futures}

    while active:
        ready, _ = exec_ctx.wait(list(active.values()), num_returns=1)
        for gen in ready:
            try:
                items = exec_ctx.get(next(gen))
                yield from items
            except StopIteration:
                del active[id(gen)]
# ---
def __str__(self) -> str:
        if self.total == 0:
            return f"{self.level} total: 0"
        return (
            f"{self.method.capitalize()} {self.level.lower()} total: {self.total:,}, "
            f"dups: {self.dups:,} ({self.dups / self.total:.2%}), unique: {self.unique:,}, "
            f"dup_clusters: {self.dup_clusters:,}"
        )
# ---
def binary_cross_entropy_loss(
    logits: NamedArray,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: None = None,
) -> jnp.ndarray | NamedArray: ...
# ---
def validate_has_variants(self):
		if not self.has_variants and frappe.db.get_value("Item", self.name, "has_variants"):
			if frappe.db.exists("Item", {"variant_of": self.name}):
				frappe.throw(_("Item has variants."))
# ---
def update_elements():
    """Updates all elements with all new values received from the user application"""
    received_elements = request.get_json()
    home_services.update_elements(received_elements)
    return 'OK'
# ---
def test_str_get(self):
        def test_impl(df):
            B = df.A.str.split(',')
            return B.str.get(1)

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def custom_fn(pred):
        pred_embeddings, pred_lm_head = pred
        loss = fused_cross_entropy_loss_and_logsumexp_penalty(
            pred_embeddings,
            pred_lm_head,
            Contract=Embed,
            Label=Vocab,
            target_y=true_ids,
            reduction=None,
            logsumexp_weight=0.5,
            block_size=block_size,
            dtype=pred_embeddings.dtype,
        )

        return loss.mean().scalar()
# ---
def __getstate__(self):
        """Always load data in-memory before pickling"""
        self.load()
        # self.__dict__ is the default pickle object, we don't need to
        # implement our own __setstate__ method to make pickle work
        state = self.__dict__.copy()
        # throw away any references to datastores in the pickle
        state['_file_obj'] = None
        return state
# ---
def _get_batch_path(self, timestamp: float, counter: int) -> str:
        """Get path for batch with timestamp and hostname."""
        timestamp_int = int(timestamp * 1000000)  # microseconds for ordering
        return f"{self.path}/{timestamp_int:020d}_{self.hostname}_{counter:06d}.pkl"
# ---
def __repr__(self):
        return f"WrappedAsyncDataset({repr(self.dataset)})"
# ---
def setcash(self, cash):
        self.startingcash = self.cash = self.p.cash = cash
# ---
def __init__(self):
        urllib.request.AbstractHTTPHandler.__init__(self)
        self.httpconn = MockHTTPClass()
# ---
def test_domain(self):
        eq_(self.record.domain, None)
# ---
def _ensure_batched(x: jax.Array) -> tuple[jax.Array, bool]:
    """Ensure `x` has a leading batch dimension.

    Template convention:
    - if `x.ndim == 1`, treat it as a single example and add a batch axis.
    - otherwise treat it as already batched.

    Returns:
        (x_batched, added_batch_dim)
    """

    if x.ndim == 1:
        return x[None, :], True
    return x, False
# ---
def main():
    """Main function to run the parallel Llama 75M speedrun."""
    steps = default_speedrun("parallel_llama_75m", speedrun_config)
    executor_main(steps=steps)
# ---
def test_drawn(color, do_open, content, content_after):
    door_card = card.door(color)
    door_card.drawn(core.Core(DoorActor(do_open), agent.Observer(), content))
    assert content == content_after
# ---
def send_js(filename):
    return static_file(filename, root='js')
# ---
def kernel_display_name(self):
        return self.json['metadata']['kernelspec']['display_name']
# ---
def __init__(self, config: FakeVmManagerConfig):
        self._config = config
        self._lock = threading.Lock()
        self._slices: dict[str, FakeVmGroup] = {}
        self._slice_counter = 0
# ---
def _get_client():
        return get_test_client()
# ---
def string_match(cls, pattern, value):
        return pattern.search(cls._normalize(value)) is not None
# ---
def _get_hf_kernels():
    pytest.importorskip("torch")
    pytest.importorskip("transformers")
    from transformers.models.qwen3_next.modular_qwen3_next import (
        torch_chunk_gated_delta_rule as hf_chunk,
        torch_recurrent_gated_delta_rule as hf_recur,
    )

    return hf_chunk, hf_recur
# ---
def execute(self, stmt, params=None, **kw):
                if "test unicode returns" in stmt:
                    raise self.engine.dialect.dbapi.DatabaseError("boom")
                else:
                    return super(MockCursor, self).execute(stmt, params, **kw)
# ---
def advertiseAction(self, action):
        """Should the web interface even show the form for ACTION?"""
        if action not in self.knownActions:
            raise KeyError("unknown action")
        cfg = self.config.get(action, False)
        if cfg:
            return True
        return False
# ---
def test_impl(df):
            df2 = df[['A']]
            df2['A'] += 10
            return df2.A, df.A
# ---
def __enter__(self):
        'Return self for use in with ... as ... statement.'
        return self
# ---
def fake_is_vdi_pv(*args, **kwargs):
            return should_return
# ---
def _hello_tpu_job():
    """Simple job that prints and returns."""
    print("Hello from TPU!")
    return 42
# ---
def create(zone: str):
    """Create a TPU VM with a random fun name in the specified zone."""
    name = create_tpu_vm(zone)
    click.echo(f"Created TPU VM: {name}")
# ---
def _make_grug_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape(1, 1, 1, len(devices))
    return Mesh(
        mesh_devices,
        axis_names=("replica_dcn", "replica", "data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit, AxisType.Explicit, AxisType.Explicit),
    )
# ---
def test_attentionmask_materialize_sliding_window_only():
    mask = AttentionMask(is_causal=False, sliding_window=1)
    allowed = mask.materialize_mask(4, 4)
    expected = jnp.array(
        [
            [True, True, True, True],
            [False, True, True, True],
            [False, False, True, True],
            [False, False, False, True],
        ],
        dtype=bool,
    )
    assert allowed is not None
    assert allowed.shape == (4, 4)
    assert jnp.array_equal(allowed, expected)
# ---
def barrier():  # Wait
    compss_barrier()
# ---
def test_edit_experiment_view_nonexistent(self):
        """Tests edit_experiment when experiment does not exist"""
        e_id = NONEXISTENT_EXPERIMENT_ID
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(e_id,)))
        self.assertTemplateNotUsed(response, "ab_tool/edit_experiment.html")
        self.assertEquals(response.status_code, 404)
# ---
def test_impl(n):
            df1 = pd.DataFrame({'key1': np.arange(n), 'A': np.arange(n) + 1.0})
            df2 = pd.DataFrame({'key2': n - np.arange(n), 'A': n + np.arange(n) + 1.0})
            df3 = pd.concat([df1, df2])
            return df3.A.sum() + df3.key2.sum()
# ---
def nansum(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nansum, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def _get_cluster_spec(self) -> str:
        if self._address == "auto":
            base = "ray"
        elif self._config_path:
            base = f"ray:{self._config_path}"
        else:
            base = "ray"

        # Append namespace as query param if present
        if self._namespace:
            base += f"?namespace={self._namespace}"

        return base
# ---
def diagnostics(self, handle: VllmServerHandle, *, max_lines: int = 200) -> dict[str, str]:
        diagnostics: dict[str, str] = {}
        if handle.log_dir:
            diagnostics["vLLM native log dir"] = handle.log_dir
        diagnostics["vLLM native logs (tail)"] = self.logs_tail(handle, max_lines=max_lines)
        return diagnostics
# ---
def test_append_error():
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=1, dtype=jnp.float32)
        with pytest.raises(ValueError):
            builder.append(jnp.array([[1.0, 2.0]]))
# ---
def AddResponses(self, responses):
    artifact_id = responses.request_data["artifact_id"]
    # TODO(user): Check whether artifact collection succeeded.
    self.state.host_data[artifact_id] = list(responses)
# ---
def test_dispatch_delayed(cluster):
    """Dispatch delayed by chaos (via heartbeat), but eventually goes through."""
    _url, client = cluster
    enable_chaos("controller.heartbeat", delay_seconds=3.0, failure_rate=1.0, max_failures=2)
    job = submit(client, _quick, "delayed-dispatch")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def __post_init__(self):
        if " " in self.name:
            raise ValueError("Job name must not contain spaces")
        if self.replicas is None:
            # Pick up replicas from ResourceConfig (set by e.g. with_tpu slice_count)
            self.replicas = self.resources.replicas
# ---
def connect(self, *args, **kwargs):
        return self.__iowait(self._connection.connect, *args, **kwargs)
# ---
def _stop_on_signal(info: levanter.callbacks.StepInfo):
            if self._should_stop:
                raise StopTrainerException()
# ---
def monitor_loop(tpu_client: tpu_v2.TpuClient):
    """Main monitoring loop - runs indefinitely."""
    while True:
        try:
            for zone, count in config.TPU_ZONES_CONFIG.items():
                ensure_tpu_vms(tpu_client, zone, count)
        except Exception as e:
            logging.error(f"Error: {e}", exc_info=True)

        time.sleep(600)
# ---
def Embed(self) -> Axis:
        pass
# ---
def _stop_db_server(self, db_version):
        sudo('svcadm disable postgresql')
# ---
def _createNode(self, appName):
        """Create a new root node in the TreeStore model with the name of the
            application.

        Arguments:
        - appName: the name of the TreeStore Node (the same of the application)
        """

        model = self.keyBindingsModel

        myiter = model.append(None)
        model.set_value(myiter, DESCRIP, appName)
        model.set_value(myiter, MODIF, False)

        return myiter
# ---
def is_finite(self) -> bool:
        return self.dataset.is_finite()
# ---
def table(self, header, body):
        """Rendering table element. Wrap header and body in it.

        :param header: header part of the table.
        :param body: body part of the table.
        """
        return (
            '<table>\n<thead>%s</thead>\n'
            '<tbody>\n%s</tbody>\n</table>\n'
        ) % (header, body)
# ---
def fabs(a: A) -> A:
    return wrap_elemwise_unary(jnp.fabs, a)
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key=None, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids)
        return self.norm(x)
# ---
def notify_complete(self, result: str) -> str:
        """Called when token passing completes."""
        self.result = result
        self._done.set()
        return "ack"
# ---
def _explicitfiles(kindpats):
    """Returns the potential explicit filenames from the patterns.

    >>> _explicitfiles([(b'path', b'foo/bar', b'')])
    ['foo/bar']
    >>> _explicitfiles([(b'rootfilesin', b'foo/bar', b'')])
    []
    """
    # Keep only the pattern kinds where one can specify filenames (vs only
    # directory names).
    filable = [kp for kp in kindpats if kp[0] not in ("rootfilesin",)]
    return _roots(filable)
# ---
def _send_payload(self, payload):
        assert jax.process_index() == 0
        out = broadcast_shard(payload, hax.partitioning.infer_resource_partitions(payload))
        return out
# ---
def _createPreamble(self):
        """
        """
        ex = []
        ex.append('M48\n') # Beginning of a part program header
        ex.append('METRIC,TZ\n') # Metric, trailing zeros
        ex.append('G90\n') # Absolute mode
        ex.append('M71\n') # Metric measuring mode
        return ex
# ---
def size(self) -> int:
        """Get total number of rollouts across all environments."""
        with self._lock:
            return sum(len(rollouts) for rollouts in self.rollout_storage.values())
# ---
def create_vllm_inference_config():
    return vLLMInferenceContextConfig(
        model_name="Qwen/Qwen3-0.6B",
        max_model_len=1024,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.90,
        sampling_params=SamplingParams(
            temperature=1.0,
            n=4,
            max_tokens=16,
            logprobs=1,
            stop=None,
            # Workaround for vllm-project/tpu-inference#1386: default top_k forces greedy sampling
            top_k=4096,
        ),
    )
# ---
def create_router_precommit(self, context, router_context):
        pass
# ---
def getCounters( self, table, attrList, condDict, older = None, newer = None, timeStamp = None,
                   rpc = '', url = '' ):
    rpcClient = self._getRPC( rpc = rpc, url = url )
    return rpcClient. getCounters( table, attrList, condDict, older, newer, timeStamp )
# ---
def __int__(self):
		return self.id
# ---
def python_compute_paragraph_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> list[dict[str, str]]:
    results = []
    for record in batch.to_pylist():
        text, record_id = record[text_col], record[id_col]
        for para in text.split("\n"):
            results.append({"hash": _str_hash_legacy(para), "id": record_id})
    return results
# ---
def get_context(self, context):
		context.show_search = True
		context.search_link = '/product_search'

		context.parents = get_parent_item_groups(self.item_group)

		self.set_variant_context(context)
		self.set_attribute_context(context)
		self.set_disabled_attributes(context)

		return context
# ---
def key_str(self, match):
        '''
        Return the specified public key or keys based on a glob
        '''
        ret = {}
        for status, keys in six.iteritems(self.name_match(match)):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                ret[status][key] = self._get_key_str(key, status)
        return ret
# ---
def dependents(self):
        """ Return the computed fields that depend on ``self``. """
        return (field for field, path in self._triggers)
# ---
def as_scanned_result(self, scan_axis: Axis):
        return NamedArray(self.array, (scan_axis,) + self.main_axes)
# ---
def __init__(self, uri, vminfo, job_id, irs):
        super(XenCommand, self).__init__(vminfo, job_id, irs)
        self._uri = uri
        self._ssh_agent = SSHAgent()
# ---
def add_worker(self, worker: ControllerWorker) -> None:
        """Add worker directly to state (test utility).

        For production use, create Event(WORKER_REGISTERED) instead.
        This method bypasses event logging and is intended for test setup only.
        """
        with self._lock:
            self._workers[worker.worker_id] = worker
# ---
def iris_ctx() -> IrisContext:
    """Get the current IrisContext, raising if not in a job.

    Returns:
        Current IrisContext

    Raises:
        RuntimeError: If not running inside an Iris job
    """
    ctx = get_iris_ctx()
    if ctx is None:
        raise RuntimeError("iris_ctx() called outside an Iris job (no job info available)")
    return ctx
# ---
def path(self):
        """
        Gets the path of this ContributorOrcid.

        :return: The path of this ContributorOrcid.
        :rtype: str
        """
        return self._path
# ---
def badmatch(match, badfn):
    """Make a copy of the given matcher, replacing its bad method with the given
    one.
    """
    m = copy.copy(match)
    m.bad = badfn
    return m
# ---
def test_make_choices_real_probs(random_seed, utilities):
    probs = mnl.utils_to_probs(utilities)
    choices = mnl.make_choices(probs)

    pdt.assert_series_equal(
        choices,
        pd.Series([1, 2], index=[0, 1]))
# ---
def test_detail(self):
        resp = FakeResponse()
        self.type_action_controller.detail(self.req, resp)
        self.assertEqual(
            [{'id': fake.VOLUME_TYPE_ID,
              'os-volume-type-access:is_public': True},
             {'id': fake.VOLUME_TYPE3_ID,
              'os-volume-type-access:is_public': False}],
            resp.obj['volume_types'])
# ---
def __init__(self, array: NamedArray, slices: SliceSpec):
        self._array = array
        self._slices = slices
# ---
def sources(self) -> Mapping[str, AudioDatasetSourceConfig]:
        return self.configs
# ---
def __call__(self):
        with self._lock:
            if self._process and self._process.poll() is None:
                self.kill()

            print(f"Syncing changes to {self._host_alias}...")
            sync_to_remote(self._host_alias, self._sync_path)

            ssh_cmd = build_ssh_command(self._host_alias, self._command_str, self._env_dict)
            print(f"Running: {self._command_str}")
            self._process = subprocess.Popen(ssh_cmd, stdin=subprocess.DEVNULL)
# ---
def tobytes(obj):
        if isinstance(obj, unicode):
            obj = obj.encode('UTF-8')
        assert isinstance(obj, str)
        return obj
# ---
def insert(self, string):
        """
        Insert string at the end. This always begins a new line.
        """
        if (self.num_lines >= MAX_NUM_LINES):
            pass

        input_num_lines = num_lines(string)

        #if (input_num_lines > self.remaining_lines):
        #    num = self.remaining_lines
        #else:
        #    num = input_num_lines
        num = input_num_lines

        new_lines = get_lines(string)

        self.lines += new_lines[-num:]
        self.update_num_lines()
# ---
def init_w_grad():
        w_grad_tile_ref[...] = jax.lax.dot_general(
            x_ref[...],
            xw_scratch_ref[...],
            (((0,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        w_write_future.start()
# ---
def num_cpus(self) -> float | int:
        """The number of CPUs this processor needs to run."""
        raise NotImplementedError
# ---
def ensure_parent_dir(path: str) -> None:
    """Create directories for `path` if necessary."""
    # Use os.path.dirname for local paths, otherwise use fsspec
    if "://" in path:
        output_dir = path.rsplit("/", 1)[0]
        fs, dir_path = fsspec.core.url_to_fs(output_dir)
        if not fs.exists(dir_path):
            fs.mkdirs(dir_path, exist_ok=True)
    else:
        output_dir = os.path.dirname(path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
# ---
def min(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.min, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def __init__(self, model_name: str, attribute_name: str, *args, **kwargs):
        super().__init__(model_name, attribute_name)
# ---
def test_iris_run_cli_job_failure(local_cluster_and_config, tmp_path):
    """Test iris_run.py returns non-zero on job failure."""
    _test_config, url, _client = local_cluster_and_config

    test_script = tmp_path / "fail.py"
    test_script.write_text("exit(1)")

    exit_code = run_iris_job(
        controller_url=url,
        command=[sys.executable, str(test_script)],
        env_vars={},
        wait=True,
    )

    assert exit_code == 1
# ---
def actual_head_size(self) -> int:
        if self.head_dim is not None:
            return self.head_dim
        return self.hidden_dim // self.num_heads
# ---
def __str__(self):
        return self.email
# ---
def fetch_job(self):
        job_num = self.main_view.job_num
        if self.main_model.job_exists(job_num):
            self.main_view.show_job_already_exists_dialog()
            return

        self.main_model.fetch_job(job_num)
# ---
def get_strength(self, nick):
        return self.data[nick.lower()]['strength']
# ---
def terminate_job(self, job_id: JobName) -> None:
        request = cluster_pb2.Controller.TerminateJobRequest(job_id=job_id.to_wire())
        self._client.terminate_job(request)
# ---
def _default_tpu_name_from_config(config_data: dict | None, username: str) -> str | None:
    if not config_data:
        return None
    cluster_name = config_data.get("cluster_name")
    if not cluster_name:
        return None
    return f"dev-{cluster_name}-{username}"
# ---
def _draw_more_comments(self, win, data):

        n_rows, n_cols = win.getmaxyx()
        n_cols -= 1

        self.term.add_line(win, '{body}'.format(**data), 0, 1)
        self.term.add_line(
            win, ' [{count}]'.format(**data), attr=curses.A_BOLD)

        attr = Color.get_level(data['level'])
        self.term.addch(win, 0, 0, self.term.vline, attr)

        return attr | self.term.vline
# ---
def __unicode__(self):
		return self.titulo
# ---
def get_parent_id(self, name, attrs):
        final_attrs = self.build_attrs(attrs, type=self.input_type, name=name)
        return final_attrs['id']
# ---
def to_dataframe(self):
        """Convert this dataset into a pandas.DataFrame.

        Non-index variables in this dataset form the columns of the
        DataFrame. The DataFrame is be indexed by the Cartesian product of
        this dataset's indices.
        """
        return self._to_dataframe(self.dims)
# ---
def test_host_reboot(self):
        self._test_host_action(self.conn.host_power_action, 'reboot')
# ---
def build():
    """Image build commands."""
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        mog = otu.tree_zeros_like(params, dtype=mu_dtype)  # gradient from
        return ScaleByMarsState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu, mog=mog)
# ---
def pop(queue_name: str, lease_timeout: float = Body(default=60.0, embed=True)):
            if queue_name not in self.queues:
                return Response(status_code=404)
            lease = self.queues[queue_name].pop(lease_timeout)
            if lease is None:
                return Response(status_code=204)
            return {"lease_id": lease.lease_id, "timestamp": lease.timestamp, "payload": pickle.dumps(lease.item).hex()}
# ---
def get_tx_metadata(self, transaction_hash) -> list:
        with self.lock:
            return self._state.get_tx_metadata(transaction_hash)
# ---
def list_tasks(
        self,
        _request: cluster_pb2.Worker.ListTasksRequest,
        _ctx: RequestContext,
    ) -> cluster_pb2.Worker.ListTasksResponse:
        """List all tasks on this worker."""
        tasks = self._provider.list_tasks()
        return cluster_pb2.Worker.ListTasksResponse(
            tasks=[task.to_proto() for task in tasks],
        )
# ---
def kill_ray():
                            # silence spam from ray stop
                            os.system("bash -c 'ray stop -g 10 --force &> /dev/null'")
# ---
def test_tpu_device(self):
        resources = ResourceConfig(device=TpuConfig(variant="v5litepod-16"))
        spec = convert_resources(resources)
        assert spec.device is not None
        assert spec.device.HasField("tpu")
        assert spec.device.tpu.variant == "v5litepod-16"
# ---
def tearDown(self):
        super(TestHooks, self).tearDown()
        for hooks in self.dynamo._hooks.values():
            while hooks:
                hooks.pop()
# ---
def load_dedup_outputs(output_dir: str) -> dict[str, dict]:
    """Load all dedupe output files and return as id->doc mapping.

    Args:
        output_dir: Directory containing .jsonl.gz output files

    Returns:
        Dictionary mapping document IDs to document records
    """
    output_files = list(Path(output_dir).glob("**/*.jsonl.gz"))
    results = []
    for output_file in output_files:
        results.extend(load_jsonl(str(output_file)))
    return {r["id"]: r for r in results}
# ---
def test_vocab_size(tok):
    expected = 3 + tok.max_seq_len + tok.base_vocab_size
    assert tok.vocab_size == expected
# ---
def __call__(self, *, input_ids, attn_mask, pos_ids, key):
        return DummyModelOutput(self._logits)
# ---
def __init__(self, min_value, max_value, instance=None,
        can_delete_vote=True, template='ratings/like_widget.html', attrs=None):
        super(LikeWidget, self).__init__(attrs)
        self.min_value = min_value
        self.max_value = max_value
        self.instance = instance
        self.can_delete_vote = can_delete_vote
        self.template = template
# ---
def user_personal_website_url(self):
        return self._get_profile().personal_website_url
# ---
def test_rust_structs(benchmark: Any, in_memory_table: pa.Table) -> None:
    """
    Python Memory -> Converts to list of Rust 'Document' Classes -> Rust -> List of Rust Classes.
    """

    def _pipeline() -> int:
        docs = [dupekit.Document(row["id"], row["text"]) for row in in_memory_table.to_pylist()]
        return len(dupekit.process_rust_structs(docs))

    assert benchmark(_pipeline) > 0
# ---
def is_local_branch(git_path, module, dest, branch):
    branches = get_branches(git_path, module, dest)
    lbranch = '%s' % branch
    if lbranch in branches:
        return True
    elif '* %s' % branch in branches:
        return True
    else:
        return False
# ---
def _on_record_button_pressed(self):
        pass
# ---
def get_default_config_path(region: str) -> str:
    """Get default config path for a region."""
    return f"infra/marin-{region}.yaml"
# ---
def merge_lora_modules(module: M) -> M:
    """
    Merges LoRA modules into their wrapped modules. That is, it adds the LoRA parameters to the wrapped weights,
    producing a modified base model with no LoRA parameters.
    """

    def _merge_lora_modules(module):
        if isinstance(module, LoraLinear):
            return module.merge()
        else:
            return module

    return jax.tree_util.tree_map(_merge_lora_modules, module, is_leaf=lambda node: isinstance(node, LoraLinear))
# ---
def GetCurrentBufferFilepath():
  return GetBufferFilepath( vim.current.buffer )
# ---
def amax(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """
    Aliax for max. See max for details.
    """
    return wrap_reduction_call(jnp.amax, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def test_raw(self):
        image_meta = {'id': 'a', 'disk_format': 'raw'}
        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_RAW)
# ---
def test_best_of_n_respects_n(params, model_cfg, tokenizer):
    """Should not return more unique candidates than n."""
    n = 4
    results = best_of_n(
        params=params,
        source="x = 1\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(7),
        n=n,
        max_depth=2,
    )
    assert len(results) <= n
# ---
def _method(self, i):
        return i
# ---
def _has_nonzero_weight(self, name: str) -> bool:
        weights = self.train_weights
        if weights is None:
            return True
        if isinstance(weights, dict):
            return weights.get(name, 0) > 0
        return any(w.get(name, 0) > 0 for _, w in weights)
# ---
def __init__(self, names, pronouns, mc):
        self.speakers = [{"name": n, "pronoun": p} for n, p in list(zip(names, pronouns))]
        self._transitions = self.make_transition_probs()
        self._speech_acts = ["said", "whispered", "shouted", "cried"]
        self._acts_transitions = [25, 2, 2, 2]
        self.mc = mc
        # self.seeds = seeds
        self.target_len = np.random.randint(5, 50, size=len(names))
# ---
def _expose_cache_rows(cache_path: str, exemplar: T, num_rows: int) -> None:
    cache = TreeStore.open(exemplar, cache_path, mode="a", cache_metadata=False)
    futures = jax.tree.leaves(jax.tree.map(lambda x: x.offsets[0].write(num_rows), cache.tree))
    for future in futures:
        future.result()
# ---
def mask_fn(model):
                decayed_paths = []
                mask = jax.tree_util.tree_map(
                    partial(_apply_on, decayed_paths, from_class_keypath=None),
                    model,
                    leaf_key_paths(model, is_leaf=is_leaf),
                    is_leaf=is_leaf,
                )

                # log all decayed weights
                levanter.tracker.log_hyperparameters({"decayed_weights": sorted(decayed_paths)})

                return mask
# ---
def _canonicalize_batch(batch: Union[dict, List[dict]]) -> List[dict]:
    if isinstance(batch, pa.RecordBatch):
        batch = dict_from_record_batch(batch)

    if isinstance(batch, dict):
        keys = list(batch.keys())
        values = list(batch.values())
        num_rows = len(values[0]) if values else 0
        return [{key: values[i][j] for i, key in enumerate(keys)} for j in range(num_rows)]
    else:
        return list(batch)
# ---

def starts_one_ends(n):
    """
    Given a positive integer n, return the count of the numbers of n-digit
    positive integers that start or end with 1.
    """
    if n == 1: return 1
    return 18 * (10 ** (n - 2))
# ---
def reset(self):
        self._calls.reset()
# ---
def status(self, job_or_id) -> dict:
        job_id = self._to_job_id_str(job_or_id)
        request = cluster_pb2.Controller.GetJobStatusRequest(job_id=job_id)
        assert self._controller_client is not None
        response = self._controller_client.get_job_status(request)
        return {
            "jobId": response.job.job_id,
            "state": cluster_pb2.JobState.Name(response.job.state),
            "exitCode": response.job.exit_code,
            "error": response.job.error,
        }
# ---
def position_token_offset(self) -> int:
        """First position token ID."""
        return 3
# ---
def check_for_active_boms(self):
		if self.default_bom:
			bom_item = frappe.db.get_value("BOM", self.default_bom, "item")
			if bom_item not in (self.name, self.variant_of):
				frappe.throw(
					_("Default BOM ({0}) must be active for this item or its template").format(bom_item))
# ---
def stop(self) -> None:
        """No-op: ManualVmManager has no background threads to stop."""
        pass
# ---
def normalized_cross(a, b):
    """
    Returns the normalized cross product between vectors.
    Uses numpy.cross().

    Parameters:
        a: First vector.
        b: Second vector.
    """
    c = np.cross(a, b)
    length = sqrt(c.dot(c))
    return c/length if length > 0 else c
# ---
def debug(ctx):
    """Cluster debugging and validation commands.

    These commands discover the controller VM via GCP, establish SSH tunnels
    transparently, and provide operational tooling.
    """
    pass
# ---
def stop(self):
        """Stop worker if running."""
        if self.worker:
            self.worker.stop()
# ---
def fold_metrics_jit(m1, m2):
        return fold(m1, m2)
# ---
def bernoulli(key, shape: AxisSpec, p: NamedOrNumeric):
    shape = axis_spec_to_shape_dict(shape)
    p = broadcast_to(p, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.bernoulli(key=key, p=p, shape=jax_shape)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def get_batch_sync(self, indices) -> List[T]:
        # TODO: would be better to batch these up
        grouped = jtu.tree_map(lambda reader: reader.get_batch_sync(indices), self.tree, is_leaf=heuristic_is_leaf)

        out = [jtu.tree_map(lambda _, leaf: leaf[i], self.tree, grouped) for i in range(len(indices))]

        return out
# ---
def is_gentarget(self, target):
    return isinstance(target, GoThriftLibrary)
# ---
def set_location(self, x, y):
        """Set the position of the window.

        :Parameters:
            `x` : int
                Distance of the left edge of the window from the left edge
                of the virtual desktop, in pixels.
            `y` : int
                Distance of the top edge of the window from the top edge of
                the virtual desktop, in pixels.

        """
        raise NotImplementedError('abstract')
# ---
def convert_to_write(self, value, target=None, fnames=None):
        return value.id
# ---
def test_multiple_jobs_complete(self, test_cluster):
        """Multiple jobs complete successfully."""
        run_id = uuid.uuid4().hex[:8]

        def fast_job(n):
            return n * 2

        job_ids = [test_cluster.submit(fast_job, i, name=f"job-{run_id}-{i}") for i in range(5)]
        for job_id in job_ids:
            status = test_cluster.wait(job_id, timeout=30)
            assert status["state"] == "JOB_STATE_SUCCEEDED"
# ---

def minSubArraySum(nums):
    """
    Given an array of integers nums, find the minimum sum of any non-empty sub-array
    of nums.
    Example
    minSubArraySum([2, 3, 4, 1, 2, 4]) == 1
    minSubArraySum([-1, -2, -3]) == -6
    """
    max_sum = 0
    s = 0
    for num in nums:
        s += -num
        if (s < 0):
            s = 0
        max_sum = max(s, max_sum)
    if max_sum == 0:
        max_sum = max(-i for i in nums)
    min_sum = -max_sum
    return min_sum
# ---
def as_dict(self):
        pass
# ---
def refresh(self):
        self.metric.refresh()
        self.fill.refresh(self.metric.value)
# ---
def tok():
    return TreeDiffusionTokenizer(max_seq_len=512)
# ---
def __init__(self, hits=None, docs=None):
        """Parse hits into docs."""
        self.hits = hits if hits else self.no_hits
        self.docs = docs if docs else []
# ---
def __class_getitem__(self, axes_spec_: tuple[type[NamedArray], NamedArrayAxesSpec]):
            _, axes_spec = axes_spec_
            axes = _parse_namedarray_axes(axes_spec)
            return Annotated[NamedArray, axes]
# ---
def do(state):
                ref_counts, sequences = state
                pages_row = sequences.page_indices["seq", i]
                ref_counts = dec_refcounts_for_seq(pages_row, ref_counts)
                sequences = sequences.release_slot(i)
                return ref_counts, sequences
# ---
def vmap(self, *extra_args, **extra_kwargs):
        """Apply each block independently using :func:`haliax.vmap`.

        Returns the stacked outputs of each block.
        """

        return haliax.vmap(type(self.stacked).__call__, self.Block)(self.stacked, *extra_args, **extra_kwargs)
# ---
def test_inference_ctx():
    return create_test_inference_context()
# ---
def fsspec_mkdirs(dir_path, exist_ok=True):
    """
    Create a directory in a fsspec filesystem.

    Args:
        dir_path (str): The path of the directory
    """

    # Use fsspec to create the directory
    fs = fsspec.core.url_to_fs(dir_path)[0]
    fs.makedirs(dir_path, exist_ok=exist_ok)
# ---
def put(self, obj: Any):
        """Store object in Ray's object store."""
        return ray.put(obj)
# ---
def lr_scale(self):
        return 1
# ---
def resize_embeddings(self, new_size: int, key: PRNGKeyArray | None = None):
        new_weights = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, token_embeddings=new_weights)
# ---
def _swap_description_payee(self, apply_to_all):
        if apply_to_all:
            panes = self.panes
        else:
            panes = [self.selected_pane]

        def switch_func(txn):
            txn.description, txn.payee = txn.payee, txn.description

        self._swap_fields(panes, switch_func)
# ---
def loss_fn(x_in, w_in, y_in):
            return fused_cross_entropy_loss_and_logsumexp_penalty(
                x_in,
                y_in,
                w_in,
                reduction="mean",
                logsumexp_weight=0.0,
                block_sizes=block_sizes,
                dtype=jnp.float32,
                logit_soft_cap=None,
                implementation="pallas_tpu",
            )
# ---
def _job_detail_page(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Job Detail", "/static/controller/job-detail.js"))
# ---
def full_name(self):
        fn = self.first_name
        ln = self.last_name
        if fn and ln:
            name = u"%s %s" % (fn, ln)
        else:
            name = str(self.email)
        return name
# ---
def test_repr(self):
        """
        The instance's repr reflects its C{__dict__}
        """
        namespace = _api._SimpleNamespace()
        namespace.y = 2
        assert repr(namespace) == "namespace(y=2)"
# ---
def indexes(self):
        """OrderedDict of pandas.Index objects used for label based indexing
        """
        return Indexes(self)
# ---
def get_status(self) -> vm_pb2.AutoscalerStatus:
        """Get autoscaler status."""
        ...
# ---
def codespan(self, text):
        """Rendering inline `code` text.

        :param text: text content for inline code.
        """
        text = escape(text.rstrip(), smart_amp=False)
        return '<code>%s</code>' % text
# ---
def _environment(self):
        # Provide some sane environment
        env = os.environ.copy()

        # virt-v2v specific variables
        env['LIBGUESTFS_BACKEND'] = 'direct'
        if 'virtio_iso_path' in self._vminfo:
            env['VIRTIO_WIN'] = self._vminfo['virtio_iso_path']
        return env
# ---
def available_gpus(self) -> int:
        """Available GPU count after subtracting committed resources."""
        return get_gpu_count(self.metadata.device) - self.committed_gpu
# ---
def _find_repo_root(start_file: Path) -> Path | None:
    """Return the repository root that also contains the experiments directory if possible."""
    for parent in start_file.parents:
        if (parent / "pyproject.toml").exists() and (parent / "experiments").exists():
            return parent
    return None
# ---
def get_vm_logs(self, request: cluster__pb2.Controller.GetVmLogsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetVmLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def _description_string(self, env):
        if self.string and env.lang:
            field = self.base_field
            name = "%s,%s" % (field.model_name, field.name)
            trans = env['ir.translation']._get_source(name, 'field', env.lang)
            return trans or self.string
        return self.string
# ---
def load_prime_intellect_env(self, env_id: str, env_args: dict) -> Any:
        """
        Get the Verifiers environment for the environment ID.
        """
        self._ensure_verifiers_installed()
        import verifiers as vf

        logger.debug(f"Loading Verifiers environment for {env_id} with arguments: {env_args}")

        if env_id not in self.ENVS:
            self.ENVS[env_id] = vf.load_environment(env_id=env_id, **env_args)

        return self.ENVS[env_id]
# ---
def test_get_job_status_returns_error_for_missing_job(client):
    """GetJobStatus RPC returns error for non-existent job."""
    resp = client.post(
        "/iris.cluster.ControllerService/GetJobStatus",
        json={"jobId": JobName.root("nonexistent").to_wire()},
        headers={"Content-Type": "application/json"},
    )
    # Connect RPC returns non-200 status for errors
    assert resp.status_code != 200
# ---
def set_channel(self, value):
        '''Set the channel of all events in this Track.
        '''
        if value not in range(1, 17):
            raise MidiException('bad channel value: %s' % value)
        for event in self.events:
            event.channel = value
# ---
def scan_aware_map(fn: Callable[..., T], tree: Any, *rest: Any, is_leaf: Callable[[Any], bool] | None = None) -> Any:
    """Alias for :func:`haliax.tree_util.scan_aware_tree_map` with :mod:`jax.tree` style naming."""

    return tree_util.scan_aware_tree_map(fn, tree, *rest, is_leaf=is_leaf)
# ---
def test_filter_selects_certain_items_from_a_list(self):
        def is_even(item):
            return (item % 2) == 0

        seq = [1, 2, 3, 4, 5, 6]
        even_numbers = list()

        for item in filter(is_even, seq):
            even_numbers.append(item)

        self.assertEqual([2,4,6], even_numbers)
# ---
def test_classify_indices_to_db(mock_rcw, mock_rw, mock_jw):
    with s.app.test_client() as c:
        resp = c.get('/api/v1/classify_documents/to_database?directory=test')

        assert mock_rcw.called
        assert mock_rw.called
        assert mock_jw.called
# ---
def loadfile(self, filename, mode='replace', **options):
        self.command('loadfile', filename.encode(fs_enc), mode, MPV._encode_options(options))
# ---
def template_choose_axe_adaptors(best_adapt, best_size):
   if best_adapt:
       return "axe_adaptors\t" + best_adapt, ["--adaptor", best_adapt]
   else:
       return "axe_adaptors\tNA", ["--adaptor", "None"]
# ---
def loss_chunk(q_arr):
        qn = hax.named(q_arr, q.axes)
        out, _ = chunk_gated_delta_rule(qn, k, v, g, beta, chunk_size=4, output_final_state=False)
        return jnp.sum(out.array)
# ---
def build_pod_config(self) -> ResourceConfig:
        return ResourceConfig.with_tpu(self.tpu_type, slice_count=self.slice_count)
# ---
def disabled(self):
        if not app.player.paused:
            self.plugin_on_paused()
# ---
def get_task_status(self, request: cluster__pb2.Controller.GetTaskStatusRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetTaskStatusResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def skip_if_hf_model_not_accessible(model_id: str):
    def try_load_hf(model_id):
        try:
            AutoConfig.from_pretrained(model_id)
        except Exception:
            return False
        else:
            return True

    return pytest.mark.skipif(not try_load_hf(model_id), reason="HuggingFace model not accessible")
# ---
def __getstate__(self):
        return {
            "path": self.path,
            "fs_args": self.fs_args,
        }
# ---
def __init__(self, root: Path):
        self._root = root
        self._root.mkdir(parents=True, exist_ok=True)
        self._artifacts: list[LogArtifact] = []
# ---
def _sympy_parse(expr: str):
    """Parses an expression with sympy."""
    py_expr = expr.replace("^", "**")
    return sympy_parser.parse_expr(
        py_expr,
        transformations=(
            *sympy_parser.standard_transformations,
            sympy_parser.implicit_multiplication_application,
        ),
    )
# ---
def _get_nemotron_split_paths(split: str):
    """Helper to get file paths for a nemotron split."""
    patterns = NEMOTRON_DATASETS[split]
    return [_nemotron_cc_path / pattern for pattern in patterns]
# ---
def fake_vdi_attached_here(*args, **kwargs):
            fake_dev = 'fakedev'
            yield fake_dev
# ---
def find_free(_: jnp.ndarray):
            free_flags = ~self.used_mask
            maybe = hax.argmax(free_flags, "seq").scalar()
            available = (~self.used_mask["seq", maybe]).scalar()
            return hax.where(available, maybe, INVALID)
# ---
def team_member_id(self):
        return self.team_member.id if self._get_member() else ''
# ---
def test_failed(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_FAILED) == JobStatus.FAILED
# ---
def l1(*arg, **kw):
            canary.append("l1")
# ---
def _wait_for_server(self, timeout: float = 5.0):
        """Wait for server to be ready to accept connections."""
        start = time.time()
        while time.time() - start < timeout:
            try:
                with httpx.Client() as client:
                    client.get(f"http://{self.host}:{self.port}/docs", timeout=1.0)
                return
            except (httpx.ConnectError, httpx.TimeoutException):
                time.sleep(1.0)
# ---
def format_rare_fraction(self) -> str:
        if self.rare_fraction >= 0.01:
            return f"{self.rare_fraction:.2f}"
        else:
            return f"{self.rare_fraction:.3f}"
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> KvPageCache:
        """
        Creates an empty page cache for this layer. Note that in order to create a decoder state, you
        need to couple the KvPageCache to the PageTable's state with a BatchInfo object.
        """
        return self.self_attn.empty_page_cache(spec, dtype=dtype)
# ---
def log_xla_to_wandb(step: StepInfo):
        nonlocal last_mtime
        save_xla_dumps_to_wandb(last_mtime)
        # update time to now
        last_mtime = time.time()
# ---
def vocab_size(self) -> int:
        return self._vocab_size
# ---
def action_cancel_draft(self, cr, uid, ids, context=None):
        """ Cancels the stock move and change inventory state to draft.
        @return: True
        """
        for inv in self.browse(cr, uid, ids, context=context):
            self.pool.get('stock.move').action_cancel(cr, uid, [x.id for x in inv.move_ids], context=context)
            self.write(cr, uid, [inv.id], {'state':'draft'}, context=context)
        return True
# ---
def done(self, lease: Lease[T_co]) -> None:
        """Mark a leased task as successfully completed."""
        ...
# ---
def assert_trees_not_close(a, b):
    try:
        assert_trees_all_close(jax.tree_util.tree_leaves(arrays_only(a)), jax.tree_util.tree_leaves(arrays_only(b)))
    except AssertionError:
        pass
    else:
        raise AssertionError("Trees are equal")
# ---
def init_fn(key):
        return hax.nn.MLP.init(In, Out, 2, 1, key=key, use_bias=False, use_final_bias=False)
# ---
def is_scalarish(x):
    if isinstance(x, haliax.NamedArray):
        return x.ndim == 0
    else:
        return jnp.isscalar(x) or (getattr(x, "shape", None) == ())
# ---
def _mesh_axis_size(axis_name) -> int:
        if mesh is None:
            return 1
        return mesh.shape[axis_name]
# ---
def on_fuzzing_end_changed(self, value: int):
        self.ui.spinBoxFuzzingStart.setMaximum(self.ui.spinBoxFuzzingEnd.value())
        new_end = self.message.convert_index(value - 1, self.proto_view, 0, False)[1] + 1
        self.current_label.end = new_end
        self.current_label.fuzz_values[:] = []
        self.update_message_data_string()
        self.fuzz_table_model.update()
        self.ui.tblFuzzingValues.resize_me()
# ---
def testDeleteLocal(self):
    self.assertEqual((0, 'ok\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          bar = 123
          del bar
          try:
            print bar
            raise AssertionError
          except UnboundLocalError:
            print 'ok'
        foo()""")))
# ---
def prefix(self):
        """Matcher will match the paths in .files() recursively --
        optimization might be possible."""
        return False
# ---
def platform(self):
        """Create Platform instance from config.

        Returns:
            Platform implementation (GCP, Manual, or Local)
        """
        from iris.cluster.vm.platform import create_platform

        return create_platform(
            platform_config=self._proto.platform,
            bootstrap_config=self._proto.defaults.bootstrap,
            timeout_config=self._proto.defaults.timeouts,
            ssh_config=self._proto.defaults.ssh,
        )
# ---
def __call__(self, x, *, key):
            return x + self.array + self.static + hax.random.normal(key, x.axes), x * 2
# ---
def list_tasks(self, job_id: JobName) -> list[cluster_pb2.TaskStatus]:
        """List all tasks for a job.

        Args:
            job_id: Job ID to query tasks for

        Returns:
            List of TaskStatus protos, one per task in the job
        """
        request = cluster_pb2.Controller.ListTasksRequest(job_id=job_id.to_wire())
        response = self._client.list_tasks(request)
        return list(response.tasks)
# ---
def get_latest_stack(self, stack_size):
        return self.get_stack(len(self.examplers), stack_size)
# ---
def testClassDef(self):
    self.assertEqual((0, "<type 'type'>\n"), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          pass
        print type(Foo)""")))
# ---
def test_client(baby_llama_config, loaded_model, inference_server):
    """Create a test client for the inference server."""
    with TestClient(inference_server.app) as client:
        yield client, inference_server
# ---
def _get_dolmino_split_paths(split: str):
    """Helper to get file paths for a dolmino split."""
    patterns = DOLMINO_DATASETS[split]
    dolmino_split_input_base_path = _dolmino_base_dir / split
    return [dolmino_split_input_base_path / pattern for pattern in patterns]
# ---
def test_quantile_parallel_int(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.int32)})
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def path(self) -> str:
        """Returns the URL path to mount the application to when serving multiple applications."""
        return "/iris.actor.ActorService"
# ---
def parent_job_id(self) -> JobName | None:
        """Parent job ID, or None if this is a root job.

        For job_id "/root/parent/child", returns "/root/parent".
        For job_id "/root", returns None.
        """
        if self.job_id is None:
            return None
        return self.job_id.parent
# ---
def __init__(self, log_tree: LogTree):
        self._start_time = time.monotonic()
        self._start_datetime = datetime.now()
        self._log_tree = log_tree
        summary_path = log_tree.get_writer("summary.md", "Execution summary")
        self._file: TextIO = open(summary_path, "w")
# ---
def vocab_size(self):
        return self.tokenizer.vocab_size
# ---
def run_buildoption_test(options):
    for job in JERRY_BUILDOPTIONS:
        ret, _ = create_binary(job, options)
        if ret:
            break

    return ret
# ---
def is_finite(self) -> bool:
        return self.cache.is_finite()
# ---
def CurrentFiletypes():
  return VimExpressionToPythonType( "&filetype" ).split( '.' )
# ---
def test_simple_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).offset(2),
            [(3, 3, 4), (4, 4, 5)],
        )
# ---
def __init__(self, job_id: str, status: JobStatus):
        self.job_id = job_id
        self.failed_status = status
        super().__init__(f"Job {job_id} finished with status {status}")
# ---
def get_sso_url(self):
        return self.doc['urlSSO']
# ---
def train_set(
        self,
        options: CacheOptions = CacheOptions.default(),
        *,
        key: Optional[PRNGKeyArray] = None,
    ) -> ProcessedAudioCache:
        ds = self.build_or_load_cache(self.train_split)
        if ds is None:
            raise ValueError("No training set!")
        return ds
# ---
def preempted_always():
        return ray.get(actor.run.remote())
# ---
def _is_frac(expr: str) -> bool:
    return bool(re.search(r"^-?[0-9]+.?/0*[1-9][0-9]*.?$", expr))
# ---
def deadline_for_course(cls, course_key):
        """
        Retrieve the verification deadline for a particular course.

        Arguments:
            course_key (CourseKey): The identifier for the course.

        Returns:
            datetime or None

        """
        try:
            deadline = cls.objects.get(course_key=course_key)
            return deadline.deadline
        except cls.DoesNotExist:
            return None
# ---
def convert_th(self, el, text, convert_as_inline):
        if convert_as_inline:
            return text + " "
        colspan = 1
        if "colspan" in el.attrs:
            colspan = _try_convert_int(el["colspan"], 1)
        return " " + text.strip().replace("\n", " ") + " |" * colspan
# ---
def test_expired_deadline_remaining_is_zero():
    """Expired deadline returns zero for remaining time."""
    deadline = Deadline.from_ms(10)
    time.sleep(0.02)  # 20ms - definitely expired

    assert deadline.expired()
    assert deadline.remaining_ms() == 0
    assert deadline.remaining_seconds() == 0.0
# ---
def validate(self, obj):
                if obj.id is None:
                    obj.id = obj.label.replace(' ', '')
                return True
# ---
def alias(self, new_name: str):
        return Axis(new_name, self.size)
# ---
def log_summary(self, metrics: typing.Mapping[str, Any]):
        import trackio

        to_log = {f"summary/{k}": _convert_value_to_loggable_rec(v) for k, v in metrics.items()}
        trackio.log(to_log)
# ---
def DeleteSession(self, request, context):
        """Ends a session, releasing server resources associated with it. This will
    asynchronously trigger cancellation of any operations that are running with
    this session.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def test_no_carry_over(self, testdir):
        p = testdir.makepyfile("""
            def test_func1():
                print ("in func1")
            def test_func2():
                print ("in func2")
                assert 0
        """)
        result = testdir.runpytest(p)
        s = result.stdout.str()
        assert "in func1" not in s
        assert "in func2" in s
# ---
def findFirstColumnEmpty(self):
        """
        Implements function to get the first column where a slot remain.
        :return: the column
        """
        for col in xrange(7):
            if self.grille[0][col] == '0':
                return col
        return -1
# ---
def getcash(self):
        return self.cash
# ---
def _transfer_from_cpu(self, model) -> PyTree:
        """Transfer params from CPU back to target devices."""
        if self.params_sharding_rules is not None:
            return jax.device_put(model, self.params_sharding_rules)
        else:
            # Use default device placement
            return jax.device_put(model, jax.devices()[0])
# ---
def _resolve(self) -> ResolveResult:
        result = self._resolver.resolve(self._name)
        with self._lock:
            self._cached_result = result
        return result
# ---
def device_port(self):
        if self._values['managed']:
            return None
        return self._values['device_port']
# ---
def reset(self) -> None:
        """Reset timer to current time."""
        self._start = time.monotonic()
# ---
def convert_br(self, el, text, convert_as_inline):
        if convert_as_inline:
            return "<br>"

        if self.options["newline_style"].lower() == markdownify.BACKSLASH:
            return "\\\n"
        else:
            return "  \n"
# ---
def _cached_load_tokenizer(tokenizer_name: str):
    return load_tokenizer(tokenizer_name)
# ---
def setvalue(self, s):
		self.erase()
		self.write(s)
		return
# ---
def _add_vm_info(vm, params):
    params['vmName'] = vm.name()
    # TODO: use new API: vm.state()[0] == libvirt.VIR_DOMAIN_SHUTOFF
    #       when supported in Xen under RHEL 5.x
    if vm.isActive():
        params['status'] = "Up"
    else:
        params['status'] = "Down"
# ---
def stop(self) -> None:
        """Stop the actor server and wait for threads to exit."""
        self._threads.stop()
# ---
def __init__(self, prev):
        self.prev = prev  # ContentOfGroup or CharClass
        self.pattern = ast.PatternChar()
        self.pattern.type = ast.PatternChar.Ascii

        self.prev.add(self.pattern)
# ---
def test_fixed_window(self):
        storage = MemcachedStorage("memcached://localhost:22122")
        limiter = FixedWindowRateLimiter(storage)
        per_min = RateLimitItemPerSecond(10)
        start = time.time()
        count = 0

        while time.time() - start < 0.5 and count < 10:
            assert limiter.hit(per_min)
            count += 1
        assert not limiter.hit(per_min)

        while time.time() - start <= 1:
            time.sleep(0.1)
        assert limiter.hit(per_min)
# ---
def format_response(response: Message) -> str:
    """Format a protobuf response message as JSON."""
    return json_format.MessageToJson(
        response,
        preserving_proto_field_name=True,
        indent=2,
    )
# ---
def http_error_401(self, *args, **kwds):
                self.parent.record("basic")
                urllib.request.HTTPBasicAuthHandler.http_error_401(self,
                                                            *args, **kwds)
# ---
def __exit__(self, *args):
        """Leave context: close port"""
        self.close()
        self.closed.wait()
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            return df.A.mean()
# ---
def __init__(self, in_array: NamedArray):
                self.array = in_array
                self.array2 = hax.zeros(Dim3)
# ---
def __rtruediv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.true_divide(other, self)
# ---
def __init__(self, view_or_error=None):
        self.wrapped = callable(view_or_error)
        error_view = None

        if self.wrapped:
            self.view = view_or_error
        else:
            error_view = view_or_error

        if not error_view:
            from django.conf import settings
            error_view = settings.PERMISSIONS_VIEW

        from django.core.urlresolvers import get_callable
        self.error_view = get_callable(error_view)
# ---
def put(self, b):
		self.write(chr(b))
		return
# ---
def log_time(label: str, level: int = logging.INFO) -> Iterator[None]:
    t_start = time.perf_counter()
    yield
    t_end = time.perf_counter()
    logger.log(level, f"{label} took {timedelta(seconds=t_end - t_start)}")
# ---
def access_bot_by_id(user_profile: UserProfile, user_id: int) -> UserProfile:
    try:
        target = get_user_profile_by_id_in_realm(user_id, user_profile.realm)
    except UserProfile.DoesNotExist:
        raise JsonableError(_("No such bot"))
    if not target.is_bot:
        raise JsonableError(_("No such bot"))
    if not user_profile.can_admin_user(target):
        raise JsonableError(_("Insufficient permission"))
    return target
# ---
def _native_logs_tail(log_dir: str | None, *, max_lines: int = 200) -> str:
    if not log_dir:
        return "<no log directory available for native vLLM server>"
    stdout_path = os.path.join(log_dir, "stdout.log")
    stderr_path = os.path.join(log_dir, "stderr.log")
    return (
        "--- stdout (tail) ---\n"
        f"{_tail_file(stdout_path, max_lines)}\n"
        "--- stderr (tail) ---\n"
        f"{_tail_file(stderr_path, max_lines)}"
    )
# ---
def tipodecambio(self):
        return self.__tcambio
# ---
def on_random_range_max_changed(self):
        self.ui.spinBoxRandomMinimum.setMaximum(self.ui.spinBoxRandomMaximum.value() - 1)
# ---
def insert(radio, genre, url) :
    db = cherrypy.session['database']
    sql =  "INSERT INTO Radio (radio, genre, url, exist) VALUES('%s', '%s', '%s', 1)" % (radio, genre, url)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        """Return the scaling factor for initializing weights
        given input and output axes."""
        raise NotImplementedError
# ---
def test_conf_set_no_read(self):
        with mock.patch.object(memcache, 'ConfigParser', ExcConfigParser):
            exc = None
            try:
                memcache.MemcacheMiddleware(
                    FakeApp(), {'memcache_servers': '1.2.3.4:5',
                                'memcache_serialization_support': '2',
                                'memcache_max_connections': '30'})
            except Exception as err:
                exc = err
        self.assertIsNone(exc)
# ---
def save_dt_model(model_name, model, folder):
    """
    Save model using Pickle binary format.

    :param dataframe model: model reference
    :param string model_name: title for the model used on the output filename
    :param string folder: location of model output
    """
    print("Saving model...")
    model_file = folder + '/models/' + model_name + '.pkl'
    path = open(model_file, 'wb')
    pickle.dump(model, path)
    print("Model saved location:", model_file)
# ---
def test_mistral_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def flops_per_token(self, vocab_size: int, context_length: int):
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=True,
        )
# ---
def remove(self, tag: str) -> None: ...
# ---
def write_watch_later_config(self):
        self.command('write_watch_later_config')
# ---
def test_create_controller_raises_on_missing_config(gcp_config: config_pb2.IrisClusterConfig):
    """create_controller raises ValueError when no oneof is set."""
    config = config_pb2.IrisClusterConfig()
    config.CopyFrom(gcp_config)
    config.controller.ClearField("gcp")

    with pytest.raises(ValueError, match="No controller config specified"):
        create_controller_vm(config)
# ---
def linear_1d(module, grad_input, grad_output):
    """No change made to gradients."""
    return None
# ---
def unassign_task(self, task_id: JobName, resources: cluster_pb2.ResourceSpecProto) -> None:
        """Unassign a task from this worker, updating committed resources."""
        self.running_tasks.discard(task_id)
        self.committed_cpu -= resources.cpu
        self.committed_mem -= resources.memory_bytes
        self.committed_gpu -= get_gpu_count(resources.device)
        self.committed_tpu -= get_tpu_chip_count(resources.device)
# ---
def _apertus_rope_scaling(max_position_embeddings: int) -> dict:
    original_max_position_embeddings = max_position_embeddings // 8
    return {
        "rope_type": "llama3",
        "factor": 8.0,
        "original_max_position_embeddings": original_max_position_embeddings,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
    }
# ---
def __call__(self, carry):
            return carry + self.w, 2 * self.w
# ---
def local_keys(self):
        '''
        Return a dict of local keys
        '''
        ret = {'local': []}
        for fn_ in salt.utils.isorted(os.listdir(self.opts['pki_dir'])):
            if fn_.endswith('.pub') or fn_.endswith('.pem'):
                path = os.path.join(self.opts['pki_dir'], fn_)
                if os.path.isfile(path):
                    ret['local'].append(fn_)
        return ret
# ---
def free_pages_for_finished(self, finished_mask: jnp.ndarray) -> "DecodeState":
        sequences, page_table = self.sequences.free_pages_for_finished(self.page_table, finished_mask)
        return dataclasses.replace(self, sequences=sequences, page_table=page_table)
# ---
def round(a: A, decimals: int = 0) -> A:
    return wrap_elemwise_unary(jnp.round, a, decimals=decimals)
# ---
def setUp(self):
        self.xe = IOSXE(node=node, username=username, password=password, disable_warnings=True)
# ---
def reload_model(self, model: LmHeadModel | None, state_dict: dict) -> LmHeadModel | None:
        raise NotImplementedError
# ---
def launch_job(
        self,
        request: cluster_pb2.Controller.LaunchJobRequest,
    ) -> cluster_pb2.Controller.LaunchJobResponse:
        """Submit a job to the controller."""
        return self._service.launch_job(request, None)
# ---
def _resolve_axis_in_tree(tree, axis):
    """
    Resolves an axis in a tree of NamedArrays. This is useful for finding the batch axis in a batch of data.
    """
    for leaf in haliax.tree_util.tree_leaves(tree):
        if isinstance(leaf, haliax.NamedArray):
            try:
                return leaf.resolve_axis(axis)
            except ValueError:
                pass

    raise ValueError(f"Could not find axis {axis} in tree {tree}")
# ---
def get_stats(self, container_id: str) -> ContainerStats: ...
# ---
def __or__(self, other: object) -> LogicalExpr:
        return LogicalExpr(self, _to_expr(other), "or")
# ---
def test_mem_read_byte(self):
        self.mda.video_ram[77] = 0xA5
        self.assertEqual(self.mda.mem_read_byte(77), 0xA5)
# ---
def load_template(self, file_name):
        filepath = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                'templates', file_name)
        f = open(filepath)
        t = template_format.parse(f.read())
        f.close()
        return t
# ---
def create_item(queries, label, thumb='', fanart='', is_folder=None, is_playable=None, total_items=0, menu_items=None, replace_menu=False):
    list_item = xbmcgui.ListItem(label, iconImage=thumb, thumbnailImage=thumb)
    add_item(queries, list_item, fanart, is_folder, is_playable, total_items, menu_items, replace_menu)
# ---
def task_schedule_status(self, task: ControllerTask, context: SchedulingContext) -> TaskScheduleResult:
        """Get the current scheduling status of a task (for dashboard display).

        Delegates to the internal scheduler.
        """
        return self._scheduler.task_schedule_status(task, context)
# ---
def test_namedarray_runtime_check_with_category():
    B = Axis("batch", 1)
    arr = NamedArray(jnp.zeros((B.size,), dtype=jnp.float32), (B,))
    assert arr.matches_axes(Float[NamedArray, "batch"])  # type: ignore
    assert not arr.matches_axes(Int[NamedArray, "batch"])
# ---
def __init__(self, interval_seconds: float):
        self._interval = interval_seconds
        self._last_run: float | None = None
# ---
def convert_to_display_name(self, value, record=None):
        return ustr(value.display_name)
# ---
def tree_leaves(tree, is_leaf=None):
    """
    Version of [jax.tree_util.tree_leaves][] that automatically treats NamedArrays as leaves.
    """
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: is_leaf(x) or is_named_array(x)

    return jax.tree_util.tree_leaves(tree, is_leaf=is_leaf)
# ---
def num_channels(self):
    """Number of color channels."""
    return 3
# ---
def _stop_server() -> None:
            logger.debug("Signaling server %s to exit", name)
            server.should_exit = True
# ---
def getInstance():
        if not ObjectBackendRegistry.instance:
            ObjectBackendRegistry.instance = ObjectBackendRegistry()

        return ObjectBackendRegistry.instance
# ---
def the_object_name_is_in_the_collection_collection(name, collection):
    assert collection in [c.name for c in the_object_name_exists(name).users_collection]
# ---
def rearrange(array: NamedArray, expression: str, **bindings: AxisSelector | int) -> NamedArray:
    pass
# ---
def _add_property(self, name, value=None):
        self.command('add_property', name, value)
# ---
def set_current_task(self, task_name: str):
        self._current_task = task_name
        if self.sample_logging_config.should_log() and task_name not in self.sample_outputs:
            self.sample_outputs[task_name] = []
# ---
def get_batch(self, indices: Sequence[int] | np.ndarray) -> Sequence[T_co]:
        return self._run_coroutine(self.dataset.get_batch(indices))
# ---
def get_score_rank(self):
        return self.weight_rank
# ---
def Confirm( message ):
  """Display |message| with Ok/Cancel operations. Returns True if the user
  selects Ok"""
  return bool( PresentDialog( message, [ "Ok", "Cancel" ] ) == 0 )
# ---
def commit_callback():
        elapsed = time.time() - start_time
        logger.info(f"Checkpoint committed to Tensorstore successfully! Total time: {elapsed:.2f}s")
# ---
def save_settings(self):
        settings = QSettings()
        settings.setValue(WINDOWSTATE_SETTING, self.saveState())
        settings.setValue(GEOMETRY_SETTING, self.saveGeometry())
        settings.setValue(FILENAME_SETTING, self.filename)
# ---
def action_confirm(self, cr, uid, ids, context=None):
        """ Confirms stock move.
        @return: List of ids.
        """
        moves = self.browse(cr, uid, ids, context=context)
        self.write(cr, uid, ids, {'state': 'confirmed'})
        self.create_chained_picking(cr, uid, moves, context)
        return []
# ---
def test_timestamp_uses_wall_clock():
    """Timestamp uses wall-clock time and advances correctly."""
    ts1 = Timestamp.now()
    time.sleep(0.02)  # 20ms
    ts2 = Timestamp.now()

    assert ts2.after(ts1)
    assert ts1.before(ts2)
    assert ts2 > ts1

    # Age should be at least 20ms
    age_ms = ts1.age_ms()
    assert age_ms >= 20
# ---
def app_settings():

    app_settings = {'GRAPHITE_HOST': settings.GRAPHITE_HOST,
                    'OCULUS_HOST': settings.OCULUS_HOST,
                    'FULL_NAMESPACE': settings.FULL_NAMESPACE,
                    }

    resp = json.dumps(app_settings)
    return resp, 200
# ---
def test_attributos_quarto_voo(self):
        p_voo = self.escala.escalas[25]
        self.assertFalse(p_voo.checkin)
        self.assertEqual(p_voo.checkin_time, None)
        self.assertEqual(p_voo.flight_no, '2872')
        self.assertEqual(p_voo.activity_info, 'AD2872')
# ---
def reset(self) -> Self:
        """Return a reset version of this cache."""
        raise NotImplementedError
# ---
def is_heartbeat_expired(self, timeout: Duration) -> bool:
        """Check if this worker's heartbeat has expired.

        Args:
            timeout: Heartbeat timeout duration

        Returns:
            True if the worker has not sent a heartbeat within the timeout period
        """
        return self.last_heartbeat.age_ms() > timeout.to_ms()
# ---


def flip_case(string: str) -> str:
    """ For a given string, flip lowercase characters to uppercase and uppercase to lowercase.
    >>> flip_case('Hello')
    'hELLO'
    """
    return string.swapcase()
# ---
def loadProfile(self, profile):
        """Load profile"""

        self.saveBasicSettings()

        self.prefsDict['activeProfile'] = profile
        _settingsManager.setProfile(profile[1])
        self.prefsDict = _settingsManager.getGeneralSettings(profile[1])

        orca.loadUserSettings(skipReloadMessage=True)

        self._initGUIState()

        braille.checkBrailleSetting()

        self._initSpeechState()

        self._populateKeyBindings()

        self.__initProfileCombo()
# ---
def add_transactions_to_block(self, block, tx_list):
        [tx.rehash() for tx in tx_list]
        block.vtx.extend(tx_list)
# ---
def run(self):
        while not self._quit:
            try:
                if self.serial.isOpen() and self.serial.inWaiting():
                    self.data_received.emit(
                        datetime.datetime.now(),
                        strip(bytearray_to_utf8(self.serial.readline()))
                    )
            except SerialException:
                pass
# ---
def run(
        self, fn: Callable, *args, name: str | None = None
    ) -> _ImmediateFuture | Generator[_ImmediateFuture, None, None]:
        """Execute function immediately and wrap result."""
        result = fn(*args)
        return _ImmediateFuture(result)
# ---
def create_mesh(devices=None):
    """Create a simple JAX mesh for testing."""
    if devices is None:
        devices = jax.local_devices()[:1]  # Use just one device for tests
    return Mesh(np.array(devices), axis_names=("batch",))
# ---
def test_metadata_works():
    processor = AutoProcessor.from_pretrained("openai/whisper-tiny")
    tokenizer = AutoTokenizer.from_pretrained("openai/whisper-tiny")
    batch_processor = BatchAudioProcessor(processor, tokenizer)
    # test this doesn't throw
    assert len(batch_processor.metadata)
# ---
def list_images(self, params=None):
        """Returns a list of all images filtered by any parameters."""
        url = 'images'
        if params:
            url += '?%s' % urllib.urlencode(params)

        resp, body = self.get(url)
        body = json.loads(body)
        self.validate_response(schema.list_images, resp, body)
        return service_client.ResponseBodyList(resp, body['images'])
# ---
def active_count(self) -> int:
        """Number of active nodes."""
        return len(self.active_nodes)
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype):
        # Use trivial cache dimensions; the cache is unused by this dummy model
        kv_heads = Axis("kv_head", 1)
        head_size = Axis("embed", 1)
        return KvPageCache.init(spec, kv_heads, head_size, dtype=dtype)
# ---
def build(self, num_train_steps: int):
        raise NotImplementedError
# ---
def wandb_xla_logger(config: WandbConfig):
    import wandb

    last_mtime = wandb.run and wandb.run.start_time or time.time()

    def log_xla_to_wandb(step: StepInfo):
        nonlocal last_mtime
        save_xla_dumps_to_wandb(last_mtime)
        # update time to now
        last_mtime = time.time()

    if config.save_xla_dumps:
        return log_xla_to_wandb
    else:
        return lambda x: None
# ---
def threadStopped(self, thread_num):
        self.connection_count -= 1
# ---
def get_metrics(self) -> dict:
        return dataclasses.asdict(self.metrics)
# ---
def coll(sx, sy, dx, dy):
    m = 0
    for p in range(32):
        m2 = m + 2**(-p)
        if inside(sx + dx * m2, sy + dy * m2): m = m2
    return (sx + dx*m, sy + dy*m)
# ---
def model_type(self) -> Type["LlamaLMHeadModel"]:
        return LlamaLMHeadModel
# ---
def test_comparison(self):
        expr = col("score") > 0.5
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def __repr__(self) -> str:
        return f"RemoteRev({self.model_name_or_path!r}, {self.revision!r})"
# ---
def _num_accelerator_chips(resources: ResourceConfig) -> int:
    """Return the total number of accelerator chips for the provided resources."""
    return resources.chip_count()
# ---
def iter(self):
        yield self
        parent = object.__getattribute__(self, '_parent')
        if parent is not None:
            for ancestor in parent.iter():
                yield ancestor
# ---
def test_corrupt_program_graceful_when_no_mutations_possible():
    """If the bank has no matching types, corruption returns the original."""
    bank = SubtreeBank()  # empty bank
    source = "x = 1\n"
    corrupted, mutations = corrupt_program(source, num_steps=5, bank=bank)
    assert corrupted == source
    assert mutations == []
# ---
def common_pile_tokenized(*, tokenizer: str = llama3_tokenizer) -> dict[str, TokenizerStep]:
    """Return tokenization steps for the Common Pile filtered datasets."""
    tokenized: dict[str, TokenizerStep] = {}
    for dataset, step in COMMON_PILE_DATASETS.items():
        tokenized[f"common_pile/{dataset}"] = default_tokenize(
            name=f"common_pile/{dataset}",
            dataset=step,
            tokenizer=tokenizer,
        )
    return tokenized
# ---
def main(inference_config: InferenceConfig):
    ray.get(run_inference.remote(inference_config))
# ---
def __init__(self, queue: InMemoryRolloutQueue):
        self._queue = queue
# ---
def __post_init__(self):
        if len(self.axes) == 0:
            raise ValueError("Empty axes not allowed")
# ---
def put(self):
            pass
# ---
def setup_class(cls):
        from sqlalchemy.engine import base, default
        cls.engine = engine = testing_engine('sqlite://')
        m = MetaData()
        cls.table = t = Table('test', m,
            Column('x', Integer, primary_key=True),
            Column('y', String(50, convert_unicode='force'))
        )
        m.create_all(engine)
        engine.execute(t.insert(), [
            {'x':i, 'y':"t_%d" % i} for i in range(1, 12)
        ])
# ---
def setplayer(p):
    global player
    player = p
# ---
def test_replica_with_invalid_slave_of_id(self, mock_logging):
        self.assertRaises(exception.NotFound,
                          Instance.create,
                          None, 'name', 1, "UUID", [], [], None,
                          self.datastore_version, 1,
                          None, slave_of_id=str(uuid.uuid4()))
# ---
def testRaiseTraceback(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        import sys
        try:
          try:
            raise Exception
          except:
            e, _, tb = sys.exc_info()
            raise e, None, tb
        except:
          e2, _, tb2 = sys.exc_info()
        assert e is e2
        assert tb is tb2""")))
# ---
def shutdown(self):
        """Shutdown engine and event loop."""
        if self.engine:
            self.engine.shutdown()
        self.bridge.stop()
# ---
def __init__(self, **kwargs):
        self.options = kwargs
# ---
def test_dontreadfrominput_has_encoding(testdir):
    testdir.makepyfile("""
        import sys
        def test_capattr():
            # should not raise AttributeError
            assert sys.stdout.encoding
            assert sys.stderr.encoding
    """)
    reprec = testdir.inline_run()
    reprec.assertoutcome(passed=1)
# ---
def __init__(self, xPos, yPos):
        self.x = xPos
        self.y = yPos
        self.th = 32
        self.tw = 32
# ---
def test_sharded_tree_size_named_array_with_abstract_mesh():
    mesh = jax.sharding.AbstractMesh((2, 2), ("data", "model"))

    Batch = hax.Axis("batch", 8)
    Hidden = hax.Axis("hidden", 16)
    named_array = hax.ones((Batch, Hidden), dtype=jnp.float32)

    mapping = {"batch": ResourceAxis.DATA, "hidden": ResourceAxis.MODEL}

    per_device_bytes = sharded_tree_size(named_array, mesh=mesh, mapping=mapping)

    assert per_device_bytes == named_array.array.nbytes // 4
# ---
def outfit():
    collection = []

    for _ in range(0, 5):
        collection.append("Item{}".format(_))

    return {
        "data": collection,
    }
# ---
def mock_bundle_cache():
    """Create mock BundleCache."""
    cache = Mock(spec=BundleCache)
    cache.get_bundle = Mock(return_value=Path("/tmp/bundle"))
    return cache
# ---
def test_re_raise_passthrough(self):
        """DynamoDBError can re-raise itself if missing original exception"""
        err = DynamoDBError(400, Code="ErrCode", Message="Ouch", args={})
        caught = False
        try:
            err.re_raise()
        except DynamoDBError as e:
            caught = True
            self.assertEqual(err, e)
        self.assertTrue(caught)
# ---
def on_upper_bound_checked_changed(self):
        if self.ui.checkBoxUpperBound.isChecked():
            self.ui.spinBoxUpperBound.setEnabled(True)
            self.ui.spinBoxBoundaryNumber.setEnabled(True)
        elif not self.ui.checkBoxLowerBound.isChecked():
            self.ui.spinBoxUpperBound.setEnabled(False)
            self.ui.spinBoxBoundaryNumber.setEnabled(False)
        else:
            self.ui.spinBoxUpperBound.setEnabled(False)
# ---
def seek(self, amount, reference="relative", precision="default-precise"):
        self.command('seek', amount, reference, precision)
# ---
def __call__(self, text, add_special_tokens=False, **kwargs):
        """Make tokenizer callable like HuggingFace tokenizers."""
        if isinstance(text, list):
            input_ids = [self.encode(t, add_special_tokens) for t in text]
        else:
            input_ids = self.encode(text, add_special_tokens)
        return {"input_ids": input_ids}
# ---
def _apply_update(tree, update, overwrite):
        if overwrite is not None:
            return overwrite

        return eqx.apply_updates(tree, update)
# ---
def visit_mo(self, element):
        text = element.get_text().strip()
        if text in _MATH_OPERATORS:
            return MacroNode(_MATH_OPERATORS[text])
        return OperatorNode(text)
# ---

def add_elements(arr, k):
    """
    Given a non-empty array of integers arr and an integer k, return
    the sum of the elements with at most two digits from the first k elements of arr.

    Example:

        Input: arr = [111,21,3,4000,5,6,7,8,9], k = 4
        Output: 24 # sum of 21 + 3

    Constraints:
        1. 1 <= len(arr) <= 100
        2. 1 <= k <= len(arr)
    """
    return sum(elem for elem in arr[:k] if len(str(elem)) <= 2)
# ---
def test_is_cloneable_share_goodformat1(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://10.61.222.333/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def make_comment(with_author=True):
    author = make_author() if with_author else None
    return Comment(id=fake.random_int(), body=fake.bs(), author=author)
# ---
def _write_tiny_corpus(path):
    os.makedirs(f"{path}/train", exist_ok=True)
    with open(f"{path}/train/docs.jsonl", "w") as f:
        for i in range(10):
            f.write(json.dumps({"text": f"hello world {i} " * 100}))
            f.write("\n")

    os.makedirs(f"{path}/validation", exist_ok=True)
    with open(f"{path}/validation/docs.jsonl", "w") as f:
        for i in range(10):
            f.write(json.dumps({"text": f"bye world {i} " * 100}))
            f.write("\n")
# ---
def sample_html_simple():
    """Fixture providing simple HTML without LaTeX."""
    return """<!DOCTYPE html>
<html>
<head><title>Simple Page</title></head>
<body>
<h1>Simple Title</h1>
<p>This is a simple paragraph.</p>
<ul>
<li>Item 1</li>
<li>Item 2</li>
</ul>
</body>
</html>"""
# ---
def get(self):
        self.finish()
# ---
def get(self, ref: Any) -> Any:
        """Retrieve an object from its reference.

        Args:
            ref: Reference to retrieve

        Returns:
            The stored object
        """
        ...
# ---
def add_password(self, realm, uri, user, password):
        self.realm = realm
        self.url = uri
        self.user = user
        self.password = password
# ---
def __call__(self, *args, **kwargs) -> M_co: ...
# ---
def resolve_trainer_config(self, override: TrainerConfig | None = None) -> TrainerConfig:
        if override is not None:
            return override
        trainer_run = self.trainer_run_name or self.run_name
        return _default_trainer_for_run(trainer_run)
# ---
def from_wire(cls, s: str) -> "JobName":
        """Parse from wire format. Alias for from_string."""
        return cls.from_string(s)
# ---
def is_numeric(str):
    try:
        _offset = int(eval(str))
    except:
        return False
    return True
# ---
def _invoice_line_hook(self, cr, uid, move_line, invoice_line_id):
        '''Call after the creation of the invoice line'''
        return
# ---
def decorator(func):
        _checks[name] = func
        return func
# ---
def get_measurement(self, block_timestamp, parent_headerhash, parent_metadata: BlockMetadata):
        with self.lock:
            return self._state.get_measurement(block_timestamp, parent_headerhash, parent_metadata)
# ---
def pool_id(self) -> str:
        return self._pool_id
# ---
def count_folder(count: int, item: T) -> tuple[bool, int]:
            return (count < size, count + 1)
# ---
def to_state_dict(self, prefix: str | None = None) -> StateDict:
        w = [self.w1.weight, self.w2.weight, self.w3.weight]
        out = {}

        num_experts = self.w1.Experts.size
        for i in range(num_experts):
            for j in range(3):
                key = f"{prefix}.{i}.w{j + 1}.weight"
                val = w[j]["experts", i].array
                out[key] = jnp.swapaxes(val, -1, -2)

        return out
# ---
def is_stale(self) -> bool:
        logger.debug(f"Is stale? {time.time()} {self.timestamp} {time.time() - self.timestamp}")
        return (time.time() - self.timestamp) > HEARTBEAT_TIMEOUT
# ---
def empty():
        return CacheMetadata()
# ---
def build_tensorboard_command(executable: str, logdir: Path, port: Optional[int]) -> list[str]:
    command = [executable, f"--logdir={logdir}"]
    if port is not None:
        command.append(f"--port={port}")
    return command
# ---
def test_add_data(self):
        self.assertFalse(self.get.has_data())
        self.assertEqual("GET", self.get.get_method())
        self.get.add_data("spam")
        self.assertTrue(self.get.has_data())
        self.assertEqual("POST", self.get.get_method())
# ---
def default_choice_name(cls) -> Optional[str]:
        return "rms"
# ---
def process(self, message):
        """
        Process a message as arriving based on a subscription.
        """
        pass
# ---


def modp(n: int, p: int):
    """Return 2^n modulo p (be aware of numerics).
    >>> modp(3, 5)
    3
    >>> modp(1101, 101)
    2
    >>> modp(0, 101)
    1
    >>> modp(3, 11)
    8
    >>> modp(100, 101)
    1
    """
    ret = 1
    for i in range(n):
        ret = (2 * ret) % p
    return ret
# ---
def deduct(self, job: ControllerJob) -> None:
        """Deduct job's resources from available capacity."""
        res = job.request.resources
        self.available_cpu -= res.cpu
        self.available_memory -= res.memory_bytes
        self.available_gpus -= get_gpu_count(res.device)
        self.available_tpus -= get_tpu_chip_count(res.device)
# ---
def __init__(self, topic):
        target = oslo_messaging.Target(topic=topic, version='1.0')
        self.client = n_rpc.get_client(target)
# ---
def step_impl(context, username, field, value):
    user = context.user_service.exists(username)

    if user is not None:
        assert user[field] == value
    else:
        raise UserNotFound(username, "User was not found")
# ---
def get_max_balance():
        return AdvancedBankAccount.MAX_BALANCE
# ---
def dot(
    *arrays: NamedArray,
    axis: AxisSelection | None,
    precision: PrecisionLike = None,
    preferred_element_type: DTypeLike | None = None,
    out_axes: PartialAxisSpec | None = ...,
    dot_general=jax.lax.dot_general,
) -> NamedArray: ...
# ---
def _buffer_response(status_headers, iterator):
        out = SpooledTemporaryFile(ProxyRouter.BUFF_RESPONSE_MEM_SIZE)
        size = 0

        for buff in iterator:
            size += len(buff)
            out.write(buff)

        content_length_str = str(size)
        # remove existing content length
        status_headers.replace_header('Content-Length',
                                      content_length_str)

        out.seek(0)
        return RewriteContent.stream_to_gen(out)
# ---
def not_found(error):
    return make_response(jsonify( { 'error': 'Page Not Found' } ), 404)
# ---
def sum(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.sum(
            self,
            axis=axis,
            dtype=dtype,
            where=where,
        )
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        """Total peak FLOP/s across all TPU chips."""
        return self.device_flops(dtype) * self.chip_count()
# ---
def test_best_of_n_sorted_by_score(params, model_cfg, tokenizer):
    """Results should be sorted: edited candidates first, then by score."""
    results = best_of_n(
        params=params,
        source="x = 1 + 2\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(5),
        n=4,
        max_depth=2,
    )
    keys = [(c.depth > 0, c.score) for c in results]
    assert keys == sorted(keys, reverse=True)
# ---
def setUp(self):
        super(NovaProxyRequestHandlerBaseTestCase, self).setUp()

        self.wh = websocketproxy.NovaProxyRequestHandlerBase()
        self.wh.socket = mock.MagicMock()
        self.wh.msg = mock.MagicMock()
        self.wh.do_proxy = mock.MagicMock()
        self.wh.headers = mock.MagicMock()
# ---
def add_child(graph, child_id, child_label, parent_id, colour):
    """
    http://www.graphviz.org/doc/info/shapes.html#polygon
    """
    node = pydot.Node(child_id, style="filled", fillcolor=colour, label=child_label, shape="polygon", fontname=FONT)
    graph.add_node(node)
    graph.add_edge(pydot.Edge(parent_id, node))
# ---
def load_lm_dataset_cache(
    cache_dir: str,
    format: LmDatasetFormatBase,
    tokenizer: HfTokenizer,
    enforce_eos: bool = True,
) -> TreeCache[dict]:
    """Load an existing cache, raising if not present."""
    processor = preprocessor_for_format(format, tokenizer, enforce_bos=True, enforce_eos=enforce_eos)
    cache = TreeCache.load(
        cache_dir,
        exemplar=processor.output_exemplar,
        options=CacheMetadata(preprocessor_metadata=processor.metadata),
    )
    return cache
# ---
def activations(
    params: GrugModelParameters,
    token_ids: Int[Array, "B S"],
    cfg: GrugModelConfig,
    *,
    mask: AttentionMask | jax.Array | None = None,
) -> Float[Array, "B S D"]:
    """Return final hidden states with shape (batch, seq, hidden_dim)."""
    return _transformer_hidden(params, token_ids, cfg, mask=mask)
# ---
def setReplicaHost( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndict = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.setReplicaHost( lfndict )
# ---
def _alias_names(artifact: "wandb.sdk.Artifact") -> set[str]:  # type: ignore[name-defined]
    names: set[str] = set()
    for alias in artifact.aliases or []:
        name = getattr(alias, "name", alias)
        if name is not None:
            names.add(str(name))
    return names
# ---
def build_model_config(*, model: str, seq_len: int) -> MixtralConfig:
    if model == MODEL_OLMOE_1B7B:
        return _build_olmoe_1b7b_config(seq_len)
    if model == MODEL_MIXTRAL_8X7B:
        return _build_mixtral_8x7b_config(seq_len)
    raise ValueError(f"Unknown model preset {model!r}. Options: {MODEL_OPTIONS}.")
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        return DefaultRotaryEmbeddings(HeadSize, self)
# ---
def test_vmap_unmapped_args():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Width, Depth))

    def vmap_fun(x):
        return x.take(Width, 2)

    selected = hax.vmap(vmap_fun, Batch)(named1)

    expected_jax = jnp.array([named1.take(Width, 2).array for _ in range(Batch.size)])
    expected_names = (Batch, Depth)

    assert jnp.all(jnp.equal(selected.array, expected_jax))
    assert selected.axes == expected_names
# ---
def bmarks():
    return_data = do_login()
    return return_data
# ---
def testDoubleRandomTranposeBoth(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(k, n, np.float64)
      y = self._randMatrix(m, k, np.float64)
      self._testCpuMatmul(x, y, True, True)
# ---
def __call__(
        self, module: M_contra, carry: CarryT, *args: P.args, **kwargs: P.kwargs
    ) -> tuple[CarryT, OutputT_co]: ...
# ---
def named_shape(self) -> Mapping[str, int]:
        return self.shape
# ---
def fake_generate_swap(cls, *args, **kwargs):
            self.called = True
# ---
def fake_stream_disk(*args, **kwargs):
            pass
# ---
def func(self, *args, **kwargs):
            ds = self.coords.to_dataset()
            for k in self.data_vars:
                ds._variables[k] = f(self._variables[k], *args, **kwargs)
            return ds
# ---
def vmap_fun(x):
        return x.take(Width, 2)
# ---
def rounded_batch_size_at_step(self, step: int) -> int:
        return self._round_batch_size(self.scheduler.batch_size_at_step(step))
# ---
def test_hf_gradient_fa():
    hf_config = HfGpt2Config.from_pretrained("gpt2")
    config = Gpt2Config.from_hf_config(hf_config)
    # keep block size small to make sure we test the tiling behavior
    config = dataclasses.replace(config, use_flash_attention=True, flash_attention_block_size=128)
    _compare_gpt2_checkpoint_gradients("gpt2", None, config=config)
# ---
def test_cats_task_reward():
    task = MoarCatsTask()

    assert task.compute_reward("cats", "cats cats cats") > task.compute_reward("cats", "cats")
    assert task.compute_reward("cats", "i love cats") > task.compute_reward("cats", "i like cats")

    assert task.compute_reward("cats", "cat") > 0
    assert task.compute_reward("cats", "dog") == 0
# ---
def main():
    """Create calibration coefficient files for AVHRR."""
    out_dir = sys.argv[1]
    coeffs = get_all_coeffs()
    save_coeffs(coeffs, out_dir=out_dir)
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        if isinstance(self.device, CpuConfig):
            return 100e9
        return self.device_flops(dtype) * self.chip_count()
# ---
def pronunciationFocusChange(self, widget, event, isFocused):
        """Callback for the pronunciation tree's focus-{in,out}-event signal."""

        _settingsManager.setSetting('usePronunciationDictionary', not isFocused)
# ---
def Out(self) -> AxisSpec:
        return self.layers[-1].Out
# ---
def __getitem__(self, key):

        # self.lib.getHMatrix.restype=ctypes.POINTER(_C_HMatrix)
        # self.lib.getHMatrix.argtypes=[ctypes.POINTER(_C_MultiHMatrix), ctypes.c_int]
        # c_data_hmatrix = self.lib.getHMatrix(self.c_data,key)
        # return HMatrix(c_data_hmatrix,**self.params)
        return self.hmatrices[key]
# ---
def fai_learned(domain, b):
    if domain.unflooded_domain == None:
        print('No unflooded training domain provided.')
        return None
    unflooded_b = modis_utilities.compute_modis_indices(domain.unflooded_domain)
    water_mask  = modis_utilities.get_permanent_water_mask()

    threshold = modis_utilities.compute_binary_threshold(get_fai(unflooded_b), water_mask, domain.bounds)
    return fai(domain, b, threshold)
# ---
def has_hf_files(path: str) -> bool:
        return fsspec_exists(os.path.join(path, "config.json"))
# ---
def create(self,problem,**kwargs):
    path = self.pathbase
    payload=json.dumps(problem)
    params = self.params.copy()
    params.update(kwargs)
    req = self.session.post(path,
                        params=params,
                        headers=self.headers,
                        data=payload)
    self.validateReply(req)
    return req.text
# ---
def _parse_avro_schema(self):
        raise NotImplementedError("Not implemented.")
# ---
def __init__(self):
        self.id = 0
        self.name = ""
        self.prom = 0.0
        self.idle = ""
        self.drinks = 0
# ---
def compute_reward(self, correct_answer: str, actual_response: str, tokenizer=None) -> float:
        return compute_soft_reward(correct_answer, actual_response)
# ---
def build(self, axis: AxisSpec) -> "GemmaRMSNorm":
        return GemmaRMSNorm.init(axis, eps=self.eps, use_weight=self.use_weight, use_bias=self.use_bias)
# ---
def __init__(self, perm, error_view=None):
        self.perm = perm
        self.error_view = error_view
# ---
def normalize_phase(self,z_data,cal_phase):
		return z_data*np.exp(-1j*cal_phase)
# ---
def init(cls, axis: AxisSpec, eps: float = 1e-6, use_weight: bool = True, use_bias: bool = False, dtype=None):
        assert use_weight, "GemmaRMSNorm does not support use_weight=False"
        assert not use_bias, "GemmaRMSNorm does not support use_bias=True"

        weight = hax.zeros(axis)
        bias = None

        return GemmaRMSNorm(axis, weight, bias, eps, dtype=jnp.float32)
# ---
def device_password(self):
        if self._values['managed']:
            return None
        return self._values['device_password']
# ---
def main():
    # Contingency Table from Wilks (2011) Table 8.3
    table = np.array([[50, 91, 71],
                      [47, 2364, 170],
                      [54, 205, 3288]])
    mct = MulticlassContingencyTable(table, n_classes=table.shape[0],
                                     class_names=np.arange(table.shape[0]).astype(str))
    print(mct.peirce_skill_score())
    print(mct.gerrity_score())
# ---
def ssh_head(ctx, extra_args):
    """SSH to cluster head node using ray attach."""
    cmd_args = _maybe_add_ray_verbose(ctx.obj, ["ray", "attach", ctx.obj.config_file])
    if extra_args:
        cmd_args.extend(["--", *extra_args])
    subprocess.run(cmd_args, check=True)
# ---
def _list_relative_files(directory: str) -> set[str]:
            rel_files: set[str] = set()
            for root, _, filenames in os.walk(directory):
                for filename in filenames:
                    full_path = os.path.join(root, filename)
                    rel_files.add(os.path.relpath(full_path, directory))
            return rel_files
# ---
def _test(self, expr, expected):
        some_table = self.tables.some_table

        with config.db.connect() as conn:
            rows = {
                value
                for value, in conn.execute(
                    select([some_table.c.id]).where(expr)
                )
            }

        eq_(rows, expected)
# ---
def __init__(self, state: ControllerState):
        self._state = state
# ---
def act(self, state):

        return self.calculate(state)
# ---
def test_full_end_to_end_cache_with_groups():
    td = tempfile.TemporaryDirectory()
    with td as tmpdir:
        cache = build_or_load_cache(
            tmpdir,
            SimpleShardSource(num_shards=5),
            TestProcessor(),
        )

        expected = simple_process(TestProcessor(), SimpleShardSource(num_shards=5))

        all_data = cache[:]

        check_datasets_equal(all_data, expected)
# ---
def set_image_metadata_item(self, image_id, key, meta):
        """Sets the value for a specific image metadata key."""
        post_body = json.dumps({'meta': meta})
        resp, body = self.put('images/%s/metadata/%s' % (str(image_id), key),
                              post_body)
        body = json.loads(body)
        self.validate_response(schema.image_meta_item, resp, body)
        return service_client.ResponseBody(resp, body['meta'])
# ---
def __call__(self, module: M_contra, carry: CarryT, *args: P.args, **kwargs: P.kwargs) -> CarryT: ...
# ---
def test_getmethod_default_no_fd(self, monkeypatch):
        from _pytest.capture import pytest_addoption
        from _pytest.config import Parser
        parser = Parser()
        pytest_addoption(parser)
        default = parser._groups[0].options[0].default
        assert default == "fd" if hasattr(os, "dup") else "sys"
        parser = Parser()
        monkeypatch.delattr(os, 'dup', raising=False)
        pytest_addoption(parser)
        assert parser._groups[0].options[0].default == "sys"
# ---
def with_output(self, x, scalar):
            out = x + self.w + scalar
            return out, x * scalar
# ---
def the_object_name_is_not_an_ifc_element(name):
    id = the_object_name_exists(name).BIMObjectProperties.ifc_definition_id
    assert id == 0, f"The ID is {id}"
# ---
def find(cls, params=None):
        if params is None:
            params = dict()
        response = cls(Api.call('sales/detail_sale', params))
        return response.sale
# ---
def test_bank_has_if_and_for(bank):
    assert bank.has_type("If"), "Should extract If statements"
    assert bank.has_type("For"), "Should extract For loops"
# ---
def html_block(node: RenderTreeNode, context: RenderContext) -> str:
    content = node.content.rstrip("\n")
    # Need to strip leading spaces because we do so for regular Markdown too.
    # Without the stripping the raw HTML and Markdown get unaligned and
    # semantic may change.
    content = content.lstrip()
    return content
# ---
def genetic_modification_RNAi(testapp, lab, award):
    item = {
        'award': award['@id'],
        'lab': lab['@id'],
        'modified_site_by_coordinates': {
            'assembly': 'GRCh38',
            'chromosome': '11',
            'start': 20000,
            'end': 21000
        },
        'purpose': 'repression',
        'category': 'deletion',
        'method': 'RNAi'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def updatelastradio(url) :
    db = cherrypy.session['database']
    sql = "UPDATE Radio SET url='%s' WHERE id=0" % (url)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        con.commit()
        con.close()
    except:
        return
# ---
def test_str_replace_noregex(self):
        def test_impl(df):
            return df.A.str.replace('AB', 'EE', regex=False)

        df = pd.DataFrame({'A': ['ABCC', 'CABBD']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def test_from_files_empty_glob_error(tmp_path):
    """Test from_files raises error when no files match and empty_glob_ok=False."""
    input_dir = tmp_path / "input"
    input_dir.mkdir()

    # Should raise FileNotFoundError
    with pytest.raises(FileNotFoundError, match="No files found"):
        Dataset.from_files(f"{input_dir}/*.txt", empty_glob_ok=False)
# ---
def test_impl(df):
            return df.A.str.split(',')
# ---
def test_apply_bracket_constraints(tokenizer):
    logits = jnp.ones(tokenizer.vocab_size)
    constrained = apply_bracket_constraints(logits, "f(x", tokenizer)

    # ) should be allowed.
    assert float(constrained[tokenizer.encode_char(")")]) == 1.0
    # ] should be masked to -inf.
    assert float(constrained[tokenizer.encode_char("]")]) < -1e8
# ---
def _new_private_method(self, m):
        return 2 * m
# ---
def _read_ovf_from_zip_ova(ova_path):
    with open(ova_path, 'rb') as fh:
        zf = zipfile.ZipFile(fh)
        name = _find_ovf(zf.namelist())
        if name is not None:
            return zf.read(name)
    raise ClientError('OVA does not contains file with .ovf suffix')
# ---
def error_page_401(status, message, traceback, version):
    html = '''<!DOCTYPE html>
<html lang="en">
<head>
  <title>My Radio Web Server</title>
  <meta name="generator" content="Vim">
  <meta charset="UTF-8">
</head>
<body>
   '''
    html += "<h1>%s</h1>" % (status)
    html += "%s<br>" % (message)

    return html
# ---
def args(self):
        if self.parsed_args is None:
            if self.query_string:
                self.parsed_args = RequestParameters(
                    parse_qs(self.query_string)
                )
            else:
                self.parsed_args = RequestParameters()
        return self.parsed_args
# ---
def test_venv_creation():
    """Test basic venv creation and cleanup."""
    venv_path = None
    with TemporaryVenv() as venv:
        venv_path = venv.venv_path
        assert os.path.exists(venv_path)
        assert os.path.exists(venv.python_path)
        assert os.path.exists(venv.bin_path)
        assert venv.python_path == os.path.join(venv_path, "bin", "python")
        assert venv.bin_path == os.path.join(venv_path, "bin")
    assert not os.path.exists(venv_path)
# ---
def test_startswith_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab##c", escape="#"), {7})
# ---


def incr_list(l: list):
    """Return list with elements incremented by 1.
    >>> incr_list([1, 2, 3])
    [2, 3, 4]
    >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [6, 4, 6, 3, 4, 4, 10, 1, 124]
    """
    return [(e + 1) for e in l]
# ---
def validation_sets(self) -> Mapping[str, AsyncDataset[np.ndarray]]:
        doc_caches = self.build_caches("validation")
        return doc_caches
# ---
def __init__(self, root, cwd, badfn=None, relativeuipath=True):
        self._root = root
        self._cwd = cwd
        if badfn is not None:
            self.bad = badfn
        self._relativeuipath = relativeuipath
# ---
def init(cls, Vocab: Axis, config: MConfig, *, key: PRNGKeyArray) -> "ModelWithHfSerializationMixin":
        pass
# ---
def embed(self, input_ids: NamedArray):
        """
        Args:
            input_ids: token IDs with shape > {Vocab}
        """
        input_embeds = self.weight.take(self.Vocab, input_ids)
        return input_embeds * self.reparam.active_scale
# ---
def program(self):
        return self.startup.current_program() if self._get_startup() else None
# ---
def match(self, item):
        return not self.subquery.match(item)
# ---
def render(self, name, value, attrs=None):
        context = self.get_context(name, value, attrs or {})
        return render_to_string(self.template, context)
# ---
def convert_to_cache(self, value, record, validate=True):
        if value is None or value is False:
            return False
        return ustr(value)
# ---
def qCleanupResources():
    QtCore.qUnregisterResourceData(rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data)
# ---
def normalised_allowed_extensions(self) -> set[str]:
        return _normalise_allowed_extensions(self.allowed_extensions, casefold=self.casefold)
# ---
def get_block_is_duplicate(self, block: Block) -> bool:
        with self.lock:
            return self._state.get_block(block.headerhash) is not None
# ---
def __init__(self, status):
        """Initialize FakeResponse.

        :param status: Either 'failed' or 'passed'
        """
        self.Status = status

        if status == 'failed':
            self.Reason = 'Sample error'
# ---
def test_spawn_vhd_glance_linux(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_VHD, None, None,
                         os_type="linux", architecture="x86-64")
        self.check_vm_params_for_linux()
# ---
def clean_title(title):
    """Clean up the title by removing 'Experiment:' prefix."""
    if title.startswith("Experiment:"):
        return title[len("Experiment:") :].strip()
    return title
# ---
def test_actor_group_wait_ready_partial(client: LocalClient):
    group = client.create_actor_group(Counter, name="counters", count=5)
    handles = group.wait_ready(count=2)
    assert len(handles) == 2
# ---
def model_type(self):  # noqa: D401 â€“ property returns type, not a str
        """Return the Levanter model class for Gemma-2."""
        return Gemma2LMHeadModel
# ---
def test_job_name_to_safe_token_and_deep_nesting():
    job = JobName.from_string("/a/b/c/d/e/0")
    assert job.to_safe_token() == "job__a__b__c__d__e__0"
    assert job.require_task()[1] == 0
# ---
def __init__(self, username, password, httpsession):
        """Initialize the data object."""
        from pyebox import EboxClient

        self.client = EboxClient(username, password, REQUESTS_TIMEOUT, httpsession)
        self.data = {}
# ---
def __init__(self, method: Callable, lock: threading.Lock, context):
        self._method = method
        self._lock = lock
        self._context = context
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)
        pattern = self._normalize(pattern)
        try:
            self.pattern = re.compile(self.pattern)
        except re.error as exc:
            # Invalid regular expression.
            raise InvalidQueryArgumentValueError(pattern,
                                                 "a regular expression",
                                                 format(exc))
# ---
def addInit(cls, init):
        """
        Adds the `init` method to the list of extensions of the `MainWindow.__init__`.
        """
        cls.__inits.append(init)
# ---
def CRISPR_introduction(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'nucleic_acid_delivery_method': ['transient transfection']
    }
# ---
def unLoadPlugin(plugin_name):
    """
    @type plugin_name: str
    """
    pass
# ---
def test_capture_early_option_parsing(testdir):
    testdir.makeconftest("""
        def pytest_runtest_setup():
            print ("hello19")
    """)
    testdir.makepyfile("def test_func(): pass")
    result = testdir.runpytest("-vs")
    assert result.ret == 0
    assert 'hello19' in result.stdout.str()
# ---
def __init__(self, proto: config_pb2.IrisClusterConfig):
        """Create IrisConfig from proto.

        Args:
            proto: Cluster configuration proto (defaults will be applied)
        """
        self._proto = apply_defaults(proto)
# ---
def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / ((lr / min_lr - 1) * jnp.maximum(count, 1) / decay_steps + 1))
        return jnp.maximum(lr * decay, min_lr)
# ---
def add_to_dup_map(record: dict):
        shard_dup_map[record["hash"]] = {"canonical": record["canonical"]}
# ---
def array_equal(a: NamedArray, b: NamedArray) -> bool:
    """Returns True if two arrays have the same shape and elements."""
    if set(a.axes) != set(b.axes):
        return False
    b = b.rearrange(a.axes)
    return bool(jnp.array_equal(a.array, b.array))
# ---
def client(service):
    dashboard = ControllerDashboard(service)
    return TestClient(dashboard._app)
# ---
def unsafe_buffer_pointer(self):  # pragma: no cover
        return self._ref.unsafe_buffer_pointer()
# ---
def delete_object(obj):  # Release
    compss_delete_object(obj)
# ---
def tearDown(self):
        import shutil
        shutil.rmtree(self.repo_path)
# ---
def comma_main_mixture(*, tokenizer: str = llama3_tokenizer, permutation_type: PermType = "feistel"):
    """LmMixtureDatasetConfig for the main training stage."""
    tokenized = common_pile_tokenized(tokenizer=tokenizer)
    components = {f"common_pile/{dataset}": tokenized[f"common_pile/{dataset}"] for dataset in COMMON_PILE_DATASETS}
    return lm_mixture_data_config(
        components=components,
        weights=COMMA_MAIN_MIXTURE_WEIGHTS,
        permutation_type=permutation_type,
    )
# ---
def test_to_obj_simple():
    msg = '{"aa": 1, "cc": 3, "bb": 2}'
    converted = jps.utils.to_obj(msg)
    assert converted.aa == 1
    assert converted.bb == 2
    assert converted.cc == 3
    # works only super simple case
    json1 = converted.to_json()
    assert json1 == msg
# ---
def __repr__(self):
        return "<gitignorematcher>"
# ---
def test_cluster_list_jobs(cluster):
    request = JobRequest(
        name="list-test-job",
        entrypoint=Entrypoint.from_callable(lambda: None),
        environment=EnvironmentConfig.create(),
    )

    job_id = cluster.launch(request)
    jobs = cluster.list_jobs()
    assert isinstance(jobs, list)
    assert len(jobs) >= 1
    assert any(job.job_id == job_id for job in jobs)
# ---
def _make_grug_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape(len(devices), 1)
    return Mesh(
        mesh_devices,
        axis_names=("data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit),
    )
# ---
def gen_keys(self):
        '''
        Use libnacl to generate and safely save a private key
        '''
        import libnacl.public
        d_key = libnacl.dual.DualSecret()
        path = '{0}.key'.format(os.path.join(
            self.opts['gen_keys_dir'],
            self.opts['gen_keys']))
        d_key.save(path, 'msgpack')
# ---
def _create_lot(self, cr, uid, ids, product_id, prefix=False):
        """ Creates production lot
        @return: Production lot id
        """
        prodlot_obj = self.pool.get('stock.production.lot')
        prodlot_id = prodlot_obj.create(cr, uid, {'prefix': prefix, 'product_id': product_id})
        return prodlot_id
# ---
def fn(*args, **kwargs):
            stacked = haliax.vmap(module.init, Block)(*args, **kwargs)
            stacked = haliax.auto_sharded(stacked)
            return Stacked(stacked, Block, gradient_checkpointing)
# ---
def _wandb_mode_disabled() -> bool:
    mode = os.getenv("WANDB_MODE")
    if mode is None:
        return False
    return mode.lower() in {"disabled", "offline", "dryrun"}
# ---
def setUp(self):
        self.cg = CharacterGeneratorMock(width = 9, height = 14)
        self.mda = MonochromeDisplayAdapter(self.cg)

        # Hijack reset so it doesn't call into Pygame during the tests.
        self.reset_count = 0
        self.mda.reset = self.reset_testable
# ---
def indentBlock(block):
            cursor = cursorAtSpaceEnd(block)
            cursor.insertText(' ' if withSpace else self.text())
# ---
def with_status_check(obj, *args, **kwargs):
            if obj.status not in valid_start_statuses:
                exception_msg = (
                    u"Error calling {} {}: status is '{}', must be one of: {}"
                ).format(func, obj, obj.status, valid_start_statuses)
                raise VerificationException(exception_msg)
            return func(obj, *args, **kwargs)
# ---
def test_connect_as_ctx_noautocommit(self):
        fn = self._trans_fn()
        self._assert_no_data()
        ctx = testing.db.connect().execution_options(autocommit=False)
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        # autocommit is off
        self._assert_no_data()
# ---
def isinf(a: A) -> A:
    return wrap_elemwise_unary(jnp.isinf, a)
# ---
def __init__(self, exemplar: T):
        self.exemplar = exemplar
# ---
def context_click(self, on_element):
        """Performs a context-click (right click) on an element.
        Args:
            on_element: The element to context-click.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.CLICK, {'button': 2}))
        return self
# ---
def _unstack_matrices(stacked_arrays, revert_indices):
    in_tuple = isinstance(stacked_arrays, tuple)
    unstacked = []
    for arr in stacked_arrays:
        unstacked.extend(jnp.split(arr, arr.shape[0]))
    array_list = [jnp.squeeze(unstacked[i], axis=0) for i in revert_indices]
    if in_tuple:
        return tuple(array_list)
    return array_list
# ---
def __init__(self, ray_options: dict | None = None):
        self.ray_options = ray_options or {}
# ---
def readlineCR(uart):
	line = b''
	while True:
		byte = uart.read()
		line += byte
		if byte == b'\r':
			return line
# ---
def resolver(self) -> Resolver:
        """Get a resolver for actor discovery.

        The resolver uses the namespace derived from this context's job ID.

        Raises:
            RuntimeError: If no client is available
        """
        return NamespacedResolver(self.client._cluster_client, self.namespace)
# ---
def create_actor(self) -> ActorHandle:
        assert self._slice_info
        slice_name = self._slice_info.slice_name
        return TPUHostActor.options(resources={slice_name: 1}, num_cpus=0.0).remote(self._slice_info)
# ---
def unique_batch_sizes(self):
        return set(seg.value for seg in self.segments)
# ---
def test_join_file_workers():
    producers = [Mock()]
    cworker = Mock()
    consumers = [Mock()]

    classify_documents._join_file_workers(cworker, producers, consumers)

    for p in producers:
        assert p.join.called
    assert cworker.set_file_producers_done.called
    for c in consumers:
        assert c.join.called
    assert cworker.clear_file_producers_done.called
# ---
def test_transaction_connection_fn_rollback(self):
        fn = self._trans_rollback_fn()
        conn = testing.db.connect()
        assert_raises(
            Exception,
            conn.transaction, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def exists(url, **kwargs) -> bool:
    """Check if a file exists on a remote filesystem."""
    fs, path = fsspec.core.url_to_fs(url, **kwargs)
    return fs.exists(path)
# ---
def __call__(self, x, key=None):
        if key is None and self.dropout.is_active:
            raise RuntimeError(
                "Cannot call LoraLinear with dropout and without a key if dropout is enabled."
                " The base model needs to be retrofitted to pass keys to the Linear layers."
            )
        x = self.dropout(x, key=key)
        z = self.lora_A(x)
        z = self.lora_B(z)
        return z * self.scale
# ---
def my_callback():
        _save_metadata(checkpoint_path, fs, step, is_temporary)
        logger.info(f"Saved checkpoint to {checkpoint_path} for step {step}")

        if commit_callback is not None:
            commit_callback()
# ---
def on_step(self, info: StepInfo[S], force: bool = False): ...
# ---
from typing import List


def concatenate(strings: List[str]) -> str:
    """ Concatenate list of strings into a single string
    >>> concatenate([])
    ''
    >>> concatenate(['a', 'b', 'c'])
    'abc'
    """
    return ''.join(strings)
# ---
def restart(self) -> str:
        """Stop then start controller."""
        self.stop()
        return self.start()
# ---
def entropy_from_logits(logits: hax.NamedArray, axis: hax.AxisSelector) -> hax.NamedArray:
    """
    Computes entropy over the given axis in a numerically stable way using raw logits.
    """
    log_z = hnn.logsumexp(logits, axis=axis)
    probs = hax.exp(logits - log_z)
    entropy = log_z - hax.sum(probs * logits, axis=axis)
    return entropy
# ---
def __str__(self) -> str:
        return repr(self)
# ---
def get_citizenship(self, nick):
        return self.data[nick.lower()]['citizenship']
# ---
def __setitem__(self, key, val):
        self.__dict__[key] = val
# ---
def test_fixed_window_with_elastic_expiry(self):
        storage = MemcachedStorage("memcached://localhost:22122")
        limiter = FixedWindowElasticExpiryRateLimiter(storage)
        per_sec = RateLimitItemPerSecond(2, 2)

        assert limiter.hit(per_sec)
        time.sleep(1)
        assert limiter.hit(per_sec)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert limiter.test(per_sec)
# ---
def save(self):
        pass
# ---
def __str__(self):
        return f"WrappedAsyncDataset({str(self.dataset)})"
# ---
def decontaminate(config: DeconConfig):
    """Main entry point for decontamination workflows."""
    if config.mode == DeconMode.DECONTAMINATE:
        return _run_decontamination(config)
    elif config.mode == DeconMode.TRAIN_TEST_OVERLAP:
        return _run_train_test_overlap(config)
    else:
        raise ValueError(f"Unknown mode {config.mode}")
# ---
def enterOff(self):
        pass
# ---
def test_vm_manager_create_returns_vm_group(
    mock_run: MagicMock,
    vm_manager_factory: tuple[str, object],
):
    """VmManager.create_vm_group() returns a VmGroup with VMs."""
    mock_run.return_value = MagicMock(returncode=0, stdout="", stderr="")
    _platform_type, manager = vm_manager_factory

    vm_group = manager.create_vm_group()

    assert vm_group is not None
    assert len(vm_group.vms()) >= 1
# ---
def do(c):
                return c.at["seq", tid].set(INVALID)
# ---
def cancel(self, order):
        try:
            self.pending.remove(order)
        except ValueError:
            # If the list didn't have the element we didn't cancel anything
            return False

        order.cancel()
        self.notify(order)
        return True
# ---
def my_base(n: str, base: int = 10) -> int:
        return int(n, base)
# ---
def test_one_hot_out_of_bound():
    i, c = hax.make_axes(i=2, c=3)
    actual = hax.nn.one_hot(hax.NamedArray(jnp.array([-1, 3]), (i,)), c)
    expected = jnp.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])
    assert jnp.all(jnp.isclose(actual.array, expected))
# ---
def __init__(self, subtype, msg=None):
        super(PhylotyperError, self).__init__(
            subtype, msg="Unrecognized subtype {}".format(subtype))
# ---
def register_endpoint(
        self,
        name: str,
        address: str,
        job_id: JobName,
        metadata: dict[str, str] | None = None,
    ) -> str:
        request = cluster_pb2.Controller.RegisterEndpointRequest(
            name=name,
            address=address,
            job_id=job_id.to_wire(),
            metadata=metadata or {},
        )
        response = self._client.register_endpoint(request)
        return response.endpoint_id
# ---
def unflatten_from_export(self, template: "XIELUActivation") -> "XIELUActivation":
        """Squeeze [1] parameters back to scalars."""
        del template
        alpha_p = hax.named(jnp.squeeze(self.alpha_p.array), ())
        alpha_n = hax.named(jnp.squeeze(self.alpha_n.array), ())
        return XIELUActivation(alpha_p, alpha_n, self.beta, self.eps)
# ---
def intermediate(self, x):
            return x + 2 * self.w
# ---
def __enter__(self):
        return self
# ---
def get_ls_desc(desc, print_id=False):
    addendum = ' : ' + desc['id'] if print_id is True else ''
    if desc['class'] in ['applet', 'workflow']:
        return BOLD() + GREEN() + desc['name'] + ENDC() + addendum
    else:
        return desc['name'] + addendum
# ---
def _make(
        name: str = "test-job",
        cpu: int = 1,
        memory_bytes: int = 1024**3,
    ) -> cluster_pb2.Controller.LaunchJobRequest:
        return cluster_pb2.Controller.LaunchJobRequest(
            name=name,
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=cpu, memory_bytes=memory_bytes),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )
# ---
def test_contains_autoescape(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b%cde", autoescape=True), {3})
# ---
def cached_resource_by_id(self, resource_id, name=None):
        return VOLUME_TYPES[resource_id]
# ---
def order_clause(self):
        order_strings = []
        for sort in self._sql_sorts():
            order = sort.order_clause()
            order_strings.append(order)

        return ", ".join(order_strings)
# ---
def __hash__(self):
        return hash((self.pattern, tuple(self.fields), self.query_class))
# ---
def __init__(self, config: CurriculumConfig):
        self._server = ActorServer(host="localhost")
        self._server.register("curriculum", Curriculum(config))
        self._server.serve_background()
# ---
def stub_add_volume_type_access(context, type_id, project_id):
            self.assertEqual(fake.VOLUME_TYPE4_ID, type_id, "type_id")
            self.assertEqual(PROJ2_UUID, project_id, "project_id")
# ---
def convert_to_export(self, value, env):
        return ','.join(name for id, name in value.name_get()) if value else ''
# ---
def test_hf_gpt2_roundtrip():
    _roundtrip_compare_gpt2_checkpoint("gpt2", None)
# ---
def read_jsonl_gz():
    """Fixture to read JSONL gzipped files."""

    def _read(path: Path) -> list[dict]:
        records = []
        with gzip.open(path, "rt", encoding="utf-8") as handle:
            for line in handle:
                if line.strip():
                    records.append(json.loads(line))
        return records

    return _read
# ---
def test_proxy(self):
        self.assertFalse(self.get.has_proxy())
        self.get.set_proxy("www.perl.org", "http")
        self.assertTrue(self.get.has_proxy())
        self.assertEqual("www.python.org", self.get.get_origin_req_host())
        self.assertEqual("www.perl.org", self.get.get_host())
# ---
def choice(question, options, default):
    "Ask the user to choose from a short list of named options"
    while True:
        sys.stdout.write("{} ({}) [{}]: ".format(question, "/".join(options), default))
        answer = sys.stdin.readline().strip()
        if len(answer) == 0:
            return default
        for opt in options:
            if answer == opt:
                return answer
# ---
def action_cancel(self, cr, uid, ids, context=None):
        """ Changes picking state to cancel.
        @return: True
        """
        for pick in self.browse(cr, uid, ids, context=context):
            ids2 = [move.id for move in pick.move_lines]
            self.pool.get('stock.move').action_cancel(cr, uid, ids2, context)
        self.write(cr, uid, ids, {'state': 'cancel', 'invoice_state': 'none'})
        return True
# ---
def _read_jsonl(path: Path) -> list[dict]:
    """Read records from a JSONL file."""
    records = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            if line.strip():
                records.append(json.loads(line))
    return records
# ---
def markdownify_kwargs(self) -> dict:
        exclude = {"include_images", "include_links"}
        return {f.name: getattr(self, f.name) for f in fields(self) if f.name not in exclude}
# ---
def negative(a: A) -> A:
    return wrap_elemwise_unary(jnp.negative, a)
# ---
def replace_latex_environment(text: str, env_name: str, replacer: Callable[[str], str]) -> str:
    """Replace \\begin{env}...\\end{env} with processed content."""
    pattern = f"\\\\begin{{{env_name}}}(.*?)\\\\end{{{env_name}}}"
    return re.sub(pattern, lambda m: replacer(m.group(1)), text, flags=re.DOTALL)
# ---
def exception(self) -> BaseException | None:
        if not self._future.done():
            return None
        return self._future.exception()
# ---
def position_from_token(self, token_id: int) -> int:
        """Extract the position index from a position token ID.

        Raises ValueError if the token is not a position token.
        """
        if not self.is_position_token(token_id):
            raise ValueError(f"Token {token_id} is not a position token")
        return token_id - self.position_token_offset
# ---
def test_empty_set_against_string_negation(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.z.notin_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [(1,), (2,), (3,), (4,)], params={"q": []})
# ---
def test_select_columns(backend):
    """Test column projection with select."""
    ds = Dataset.from_list(
        [
            {"id": 1, "name": "alice", "score": 80, "extra": "x"},
            {"id": 2, "name": "bob", "score": 60, "extra": "y"},
        ]
    ).select("id", "name")

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert results[0] == {"id": 1, "name": "alice"}
    assert results[1] == {"id": 2, "name": "bob"}
# ---
def _is_url_like(path):
        return urllib.parse.urlparse(path).scheme != ""
# ---
def test_synthetic_subtrees_no_duplicates(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=20)
    sources = [e.source for e in entries]
    assert len(sources) == len(set(sources))
# ---
def geturl(self):
        return self.url
# ---
def get_absolute_url(self):
        return reverse('software_edit', kwargs={'pk': self.pk})
# ---
def copy(self) -> "TransformAdapter":
        return dataclasses.replace(self)
# ---
def abs(self, f):
        """Convert a repo path back to path that is relative to the root of the
        matcher."""
        return f
# ---
def read_log(path: str):
    with open(path) as f:
        return list(map(json.loads, f.readlines()))
# ---
def test_can_accept_demand_true_when_available(self, unbounded_config: config_pb2.ScaleGroupConfig):
        """can_accept_demand() returns True when AVAILABLE."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(unbounded_config, manager)

        assert group.can_accept_demand() is True
# ---
def get_task_status(self, task_name: JobName) -> cluster_pb2.TaskStatus:
        return self._remote_client.get_task_status(task_name)
# ---
def remove_footnotes(html: BeautifulSoup):
    # Remove footnotes since they are plopped in the middle of the text
    footnotes = html.findAll("div", {"class": "ltx_role_footnote"})
    for fn in footnotes:
        fn.decompose()
# ---
def test___cmp__ge(self):
        self._test__cmp__(
            lambda left, right: left >= right,
            (
                True,
                True,
                True,
                False,
                True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
            ),
            '>='
        )
# ---
def FetchTaskLogs(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def copy(self, cr, uid, id, default=None, context=None):
        if default is None:
            default = {}
        default = default.copy()
        default.update({'move_ids': [], 'date_done': False})
        return super(stock_inventory, self).copy(cr, uid, id, default, context=context)
# ---
def registry(self) -> VmRegistry:
        """Access the underlying registry for cleanup operations."""
        ...
# ---
def barf(*args):
        raise NotImplementedError('Access denied')
# ---
def test_find_span_end_call():
    source = "x = foo(1)\n"
    # The Call "foo(1)" starts at offset 4.
    end = _find_span_end(source, 4)
    assert end is not None
    assert source[4:end] == "foo(1)"
# ---
def __init__(self, volume_size=0):
        self.volume_name = None
        self.name = None
        self.volume_id = None
        self.volume_size = volume_size
        self.user_id = None
        self.status = None
# ---
def test_other_dtype_annotation():
    def bar(x: i32["batch"]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(bar, include_extras=True)["x"])[1]
    assert spec.dtype == jnp.int32
    assert spec.before == ("batch",)
# ---
def divmod(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.divmod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.divmod.html)
    """
    return jnp.divmod(x1, x2)
# ---
def safe_update_wrapper(wrapper, wrapped):
    """
    As [safe_wraps][] but not a decorator.
    Args:
        wrapper:
        wrapped:

    Returns:

    """
    if isinstance(wrapper, equinox.Module):
        return equinox.module_update_wrapper(wrapper, wrapped)
    else:
        return functools.update_wrapper(wrapper, wrapped)
# ---
def test_get_profiler_and_init(self):
        p = profiler.init("secret", base_id="1", parent_id="2")
        self.assertEqual(profiler.get(), p)

        self.assertEqual(p.get_base_id(), "1")
        # NOTE(boris-42): until we make first start we don't have
        self.assertEqual(p.get_id(), "2")
# ---
def release_slot(self, slot_id: int) -> "SequenceTable":
        mask = hax.zeros_like(self.used_mask)
        mask = mask.at["seq", slot_id].set(True)
        return self.clear_slots(mask)
# ---
def main():
    unittest.main()
# ---
def _get_cached_model_files(cache_dir: str) -> list[str]:
    """Find all model weight files in the HF cache directory."""
    model_files = []
    if os.path.exists(cache_dir):
        for root, _, files in os.walk(cache_dir):
            for f in files:
                if f.endswith((".safetensors", ".bin", ".pt", ".pth")):
                    model_files.append(os.path.join(root, f))
    return model_files
# ---
def test_lookup(self):
        assert event.Priority.lookup(event.Priority.CORE) is event.Priority.CORE
        assert event.Priority.lookup(event.Priority.CORE.value) is event.Priority.CORE
        assert event.Priority.lookup(-12312412) == -12312412
# ---
def build(self, reference_model: eqx.Module) -> eqx.Module:
        """Initialize any learned components (e.g., value heads)."""
        return self
# ---
def style(self):
        """The window style; one of the ``WINDOW_STYLE_*`` constants.
        Read-only.

        :type: int
        """
        return self._style
# ---
def test_magic_index_props(self):
        """Index can look up properties on response object"""
        index = GlobalIndex.all("idx-name", DynamoKey("id"))
        index.response = {"FooBar": 2}
        self.assertEqual(index["FooBar"], 2)
        with self.assertRaises(KeyError):
            self.assertIsNotNone(index["Missing"])
# ---
def never(root, cwd):
    return nevermatcher(root, cwd)
# ---
def testFunctionDefGeneratorReturnValue(self):
    self.assertRaisesRegexp(
        util.ParseError, 'returning a value in a generator function',
        _ParseAndVisit, 'def foo():\n  yield 1\n  return 2')
# ---
def test_equal(self):
        expr = col("score") == 100
        assert expr.evaluate({"score": 100}) is True
        assert expr.evaluate({"score": 50}) is False
# ---
def TrainBatch(self):
        if not isinstance(self.train_batch_size, int):
            raise ValueError("TrainBatch is only valid for a single batch size. Use batch_axis_at_step instead")
        return Axis(self.batch_axis_name, self.train_batch_size)
# ---
def do_scan(init: CarryT, *args, **kwargs) -> tuple[CarryT, OutputT_co]:
            return haliax.scan(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *args, **kwargs)
# ---
def include_file(path: str):
                return os.path.getmtime(path) > initial_time
# ---
def check_bot_name_available(realm_id: int, full_name: str) -> None:
    dup_exists = UserProfile.objects.filter(
        realm_id=realm_id,
        full_name=full_name.strip(),
        is_active=True,
    ).exists()

    if dup_exists:
        raise JsonableError(_("Name is already in use!"))
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.four.unique() == 3.0).sum()
# ---
def finish(self):
        import trackio

        logger.info("Finishing trackio run...")
        trackio.finish()
# ---
def __init__(self, inference_config: vLLMInferenceContextConfig):
        inference_config.mode = InferenceMode.ASYNC
        super().__init__(inference_config)
# ---
def genetic_modification_6(lab, award, crispr_gm, source):
    return {
        'purpose': 'validation',
        'category': 'deeltion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'description': 'blah blah description blah',
        "method": "CRISPR",
        "modified_site_by_target_id": "/targets/FLAG-ZBTB43-human/",
        "reagents": [
            {
                "identifier": "placeholder_id",
                "source": source['uuid']
            }
        ]
    }
# ---
def list_jobs(filters: list[str] | None = None) -> list[dict]:
    """Fetch the list of jobs using the Ray CLI."""
    cmd = ["ray", "list", "jobs", "--detail", "--format=json", "--limit=10000"]
    for f in filters or []:
        cmd.extend(["--filter", f])

    result = run_ray_command(cmd)
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError:
        logger.warning(f"Failed to parse JSON output from ray list jobs: {result.stdout} -- {result.stderr}")
        return []
# ---
def _norm(v: Union[str, Sequence[str]]) -> Union[str, Tuple[str, ...]]:
    if isinstance(v, (list, tuple)):
        v = tuple(v)
        return v if len(v) > 1 else v[0]
    return v
# ---
def get_code(self, indices=None):
        cells = self.get_code_cells()
        if indices is None:
            indices = list(range(len(cells)))
        return '\n\n\n'.join([cells[i] for i in indices])
# ---
def __matmul__(self, other) -> "NamedArray":  # pragma: no cover
        raise ValueError("matmul is too ambiguous with NamedArrays. Use dot instead.")
# ---
def test_task_config():
    task_spec = [
        TaskConfig(
            task="hellaswag",
            task_alias="hellaswag_10shot",
            num_fewshot=10,
        ),
        TaskConfig(
            task="hellaswag",
            task_alias="hellaswag_5shot",
            num_fewshot=5,
        ),
        "lambada_openai",
    ]

    config = LmEvalHarnessConfig(
        task_spec=task_spec,
    )

    q = config.to_task_dict()

    assert len(q) == 3
# ---
def get_ports(self):
        ''' get a list of ports '''
        return self.get(Service.port_path) or []
# ---
def _sample_array():
    Height, Width = hax.make_axes(Height=2, Width=3)
    data = jnp.array([[1.0, jnp.nan, 3.0], [jnp.nan, 5.0, 6.0]])
    arr = hax.named(data, (Height, Width))
    return Height, Width, data, arr
# ---
def calculate_sharpe(self):
        """
        http://en.wikipedia.org/wiki/Sharpe_ratio
        """
        return sharpe_ratio(self.algorithm_volatility,
                            self.algorithm_period_returns,
                            self.treasury_period_return)
# ---
def match(self, item):
        return False
# ---
def test_write_bytes_to_buffer(self):
        """In python3, stdout / stderr are text io wrappers (exposing a buffer
        property of the underlying bytestream).  See issue #1407
        """
        f = capture.CaptureIO()
        f.buffer.write(b'foo\r\n')
        assert f.getvalue() == 'foo\r\n'
# ---
def get_dates(schema):
    """Return list of datetime fields for given schema."""
    dates = [config.LAST_UPDATED, config.DATE_CREATED]
    for field, field_schema in schema.items():
        if field_schema['type'] == 'datetime':
            dates.append(field)
    return dates
# ---
def asdict_without_nones(obj: DataclassInstance) -> dict:
    """Convert dataclass to dictionary, omitting None values."""
    if not dataclasses.is_dataclass(obj):
        raise ValueError(f"Expected dataclass, got '{obj}'")
    return dataclasses.asdict(obj, dict_factory=lambda x: {k: v for (k, v) in x if v is not None})
# ---
def activate(self):
        """Attempt to restore keyboard focus to the window.

        Depending on the window manager or operating system, this may not
        be successful.  For example, on Windows XP an application is not
        allowed to "steal" focus from another application.  Instead, the
        window's taskbar icon will flash, indicating it requires attention.
        """
        raise NotImplementedError('abstract')
# ---
def _chunk(x: NamedArray) -> NamedArray:
        return x.unflatten_axis(PosPad, (Chunks, C))
# ---
def d(self, id=""):
        html = "<h2>Delete</h2>"
        if id == "" or id == None :
            html += "Error"
            return html

        if id == "0" :
          html += "0 is reserved, sorry"
          return html

        #if delete(id) == False:
        if nonexist(id) == False:
            html += "Delete error in id" % id
            html += getfooter()
            return html

        html += "Item %s set as non existent" % id
        return html
# ---
def step_impl(context):
    context.execute_steps(u'''
        given I open History dialog
    ''')
    # Click on import
    context.browser.find_element_by_id('ImportHistory').click()
    WebDriverWait(context.browser, 10).until(
        expected_conditions.visibility_of_element_located(
            (By.ID, 'ImportRequestForm')))
# ---
def list_endpoints(self, prefix: str) -> list[cluster_pb2.Controller.Endpoint]:
        request = cluster_pb2.Controller.ListEndpointsRequest(prefix=prefix)
        response = self._client.list_endpoints(request)
        return list(response.endpoints)
# ---
def _patch_source_config(
    input_config: LmDatasetSourceConfigBase, output_path: str, extra_tags: list[str]
) -> LmDatasetSourceConfigBase:
    """
    Patch the source config to point to the new cache.

    TODO: would be better to make this more explicit somehow...
    """
    base_tags = input_config.tags or []
    return dataclasses.replace(input_config, cache_dir=output_path, tags=base_tags + extra_tags)
# ---
def fold_fun(acc, x, static1, *, static2):
        assert static1 is True
        assert static2 is False
        return NamedArray(acc.array + x.rearrange(acc.axes).array, acc.axes)
# ---
def flashPersistenceToggled(self, checkbox):
        grid = self.get_widget('flashMessageDurationGrid')
        grid.set_sensitive(not checkbox.get_active())
        self.prefsDict["flashIsPersistent"] = checkbox.get_active()
# ---
def num_cpus(self) -> float:
        return 0.1
# ---
def load_apartment_by_doorbell_id(doorbell_id):
        # type: (int) -> Optional[ApartmentDTO]
        apartment_orm = Apartment.select().where(Apartment.doorbell_rebus_id == doorbell_id).first()
        if apartment_orm is None:
            return None
        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
        return apartment_dto
# ---
def test_activity_regularization(self):
    layer = keras.layers.ActivityRegularization(l1=0.1)
    layer(keras.backend.variable(np.ones((2, 4))))
    self.assertEqual(1, len(layer.losses))
    config = layer.get_config()
    self.assertEqual(config.pop('l1'), 0.1)
# ---
def delete_floatingip_precommit(self, context, fip_context):
        pass
# ---
def test_preserved(self):
        serial, timestamp, tyield = self.PRESERVE_RECORD

        assert_equals(self.db.get_last_historic(serial), timestamp)
        assert_equals(self.db.get_one_historic(serial, timestamp), tyield)
# ---
def __init__(self, execute = False, ip = '127.0.0.1', port = 5000, npc = False):
		self.ip = ip
		self.port = port
		self.npc = npc
		if (execute):
			self.iniciaBatalha()
# ---
def get_tracker(name: Literal["tensorboard"]) -> TensorboardTracker: ...
# ---
def tokenize_nemotron_subset(name: str, tokenizer: str | None = None) -> ExecutorStep[TokenizeConfig]:
    """Get a specific nemotron split tokenization step."""
    assert name in NEMOTRON_DATASETS, f"Split {name} not found in NEMOTRON_DATASETS"
    return tokenize_nemotron(tokenizer=tokenizer)[f"nemotron_cc/{name}"]
# ---
def get_service(name: str) -> ServiceInfo | None:
    """Get a service by name."""
    register_services()
    return SERVICES.get(name)
# ---
def _compute_proc_stats(self, start_proc_time):
        cur_time = get_ion_ts_millis()
        self._record_proc_time(cur_time)
        proc_time = cur_time - start_proc_time
        self._proc_time += proc_time
# ---
def value_dim(self) -> int:
        return self.num_v_heads * self.head_v_dim
# ---
def getText(self, callingWindow, itmContext, mainItem):
        return "Set {} as Damage Pattern".format(itmContext if itmContext is not None else "Item")
# ---
def test_profiler_get_id(self, mock_generate_uuid):
        mock_generate_uuid.return_value = "43"
        prof = profiler._Profiler("secret")
        prof.start("test")
        self.assertEqual(prof.get_id(), "43")
# ---
def test_get_with_invalid_election_id_non_existent_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': '69' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified an ID for a non-existent election.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def wait_for_ready(expression: str, label: str) -> None:
            try:
                page.wait_for_function(expression, timeout=30000)
            except PlaywrightTimeoutError:
                logger.warning("Timed out waiting for %s; capturing anyway", label)
# ---
def test_build_bootstrap_script_prepends_discovery_preamble(
        self, minimal_bootstrap_config: config_pb2.BootstrapConfig
    ):
        """Discovery preamble is prepended to script."""
        preamble = "export CONTROLLER_ADDRESS=http://10.0.0.1:10000\n"
        script = _build_bootstrap_script(
            minimal_bootstrap_config,
            vm_address="10.0.0.1",
            discovery_preamble=preamble,
        )

        assert script.startswith(preamble)
# ---
def init(a1, a2):
            return MyModule2(MyModule(a1, a2), MyModule(a1, a2))
# ---
def _get_first_task_error(self, job_id: JobName) -> str | None:
        """Get the first error message from failed/killed tasks in a job."""
        for task_id in self._tasks_by_job.get(job_id, []):
            task = self._tasks.get(task_id)
            if task and task.error:
                return task.error
        return None
# ---
def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / jnp.sqrt(jnp.maximum(count + warmup_steps, 1) / timescale))
        return jnp.maximum(lr * decay, min_lr)
# ---
def create_buffer():
        buffer = io.BytesIO()
        test_data.to_parquet(buffer, index=False)
        buffer.seek(0)
        return buffer
# ---
def fmax(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.fmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fmax.html)
    """
    return jnp.fmax(x1, x2)
# ---
def __bool__(self):
        return bool(self.reused_arrays or self.static_arrays)
# ---
def test_entrypoint_proto_roundtrip_preserves_bytes():
    """Bytes survive to_proto -> from_proto without deserialization."""
    ep = Entrypoint.from_callable(_add, 1, 2)
    original_bytes = ep.callable_bytes

    proto = ep.to_proto()
    ep2 = Entrypoint.from_proto(proto)

    assert ep2.callable_bytes == original_bytes
    fn, args, kwargs = ep2.resolve()
    assert fn(*args, **kwargs) == 3
# ---
def __init__(self, maxlen: int | None = None):
        self._queue: list[RolloutBatch] = []
        self._maxlen = maxlen
        self._lock = threading.Lock()
        self._not_full = threading.Condition(self._lock)
        self._not_empty = threading.Condition(self._lock)
# ---
def schemes(self):
        '''the scheme, split by plus signs'''
        return self.scheme.split('+')
# ---
def testRaiseAgain(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        try:
          try:
            raise AssertionError('foo')
          except AssertionError:
            raise
        except Exception as e:
          print e""")))
# ---
def ones_like(a: NamedArray, dtype=None) -> NamedArray:
    """Creates a NamedArray with all elements set to 1"""
    return NamedArray(jnp.ones_like(a.array, dtype=dtype), a.axes)
# ---
def _setup_regular(self, env):
        super(Reference, self)._setup_regular(env)
        assert isinstance(self.size, (NoneType, int)), \
            "Reference field %s with non-integer size %r" % (self, self.size)
# ---
def product_types(cls):
    return ['go']
# ---
def _filter_token(tok):
	return str(tok).translate(_TOKEN_FILTER_MAP)
# ---
def _start_daemon(self):
        ovn_nbctl = self._get_ovn_controller(self.install_method)
        return ovn_nbctl.start_daemon()
# ---
def apply(self, source: str) -> str:
        """Apply this mutation to a source string."""
        return source[: self.start] + self.replacement + source[self.end :]
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Call method synchronously."""
        return self._method(*args, **kwargs)
# ---
def test_rolling3(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            Ac = df.A.rolling(3, center=True).apply(lambda a: a[0] + 2 * a[1] + a[2])
            return Ac.sum()

        hpat_func = self.jit(test_impl)
        n = 121
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def add(self, a: int, b: int) -> int:
        return a + b
# ---
def _small_dataset(seq_len=128, num_sequences=200) -> AsyncDataset[Sequence[int]]:
    sequences = [np.arange(seq_len) + 1000 * i for i in range(num_sequences)]

    return ListAsyncDataset(sequences, is_complete=True)
# ---
def __init__(self, args):
        self.args = args
        self.setattributes(self.args)
# ---
def pow_(cls, base: "PyExpr", exp: "PyExpr") -> "PyExpr": ...
# ---
def test_can_restore_from_backup_with_almost_equal_size(self):
        # target size equals to "1Gb"
        self.backup.size = 0.99
        self.backup.save()
        instance = models.Instance.create(
            self.context, self.name, self.flavor_id,
            self.image_id, self.databases, self.users,
            self.datastore, self.datastore_version,
            self.volume_size, self.backup_id,
            self.az, self.nics, self.configuration)
        self.assertIsNotNone(instance)
# ---
def norm(x, y):
    l = (x*x + y*y)**0.5
    return (x/l, y/l)
# ---
def percentage(self) -> float:
        """Calculate usage percentage, handling unit conversions."""
        try:
            used_val = float(self.used.replace("TiB", "").replace("GiB", "").replace("MiB", "").replace("KiB", ""))
            total_val = float(self.total.replace("TiB", "").replace("GiB", "").replace("MiB", "").replace("KiB", ""))
            return (used_val / total_val * 100) if total_val > 0 else 0.0
        except (ValueError, ZeroDivisionError):
            return 0.0
# ---
def defineCharacteristics(self):
        self.name = "Split RGB bands"
        self.group = "Grid - Tools"
        self.addParameter(ParameterRaster(SplitRGBBands.INPUT, "Input layer", False))
        self.addOutput(OutputRaster(SplitRGBBands.R, "Output R band layer"))
        self.addOutput(OutputRaster(SplitRGBBands.G, "Output G band layer"))
        self.addOutput(OutputRaster(SplitRGBBands.B, "Output B band layer"))
# ---
def onShortcutUnindentWithBackspace(self):
        """Backspace pressed, unindent
        """
        assert self._qpart.textBeforeCursor().endswith(self.text())

        charsToRemove = len(self._qpart.textBeforeCursor()) % len(self.text())
        if charsToRemove == 0:
            charsToRemove = len(self.text())

        cursor = self._qpart.textCursor()
        cursor.setPosition(cursor.position() - charsToRemove, QTextCursor.KeepAnchor)
        cursor.removeSelectedText()
# ---
def enterTransitionToCostume(self):
        pass
# ---
def test_select_from_plain_union(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2)
        s2 = select([table]).where(table.c.id == 3)

        u1 = union(s1, s2).alias().select()
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def test_capturing_reset_simple(self):
        with self.getcapture() as cap:
            print("hello world")
            sys.stderr.write("hello error\n")
            out, err = cap.readouterr()
        assert out == "hello world\n"
        assert err == "hello error\n"
# ---
def shift(self, tok, value):
        """Adds one more token to the state.  Calls reduce()."""

        self.tokens.append(tok)
        self.values.append(value)

        # Do a greedy reduce...
        self.reduce()
# ---
def restore(self):
        if self.ex is not None:
            exc_value = self.ex.with_traceback(self.tb.as_traceback())
            return (self.ex.__class__, exc_value, self.tb.as_traceback())
        else:
            return (Exception, Exception("Process failed with no exception"), self.tb.as_traceback())
# ---
def result(self, timeout: float | None = None) -> Any:
        return ray.get(self._object_ref, timeout=timeout)
# ---
def countdown(n):
            for i in range(n, 0, -1):
                print(f"Countdown: {i}")
                time.sleep(0.3)
            print("Liftoff!")
            return "Done!"
# ---
def permutation_subjects_ktst(y):
    """Permute class labels of Contextual Disorder dataset for KTST.
    """
    yp = np.random.randint(0, 2, len(y)/2)
    yp = np.concatenate((yp, np.logical_not(yp).astype(int)))
    y_perm = np.arange(len(y))
    for i in range(len(y)/2):
        if yp[i] == 1:
            y_perm[i] = len(y)/2+i
            y_perm[len(y)/2+i] = i
    return y_perm
# ---
def gamma(key, shape: AxisSpec, a: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    a = broadcast_to(a, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.gamma(key=key, a=a, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def test_rerank_preserves_all_candidates():
    candidates = [_make_candidate(f"x = {i}\n", score=-float(i)) for i in range(5)]
    ranked = rerank_candidates(candidates, ["assert True"])
    assert len(ranked) == 5
# ---
def discover(self) -> str | None:
        """Return address if controller is healthy via SSH check."""
        host = self._manual_config.host
        port = self._manual_config.port or DEFAULT_CONTROLLER_PORT
        conn = self._create_ssh_connection(host)
        if check_health(conn, port):
            return self.address
        return None
# ---
def finger(self, match):
        self._call_all('finger', match)
# ---
def main():
    """ Parse arguments and get things going for the web interface """
    parser = argparse.ArgumentParser(description=HELP_TEXT)

    parser.add_argument(
        '-p', '--port',
        help="Port to serve the interface on.",
        type=int,
        default=5050
    )

    parser.add_argument(
        '-a', '--host',
        help="Host to server the interface on.",
    )

    args = parser.parse_args()

    app.run(port=args.port, host=args.host, debug=False)
# ---
def to_unbatched_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            if selects_axis(leaf.main_axes, axis_to_strip):
                return leaf.strip_axis(axis_to_strip)
            else:
                return leaf.to_named_array()
        else:
            return leaf
# ---
def dependency_index_str(i: int) -> str:
    return f"DEP[{i}]"
# ---
def result(self, timeout: float | None = None) -> T:
        """Block until result is available.

        Args:
            timeout: Maximum time to wait in seconds

        Returns:
            The return value of the submitted callable

        Raises:
            TimeoutError: If result not available within timeout
            Exception: Any exception raised by the callable
        """
        return self._future.result(timeout=timeout)
# ---
def _build_explicit_mesh():
    devices = jax.devices()
    mesh = Mesh(np.array(devices), ("data",), axis_types=(AxisType.Explicit,))
    return mesh
# ---
def state():
    return ControllerState()
# ---
def digits(self):
        if callable(self._digits):
            with fields._get_cursor() as cr:
                return self._digits(cr)
        else:
            return self._digits
# ---
def findForce(system, forcetype, add=True):
  """ Finds a specific force in the system force list - added if not found."""
  for force in system.getForces():
    if isinstance(force, forcetype):
      return force
  if add==True:
    system.addForce(forcetype())
    return findForce(system, forcetype)
  return None
# ---
def fn(config: Config):
            time.sleep(config.wait)
            with open(config.path, "r") as f:
                number = int(f.read())
            with open(config.path, "w") as f:
                f.write(str(number + config.number))
# ---
def fake_authorize(context, target=None, action=None):
            raise exception.PolicyNotAuthorized(action='index')
# ---
def output_exemplar(self):
        return {
            "input_ids": np.zeros((0,), dtype=np.int32),
            "assistant_masks": np.zeros((0,), dtype=np.int32),
        }
# ---
def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
# ---
def _check_product_lot(self, cr, uid, ids, context=None):
        """ Checks whether move is done or not and production lot is assigned to that move.
        @return: True or False
        """
        for move in self.browse(cr, uid, ids, context=context):
            if move.prodlot_id and move.state == 'done' and (move.prodlot_id.product_id.id != move.product_id.id):
                return False
        return True
# ---
def compute(a, b):
            result = a + b
            print(f"{a} + {b} = {result}")
            return result
# ---
def _get_partner_to_invoice(self, cr, uid, picking, context=None):
        """ Gets the partner that will be invoiced
            Note that this function is inherited in the sale and purchase modules
            @param picking: object of the picking for which we are selecting the partner to invoice
            @return: object of the partner to invoice
        """
        return picking.partner_id and picking.partner_id.id
# ---
def block_code(self, code, lang=None):
        """Rendering block level code. ``pre > code``.

        :param code: text content of the code block.
        :param lang: language of the given code.
        """
        code = code.rstrip('\n')
        if not lang:
            code = escape(code, smart_amp=False)
            return '<pre><code>%s\n</code></pre>\n' % code
        code = escape(code, quote=True, smart_amp=False)
        return '<pre><code class="lang-%s">%s\n</code></pre>\n' % (lang, code)
# ---
def result(self, timeout: float | None = None) -> Any:
        """Block until result is available."""
        ...
# ---
def build_worker_image(
    tag: str,
    push: bool,
    dockerfile: str | None,
    context: str | None,
    platform: str,
    region: tuple[str, ...],
    project: str,
):
    """Build Docker image for Iris worker."""
    _build_image("worker", tag, push, dockerfile, context, platform, region, project)
# ---
def _get_host_ip() -> str:
    """Get the routable IP of this host via the default route.

    Opens a UDP socket to a public IP (no traffic sent) and reads back the
    local address the OS selected. With --network=host this returns the real
    machine IP visible to other machines in the same VPC.
    """
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(("8.8.8.8", 80))
        return s.getsockname()[0]
    finally:
        s.close()
# ---
def _write_metadata(cfg: UncheatableEvalDownloadConfig, records: list[dict[str, Any]]) -> None:
    if not records:
        return
    metadata_path = posixpath.join(str(cfg.output_path), cfg.metadata_filename)
    with fsspec.open(metadata_path, "w", encoding="utf-8") as meta_file:
        json.dump(records, meta_file, indent=2, ensure_ascii=False)
    logger.info("Wrote metadata to %s", metadata_path)
# ---
def __call__(self, view_or_request, *args, **kwargs):
        if not self.wrapped:
            self.view = view_or_request

        def dec(*args, **kwargs):
            try:
                return self.view(*args, **kwargs)
            except PermissionRequired as e:
                kwargs['_perm'] = e.perm
                kwargs['_view'] = self.view
                return self.error_view(*args, **kwargs)

        return dec(view_or_request, *args, **kwargs) if self.wrapped else dec
# ---
def __hash__(self):
        """Since subqueries are mutable, this object should not be hashable.
        However and for conveniences purposes, it can be hashed.
        """
        return reduce(mul, map(hash, self.subqueries), 1)
# ---
def unembed(self, x: NamedArray):
        return hax.dot(x, self.token_embeddings.weight, axis="embed_dim")
# ---
def cmd_modules(self, argument):
        """List active modules"""
        index = self.bot.help_index
        output = "active modules   -- %s" % " ".join(index['modules'].keys())
        self.send(self.target, output)
# ---
def bytes_free_errcheck(res, func, *args):
    notnull_errcheck(res, func, *args)
    rv = cast(res, c_void_p).value
    _mpv_free(res)
    return rv
# ---
def __lt__(self, other):
        if not hasattr(other, "handler_order"):
            # No handler_order, leave in original order.  Yuck.
            return True
        return self.handler_order < other.handler_order
# ---
def __init__(self, username: str, tpu_name: str, tpu_type: str = "v4-8"):
        """Initialize the actor."""
        self.username = username
        self.tpu_name = tpu_name
        self.tpu_type = tpu_type
# ---
def setUp(self):
        super(XenAPISRSelectionTestCase, self).setUp()
        xenapi_fake.reset()
# ---
def parse_date(date_str):
    """Parse elastic datetime string."""
    try:
        date = arrow.get(date_str)
    except TypeError:
        date = arrow.get(date_str[0])
    return date.datetime
# ---
def convert_math(self, el, text, convert_as_inline):
        try:
            x = mathml_to_markdown(el)
            return x
        except Exception as e:
            logger.exception(f"Error converting math: {e}")
            return text
# ---
def _shards_for_pspec(pspec):
        if mesh is None or pspec is None:
            return 1

        count = 1
        for axis in pspec:
            if axis is None:
                continue
            if isinstance(axis, tuple):
                for sub_axis in axis:
                    count *= _mesh_axis_size(sub_axis)
            else:
                count *= _mesh_axis_size(axis)
        return count
# ---
def __init__(self, config):
        self.config = config
# ---
def dispatch_loglikelihood(self, packed_request):
        self._send_message(_Message.LOGLIKELIHOOD)
        packed_request = self._send_payload(packed_request)
        return self.process_loglikelihood(packed_request)
# ---
def shard_names(self) -> Sequence[str]:
        raise NotImplementedError
# ---
def test_get_memory_size(self):
        self.assertEqual(self.mda.get_memory_size(), 4096)
# ---
def __init__(self, *args, **kwargs): # real signature unknown
        pass
# ---
def should_allow_eval(expr: str):
    # we don't want to try parsing unknown text or functions of more than two
    # variables
    if count_unknown_letters_in_expr(expr) > 2:
        return False

    for bad_string in BAD_SUBSTRINGS:
        if bad_string in expr:
            return False

    for bad_regex in BAD_REGEXES:
        if re.search(bad_regex, expr) is not None:
            return False

    return True
# ---
def __mul__(self, other: "PyExpr") -> "PyExpr": ...
# ---
def get_program_changes(self):
        '''Get all unique program changes used in this Track, sorted.
        '''
        post = []
        for event in self.events:
            if event.type_ == 'PROGRAM_CHANGE':
                if event.data not in post:
                    post.append(event.data)
        return post
# ---
def __repr__(self):
        return f"IdentityMap({list(repr(x) for x in self._data.values())})"
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """Peak FLOP/s for a single GPU."""
        from fray.cluster.device_flops import device_flops

        flops = device_flops(self.variant, dtype)
        if flops is None:
            raise ValueError(f"Unknown device/dtype: {self.variant}/{dtype}")
        return flops
# ---
def create_tokenization_step(dataset_identifier: str, short_name: str) -> ExecutorStep:
    dataset_config = INSTRUCTION_DATASET_NAME_TO_CONFIG[dataset_identifier]
    dataset = get_instruction_dataset(dataset_identifier, splits=dataset_config.splits)
    return default_tokenize(
        name=f"{short_name}_marin_tokenizer",
        dataset=dataset / "**/*.jsonl.gz",
        tokenizer=marin_tokenizer,
        format=ChatLmDatasetFormat(),
    )
# ---
def test_register(self, dispatcher):
        name = "some_name"

        @event.event(name)
        async def corofunc(*args):
            return True

        h_inst = event.HandlerInstance.from_handler(corofunc)
        dispatcher.register(h_inst)
        assert h_inst in dispatcher.event_map["some_name"]
# ---
def _simple_flatten(axis: AxisSpec, new_axis: AxisSelector) -> Axis:
    size = haliax.axis_size(axis)
    if isinstance(new_axis, Axis):
        if new_axis.size != size:
            raise ValueError(f"Cannot merge {axis} into {new_axis}: size mismatch")
        return new_axis

    assert isinstance(new_axis, str)
    return Axis(new_axis, size)
# ---
def check_load_config(config_class, config_file):
    try:
        draccus.parse(config_class, config_file, args=[])
    except Exception as e:
        raise Exception(f"failed to parse {config_file}") from e
# ---
def test_handles_reasonable_dtypes(PermutationClass, dtype):
    length = 31000
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    index = np.arange(length, dtype=dtype)
    result = permutation(index)
    assert isinstance(result, np.ndarray)
    assert len(result) == length
    # check it's a permutation
    sorted = np.sort(result)
    assert np.all(sorted == np.arange(length, dtype=dtype))
# ---
def output_list_item(self):
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'list_item_end':
            if self.token['type'] == 'text':
                body += self.tok_text()
            else:
                body += self.tok()

        return self.renderer.list_item(body)
# ---
def build(
        self,
        bundle_path: Path,
        dockerfile: str,
        job_id: str,
        task_logs: TaskLogs | None = None,
    ) -> BuildResult: ...
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str | None = None
    ) -> HFCheckpointConverter["MixtralConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfMixtralConfig,
        )
# ---
def get_cookies(self):
        return list(self.cookies.values())
# ---
def blacklisted(root, path, _cache={}):
    orig_path = path
    if path not in _cache:
        _cache[orig_path] = False
        while (path + os.path.sep).startswith(root):
            if os.path.exists(os.path.join(path, '.no-headers')):
                _cache[orig_path] = True
                break
            path = os.path.normpath(os.path.join(path, '..'))
    return _cache[orig_path]
# ---
def available_memory(self) -> int:
        """Available memory bytes after subtracting committed resources."""
        return self.metadata.memory_bytes - self.committed_mem
# ---
def transform_to(self, srs):
        if srs == self.srs:
            return self

        geom = transform_geometry(self.srs, srs, self.geom)
        return GeomCoverage(geom, srs)
# ---
def deposit(self, amount):
        self._balance += amount
        return self._balance
# ---
def now(cls, source: str, data: str) -> "LogLine":
        return cls(timestamp=datetime.now(timezone.utc), source=source, data=data)
# ---
def col_clause(self):
        return None, ()
# ---
def test_metric_fold():
    """fold() combines metrics correctly."""
    m1 = Metric.from_value(10.0, ReductionType.MEAN)
    m2 = Metric.from_value(20.0, ReductionType.MEAN)
    result = fold(m1, m2)

    assert result.reduction == ReductionType.MEAN
    assert jnp.allclose(result.value(), 15.0)
# ---
def _set_changed_options(self):
        changed = {}
        for key in Parameters.returnables:
            if getattr(self.want, key) is not None:
                changed[key] = getattr(self.want, key)
        if changed:
            self.changes = UsableChanges(params=changed)
# ---
def _get_pitch(self):
        if self.type_ in ['NOTE_ON', 'NOTE_OFF']:
            return self._parameter1
        else:
            return None
# ---
def test_std_functional(self, testdir, opt):
        reprec = testdir.inline_runsource("""
            def test_hello(capsys):
                print (42)
                out, err = capsys.readouterr()
                assert out.startswith("42")
        """, *opt)
        reprec.assertoutcome(passed=1)
# ---
def test_full_end_to_end_cache():
    td = tempfile.TemporaryDirectory()
    with td as tmpdir:
        cache = build_or_load_cache(
            tmpdir,
            SimpleShardSource(num_shards=15),
            TestProcessor(),
        )

        expected = simple_process(TestProcessor(), SimpleShardSource(num_shards=15))

        all_data = cache[:]

        check_datasets_equal(all_data, expected)
# ---
def wait(self, poll_interval: float = 0.1, timeout: Duration | None = None) -> None:
        """Block until the sentinel file exists."""
        deadline = Deadline.from_now(timeout) if timeout is not None else None
        while not os.path.exists(self._path):
            if deadline is not None and deadline.expired():
                raise TimeoutError(f"SentinelFile {self._path} not signalled within {timeout}")
            time.sleep(poll_interval)
# ---
def test_variable_no_encryption(self):
        """
        Test variables without encryption
        """
        Variable.set('key', 'value')
        session = settings.Session()
        test_var = session.query(Variable).filter(Variable.key == 'key').one()
        self.assertFalse(test_var.is_encrypted)
        self.assertEqual(test_var.val, 'value')
# ---
def output(self):
        return luigi.s3.S3Target('{}/{}/{}/YEAR={}/{}.psv'.format(self.root_path,
                                                                  self.etl_path,
                                                                  self.task_name,
                                                                  self.year,
                                                                  str(self.month).zfill(2)))
# ---
def to_proto(self) -> cluster_pb2.Worker.LogEntry:
        proto = cluster_pb2.Worker.LogEntry(
            source=self.source,
            data=self.data,
        )
        proto.timestamp.CopyFrom(Timestamp.from_seconds(self.timestamp.timestamp()).to_proto())
        return proto
# ---
def cleared(self) -> "TokenQueue":
        """
        Returns a new JitScheduler with all buffers cleared.
        This is useful for resetting the scheduler state.
        """
        return TokenQueue.init(
            max_queued_tokens=self.queued_tokens.axis_size("position"),
        )
# ---
def set_debuglevel(self, level):
        self.level = level
# ---
def with_output(self, x):
            out = x + self.w
            return out, 2 * self.w
# ---
def _to_torch(x):
        return torch.from_numpy(np.array(x))
# ---
def __create_context(self, main_root):
		context_dir = os.path.join(main_root, 'context')
		safe_mkdir(context_dir)

		action_default = {}
		for unit, param in self.__task.data['workflow'].items():
			action_default[unit] = self.__action_mgr.default_config(param['type'])

		navigators = self.__config_mgr.navigator()
		context_format = self.__navigator_mgr.context_format(navigators)

		self.__context_mgr.create_context_file(self.__task.data, action_default, context_format, context_dir)
# ---
def assemblyStatus(self, percent):
        self.status.assembly = True
        self.status.assembly_percent = percent
# ---
def __init__(self, tpu_type: str):
        super().__init__()
        self._tpu_type = tpu_type
        self._last_scale_multislice_time: float | None = None
        self._last_check_should_scale_up_multislice_time: float | None = None
# ---
def __init__(self, urls, text_key="sentence", audio_key="audio", sampling_rate=16000):
        super().__init__(urls)
        self.text_key = text_key
        self.audio_key = audio_key
        self.sampling_rate = sampling_rate
# ---
def click_remove_button(self):
        """
        :rtype: HomePage
        """
        self._click(BrowseMoviePageLocators.REMOVE_BUTTON_LOCATOR)
        self.alert_accept()
        from .home import HomePage
        return HomePage(self._driver)
# ---
def mean(x, axis=0):
    return _Nmean(x, axis)
# ---
def test_submit_callable_failure_raises(client: LocalClient):
    handle = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    with pytest.raises(RuntimeError, match="intentional failure"):
        handle.wait(raise_on_failure=True)
# ---
def remove(self, item):
        """Remove an element from the queue.

        Parameters
        ----------
        item :
            The element to remove.

        """
        super(PriorityQueue, self).remove(item)
        heapq.heapify(self._queue)
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"blocks": "h"}
# ---
def test_add_permission(self):
        """
        Tests that staff cannot add entries
        """
        self.assertFalse(self.creator_admin.has_add_permission(self.request))
# ---
def shutdown(self, wait: bool = True) -> None:
        self._iris.shutdown(wait=wait)
# ---
def _get_width(Width: int | Axis) -> Axis:
    if isinstance(Width, int):
        return Axis(DEFAULT_WIDTH_NAME, Width)
    else:
        return Width
# ---
def TextBeforeCursor():
  """Returns the text before CurrentColumn."""
  return ToUnicode( vim.current.line[ :CurrentColumn() ] )
# ---
def _get_mesh() -> Mesh | None:
    # Backward-compatible helper for callers that expect a concrete or abstract mesh.
    return _resolve_mesh()
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("x", Integer),
            Column("y", Integer),
            Column("q", String(50)),
            Column("p", String(50)),
        )
# ---
def to_proto(self) -> cluster_pb2.TaskStatus:
        """Convert to protobuf TaskStatus message."""
        ...
# ---
def crispr_deletion(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'deletion',
        'purpose': 'repression',
        'method': 'CRISPR'
    }
# ---
def test_text(self):
        f = capture.CaptureIO()
        f.write("hello")
        s = f.getvalue()
        assert s == "hello"
        f.close()
# ---
def _scale_weights(weights: dict[str, float], fraction: float) -> dict[str, float]:
    """Normalize `weights` and rescale them so they sum to `fraction`."""
    total_weight = sum(weights.values())
    if total_weight <= 0:
        raise ValueError("Mixture weights must sum to a positive value.")
    return {name: (value * fraction) / total_weight for name, value in weights.items()}
# ---
def attrs(self, value):
        self._attrs = OrderedDict(value)
# ---
def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig, _encoding)
# ---
def mark_old(node):
    if node.parent and node.parent.label == "Deprecated node!":
        return
    ng = node.id_data
    frame = ng.nodes.new("NodeFrame")
    if node.parent:
        frame.parent = node.parent
    node.parent = frame
    frame.label = "Deprecated node!"
    frame.use_custom_color = True
    frame.color = (.8, 0, 0)
    frame.shrink = True
# ---
def __init__(self):
        self.transport = None
        self.connected = threading.Event()
        self.disconnected = threading.Event()
        self.port = None
# ---
def loss_ref(v):
        return jnp.sum(reference_impl_batched(v[None, :])[0])
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        """Returns a dict mapping eqx.Module keys to torch keys that need to be renamed for serialization"""
        return {}
# ---
def _fix_sqrt(string):
    """Convert \\sqrt{x} or \\sqrt x to \\sqrt{x}. (Legacy - use process_latex_sqrt)"""
    # Use new abstraction for converting to text format
    return process_latex_sqrt(string)
# ---
def timing(self, stats, time, sample_rate=1):
        """
        Log timing information
        """
        unit = 'ms'
        log.debug('%r took %s %s' % (stats, time, unit))
        self.update_stats(stats, "%s|%s" % (time, unit), sample_rate)
# ---
def test_shard_scalar_in_module():
    with axis_mapping(resource_map):

        class MyModule(eqx.Module):
            scalar: jnp.ndarray

            def __init__(self):
                self.scalar = jnp.zeros(
                    (),
                )

        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModule)()
            assert mod.scalar.sharding.is_fully_replicated
# ---
def summarize_document(doc: dict) -> DocumentSummary:
    """Validate that a document dict is in valid Dolma-format, and return summary (e.g., footprint in bytes, etc.)."""
    validated_doc = DolmaDocument.model_validate(doc)
    json_blob = json.dumps(doc)
    return DocumentSummary(document_bytes=get_size_bytes(json_blob), text_bytes=get_size_bytes(validated_doc.text))
# ---
def loss_fn(model, data):
            m = jax.vmap(model)
            return jnp.mean(jnp.square(m(data)))
# ---
def __call__(self, q: NamedArray, position_ids: NamedArray) -> NamedArray:
        raise NotImplementedError("This is an abstract base class for RotaryEmbeddings. Use a subclass instead.")
# ---
def signal(self) -> None:
        dirname = os.path.dirname(self._path)
        if dirname:
            os.makedirs(dirname, exist_ok=True)
        with open(self._path, "w"):
            pass
# ---
def test_dtype_category_annotation_and_check():
    def baz(x: Float["b"]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(baz, include_extras=True)["x"])[1]
    assert str(spec.dtype) == "float"

    B = Axis("b", 1)
    arr = NamedArray(jnp.ones((B.size,), dtype=jnp.float32), (B,))
    assert arr.matches_axes(Float["b"])  # type: ignore
    assert not arr.matches_axes(Int["b"])
# ---
def run(argv):
    try:
        return subprocess.check_output(argv, stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        print(e.output.decode(), file=sys.stderr)
        raise
# ---
def test_stage_name_truncation():
    """PhysicalStage.stage_name() truncates long names."""
    stage = PhysicalStage(operations=[Map(fn=lambda x: x) for _ in range(20)])
    name = stage.stage_name(max_length=20)
    assert len(name) <= 20
    assert name.endswith("...")
# ---
def test_is_stop_signal_multiple_stop_sequences_one_matches():
    # Multiple stop_sequences, only one matches
    tail_tokens = hax.named(jnp.array([8, 9, 10], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(
        jnp.array([[1, 2, 3], [8, 9, 10], [4, 5, 6]], dtype=jnp.int32), axis=("seq", "position")
    )
    assert is_stop_signal(tail_tokens, stop_sequences)
# ---
def ha_interface_ip_address_needed(self, context, router, port,
                                       ha_settings_db, ha_group_uuid):
        if port['device_owner'] == bc.constants.DEVICE_OWNER_ROUTER_GW:
            return False
        else:
            return True
# ---
def get_queryset(self):
        return CommandExecution.objects.filter(
            user_id=str(self.request.user.id)
        )
# ---
def selected_pane_index(self, value):
        if value >= len(self.panes):
            return
        self._selected_pane_index = value
        self._refresh_target_selection()
        self._update_selected_pane()
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        pass
# ---
def delete_secret(self, key):
        ''' delete secret'''
        try:
            del self.secrets[key]
        except KeyError as _:
            return False

        return True
# ---
def _create_lm_example(tokens):
                tokens = hax.named(tokens, self.Pos)
                example = LmExample.causal(
                    tokens=tokens,
                    eos_id=eos_id,
                    block_cross_document_attention=block_cross_document_attention,
                )
                example = jax.lax.with_sharding_constraint(example, sharding)
                return example
# ---
def kill(self, job_or_id) -> None:
        job_id = self._to_job_id_str(job_or_id)
        request = cluster_pb2.Controller.TerminateJobRequest(job_id=job_id)
        assert self._controller_client is not None
        self._controller_client.terminate_job(request)
# ---
def test_run_blocking_with_error():
    with TemporaryVenv() as venv:
        with pytest.raises(subprocess.CalledProcessError):
            venv.run([venv.python_path, "-c", "import sys; sys.exit(1)"])
# ---
def test_pad():
    Height = Axis("Height", 3)
    Width = Axis("Width", 2)

    arr = hax.arange((Height, Width))
    padded = hax.pad(arr, {Height: (1, 2), Width: (0, 1)}, mode="constant", constant_values=0)

    expected = jnp.pad(arr.array, [(1, 2), (0, 1)], mode="constant", constant_values=0)
    assert padded.axes[0].size == Height.size + 3
    assert padded.axes[1].size == Width.size + 1
    assert jnp.all(expected == padded.array)
# ---
def match(self, item):
        if self.field not in item:
            return False
        value = item[self.field]
        if isinstance(value, str):
            value = self._convert(value)

        if self.point is not None:
            return value == self.point
        else:
            if self.rangemin is not None and value < self.rangemin:
                return False
            if self.rangemax is not None and value > self.rangemax:
                return False
            return True
# ---
def get_host_name(provider):
    cfme_host = random.choice(provider.data["hosts"])
    return cfme_host.name
# ---
def stateful_ops(self):
    """Returns the list of stateful ops in function definition.

    Returns:
      A list of (op.name, op.type) pairs.
    """
    self._create_definition_if_needed()
    return self._stateful_ops
# ---
def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
# ---
def _match_greater_than(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) > str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def test_get_typed_cols(self):
        print(get_typed_cols(go('Integer')))
        print(get_typed_cols(go('String')))
        print(get_typed_cols(go('Point')))
        print(get_typed_cols(go('Role')))
        print(get_typed_cols(go('RoleIO')))
        print(get_typed_cols(go('Log')))
        print(get_typed_cols(go('Meta')))
# ---
def _is_special_module(module):
        # TODO: add conv?
        return isinstance(module, hnn.Linear) or isinstance(module, hnn.Stacked)
# ---
def ping(self):
        return True
# ---
def _check_worker_healthy(self) -> bool:
        """Check if worker container is already running and healthy."""
        port = self._bootstrap_config.worker_port or 10001
        try:
            result = self._conn.run(f"curl -sf http://localhost:{port}/health", timeout=Duration.from_seconds(10))
            return result.returncode == 0
        except Exception:
            return False
# ---
def __and__(self, other: object) -> LogicalExpr:
        return LogicalExpr(self, _to_expr(other), "and")
# ---
def parent(self):
        return object.__getattribute__(self, '_parent')
# ---
def test_many(self, capfd):
        with lsof_check():
            for i in range(10):
                cap = StdCaptureFD()
                cap.stop_capturing()
# ---
def add(self, func):
        self[func.__name__] = func
        return func
# ---
def test_metric_float_conversion():
    """Metrics support float() conversion."""
    m = Metric.from_value(42.0, ReductionType.MEAN)
    assert float(m) == 42.0
# ---
def GetVimGlobalsKeys():
  return vim.eval( 'keys( g: )' )
# ---
def round_flops_to_bucket(flops: float, base: float = 1.1) -> float:
    """Round FLOP count to the nearest power of base.

    Args:
        flops: FLOP count to round.
        base: Base for the power buckets (default 1.1 for ~10% buckets).
    """
    if flops <= 0:
        return flops

    k = math.log(flops) / math.log(base)
    return base ** round(k)
# ---
def hasattr(self, attr):
        if attr in self.lookup:
            return True
        return False
# ---
def test_str_flatten(self):
        def test_impl(df):
            A = df.A.str.split(',')
            return pd.Series(list(itertools.chain(*A)))

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def test_lon_lat_output(self):
        """Asserts that the vertices in the lat-lon output are in the
        right order (lat before long)."""
        for vertex in self.polycircle.to_lon_lat():
            assert_almost_equal(vertex[0], self.longitude, places=2)
            assert_almost_equal(vertex[1], self.latitude, places=2)
# ---
def test_ckpt_path_with_valid_string_path():
    path = "checkpoints/llama-8b-tootsie-phase2/checkpoints/step-730000"
    assert ckpt_path_to_step_name(path) == "llama-8b-tootsie-phase2-730000"
# ---
def Embed(self) -> Axis:
        return Axis("embed", self.hidden_dim)
# ---
from typing import List


def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:
    """ Filter an input list of strings only for ones that start with a given prefix.
    >>> filter_by_prefix([], 'a')
    []
    >>> filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a')
    ['abc', 'array']
    """
    return [x for x in strings if x.startswith(prefix)]
# ---
def _check_minions_directories(self):
        '''
        Return the minion keys directory paths
        '''
        accepted = os.path.join(self.opts['pki_dir'], self.ACC)
        pre = os.path.join(self.opts['pki_dir'], self.PEND)
        rejected = os.path.join(self.opts['pki_dir'], self.REJ)
        return accepted, pre, rejected
# ---
def __gt__(self, other: "Duration") -> bool:
        return self._ms > other._ms
# ---
def fake_volume_type_get(context, id, inactive=False, expected_fields=None):
    vol = VOLUME_TYPES[id]
    if expected_fields and 'projects' in expected_fields:
        vol['projects'] = [a['project_id']
                           for a in ACCESS_LIST if a['volume_type_id'] == id]
    return vol
# ---
def add_cluster_ip(self, sip):
        '''add cluster ip'''
        self.put(Service.cluster_ip, sip)
# ---
def tiny_corpus_config(path):
    _write_tiny_corpus(path)
    component = DatasetComponent(
        source=UrlDatasetSourceConfig(
            train_urls=[f"file://{path}/train/docs.jsonl"],
            validation_urls=[f"file://{path}/validation/docs.jsonl"],
        ),
        cache_dir=f"{path}/cache",
    )
    return LmDataConfig(components={"tiny": component})
# ---
def __init__(self, job: IrisJob):
        self._job = job
# ---
def main(ctx: click.Context, cluster: str):
    """Fray cluster job management."""
    ctx.ensure_object(dict)
    cluster_obj = create_cluster(cluster)
    ctx.with_resource(cluster_obj.connect())
    ctx.obj["cluster"] = cluster_obj
# ---
def _leaf_conforms(shape_spec: Union[ShapeSpec, NamedShapeSpec], leaf):
        if isinstance(shape_spec, ShapeSpec):  # type: ignore
            return shape_spec.shape == leaf.shape and shape_spec.dtype == leaf.dtype
        else:
            return (shape_spec.shape is None or shape_spec.shape == leaf.axes) and (
                shape_spec.dtype is None or shape_spec.dtype == leaf.dtype
            )
# ---
def embed(self, input_ids, *, key, pos_ids: NamedArray):
        input_embeds = self.token_embeddings(input_ids)
        position_embeds = self.position_embeddings.embed(pos_ids)
        x = input_embeds + position_embeds
        x = self.dropout(x, key=key)

        return x
# ---
def escape(self, text):
        """Rendering escape sequence.

        :param text: text content.
        """
        return escape(text)
# ---
def test_raw_metric_host_memory(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'derived_memory_used',
        host_name)

    for record in query:
        if record.derived_memory_used is not None:
            assert record.derived_memory_used > 0, 'Zero Host Memory Usage'
            break
# ---
def pop(self, lease_timeout: float = 60.0) -> Lease[T_co] | None:
        """Acquire a lease on the next available item."""
        ...
# ---
def test_dense_dtype(self):
    inputs = ops.convert_to_tensor(
        np.random.randint(low=0, high=7, size=(2, 2)))
    layer = keras.layers.Dense(5, dtype='float32')
    outputs = layer(inputs)
    self.assertEqual(outputs.dtype, 'float32')
# ---
def job_logs(ctx, job_id, follow, tail, grep):
    """View logs for a Ray job."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        if follow:
            subprocess.run(["ray", "job", "logs", "--follow", job_id], check=False)
            return
        _print_job_logs(job_id, tail=tail, grep=grep)
# ---
def __init__(self, *args):
    self.custom_objects = args
    self.backup = None
# ---
def __repr__(self):
        """ Helper method for debugging """

        _str = "<S3ImportItem %s {item_id=%s uid=%s id=%s error=%s data=%s}>" % \
               (self.table, self.item_id, self.uid, self.id, self.error, self.data)
        return _str
# ---
def pop(self):
        if not self.tokens:
            return None
        self.token = self.tokens.pop()
        return self.token
# ---


def unique(l: list):
    """Return sorted unique elements in a list
    >>> unique([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [0, 2, 3, 5, 9, 123]
    """
    return sorted(list(set(l)))
# ---
def test_uniform_with_bounds_broadcast_and_scalar():
    key = jax.random.PRNGKey(0)
    lb = hax.full(Height, -3.0)
    ub = 0.5
    u = hax.random.uniform(key, shape=(Height, Width), minval=lb, maxval=ub)

    assert u.axes == (Height, Width)

    assert hax.all(u >= -3.0)
    assert hax.all(u <= 0.5)
# ---
def get_caller_path() -> str:
    """Return the path of the file that called this function."""
    return inspect.stack()[-1].filename
# ---
def progressBarUpdateIntervalValueChanged(self, widget):
        """Signal handler for the "value_changed" signal for the
           progressBarUpdateIntervalSpinButton GtkSpinButton widget.

        Arguments:
        - widget: the component that generated the signal.
        """

        self.prefsDict["progressBarUpdateInterval"] = widget.get_value_as_int()
# ---
def graphviz_setup(gviz_path):
    os.environ['PATH'] = gviz_path + ";" + os.environ['PATH']
# ---
def scale_group(self) -> str:
        return self._scale_group
# ---
def _strip_sizes(axes: AxisSpec) -> AxisSelection:
    """Strip sizes from axes, returning only the names."""
    if isinstance(axes, hax.Axis):
        return axes.name
    return tuple(axis.name for axis in axes)
# ---
def action_from_output(cls, output_array):
        return np.argmax(output_array)
# ---
def status(self) -> cluster_pb2.TaskStatus:
        """Get current task status.

        Returns:
            TaskStatus proto containing state, worker assignment, and metrics
        """
        return self._client._cluster_client.get_task_status(self.task_id)
# ---
def print_ls_l_desc(desc, **kwargs):
    print(get_ls_l_desc(desc, **kwargs))
# ---
def hard_wrap(self):
        """Grammar for hard wrap linebreak. You don't need to add two
        spaces at the end of a line.
        """
        self.linebreak = re.compile(r'^ *\n(?!\s*$)')
        self.text = re.compile(
            r'^[\s\S]+?(?=[\\<!\[_*`~]|https?://| *\n|$)'
        )
# ---
def from_binary(command: str, args: Sequence[str]) -> Self:
        return Entrypoint(binary_entrypoint=BinaryEntrypoint(command=command, args=args))
# ---
def _check_deadline(self) -> bool:
        """Returns True if the global deadline has passed. Sets interrupted flag."""
        if self._deadline is not None and time.monotonic() > self._deadline:
            self.logger.log(
                f"Global timeout ({self.config.timeout_seconds}s) exceeded!",
                level="ERROR",
            )
            self._interrupted = True
            return True
        return False
# ---
def vocab_size(self) -> int:
        return self.Vocab.size
# ---
def _raw_indices_for_at(array, indexes):
    sliced_axes, ordered_slices = _compute_new_axes_and_slices_for_index(array, indexes)
    _sliced = index(array, indexes)
    ordered_slices = [s.array if isinstance(s, NamedArray) else s for s in ordered_slices]
    return ordered_slices, _sliced.axes
# ---
def paragraph(self, text):
        """Rendering paragraph tags. Like ``<p>``."""
        return '<p>%s</p>\n' % text.strip(' ')
# ---
def receive_serialdata(self, time, data):
        self.loggingWidget.log_input(data)

        try:
            self.rootnode.from_json(data)
        except ValueError as e:
            logger.error(str(e))

        # refresh widgets
        self.objectexplorer.refresh()
        self.plot.refresh(time)
        if self.recording_enabled:
            self.recordWidget.add_data(time, self.rootnode)
# ---
def __enter__(self) -> "SerialCacheWriter":
        return self
# ---
def skip_if_not_enough_devices(count: int, reason: str | None = None):
    msg = f"Not enough devices ({len(jax.devices())})"
    if reason:
        msg += f": {reason}"
    return pytest.mark.skipif(len(jax.devices()) < count, reason=msg)
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)
        return ScaleByMuonState(momentum_buffer=momentum_buffer)
# ---
def eval_model_multithread(cfg, nr_eval, get_player_fn):
    func = OfflinePredictor(cfg)
    NR_PROC = min(multiprocessing.cpu_count() // 2, 8)
    mean, max = eval_with_funcs([func] * NR_PROC, nr_eval, get_player_fn)
    logger.info("Average Score: {}; Max Score: {}".format(mean, max))
# ---
def sort(self, items):
        slow_sorts = []
        switch_slow = False
        for sort in reversed(self.sorts):
            if switch_slow:
                slow_sorts.append(sort)
            elif sort.order_clause() is None:
                switch_slow = True
                slow_sorts.append(sort)
            else:
                pass

        for sort in slow_sorts:
            items = sort.sort(items)
        return items
# ---
def init(Order: Axis, w: float = 10, *, key=None):
        return Sin(w * hax.ones((Order,)))
# ---
def test_reentrancy(max_capacity):
    test_data = list(range(1, 101))
    background_iterable = BackgroundIterable(lambda: iter(test_data), max_capacity=max_capacity)

    iter1 = iter(background_iterable)
    iter2 = iter(background_iterable)

    data1 = list(iter1)
    data2 = list(iter2)

    assert data1 == data2
    assert data1 == test_data
# ---
def test_type(self):
        rset = event.ResultSet()
        with pytest.raises(NotImplementedError):
            rset.extend([])
        with pytest.raises(NotImplementedError):
            rset.extend(False)
# ---
def do_shutdown():
        logger.info("Shutting down ray...")
        ray.shutdown()
# ---
def max_gen_toks(self) -> int:
        """Backward compatibility property for max_gen_toks."""
        return self.generation_kwargs.get("max_gen_toks", 256)
# ---
def __str__(self):
        return self.name
# ---
def test_endswith_unescaped(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e%fg"), {1, 2, 3, 4, 5, 6, 7, 8, 9})
# ---
def min_slices(self) -> int:
        """Minimum number of VM groups to maintain."""
        return self._config.min_slices
# ---
def test_on_end_date(self):
        """Test when create_time is exactly on the end date"""
        self.assertTrue(check_create_time("2023-01-31 23:59:59 PST", "2023-01-01", "2023-01-31"))
# ---
def test_opposites_task_reward():
    task = OppositesTask()
    examples = task.generate_examples(10, np.random.default_rng(42))
    assert len(examples) == 10

    assert task.compute_reward("cold", "cold") == pytest.approx(1.0)
    assert task.compute_reward("cold", "warm") == pytest.approx(0.0)
# ---
def init(cls, Vocab: Axis, config: LmConfigT, *, key: PRNGKeyArray) -> "LmHeadModel[LmConfigT]":
        pass
# ---
def add_sort(self, sort):
        self.sorts.append(sort)
# ---
def __init__(self, urls, text_key="text"):
        self.text_key = text_key
        self.base_ds = UrlDataSource(urls, columns=[text_key])
# ---
def compute_and_viz_log_probs(step: StepInfo):
        model = step.eval_model
        os.makedirs(html_dir, exist_ok=True)
        path = os.path.join(html_dir, f"step_{step.step}.html")

        compute_and_visualize_log_probs(path, model, tokenizer, log_prob_fn, test_data, max_docs=max_docs)
        # TODO: convert to generic logging
        import wandb

        wandb.log({"log_probs": wandb.Html(path)}, step=step.step)
# ---
def stop(self) -> None:
        """No-op: TpuVmManager has no background threads to stop."""
        pass
# ---
def get_stub(self, address: str) -> WorkerClient:
        return WorkerServiceClientSync(
            address=f"http://{address}",
            timeout_ms=10000,
        )
# ---
def set_address(self, host, port=8125):
        try:
            self.addr = (socket.gethostbyname(host), port)
        except socket.gaierror:
            self.addr = None
            self.enabled = False
# ---

def multiply(a, b):
    """Complete the function that takes two integers and returns
    the product of their unit digits.
    Assume the input is always valid.
    Examples:
    multiply(148, 412) should return 16.
    multiply(19, 28) should return 72.
    multiply(2020, 1851) should return 0.
    multiply(14,-15) should return 20.
    """
    return abs(a % 10) * abs(b % 10)
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step, commit: Optional[bool] = None):
        pass
# ---
def logsumexp(a: A, axis: AxisSelection | None = None) -> A:
    # TODO: logsumexp indirectly supports where via `b`. we should support it directly
    return wrap_reduction_call(jnn.logsumexp, a, axis=axis, single_axis_only=False, supports_where=False)
# ---
def __len__(self):
        if self.tree is None:
            return 0
        else:
            return len(jax.tree.leaves(self.tree)[0])
# ---
def needs_update_replicas(self, replicas):
        ''' verify whether a replica update is needed '''
        current_reps = self.get(DeploymentConfig.replicas_path)
        return not current_reps == replicas
# ---
def is_active(self):
        """Returns `True` if dropout is active (and therefore needs a key), `False` otherwise."""
        return not self.inference and self.pdrop > 0
# ---
def get_location(self):
        return self.location
# ---
def __init__(self, bot, settings):
		self.bot = bot
		self.settings = settings
# ---
def loss_fn(x_arr):
        x = hax.named(x_arr, x0.axes)
        y, _ = layer(x, inference=False, chunk_size=8)
        return jnp.sum(y.array)
# ---
def rel(self, f):
        return self._matcher.rel(self._path + "/" + f)
# ---
def test_mup_embedding_init_matches_embedding(init_scale: float):
    Vocab = hax.Axis("V", 8)
    Embed = (hax.Axis("E", 4),)

    key = jrandom.PRNGKey(0)

    baseline = Embedding.init(Vocab, Embed, key=key, init_scale=init_scale)
    mup = Embedding.init(Vocab, Embed, key=key, init_scale=init_scale, reparam_cls=EmbeddingMup)

    assert mup.weight.axes == baseline.weight.axes
    scale_factor = hax.axis_size(Embed)
    assert jnp.allclose(mup.weight.array, baseline.weight.array * scale_factor)
# ---
def test_xxh3_64_vector():
    # Catch un-intentional regressions
    assert hash_xxh3_64(b"hello") == 10760762337991515389
# ---
def get_autoscaler_status(self, request: cluster__pb2.Controller.GetAutoscalerStatusRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetAutoscalerStatusResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def test_composed_int_desc(self):
        table = self.tables.some_table
        lx = (table.c.x + table.c.y).label("lx")
        self._assert_result(
            select([lx]).order_by(lx.desc()), [(7,), (5,), (3,)]
        )
# ---
def test_dense_with_policy(self):
    inputs = ops.convert_to_tensor(
        np.random.randint(low=0, high=7, size=(2, 2)), dtype='float16')
    layer = keras.layers.Dense(5, dtype=policy.Policy('infer_float32_vars'))
    outputs = layer(inputs)
    self.assertEqual(outputs.dtype, 'float16')
    self.assertEqual(layer.kernel.dtype, 'float32')
# ---
def empty_queue_space(self) -> jnp.ndarray:
        """Expose remaining queue capacity from ``TokenQueue``."""
        return self.tqueue.empty_queue_space
# ---
def __hash__(self):
        # Hash based on the initial configuration values
        return hash(self.build_name())
# ---
def from_hf_config(rope_theta, config: dict | None) -> "RotaryEmbeddingsConfig":
        if config is None:
            return DefaultRotaryEmbeddingsConfig(theta=rope_theta)
        tpe = config.get("rope_type") or config.get("type") or "default"
        return RotaryEmbeddingsConfig.get_choice_class(tpe).make_from_hf_config(rope_theta, config)
# ---
def test_load_vortex_basic(self, vortex_file):
        """Test basic vortex file reading."""
        records = list(load_vortex(str(vortex_file)))
        assert len(records) == 100
        assert records[0]["id"] == 0
        assert records[0]["name"] == "item_0"
        assert records[0]["score"] == 0
# ---
def set_exception(self, exc=True):
        self.exception = exc
# ---
def pspec_for_axis(axis: AxisSelection, mapping: ResourceMapping | None = None) -> PartitionSpec:
    """Get the PartitionSpec for a single axis"""
    axis = axis_spec_to_shape_dict(axis)
    return PartitionSpec(*(physical_axis_name(a, mapping) for a in axis))
# ---
def split_sizes(self):
        return self._split_sizes
# ---
def mask_fn(param, path):
            path_str = ".".join(path) if isinstance(path, (list, tuple)) else str(path)
            if "Embedding" in path_str:
                return "adam"
            elif isinstance(param, Linear):
                return dataclasses.replace(param, weight="adamh", bias="adam" if param.bias is not None else None)
            else:
                return "adam"
# ---
def done(self, lease: Lease[T]) -> None:
        try:
            self.fs.rm(self.processing_dir / lease.lease_id)
        except FileNotFoundError:
            raise ValueError(f"Invalid lease: {lease.lease_id} not found (already done or expired)") from None
# ---
def with_config_overrides(self, config_overrides: dict, merge: bool = True) -> "HFCheckpointConverter":
        if self.config_overrides is not None and merge:
            config_overrides = mergedeep.merge({}, self.config_overrides, config_overrides)
        return dataclasses.replace(self, config_overrides=config_overrides)
# ---
def compare_to(self, other: "CacheMetadata") -> deepdiff.DeepDiff:
        if other.preprocessor_metadata is None:
            sorta_self = dataclasses.replace(self, preprocessor_metadata=None)
        else:
            sorta_self = self
        return deepdiff.DeepDiff(sorta_self, other)
# ---
def _check_for_unused_aliases(axis_aliases, used_aliases, equation):
    if any(alias not in used_aliases for alias in axis_aliases):
        unused_aliases_str = ", ".join([alias for alias in axis_aliases if alias not in used_aliases])
        raise_parse_error(f"Unused aliases from kwargs: {unused_aliases_str}", equation, None)
# ---
def __init__(self, config_node, private=False):
        assert isinstance(config_node, EnkfConfigNode)

        if private:
            c_pointer = EnkfNode.cNamespace().alloc_private(config_node)
        else:
            c_pointer = EnkfNode.cNamespace().alloc(config_node)

        super(EnkfNode, self).__init__(c_pointer, config_node, True)
# ---
def test_delete_nonexistent_track(self):
        """ Tests that delete_track method succeeds, by design, when deleting a nonexistent track"""
        experiment = self.create_test_experiment()
        self.assertEqual(experiment.tracks.count(), 0)
        response = self.client.post(reverse("ab_testing_tool_delete_track", args=(NONEXISTENT_TRACK_ID,)),
                                    follow=True)
        self.assertEqual(experiment.tracks.count(), 0)
        self.assertOkay(response)
# ---
def test_fray_job_ctx_invalid():
    with pytest.raises(ValueError, match="Unknown context type"):
        create_job_ctx("invalid")
# ---
def flow_backend_ctx():
    """Set up sync backend for all transform tests."""
    with fray_default_job_ctx(create_job_ctx("sync")):
        yield
# ---
def vm_groups(self) -> list[VmGroupProtocol]:
        """All VM groups in this scale group."""
        with self._vm_groups_lock:
            return list(self._vm_groups.values())
# ---
def empty_autoscaler(scale_group_config):
    """Empty autoscaler ready for scale-up tests."""
    manager = make_mock_vm_manager()
    group = ScalingGroup(scale_group_config, manager, scale_up_cooldown=Duration.from_ms(0))
    autoscaler = make_autoscaler({"test-group": group})
    yield autoscaler
    autoscaler.shutdown()
# ---
def convert_all_json_to_yaml(self, dirpath):
        for path in os.listdir(dirpath):
            if not path.endswith('.template') and not path.endswith('.json'):
                continue
            f = open(os.path.join(dirpath, path), 'r')
            json_str = f.read()

            yml_str = template_format.convert_json_to_yaml(json_str)
            yield (json_str, yml_str, f.name)
# ---
def bank(toy_corpus):
    return SubtreeBank.from_corpus(toy_corpus)
# ---
def test_column_definition(self):
        s = option.SqlStore()
        print(s.column_definition(go('Integer'))[1])
        print(s.column_definition(go('String'))[1])
        print(s.column_definition(go('Point'))[1])
        print(s.column_definition(go('Role'))[1])
        print(s.column_definition(go('RoleIO'))[1])
        print(s.column_definition(go('Log'))[1])
        print(s.column_definition(go('Meta'))[1])
# ---
def inner(args, kwargs):
            return f(*args, **kwargs)
# ---
def shutdown(self) -> None:
        """Terminate all actor jobs."""
        ...
# ---
def before_insert(self):
		if not self.description:
			self.description = self.item_name
# ---
def __init__(self, artist, title, year=None):
        super(Painting, self).__init__(artist, title, year)
# ---
def info(self):
        return self.headers
# ---
def case(context):
        """Add to the context."""
        assert context == {"squee": "kapow"}
        context.boing = "thunk"
# ---
def get_metrics(self) -> dict:
        pass
# ---
def read_files(self, tfile, members):
        '''
        array with txt data from tarfile object
        '''
        self.data = [tfile.extractfile(member).read() for member in members if
                     tfile.extractfile(member) is not None]
# ---
def downgrade(engine_name):
    globals()["downgrade_%s" % engine_name]()
# ---
def load_lua():
    """ Use this function if you intend to use mpv's built-in lua interpreter. This is e.g. needed for playback of
    youtube urls. """
    CDLL('liblua.so', mode=RTLD_GLOBAL)
# ---
def stop(self) -> None:
        """Stop the controller and clean up resources.

        Shutdown ordering:
        1. Stop the controller (which stops its threads and autoscaler)
        2. Wait on the root ThreadContainer to verify all threads have exited
        """
        if self._controller:
            self._controller.stop()
            self._controller = None
            logger.info("Controller stopped")

        self._threads.wait()
# ---
def declared_input_types(self):
    """Returns the list of data types of explicit declared inputs."""
    return self._input_types
# ---
def get_fn_name(fn: ExecutorFunction, short: bool = False):
    """Just for debugging: get the name of the function."""
    if fn is None:
        return "None"
    import ray

    if isinstance(fn, ray.remote_function.RemoteFunction):
        return fn._function.__name__
    if short:
        return f"{fn.__name__}"
    else:
        return str(fn)
# ---
def scan(
    f: Callable[[Carry, X], tuple[Carry, Y]],
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable[[Carry, PyTree[X]], tuple[Carry, PyTree[Y]]]: ...
# ---
def terminate_job(
        self,
        job_id: str,
    ) -> cluster_pb2.Empty:
        """Terminate a running job."""
        request = cluster_pb2.Controller.TerminateJobRequest(job_id=job_id)
        return self._service.terminate_job(request, None)
# ---
def _job_status_to_result(name: str, status: cluster_pb2.JobStatus, duration: float) -> ValidationResult:
    if status.state == cluster_pb2.JOB_STATE_SUCCEEDED:
        return ValidationResult(name, True, f"Job completed in {duration:.1f}s", duration)
    state_name = cluster_pb2.JobState.Name(status.state)
    return ValidationResult(name, False, f"Job ended with state {state_name}: {status.error}", duration)
# ---

def can_arrange(arr):
    """Create a function which returns the largest index of an element which
    is not greater than or equal to the element immediately preceding it. If
    no such element exists then return -1. The given array will not contain
    duplicate values.

    Examples:
    can_arrange([1,2,4,3,5]) = 3
    can_arrange([1,2,3]) = -1
    """
    ind=-1
    i=1
    while i<len(arr):
      if arr[i]<arr[i-1]:
        ind=i
      i+=1
    return ind
# ---
def _workflow_signal(self, cr, uid, ids, signal, context=None):
        #override in order to fire the workflow signal on given stock.picking workflow instance
        #instead of it's own workflow (which is not existing)
        return self.pool.get('stock.picking')._workflow_signal(cr, uid, ids, signal, context=context)
# ---
def intersect_axes(ax1: tuple[AxisSelector, ...], ax2: AxisSpec) -> tuple[Axis, ...]: ...
# ---
def accelerator_type_from_extra(extra: list[str] | None = None) -> AcceleratorType:
    if not extra:
        return AcceleratorType.NONE

    extra_set = set(extra)
    if "tpu" in extra_set:
        return AcceleratorType.TPU
    elif "gpu" in extra_set or "vllm" in extra_set:
        return AcceleratorType.GPU
    elif "cpu" in extra_set:
        return AcceleratorType.CPU
    else:
        return AcceleratorType.NONE
# ---
def write(self, s):
            pass
# ---
def make_fake_popen(lines: list[str] | None = None):
    """Create a mock Popen-like object for streaming tests."""
    if lines is None:
        lines = ["[iris-init] Bootstrap starting", "[iris-init] Bootstrap complete"]
    mock = MagicMock()
    mock.stdout = iter(line + "\n" for line in lines)
    mock.returncode = 0
    mock.wait.return_value = 0
    mock.args = []
    return mock
# ---
def venv_path(self) -> str:
        """Path to the virtual environment directory (.venv/ if workspace, else temp root)."""
        if not self._temp_dir:
            raise RuntimeError("TemporaryVenv must be entered before accessing venv_path")
        if self._workspace:
            return os.path.join(self.workspace_path, ".venv")
        else:
            return self.workspace_path
# ---
def set_git_ssh(ssh_wrapper, key_file, ssh_opts):

    if os.environ.get("GIT_SSH"):
        del os.environ["GIT_SSH"]
    os.environ["GIT_SSH"] = ssh_wrapper

    if os.environ.get("GIT_KEY"):
        del os.environ["GIT_KEY"]

    if key_file:
        os.environ["GIT_KEY"] = key_file

    if os.environ.get("GIT_SSH_OPTS"):
        del os.environ["GIT_SSH_OPTS"]

    if ssh_opts:
        os.environ["GIT_SSH_OPTS"] = ssh_opts
# ---
def _get_value(self, name):
		if name not in self._values:
			raw_value = self[self.structure.index(name)]

			self._set_value(name, raw_value)

		return self._values[name]
# ---
def radians(a: A) -> A:
    return wrap_elemwise_unary(jnp.radians, a)
# ---
def modify(id, radio, url, genre) :
    db = cherrypy.session['database']

    sql = "UPDATE Radio SET radio='%s', url='%s', genre='%s', exist=1 WHERE id = %s" % (radio, url, genre, id)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def records():
        batch = []
        pbar = tqdm_logging(desc=f"Shard {shard_name}")
        for example in source.open_shard_at_row(shard_name, 0):
            batch.append(example)
            if len(batch) >= options.batch_size:
                processed = processor(batch)
                yield from _canonicalize_batch(processed)
                batch.clear()
            pbar.update(1)
        if batch:
            processed = processor(batch)
            yield from _canonicalize_batch(processed)
# ---
def today(*args):
        """ Return the current day in the format expected by the ORM.
            This function may be used to compute default values.
        """
        return date.today().strftime(DATE_FORMAT)
# ---
def rearrange(array: NamedArray, axes: Sequence[AxisSelector | EllipsisType]) -> NamedArray:
    pass
# ---
def __init__(self, text):
        self.text = text
        self.clicked = Signal()
# ---
def the_collection_name1_is_not_in_the_collection_name2(name1, name2):
    assert not bpy.data.collections.get(name2).children.get(name1)
# ---
def loraize_hf_model(model):
            return loraize(model, config.lora, key=lora_key)
# ---
def job_id(self) -> JobName:
        """Parent job identifier."""
        return self._task_name.parent or self._task_name
# ---
def is_finite(self) -> bool:
        """
        Returns whether the dataset will have a known length in the future (e.g. if it's being constructed).
        If this returns False, the length of the dataset is infinite or unknowable.
        """
        raise NotImplementedError
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()
        return False
# ---
def setThreadStopEvent(self, threadStopEvent):
        self.threadStopEvent = threadStopEvent
        self._program.setThreadStopEvent(threadStopEvent)
# ---
def _fill_queue_with_batches(self):
        try:
            iterator = self._producer_fn()
            if isinstance(iterator, Iterator):
                self._produce_batches_sync(iterator)
            else:
                asyncio.run(self._produce_batches_async(iterator))
        except Exception:
            self.q.put(_ExceptionWrapper(sys.exc_info()))
# ---
def visitdir(self, dir):
        r = False
        for m in self._matchers:
            v = m.visitdir(dir)
            if v == "all":
                return v
            r |= v
        return r
# ---
def __init__(self, amount):
        super().__init__()
        self.amount = amount
# ---
def check_model_works_with_seqlen(model_type, config, input_len):
    key = PRNGKey(0)
    Vocab = hax.Axis("vocab", 128)
    model = model_type.init(Vocab, config, key=key)
    input_ids = hax.arange(config.max_Pos.resize(input_len), dtype=jax.numpy.int32)
    causal_mask = AttentionMask.causal()
    a1 = model(input_ids, key=key, attn_mask=causal_mask)
    assert a1.axis_size("position") == input_len
# ---
def from_cluster(cls, cluster_name: str, ray_init: bool = False) -> "DashboardConfig":
        """Create config for a single cluster by name."""
        return cls(cluster_configs=[cluster_name], ray_init=ray_init)
# ---
def __getitem__(self, key):
        return self.__dict__[key]
# ---
def check_gen_is_equal(
    jax_fn: Callable[[PRNGKeyArray, tuple], jnp.ndarray],
    hax_fn: Callable[[PRNGKeyArray, hax.AxisSpec], hax.NamedArray],
):
    key = jax.random.PRNGKey(0)

    hax_out = hax_fn(key, (Height, Width))
    jax_out = jax_fn(key, (Height.size, Width.size))

    assert hax_out.array.shape == jax_out.shape
    assert jnp.allclose(hax_out.array, jax_out)
# ---
def test_multi_buffer(self):
        grid = Grid((3, 3))
        f = TimeFunction(name="f", grid=grid)
        g = TimeFunction(name="g", grid=grid, save=Buffer(7))

        op = Operator([Eq(f.forward, 1), Eq(g, f.forward)])
        op(time_M=3)
        # f looped all time_order buffer and is 1 everywhere
        assert np.allclose(f.data, 1)
        # g looped indices 0 to 3, rest is still 0
        assert np.allclose(g.data[0:4], 1)
        assert np.allclose(g.data[4:], 0)
# ---
def peek(self) -> T | None:
        files = sorted(self.fs.ls(str(self.pending_dir), detail=False))
        files = [f for f in files if f.rstrip("/") != str(self.pending_dir).rstrip("/")]

        if not files:
            return None

        with self.fs.open(files[0], "rb") as f:
            return pickle.load(f)
# ---
def get_date_range_list(self):
        return [d for d in self.doc.xpath('//select[@name="date"]/option/@value') if d]
# ---
def _is_special_module(module):
        return _is_lora_compatible_module(module) or isinstance(module, hnn.Stacked)
# ---
def test_check_health_returns_unhealthy_on_failure():
    """check_health returns unhealthy result when curl fails."""
    conn = MagicMock()
    conn.run.return_value = MagicMock(returncode=1, stderr="Connection refused", stdout="")
    result = check_health(conn, port=10001)
    assert result.healthy is False
    assert "exit code 1" in result.curl_error or "Connection refused" in result.curl_error
# ---
def right_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.right_shift(x1, x2)
# ---
def log_summary(self, metrics: dict[str, Any]):
        pass
# ---
def celu(a: A) -> A:
    return wrap_elemwise_unary(jnn.celu, a)
# ---
def fake_vm_start(*args, **kwargs):
            self.fake_vm_start_called = True
# ---
def _get_fineweb2_split_paths(split):
    patterns = FINEWEB2_DATASETS[split]
    fineweb2_split_paths = [output_path_of(fineweb2_raw, pattern) for pattern in patterns]
    return fineweb2_split_paths
# ---
def checkpoint_sort_key(ckpt_dir):
        metadata = json.load(fs.open(os.path.join(ckpt_dir, "metadata.json")))
        return (datetime.datetime.fromisoformat(metadata["timestamp"]), metadata["step"])
# ---
def __add__(self, other: "DupCounters") -> "DupCounters":
        assert isinstance(other, DupCounters)

        return DupCounters(
            method=self.method,
            level=self.level,
            total=self.total + other.total,
            dups=self.dups + other.dups,
            unique=self.unique + other.unique,
            dup_clusters=self.dup_clusters + other.dup_clusters,
        )
# ---
def num_devices(self) -> int:
        """Get the number of devices."""
        return self.train_config.resources.chip_count()
# ---
def login_required(view):
    @wraps(view)
    def inner(method, notebook, data, *args, **kwargs):
        if not is_logged():
            return views.render('login_form', notebook)
        return view(method, notebook, data, *args, **kwargs)
    return inner
# ---
def make_json_serializable(row: dict) -> dict:
    """Make a row JSON serializable"""
    for key, value in row.items():
        if isinstance(value, dict):
            row[key] = make_json_serializable(value)
        if isinstance(value, datetime.datetime):
            row[key] = value.isoformat()
        if isinstance(value, np.ndarray):
            row[key] = value.tolist()
        if isinstance(value, np.float32 | np.float64):
            row[key] = float(value)
    return row
# ---
def test_capture_not_started_but_reset():
    capsys = StdCapture()
    capsys.stop_capturing()
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)

        return x
# ---
def key_dim(self) -> int:
        return self.num_k_heads * self.head_k_dim
# ---
def training_data(self) -> Iterator[MockEnvExample]:
        """Stream training data."""
        for example in self.train_examples:
            yield MockEnvExample(
                raw_prompt=example["prompt"],
                raw_answer=example["answer"],
                processed_prompt=example["prompt"],
                processed_answer=example["answer"],
                metadata={"task_type": self.task_type},
            )
# ---
def testEmpty(self):
        hpcp = HPCP()([], [])
        self.assertEqualVector(hpcp, [0.]*12)
# ---
def test_fold():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    acc = hax.zeros((Height, Width))

    total = hax.fold(lambda x, y: x + y, Depth)(acc, named1)

    assert jnp.all(jnp.isclose(total.rearrange(acc.axes).array, jnp.sum(named1.array, axis=2)))
# ---
def getBackend(name):
        if not name in ObjectBackendRegistry.backends:
            raise ValueError(C.make_error("BACKEND_NOT_FOUND", name))

        return ObjectBackendRegistry.backends[name]
# ---
def test_type(self):
        assert isinstance(event.Priority.DEFAULT, int)
# ---
def train_step(state: TinyMLP, xb: NamedArray, yb: NamedArray):
        loss, grads = _loss_and_grad(state, xb, yb)
        new_state = _apply_sgd(state, grads, base_lr=base_lr, use_mup=use_mup)
        return new_state, loss
# ---
def test_augment_bank_all_entries_valid_python(bank):
    augmented, _ = augment_bank_with_egraph(bank)
    for node_type, entries in augmented.entries.items():
        for entry in entries:
            try:
                ast.parse(entry.source, mode="eval")
            except SyntaxError:
                try:
                    ast.parse(entry.source)
                except SyntaxError:
                    pytest.fail(f"Invalid entry [{node_type}]: {entry.source!r}")
# ---
def test_impl(df):
            return df['A'][df['B']].values
# ---
def sort(a: NamedArray, axis: AxisSelector) -> NamedArray:
    """
    Named version of [jax.numpy.sort](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.sort.html)
    """
    return wrap_axiswise_call(jnp.sort, a, axis, single_axis_only=True)
# ---
def _check_env_closure(output_path: str):
        with open(output_path, "w") as f:
            json.dump(dict(os.environ), f)
# ---
def _solve_puzzle_parts(self):
        old_nice_count = 0
        new_nice_count = 0
        for string in self._puzzle_input:
            if not string:
                continue
            if self.__is_nice_string_using_old_rules(string):
                old_nice_count += 1
            if self.__is_nice_string_using_new_rules(string):
                new_nice_count += 1
        return (old_nice_count, new_nice_count)
# ---
def _can_swap_date_fields(self, first, second): # 'day', 'month', 'year'
        pane = self.selected_pane
        if pane is None:
            return False
        return pane.can_swap_date_fields(first, second)
# ---
def _str_to_int(x_str: str) -> int:
    x_str = x_str.replace(",", "")
    x = float(x_str)
    return int(x)
# ---
def get_containing(self, name, depth = 0):
        """Return the n-th (n = ``depth``) context containing attribute named ``name``."""
        ctx_dict = object.__getattribute__(self, '__dict__')
        if name in ctx_dict:
            if depth <= 0:
                return self
            depth -= 1
        parent = ctx_dict.get('_parent')
        if parent is None:
            return None
        return parent.get_containing(name, depth = depth)
# ---
def finish(self):
        self._run.finish()
# ---
def scopes_del(self):
        del self._scopes
        if self.req is not None and self.req.environ.get('wenoit_etalage') is not None \
                and '_scopes' in self.req.environ['wenoit_etalage']:
            del self.req.environ['wenoit_etalage']['_scopes']
# ---
def sell(self, owner, data,
             size, price=None, plimit=None,
             exectype=None, valid=None):

        order = SellOrder(owner=owner, data=data,
                          size=size, price=price, pricelimit=plimit,
                          exectype=exectype, valid=valid)

        return self.submit(order)
# ---
def test_instance_not_auto_disk_config(self):
        """Should not partition unless instance is marked as
        auto_disk_config.
        """
        self.instance_values['auto_disk_config'] = False
        self.assertIsPartitionCalled(False)
# ---
def is_checkpoint_dir(path: str):
        return fs.exists(os.path.join(path, "metadata.json"))
# ---
def post_jit(x):
        return jax.device_get(x.addressable_data(0))
# ---
def convert_ova(ova_path, vminfo, job_id, irs):
    command = OvaCommand(ova_path, vminfo, job_id, irs)
    job = ImportVm(job_id, command)
    job.start()
    _add_job(job_id, job)
    return response.success()
# ---
def test_filter_passing_none_pass():
    candidates = [
        _make_candidate("def f(x):\n    return 0\n"),
        _make_candidate("def f(x):\n    return -1\n"),
    ]
    tests = ["assert f(1) == 1"]
    passing = filter_passing(candidates, tests)
    assert passing == []
# ---
def method2(self, d, e):
        return d - e
# ---
def get_block_header_hash_by_number(self, block_number) -> Optional[bytes]:
        with self.lock:
            return self._state.get_block_header_hash_by_number(block_number)
# ---
def _is_float(num: str) -> bool:
    try:
        float(num)
        return True
    except ValueError:
        return False
# ---
def volts(self):
        """ADC voltages presented as a list"""
        return self._volts
# ---
def paths(self):
        '''the path attribute split by /'''
        return filter(None, self.path.split('/'))
# ---
def get_job_form(method, notebook, data):
    context = {'platforms': notebook.get_platforms()}
    context['values'] = ({'current': {'options': {}}} if notebook.current_job is None
                         else notebook.current_job.data)
    return context
# ---
def config_to_dict(config: config_pb2.IrisClusterConfig) -> dict:
    """Convert config to dict for YAML serialization."""
    return MessageToDict(config, preserving_proto_field_name=True)
# ---
def reasonable_default(module, path):
            # TODO: gross
            if "LayerNorm" in path:
                return False
            if "RMSNorm" in path:
                return False
            if "RmsNorm" in path:
                return False
            if "Embedding" in path:
                return False
            if path.endswith("bias"):
                return False
            return None
# ---
def output(self):
        return luigi.s3.S3Target(path='{}/{}/{}-{}.zip'.format(self.root_path,
                                                               self.raw_path,
                                                               str(self.month).zfill(2),
                                                               self.year))
# ---
def list_methods(self, request: actor__pb2.ListMethodsRequest, ctx: RequestContext) -> actor__pb2.ListMethodsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def run_async(
        self,
        cmd: list[str],
        *,
        env: dict[str, str] | None = None,
        **kwargs,
    ) -> subprocess.Popen:
        """Start a command within the venv without waiting."""
        if env is None:
            env = self.get_env()
        return self._job_group.run_async(cmd, env=env, **kwargs)
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"blocks": None}
# ---
def compute(model, input):
        model_output = model(input, attn_mask=attn_mask)
        return model_output
# ---
def fake_init_pool(id, name):
            fake_init_pool.called = True
# ---
def _assert_empty(args, msg='%s'):
    if args:
        raise ValueError(msg % args)
# ---
def host(self):
        """
        Gets the host of this ContributorOrcid.

        :return: The host of this ContributorOrcid.
        :rtype: str
        """
        return self._host
# ---
def create_reader(self) -> "RolloutReader":
        if self.storage_type == StorageType.FILE:
            if self.path is None:
                raise ValueError("path must be specified for FILE storage type")
            return FileRolloutReader(self.path)
        else:
            if self.queue_name is None:
                raise ValueError("queue_name must be specified for IN_MEMORY storage type")
            return _get_or_create_queue(self.queue_name, self.queue_maxlen).reader()
# ---
def compute(obj):  # Submit task
    return obj
# ---
def _get_disk_bytes() -> int:
    """Get available disk space in bytes."""
    try:
        stat = os.statvfs("/")
        return stat.f_bavail * stat.f_frsize
    except Exception:
        return 100 * 1024**3
# ---
def test_plain(self):
        table = self.tables.some_table
        lx = table.c.x.label("lx")
        self._assert_result(select([lx]).order_by(lx), [(1,), (2,), (3,)])
# ---
def any(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """True if any elements along a given axis or axes are True. If axis is None, any elements are True."""
    return wrap_reduction_call(jnp.any, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def test_aggregate_one(self):
        val = self.db.get_aggregate_one_historic(self.dusk,
                                                 (self.serial1, self.serial2))
        assert_equals(val, 3*((self.dusk - self.dawn - 2) / 300))
# ---
def mask_fn(param, path):
            path_str = ".".join(path) if isinstance(path, (list, tuple)) else str(path)
            if "Embedding" in path_str or "lm_head" in path_str:
                return "signum"
            elif isinstance(param, Linear):
                # scion for linear layers
                return dataclasses.replace(param, weight="scion", bias="signum" if param.bias is not None else None)
            else:
                return "signum"
# ---
def write_xml(self, uid, title, authors):
        'Write the xml code for the table of contents.'
        xml = self._head.format(uid, self.max_depth, title)
        for aut in authors:
            xml += self._doc_author.format(aut)
        xml += '  <navMap>\n'
        for entry in self.entries:
            xml += self._navp_xml(entry, 2)
        xml += '  </navMap>\n</ncx>'
        return xml
# ---
def _tail_file(path: str, max_lines: int) -> str:
    try:
        with open(path, "r") as f:
            lines = f.readlines()
        return "".join(lines[-max_lines:])
    except Exception as exc:
        return f"<failed to read {path}: {exc}>"
# ---
def vm_count(self) -> int:
        """Return the number of registered VMs."""
        with self._lock:
            return len(self._vms)
# ---
def wait_for_image_status(self, image_id, status):
        """Waits for an image to reach a given status."""
        waiters.wait_for_image_status(self, image_id, status)
# ---
def CurrentLineContents():
  return ToUnicode( vim.current.line )
# ---
def copy_page(self, src_page: int, dst_page: int) -> "KvPageCache":
        """Copy the entire contents of page ``src_page`` into ``dst_page``.

        This is used when creating clones that should have an identical last partial page, but mapped to a fresh page.
        """
        new_k = self.kv_pages.at["page", dst_page].set(self.kv_pages["page", src_page])
        return dataclasses.replace(self, kv_pages=new_k)
# ---
def search(self, cr, user, args, offset=0, limit=None, order=None, context=None, count=False):
        return self.pool.get('stock.picking').search(cr, user, args, offset, limit, order, context, count)
# ---
def _patsplit(pattern, default):
    """Split a string into the optional pattern kind prefix and the actual
    pattern."""
    if ":" in pattern:
        kind, pat = pattern.split(":", 1)
        if kind in allpatternkinds:
            return kind, pat
    return default, pattern
# ---
def backwards(self, orm):
        # Deleting field 'UserProject.drive_auth'
        db.delete_column(u'user_project', 'drive_auth')
# ---
def get_jobs(self):
        if not self.is_supported:
            return []

        jobs_data = requests.get(JOBS_URL_PATTERN % self.id, auth=SAAGIE_BASIC_AUTH_TOKEN).json()
        return [SaagieJob(self.notebook, job_data) for job_data in jobs_data
                if job_data['category'] == 'processing' and
                job_data['capsule_code'] in self.SUPPORTED_CAPSULE_TYPES]
# ---
def fold_via(
        self, fn: Callable[[M, CarryT], CarryT], *, unroll: int | bool | None = None
    ) -> Callable[[CarryT], CarryT]: ...
# ---
def test_compile_1(self):
        compiler = PatternCompiler(pattern_set=dict(
            TEST=r'\w+'
        ))

        try:
            c1 = compiler.compile('$1{TEST}')
        except Exception as exc:
            self.assertTrue(1)

        c1 = compiler.compile('$1{TEST}', ['test'])
        self.assertEqual(c1, r'(?:(?P<test>(\w+)))')
# ---
def __init__(self):
        c_ptr = SummaryKeyMatcher.cNamespace().alloc()

        super(SummaryKeyMatcher, self).__init__(c_ptr)
# ---
def erase(self):
		self.truncate(0)
		self.seek(0, 0)
		return
# ---
def rademacher(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.rademacher(key, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def signal_handler(signal, frame):
	print('You pressed Ctrl+C!')
	sys.exit(0)
# ---
def expanded_path(self, run_id) -> str:
        if self.append_run_id_to_base_path:
            return os.path.expanduser(os.path.join(self.base_path, run_id))
        return os.path.expanduser(self.base_path)
# ---
def generate_job_name(command: str) -> str:
    """Generate job name from command."""
    parts = command.split()
    entrypoint = parts[0] if parts else "unknown"
    if "/" in entrypoint:
        entrypoint = entrypoint.split("/")[-1]
    if "." in entrypoint:
        entrypoint = entrypoint.split(".")[0]

    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    return f"fray-{getpass.getuser()}-{entrypoint}-{timestamp}"
# ---
def test_submit_callable_succeeds(client: LocalClient):
    handle = client.submit(JobRequest(name="ok", entrypoint=Entrypoint.from_callable(_noop)))
    status = handle.wait()
    assert status == JobStatus.SUCCEEDED
# ---
def _flatten_nested_dict(d):
    def items():
        for key, value in d.items():
            if isinstance(value, dict):
                for subkey, subvalue in _flatten_nested_dict(value).items():
                    yield key + "/" + subkey, subvalue
            else:
                yield key, value

    return dict(items())
# ---
def _read_ovf_from_ova_dir(ova_path):
    files = os.listdir(ova_path)
    name = _find_ovf(files)
    if name is not None:
        with open(os.path.join(ova_path, name), 'r') as ovf_file:
            return ovf_file.read()
    raise ClientError('OVA directory %s does not contain ovf file' % ova_path)
# ---
def get_block_datapoint(self, headerhash):
        with self.lock:
            return self._state.get_block_datapoint(headerhash)
# ---
def test_format_accelerator_display_handles_unknown():
    """Unknown accelerator types are handled gracefully."""
    result = format_accelerator_display(999, "some-variant")
    assert "unknown" in result.lower()
# ---
def discover_hf_checkpoints(base_path: str):
    """
    Discover the Hugging Face checkpoints in the given path, sorted by the last modified time. (Most recent last)
    Args:
        base_path:  Fsspec Path to the directory containing the checkpoints, possibly in nested directories.
    Returns:
        List of paths to the checkpoints, sorted by the last modified time.
    """

    return discover_checkpoints(base_path, "**/config.json", ["config.json", "tokenizer_config.json"])
# ---
def checkCollision(self, otherSprite):
        if (self.x < otherSprite.x + otherSprite.tw and otherSprite.x < self.x + self.tw
            and self.y < otherSprite.y + otherSprite.th and otherSprite.y < self.y + self.th):
            return True
        else:
            return False
# ---
def test_list(self):
        results = spell_checker.check([u'ì•ˆë…• í•˜ì„¸ìš”.', u'ì €ëŠ” í•œêµ­ì¸ ìž…ë‹ˆë‹¤.'])
        assert results[0].checked == u'ì•ˆë…•í•˜ì„¸ìš”.'
        assert results[1].checked == u'ì €ëŠ” í•œêµ­ì¸ìž…ë‹ˆë‹¤.'
# ---
def format_example(data: dict) -> str:
    """
    Converts example to fastText training data format.
    """
    text = re.sub(r"[\n\r]", " ", data["text"])
    return f"__label__{data['label']}" + " " + text
# ---
def main(config: DeconConfig):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

    result = decontaminate(config)
    print(f"Decontamination completed: {result}")
# ---
def Heads(self) -> Axis:
        return Axis("heads", self.num_heads)
# ---
def first(self):
        """Get first doc."""
        return self.docs[0] if self.docs else None
# ---
def generate_submission_id(command: str) -> str:
    """Generate a nice submission ID based on the inferred experiment."""
    parsed = parse_user_command_line(command)
    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    parts = ["ray-run", getpass.getuser(), parsed["entrypoint"], timestamp]
    return "-".join(parts)
# ---
def __init__(self, randomize):
    self.idx = 0
    self.files = os.listdir(PARAMS["PATH"])

    if len(self.files) == 0:
      raise EnvironmentError("No file available")

    self.files.sort()

    if randomize:
      print("RANDOMIZE")
      random.shuffle(self.files)
# ---
def assign_rest_molecule(rest_molecule, output_atom_group,
                         model_id="model_1", chain_id="Z", res_name="UNK"):
    chain = bridge.AtomGroup()
    res = bridge.AtomGroup()
    res.name = res_name
    atom_id = 1
    for atom in rest_molecule.get_atom_list():
        res.set_atom(atom_id, atom)
        atom_id += 1
    chain.set_group(1, res)

    output_atom_group[model_id].set_group(chain_id, chain)
# ---
def metadata(self) -> dict[str, Any]:
        return {
            "input_ids_key": self.input_ids_key,
            "loss_weights_key": self.loss_weights_key,
        }
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        """Invoke the method remotely. Returns a future."""
        ...
# ---
def permutation_subjects(y):
    """Permute class labels of Contextual Disorder dataset.
    """
    y_perm = np.random.randint(0, 2, len(y)/2)
    y_perm = np.concatenate((y_perm, np.logical_not(y_perm).astype(int)))
    return y_perm
# ---
def update_item_price(self):
		frappe.db.sql("""update `tabItem Price` set item_name=%s,
			item_description=%s, brand=%s where item_code=%s""",
					(self.item_name, self.description, self.brand, self.name))
# ---
def with_prefix(prefix: str | None, leaf: str | None) -> str | None:
    """Joins two optional path strings in a way compatible with pytorch state dict serialization"""
    if prefix is None:
        return leaf
    elif leaf is None:
        return prefix
    else:
        return f"{prefix}.{leaf}"
# ---
def save_file(graph, fn):
    filename = "%s.png" % fn
    graph.write_png(filename)
    graph.write("%s.dot" % fn)
    os.startfile(filename)
# ---
def format_string_list(objs, field):
    objs[field] = ", ".join(objs[field])
# ---
def fetch_users_by_id(user_ids: List[int]) -> List[UserProfile]:
        return list(UserProfile.objects.filter(id__in=user_ids).select_related())
# ---
def test_extend_with_multiple(cache_metadata):
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=2, dtype=jnp.float32, cache_metadata=cache_metadata)

        data1 = jnp.array([[1.0, 2.0], [3.0, 4.0]])
        data2 = jnp.array([[5.0]])

        builder.extend([data1, data2])

        assert len(builder) == 2

        result1 = builder[0]
        assert jnp.all(result1 == data1)

        result2 = builder[1]
        assert jnp.all(result2 == data2)
# ---
def check_format(self, sample_str: str) -> bool:
        """Check if the response follows the boxed format."""
        try:
            _ = extract_boxed(sample_str)
            return True
        except ValueError:
            return False
# ---
def batch_completions(self, prompts, temperature, n, max_tokens=None, stop=None, system_prompt=None):
            completions = []
            for prompt in prompts:
                responses = [f"mock_response_{i}" for i in range(n)]
                completion = create_test_chat_completion(prompt, responses)
                completions.append(completion)
            return completions
# ---
def __iter__(cls):
        yield from cls.members
# ---

def special_factorial(n):
    """The Brazilian factorial is defined as:
    brazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!
    where n > 0

    For example:
    >>> special_factorial(4)
    288

    The function will receive an integer as input and should return the special
    factorial of this integer.
    """
    fact_i = 1
    special_fact = 1
    for i in range(1, n+1):
        fact_i *= i
        special_fact *= fact_i
    return special_fact
# ---
def _infer_mapping(model_name: str) -> dict:
    """Infer the vLLM mapping for a model name, falling back to substring matching."""
    if model_name in _MODEL_MAPPINGS:
        return _MODEL_MAPPINGS[model_name]
    if "Qwen2.5" in model_name:
        return levanter_qwen_to_vllm_mapping()
    raise KeyError(f"No MODEL_MAPPING registered for model: {model_name}")
# ---
def get_target_input(module, input, output):
    """A forward hook which saves the tensor - attached to its graph.
    Used if we want to explain the interim outputs of a model
    """
    try:
        del module.target_input
    except AttributeError:
        pass
    setattr(module, 'target_input', input)
# ---
def zone(self) -> str:
        """Zone/location, or empty string if not applicable."""
        ...
# ---
def __init__(self, head):
        self.head = head
        self.lsize = 0
        while head.next:
            head = head.next
            self.lsize += 1

        self.m1_idx = None
        self.m2_idx = None
        if self.lsize > self._largesize:
            self.m1_idx = self.lsize / 3   # start from 1/3
            self.m1 = self._getN(self.m1_idx)
            self.m2_idx = self.m1_idx * 2  # start from 2/3
            self.m2 = self._getN(self.m2_idx)
# ---
def __init__ (self, url=None, scheduler='default', session=None) :

        Attributes.__init__ (self)
# ---
def sum(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.sum, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype)
# ---
def total_trainable_params(self, vocab_size: int) -> Optional[float]:
        return None
# ---
def get_project_id() -> str | None:
    """Get the current GCP project ID."""
    try:
        result = run_gcloud_command(["gcloud", "config", "get-value", "project"])
        return result.stdout.strip() or None
    except RuntimeError:
        return None
# ---
def test_get_lines_with_empty_string():
        assert get_lines("") == [""]
# ---
def __init__(self, inputfiles):
        """
        :param inputfiles: list of pdb files needed for averaging
        """
        self.inputs = inputfiles
        self.size = []
        self.nbknots = None
        self.radius = None
        self.coordknots = []
# ---
def _gen_image(self, index):
        image = (
            np.arange(self.Height.size * self.Width.size, dtype=np.int32).reshape(self.Height.size, self.Width.size)
            + index * 1000
        )

        return haliax.named(image, (self.Height, self.Width))
# ---
def mix_qkvz_axis(self) -> Axis:
        # [Q | K | V | Z]; the layer projects all at once
        return Axis("qkvz", self.key_dim * 2 + self.value_dim * 2)
# ---
def test_product_search_client_creation(self, mock_client, mock_get_creds, mock_client_info):
        result = self.hook.get_conn()
        mock_client.assert_called_once_with(
            credentials=mock_get_creds.return_value, client_info=mock_client_info.return_value
        )
        assert mock_client.return_value == result
        assert self.hook._client == result
# ---
def broadcast_arrays_and_return_axes(
    *arrays: NamedArray, require_subset: bool = True, ensure_order: bool = True
) -> tuple[tuple[NamedArray, ...], tuple[Axis, ...]]: ...
# ---
def _dslice_params(value: Any) -> tuple[Any, int, Any] | None:
    """
    For slice like objects, return (start, size, step).
    """
    if isinstance(value, HaliaxDSlice):
        return value.start, value.size, 1
    if is_pallas_dslice(value):
        start = value.start
        size = value.size
        step = value.stride
        return start, size, step

    return None
# ---
def testWhileElse(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        while False:
          print 'foo'
        else:
          print 'bar'""")))
# ---
def test_iosxe_is_a_IOSXE(self):
        self.assertIsInstance(self.xe, IOSXE)
# ---
def hello():
            return 42
# ---
def _track_rollout_generation(self):
        """Called when rollout is generated."""
        self.rollouts_generated += 1
# ---
def fast_read_adc0(self):
        """This reads the actual ADC value of channel 0, with as little overhead as possible.
        Use with SPIDEV ONLY!!!!
        returns: The ADC value as an n-bit integer value, with n=10 or 12 depending on the chip."""

        dat = self._dev.xfer(self._control0)
        value = (dat[1] << 8) + dat[2]
        return value
# ---
def profiles():
    """Gets all profiles for all elements for user application to display and manipulate elements"""
    return jsonify(home_services.get_profiles())
# ---
def test_failure(self, apply_failing_clock_call, errno_value, strerror):
        """
        A failure in C{clock_getres} results in an L{OSError} that
        presents the failure's errno.
        """
        calls = apply_failing_clock_call('_clock_getres', errno_value)

        with pytest.raises(OSError) as exc:
            get_clock_info("monotonic")

        assert len(calls) == 1
        assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC

        assert str(exc.value) == strerror
# ---
def _merge_dataset(self, other, overwrite_vars, compat, join):
    aligned_self, other = partial_align(self, other, join=join, copy=False)

    replace_vars, new_vars, new_coord_names = _merge_expand(
        aligned_self, other._variables, overwrite_vars, compat)
    new_coord_names.update(other._coord_names)

    return replace_vars, new_vars, new_coord_names
# ---
def combine_masks_or(mask1: NamedArray | None, mask2: NamedArray | None) -> NamedArray | None:
    if mask1 is None:
        return mask2
    if mask2 is None:
        return mask1
    return mask1 | mask2.broadcast_axis(mask1.axes)
# ---
def In(self) -> AxisSpec:
        return self.layers[0].In
# ---
def build(self) -> "WatchCallback":
        return WatchCallback(
            watch_targets=self.watch_targets,
            include_norms=self.include_norms,
            include_per_parameter_norms=self.include_per_parameter_norms,
            include_histogram=self.include_histograms,
            split_scan_layers=self.split_scan_layers,
        )
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)

        return x
# ---
def _cleanup_stale_vpn_processes(self, vpn_router_ids):
        process_ids = [pid for pid in self.processes
                       if pid not in vpn_router_ids]
        for process_id in process_ids:
            self.destroy_process(process_id)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float32)})
            df.A[0:100] = np.nan
            df.A[200:331] = np.nan
            return df.A.quantile(.25)
# ---
def input_reparam(use_mup: bool = True) -> type[AbstractLinearReparam]:
        """Return the reparameterization class for an input linear layer."""

        return mup.InputLinearMup if use_mup else mup.LinearStandardParam
# ---
def matches_pattern(file_path: pathlib.Path, patterns: list[str]) -> bool:
    relative_path = str(file_path.relative_to(ROOT_DIR))
    for pattern in patterns:
        if fnmatch.fnmatch(relative_path, pattern):
            return True
    return False
# ---
def get_all_actors_in_pool(self) -> list[ActorHandle]:
        return [member.actor for member in self._actor_pool]
# ---
def on_lower_bound_changed(self):
        self.ui.spinBoxUpperBound.setMinimum(self.ui.spinBoxLowerBound.value())
        self.ui.spinBoxBoundaryNumber.setMaximum(math.ceil((self.ui.spinBoxUpperBound.value()
                                                            - self.ui.spinBoxLowerBound.value()) / 2))
# ---
def test_conditional_escaping():
    """Tests *, _ with and without surrounding spaces."""
    test_cases = [
        ("word*bold*", r"word\*bold\*"),
        ("* this is not bold *", "* this is not bold *"),
        ("normal *bold*", "normal \\*bold\\*"),
        ("_italic_", "\\_italic\\_"),
        ("normal _italic_", "normal \\_italic\\_"),
    ]
    for text, expected in test_cases:
        assert minimal_markdown_escape(text) == expected
# ---
def __init__(self,
                 sname,
                 namespace,
                 kubeconfig,
                 secrets=None):
        ''' constructor for handling secret options '''
        self.kubeconfig = kubeconfig
        self.name = sname
        self.namespace = namespace
        self.secrets = secrets
        self.data = {}

        self.create_dict()
# ---
def list_jobs(self) -> list[JobInfo]:
        return [job.get_info() for job in self._jobs.values()]
# ---
def _tee_handler(cmd, *ignore_args, **ignore_kwargs):
            self._tee_executed = True
            return '', ''
# ---
def _fix_sqrt(string: str) -> str:
    if "\\sqrt" not in string:
        return string
    splits = string.split("\\sqrt")
    new_string = splits[0]
    for split in splits[1:]:
        if split[0] != "{":
            a = split[0]
            new_substr = "\\sqrt{" + a + "}" + split[1:]
        else:
            new_substr = "\\sqrt" + split
        new_string += new_substr
    return new_string
# ---
def __init__(self, string=None, digits=None, **kwargs):
        super(Float, self).__init__(string=string, _digits=digits, **kwargs)
# ---
def fn(config: Config):
            random_str = str(random.randint(0, 1000))
            time.sleep(2)
            with open(os.path.join(config.path, random_str), "w") as f:
                f.write("1")
# ---
def _extract_image_params(image_tag: str, image_type: Literal["worker", "controller"]) -> _ImageBuildParams | None:
    parsed = _parse_artifact_registry_tag(image_tag)
    if not parsed:
        return None
    region, project, image_name, version = parsed
    return _ImageBuildParams(
        image_type=image_type, region=region, project=project, image_name=image_name, version=version
    )
# ---

def string_to_md5(text):
    """
    Given a string 'text', return its md5 hash equivalent string.
    If 'text' is an empty string, return None.

    >>> string_to_md5('Hello world') == '3e25960a79dbc69b674cd4ec67a72c62'
    """
    import hashlib
    return hashlib.md5(text.encode('ascii')).hexdigest() if text else None
# ---
def device_flops(self) -> float:
        """Get the peak FLOPs/s for the device type."""
        device_flops = self.train_config.resources.device_flops()
        if device_flops is None:
            raise ValueError("Resources must provide device_flops() for speedrun calculations.")
        return device_flops
# ---
def test_stderr(self):
        cap = capture.FDCapture(2)
        cap.start()
        print("hello", file=sys.stderr)
        s = cap.snap()
        cap.done()
        assert s == "hello\n"
# ---
def visit_FunctionDef(self, node: ast.FunctionDef) -> ast.FunctionDef:
        if node.name in self.mapping:
            node = ast.FunctionDef(
                name=self.mapping[node.name],
                args=node.args,
                body=node.body,
                decorator_list=node.decorator_list,
                returns=node.returns,
                type_comment=node.type_comment,
            )
            ast.copy_location(node, node)
        self.generic_visit(node)
        return node
# ---
def getPluginsWithStatus(): pass
# ---
def compute_success_ratio(stats: LessonStats, current_step: int, max_staleness: int = 1000) -> float:
    """Get success rate for a lesson."""
    return compute_smoothed_success(stats.training_stats.reward_history)
# ---
def zone(self) -> str:
        return self._zone
# ---
def print_locked_workflow_note():
    print_field('Note',
                'This workflow has an explicit input specification (i.e. it is locked), and as such stage inputs cannot be modified at run-time.')
# ---
def dtype(self) -> DType: ...
# ---
def _block_disk_supported(conn, root):
    '''
    Currently we do not support importing VMs with block device from
    Xen on Rhel 5.x
    '''
    if conn.getType() == 'Xen':
        block_disks = root.findall('.//disk[@type="block"]')
        block_disks = [d for d in block_disks
                       if d.attrib.get('device', None) == "disk"]
        return len(block_disks) == 0

    return True
# ---
def whatis(self, value):
        post = self.reverse_lookup[value]
        return post
# ---
def init(cls, Vocab: Axis, config: Gemma3Config, *, key):
        k_t, k_emb = jrandom.split(key, 2)
        transformer = Gemma2Transformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return Gemma3LMHeadModel(transformer, embeddings, lm_head)
# ---
def _can_scan(layer_types: Sequence[str]) -> bool:
        """Check if all layers have the same type (can use Stacked scan)."""
        return len(set(layer_types)) <= 1
# ---
def flush(self):
        pass
# ---
def setup_env(ctx):
    """Set up the remote environment on the development TPU."""
    tpu_name = ctx.obj.tpu_name
    host_alias = f"dev-tpu-{tpu_name}"
    setup_remote_environment(host_alias)
# ---
def pre_jit(x):
        if jax.process_index() == source:
            inp = np.array(x)
        else:
            inp = jnp.zeros(x.shape, dtype=x.dtype)

        shape = (len(jax.devices()),) + inp.shape
        inp = jnp.expand_dims(inp, axis=0)
        out = jax.make_array_from_callback(shape, sharding, lambda _: inp)

        return out
# ---
def backwards(self, orm):
        # Removing unique constraint on 'Vendeur', fields ['code_permanent']
        db.delete_unique(u'encefal_vendeur', ['code_permanent'])
# ---
def peek(self) -> T_co | None:
        self._recover_expired_leases()
        if self.queue:
            return self.queue[0]
        return None
# ---
def dummy_server():
    return DummyInferenceServer()
# ---
def make_batches(size, batch_size):
  """Returns a list of batch indices (tuples of indices).

  Arguments:
      size: Integer, total size of the data to slice into batches.
      batch_size: Integer, batch size.

  Returns:
      A list of tuples of array indices.
  """
  num_batches = int(np.ceil(size / float(batch_size)))
  return [(i * batch_size, min(size, (i + 1) * batch_size))
          for i in range(0, num_batches)]
# ---
def get_address_balance(self, address: bytes) -> int:
        with self.lock:
            return self._state.get_address_balance(address)
# ---
def truncated_normal(key, shape: AxisSpec, lower: NamedOrNumeric, upper: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    lower = broadcast_to(lower, shape).array
    upper = broadcast_to(upper, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.truncated_normal(key=key, lower=lower, upper=upper, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def release_lock(self) -> None:
        """Release the lock if we hold it."""
        try:
            _, lock_data = self._read_lock_with_generation()
            if lock_data and lock_data.worker_id == self.worker_id:
                self.fs.rm(self._lock_path)
                logger.debug("[%s] Released lock", self.worker_id)
        except FileNotFoundError:
            pass
# ---

def digits(n):
    """Given a positive integer n, return the product of the odd digits.
    Return 0 if all digits are even.
    For example:
    digits(1)  == 1
    digits(4)  == 0
    digits(235) == 15
    """
    product = 1
    odd_count = 0
    for digit in str(n):
        int_digit = int(digit)
        if int_digit%2 == 1:
            product= product*int_digit
            odd_count+=1
    if odd_count ==0:
        return 0
    else:
        return product
# ---
def bottom_right_corner3d(self):
        return self.edge_points3d[2]
# ---
def increase(rank):
    pass
# ---
def EscapedFilepath( filepath ):
  return filepath.replace( ' ' , r'\ ' )
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        """Convert to proto for RPC APIs."""

        return vm_pb2.SliceInfo(
            slice_id=self._group_id,
            scale_group=self._scale_group,
            created_at=self._created_at.to_proto(),
            vms=[vm.info for vm in self._vms],
        )
# ---
def test_add_entry():
    bank = SubtreeBank()
    entry = SubtreeEntry(source="x + y", node_type="BinOp", stmt_count=0)
    bank.add(entry)
    assert bank.total_entries == 1
    assert bank.has_type("BinOp")
    assert not bank.has_type("Call")
# ---
def setup(bot):
    n = Runescapecompare(bot)
    bot.add_cog(n)
# ---
def materialize_mask(
    mask: NamedArray | AttentionMask,
    QPos: Axis,
    KPos: Axis,
    q_slice: Optional[haliax.dslice] = None,
    k_slice: Optional[haliax.dslice] = None,
) -> NamedArray: ...
# ---
def test_with_trace_etype(self, mock_start, mock_stop):

        def foo():
            with profiler.Trace("foo"):
                raise ValueError("bar")

        self.assertRaises(ValueError, foo)
        mock_start.assert_called_once_with("foo", info=None)
        mock_stop.assert_called_once_with(info={
            "etype": "ValueError",
            "message": "bar"
        })
# ---
def publisher(self):
        'Publisher name. (optional)'
        try:
            return self.opt_meta['publisher']
        except KeyError:
            return None
# ---
def test_device_flops_for_jax_device(jax_device_kind, expected_flops):
    assert device_flops_for_jax_device(jax_device_kind) == expected_flops
# ---
def _type_list_to_str(types):
  if any([_ not in _DTYPE_TO_STR for _ in types]):
    raise ValueError("Unsupported dtypes: %s" % types)
  return "".join([_DTYPE_TO_STR[_] for _ in types])
# ---
def config(self) -> config_pb2.ScaleGroupConfig:
        """Configuration for this scale group."""
        return self._config
# ---
def raw_args(self):
        return {k: v[0] for k, v in self.args.items()}
# ---
def _prepend_named_batch_axis(leading_axis: Axis):
    def to_active_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            return leaf.as_scanned_result(leading_axis)
        else:
            return leaf

    return to_active_named_array
# ---
def __init__(self):
        self._current_time = 0.0
        self._sleepers = []  # min-heap of SleepEvent
        self._lock = threading.Lock()
# ---
def __repr__(self):
        'Returns the text.'
        return self.text
# ---
def _column_selection(self):
        if isinstance(self.selection, basestring):
            method = self.selection
            return lambda self, *a, **kw: getattr(self, method)(*a, **kw)
        else:
            return self.selection
# ---
def create_and_sign_transaction(self, spend_tx, value, script=CScript([OP_TRUE])):
        tx = self.create_tx(spend_tx, 0, value, script)
        self.sign_tx(tx, spend_tx)
        tx.rehash()
        return tx
# ---
def divide(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.divide](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.divide.html)
    """
    return jnp.divide(x1, x2)
# ---
def _metric_flatten(m: Metric):
    """Flatten Metric for JAX - reduction is aux_data, value/count are children."""
    return (m._value, m._count), m.reduction
# ---
def wait_ready(self, count: int | None = None, timeout: float = 300.0) -> list[ActorHandle]:
        """Return ready actor handles. Ray actors are ready immediately."""
        if count is None:
            count = len(self._handles)
        self._yielded = True
        return self._handles[:count]
# ---
def tpus_per_node(tpu_type: str) -> int:
    """Return the number of TPU chips per node for a given TPU type."""
    if tpu_type in {"v4-8", "v5p-8"}:
        return 4
    match = re.search(r"-(\d+)$", tpu_type)
    if not match:
        raise ValueError(f"Cannot parse TPU type: {tpu_type}")
    chips = int(match.group(1))
    if chips > 8:
        raise ValueError("Only single tpu nodes are supported with the CLI")
    return chips
# ---
def setArtist(self, artist):
        self.__artist = artist
# ---
def auto_sharded(x: T, mesh: Optional[Mesh] = None) -> T:
    """
    Shard a PyTree using the global axis mapping. NamedArrays in the PyTree are sharded using the axis mapping
     and the names in the tree.

    If there is no axis mapping, the global axis mapping, this function is a no-op.
    """
    mapping = current_thread_local_mapping()

    if mapping is None:
        return x

    return shard(x, mapping=mapping, mesh=mesh)
# ---
def tree_broadcast_to(prefix: PyTree[L], t: T, *, is_leaf: Optional[Callable[[Any], bool]] = None) -> T:
    """
    Broadcasts a prefix tree to match the structure of a full tree. This is useful when you need to
    tree_map over t and prefix (using t as the leaves) but prefix is a tree prefix of t.
    """
    return jax.tree.map(
        # note the swap
        lambda pref, xtree: jax.tree.map(lambda x: pref, xtree, is_leaf=is_leaf),
        prefix,
        t,
        is_leaf=is_leaf,
    )
# ---
def __init__(self, registry: VmRegistry) -> None:
        self._registry = registry
# ---
def _json_default(value):
    """
    Provide a best-effort JSON serialization for objects returned by the eval harness.
    """
    if dataclasses.is_dataclass(value):
        return dataclasses.asdict(value)

    if isinstance(value, set):
        return list(value)

    if hasattr(value, "to_dict") and callable(value.to_dict):
        try:
            return value.to_dict()
        except Exception:
            pass

    return repr(value)
# ---
def __init__(self, start: int, length: int | Axis):
        """
        As a convenience, if length is an Axis, it will be converted to `length.size`
        Args:
            start:
            length:
        """
        self.start = start
        if isinstance(length, Axis):
            self.size = length.size
        else:
            self.size = length
# ---
def get_id(self, nick):
        return self.data[nick.lower()]['id']
# ---
def empty_queue_space(self) -> jnp.ndarray:
        return self.tokens.axis_size("position") - self.num_tokens
# ---
def accelerator_type_friendly(accel_type: int) -> str:
    """Return human-friendly accelerator type name.

    Examples:
        ACCELERATOR_TYPE_UNSPECIFIED (0) -> "unspecified"
        ACCELERATOR_TYPE_CPU (1) -> "cpu"
        ACCELERATOR_TYPE_GPU (2) -> "gpu"
        ACCELERATOR_TYPE_TPU (3) -> "tpu"
    """
    name = accelerator_type_name(accel_type)
    if name.startswith("ACCELERATOR_TYPE_"):
        return name.replace("ACCELERATOR_TYPE_", "").lower()
    return name.lower()
# ---
def __init__(self, data_format='default'):
        if data_format == 'default':
            data_format = default_data_format
        self.dim_ordering = data_format
# ---
def test_hf_audio_loading():
    # Use the Real Librispeech Valudation. Testing one doesn't support streaming.
    ac = AudioDatasetSourceConfig(id="WillHeld/test_librispeech_parquet", text_key="text")
    audio_iterator = ac.doc_iterator("validation")
    for i in range(10):
        audio, sample, text = next(audio_iterator)
# ---
def __del__(self):
        self.stop()
# ---
def zeros_like(a: NamedArray, dtype=None) -> NamedArray:
    """Creates a NamedArray with all elements set to 0"""
    return NamedArray(jnp.zeros_like(a.array, dtype=dtype), a.axes)
# ---
def _get_job(job_id):
    with _lock:
        if job_id not in _jobs:
            raise NoSuchJob("No such job %r" % job_id)
        return _jobs[job_id]
# ---
def assert_inside_pjit(arr, expected: NamedSharding):
            def assert_eq(x, y):
                assert x == y

            jax.debug.inspect_array_sharding(arr.array, callback=lambda x: assert_eq(x, expected))
# ---
def __setstate__(self, state):
        self.host = state["host"]
        self.port = state["port"]
        self.queue_name = state["queue_name"]
# ---
def ready_count(self) -> int:
        """All Ray actors are ready immediately after creation."""
        return len(self._handles)
# ---
def _lookup_indices(self, axis: AxisSelection) -> tuple[int | None, ...]: ...
# ---
def __len__(self) -> int:
        return self._run_coroutine(self.dataset.async_len())
# ---
def add_endpoint(self, endpoint: ControllerEndpoint, task_id: JobName | None = None) -> None:
        """Add an endpoint, optionally associating it with a task."""
        with self._lock:
            self._endpoints[endpoint.endpoint_id] = endpoint
            if task_id:
                self._endpoints_by_task.setdefault(task_id, set()).add(endpoint.endpoint_id)
# ---
def test_manual_controller_start_requires_image(ssh_bootstrap_config: config_pb2.IrisClusterConfig):
    """start() requires image to be configured."""
    config = config_pb2.IrisClusterConfig()
    config.CopyFrom(ssh_bootstrap_config)
    config.controller.image = ""

    controller = ManualController(config)
    with pytest.raises(RuntimeError, match="image required"):
        controller.start()
# ---
def from_state_dict(self: Mod, state_dict: StateDict, prefix: Optional[str] = None) -> Mod:
        unscaled = default_eqx_module_from_state_dict(self, state_dict, prefix)
        if unscaled.weight is not None:
            unscaled = dataclasses.replace(unscaled, weight=unscaled.weight / self.reparam.active_scale)
        return unscaled
# ---
def key_helper(self):
        """ Generates a public key and a private key and stores them in the config. The public key will be applied to
        all the instances in the deployment later on when wait() is called.
        """
        if self.config['key_helper']:
            private_key, public_key = util.generate_rsa_keypair()
            self.config['ssh_private_key'] = private_key.decode()
            self.config['ssh_public_key'] = public_key.decode()
# ---
def Step (self,
              Message,
              Delay_In_Seconds = 0.0):
        if self.Next_Step is None:
            self.Next_Step = 1
        if self.Start_Time is None:
            self.Start_Time = clock ()

        logging.info ("Step " + str (self.Next_Step) + ": " + Message)
        sleep (Delay_In_Seconds)
        self.Next_Step += 1
# ---
def resolve(self, name: str) -> ResolveResult:
        urls = self._endpoints.get(name, [])
        endpoints = [ResolvedEndpoint(url=url, actor_id=f"fixed-{name}-{i}") for i, url in enumerate(urls)]
        return ResolveResult(name=name, endpoints=endpoints)
# ---
def init(self, run_id: Optional[str]) -> JsonLoggerTracker:
        del run_id
        log = logging.getLogger(self.logger_name)
        log.setLevel(self.level)
        return JsonLoggerTracker(log)
# ---
def with_prefix(prefix: str | None, leaf: str) -> str: ...
# ---
def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):
        super(LW_MaxPooling3D, self).__init__(pool_size, strides, border_mode, dim_ordering)
# ---
def _kindpatsalwaysmatch(kindpats):
    """ "Checks whether the kindspats match everything, as e.g.
    'relpath:.' does.
    """
    for kind, pat, source in kindpats:
        # TODO: update me?
        if pat != "" or kind not in ["relpath", "glob"]:
            return False
    return True
# ---
def __init__(self, pool_size=2, strides=None, padding='valid'):
        if strides is None:
            strides = pool_size
        assert padding in {'valid', 'same'}, 'border_mode must be in {valid, same}'
        self.pool_length = pool_size
        self.stride = strides
        self.border_mode = padding
# ---
def execute_codegen(self, target, target_workdir):
    self._generate_thrift(target, target_workdir)
# ---
def serialize_keras_object(instance):
  _, instance = tf_decorator.unwrap(instance)
  if instance is None:
    return None
  if hasattr(instance, 'get_config'):
    return serialize_keras_class_and_config(instance.__class__.__name__,
                                            instance.get_config())
  if hasattr(instance, '__name__'):
    return instance.__name__
  raise ValueError('Cannot serialize', instance)
# ---
def mk_LayerNorm(self, axis: AxisSpec):
        return self.norm_config.build(axis)
# ---
def the_object_name_is_an_ifc_class(name, ifc_class):
    ifc = an_ifc_file_exists()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    assert element.is_a(ifc_class), f'Object "{name}" is an {element.is_a()}'
# ---
def wait(self, futures: list[_ImmediateFuture], num_returns: int = 1) -> tuple[list, list]:
        """All futures are immediately ready."""
        return futures[:num_returns], futures[num_returns:]
# ---
def _copy(_):
            src_page = decode_state.sequences.page_indices["seq", src_slot_id, "page", last_idx].scalar()
            dst_page = decode_state.sequences.page_indices["seq", dst_slot_id, "page", last_idx].scalar()
            return cache.copy_page(src_page, dst_page)
# ---
def max_pool_mean(x):
        pooled = max_pool(
            (hax.Axis("H", 2), hax.Axis("W", 2), hax.Axis("D", 2)), x, stride=1, padding=((0, 1), (0, 1), (0, 1))
        )
        return hax.mean(pooled).scalar()
# ---
def test_parse_json_document(self):
        tmpl_str = '["foo" , "bar"]'
        msg = 'The template is not a JSON object or YAML mapping.'
        self._parse_template(tmpl_str, msg)
# ---
def test_load_hf_model_streaming(model_id: str, config, min_params: int):
    """Test loading HF models via streaming."""
    converter = config.hf_checkpoint_converter()

    with use_test_mesh():
        state_dict = converter.load_state_dict(model_id)

    assert isinstance(state_dict, dict)
    assert len(state_dict) > 0

    param_count = _count_params(state_dict)
    assert param_count >= min_params, f"Expected at least {min_params} params, got {param_count}"
# ---
def _default_stock_location(self, cr, uid, context=None):
        try:
            location_model, location_id = self.pool.get('ir.model.data').get_object_reference(cr, uid, 'stock', 'stock_location_stock')
            with tools.mute_logger('openerp.osv.orm'):
                self.pool.get('stock.location').check_access_rule(cr, uid, [location_id], 'read', context=context)
        except (orm.except_orm, ValueError):
            location_id = False
        return location_id
# ---
def list2cols(cols, objs):
    return cols, [tuple([o[k] for k in cols])
                  for o in objs]
# ---
def test_unrescue(self):
        instance = self._create_instance()
        conn = xenapi_conn.get_connection(False)
        # Unrescue expects the original instance to be powered off
        conn.power_off(instance)
        rescue_vm = xenapi_fake.create_vm(instance.name + '-rescue', 'Running')
        conn.unrescue(instance, None)
# ---
def photo_preview(self):
        img = get_thumbnail(self.photo, '75x75', crop='center')
        return format_html('<a href="{}" target="_blank"><img style="width:75px; height:75px;" src="{}"></a>',
                           self.photo.url, img.url)
# ---
def _compute_output_axes(inputs, batch_dims, In, Out):
    """
    Does two things:
    1. Replace In with Out
    2. turn spatial dims (non-batch, non-In, non-Out) into raw names b/c they change size in convolutions
    """
    unchanging_dims = [Out, *batch_dims]
    return [ax.name if ax not in unchanging_dims else ax for ax in replace_axis(inputs.axes, In, Out)]
# ---
def generator(self, data_dir, tmp_dir, is_training):
    raise NotImplementedError()
# ---
def __getSynsets(self, word, wordNetCode):
        """
        It returns the synsets given both word and language code
        """
        from nltk.corpus import wordnet as wn

        synsets = wn.synsets(word, lang=wordNetCode)
        return synsets
# ---
def __getitem__(self, key):
        """Access variables or coordinates this dataset as a
        :py:class:`~xray.DataArray`.

        Indexing with a list of names will return a new ``Dataset`` object.
        """
        from .dataarray import DataArray

        if utils.is_dict_like(key):
            return self.isel(**key)

        key = np.asarray(key)
        if key.ndim == 0:
            return DataArray._new_from_dataset(self, key.item())
        else:
            return self._copy_listed(key)
# ---
def copy(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.copy(), self.axes)
# ---
def body_push(self, data):
        self.body.append(data)
# ---
def init(self, run_id: Optional[str]) -> Tracker:
        return NoopTracker()
# ---
def __delitem__(self, item):
		if item in self._values:
			del self._values[item]
		del self._addresses[item]
# ---
def auto_stop(cluster: Cluster, job_id: str, should_stop: bool):
    """Terminate `job_id` on exit if `should_stop` is True."""
    if not should_stop:
        yield
        return

    try:
        yield
    finally:
        click.echo(f"Auto-stopping job {job_id}...")
        try:
            cluster.terminate(job_id)
        except Exception as e:
            logger.error(f"Failed to stop: {e}")
# ---
def submit_unit (self, description) :
        """
        Instantiate and return (Compute or Data)-Unit object(s)
        """

        raise Exception ("%s.submit_unit() is not implemented" % self.__class__.__name__)
# ---
def format_rpms(rpms):
    return format_line(prefix='rpms'.rjust(RJUST), values=rpms)
# ---
def test_beam_candidate_creation():
    c = BeamCandidate(source="x = 1\n", score=-2.5, depth=1, edits=())
    assert c.source == "x = 1\n"
    assert c.score == -2.5
    assert c.depth == 1
    assert c.edits == ()
# ---
def assert_equal(in_tree, fail_message: str = ""):
    """Verifies that all the hosts have the same tree of values."""
    expected = broadcast_one_to_all(in_tree)
    if not jax.tree_util.tree_all(jax.tree_util.tree_map(lambda *x: np.all(np.equal(*x)), in_tree, expected)):
        raise AssertionError(f"{fail_message} Expected: {expected}; got: {in_tree}.")
# ---
def __iter__(self):
        """
        Iterate over all data in the dataset, in order.
        """
        for shard_name in self.shard_names:
            for doc in self.open_shard(shard_name):
                yield doc
# ---
def __eq__(self, other):
        return type(self) == type(other)
# ---
def _to_tuple(index: tuple[slice, ...]) -> tuple[tuple[int, int], ...]:
            my_indices: tuple[tuple[int, int], ...] = tuple(
                s.indices(axis_size)[0:2] for axis_size, s in zip(array.shape, index, strict=False)
            )

            return my_indices
# ---
def reject_all(self, include_accepted=False):
        self._call_all('reject_all', include_accepted)
# ---
def test_einsum_ordered_ellipsis_preserves_axis_order():
    Batch = Axis("batch", 4)
    SeqQ = Axis("seq_q", 4)
    SeqK = Axis("seq_k", 4)
    Head = Axis("head", 2)
    DHead = Axis("dhead", 8)

    q = hax.ones((Batch, SeqQ, Head, DHead))
    k = hax.ones((Batch, SeqK, Head, DHead))

    out = hax.einsum("... s h d, ... t h d -> ... s t h d", q, k)

    assert out.axes == (Batch, SeqQ, SeqK, Head, DHead)
    assert out.array.shape == (Batch.size, SeqQ.size, SeqK.size, Head.size, DHead.size)
# ---
def where(
    condition: NamedOrNumeric | bool,
    x: NamedOrNumeric,
    y: NamedOrNumeric,
) -> NamedArray: ...
# ---
def to_seconds(self) -> float:
        """Convert to seconds."""
        return self._ms / 1000.0
# ---
def getCurrentColor(self):
        return self._program.getCurrentColor()
# ---
def destroy(self, request, *args, **kwargs):
        raise MethodNotAllowed(self.action)
# ---
def cluster():
    """Boots a local cluster via ClusterManager, yields (url, client)."""
    config = load_config(DEFAULT_CONFIG)
    config = make_local_config(config)
    manager = ClusterManager(config)
    with manager.connect() as url:
        client = IrisClient.remote(url, workspace=IRIS_ROOT)
        yield url, client
# ---
def calc():
	h, l = input().split(' ')

	mapa = []

	for i_row in range(int(h)):
		mapa.append(input().split(' '))

	maior_num = 0



	for row in mapa:
		for col in row:
			n = int(col)
			if (n > maior_num):
				maior_num = n



	qtd = [0 for i in range(maior_num + 1)]

	for row in mapa:
		for col in row:
			n = int(col)

			qtd[n] = qtd[n] + 1

	menor = 1
	for i in range(1, len(qtd)):
		if (qtd[i] <= qtd[menor]):
			menor = i



	print(menor)
# ---
def init(named):
            return Module(named=named)
# ---
def accept_all(self, include_rejected=False):
        self._call_all('accept_all', include_rejected)
# ---
def beta(key, shape: AxisSpec, a: NamedOrNumeric, b: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    a = broadcast_to(a, shape).array
    b = broadcast_to(b, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.beta(key=key, a=a, b=b, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def compute_amax_history(x, amax_history):
    amax_update = jnp.max(jnp.abs(x)).astype(amax_history.dtype)
    new_history = jnp.roll(amax_history, shift=-1, axis=0).at[0].set(amax_update)
    return new_history
# ---
def test_no_name_exception(self):
        def define_class_with_no_name():
            @six.add_metaclass(profiler.TracedMeta)
            class FakeTraceWithMetaclassNoName(FakeTracedCls):
                pass
        self.assertRaises(TypeError, define_class_with_no_name, 1)
# ---
from typing import List


def string_xor(a: str, b: str) -> str:
    """ Input are two strings a and b consisting only of 1s and 0s.
    Perform binary XOR on these inputs and return result also as a string.
    >>> string_xor('010', '110')
    '100'
    """
    def xor(i, j):
        if i == j:
            return '0'
        else:
            return '1'

    return ''.join(xor(x, y) for x, y in zip(a, b))
# ---
def test_scheduler_returns_empty_when_no_workers(scheduler, state, job_request):
    """Verify scheduler returns empty result when no workers available."""
    submit_job(state, "j1", job_request())

    pending_tasks = state.peek_pending_tasks()
    workers = state.get_available_workers()  # Empty

    result = scheduler.find_assignments(pending_tasks, workers)

    assert len(result.assignments) == 0
    assert len(result.timed_out_tasks) == 0
# ---
def _array_if_named(x):
        if isinstance(x, NamedArray):
            return x.array
        return x
# ---
def log_softmax(data, axis):
    r"""Computes log softmax.

    .. math::

        \text{log_softmax}(x)_i = \log \frac{exp(x_i)}{\sum_j exp(x_j)}

    .. note::
        This operator can be optimized away for inference.

    Parameters
    ----------
    data: relay.Expr
        The input data to the operator.

    axis: int
        The axis to sum over when computing softmax
    """

    return _make.log_softmax(data, axis)
# ---
def _dataset_of_id(self, id):
        return self.datasets[self.dataset_index[id]]
# ---
def start_server(self, model: LmHeadModel) -> None:
        pass
# ---
def get_platform():
    """Get an instance of the Platform most appropriate for this
    system.

    :deprecated: Use `pyglet.canvas.Display`.

    :rtype: `Platform`
    :return: The platform instance.
    """
    return Platform()
# ---
def get_id():
    return addon.getAddonInfo('id')
# ---
def __post_init__(self) -> None:
        """Validate configuration."""
        if self.hidden_dim % self.num_heads != 0 and self.head_dim is None:
            raise ValueError(
                f"hidden_dim={self.hidden_dim} must be divisible by "
                f"num_heads={self.num_heads}, or set head_dim explicitly"
            )
        if self.num_heads % self.num_kv_heads != 0:
            raise ValueError(f"num_heads={self.num_heads} must be divisible by num_kv_heads={self.num_kv_heads}")
# ---
def client(server):
    """Create test client for HTTP requests."""
    return TestClient(server._app)
# ---
def __isLanguageAvailable(self, code=None, language_name=None):
        """
        Check if a language is available
        """
        if code is None and language_name is None:
            raise Exception("Error evaluating the correct language")

        if code is not None and code.lower() in AVAILABLE_LANGUAGES:

            return True

        if language_name is not None and language_name.lower() in AVAILABLE_LANGUAGES_NAMES:
            return True

        return False
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["ApertusConfig"]:
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=False,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfApertusConfig,
        )
# ---
def __enter__(self):
        global _global_tracker
        self.old_tracker = _global_tracker
        _global_tracker = self.tracker

        return self.tracker
# ---
def output(self):
        return luigi.LocalTarget(os.path.join(os.getcwd(), "data", "hola_mundo_desde_R.psv"))
# ---
def __init__(self, training_worker_config):
        super().__init__(training_worker_config)
        self.training_worker_config = training_worker_config

        self.steps_completed = 0
        self.losses = []
        self.trained_model = None
        self.reference_model = None
        self.all_steps_seen = []
# ---
def real(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.real, self.axes)
# ---
def __complex__(self) -> complex:  # pragma: no cover
        return complex(self.array)
# ---
def convert_to_onchange(self, value):
        """ convert ``value`` from the cache to a valid value for an onchange
            method v7.
        """
        return self.convert_to_write(value)
# ---
def test_load_datasets_first(self):

        dataset_code = "nipa-section1-10101-a"
        self._load_files(dataset_code)
        self.assertLoadDatasetsFirst([dataset_code])
# ---
def _wait_for_inflight(self) -> None:
        """Wait for in-flight scale-ups to complete without terminating anything.

        Test-only: Waits for all scale-up threads to complete.
        """
        # Wait for all threads in the container to finish
        self._threads.wait()
# ---
def test_llama_flops():
    # Check that the forward flops is within 10% of the naive calculation
    hf_config = transformers.LlamaConfig.from_pretrained("NousResearch/Llama-2-7b-hf")
    llama_config = LlamaConfig.from_hf_config(hf_config)
    n_params = 6.738415616e9
    ratio = llama_config.flops_per_token(hf_config.vocab_size, llama_config.max_seq_len) / (2 * n_params)
    assert ratio > 1.1, f"ratio {ratio} < 1.1"
    assert ratio < 1.2, f"ratio {ratio} > 1.2"
# ---
def __repr__(self):
		return "%s(file=%r, build=%r)" % (self.__class__.__name__, self.file, self.build)
# ---
def test_impl(df):
            B = pd.to_numeric(df.A, errors='coerce')
            return B
# ---
def callback(self, r):
        print("Iteration %d completed at %s" %
              (self.cur_iter, datetime.now().strftime("%d/%m/%Y %H:%M:%S")))
        self.cur_iter += 1
# ---
def testWhile(self):
    self.assertEqual((0, '2\n1\n'), _GrumpRun(textwrap.dedent("""\
        i = 2
        while i:
          print i
          i -= 1""")))
# ---
def t(key, shape: AxisSpec, df: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    df = broadcast_to(df, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.t(key, df.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def get_process_status_cache(self, process):
        if not self.process_status_cache.get(process.id):
            self.process_status_cache[process.id] = {
                'status': None,
                'id': process.vpnservice['id'],
                'updated_pending_status': False,
                'ipsec_site_connections': {}}
        return self.process_status_cache[process.id]
# ---
def test_nameservers(self):
        eq_(self.record.nameservers.__class__.__name__, 'list')
        eq_(self.record.nameservers, [])
# ---
def created_at_ms(self) -> int:
        """Timestamp when this VM group was created (milliseconds since epoch)."""
        ...
# ---
def get_api_key(user_profile: UserProfile) -> str:
    return user_profile.api_key
# ---
def _is_gcs(self) -> bool:
        return self.path.startswith("gs://")
# ---
def __init__(self, rname, namespace, kubeconfig, options):
        self.kubeconfig = kubeconfig
        self.name = rname
        self.namespace = namespace
        self._options = options
# ---
def get_batch_sync(self, indices_or_slice, *, timeout: Optional[float] = None):
        if isinstance(indices_or_slice, slice):
            indices_or_slice = range(
                indices_or_slice.start or 0,
                indices_or_slice.stop or len(self),
                indices_or_slice.step or 1,
            )
        return self.store.get_batch_sync(indices_or_slice)
# ---
def test_ray_not_detected_when_not_initialized():
    """Should not detect Ray when ray.is_initialized() is False."""
    with patch("iris.client.client.get_iris_ctx", return_value=None):
        with patch("ray.is_initialized", return_value=False):
            client = current_client()
            assert isinstance(client, LocalClient)
# ---
def plugin_on_paused(self):
        if self.__cookie is None:
            return

        try:
            bus = dbus.SessionBus()
            obj = bus.get_object(self.DBUS_NAME, self.DBUS_PATH)
            iface = dbus.Interface(obj, self.DBUS_INTERFACE)
            iface.Uninhibit(self.__cookie)
            self.__cookie = None
        except dbus.DBusException:
            pass
# ---
def _get_olmo3_attention(config, layer_idx: int, key):
    return config.init_attention(layer_idx, key=key)
# ---
def test_clean_text():
    """Test text cleaning (lowercase, punct removal, whitespace norm)."""
    text = "Hello,   World! This is a test."
    expected = "hello world this is a test"
    batch = pa.RecordBatch.from_pydict({"text": [text, None, "   "]})
    pipeline = [Transformation.CleanText(input_col="text", output_col="clean")]
    clean = transform(batch, pipeline)["clean"]
    assert clean[0].as_py() == expected
    assert clean[1].as_py() is None
    assert clean[2].as_py() == ""
# ---
def genetic_modification_11(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'disruption',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def randint(key, shape: AxisSpec, minval: NamedOrNumeric, maxval: NamedOrNumeric, dtype=int):
    shape = axis_spec_to_shape_dict(shape)
    minval = broadcast_to(minval, shape).array
    maxval = broadcast_to(maxval, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.randint(key=key, shape=jax_shape, minval=minval, maxval=maxval, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def total_tokens(self) -> int:
        return sum(len(req.prompt_tokens) + req.max_tokens for req in self)
# ---
def _mem_to_mib(size, unit):
    lunit = unit.lower()
    if lunit in ('bytes', 'b'):
        return size / 1024 / 1024
    elif lunit in ('kib', 'k'):
        return size / 1024
    elif lunit in ('mib', 'm'):
        return size
    elif lunit in ('gib', 'g'):
        return size * 1024
    elif lunit in ('tib', 't'):
        return size * 1024 * 1024
    else:
        raise InvalidVMConfiguration("Invalid currentMemory unit attribute:"
                                     " %r" % unit)
# ---
def get_bundle(self, gcs_path: str, expected_hash: str | None = None) -> Path: ...
# ---
def run(self, fn: Callable, *args, name: str | None = None) -> Future | GeneratorFuture:
        """Submit function to thread pool, returning GeneratorFuture for generator functions."""
        if inspect.isgeneratorfunction(fn):
            future = self.executor.submit(lambda: list(fn(*args)))
            return GeneratorFuture(future)
        else:
            return self.executor.submit(fn, *args)
# ---
def reduceCongr(a, b, m):
        gcdAB = gcd(a, b)
        a /= gcdAB
        b /= gcdAB
        m /= gcd(gcdAB, m)
        modinv = modInverse(a, m)
        b *= modinv
        return (1, b, m)
# ---
def _update_selected_pane(self):
        self.import_table.refresh()
        self._refresh_swap_list_items()
        self.view.update_selected_pane()
        self.view.set_swap_button_enabled(self.can_perform_swap())
# ---
def port(self) -> int:
        return self._port
# ---
def sin(a: A) -> A:
    return wrap_elemwise_unary(jnp.sin, a)
# ---
def test_corrupt_program_produces_valid_python(bank):
    source = CORPUS[0]  # fibonacci
    rng = random.Random(42)

    corrupted, mutations = corrupt_program(source, num_steps=3, bank=bank, rng=rng)

    assert len(mutations) > 0
    assert corrupted != source

    try:
        ast.parse(corrupted)
    except SyntaxError:
        pytest.fail(f"corrupt_program produced invalid Python:\n{corrupted}")
# ---
def run_inference_mode(args):
    """Run in inference worker mode."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger = logging.getLogger("rollout_worker")

    logger.info("Starting inference worker mode...")

    # cleanup()
    worker_config = llama_small_rollout_worker_config(CHECKPOINT_DIR, str(RUN_ID))
    worker = RolloutWorker(
        config=worker_config,
    )

    worker.run()
    logger.info("Inference worker completed")
# ---
def elapsed_seconds(self) -> float:
        """Get elapsed time in seconds."""
        return time.monotonic() - self._start
# ---
def _create_ssh_connection(self, host: str) -> DirectSshConnection:
        """Create an SSH connection for the given host."""
        return DirectSshConnection(
            host=host,
            user=self._ssh_config.user,
            port=self._ssh_config.port,
            key_file=self._ssh_config.key_file,
            connect_timeout=self._ssh_config.connect_timeout,
        )
# ---
def __ge__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.greater_equal(self, other)
# ---
def amin(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """
    Aliax for min. See min for details.
    """
    return wrap_reduction_call(jnp.amin, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def emit(self, record: logging.LogRecord) -> None:
        self._buffer.append(
            BufferedLogRecord(
                timestamp=record.created,
                level=record.levelname,
                logger_name=record.name,
                message=self.format(record),
            )
        )
# ---
def log_cb(step, metrics):
        logged_metrics.append((step, {k: float(v) for k, v in metrics.items()}))
# ---
def update_form(self, Dialog):
        # empty variables
        self.edits = None
        self.combobox = None
        self.buttons = None
        self.radios = None
        self.labs = None
        self.labels = None

        # empty layout
        for i in reversed(range(self.gridLayout.count())):
            self.gridLayout.itemAt(i).widget().setParent(None)


        self.prepare_form(Dialog)
# ---
def mk_LayerNorm(self, axis: Axis) -> hnn.RmsNorm:
        return hnn.RmsNorm.init(
            axis, eps=self.layer_norm_epsilon, use_weight=self.use_layer_norm_weight, use_bias=self.use_bias
        )
# ---
def file_exists(self):
        ''' return whether file exists '''
        if os.path.exists(self.filename):
            return True

        return False
# ---
def _service_deps(self):
    service_deps = self.get_options().get('service_deps')
    return list(self.resolve_deps(service_deps)) if service_deps else self._deps
# ---
def is_character_level(self):
    raise NotImplementedError()
# ---
def test_different_expr_different_hash(self):
        expr1 = col("score") > 0.5
        expr2 = col("score") < 0.5
        assert hash(expr1) != hash(expr2)
# ---
def validate_hidden_field(form, field):
        raise ValidationError('Always wrong')
# ---
def diagnostics(self, handle: VllmServerHandle, *, max_lines: int = 200) -> dict[str, str]:
        raise NotImplementedError
# ---
def test_build_data_tree(self):

        dataset_code = "nipa-section1-10101-a"
        self.assertDataTree(dataset_code)
# ---
def test_binary_equal(self):
        """Binary should eq other Binaries and also raw bytestrings"""
        self.assertEqual(Binary("a"), Binary("a"))
        self.assertEqual(Binary("a"), b"a")
        self.assertFalse(Binary("a") != Binary("a"))
# ---
def reserve(c):
    """`build`, then `serve`"""
    build(c)
    serve(c)
# ---
def choosers(test_data):
    filen = os.path.join(
        os.path.dirname(__file__), 'data', test_data['choosers'])
    return pd.read_csv(filen)
# ---
def setup_class(cls):
        cls.test_patterns = MustHavePatterns(Successor)
# ---
def test_greater_equal(self):
        expr = col("score") >= 100
        assert expr.evaluate({"score": 50}) is False
        assert expr.evaluate({"score": 100}) is True
        assert expr.evaluate({"score": 150}) is True
# ---
def combine(*args, **kwargs):
    import warnings

    warnings.warn("combine is deprecated, use eqx.combine instead", DeprecationWarning)
    return eqx.combine(*args, **kwargs)
# ---
def pass_through_fn(rngkey, qs, grads_in, bal_counter):
            if have_qs_sharding:
                qs = _safe_sharding_constraint(qs, Qs_sharding)
            return qs, bal_counter
# ---
def reload(self) -> str:
        """Re-run bootstrap on existing VM without recreating it.

        Unlike restart() which deletes and recreates the VM, reload() SSHs
        into the existing VM and re-runs the bootstrap script to pull the
        latest image and restart the container.

        Raises:
            RuntimeError: If the controller VM doesn't exist or health check fails.
        """
        ...
# ---
def fake_get_rrd(host, vm_uuid):
            with open('xenapi/vm_rrd.xml') as f:
                return re.sub(r'\s', '', f.read())
# ---
def mark_dispatched(self) -> None:
        """Mark job as running. Called when first task starts."""
        self.state = cluster_pb2.JOB_STATE_RUNNING
        self.started_at = Timestamp.now()
# ---
def state(self) -> cluster_pb2.JobState:
        """Get current job state (shortcut for status().state)."""
        return self.status().state
# ---
def get_lm_head(self) -> NamedArray:
        return self.lm_head
# ---
def __repr__(self):
        return formatting.dataset_repr(self)
# ---
def __setitem__(self, item, value):
		if not isinstance(item, int):
			raise TypeError("DBFile indices must be integers, not %s" % (type(item)))

		if isinstance(value, DBRow):
			self._values[item] = value
			self._addresses[item] = -1
		else:
			# FIXME technically we should allow DBRow, but this is untested and will need resetting parent
			raise TypeError("Unsupported type for DBFile.__setitem__: %s" % (type(value)))
# ---
def collapse_alt(self):
        if self.is_alt:
            self.alt.parts = self.alt.parts + ((),)
        else:
            self.is_alt = True
            first_alt_elems = self.group.seq
            self.group.seq = (ast.Alternative(),)
            self.alt.parts = (first_alt_elems,())
# ---
def vm_manager(
        self,
        group_config: config_pb2.ScaleGroupConfig,
        vm_factory: TrackedVmFactory,
        *,
        dry_run: bool = False,
    ) -> VmManagerProtocol: ...
# ---
def __call__(self, indices: np.ndarray | jnp.ndarray) -> np.ndarray: ...
# ---
def preemptible_constraint(preemptible: bool = True) -> Constraint:
    """Constraint requiring workers to be preemptible (or not)."""
    return Constraint(key=PREEMPTIBLE_ATTRIBUTE_KEY, op=ConstraintOp.EQ, value=str(preemptible).lower())
# ---
def __init__(self, difficulty: str = "medium"):
        # Difficulty not used for this task
        pass
# ---
def _create_definition_if_needed(self):
    """Creates the function definition if it's not created yet."""
    with context.graph_mode():
      self._create_definition_if_needed_impl()
# ---
def test_transaction_connection_ctx_rollback(self):
        fn = self._trans_rollback_fn(True)
        conn = testing.db.connect()
        ctx = conn.begin()
        assert_raises_message(
            Exception,
            "breakage",
            testing.run_as_contextmanager, ctx, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def discover_levanter_checkpoints(base_path: str):
    """
    Discover the Levanter checkpoints in the given path, sorted by the last modified time. (Most recent last)
    Args:
        base_path:  Fsspec Path to the directory containing the checkpoints, possibly in nested directories.
    Returns:
        List of paths to the checkpoints, sorted by the last modified time.
    """

    return discover_checkpoints(base_path, "**/metadata.json", ["metadata.json", "model"])
# ---
def clean_wiki_html(html: str, remove_reference_section: bool = True) -> str:
    """
    Cleans the HTML by removing unwanted elements.
    """
    html = unwrap_eqn(html)
    html = remove_and_append_infobox(html)

    if remove_reference_section:
        html = remove_references_from_html(html)

    return html
# ---
def init(HeadDim, config):
        return Llama3RotaryEmbeddings(HeadDim, config)
# ---
def on_step(self, step_info: S, cb_info: CBInfo):
        """
        This function is called after the JIT-compiled function has completed. You have access to the `step_info`
        which contains information about the step that just completed, as well as the `cb_info` which is whatever
        was returned from `inside_step`.
        """
        ...
# ---
def unload(self):
        """Unload the inference model to free up resources."""
        logger.info("Unloading inference model...")
        with self.model_lock:
            self.model = None
            self.engine = None  # type: ignore[assignment]
        logger.info("Inference model unloaded.")
# ---
def get_metrics(self) -> WeightTransferServerMetrics:
        """Get transfer metrics."""
        return self.metrics
# ---
def test_routing_callable_job():
    """Callable entrypoint on non-TPU device routes to _launch_callable_job."""
    request = JobRequest(
        name="callable-job",
        entrypoint=Entrypoint.from_callable(lambda: 42),
        resources=ResourceConfig(device=CpuConfig()),
    )
    assert request.entrypoint.callable_entrypoint is not None
    assert request.entrypoint.binary_entrypoint is None
    assert not isinstance(request.resources.device, TpuConfig)
# ---
def __init__(self, field, fast=True):
        super().__init__(field, None, fast)
# ---
def list_all(self):
        '''
        Print out all keys
        '''
        salt.output.display_output(
                self.key.list_keys(),
                'key',
                self.opts)
# ---
def print_generic_desc(desc):
    for field in desc:
        print_json_field(field, desc[field])
# ---
def show_text(self, string, duration='-', level=None):
        self.command('show_text', string, duration, level)
# ---
def updated_slice(
        self, start: Mapping[AxisSelector, int | "NamedArray"], update: "NamedArray"
    ) -> "NamedArray":  # pragma: no cover
        return haliax.updated_slice(self, start=start, update=update)
# ---
def __init__(
        self,
        producer_fn: Callable[[], Union[Iterator[Ex], AsyncIterator[Ex]]],
        max_capacity: Optional[int] = None,
    ):
        self.max_capacity = max_capacity
        self._producer_fn = producer_fn
# ---
def __init__(self, run: Optional[TrackioRun] = None):
        import trackio

        if run is None:
            logger.warning("Trackio run is not initialized. Initializing a new run.")
            self.run = trackio.init(project="levanter")
        else:
            self.run = run
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        self.run.log_artifact(artifact_path, name=name, type=type)
# ---
def load_thermo_from_postgres(
        self, postgres_uri: str = "postgresql:///eq_compounds"
    ) -> None:
        """Load a LocalCompoundCache from a postgres uri for equilibrator.

        Parameters
        ----------
        postgres_uri : str, optional
            uri of the postgres DB to use, by default "postgresql:///eq_compounds"
        """
        self.lc = LocalCompoundCache()
        self.lc.ccache = CompoundCache(create_engine(postgres_uri))

        self._water = self.lc.get_compounds("O")
# ---
def clear(self):
        """Clear the window.

        This is a convenience method for clearing the color and depth
        buffer.  The window must be the active context (see `switch_to`).
        """
        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)
# ---
def test_noncontig_selectors():
    B, X, Z, Y = Axis("batch", 2), Axis("x", 4), Axis("z", 6), Axis("y", 5)
    a = hax.arange((B, X, Z, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a["x", ix, "y", iy]
    assert out.axes == (B, Z)
    ref = a.array[jnp.arange(2), ix.array, :, iy.array]
    assert jnp.array_equal(out.array, ref)
# ---
def greater(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.greater](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.greater.html)
    """
    return jnp.greater(x1, x2)
# ---
def testSubmediantPosition(self):
        # Make sure that the submediant of a key based on 440 is in the
        # correct location (submediant was randomly selected from all the
        # tones)
        tonic = 440
        submediant = tonic * 2**(9./12.)
        hpcp = HPCP()([submediant], [1])

        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,1.,0.,0.])
# ---
def auth_fields(self):
        return self.__data['list_authkeys']
# ---
def test_diagnose_common_issues_repeated():
    class M(eqx.Module):
        a: jnp.ndarray = eqx.field()
        b: jnp.ndarray = eqx.field()

        def __init__(self):
            super().__init__()
            self.a = jnp.zeros(1)
            self.b = self.a

    try:
        hax.debug.diagnose_common_issues(M())
        pytest.fail("Should have raised an exception")
    except hax.debug.ModuleProblems as e:
        assert len(e.reused_arrays) == 1
        assert len(e.static_arrays) == 0
# ---
def set_parameters(self, parameters_dict):

        if "power" in parameters_dict:
            self.set_power(parameters_dict["power"])

        if "freq" in parameters_dict:
            self.set_frequency(parameters_dict["freq"])

        if "dac_overridden" in parameters_dict:
            self._dac_overridden = parameters_dict["dac_overridden"]
        else:
            self._dac_overridden = False
# ---
def combine_lora_params(params: M, lora_params: M) -> M:
    """
    Combines the given LoRA parameters with the given parameter tree.
    """
    return eqx.combine(params, lora_params, is_leaf=is_lora_param)
# ---
def __repr__(self):
        return f"SelectOp(columns={self.columns})"
# ---
def multiply(self,accum,item):
        return accum * item
# ---
def update_floatingip_precommit(self, context, fip_context):
        pass
# ---
def copy_file(filename: str) -> None:
        """Copy a single file if it doesn't already exist at destination."""
        output_filename = os.path.join(config.output_path, os.path.basename(filename))
        if not fsspec_exists(output_filename):
            # Ensure output directory exists
            fs.makedirs(config.output_path, exist_ok=True)
            fs.copy(filename, output_filename)
# ---
def __init__(self):
                self.name = "instance-0001"
                self.uuid = "1-2-3-4-5"
# ---
def _conjB(Q, G, V):
    """Compute conjB."""
    order = G.ndim
    p = list(range(order))
    conjB = jnp.transpose(V, p[1:] + p[:1])
    for i, q in enumerate(Q):
        conjB = conjB / q if q.ndim < 2 else _solve_triangular_right(conjB, q)
        if i < order - 1:
            conjB = jnp.swapaxes(conjB, i, order - 1)
    return conjB
# ---
def get_step_times_from_wandb(run_id: str, entity: str | None = None, project: str = WANDB_PROJECT) -> list[float]:
    if entity is None:
        entity = _default_wandb_entity()
    run = wandb.Api().run(f"{entity}/{project}/{run_id}")
    return [
        row["throughput/duration"]
        for row in run.scan_history(keys=["throughput/duration"])
        if "throughput/duration" in row
    ]
# ---
def _teardown(self, file_name):
        if path.isfile(file_name):
            try:
                remove(file_name)
            except:
                pass
# ---
def tokenizer():
    return TreeDiffusionTokenizer(max_seq_len=MAX_SEQ_LEN)
# ---
def with_gpu(gpu_type: str = "auto", count: int = 1, **kwargs) -> ResourceConfig:
        device = GpuConfig(variant=gpu_type, count=count)
        return ResourceConfig(device=device, **kwargs)
# ---
def step_impl(context):
    # Import request
    context.browser.find_element_by_xpath("//*[@id='ImportRequestForm']//input[@value='Import']").click()
# ---
def traslado_isr(self):
        return self.__trisr
# ---
def validateReply(self,req):
    if req.status_code >= 400 and req.status_code <= 500:
      try:
        j = req.json()
      except ValueError:
        raise exception.InternalError(req.text,req.status_code)
      raise exception.jsonToException(req.json())
# ---
def _get_job(self, job_id: JobId) -> "_LocalJob":
        if job_id not in self._jobs:
            raise KeyError(f"Job {job_id} not found")
        return self._jobs[job_id]
# ---
def errno_value():
    """
    A particular errno.
    """
    return errno.EINVAL
# ---
def interference_genetic_modification(
        testapp,
        lab,
        award,
        document,
        target):
    item = {
        'award': award['@id'],
        'documents': [document['@id']],
        'lab': lab['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'RNAi',
        'modified_site_by_target_id': target['@id']
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def test_brackets_in_strings_ignored():
    assert brackets_balanced('"(not a bracket"')
    assert brackets_balanced("'[still balanced'")
# ---
def delete_lines(self, min_row, max_row):
        if min_row == -1:
            self.current_label.fuzz_values = self.current_label.fuzz_values[:-1]
        else:
            self.current_label.fuzz_values = self.current_label.fuzz_values[:min_row] + self.current_label.fuzz_values[
                                                                                        max_row + 1:]

        _ = self.current_label  # if user deleted all, this will restore a fuzz value

        self.fuzz_table_model.update()
# ---
def logout(method, notebook, data):
    global SAAGIE_BASIC_AUTH_TOKEN
    global SAAGIE_ROOT_URL
    global SAAGIE_USERNAME
    SAAGIE_BASIC_AUTH_TOKEN = None
    SAAGIE_ROOT_URL = None
    SAAGIE_USERNAME = None
    return {}
# ---
def url(self) -> str:
        return f"http://{self._config.host}:{self.port}"
# ---
def class_labels(self):
    return ["ID_%d" % i for i in range(self.num_classes)]
# ---
def arctan(a: A) -> A:
    return wrap_elemwise_unary(jnp.arctan, a)
# ---
def wait(self) -> None:
        self._threads.wait()
# ---
def stop(self) -> None:
        if self._controller:
            self._controller.stop()
            self._controller = None
        # Clean up autoscaler's temp dir
        if self._autoscaler and hasattr(self._autoscaler, "_temp_dir"):
            self._autoscaler._temp_dir.cleanup()
            self._autoscaler = None
        # Clean up controller's temp dir
        if self._temp_dir:
            self._temp_dir.cleanup()
            self._temp_dir = None
# ---
def batch_preparer(self):
        return TreeBatchPreparer(jtu.tree_map(lambda writer: 9, self.tree, is_leaf=heuristic_is_leaf))
# ---
def register_method_args(self):
        """
        That is the method where users should override in their
        modules according to be able to send their method arguments
        to the Overlord. If they dont have it nothing breaks
        just that one in the base class is called

        @return : empty {}
        """

        # to know they didnt implement it
        return {}
# ---
def get_local_ip_from_hostname():
    hostname = socket.gethostname()
    ip_address = socket.gethostbyname(hostname)
    return ip_address
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None:
        zones = _resolve_zones(group_config, self._platform)
        for zone in zones:
            if _gcloud_delete_tpu(self._platform.project_id, zone, slice_id):
                return
        raise RuntimeError(f"Failed to delete TPU slice {slice_id} in any zone")
# ---
def __repr__(self):
        return "<alwaysmatcher>"
# ---
def get_actor_pool_name(self) -> str:
        return str(self)
# ---
def path_is_jnetfs(path):
  #check if PATH is VFS or not

  df_output_lines = os.popen("df -Ph '%s'" % path).read().splitlines()

  return df_output_lines and "JnetFS" in df_output_lines[1]
# ---
def default_window_title(self):
		"""ãƒ‰ãƒ©ã‚´ãƒ³ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¿ã‚¤ãƒˆãƒ«(ãƒ©ãƒ™ãƒ«)ã‚’å¿œç­”ã™ã‚‹ã€‚"""
		if TRACE: print(__name__), self.default_window_title.__doc__

		return "Dragon"
# ---
def __init__(self, items: Iterable[T]):
        self.total_time = 0.0
        start = time.perf_counter()
        self.items = iter(items)
        self.total_time += time.perf_counter() - start
        self.this_load_time = 0.0
# ---
def _infer_attention_output_block_shape(QPos, KPos, Key, q_i, k, v):
    out_shape = filter_eval_shape(hnn.attention.dot_product_attention, KPos, Key, q_i, k, v)
    return out_shape.axes
# ---
def __init__(self, field, record, value):
        self.args = (field, record, value)
# ---
def _remove_right_units(string: str) -> str:
    # "\\text{ " only ever occurs (at least in the val set) when describing units
    if "\\text{ " in string:
        splits = string.split("\\text{ ")
        assert len(splits) == 2
        return splits[0]
    else:
        return string
# ---
def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):
        sema = threading.Semaphore(value=0)
        def observer(val):
            if cond(val):
                sema.release()
        self.observe_property(name, observer)
        if not level_sensitive or not cond(getattr(self, name.replace('-', '_'))):
            sema.acquire()
        self.unobserve_property(name, observer)
# ---
def visit_mrow(self, element):
        return self._visit_children(element)
# ---
def test_logout_route_requires_login(self):
        # Ensure logout route requres logged in user.
        response = self.client.get("/logout", follow_redirects=True)
        self.assertIn(b"Please log in to access this page", response.data)
# ---
def shard_names(self) -> Sequence[str]:
        return [f"shard_{i}" for i in range(self._num_shards)]
# ---
def is_full(self):
        return self._queue.full()
# ---
def updateGrid(self):
        """
        Implements function to update the grid to alter n-1
        round values

        """
        with open(self.fichier, 'r') as fi:
            for line in fi.readlines():
                i = 0
                for car in line:
                    j = 0
                    if car != '\n':
                        self.grille[i][j] = car
                        j += 1
                    i += 1
# ---
def __iter__(self):
        for f in self._files:
            yield f
# ---
def __init__(self, root, cwd, badfn=None, relativeuipath=False):
        super(alwaysmatcher, self).__init__(
            root, cwd, badfn, relativeuipath=relativeuipath
        )
# ---
def __str__(self):
        return "NoncentralBetaDistr(alpha={0},beta={1},lambda={2})#{3}".format(self.alpha, self.beta, self.lmbda, self.id())
# ---
def worker_id() -> str:
    import threading

    return f"{os.uname()[1]}-{threading.get_ident()}"
# ---
def test_bincount():
    X = Axis("X", 6)
    x = hax.named([0, 1, 1, 2, 3, 1], (X,))
    B = Axis("B", 5)

    out = hax.bincount(x, B)
    expected = jnp.bincount(x.array, length=B.size)
    assert out.axes == (B,)
    assert jnp.all(out.array == expected)

    w = hax.arange((X,), dtype=jnp.float32)
    out_w = hax.bincount(x, B, weights=w)
    expected_w = jnp.bincount(x.array, weights=w.array, length=B.size)
    assert jnp.allclose(out_w.array, expected_w)
# ---
def __call__(self, x):
            return x.sum(self.field)
# ---
def increment(self, stats, sample_rate=1):
        """
        Increments one or more stats counters
        """
        self.update_stats(stats, 1, sample_rate)
# ---
def axis_spec_to_shape_dict(axis_spec: AxisSelection) -> dict[str, int | None]:  # type: ignore
    ...
# ---
def test_nested_lists():
    html = """<ul>
    <li>Item 1
        <ul>
            <li>Subitem 1</li>
            <li>Subitem 2</li>
        </ul>
    </li>
    <li>Item 2</li>
</ul>"""

    expected = """* Item 1
    + Subitem 1
    + Subitem 2
* Item 2
"""
    assert to_markdown(html) == expected
# ---
def __enter__(self) -> "IrisClient":
        return self
# ---
def make_account():
    return {'balance': 0}
# ---
def test_profiler_get_parent_id(self, mock_generate_uuid):
        mock_generate_uuid.return_value = "42"
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        prof.start("test")
        self.assertEqual(prof.get_parent_id(), "2")
# ---
def update(val):
			self.__lam = 10**sSmooth.val
			self.__p = sAsym.val
			self.__baseline = sbcorr.val*self._baseline_als(np.absolute(self.z_data_raw),self.__lam,self.__p,niter=niter)
			l0.set_ydata(np.absolute(self.z_data_raw))
			l0b.set_ydata(np.absolute(self.__baseline))
			l1.set_ydata(np.absolute(self.z_data_raw/self.__baseline))
			fig.canvas.draw_idle()
# ---
def flatten_axes(axis: AxisSpec, old_axes: AxisSelection, new_axis: AxisSelector) -> AxisSpec:
    pass
# ---
def test_marin_tokenizer_integration_checks(fresh_marin_tokenizer):
    tokenizer = fresh_marin_tokenizer
    run_all_tests(tokenizer)
# ---
def _output_ckpt_name(self):
        return f"BlockSeq[{self.Block}, {self.blocks[0].__class__.__name__}].outputs"
# ---
def openai_address(self) -> str:
        return f"http://{self._inference_server.address()}/v1"
# ---
def destroy_router(self, process_id):
        pass
# ---
def is_too_many_lines(offset: int, count: int) -> bool:
    return server.config.max_lines is not None and offset + count > server.config.max_lines
# ---
def nbytes(self):
        return sum(v.nbytes for v in self.variables.values())
# ---
def __init__(self, inputfiles, grid):
        """
        :param inputfiles: list of pdb files of aligned models
        :param grid: 2d-array coordinates of each point of a grid, fourth column full of zeros
        """
        self.inputfiles = inputfiles
        self.models = []
        self.header = []
        self.radius = None
        self.atoms = []
        self.grid = grid
# ---
def open(self, request, *args, **kwargs):
        return super().create(request, *args, **kwargs)
# ---
def socket(self):
        if not hasattr(self, "_socket"):
            self._get_address()
        return self._socket
# ---
def test_bitwise_count_invert():
    A = Axis("A", 4)
    x = hax.named(jnp.array([0, 1, 2, 3], dtype=jnp.uint8), (A,))

    inv = hax.bitwise_invert(x)
    assert jnp.all(inv.array == jnp.bitwise_invert(x.array))

    cnt = hax.bitwise_count(x)
    assert jnp.all(cnt.array == jnp.bitwise_count(x.array))
# ---
def test_string_set(self):
        """Store and retrieve a string set"""
        self.make_table()
        item = {
            "id": "a",
            "datas": set(["a", "b"]),
        }
        self.dynamo.put_item("foobar", item)
        ret = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(ret, item)
# ---
def init(cls, Vocab: Axis, config: ToyLmConfig, *, key: PRNGKeyArray) -> "ToyLmHeadModel":
        k_embed, k_head = jax.random.split(key, 2)
        embed_weight = hax.random.normal(k_embed, (Vocab, config.Embed), dtype=jnp.float32)
        lm_head = hax.random.normal(k_head, (config.Embed, Vocab), dtype=jnp.float32)
        return cls(config, Vocab, embed_weight, lm_head, jnp.array(0.0, dtype=jnp.float32))
# ---
def is_point_on_line(self, point):
        return self.a * point[0] + self.b * point[1] + self.c == 0
# ---
def _format_reused_arrays(self):
        return [f"  Reused array {describe_array(arr)} at paths {paths}" for arr, paths in self.reused_arrays]
# ---
def address(self) -> str:
        return self.vm_name
# ---
def rpc_post(client, method, body=None):
    """Call a Connect RPC method via the test client."""
    return client.post(
        f"/iris.cluster.WorkerService/{method}",
        json=body or {},
        headers={"Content-Type": "application/json"},
    )
# ---
def requirements():
    """Build the requirements list for this project"""
    requirements_list = []

    with open('requirements.txt') as requirements:
        for install in requirements:
            requirements_list.append(install.strip())

    return requirements_list
# ---
def generate_id(line: str, line_number: str) -> str:
    """
    Generate a unique ID for a line based on its content and line number.

    Args:
        line (str): The content of the line.
        line_number (int): The line number in the file.

    Returns:
        str: A SHA-256 hash hexdigest representing the unique ID.
    """
    unique_string = f"{line_number}:{line}"
    hash_object = hashlib.sha256(unique_string.encode("utf-8"))

    return hash_object.hexdigest()
# ---
def exitOff(self):
        DistributedCCharBaseAI.DistributedCCharBaseAI.exitOff(self)
# ---

def circular_shift(x, shift):
    """Circular shift the digits of the integer x, shift the digits right by shift
    and return the result as a string.
    If shift > number of digits, return digits reversed.
    >>> circular_shift(12, 1)
    "21"
    >>> circular_shift(12, 2)
    "12"
    """
    s = str(x)
    if shift > len(s):
        return s[::-1]
    else:
        return s[len(s) - shift:] + s[:len(s) - shift]
# ---
def playlist_prev(self, mode='weak'):
        self.command('playlist_prev', mode)
# ---
def escape(self, text):
        return minimal_markdown_escape(text)
# ---
def stop(self, handle: VllmServerHandle) -> None:
        if handle.process is not None:
            handle.process.kill()
# ---
def get_matching_pattern(self, pattern, name, path):
        pattern = pattern[name]
        if isinstance(pattern, six.string_types):
            return pattern
        else:
            match = pattern.match(path)
            if match:
                return match.group(name)
        return None
# ---
def play(self, clock=None, server=None):
        import supriya.patterns
        import supriya.realtime

        event_player = supriya.patterns.RealtimeEventPlayer(
            self, clock=clock, server=server or supriya.realtime.Server.default()
        )
        event_player.start()
        return event_player
# ---
def _map_fn(lax_map, bs, n_maps, fn, *args):
    """Maybe map a fn along multiple leading axes."""
    if n_maps <= 0:
        return fn(*args)

    if lax_map:
        mapped_fn = lambda xs: _map_fn(lax_map, bs, n_maps - 1, fn, *xs)
        return jax.lax.map(mapped_fn, xs=args, batch_size=bs if bs > 1 else None)
    else:
        mapped_fn = lambda *xs: _map_fn(lax_map, bs, n_maps - 1, fn, *xs)
        return vmap(mapped_fn)(*args)
# ---
def cancel_or_abort_call(self, ar):
        """
        Either cancels a future pending call, or aborts the current processing if the given AR is unset.

        The pending call is keyed by the AsyncResult returned by _routing_call.
        """
        if not self._cancel_pending_call(ar) and not ar.ready():
            self._interrupt_control_thread()
# ---
def __init__(self, fn, batch_size, num_cpus, num_gpus, resources, output_exemplar=None):
        self.fn = fn
        self.batch_size = batch_size
        self.num_cpus = num_cpus
        self.num_gpus = num_gpus
        self.resources = resources
        self.output_exemplar = output_exemplar
# ---
def test_wait_all_empty():
    assert wait_all([]) == []
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        return self.device.device_flops(dtype)
# ---
def exact(root, cwd, files, badfn=None):
    return exactmatcher(root, cwd, files, badfn=badfn)
# ---
def _get_num_train_steps(param_count: int, batch_size: int, seq_len: int, tpp: int = 20) -> int:
    total_tokens = param_count * tpp
    return max(1, total_tokens // (batch_size * seq_len))
# ---
def beta(self):
        return self.__data['bool_tester']
# ---
def _maybe_fold_in_key(self, key, indices: Sequence[int]):
        if key is not None:
            key = _fold_in_key_vmap(key, np.array(indices))
        return key
# ---
def image_renderer(node: RenderTreeNode, context: RenderContext) -> str:
        return _render_inline_as_text(node, context)
# ---
def __getitem__(self, index: int) -> T:
        return self._index_to_obj[index]
# ---
def upgrade(engine_name):
    globals()["upgrade_%s" % engine_name]()
# ---
def test_raw_metric_host_network(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'net_usage_rate_average',
        host_name)

    for record in query:
        if record.net_usage_rate_average is not None:
            assert record.net_usage_rate_average > 0, 'Zero Host Network IO'
            break
# ---
def __init__(self, parent: OpeningOfGroup, initial: bool=False):
        self.parent = parent
        self.is_initial = initial
        self.limited_prev = parent if initial else self
        self.quantified = ContentOfGroup.NotQuantified

        # forward of function
        self.add = self.parent.add
# ---
def tree_unflatten(cls, aux, tree: Any) -> Any:
        assert len(tree) == 1
        # We don't want check shapes b/c there are intermediate states where the shape is wrong
        # e.g. in eqxi.while_loop
        with enable_shape_checks(False):
            return cls(tree[0], axis_names=aux)
# ---
def getRandomLine():
    return list(zip(np.random.uniform(-1,1.00,2),np.random.uniform(-1,1.00,2)))
# ---
def test_make_token_requires_login(self):
        token1 = csrf.make_token()
        self.assertIsNone(token1)
        self.login()
        token2 = csrf.make_token()
        self.assertIsNotNone(token2)
# ---
def _zeros_like_if_needed(ct, like):
    if isinstance(ct, ad_util.Zero):
        return jnp.zeros_like(like)
    return ct
# ---
def process_shard(job: dict):
        return _build_single_shard_cache(
            shard_name=job["shard_name"],
            shard_index=job["index"],
            temp_root=temp_root,
            source=source,
            processor=processor,
            options=options,
            metadata=metadata,
        )
# ---
def remove_ar5iv_footer(html: BeautifulSoup):
    # This is the ar5iv footer generated on xyz date
    footer = html.findAll("footer")
    for fn in footer:
        fn.decompose()
# ---
def __lt__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "lt")
# ---
def err(context):
            stmt = context.statement
            exception = context.original_exception
            if "ERROR ONE" in str(stmt):
                return MyException("my exception")
            elif "ERROR TWO" in str(stmt):
                return exception
            else:
                return None
# ---
def step_tracking_hook(info):
                current_step = int(info.step)
                self.all_steps_seen.append(current_step)
                self._track_training_step()
                current_loss = float(info.loss)
                self.losses.append(current_loss)
# ---
def scanned_f(init, *args, **kwargs):
        return scan_preconfig(init, *args, **kwargs)[0]
# ---
def search(self, query):
        raise NotImplementedError
# ---
def test_connection_available_returns_false_on_timeout():
    """connection_available returns False on timeout."""
    conn = MagicMock()
    conn.run.side_effect = subprocess.TimeoutExpired(cmd="ssh", timeout=30)
    assert connection_available(conn) is False
# ---
def test_capfdbinary(self, testdir):
        reprec = testdir.inline_runsource("""
            def test_hello(capfdbinary):
                import os
                # some likely un-decodable bytes
                os.write(1, b'\\xfe\\x98\\x20')
                out, err = capfdbinary.readouterr()
                assert out == b'\\xfe\\x98\\x20'
                assert err == b''
        """)
        reprec.assertoutcome(passed=1)
# ---
def chaos_raise(key: str) -> None:
    """Convenience: raise an exception if chaos fires for this key.

    Handles delay_seconds before raising.
    """
    if rule := chaos(key):
        time.sleep(rule.delay_seconds)
        raise rule.error or RuntimeError(f"chaos: {key}")
# ---
def _all_compat(self, other, compat_str):
        """Helper function for equals and identical"""
        # some stores (e.g., scipy) do not seem to preserve order, so don't
        # require matching order for equality
        compat = lambda x, y: getattr(x, compat_str)(y)
        return (self._coord_names == other._coord_names
                and utils.dict_equiv(self._variables, other._variables,
                                     compat=compat))
# ---
def _test_do_executemany(self, retval):
        with self._run_test(retval) as (conn, m1):
            result = conn.execute("insert into table foo",
                            [{"foo": "bar"}, {"foo": "bar"}])
        self._assert(
            retval,
            m1.do_executemany, m1.real_do_executemany,
            [call(
                    result.context.cursor,
                    "insert into table foo",
                    [{"foo": "bar"}, {"foo": "bar"}], result.context)]
        )
# ---
def __exit__(self, type, value, tb):
        gui, backend = self.shell.enable_matplotlib(self.old_backend)
# ---
def not_equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.not_equal(x1, x2)
# ---
def get_job_status(self, request: cluster__pb2.Controller.GetJobStatusRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetJobStatusResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def with_effect(self, synthdef, release_time=0.25, **settings):
        import supriya.patterns

        return supriya.patterns.Pfx(
            self, synthdef=synthdef, release_time=release_time, **settings
        )
# ---
def sub_remove(self, sub_id=None):
        self.command('sub_remove', sub_id)
# ---
def _create_and_run_worker(self):
        """Create and run the worker. Must be implemented by subclasses."""
        pass
# ---
def output_exemplar(self) -> dict[str, np.ndarray]:
        return {"data": np.array([0], dtype=np.int64)}
# ---
def action_assign(self, cr, uid, ids, *args):
        """ Changes state to confirmed or waiting.
        @return: List of values
        """
        todo = []
        for move in self.browse(cr, uid, ids):
            if move.state in ('confirmed', 'waiting'):
                todo.append(move.id)
        res = self.check_assign(cr, uid, todo)
        return res
# ---
def __init__(self, pages=None, *args, **kwargs):
        super(GrantsSpider, self).__init__(*args, **kwargs)

        if pages is not None:
            self.pages = pages
            self.start_urls = [ self.start_url_str % str(page) for page in xrange(1,int(self.pages)+1)]
# ---
def extend(self, items):
        """Extend the queue by a number of elements.

        Parameters
        ----------
        items : list
            A list of items.

        """
        for item in items:
            self.push(item)
# ---
def get_vocab_size_for_tokenizer(tokenizer_name: str) -> int:
    """Return the vocabulary size for a tokenizer name.

    Args:
        tokenizer_name: HuggingFace tokenizer name or path.

    Returns:
        Vocabulary size for the tokenizer.
    """
    resolved_name = unwrap_versioned_value(tokenizer_name)
    if resolved_name in _KNOWN_VOCAB_SIZES:
        return _KNOWN_VOCAB_SIZES[resolved_name]

    tokenizer = _load_tokenizer(resolved_name)
    return len(tokenizer)
# ---
def as_dict(self):
        return { 'args': [ self.args[i].decode('utf-8') for i in range(self.num_args) ] }
# ---
def test_identical_programs_no_edits():
    source = "x = 1\n"
    assert tree_diff(source, source) == []
# ---
def conj(a: A) -> A:
    return wrap_elemwise_unary(jnp.conj, a)
# ---
def __init__(self, provider: TaskProvider, log_buffer: LogBuffer | None = None):
        self._provider = provider
        self._log_buffer = log_buffer
        self._timer = Timer()
# ---
def test_hash_verification_success(temp_cache_dir, test_bundle, test_bundle_hash):
    """Test that hash verification passes with correct hash."""
    cache = BundleCache(temp_cache_dir)

    file_url = f"file://{test_bundle}"

    # Get bundle with correct hash - should succeed without raising
    extract_path = cache.get_bundle(file_url, expected_hash=test_bundle_hash)

    # Verify path is valid by checking we got something back
    assert extract_path is not None
# ---
def test_delete_existing_snapshot(self):
        drv = self._driver
        mox = self._prepare_delete_snapshot_mock(True)

        drv.delete_snapshot(FakeSnapshot())

        mox.VerifyAll()
# ---
def crispr_tag_1(testapp, lab, award, ctcf):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'method': 'CRISPR',
        'modified_site_by_gene_id': ctcf['@id'],
        'introduced_tags': [{'name': 'mAID-mClover', 'location': 'C-terminal'}]
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def get_create_serializer(self):
        return serializers.Resource(MaxCount=1, MinCount=1)
# ---
def test_job_id_contains_name(client: LocalClient):
    handle = client.submit(JobRequest(name="my-job", entrypoint=Entrypoint.from_callable(_noop)))
    assert "my-job" in handle.job_id
    handle.wait()
# ---
def getReplicas( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfns = res['Value'].keys()
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.getReplicas( lfns )
# ---
def __repr__(self) -> str:
        op_symbol = "&" if self.op == "and" else "|"
        return f"({self.left} {op_symbol} {self.right})"
# ---
def opener_has_handler(self, opener, handler_class):
        self.assertTrue(any(h.__class__ == handler_class
                            for h in opener.handlers))
# ---
def test_spawn_raw_glance(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_RAW, None, None)
        self.check_vm_params_for_linux()
# ---
def add_ports(self, inc_ports):
        ''' add a port object to the ports list '''
        if not isinstance(inc_ports, list):
            inc_ports = [inc_ports]

        ports = self.get_ports()
        if not ports:
            self.put(Service.port_path, inc_ports)
        else:
            ports.extend(inc_ports)

        return True
# ---
def after_insert(self):
		'''set opening stock and item price'''
		if self.standard_rate:
			for default in self.item_defaults:
				self.add_price(default.default_price_list)

		if self.opening_stock:
			self.set_opening_stock()
# ---
def run_unittests(options):
    ret_build = ret_test = 0
    for job in JERRY_UNITTESTS_OPTIONS:
        ret_build, build_dir_path = create_binary(job, options)
        if ret_build:
            break

        ret_test |= run_check([
            settings.UNITTEST_RUNNER_SCRIPT,
            os.path.join(build_dir_path, 'tests'),
            "-q" if options.quiet else "",
        ])

    return ret_build | ret_test
# ---
def set_data(self, data):
        self.data = []
        self.rewards = []

        for game in data:
            game = np.vstack((game, np.zeros(self.num_features + 1)))
            self.data.append(game[:, :-1])
            self.rewards.append(game[:, -1:])
# ---
def __init__(self, job_id: JobName, status: cluster_pb2.JobStatus):
        self.job_id = job_id
        self.status = status
        state_name = cluster_pb2.JobState.Name(status.state)
        msg = f"Job {job_id} {state_name}"
        if status.error:
            msg += f": {status.error}"
        super().__init__(msg)
# ---
def test_unrescue_not_in_rescue(self):
        instance = self._create_instance()
        conn = xenapi_conn.get_connection(False)
        # Ensure that it will not unrescue a non-rescued instance.
        self.assertRaises(exception.InstanceNotInRescueMode, conn.unrescue,
                          instance, None)
# ---
def test_iris_config_empty_file(tmp_path):
    """Test error on empty config file."""
    bad_config = tmp_path / "bad.yaml"
    bad_config.write_text("")
    with pytest.raises(ValueError, match="Config file is empty"):
        IrisConfig.load(bad_config)
# ---
def f(x):
        x_ref = hax.new_ref(hax.zeros(X))

        def scan_fn(_, i):
            slice = x_ref.slice({"x": i})
            slice[...] = jnp.sin(x * i)
            return None, None

        hax.scan(scan_fn, X)(None, jnp.arange(X.size))
        return x_ref[...].sum().scalar()
# ---
def __init__(self):
                self.array = hax.ones((Dim1, Dim2))
# ---
def __init__(self, dataset):
        self.dataset = dataset
# ---
def __int__(self) -> int:  # pragma: no cover
        return int(self.array)
# ---
def __add__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "add")
# ---
def __post_init__(self):
        assert self.num_heads % self.num_kv_heads == 0, "num_heads must be divisible by num_kv_heads"
        if self.head_dim is None:
            assert self.hidden_dim % self.num_heads == 0, "hidden_dim % num_heads must be 0 when head_dim=None"
# ---
def test_score_candidate_syntax_error():
    candidate = _make_candidate("def f(:\n")
    tests = ["assert f(1) == 1"]
    result = score_candidate(candidate, tests)
    assert result.tests_passed == 0
# ---
def test_map_transforms_elements_of_a_list(self):
        seq = [1, 2, 3]
        mapped_seq = list()

        mapping = map(self.add_ten, seq)

        self.assertNotEqual(list, mapping.__class__)
        self.assertEqual(map, mapping.__class__)
        # In Python 3 built in iterator funcs return iterable view objects
        # instead of lists

        for item in mapping:
            mapped_seq.append(item)

        self.assertEqual([11, 12, 13], mapped_seq)
# ---
def test_method(self):
        self.assertEqual("POST", self.post.get_method())
        self.assertEqual("GET", self.get.get_method())
# ---
def replace_axis(axis_spec: AxisSelection, old: AxisSelector, new: AxisSelection) -> AxisSelection: ...
# ---
def is_remote_tag(git_path, module, dest, remote, version):
    cmd = '%s ls-remote %s -t refs/tags/%s' % (git_path, remote, version)
    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)
    if to_native(version, errors='surrogate_or_strict') in out:
        return True
    else:
        return False
# ---
def visit_BinOp(self, node: ast.BinOp) -> ast.BinOp:
        node.op = self._maybe_swap(node.op, _ARITHMETIC_OPS)
        self.generic_visit(node)
        return node
# ---
def get_next(next_link=None):
            request = prepare_request(next_link)

            pipeline_response = self._client._pipeline.run(request, stream=False, **kwargs)
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                raise HttpResponseError(response=response, error_format=ARMErrorFormat)

            return pipeline_response
# ---
def vortex_file(tmp_path):
    """Create a test vortex file with sample data."""
    records = [{"id": i, "name": f"item_{i}", "score": i * 10} for i in range(100)]
    path = tmp_path / "test.vortex"
    write_vortex_file(records, str(path))
    return path
# ---
def testWriteExceptDispatcherBareExceptionNotLast(self):
    visitor = stmt.StatementVisitor(_MakeModuleBlock())
    handlers = [ast.ExceptHandler(type=None),
                ast.ExceptHandler(type=ast.Name(id='foo'))]
    self.assertRaisesRegexp(util.ParseError, r"default 'except:' must be last",
                            visitor._write_except_dispatcher,  # pylint: disable=protected-access
                            'exc', 'tb', handlers)
# ---
def move_to_element(self, to_element):
        """Moving the mouse to the middle of an element.
        Args:
            to_element: The element to move to.
        """
        self._actions.append(lambda:
            self._driver.execute(Command.MOVE_TO, {'element': to_element.id}))
        return self
# ---
def load_pipeline(pipeline_file):
        """
        Loads scikit model/pipeline
        """
        print(colored('Loading pipeline: ' + pipeline_file, 'green'))
        return joblib.load(pipeline_file)
# ---
def test_len():
    with tempfile.TemporaryDirectory() as tmpdir:
        exemplar = {"a": np.array([0], dtype=np.float64), "b": np.array([0], dtype=np.float64)}
        builder = TreeStore.open(exemplar, tmpdir)

        assert len(builder) == 0

        batch = [
            {"a": np.array([1.0, 2.0]), "b": np.array([3.0, 4.0])},
            {"a": np.array([5.0, 6.0]), "b": np.array([7.0, 8.0])},
        ]
        builder.extend(batch)

        assert len(builder) == 2
# ---
def test_zero_centrality(self):
        G = nx.path_graph(3)
        prev_cc = nx.closeness_centrality(G)
        edge = self.pick_remove_edge(G)
        test_cc = nx.incremental_closeness_centrality(G, edge, prev_cc, insertion=False)
        G.remove_edges_from([edge])
        real_cc = nx.closeness_centrality(G)
        shared_items = set(test_cc.items()) & set(real_cc.items())
        assert len(shared_items) == len(real_cc)
        assert 0 in test_cc.values()
# ---
def asr_model_type(self) -> Type["WhisperASRModel"]:
        return WhisperASRModel
# ---
def newExec(self):
    pass
# ---
def __eq__(self, other: object) -> CompareExpr:  # type: ignore[override]
        return CompareExpr(self, _to_expr(other), "eq")
# ---
def __call__(self, x):
            return self.transform(x)
# ---
def proto(self) -> config_pb2.IrisClusterConfig:
        """Access underlying proto (read-only)."""
        return self._proto
# ---
def launch(self, job_request: JobRequest) -> None:
        """Launch job and start heartbeat thread."""
        self._status_file.write_status(STATUS_RUNNING)
        self._job_id = self.cluster.launch(job_request)
        self._start_heartbeat()
# ---
def autocomplete(self, word):
        for w in self._word_list:
            if w.startswith(word):
                return w, w[len(word):]
        return None, u''
# ---
def __call__(self, t, x):
        """Apply exponential modulation to input.

        Args:
            t: Time values
            x: Input tensor to modulate

        Returns:
            Modulated tensor
        """
        if self.modulate:
            decay = hax.exp(-t * self.deltas_abs.broadcast_axis(self.PosPerBlock))
            x = x * (decay + self.shift)

        return x
# ---
def _get_radio(self):
		r = Radio(self.i, self._dispatcher, self.send_delay)
		self.radios.append(r)

		self.i += 1

		return r
# ---
def test_capturing_bytes_in_utf8_encoding(testdir, method):
    testdir.makepyfile("""
        def test_unicode():
            print ('b\\u00f6y')
    """)
    result = testdir.runpytest("--capture=%s" % method)
    result.stdout.fnmatch_lines([
        "*1 passed*"
    ])
# ---
def getapiname(self):
		return 'taobao.subusers.get'
# ---
def _get_image_ids() -> set[str]:
    """Get all image IDs."""
    result = subprocess.run(["docker", "images", "-q"], capture_output=True, text=True, check=False)
    return set(result.stdout.strip().split()) if result.stdout.strip() else set()
# ---
def __str__(self):
        if not self:
            return "No problems found"
        else:
            return "\n".join(
                [
                    "Found some problems with your module:",
                    *self._format_reused_arrays(),
                    *self._format_static_arrays(),
                ]
            )
# ---
def loss_full_fn(x):
        loss, _ = _full_loss_and_logz(x, lm_head, labels, precision=jax.lax.Precision.HIGHEST)
        return jnp.mean(loss)
# ---
def check_datasets_equal(ds1, ds2):
    ds1 = list(ds1)
    ds2 = list(ds2)
    assert len(ds1) == len(ds2)
    for r1, r2 in zip(ds1, ds2):
        assert r1.keys() == r2.keys()
        for key in r1.keys():
            np.testing.assert_array_equal(r1[key], r2[key])
# ---
def create_vm_with_labels(**kwargs):
        vm = MagicMock()
        vm.info = vm_pb2.VmInfo(
            vm_id=kwargs["vm_id"],
            slice_id=kwargs["slice_id"],
            scale_group=kwargs["scale_group"],
            zone=kwargs["zone"],
            address=kwargs.get("address", ""),
            state=vm_pb2.VM_STATE_BOOTING,
            labels=kwargs["labels"],
        )
        return vm
# ---
def delete_local(self):
        '''
        Delete the local private key file
        '''
        path = os.path.join(self.opts['pki_dir'], 'local.key')
        if os.path.isfile(path):
            os.remove(path)
# ---
def _raw_weight(self, step: int) -> float:
        if step < self.switch_step:
            return 1.0 - self.beta
        t = step - self.switch_step
        frac = jnp.clip(t / self.decay_steps, 0.0, 1.0)
        return float(1.0 - jnp.sqrt(frac))
# ---
def escape_link(url):
    """Remove dangerous URL schemes like javascript: and escape afterwards."""
    lower_url = url.lower().strip('\x00\x1a \n\r\t')
    for scheme in _scheme_blacklist:
        if lower_url.startswith(scheme):
            return ''
    return escape(url, quote=True, smart_amp=False)
# ---
def sanepathname2url(path):
    urlpath = urllib.request.pathname2url(path)
    if os.name == "nt" and urlpath.startswith("///"):
        urlpath = urlpath[2:]
    # XXX don't ask me about the mac...
    return urlpath
# ---
def axis_indices(self, axis: Sequence[AxisSelector]) -> tuple[int | None, ...]: ...
# ---
def __getitem__(self, i):
                return list.__getitem__(self.l, i)
# ---
def _ensure_first(axis):
    def ensure_first(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.rearrange((axis, ...))
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            return leaf

    return ensure_first
# ---
def get_namespace(self, router_id):
        """Get namespace of router.

        :router_id: router_id
        :returns: namespace string.
        """
        return 'vpn-' + router_id
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        k_gate, k_up, k_down = maybe_rng_split(key, 3)
        h = self.act(self.gate_proj(x, key=k_gate)) * self.up_proj(x, key=k_up)
        return self.down_proj(h, key=k_down)
# ---
def test_describe(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.describe()

        hpat_func = self.jit(test_impl)
        n = 1001
        hpat_func(n)
        # XXX: test actual output
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def remove_router_interface_postcommit(self, context, r_port_context):
        pass
# ---
def init(config: Gpt2Config, *, key):
        # vectorize the blocks
        blocks = Stacked.init(config.Layers, Gpt2Block, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = hnn.LayerNorm.init(config.Embed, eps=config.layer_norm_epsilon, use_bias=config.use_bias)

        return Gpt2Transformer(config, blocks, ln_f)
# ---
def _init_weight(key: PRNGKeyArray, shape: tuple[int, ...], std: float) -> Float[Array, "..."]:
    """Initialize weights with truncated normal."""
    return std * random.truncated_normal(key, -3, 3, shape)
# ---
def test_logout_behaves_correctly(self):
        # Ensure logout behaves correctly - regarding the session.
        with self.client:
            self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="admin_user"),
                follow_redirects=True,
            )
            response = self.client.get("/logout", follow_redirects=True)
            self.assertIn(b"You were logged out. Bye!", response.data)
            self.assertFalse(current_user.is_active)
# ---
def train_set(
        self,
        options: CacheOptions = CacheOptions.default(),
        *,
        key: Optional[PRNGKeyArray] = None,
    ) -> AsyncDataset[AudioTextDict]:
        pass
# ---
def max_length(self) -> int:
        """Maximum sequence length the model supports for inputs."""
        return self.config.max_seq_len
# ---
def scan_via(
        self,
        fn: Callable[[M, CarryT], tuple[CarryT, OutputT_co]],
        *,
        unroll: int | bool | None = None,
    ) -> Callable[[CarryT], tuple[CarryT, OutputT_co]]: ...
# ---
def _feature_dtype(feature):
    """Recursively extract a dtype string from a datasets Feature."""
    dtype = getattr(feature, "dtype", None)
    if dtype is not None:
        return str(dtype)
    nested = getattr(feature, "feature", None)
    if nested is not None:
        return f"list[{_feature_dtype(nested)}]"
    return str(feature)
# ---
def uuid2dn(self, backend, uuid, from_db_only=False):
        dn = ObjectBackendRegistry.backends[backend].uuid2dn(uuid)
        if dn is None and from_db_only is True:
            # fallback to db
            if self.__index is None:
                self.__index = PluginRegistry.getInstance("ObjectIndex")
            res = self.__index.search({'uuid': uuid}, {'dn': 1})
            if len(res) == 1:
                dn = res[0]['dn']
        return dn
# ---
def _identity(_):
        return state.cache
# ---
def __create_launcher(self, run_root):
		launcher = self.__task.data['backend']['launcher']
		return self.__launcher_mgr.create_launcher(launcher, run_root)
# ---
def native_value(self):
        """Return sensor state."""
        printer: OctoprintPrinterInfo = self.coordinator.data["printer"]
        if not printer:
            return None

        return printer.state.text
# ---
def var(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.var, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def get_address(self) -> str:
        return f"http://{self._server.address}"
# ---
def test_str_replace_regex(self):
        def test_impl(df):
            return df.A.str.replace('AB*', 'EE', regex=True)

        df = pd.DataFrame({'A': ['ABCC', 'CABBD']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def __init__(self, cfg=None, service_url=None):
        self.cfg = cfg or Config()
        self.service_url = service_url or self.cfg.get('service', 'url')
        user = self.cfg.get('service', 'user', default=None)
        password = self.cfg.get('service', 'password', default=None)
        self.session = requests.Session()
        if user and password:
            self.session.auth = (user, password)
# ---
def unregister(self, vm_id: str) -> None:
        """Unregister a VM by ID.

        Called when a VM is terminated. Safe to call if the VM is not registered.
        """
        with self._lock:
            self._vms.pop(vm_id, None)
# ---
def receive_election():
    '''
    This is a test route to be able to test that callbacks are correctly sent
    '''
    print("ATTENTION received election callback: ")
    print(request.get_json(force=True, silent=True))
    return make_response("", 202)
# ---
def test_parse_chat_completion_tokens_empty_response():
    """Test handling of empty response."""
    tokenizer = AutoTokenizer.from_pretrained("gpt2")

    chat_completion = create_mock_chat_completion_with_logprobs("", [], [])

    parsed_tokens = parse_chat_completion_tokens_from_bytes(chat_completion, tokenizer)
    assert parsed_tokens == []
# ---
def test_deadline_raise_if_expired():
    """Expired deadline raises TimeoutError."""
    deadline = Deadline.from_seconds(0.05)
    time.sleep(0.1)
    with pytest.raises(TimeoutError, match="Test timeout"):
        deadline.raise_if_expired("Test timeout")
# ---
def _check_auth(self):

        if not self.username or not self.password or not self.pin:
            logger.warning("Invalid username or password or pin. Check your settings")

        return True
# ---
def crispr_gm(lab, award, source):
    return {
        'lab': lab['uuid'],
        'award': award['uuid'],
        'source': source['uuid'],
        'guide_rna_sequences': [
            "ACA",
            "GCG"
        ],
        'insert_sequence': 'TCGA',
        'aliases': ['encode:crispr_technique1'],
        '@type': ['Crispr', 'ModificationTechnique', 'Item'],
        '@id': '/crisprs/79c1ec08-c878-4419-8dba-66aa4eca156b/',
        'uuid': '79c1ec08-c878-4419-8dba-66aa4eca156b'
    }
# ---
def add_to_dup_map(record: dict):
        shard_dup_map[record["id"]] = record["fuzzy_duplicate"]
# ---
def unpermute_sharded(out_repeat_sort_: Array, sort_idx_: Array):
            inv_sort_idx_ = jnp.argsort(sort_idx_)
            out_repeat_ = jnp.take(out_repeat_sort_, inv_sort_idx_, axis=0)
            out_repeat_unflat_ = jnp.reshape(out_repeat_, (-1, self.config.num_experts_per_tok, self.config.hidden_dim))

            return out_repeat_unflat_
# ---
def is_not_null(self) -> NotExpr:
        return NotExpr(IsNullExpr(self))
# ---
def __ne__(self, other):
        if not isinstance(other, BBOXCoverage):
            return NotImplemented
        return not self.__eq__(other)
# ---
def testBareAssert(self):
    # Assertion errors at the top level of a block should raise:
    # https://github.com/google/grumpy/issues/18
    want = (0, 'ok\n')
    self.assertEqual(want, _GrumpRun(textwrap.dedent("""\
        def foo():
         assert False
        try:
         foo()
        except AssertionError:
         print 'ok'
        else:
         print 'bad'""")))
# ---
def __call__(
        self, batch: Sequence[T_contra]
    ) -> Sequence[U_co] | U_co:  # U can be batched "structure of arrays" form
        """
        Process a batch of data. You should return a sequence of dicts (one per output
        example), or a dict of sequences (one per output field).
        """
        raise NotImplementedError
# ---
def fn(x):
        return x + 1
# ---
def doMonteCarloQP(pointa, pointb, clf, nopoint):
    #print "weights ", weight
    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    #print dataset_Monte
    return getMisMatchesQP(dataset_Monte, clf)
# ---
def __init__(self, language="en"):
        """
        Constructor for the wordnet manager.
        It takes a main language.
        """
        self.__language = language
# ---
def first_message(database, write):
    messages = [
            firestore_pb2.WriteRequest(database = database, writes = [])
    ]
    for msg in messages:
            yield msg
# ---
def to_arrow(self):
        """Create an :class:`pyarrow.RecordBatch` of rows in the page.

        Returns:
            pyarrow.RecordBatch:
                Rows from the message, as an Arrow record batch.
        """
        return self._stream_parser.to_arrow(self._message)
# ---
def scan(self, init: T, *extra_args, unroll: int | bool | None = None, **extra_kwargs): ...
# ---
def state(self) -> cluster_pb2.TaskState:
        """Get current task state (shortcut for status().state)."""
        return self.status().state
# ---
def get_mod_ndwi(b):
    return b['b6'].subtract(b['b4']).divide(b['b4'].add(b['b6'])).select(['sur_refl_b06'], ['b1'])
# ---
def binary_cross_entropy_loss(
    logits: NamedArray,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: AxisSelection = ...,
) -> NamedArray: ...
# ---
def announce(self):
        print("closed session. gets: %r, regets: %r, puts: %r, dels: %r, renames: %r get_dirs: %r, put_dirs: %r, get_bytes: %r put_bytes: %r window_seconds: %d" % \
              (self.get_requests, self.reget_requests, self.put_requests, self.del_requests, self.rename_requests, self.get_dirs, self.put_dirs, self.get_bytes, self.put_bytes, self.window_seconds))
# ---
def transform(self, x, static1, *, static2):
            assert static1 == 1.0
            assert static2 is False
            return x + self.w + static1
# ---
def fn(a, b):
        return a + b
# ---
def _extract_node_name(self, resource_name: str) -> str:
        """Extract node name from GCP resource path.

        GCP returns 'projects/proj/locations/zone/nodes/my-tpu'
        but gcloud delete expects just 'my-tpu'.
        """
        if "/" in resource_name:
            return resource_name.split("/")[-1]
        return resource_name
# ---
def resource_spec():
    """Create a ResourceSpec for testing with enough capacity for multiple jobs."""

    def _make(cpu: int = 10, memory_bytes: int = 10 * 1024**3) -> cluster_pb2.ResourceSpecProto:
        return cluster_pb2.ResourceSpecProto(cpu=cpu, memory_bytes=memory_bytes, disk_bytes=10 * 1024**3)

    return _make
# ---
def __eq__(self, other):
        return self is other or self.value == other or self.value == int(other)
# ---
def test_teardown_capturing_final(self, testdir):
        p = testdir.makepyfile("""
            def teardown_module(mod):
                print ("teardown module")
                assert 0
            def test_func():
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*def teardown_module(mod):*",
            "*Captured stdout*",
            "*teardown module*",
            "*1 error*",
        ])
# ---
def _jit_train_step_fn(self):
        return named_jit(
            self._train_step,
            axis_resources=self.parameter_axis_mapping,
            out_axis_resources=self.parameter_axis_mapping,
            donate_args=(True,),
        )
# ---
def build_resources(
    tpu: str | None,
    gpu: int | None,
    cpu: int | None,
    memory: str | None,
) -> ResourceSpec:
    """Build ResourceSpec from CLI arguments."""
    spec = ResourceSpec(
        cpu=cpu or 1,
        memory=memory or "2GB",
    )

    if tpu:
        spec.device = tpu_device(tpu)
    elif gpu:
        raise ValueError("GPU support not yet implemented in Iris")

    return spec
# ---
def __init__(
        self,
    ):
        # Mongo params
        self.mongo_uri = None
        self.client = None
        self._core = None

        # eQ params
        self.CC = ComponentContribution()
        self.lc = None
        self._water = None
# ---
def _get_checkpoint_steps(checkpoint_dir):
    paths = list(pathlib.Path(checkpoint_dir).iterdir())
    return sorted([_load_metadata(f)["step"] for f in paths])
# ---
def clause(self):
        """Generate an SQLite expression implementing the query.

        Return (clause, subvals) where clause is a valid sqlite
        WHERE clause implementing the query and subvals is a list of
        items to be substituted for ?s in the clause.
        """
        return None, ()
# ---
def remove_references_from_html(html: str) -> str:
    """
    Removes the references list and heading from the article.
    """
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html, "html.parser")

    reflist = soup.find("div", {"class": "reflist"})
    if reflist:
        reflist.extract()

    ref_heading = soup.find("h2", {"id": "References"})
    if ref_heading:
        ref_heading.extract()

    return str(soup)
# ---
def reset_daily_weights(self):
        for day in ["M", "T", "W", "R", "F", "S"]:
            self.daily_weights[day] = 0
            self.daily_totals[day]  = 0
# ---
def start(self):
        """Start the event loop in a background thread."""
        if self._thread is None:
            raise RuntimeError("AsyncBridge already stopped.")
        self._thread.start()

        # Wait for loop to start
        self._started.wait()
# ---
def time_elapsed(self) -> float:
        return self.time_end - self.time_start
# ---
def finish(self):
        summary = {**self._summary_metrics, **self._last_metrics}
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "finish",
                "summary": summary,
            }
        )
        self.logger.info(json.dumps(record))
# ---
def cleanup(self) -> None:
        """Cleanup resources."""
        pass
# ---
def _hackable_125m_config() -> HackableTransformerConfig:
    # Match the 130m preset dims from hackable transformer starter, but use 2048 context for parity with grug defaults.
    return HackableTransformerConfig(
        max_seq_len=2048,
        hidden_dim=512,
        intermediate_dim=1792,
        num_layers=6,
        num_heads=8,
        num_kv_heads=8,
        head_dim=None,
        use_attention_sink=False,
    )
# ---
def init_fn(params):
        return {"count": jnp.array(0, dtype=jnp.int32)}
# ---
def __init__(self, kind, match):
        """Initiates Check instance.

        :param kind: The kind of the check, i.e., the field before the
                     ':'.
        :param match: The match of the check, i.e., the field after
                      the ':'.
        """

        self.kind = kind
        self.match = match
# ---
def __init__(self, result: Any):
        self._result = result
        self._iterator: Iterator[Any] | None = None
# ---
def get(self, name, default=None):
        """Return the first value, either the default or actual"""
        return super().get(name, [default])[0]
# ---
def inspect_side_effect(container_id):
        call_count[0] += 1
        if call_count[0] == 1:
            return ContainerStatus(running=True)
        return ContainerStatus(running=False, exit_code=0)
# ---
def __init__(self):
        self.events_count = 0
        self.events_count_by_type = dict()
# ---
def batch_axis_at_step(self, step: int) -> Axis:
        bs = value_at_step(self.train_batch_size, step)
        return Axis(self.batch_axis_name, bs)
# ---
def _add_snapshot_info(conn, vm, params):
    # Snapshot related API is not yet implemented in the libvirt's Xen driver
    if conn.getType() == 'Xen':
        return

    try:
        ret = vm.hasCurrentSnapshot()
    except libvirt.libvirtError:
        logging.exception('Error checking for existing snapshots.')
    else:
        params['has_snapshots'] = ret > 0
# ---
def set_current_client(client: Client) -> Generator[Client, None, None]:
    """Context manager that sets the current client and restores on exit."""
    token = _current_client_var.set(client)
    try:
        yield client
    finally:
        _current_client_var.reset(token)
# ---
def dot(
    axis: AxisSelection | None,
    *arrays: NamedArray,
    precision: PrecisionLike = None,
    preferred_element_type: DTypeLike | None = None,
    out_axes: PartialAxisSpec | None = ...,
    dot_general=jax.lax.dot_general,
) -> NamedArray: ...
# ---
def __init__(self, ndb, config):
        self.ndb = ndb
        self.config = config
# ---
def sample_data():
    """Sample data for testing."""
    return list(range(1, 11))
# ---
def order_clause(self):
        order = "ASC" if self.ascending else "DESC"
        if self.case_insensitive:
            field = '(CASE ' \
                    'WHEN TYPEOF({0})="text" THEN LOWER({0}) ' \
                    'WHEN TYPEOF({0})="blob" THEN LOWER({0}) ' \
                    'ELSE {0} END)'.format(self.field)
        else:
            field = self.field
        return f"{field} {order}"
# ---
def __len__(self):
        return self.num_rows
# ---
def _find_dim(n_rot: float):
                return (head_dim * math.log(self.config.original_max_position_embeddings / (n_rot * 2 * math.pi))) / (
                    2 * math.log(self.config.theta)
                )
# ---
def _get_sublocations(self, cr, uid, ids, context=None):
        """ return all sublocations of the given stock locations (included) """
        return self.search(cr, uid, [('id', 'child_of', ids)], context=context)
# ---
def getCCLocation(self):
        if self.diffPath == None:
            return 1
        else:
            return 0
        return
# ---
def test_optimize_map_filter_map():
    """Map, filter, map should be fused into a single stage."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            FilterOp(lambda x: x > 5),
            MapOp(lambda x: x + 1),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    fused_op = plan.stages[0].operations[0]
    assert isinstance(fused_op, Map)
# ---
def __call__(self, *args: Args.args, **kwargs: Args.kwargs) -> R:
        raise NotImplementedError
# ---
def as_input_name(self) -> "InputName":
        return InputName(step=self, name=None)
# ---
def do_pause(self, e):
        self.remote.do_pause()
# ---
def _setPitchForVoiceType(self, voiceType, value):
        """Sets the pitch value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM
        - value: the pitch value to set.
        """

        voiceACSS = self._getACSSForVoiceType(voiceType)
        voiceACSS[acss.ACSS.AVERAGE_PITCH] = value
        voiceACSS['established'] = True
# ---
def days_till_due(self):
        """
        Returns the number of days till the due date. Returns a negative number
        of days when the due date is in the past.
        Returns 0 when the task has no due date.
        """
        due = self.due_date()
        if due:
            diff = due - date.today()
            return diff.days
        return 0
# ---
def _get_random_inputs(config: Olmo2Config, override_Pos=None):
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()

    return x, mask
# ---
def validate_password(self, login, password):
    if login in users :
        if encrypt(password) == users[login] :
            cherrypy.session['username'] = login
            cherrypy.session['database'] = userdatabase(login)
            return True

    return False
# ---
def test_check_password(self):
        # Ensure given password is correct after unhashing.
        user = User.query.filter_by(email="ad@min.com").first()
        self.assertTrue(
            bcrypt.check_password_hash(user.password, "admin_user")
        )
        self.assertFalse(bcrypt.check_password_hash(user.password, "foobar"))
# ---
def testTryMultipleExcept(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        try:
          raise AssertionError
        except RuntimeError:
          print 'foo'
        except AssertionError:
          print 'bar'
        except:
          print 'baz'""")))
# ---
def test_capture_badoutput_issue412(testdir):
    testdir.makepyfile("""
        import os

        def test_func():
            omg = bytearray([1,129,1])
            os.write(1, omg)
            assert 0
        """)
    result = testdir.runpytest('--cap=fd')
    result.stdout.fnmatch_lines('''
        *def test_func*
        *assert 0*
        *Captured*
        *1 failed*
    ''')
# ---
def test_clone_volume_clear(self):
        drv = self._driver
        mox = self._prepare_clone_mock('fail')

        mox.ReplayAll()

        volume_name = 'volume_name'
        clone_name = 'clone_name'
        volume_id = volume_name + str(hash(volume_name))
        try:
            drv._clone_volume(volume_name, clone_name, volume_id)
        except Exception as e:
            if isinstance(e, api.NaApiError):
                pass
            else:
                raise

        mox.VerifyAll()
# ---
def next(self, psm: PSM):
        if psm.char in string.hexdigits:
            self.pattern.pattern += psm.char
            count = len(self.pattern.pattern)
            return self.prev if count >= 4 else self
        else:
            psm.error = "expected ASCII hexadecimal character"
# ---
def mlp(block: GrugBlockParams, x: Float[Array, "B S D"]) -> Float[Array, "B S D"]:
    gate = jnp.einsum("bsh,hm->bsm", x, block.mlp_gate)
    up = jnp.einsum("bsh,hm->bsm", x, block.mlp_up)
    activated = jax.nn.silu(gate) * up
    return jnp.einsum("bsm,mh->bsh", activated, block.mlp_down, out_sharding=Pbatch)
# ---
def test_join_workers():
    producers = [Mock()]
    cworker = Mock()
    consumers = [Mock()]

    classify_documents._join_workers(cworker, producers, consumers)

    for p in producers:
        assert p.join.called
    assert cworker.set_producers_done.called
    for c in consumers:
        assert c.join.called
    assert cworker.clear_producers_done.called
# ---
def setup_class(cls):
        global users, metadata
        metadata = MetaData(testing.db)
        users = Table('users', metadata,
            Column('user_id', INT, primary_key=True,
                            test_needs_autoincrement=True),
            Column('user_name', VARCHAR(20)),
        )
        metadata.create_all()
# ---
def __init__(self):
                self.array = jnp.zeros((8, 8))
# ---
def drawSquare(turtle, x, y, length):
    turtle.up()
    turtle.move(x, y)
    turtle.setDirection(270)
    turtle.down()
    for count in xrange(4):
        turtle.move(length)
        turtle.turn(90)
# ---
def __init__(self, index):
        self.index = index
        self.events = []
        self.length = 0
# ---
def test_select_exists(self, connection):
        stuff = self.tables.stuff
        eq_(
            connection.execute(
                select([literal(1)]).where(
                    exists().where(stuff.c.data == "some data")
                )
            ).fetchall(),
            [(1,)],
        )
# ---
def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        raise NotImplementedError
# ---
def __enter__(self):
        """Context manager entry - start the worker."""
        self.start()
        return self
# ---
def __exit__(self, *args):
        self.stop()
        self.reset()
# ---
def set_last_purchase_rate(self, new_name):
		last_purchase_rate = get_last_purchase_details(new_name).get("base_rate", 0)
		frappe.db.set_value("Item", new_name, "last_purchase_rate", last_purchase_rate)
# ---
def testInitialization(self):
    """Tests the initialization."""
    event_formatter = chrome_cache.ChromeCacheEntryEventFormatter()
    self.assertIsNotNone(event_formatter)
# ---
def main():
    app = QtGui.QApplication(sys.argv)
    window=PreprocessDialog()

    window.show()

    sys.exit(app.exec_())
# ---
def run_hooks(self, info: StepInfo, force: bool = False):
        for hook in self.hooks:
            if force or info.step % hook.every == 0:
                hook.fn.on_step(info, force=force)
# ---
def _get_max_disk_size(populated_size, size):
    if populated_size is None:
        return size
    if size is None:
        return populated_size
    return str(max(int(populated_size), int(size)))
# ---
def create(self, model: M) -> EmaModelAveraging[M]:
        return EmaModelAveraging(model=model, beta=self.beta)
# ---
def callInit(self):
        """
        Handle calls to `__init__` methods of extensions of the MainWindow.
        """
        for init in self.__inits:
            init(self)
# ---
def clean_segundo_numero(self):
        cleaned_data = self.cleaned_data

        telefone = Telefone()
        telefone.tipo = self.data['segundo_tipo']
        telefone.ddd = self.data['segundo_ddd']
        telefone.numero = self.data['segundo_numero']
        telefone.principal = self.data['segundo_principal']

        cleaned_data['segundo_telefone'] = telefone
        return cleaned_data
# ---
def __checkArgumentFormat( self, path ):
    if type( path ) in types.StringTypes:
      urls = {path:False}
    elif type( path ) == types.ListType:
      urls = {}
      for url in path:
        urls[url] = False
    elif type( path ) == types.DictType:
      urls = path
    else:
      return S_ERROR( "TransformationClient.__checkArgumentFormat: Supplied path is not of the correct format." )
    return S_OK( urls )
# ---
def _make_grug_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    # We only require a mesh context here so the loss can provide `out_sharding=...`.
    mesh_devices = np.array(devices).reshape(len(devices), 1)
    return Mesh(
        mesh_devices,
        axis_names=("data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit),
    )
# ---
def do_wrap(self) -> bool:
        wrap_mode = self.options.get("mdformat", {}).get("wrap", DEFAULT_OPTS["wrap"])
        return isinstance(wrap_mode, int) or wrap_mode == "no"
# ---
def out_qdq_bwd(compute_dtype, res, g):
    scale, amax_history = res
    q_g, new_scale, new_history = qdq_and_return(g, jnp.float8_e5m2, scale, amax_history, compute_dtype)
    return q_g, new_scale, new_history
# ---
def detect_paired_end(path_fastqs):
    path_fastqs = [f for f in path_fastqs if f.endswith('.fastq') or f.endswith('.fq') or f.endswith('.fastq.gz') or f.endswith('.fq.gz')]
    if len(path_fastqs) % 2 == 1: return False, [path_fastqs, None, None, None]
    pair_obj = match_pairs(path_fastqs, True)
    path_fastqs = pair_obj[0]
    if pair_obj[1]==None: return False, pair_obj
    return True, pair_obj
# ---
def test_sample_returns_entry(bank):
    rng = random.Random(42)
    entry = bank.sample("If", rng)
    assert entry is not None
    assert entry.node_type == "If"
    assert len(entry.source) > 0
# ---
def maximize(self):
        """Maximize the window.

        The behaviour of this method is somewhat dependent on the user's
        display setup.  On a multi-monitor system, the window may maximize
        to either a single screen or the entire virtual desktop.
        """
        raise NotImplementedError('abstract')
# ---
def show_progress(self):
        self.command('show_progress')
# ---
def _on_Server(self, host, port, use_ssl, connect_timeout, get_info=None,
                   tls=None):
        # mangle request packet

        return "FakeServerObject"
# ---
def test_brackets_balanced_empty():
    assert brackets_balanced("")
# ---
def remove_endpoint(self, endpoint_id: str) -> ControllerEndpoint | None:
        with self._lock:
            endpoint = self._endpoints.pop(endpoint_id, None)
            if endpoint:
                # Remove from task tracking
                for task_endpoints in self._endpoints_by_task.values():
                    task_endpoints.discard(endpoint_id)
            return endpoint
# ---
def visit_mphantom(self, element):
        content = self._visit_children(element)
        return BracedNode(f"\\phantom{{{content}}}")
# ---
def gcloud_config():
    client = storage.Client()
    out: dict[str, str | None] = {
        "project": client.project,
    }
    try:
        out["zone"] = get_default_zone()
    except subprocess.CalledProcessError:
        out["zone"] = None

    return out
# ---
def list_image_metadata(self, image_id):
        """Lists all metadata items for an image."""
        resp, body = self.get("images/%s/metadata" % str(image_id))
        body = json.loads(body)
        self.validate_response(schema.image_metadata, resp, body)
        return service_client.ResponseBody(resp, body['metadata'])
# ---
def draw(self, x, y):
        gl.glPushAttrib(gl.GL_ENABLE_BIT | gl.GL_CURRENT_BIT)
        gl.glColor4f(1, 1, 1, 1)
        gl.glEnable(gl.GL_BLEND)
        gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE_MINUS_SRC_ALPHA)
        self.texture.blit(x - self.hot_x, y - self.hot_y, 0)
        gl.glPopAttrib()
# ---
def score_autocomplete(stack):
    points_autocomplete = {')': 1, ']': 2, '}': 3, '>': 4}
    s_auto = 0

    for char in stack:
        s_auto *= 5
        s_auto += points_autocomplete[char]

    return s_auto
# ---
def test_less_than(self):
        """Test that cmp_version compares a as less than b"""
        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.5') < 0)
# ---
def from_minutes(cls, minutes: int) -> "Duration":
        """Create duration from minutes."""
        return cls(minutes * 60 * 1000)
# ---
def load_tokenizer(tokenizer_path: Path):
    return AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)
# ---
def _check_size_consistency(
    spec1: AxisSelection, spec2: AxisSelection, name: str, size1: int | None, size2: int | None
):
    if size1 is not None and size2 is not None and size1 != size2:
        raise ValueError(f"Axis {name} has different sizes in {spec1} and {spec2}: {size1} != {size2}")
# ---
def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1, df2])
            return (A3.two == 'foo').sum()
# ---
def is_on(self):
        """Return true if device is on."""
        return self._state
# ---
def test_validate_edit_valid():
    source = "x = 1 + 2\n"
    mutation = Mutation(start=4, end=9, replacement="3 * 4", node_type="BinOp", original="1 + 2")
    assert validate_edit(source, mutation)
# ---
def _schedulable(self, node=None, selector=None, schedulable=True):
        ''' perform oadm manage-node scheduable '''
        cmd = ['manage-node']
        if node:
            cmd.extend(node)
        else:
            cmd.append('--selector={}'.format(selector))

        cmd.append('--schedulable={}'.format(schedulable))

        return self.openshift_cmd(cmd, oadm=True, output=True, output_type='raw')
# ---
def testTryFinally(self):
    result = _GrumpRun(textwrap.dedent("""\
        try:
          print 'foo',
        finally:
          print 'bar'
        try:
          print 'foo',
          raise Exception
        finally:
          print 'bar'"""))
    self.assertEqual(1, result[0])
    self.assertIn('foo bar\nfoo bar\n', result[1])
    self.assertIn('Exception\n', result[1])
# ---
def __init__(
        self,
        config: WeightTransferConfig,
        axis_mapping: ResourceMapping | None = None,
        mesh: Mesh | None = None,
    ):
        self.config = config
        self.axis_mapping = axis_mapping
        self.mesh = mesh
        self.weight_step = -1
        self.metrics = WeightTransferClientMetrics()
# ---
def author():
    return make_author()
# ---
def always(self):
        return True
# ---
def load_web_rules(chainlink_dir):
    """Load web.md rules from .chainlink/rules/."""
    if not chainlink_dir:
        return get_fallback_rules()

    rules_path = os.path.join(chainlink_dir, 'rules', 'web.md')
    try:
        with open(rules_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except (OSError, IOError):
        return get_fallback_rules()
# ---
def introduced_elements(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'episome',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transient transfection'],
        'introduced_elements': 'genomic DNA regions'
    }
# ---
def output_hrule(self):
        return self.renderer.hrule()
# ---
def __init__(self, name, fileas = None, role = 'aut'):
        '''Initialize the object. If the argument "fileas" is not given,
        "Last-name, First-name" is used for the file-as attribute. If
        the argument "role" is not given, "aut" is used for the role
        attribute.'''
        if not fileas:
            fileas = _normalize(name)
        self.tag = 'dc:creator'
        self.text = name
        self.attr = (('opf:file-as', fileas), ('opf:role', role))
# ---
def to_kebab_case(name: str) -> str:
    """Convert PascalCase to kebab-case."""
    return re.sub(r"(?<!^)(?=[A-Z])", "-", name).lower()
# ---
def _delete_tpu(name: str, zone: str, project: str) -> bool:
    result = _run_gcloud(
        ["compute", "tpus", "tpu-vm", "delete", name, f"--project={project}", f"--zone={zone}", "--quiet"]
    )
    if result.returncode != 0:
        error = result.stderr.strip()
        if "not found" in error.lower():
            click.echo(f"  TPU {name} already deleted")
            return True
        click.echo(f"  Failed to delete TPU {name}: {error}", err=True)
        return False
    return True
# ---
def test_numpy_inputs(self):
    if context.executing_eagerly():
      layer = keras.layers.RepeatVector(2)
      x = np.ones((10, 10))
      self.assertAllEqual(np.ones((10, 2, 10)), layer(x))

      layer = keras.layers.Concatenate()
      x, y = np.ones((10, 10)), np.ones((10, 10))
      self.assertAllEqual(np.ones((10, 20)), layer([x, y]))
# ---
def __len__(self):
        return len(self.subqueries)
# ---
def __call__(self, target, cred):
        """Triggers if instance of the class is called.

        Performs the check. Returns False to reject the access or a
        true value (not necessary True) to accept the access.
        """

        pass
# ---
def test_keras_mask(self):
    x = np.ones((10, 10))
    y = keras.layers.Masking(1.)(x)
    self.assertTrue(hasattr(y, '_keras_mask'))
    self.assertTrue(y._keras_mask is not None)
    self.assertAllClose(self.evaluate(y._keras_mask), np.zeros((10,)))
# ---
def failing_generator(items):
        for item in items:
            if item == 3:
                raise ValueError("Test error")
            yield item
# ---
def do_fold(init: CarryT, *args, **kwargs) -> CarryT:
            return haliax.fold(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *args, **kwargs)
# ---
def test___cmp__gt(self):
        self._test__cmp__(
            lambda left, right: left > right,
            (
                True,
                False,
                False,
                False,
                False,
                TypeError if PY3 else True,
                TypeError if PY3 else False,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
            ),
            '>'
        )
# ---
def _draw_item(self, win, data, inverted):

        if data['type'] == 'MoreComments':
            return self._draw_more_comments(win, data)
        elif data['type'] == 'HiddenComment':
            return self._draw_more_comments(win, data)
        elif data['type'] == 'Comment':
            return self._draw_comment(win, data, inverted)
        else:
            return self._draw_submission(win, data)
# ---
def _testGpuMatmul(self, x, y, transpose_x=False, transpose_y=False):
    x_mat = np.matrix(x).T if transpose_x else np.matrix(x)
    y_mat = np.matrix(y).T if transpose_y else np.matrix(y)
    np_ans = x_mat * y_mat
    with self.test_session(use_gpu=True):
      tf_ans = tf.matmul(x, y, transpose_x, transpose_y).eval()
    self.assertAllClose(np_ans, tf_ans)
    self.assertAllEqual(np_ans.shape, tf_ans.shape)
# ---
def format_exception_with_traceback(exc: Exception) -> str:
    """Format an exception with its full traceback as a string.

    Suitable for embedding in JobStatus.error field.
    """
    tb = "".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
    return f"{type(exc).__name__}: {exc}\n\nTraceback:\n{tb}"
# ---
def status(self) -> JobStatus:
        if self._ref is not None:
            return self._poll_ref()
        return self._poll_submission()
# ---
def test_impl(df):
            return df.A.str.split()
# ---
def test_no_duplicates():
    variants = generate_expression_variants("a + b", max_variants=20)
    assert len(variants) == len(set(variants))
# ---
def axis_spec_to_tuple(axis_spec: AxisSelection) -> tuple[AxisSelector, ...]:
    if isinstance(axis_spec, Mapping):
        return tuple(Axis(name, size) if size is not None else name for name, size in axis_spec.items())

    if isinstance(axis_spec, Axis | str):
        return (axis_spec,)

    if isinstance(axis_spec, Sequence):
        return tuple(axis_spec)

    raise ValueError(f"Invalid axis spec: {axis_spec}")
# ---
def tmpfile(testdir):
    f = testdir.makepyfile("").open('wb+')
    yield f
    if not f.closed:
        f.close()
# ---
def set_asset_naming_series(self):
		if not hasattr(self, '_asset_naming_series'):
			from erpnext.assets.doctype.asset.asset import get_asset_naming_series
			self._asset_naming_series = get_asset_naming_series()

		self.set_onload('asset_naming_series', self._asset_naming_series)
# ---
def fake_join_subordinate(id, compute_uuid, host, url, user, password):
            fake_join_subordinate.called = True
# ---
def _description_sortable(self):
        return self.store or (self.inherited and self.related_field._description_sortable)
# ---
def initialize(self, io_loop=None, executor=None):
        self.io_loop = io_loop or IOLoop.current()
        self.executor = executor or dummy_executor
# ---
def __repr__(self):
        return '{0.__class__.__name__}({0.slot!r})'.format(self)
# ---
def create_datasets():
    ds1 = ListAsyncDataset([1, 2, 3, 4, 5])
    ds2 = ListAsyncDataset([10, 20, 30, 40, 50])
    ds3 = ListAsyncDataset([100, 200, 300, 400, 500])
    ds1.finalize()
    ds2.finalize()
    ds3.finalize()
    return {"ds1": ds1, "ds2": ds2, "ds3": ds3}
# ---
def _is_int(x: float) -> bool:
    try:
        return abs(x - int(round(x))) <= 1e-7
    except (ValueError, OverflowError):
        return False
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return df.two.nunique()
# ---
def visitdir(self, dir):
        matched = self._matcher.match_recursive(dir)
        if matched is None:
            return True
        elif matched is True:
            return "all"
        else:
            assert matched is False
            return False
# ---
def to_sharding(node: typing.Any, spec: typing.Any):
        if spec is None:
            if isinstance(node, NamedArray):
                return getattr(node.array, "sharding", None)
            elif is_jax_array_like(node):
                return getattr(node, "sharding", None)
            else:
                return None
        else:
            return NamedSharding(resolved_mesh, spec)
# ---
def test_capture_conftest_runtest_setup(testdir):
    testdir.makeconftest("""
        def pytest_runtest_setup():
            print ("hello19")
    """)
    testdir.makepyfile("def test_func(): pass")
    result = testdir.runpytest()
    assert result.ret == 0
    assert 'hello19' not in result.stdout.str()
# ---
def parse_paragraph(self, m):
        text = m.group(1).rstrip('\n')
        self.tokens.append({'type': 'paragraph', 'text': text})
# ---
def lower(self, *args, **kwargs) -> jax.stages.Lowered:
        return self._call(True, *args, **kwargs)
# ---
def on_key_release(symbol, modifiers):
            """A key on the keyboard was released.

            :Parameters:
                `symbol` : int
                    The key symbol pressed.
                `modifiers` : int
                    Bitwise combination of the key modifiers active.

            :event:
            """
# ---
def test_bound_in_scalar(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.in_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [(2,), (3,), (4,)], params={"q": [2, 3, 4]})
# ---
def test_is_none_1(self):
        self.assertIsNone(string_color('a'))
# ---
def yesno(question, default="y"):
    "Ask the user a yes/no question"
    while True:
        sys.stdout.write("{} (y/n) [{}]: ".format(question, default))
        answer = sys.stdin.readline().strip().lower()
        if len(answer) == 0:
            answer = default
        if answer == "y":
            return True
        elif answer == "n":
            return False
# ---
def sample_data(self):
        pass
# ---
def test_vmap_mapped_args():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Batch, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, Batch)(named1)

    expected_jax = jnp.array([named1.sum(Width).array for _ in range(Batch.size)])
    expected_names = (Batch, Depth)

    assert jnp.all(jnp.equal(selected.array, expected_jax))
    assert selected.axes == expected_names
# ---
def _create_app(self) -> ActorServiceASGIApplication:
        return ActorServiceASGIApplication(service=self)
# ---
def test_helper():
    return "test helper text"
# ---
def test_encrypt_simple_message(self):
        self._test_encryption('This is a simple message.')
# ---
def log(self, action: str, entity_id: str, **details: Any) -> None:
        """Record an action taken during event handling."""
        self.actions.append(
            Action(
                timestamp=Timestamp.now(),
                action=action,
                entity_id=str(entity_id),
                details=details,
            )
        )
# ---
def ready_count(self) -> int:
        """Number of actors that are available for RPC."""
        ...
# ---
def testPrintStatement(self):
    self.assertEqual((0, 'abc 123\nfoo bar\n'), _GrumpRun(textwrap.dedent("""\
        print 'abc',
        print '123'
        print 'foo', 'bar'""")))
# ---
def find_packages():
        return list(_find_packages(mn.__path__, mn.__name__))
# ---
def test_pass_different_length_seq(num_kv_heads):
    config = MistralConfig(
        max_seq_len=64,
        hidden_dim=32,
        intermediate_dim=32,
        num_heads=2,
        num_kv_heads=num_kv_heads,
        use_flash_attention=True,
    )
    check_model_works_with_seqlen(MistralLMHeadModel, config, 16)
# ---
def health_check(self, request: cluster__pb2.Empty, ctx: RequestContext) -> cluster__pb2.Worker.HealthResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def model_config():
    config = ModelConfig(
        name="test-llama-200m",
        path="gs://marin-us-east5/gcsfuse_mount/perplexity-models/llama-200m",
        engine_kwargs={"enforce_eager": True, "max_model_len": 1024},
        generation_params={"max_tokens": 16},
    )
    return config
# ---
from typing import List


def rescale_to_unit(numbers: List[float]) -> List[float]:
    """ Given list of numbers (of at least two elements), apply a linear transform to that list,
    such that the smallest number will become 0 and the largest will become 1
    >>> rescale_to_unit([1.0, 2.0, 3.0, 4.0, 5.0])
    [0.0, 0.25, 0.5, 0.75, 1.0]
    """
    min_number = min(numbers)
    max_number = max(numbers)
    return [(x - min_number) / (max_number - min_number) for x in numbers]
# ---
def test_impl(df):
            A = df.A.str.split(',')
            return pd.Series(list(itertools.chain(*A)))
# ---
def _handle_heartbeat_failure(self, worker: ControllerWorker) -> None:
        """Handle a failed heartbeat RPC via the state layer."""
        self._state.handle_event(
            WorkerHeartbeatFailedEvent(
                worker_id=worker.worker_id,
                error=f"Heartbeat failed for worker {worker.worker_id}",
            )
        )
# ---
def default_choice_name(cls) -> Optional[str]:
        return "wandb"
# ---
def __call__(self, target, cred):
        """Check the policy.

        Requires that at least one rule accept in order to return True.
        """

        for rule in self.rules:
            if rule(target, cred):
                return True

        return False
# ---
def __init__(self):
        self.mainFrame = gui.mainFrame.MainFrame.getInstance()
# ---
def test_no_ellipsis():
    partial_order = ("apple", "banana", "cherry")
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("apple", "banana", "cherry")
# ---
def test_capture_is_represented_on_failure_issue128(self, testdir, method):
        p = testdir.makepyfile("""
            def test_hello(cap%s):
                print ("xxx42xxx")
                assert 0
        """ % method)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "xxx42xxx",
        ])
# ---
def prefetch_rule(context: models.Context):
  # Make sure that we have the IAM policy in cache.
  project_ids = {c.project_id for c in gke.get_clusters(context).values()}
  for pid in project_ids:
    iam.get_project_policy(pid)
# ---
def test_llama_tokenizer_needs_long_sequence_workaround():
    tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-hf")
    batch_tokenizer = BatchTokenizer(tokenizer)
    assert batch_tokenizer._needs_long_sequence_workaround
# ---
def test_visualize_shardings_inside_jit(capsys):
    mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))

    @named_jit
    def fn(x):
        x = hax.shard(x)
        visualize_shardings(x)
        return x

    with axis_mapping({"dim1": ResourceAxis.DATA}), mesh:
        x = hax.ones({"dim1": 8 * len(jax.devices())})
        x = hax.shard(x)
        fn(x)

    out = capsys.readouterr().out
    assert "dim1" in out
# ---
def test_digraph(self):
        G = nx.path_graph(3, create_using=nx.DiGraph())
        c = nx.closeness_centrality(G)
        cr = nx.closeness_centrality(G.reverse())
        d = {0: 0.0, 1: 0.500, 2: 0.667}
        dr = {0: 0.667, 1: 0.500, 2: 0.0}
        for n in sorted(self.P3):
            assert almost_equal(c[n], d[n], places=3)
            assert almost_equal(cr[n], dr[n], places=3)
# ---
def main(config: InferenceReplConfig):
    """Main entry point."""
    commands = ReplContext(config)
    os.environ["EQX_ON_ERROR"] = "nan"

    # Determine mode
    if config.command:
        cli_mode(config, commands)
    else:
        repl_mode(config, commands)
# ---
def __repr__(self) -> str:
        return f"col({self.name!r})"
# ---
def log2(a: A) -> A:
    return wrap_elemwise_unary(jnp.log2, a)
# ---
def broadcast_arrays(
    *arrays: NamedOrNumeric | None, require_subset: bool = True, ensure_order: bool = True
) -> tuple[NamedOrNumeric | None, ...]: ...
# ---
def logs_tail(self, *, max_lines: int = 200) -> str:
        if self.vllm_server is None:
            raise RuntimeError("vLLM server is not running in this environment.")
        return self._backend.logs_tail(self.vllm_server, max_lines=max_lines)
# ---
def test_transaction_engine_ctx_begin_fails(self):
        engine = engines.testing_engine()

        mock_connection = Mock(
            return_value=Mock(
                        begin=Mock(side_effect=Exception("boom"))
                    )
        )
        engine._connection_cls = mock_connection
        assert_raises(
            Exception,
            engine.begin
        )

        eq_(
            mock_connection.return_value.close.mock_calls,
            [call()]
        )
# ---
def removeDirectory( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def test_raise(self):
        # raising URLError stops processing of request
        o = OpenerDirector()
        meth_spec = [
            [("http_open", "raise")],
            [("http_open", "return self")],
            ]
        handlers = add_ordered_mock_handlers(o, meth_spec)

        req = Request("http://example.com/")
        self.assertRaises(urllib.error.URLError, o.open, req)
        self.assertEqual(o.calls, [(handlers[0], "http_open", (req,), {})])
# ---
def _axis_size(self, axis: str) -> int:
        return self._mesh_axis_totals.get(axis, 1)
# ---
def test_xxh3_64_batch():
    # Batch vs. rowwise xxh3_64 parity
    inputs = [b"one", b"two", b"three", b"four"]
    expected = [hash_xxh3_64(x) for x in inputs]
    actual = hash_xxh3_64_batch(inputs)
    assert actual == expected
# ---
def f(c, x, y):
        return c, x + y
# ---
def load_environment_from_spec(config: EnvConfig) -> MarinEnv:
    """Load an environment from the given configuration."""
    env_class = config.env_class
    env_args = config.env_args
    # Dynamically import the environment class
    module_name, class_name = env_class.rsplit(".", 1)
    env_module = __import__(module_name, fromlist=[class_name])
    env_class = getattr(env_module, class_name)

    # TODO(power) - thread random seed from the rollout worker.
    return env_class(**env_args)
# ---
def __init__(self) -> None:
        self._vms: dict[str, ManagedVm] = {}
        self._lock = threading.Lock()
# ---
def __getattr__(self, name):
        """ Access non-slot field attribute. """
        try:
            return self._attrs[name]
        except KeyError:
            raise AttributeError(name)
# ---
def epoch_seconds(self) -> float:
        """Get seconds since epoch."""
        return self._epoch_ms / 1000.0
# ---


def is_palindrome(text: str):
    """
    Checks if given string is a palindrome
    >>> is_palindrome('')
    True
    >>> is_palindrome('aba')
    True
    >>> is_palindrome('aaaaa')
    True
    >>> is_palindrome('zbcd')
    False
    """
    for i in range(len(text)):
        if text[i] != text[len(text) - 1 - i]:
            return False
    return True
# ---
def __len__(self):
        return len(self._calls)
# ---
def lsof_check():
    pid = os.getpid()
    try:
        out = py.process.cmdexec("lsof -p %d" % pid)
    except (py.process.cmdexec.Error, UnicodeDecodeError):
        # about UnicodeDecodeError, see note on pytester
        pytest.skip("could not run 'lsof'")
    yield
    out2 = py.process.cmdexec("lsof -p %d" % pid)
    len1 = len([x for x in out.split("\n") if "REG" in x])
    len2 = len([x for x in out2.split("\n") if "REG" in x])
    assert len2 < len1 + 3, out2
# ---
def hard_tanh(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_tanh, a)
# ---
def token(self):
        """Attempt to return the auth header token.

        :return: token related to request
        """
        prefixes = ("Bearer", "Token")
        auth_header = self.headers.get("Authorization")

        if auth_header is not None:
            for prefix in prefixes:
                if prefix in auth_header:
                    return auth_header.partition(prefix)[-1].strip()

        return auth_header
# ---
def remember(self, request, principal, **kw):
		return self.match(request).remember(request, principal, **kw)
# ---
def test_actor_group_create_and_wait_ready(client: LocalClient):
    group = client.create_actor_group(Counter, name="counters", count=3)
    assert group.ready_count == 3
    handles = group.wait_ready()
    assert len(handles) == 3

    for i, h in enumerate(handles):
        h.increment.remote(i + 1).result()

    values = [h.get.remote().result() for h in handles]
    assert values == [1, 2, 3]
# ---
def do_fold(init, *args, **kwargs):
            carry = init
            for i, block in enumerate(self.blocks):
                (block_args, block_kwargs) = haliax.tree_util.tree_map(
                    functools.partial(BlockSeq._slice_out, self.Block, i),
                    (args, kwargs),
                )
                carry = block(carry, *block_args, **block_kwargs)
                carry = tree_checkpoint_name(carry, self._carry_ckpt_name)
            return carry
# ---
def eliminate_axes(axis_spec: AxisSelection, axes: AxisSelection) -> AxisSelection:  # type: ignore
    ...
# ---
def the_tokenizer(self):
        """Load and return the tokenizer from the specified path."""
        return load_tokenizer(self.tokenizer)
# ---
def scan_fn(_, i):
            slice = x_ref.slice({"x": i})
            slice[...] = jnp.sin(x * i)
            return None, None
# ---
def sort_by_name(self, reverse=False):
        Util.validate_type(reverse, "bool")
        return self._sort_by_name(reverse)
# ---
def stop_tokens(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    stop_tokens = tokenizer.eos_token_id
    print("STOP", tokenizer.eos_token, stop_tokens)
    return [stop_tokens]
# ---
def is_in_jit():
    return isinstance(jnp.zeros((), dtype=jnp.float32), jax.core.Tracer)
# ---
def init(max_queued_tokens: int) -> "TokenQueue":
        """Create a ``JitScheduler`` with empty buffers."""
        return TokenQueue(
            queued_tokens=hax.full({"position": max_queued_tokens}, INVALID, dtype=jnp.int32),
            queued_slot_ids=hax.full({"position": max_queued_tokens}, INVALID, dtype=jnp.int32),
            queued_pos_ids=hax.full({"position": max_queued_tokens}, INVALID, dtype=jnp.int32),
            num_queued_tokens=jnp.array(0, dtype=jnp.int32),
        )
# ---
def is_alive(self) -> bool:
        """True if any thread in this container or its children is still running."""
        with self._lock:
            threads = list(self._threads)
            children = list(self._children)
        return any(t.is_alive for t in threads) or any(c.is_alive for c in children)
# ---
def add(self, duration: Duration) -> "Timestamp":
        """Return new timestamp offset by duration."""
        return Timestamp(self._epoch_ms + duration.to_ms())
# ---
def fsspec_mtime(file_path: str) -> datetime:
    """Get file modification time (in seconds since epoch) of a file on an `fsspec` filesystem."""
    fs = fsspec.core.url_to_fs(file_path)[0]

    return fs.modified(file_path)
# ---
def f(olmo2_model, input_ids, mask):
        out = olmo2_model(input_ids, mask)
        return hax.sum(out).scalar()
# ---
def copy_page(self, src_page: int, dst_page: int) -> "ListCache[PageCacheT]":
        return ListCache(tuple(cache.copy_page(src_page, dst_page) for cache in self.caches))
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        keys = hax.jax_utils.maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.blocks.fold(x, key=keys)
        x = self.ln_f(x)

        return x
# ---
def raw_loss_fn(model, batch):
        val = hax.mean(batch["value"])
        if has_metrics:
            return val, {"metric_a": val * 2, "metric_b": val + 10}
        return val
# ---
def _on_record_button_released(self):
        if self._should_trigger_recording:
            self._trigger_recording()

        self._should_trigger_recording = True
# ---
def __init__(self):
            raise NotImplementedError('deprecated')
# ---
def set_default_settings(settings: QSettingsManager):
    settings.set_defaults({
        DECIMAL_SETTING: ',',
        SEPARATOR_SETTING: ';'
    })
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"_layers": "layers"}
# ---
def _get_renderer(model_name: str, tokenizer) -> Renderer:
        """Get the appropriate renderer based on model name."""
        model_name_lower = model_name.lower()
        if "qwen" in model_name_lower:
            return Qwen3Renderer(tokenizer)
        elif "llama" in model_name_lower:
            return Llama3Renderer(tokenizer)
        else:
            raise ValueError(f"Unsupported model type for {model_name}. Only Qwen3 and Llama3.1 models are supported.")
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' or '.join(str(r) for r in self.rules)
# ---
def _pspec_parts(spec_part) -> str:
    if spec_part is None:
        return "unsharded"
    elif isinstance(spec_part, (tuple, list)):
        return "+".join(str(p) for p in spec_part)
    else:
        return str(spec_part)
# ---
def gumbel(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.gumbel(key, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def test_expr_in_set(self):
        expr1 = col("score") > 0.5
        expr2 = col("score") > 0.5
        s = {expr1}
        assert expr2 in s
# ---
def Pos(self) -> Axis:
        return Axis("position", self.grug_config.max_seq_len)
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        """Convert to proto for RPC APIs."""
        return vm_pb2.SliceInfo(
            slice_id=self._slice_id,
            scale_group=self._scale_group,
            created_at=self._created_at.to_proto(),
            vms=[vm.info for vm in self._vms],
        )
# ---
def reset_chaos() -> None:
    _rules.clear()
# ---
def stop(self, timeout: Duration = Duration.from_seconds(10.0)) -> None:
        pass
# ---
def selected_target(self, value):
        self._selected_target = value
        self._match_entries()
# ---
def _transition(self, new_state: vm_pb2.VmState) -> None:
        """Update state with timestamp and log the transition."""
        old_state = self.info.state
        self.info.state = new_state
        self.info.state_changed_at.CopyFrom(Timestamp.now().to_proto())
        if self._phase:
            self.info.init_phase = self._phase
        logger.info("VM %s: %s -> %s", self.info.vm_id, vm_state_name(old_state), vm_state_name(new_state))
# ---
def get_calibration_widget(self):
        return self._calibration_widget
# ---
def __call__(self, target, cred):
        """Check the policy."""

        return False
# ---
def __tree_pp__(self, **kwargs):
        # For Equinox's tree pretty printer
        import jax._src.pretty_printer as pp

        if kwargs.get("short_arrays", True) and is_jax_array_like(self.array):
            return pp.text(f"Named({self.dtype}{self.shape})")
        else:
            return pp.text(str(self))
# ---
def cursor_execute(conn, *args, **kw):
            canary.append('cursor_execute')
# ---
def _interpret(self):
        if not self.max:
            return

        try:
            count = int("".join(self.max))
        except ValueError:
            assert False, "internal error: cannot convert to number maximum of repetition"
        self.repeat.between.max = count
# ---
def getManagerInfo(atomFeed):
	try:
		entries = atomFeed.getElementsByTagName('entry')[1]
	except:
		return None
	try:
		managerId = entries.getElementsByTagName('snx:userid')[0]
		return managerId.firstChild.data
	except:
		return None
# ---
def __call__(self, x):
            hidden = hax.dot(Hidden, x, self.w1)
            hidden = hax.tanh(hidden)
            return hax.dot(Mlp, hidden, self.w2)
# ---
def update_str(s):
      update_num(len(s))
      hasher.update(compat.as_bytes(s))
# ---
def _get_size(n_items):
    """
    Calculate the size of the subplot layouts based on number of items.
    """
    n_cols = math.ceil(math.sqrt(n_items))
    n_rows = math.floor(math.sqrt(n_items))
    if n_cols * n_rows < n_items:
        n_cols += 1
    return int(n_rows), int(n_cols)
# ---
def selected_pane_index(self):
        return self._selected_pane_index
# ---
def response_tokens_from_choice(self, choice: Choice) -> np.ndarray:
        """Extract response token IDs directly from the choice.

        Uses the response_token_ids attached during vLLM-to-OpenAI conversion,
        avoiding the lossy convert_ids_to_tokens/convert_tokens_to_ids round-trip
        that fails for padding token IDs not in the tokenizer vocabulary.
        """
        return np.array(choice.response_token_ids, dtype=np.int32)
# ---
def cumsum(self, axis: AxisSelector, *, dtype=None) -> "NamedArray":  # pragma: no cover
        return haliax.cumsum(self, axis=axis, dtype=dtype)
# ---
def _flatmap_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield from fn(item)
# ---
def is_old(node_info):
    '''
    Check if node or node.bl_idname is among
    the old nodes
    '''
    if isinstance(node_info, str):
        # assumes bl_idname
        return node_info in old_bl_idnames
    elif isinstance(node_info, bpy.types.Node):
        return node_info.bl_idname in old_bl_idnames
    else:
        return False
# ---
def _find_packages(path='.', prefix=''):
        yield prefix
        prefix = prefix + "."
        for _, name, ispkg in walk_packages(path,
                                            prefix,
                                            onerror=lambda x: x):
            if ispkg:
                yield name
# ---
def get_dir(self, name: str, description: str) -> Path:
        """Register and return path for a directory artifact."""
        path = self._root / name
        path.mkdir(parents=True, exist_ok=True)
        self._artifacts.append(LogArtifact(path=path, description=description))
        return path
# ---
def conj(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.conj(), self.axes)
# ---


def triangle_area(a, h):
    """Given length of a side and high return area for a triangle.
    >>> triangle_area(5, 3)
    7.5
    """
    return a * h / 2.0
# ---
def do_scan(init, *extra_args, **extra_kwargs):
            carry, out = haliax.scan(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *extra_args, **extra_kwargs)
            return carry, out
# ---
def test_encode_source_ascii(tok):
    source = "abc"
    ids = tok.encode_source(source)
    assert len(ids) == 3
    # Each should be in the base token range.
    for tid in ids:
        assert tid >= tok.base_token_offset
        assert tid < tok.base_token_offset + tok.base_vocab_size
# ---
def get_match_urls(response):
    for match in response.xpath("//a[contains(@href, 'stats/games/')]/@href").extract():
                yield response.urljoin(match)
# ---
def docker_runtime(docker_cleanup_scope):
    """DockerRuntime that cleans up containers and images created during test."""
    yield DockerRuntime()
# ---
def __iter__(self):
        """
        Iterator for CmdText object.
        """
        for l in self.lines:
            yield l
# ---
def dem_threshold(domain, b):
    '''Just use a height threshold on the DEM!'''

    heightLevel = float(domain.algorithm_params['dem_threshold'])
    dem         = domain.get_dem().image
    return dem.lt(heightLevel).select(['elevation'], ['b1'])
# ---
def global_reducer(sketches: Iterator[DDSketch]) -> DDSketch:
        """Merge all shard sketches into one."""
        combined = DDSketch()
        for sketch in sketches:
            combined.merge(sketch)
        return combined
# ---
def test_actor_named_get_if_exists(job_context):
    actor1 = job_context.create_actor(SimpleActor, 100, name="test_actor", get_if_exists=True)
    future1 = actor1.increment.remote(10)
    job_context.get(future1)

    actor2 = job_context.create_actor(SimpleActor, 999, name="test_actor", get_if_exists=True)
    future2 = actor2.increment.remote(0)
    assert job_context.get(future2) == 110
# ---
def kill():
    backProc.terminate()
    return 'killed: ' + str(backProc.pid)
# ---
def _root_key_m(self):
        return "LicenseInfo"
# ---
def cli():
    """Iris Worker - Job execution daemon."""
    pass
# ---
def test_lambda_output_shape(self):
    l = keras.layers.Lambda(lambda x: x + 1, output_shape=(1, 1))
    l(keras.backend.variable(np.ones((1, 1))))
    self.assertEqual((1, 1), l.get_config()['output_shape'])
# ---
def rfftfreq(axis: Axis, d: float = 1.0) -> NamedArray:
    """Named version of :func:`jax.numpy.fft.rfftfreq`."""

    new_axis = axis.resize(axis.size // 2 + 1)
    return NamedArray(jfft.rfftfreq(axis.size, d), (new_axis,))
# ---
def get(self, ref):
        return ray.get(ref)
# ---
def to_column(self):
        self.column = fields.integer(self.string)
        return self.column
# ---
def test_eq(self):
        """
        Two instances with equal C{__dict__}s are equal.
        """
        assert _api._SimpleNamespace(a=1) == _api._SimpleNamespace(a=1)
# ---
def is_task(self) -> bool:
        """True if this is a task (last component is numeric)."""
        return self.task_index is not None
# ---
def traslado_iva(self):
        return self.__triva
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("x", Integer),
            Column("y", Integer),
        )
# ---
def extent(self):
        return reduce(operator.add, [c.extent for c in self.coverages])
# ---
def test_capture_results_accessible_by_attribute(self):
        with self.getcapture() as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            capture_result = cap.readouterr()
        assert capture_result.out == "hello"
        assert capture_result.err == "world"
# ---
def run(self):
        try:
            super().run()
        except Exception as e:
            self._exception = e
# ---
def test_start_with_ellipsis():
    partial_order = (..., "apple", "banana")
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("cherry", "apple", "banana")
# ---
def scan_fn(_, x):
            ref_slice = ref.slice({"x": x})
            ref_slice[...] = (x * x).astype(ref_slice.dtype)
            return None, x * 2
# ---
def __call__(self, batch: Sequence[Sequence[int]]):
                raise RuntimeError("This should not be called")
# ---
def sources_list(self, mode='short'):
        ret = {}
        mode = bottle.request.query.mode or mode
        for name, spec in self.ndb.sources.items():
            ret[name] = {'class': spec.nl.__class__.__name__,
                         'status': spec.status}
            if mode == 'full':
                ret[name]['config'] = spec.nl_kwarg
        return bottle.template('{{!ret}}', ret=json.dumps(ret))
# ---
def build(self, ctx: LrScheduleContext):
        def schedule(step):
            tokens_trained = step * self.batch_size * self.seq_length
            return jnp.minimum(ctx.learning_rate, self.batch_size * self.a * tokens_trained**self.b)

        return schedule
# ---
def call_kernel(q_b, k_b, v_b, si, sink):
            if sink is None:
                return kernel(q_b, k_b, v_b, segment_ids=si)
            return kernel(q_b, k_b, v_b, segment_ids=si, sinks=sink)
# ---
def build_image():
    """Build and push the TPU CI Docker image to ghcr.io."""
    logging.info("Building TPU CI Docker image...")
    build_and_push_docker_image()
    logging.info("âœ“ Image build complete")
# ---
def scopes_get(self):
        return self._scopes
# ---
def _load_file_gen(stream: Iterator) -> Iterator:
    from zephyr.readers import load_file

    for spec in stream:
        try:
            yield from load_file(spec)
        except Exception as e:
            logger.exception(f"Failed to load from {spec}")
            raise RuntimeError(f"Failed to load from {spec}: {e}") from e
# ---
def unbounded_config() -> config_pb2.ScaleGroupConfig:
    """A scale group with no min/max constraints."""
    return config_pb2.ScaleGroupConfig(
        name="unbounded-group",
        min_slices=0,
        max_slices=100,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def check_docker_available():
    """Check if Docker is available and running."""
    try:
        result = subprocess.run(
            ["docker", "info"],
            check=True,
            capture_output=True,
            timeout=5,
        )
        return result.returncode == 0
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False
# ---
def load_dataset_with_backoff(
    *,
    context: str,
    max_attempts: int = 6,
    initial_delay: float = 2.0,
    max_delay: float = 120.0,
    logger: logging.Logger | None = None,
    **dataset_kwargs: Any,
):
    return call_with_hf_backoff(
        lambda: datasets.load_dataset(**dataset_kwargs),
        context=context,
        max_attempts=max_attempts,
        initial_delay=initial_delay,
        max_delay=max_delay,
        logger=logger,
    )
# ---
def define_tables(cls, metadata):
        cls.table = Table('exec_test', metadata,
            Column('a', Integer),
            Column('b', Integer),
            test_needs_acid=True
        )
# ---
def test_spawn_iso_glance(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_ISO, None, None,
                         os_type="windows", architecture="i386")
        self.check_vm_params_for_windows()
# ---
def test_conf_default(self):
        with mock.patch.object(memcache, 'ConfigParser', EmptyConfigParser):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '127.0.0.1:11211')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, False)
        self.assertEqual(
            app.memcache._client_cache['127.0.0.1:11211'].max_size, 2)
# ---
def __iter__(self):
        return iter(self.options)
# ---
def _parse_hits(self, hits, resource):
        """Parse hits response into documents."""
        datasource = self._datasource(resource)
        schema = config.DOMAIN[datasource[0]]['schema']
        dates = get_dates(schema)
        docs = []
        for hit in hits.get('hits', {}).get('hits', []):
            docs.append(format_doc(hit, schema, dates))
        return ElasticCursor(hits, docs)
# ---
def physical_axis_name(axis: AxisSelector, mapping: ResourceMapping | None = None) -> PhysicalAxisSpec | None:
    """Get the physical axis name for a logical axis from the mapping. Returns none if the axis is not mapped."""
    if mapping is None:
        mapping = current_thread_local_mapping()
    if mapping is None:
        return None
    elif isinstance(axis, str):
        return mapping.get(axis, None)
    else:
        return mapping.get(axis.name, None)
# ---
def run_command(*args, **kwargs):
    print("Running:", " ".join(list(args)))
    return subprocess.check_call(args, **kwargs)
# ---
def list_workers(limit: int = 10000) -> list[dict[str, Any]]:
    """Get list of Ray workers."""
    result = run_ray_command(
        ["ray", "list", "workers", "--format=json", f"--limit={limit}"],
    )
    return json.loads(result.stdout)
# ---
def test_replicas_on_job_request():
    """JobRequest.replicas is the gang-scheduling count (maps to num_slices for TPU)."""
    request = JobRequest(
        name="multi-slice",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=TpuConfig(variant="v4-8")),
        replicas=4,
    )
    assert request.replicas == 4
# ---
def test_set_enable_host_enable(self):
        self._test_host_action(self.conn.set_host_enabled, True, 'enabled')
# ---
def build(self) -> optax.GradientTransformation:
        return clip_update_by_historical_norm(self.rolling_interval_length, self.sigma_factor)
# ---
def KVCombinedSize(self) -> Axis:
        return Axis("kv_combined", self.kv_lora_rank + self.qk_rope_head_dim)
# ---
def __init__(self, model_name: str, attribute_name: str, model_type: str | None, *args, **kwargs):
        self.model_name = model_name
        self.model_type = model_type
        self.attribute_name = attribute_name
        self.cls = self.from_model_path(model_name, attribute_name, model_type, *args, **kwargs)
# ---
def process():
    """ Process submitted form data. """
    format = request.form['format']

    try:
        node = {
            'png': 'map_png',
            'svg': 'map_svg',
            'jpg': 'map_jpg',
        }[format]
    except KeyError:
        flash("The output format you selected is not supported.")
        return redirect(url_for('error'))
    else:
        return redirect(url_for(node, _method='POST'), code=307)
# ---
def to_seconds(time, unit):
    if unit == 's':
        return float(time)
    elif unit == 'm':
        return float(time) * 60
    elif unit == 'h':
        return float(time) * 60 * 60
    elif unit == 'd':
        return float(time) * 60 * 60 * 24
# ---
def get_bytes(self):
        midi_str = put_variable_length_number(self.time)
        return midi_str
# ---
def quantize_linear_layers(tree: T, config: QuantizationConfig) -> T:
    """
    Converts a module tree to use FP8/INT8 quantization.
    """
    if config.fp8:
        return _quantize_linear_layers(tree, config, Fp8DotGeneralOp, config.amax_history_length, config.compute_dtype)
    elif config.int8:
        return _quantize_linear_layers(tree, config, Int8DotGeneralOp)
    else:
        warnings.warn("Both fp8 and int8 are set to False. `quantize_linear_layers()` is no-op.")
        return tree
# ---
def reset_prefix_cache(self):
        return self.bridge.run(self.engine.reset_prefix_cache())
# ---
def testFunctionDecorator(self):
    self.assertEqual((0, '<b>foo</b>\n'), _GrumpRun(textwrap.dedent("""\
        def bold(fn):
          return lambda: '<b>' + fn() + '</b>'
        @bold
        def foo():
          return 'foo'
        print foo()""")))
# ---
def compute_xxh3_128_hex(text: str) -> str:
    val = hash_xxh3_128(text.encode("utf-8"))
    return f"{val:032x}"
# ---
def checkpoint(self, carry_name: str, input_name: str, callable):
        if self.disable:
            return callable
        elif self.simple:
            return eqx.filter_checkpoint(callable, prevent_cse=self.prevent_cse)
        else:
            policy = self._to_jax_policy(carry_name, input_name)
            return eqx.filter_checkpoint(callable, policy=policy, prevent_cse=self.prevent_cse)
# ---
def the_file_name_should_contain_value(name, value):
    with open(name, "r") as f:
        assert value in f.read()
# ---
def bindproperty(MPV, name, proptype, access, decode_str=False):
    getter = lambda self: self._get_property(name, proptype, decode_str)
    setter = lambda self, value: self._set_property(name, value, proptype)

    def barf(*args):
        raise NotImplementedError('Access denied')

    setattr(MPV, name.replace('-', '_'), property(getter if 'r' in access else barf, setter if 'w' in access else barf))
# ---
def run_cmd(cmd: list[str], check: bool = False) -> subprocess.CompletedProcess:
    click.echo(f"  $ {' '.join(cmd)[:200]}")
    return subprocess.run(cmd, cwd=ROOT_DIR, check=check)
# ---
def _product_get_all_report(self, cr, uid, ids, product_ids=False, context=None):
        return self._product_get_report(cr, uid, ids, product_ids, context, recursive=True)
# ---
def preprocess_example(self, example, mode, hparams):
    if not self._was_reversed:
      example["inputs"] = tf.image.per_image_standardization(example["inputs"])
    return example
# ---
def test_ckpt_path_with_hf_path():
    path = "meta-llama/Meta-Llama-3.1-8B"
    assert ckpt_path_to_step_name(path) == "Meta-Llama-3.1-8B"
# ---
def max_tokens(self) -> int:
        """Maximum number of tokens that can be generated for each sequence, including any prefix tokens."""
        return self.tokens.axis_size("position")
# ---
def get_key(opts):
    if opts['transport'] in ('zeromq', 'tcp'):
        return Key(opts)
    else:
        return RaetKey(opts)
# ---
def __init__(
        self,
        slice_id: str,
        scale_group: str,
        zone: str,
        vms: list[FakeVm],
        created_at_ms: int | None = None,
    ):
        self._slice_id = slice_id
        self._scale_group = scale_group
        self._zone = zone
        self._vms = vms
        self._created_at = Timestamp.from_ms(created_at_ms) if created_at_ms is not None else Timestamp.now()
        self._terminated = False
# ---
def _infer_model_name_for_path(model_path: str) -> str:
    """
    Infer model name from model path.
    """
    # path names are like gs://marin-us-central2/checkpoints/dclm_7b2x/hf/dclm_7b0828/dclm_7b0828/step-479999/
    # we want something like: dclm_7b0828_step-479999
    if model_path.endswith("/"):
        model_path = model_path[:-1]

    return "_".join(model_path.split("/")[-2:])
# ---
def calculate_damage(self, rank_points, strength, weapon_power, level, bonus):
        index = 0

        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:
            if(self.rank_to_pos.index(k) > index):
                index = self.rank_to_pos.index(k)

        return(math.trunc(((index / 20) + 0.3) * ((strength / 10) + 40) * (1 + (weapon_power / 100)) * (1.1 if level > 99 else 1) * bonus))
# ---
def test_float(self):
        """Store and retrieve a float"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "num": 1.1})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertAlmostEqual(float(item["num"]), 1.1)
# ---
def test_task_timeout(cluster, sentinel):
    """Task times out, marked FAILED."""
    _url, client = cluster
    job = submit(client, _block, "timeout-test", sentinel, timeout=Duration.from_seconds(5))
    status = wait(client, job, timeout=30)
    assert status.state == cluster_pb2.JOB_STATE_FAILED
# ---
def load_apartments():
        # type: () -> List[ApartmentDTO]
        apartments = []
        for apartment_orm in Apartment.select():
            apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
            apartments.append(apartment_dto)
        return apartments
# ---
def __iter__(self):
        if self.tree is None:
            return
        else:
            for i in range(len(self)):
                yield self[i]
# ---
def _worker_loop(self) -> None:
        """Main worker loop for background data loading."""
        while not self._stop_event.is_set():
            try:
                self._collect_rollouts()
                time.sleep(self.rollout_fetch_interval)
            except Exception as e:
                logger.error(f"Error in ReplayDataLoader worker loop: {e}", exc_info=True)

            self._stop_event.wait(self.rollout_fetch_interval)
# ---
def shortname(self):
    # This is the only property provided from above
        return self.__shortname
# ---
def _virtual_offset(base: ts.TensorStore, offset_amount):
    async def do_read(domain: ts.IndexDomain, array: np.ndarray, read_params: ts.VirtualChunkedReadParameters):
        array[...] = (await base[domain].read()) + offset_amount

    return ts.virtual_chunked(do_read, dtype=base.dtype, domain=base.domain, shape=base.shape)
# ---
def tanh(a: A) -> A:
    return wrap_elemwise_unary(jnp.tanh, a)
# ---
def num_gpus(self) -> int:
        if self.override_resources is not None:
            return self.override_resources.get("num_gpus", 0)
        return 0
# ---
def json_loads(data):
            # on Python 3.5 json.loads only supports str not bytes
            return json.loads(data.decode())
# ---
def test_create_snapshot(self):
        """Test snapshot can be created and deleted."""
        mox = self.mox
        drv = self._driver

        mox.StubOutWithMock(drv, '_clone_volume')
        drv._clone_volume(IgnoreArg(), IgnoreArg(), IgnoreArg())
        mox.ReplayAll()

        drv.create_snapshot(FakeSnapshot())

        mox.VerifyAll()
# ---
def launch(self, request: JobRequest) -> JobId:
        """Launch a job on the cluster.

        Args:
            request: Job specification including resources, environment, and entrypoint

        Returns:
            Unique identifier for the launched job

        Raises:
            ValueError: If the request is invalid
            RuntimeError: If job submission fails
        """
        ...
# ---
def unpermute_sharded(out_repeat_sort_: Array, sort_idx_: Array):
            inv_sort_idx_ = jnp.argsort(sort_idx_)
            out_repeat_ = jnp.take(out_repeat_sort_, inv_sort_idx_, axis=0)
            out_repeat_unflat_ = jnp.reshape(
                out_repeat_, (-1, self.config.num_experts_per_tok, self.config.hidden_dim)
            )

            return out_repeat_unflat_
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        self.run.config.update(_convert_value_to_loggable_rec(hparams))
# ---
def subfunction(self):
        run(function(self))
# ---
def unflatten(treedef: Any, leaves: Iterable[Any]) -> Any:
    """Alias for :func:`haliax.tree_util.tree_unflatten` matching :func:`jax.tree.unflatten`."""

    return tree_util.tree_unflatten(treedef, leaves)
# ---
def test_scheme_not_supported():

    with pytest.raises(NotImplementedError):
        UrlPath('http:///tmp/test').touch()
# ---
def __init__(
        self,
        inference_config: LevanterInferenceContextConfig,
    ):
        self.inference_server_config = inference_config.inference_server_config
        self.tokenizer = inference_config.tokenizer
        self._stop_tokens = inference_config.stop_tokens
        self.max_tokens = inference_config.max_tokens
        self.mesh = inference_config.mesh
        self.axis_mapping = inference_config.axis_mapping
# ---
def test_bank_contains_expected_types(bank):
    # Our sample programs should produce at least some statements and expressions.
    type_names = set(bank.node_types)
    assert type_names & STATEMENT_TYPES, "Expected some statement types"
    assert type_names & EXPRESSION_TYPES, "Expected some expression types"
# ---
def test_axis_shapes_overlap_error():
    cfg = MeshConfig(axes={"data": 1}, dcn_axes={"data": 1})
    with pytest.raises(ValueError):
        cfg.axis_shapes(num_devices=4, num_slices=1)
# ---
def get_git_commit():
    """Get the current git commit hash."""
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("utf-8").strip()
# ---
def __init__(self, slice_info: SliceInfo):
        self._awaitable: ray.ObjectRef | None = None
        self._host_info: TPUHostInfo | None = None
        self._slice_info = slice_info
# ---
def points_in_radius(x,y,target_x, target_y,radius):
    inside=np.zeros(x.size,dtype=bool)
    d2=(x-target_x)**2+(y-target_y)**2
    inside = d2<=radius**2
    return x[inside],y[inside], inside
# ---
def to_return(self):
        result = {}
        try:
            for returnable in self.returnables:
                result[returnable] = getattr(self, returnable)
            result = self._filter_params(result)
        except Exception:
            raise
        return result
# ---
def __del__(self):
        # Always try to clean up the window when it is dereferenced.
        # Makes sure there are no dangling pointers or memory leaks.
        # If the window is already closed, pass silently.
        try:
            self.close()
        except:   # XXX  Avoid a NoneType error if already closed.
            pass
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None: ...
# ---
def parse_nptable(self, m):
        item = self._process_table(m)

        cells = re.sub(r'\n$', '', m.group(3))
        cells = cells.split('\n')
        for i, v in enumerate(cells):
            cells[i] = re.split(r' *\| *', v)

        item['cells'] = cells
        self.tokens.append(item)
# ---
def default(self, obj):
        if isinstance(obj, timedelta):
            return {"days": obj.days, "seconds": obj.seconds, "microseconds": obj.microseconds}
        if isinstance(obj, Path):
            return str(obj)
        if obj in (float32, bfloat16):
            return str(obj)
        try:
            return super().default(obj)
        except TypeError:
            logger.warning(f"Could not serialize object of type {type(obj)}: {obj}")
            return str(obj)
# ---
def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]
# ---
def summary(self) -> str:
        if self.healthy:
            return "healthy"
        parts = []
        if self.container_status:
            parts.append(f"container={self.container_status}")
        if self.curl_error:
            parts.append(f"curl_error={self.curl_error[:50]}")
        return ", ".join(parts) if parts else "unknown failure"
# ---
def is_valid_ip(ip):
    """Returns true if the given string is a well-formed IP address.

    Supports IPv4 and IPv6.
    """
    try:
        res = socket.getaddrinfo(ip, 0, socket.AF_UNSPEC,
                                 socket.SOCK_STREAM,
                                 0, socket.AI_NUMERICHOST)
        return bool(res)
    except socket.gaierror as e:
        if e.args[0] == socket.EAI_NONAME:
            return False
        raise
    return True
# ---
def _nodes_equal(a: ast.AST, b: ast.AST) -> bool:
    """Check structural equality of two AST nodes (recursively).

    Uses ast.dump for a deep comparison that ignores source positions
    and formatting differences.
    """
    return ast.dump(a) == ast.dump(b)
# ---
def status(self):
        return self._status
# ---
def _unflatten_module(module, template):
        if isinstance(module, ModuleWithStateDictSerialization):
            module = module.unflatten_from_export(template)
            module = scan_aware_tree_map(
                _unflatten_module,
                module,
                template,
                is_leaf=lambda x: x is not module and isinstance(x, ModuleWithStateDictSerialization),
            )
        return module
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetHoursColumn(), 1)
# ---
def heidke_skill_score(self):
        n = float(self.table.sum())
        nf = self.table.sum(axis=1)
        no = self.table.sum(axis=0)
        correct = float(self.table.trace())
        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (nf * no).sum() / n ** 2)
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {
            "transformer": "model",
            "embeddings": "model",
            "lm_head": "lm_head",
        }
# ---
def rust_compute_document_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> pa.RecordBatch:
    pipeline = [
        dupekit.Transformation.Hash(input_col=text_col, output_col="hash", algo=dupekit.HashAlgorithm.Xxh3_128),
    ]
    return dupekit.transform(batch, pipeline)
# ---

def even_odd_count(num):
    """Given an integer. return a tuple that has the number of even and odd digits respectively.

     Example:
        even_odd_count(-12) ==> (1, 1)
        even_odd_count(123) ==> (1, 2)
    """
    even_count = 0
    odd_count = 0
    for i in str(abs(num)):
        if int(i)%2==0:
            even_count +=1
        else:
            odd_count +=1
    return (even_count, odd_count)
# ---
def count(self) -> int:
        """Total number of items across all chunks."""
        return sum(c.count for c in self.chunks)
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "YarnRotaryEmbeddingsConfig":
        return YarnRotaryEmbeddingsConfig(
            theta=rope_theta,
            factor=float(config.get("factor", 1.0)),
            beta_fast=float(config.get("beta_fast", 32.0)),
            beta_slow=float(config.get("beta_slow", 1.0)),
            original_max_position_embeddings=int(config.get("original_max_position_embeddings", 2048)),
            mscale=float(config.get("mscale", 1.0)),
        )
# ---
def upload_to_gcs(local_path: str, gcs_path: str) -> None:
    """
    Uploads a folder `local_path` to Google Cloud Storage (GCS).
    """
    print(f"Uploading {local_path}.")
    fs = fsspec.filesystem("gcs")
    # The slash is needed to upload the contents of the folder to `gcs_path`
    fs.put(local_path + "/", gcs_path, recursive=True)
    logger.info(f"Uploaded {local_path} to {gcs_path}.")
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        client = self._handle._resolve()
        executor = _get_shared_executor()
        future = executor.submit(lambda: getattr(client, self._method)(*args, **kwargs))
        return future
# ---
def test_clock_increases():
    """
    A monotonic moment is never greater than a succeeding monotonic
    moment.
    """
    assert monotonic() <= monotonic()
# ---
def power_digit_sum(exponent):
    power_of_2 = str(2 ** exponent)
    return sum([int(x) for x in power_of_2])
# ---
def context(self):
        """The OpenGL context attached to this window.  Read-only.

        :type: `pyglet.gl.Context`
        """
        return self._context
# ---
def _announce_deprecations(self, result):
        warnings = result.pop('__warnings', [])
        for warning in warnings:
            self.module.deprecate(
                msg=warning['msg'],
                version=warning['version']
            )
# ---
def main():
    argspec = hashivault_argspec()
    argspec['name'] = dict(required=True, type='str')
    argspec['mount_point'] = dict(required=False, type='str', default='approle')
    module = hashivault_init(argspec)
    result = hashivault_approle_role_get(module.params)
    if result.get('failed'):
        module.fail_json(**result)
    else:
        module.exit_json(**result)
# ---
def test_stdin(self, tmpfile):
        cap = capture.FDCapture(0)
        cap.start()
        x = os.read(0, 100).strip()
        cap.done()
        assert x == tobytes('')
# ---
def foo(x: f32[NamedArray, "batch embed"]):  # type: ignore  # noqa: F722
        pass
# ---
def _materialize_sliding_window_mask(
    window: int, QPos: Axis, KPos: Axis, q_slice: haliax.dslice, k_slice: haliax.dslice
) -> NamedArray:
    """Materialize a causal sliding window mask."""
    sub_q = QPos.resize(q_slice.size)
    sub_k = KPos.resize(k_slice.size)
    q_pos = hax.arange(sub_q) + q_slice.start
    k_pos = hax.arange(sub_k) + k_slice.start
    diff = q_pos.broadcast_axis(sub_k) - k_pos.broadcast_axis(sub_q)
    return (diff >= 0) & (diff < window)
# ---


def string_sequence(n: int) -> str:
    """ Return a string containing space-delimited numbers starting from 0 upto n inclusive.
    >>> string_sequence(0)
    '0'
    >>> string_sequence(5)
    '0 1 2 3 4 5'
    """
    return ' '.join([str(x) for x in range(n + 1)])
# ---
def test_build_runtime_env_tpu_clears_jax_platforms():
    from fray.v2.ray_backend.backend import build_runtime_env

    request = JobRequest(
        name="tpu-test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=TpuConfig(variant="v4-8")),
    )
    env = build_runtime_env(request)
    assert env["env_vars"]["JAX_PLATFORMS"] == ""
# ---
def interaction_choosers():
    return pd.DataFrame({
        'attr': ['a', 'b', 'c', 'b']},
        index=['w', 'x', 'y', 'z'])
# ---
def __iter__(self):
		return self._addresses.__iter__()
# ---
def __call__(self, x):
            return self.blocks.fold(x)
# ---
def __init__(self, number: int):
        self.name = 'Ð¸Ð³Ñ€Ð¾Ðº{}'.format(number)
        tactic = randint(0, FIG_LEN-1)
        self.main_figure = FIGURES[tactic]
        self.__figures = [FIGURES[(tactic+i) % FIG_LEN] for i in range(FIG_LEN)]
# ---
def get_all_api_keys(user_profile: UserProfile) -> List[str]:
    # Users can only have one API key for now
    return [user_profile.api_key]
# ---
def _load_metadata(checkpoint_path, fs=None):
    if fs is None:
        fs, _, _ = fsspec.get_fs_token_paths(str(checkpoint_path))
    with fs.open(os.path.join(checkpoint_path, "metadata.json")) as metadata_in:
        metadata = json.load(metadata_in)
    return metadata
# ---
def test_subtract(self):
        expr = col("a") - col("b")
        assert expr.evaluate({"a": 10, "b": 3}) == 7
# ---
def get_custom_objects():
  """Retrieves a live reference to the global dictionary of custom objects.

  Updating and clearing custom objects using `custom_object_scope`
  is preferred, but `get_custom_objects` can
  be used to directly access `_GLOBAL_CUSTOM_OBJECTS`.

  Example:

  ```python
      get_custom_objects().clear()
      get_custom_objects()['MyObject'] = MyObject
  ```

  Returns:
      Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).
  """
  return _GLOBAL_CUSTOM_OBJECTS
# ---
def spacesCount(text):
            return len(text) - len(text.rstrip(' '))
# ---
def __init__(self, start, end):
        if start is not None and end is not None and not start < end:
            raise ValueError("start date {} is not before end date {}"
                             .format(start, end))
        self.start = start
        self.end = end
# ---
def get_shard_source(self, split) -> ShardedDataSource[dict] | None:
        split_urls = self.urls_for_split(split)

        if len(split_urls) == 0:
            return None

        return UrlDataSource(split_urls)
# ---
def free(self):
        SummaryKeyMatcher.cNamespace().free(self)
# ---
def real_worker(cache_dir, docker_cleanup_scope):
    """Create Worker with real components (not mocks)."""
    config = WorkerConfig(
        port=0,
        cache_dir=cache_dir,
        registry="localhost:5000",
        port_range=(40000, 40100),
        poll_interval=Duration.from_seconds(0.5),  # Faster polling for tests
    )
    return Worker(config)
# ---
def test_parse_yaml_template(self):
        tmpl_str = 'heat_template_version: 2013-05-23'
        expected = {'heat_template_version': '2013-05-23'}
        self.assertEqual(expected, template_format.parse(tmpl_str))
# ---
def evaluate(
        self,
        model: ModelConfig,
        evals: list[EvalTaskConfig],
        output_path: str,
        max_eval_instances: int | None = None,
        wandb_tags: list[str] | None = None,
    ) -> None:
        """What to run to evaluate."""
        pass
# ---
def extend(self, items):
        """Append a list of elements at the end of the queue.

        Parameters
        ----------
        items : list
            List of elements.

        """
        self._queue.extend(items)
# ---
def __init__(self):
        columns = ['mean_height', 'min_height', 'max_height', 'mean_width', 'min_width', 'max_width', 'time', 'girth','id']
        self.data = DataFrame(columns=columns)
        self.event = []
# ---
def build(self, Vocab: Axis, *, key: PRNGKeyArray) -> GrugWrapper:
        core_cfg = self.to_grug_model_config()
        cfg = GrugformerAttnSinkModelConfig(core=core_cfg, sink=self.sink)
        params = _init_grugformer_with_sinks(cfg, key=key)
        return GrugWrapper(
            params=params,
            grug_config=cfg,
            init_fn=_init_grugformer_with_sinks,
            forward_fn=_grug_activations_with_sinks,
            lm_head_fn=_lm_head_from_sink_params,
        )
# ---
def _randMatrix(self, rows, cols, dtype):
    if dtype is np.complex64:
      real = self._randMatrix(rows, cols, np.float32)
      imag = self._randMatrix(rows, cols, np.float32)
      return real + np.complex(0, 1) * imag
    else:
      return np.random.uniform(low=1.0, high=100.0, size=rows * cols).reshape(
          [rows, cols]).astype(dtype)
# ---
def transform_linear_layer(layer: haliax.nn.Linear):
            assert layer.weight.ndim == 2
            # steps is now a concrete int
            array = layer.weight.array
            updated_weight_array = zeropower_via_newtonschulz5(
                array, steps=steps, eps=muon_eps, coefficient_type=coefficient_type
            )

            updated_weight = dataclasses.replace(layer.weight, array=updated_weight_array)

            return dataclasses.replace(layer, weight=updated_weight)
# ---
def get_file_size(path):
    return os.path.getsize(path)
# ---
def bad(self, f, msg):
        """Callback from dirstate.walk for each explicit file that can't be
        found/accessed, with an error message."""
# ---
def handle_heartbeat(self, request: cluster_pb2.HeartbeatRequest) -> cluster_pb2.HeartbeatResponse: ...
# ---
def polyCongr(coefficients, m):
        solutions = []
        for i in xrange(m):
                value = 0
                for degree in xrange(len(coefficients)):
                        value += coefficients[degree] * (i ** (len(coefficients) - degree - 1))
                if value % m == 0:
                        solutions.append(i)

        return solutions
# ---
def _get_choices(self):
		if isinstance(self._choices, RegistryIterator):
			return self._choices.copy()
		elif hasattr(self._choices, 'next'):
			choices, self._choices = itertools.tee(self._choices)
			return choices
		else:
			return self._choices
# ---
def frexp(a: A) -> A:
    return wrap_elemwise_unary(jnp.frexp, a)
# ---
def update_num_lines(self):
        """
        Update the number of lines member.
        """
        self.num_lines = len(self.lines)
# ---
def round(self, decimals=0) -> "NamedArray":  # pragma: no cover
        return haliax.round(self, decimals=decimals)
# ---
def num_pages(self) -> int:
        return self.page_ref_counts.axis_size("page")
# ---
def tick(self, ts: int) -> None:
        """Advance VM state transitions."""
        for vm in self._vms:
            vm.tick(ts)
# ---
def restart(self) -> str:
        self.stop()
        return self.start()
# ---
def _read(path: Path) -> list[dict]:
        records = []
        with gzip.open(path, "rt", encoding="utf-8") as handle:
            for line in handle:
                if line.strip():
                    records.append(json.loads(line))
        return records
# ---
def optimized_func_der(self, r):
        p = Pool(processes=4)

        self_args = [self] * len(r)
        i_args = range(len(r))
        r_args = [r] * len(r)

        return np.array(p.map(optimized_func_i_der,
                              zip(self_args, r_args, i_args)))
# ---
def output_list(self):
        ordered = self.token['ordered']
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'list_end':
            body += self.tok()
        return self.renderer.list(body, ordered)
# ---
def real_service(real_worker):
    """Create WorkerServiceImpl with real worker."""
    return WorkerServiceImpl(real_worker)
# ---
def artist(self):
        return self.__artist
# ---
def contractionTableComboChanged(self, combobox):
        model = combobox.get_model()
        myIter = combobox.get_active_iter()
        self.prefsDict["brailleContractionTable"] = model[myIter][1]
# ---
def _load_dataset(self):
        # obnoxiously, the dataset loading stuff doesn't work with ray because of multiprocessing
        # so we have to do this hacky thing where we load the dataset in the worker
        return datasets.load_dataset(self.id, split=self.split, streaming=self.streaming, **self.kwargs)
# ---
def _redact_docker_run_command(cmd: list[str]) -> str:
    redacted = list(cmd)
    i = 0
    while i < len(redacted):
        if redacted[i] == "-e" and i + 1 < len(redacted):
            kv = redacted[i + 1]
            if "=" in kv:
                key, value = kv.split("=", 1)
                if key in _SENSITIVE_ENV_KEYS and value:
                    redacted[i + 1] = f"{key}=<redacted>"
        i += 1
    return shlex.join(redacted)
# ---
def test_no_params_option(self):
        stmt = "SELECT '%'" + testing.db.dialect.statement_compiler(
                                    testing.db.dialect, None).default_from()

        conn = testing.db.connect()
        result = conn.\
                execution_options(no_parameters=True).\
                scalar(stmt)
        eq_(result, '%')
# ---
def _setup_digits(self, env):
        """ Setup the digits for ``self`` and its corresponding column """
        pass
# ---
def test_byte_length_of_token_gpt2():
    tok = load_tokenizer("gpt2")
    ids = tok("this is hello a test", add_special_tokens=False)["input_ids"]
    assert byte_length_of_token(tok, ids[2]) == len(" hello".encode("utf-8"))

    eos = tok.eos_token_id
    assert byte_length_of_token(tok, eos) == 0
# ---
def i_add_a_cube():
    bpy.ops.mesh.primitive_cube_add()
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: NamedArray | AttentionMask | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        x = self.embeddings.embed(input_ids)
        normalizer = jnp.sqrt(self.config.hidden_dim).astype(x.dtype)
        x = x * normalizer
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def set_frequency(self, freq):
        self._frequency = freq
        self._lo.set_frequency(self._frequency + self._if_frequency)
        self._requested_cal = self.get_calibration(self._frequency,
                                                   self._power)
        self._output_SSB()
# ---
def pytest_configure(config):
    """Register markers related to testimony tokens"""
    for marker in [
        'importance: CaseImportance testimony token, use --importance to filter',
        'component: Component testimony token, use --component to filter',
        'assignee: Assignee testimony token, use --assignee to filter',
    ]:
        config.addinivalue_line("markers", marker)
# ---
def collect_workdir_size_mb(workdir: Path) -> int:
    """Calculate workdir size in MB using du -sm."""
    if not workdir.exists():
        return 0

    result = subprocess.run(
        ["du", "-sm", str(workdir)],
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        return 0

    # du -sm output format: "SIZE\tPATH"
    output = result.stdout.strip()
    size_str = output.split("\t")[0]

    return int(size_str)
# ---
def startup_name(self):
        return self._get_title_and_company()['company']
# ---
def __init__(self):
        self.comminfo = dict()
        self.init()
# ---
def service(state, mock_scheduler):
    """Create a ControllerServiceImpl for testing."""
    return ControllerServiceImpl(state, mock_scheduler, bundle_prefix="file:///tmp/iris-test-bundles")
# ---
def arrays_only(x):
    return eqx.filter(x, eqx.is_array_like)
# ---
def combine_masks_and(mask1: NamedArray | None, mask2: NamedArray | None) -> NamedArray | None:
    if mask1 is None:
        return mask2
    if mask2 is None:
        return mask1
    return mask1 & mask2.broadcast_axis(mask1.axes)
# ---
def __iter__(self):
        """A ``ReadRowsPage`` is an iterator."""
        return self
# ---
def test_score_syntax_errors():
    assert score_syntax_errors(open('input/10.test').read().splitlines()) == (26397, 288957)
# ---
def init_logits():
        xw_tiled[...] = jnp.zeros_like(xw_tiled)
# ---
def scale_group(self) -> str:
        """Name of the scale group this VM group belongs to."""
        ...
# ---
def __init__(self, system, capForce, particleGroup = None):
        if not (pmi._PMIComm and pmi._PMIComm.isActive()) or pmi._MPIcomm.rank in pmi._PMIComm.getMPIcpugroup():
            if (particleGroup == None) or (particleGroup.size() == 0):
                cxxinit(self, integrator_CapForce, system, capForce)
            else:
                cxxinit(self, integrator_CapForce, system, capForce, particleGroup)
# ---
def streaming_dedup(items: Iterator[T]) -> Iterator[T]:
            """Deduplicate items within a shard."""
            seen = set()
            for item in items:
                k = key(item)
                if k not in seen:
                    seen.add(k)
                    yield item
# ---
def delete_all(self):
        '''
        Delete all keys
        '''
        self.delete('*')
# ---
def add(self, entry: SubtreeEntry) -> None:
        """Add a subtree entry to the bank."""
        if entry.node_type not in self.entries:
            self.entries[entry.node_type] = []
        self.entries[entry.node_type].append(entry)
# ---
def test_position_token_id_out_of_range(tok):
    with pytest.raises(ValueError):
        tok.position_token_id(-1)
    with pytest.raises(ValueError):
        tok.position_token_id(512)
# ---
def test_get_autoscaler_status_returns_disabled_when_no_autoscaler(client):
    """GetAutoscalerStatus RPC returns empty status when autoscaler is not configured."""
    resp = rpc_post(client, "GetAutoscalerStatus")
    status = resp.get("status", {})

    # When no autoscaler, should return empty status
    assert status.get("groups", []) == []
# ---
def __init__(self, opts):
        opts['__multi_key'] = True
        super(MultiKeyCLI, self).__init__(opts)
        # Remove the key attribute set in KeyCLI.__init__
        delattr(self, 'key')
        zopts = copy.copy(opts)
        ropts = copy.copy(opts)
        self.keys = {}
        zopts['transport'] = 'zeromq'
        self.keys['ZMQ Keys'] = KeyCLI(zopts)
        ropts['transport'] = 'raet'
        self.keys['RAET Keys'] = KeyCLI(ropts)
# ---
def _run_coroutine(self, coro):
        return thread_utils.blocking_wait(coro)
# ---
def clear_sample_outputs(self):
        """
        Clear all stored sample outputs.

        Removes all previously collected sample outputs from memory.
        """
        self.sample_outputs.clear()
# ---
def __init__(self, allow=None, disallow=None, secure=True, *args, **kwargs):
		super(TemplateField, self).__init__(*args, **kwargs)
		self.validators.append(TemplateValidator(allow, disallow, secure))
# ---
def __init__(self, file=None, build=None, environment=None):
		self._addresses = {}
		self._values = {}
		self.file = file
		self.build = build
		self.environment = environment
# ---
def __call__(self, batch: Sequence[Sequence[int]]) -> Sequence[dict[str, Sequence[int]]]:
        return [{"data": x} for x in batch]
# ---
def myroot():
    return_data = get_index()
    return return_data
# ---
def run_streaming(self, command: str) -> Any:
        """Run command with streaming stdout.

        Returns subprocess.Popen in production, FakePopen in tests.
        """
        ...
# ---
def stop_wrapper():
                        stop_called.append(vm_id)
                        return orig_stop()
# ---
def assertVolumeTypeListEqual(self, expected, observed):
        self.assertEqual(len(expected), len(observed))
        expected = sorted(expected, key=lambda item: item['id'])
        observed = sorted(observed, key=lambda item: item['id'])
        for d1, d2 in zip(expected, observed):
            self.assertEqual(d1['id'], d2['id'])
# ---
def _visit(self, element):
        if isinstance(element, NavigableString):
            text = element.strip()
            return TextNode(text) if text else TextNode("")

        if not element.name:
            return TextNode("")

        method_name = f"visit_{element.name}"
        if hasattr(self, method_name):
            return getattr(self, method_name)(element)

        # Default recursive visit
        return self._visit_children(element)
# ---
def set_rt(self, rt):
        assert(len(rt) == self.num_features)
        self.rt = rt
# ---
def _wrapped(x):
            return fn(x)
# ---
def test_ports_list(self):
        self.assertEqual(self.mda.get_ports_list(), [0x03B0, 0x03B1, 0x03B2, 0x03B3,
                                                     0x03B4, 0x03B5, 0x03B6, 0x03B7,
                                                     0x03B8, 0x03B9, 0x03BA, 0x03BB])
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()
# ---
def test_reduce_count(backend):
    """Test count reduction."""
    ds = Dataset.from_list([{"id": i} for i in range(50)]).reduce(
        local_reducer=lambda items: sum(1 for _ in items),
        global_reducer=sum,
    )
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == 50
# ---
def remote(self, *args, **kwargs) -> Any:
        """Call method asynchronously, returning a future compatible with ctx.get()."""
        raise NotImplementedError
# ---
def test_validate_invalid_password(self):
        # Ensure user can't login when the pasword is incorrect.
        with self.client:
            response = self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="foo_bar"),
                follow_redirects=True,
            )
        self.assertIn(b"Invalid email and/or password.", response.data)
# ---
def __init__(self, string, cind=None):
        CmdText.__init__(self)
        self.insert(string)

        if (cind is not None):
            self.command = cind
# ---
def are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str):
    are_equal = False
    try:
        expr = f"({ground_truth_normalized})-({given_normalized})"
        if should_allow_eval(expr):
            sympy_diff = _sympy_parse(expr)
            simplified = sympy.simplify(sympy_diff)
            if simplified == 0:
                are_equal = True
    except Exception:
        pass
    return are_equal
# ---
def test_job_failure_propagates(self, test_cluster):
        """Job that raises exception is marked FAILED."""

        def failing_job():
            raise ValueError("intentional failure")

        job_id = test_cluster.submit(failing_job, name=unique_name("fail-job"))
        status = test_cluster.wait(job_id, timeout=30)
        assert status["state"] == "JOB_STATE_FAILED"
# ---
def transition(
        self,
        new_state: int,
        *,
        exit_code: int | None = None,
        error: str | None = None,
    ) -> None:
        """Transition this attempt to a new state."""
        self.state = new_state
        now = Timestamp.now()

        if new_state == cluster_pb2.TASK_STATE_RUNNING:
            self.started_at = now

        if new_state in TERMINAL_TASK_STATES:
            self.finished_at = now
            self.exit_code = exit_code
            self.error = error
# ---
def content_only(args: list[str]) -> str:
        return args[0] if args else ""
# ---


def truncate_number(number: float) -> float:
    """ Given a positive floating point number, it can be decomposed into
    and integer part (largest integer smaller than given number) and decimals
    (leftover part always smaller than 1).

    Return the decimal part of the number.
    >>> truncate_number(3.5)
    0.5
    """
    return number % 1.0
# ---
def __init__(self):
        self.level = 0
        self.req_headers = []
        self.data = None
        self.raise_on_endheaders = False
        self._tunnel_headers = {}
# ---
def exit_with_code(code: int):
        sys.exit(code)
# ---
def multi_worker_cluster(use_docker, docker_cleanup_scope):
    """Provide a cluster with multiple workers."""
    with E2ECluster(num_workers=3, use_docker=use_docker) as cluster:
        yield cluster
# ---
def synthetic_target_extra_dependencies(self, target, target_workdir):
    for source in target.sources_relative_to_buildroot():
      if self._declares_service(os.path.join(get_buildroot(), source)):
        return self._service_deps
    return self._deps
# ---
def vocab_size(self) -> int:
        """Total vocabulary size including all special tokens."""
        return self.base_token_offset + self.base_vocab_size
# ---
def _try_delete_vm(vm: dict, zone: str) -> bool:
    """
    Try to delete a VM. Returns True if deletion was successful.
    """
    logging.warning(f"[{zone}] TPU {vm['name']} is in state {vm['state']}, deleting...")
    try:
        delete_tpu_vm(vm["name"], zone)
        return True
    except Exception as e:
        logging.error(f"[{zone}] Failed to delete {vm['name']}: {e}")
        return False
# ---
def from_callable(
        c: Callable[..., Any],
        args: Sequence[Any] = (),
        kwargs: dict[str, Any] | None = None,
    ) -> Self:
        return Entrypoint(callable_entrypoint=CallableEntrypoint(callable=c, args=args, kwargs=kwargs or {}))
# ---
def get_default_zone() -> Optional[str]:
    try:
        result = subprocess.run(["gcloud", "config", "get-value", "compute/zone"], stdout=subprocess.PIPE, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        return None
# ---
def is_metadata_ref(thing, reftype=dict):
    return isinstance(thing, reftype) and \
        len(thing) == 1 and \
        isinstance(thing.get('$dnanexus_link'), reftype) and \
        isinstance(thing['$dnanexus_link'].get('metadata'), basestring)
# ---
def method_not_allowed(error):
    return make_response(jsonify( { 'error': 'Method Not Allowed' } ), 405)
# ---
def __getattr__(self, method_name: str) -> Callable[..., Any]:
        def call(*args, **kwargs):
            endpoint = self._pool._get_next_endpoint()
            return self._pool._call_endpoint(endpoint, method_name, args, kwargs)

        return call
# ---
def update(self, other):
		for k in other:
			self[k] = other[k]
# ---
def test_empty_shape():
    key = jax.random.PRNGKey(0)
    hax.random.uniform(key, shape=())
# ---
def test_corrupt_program_returns_mutations_in_order(bank):
    source = CORPUS[0]
    rng = random.Random(42)

    corrupted, mutations = corrupt_program(source, num_steps=3, bank=bank, rng=rng)

    # Replay the mutations to verify they produce the same result.
    current = source
    for m in mutations:
        current = m.apply(current)
    assert current == corrupted
# ---
def group_id(self) -> str:
        return self._group_id
# ---
def get_index_from_jbor(thing):
    '''
    :returns: Array index of the JBOR if applicable; None otherwise

    Assumes :func:`is_job_ref` evaluates to True
    '''
    if '$dnanexus_link' in thing:
        return thing['$dnanexus_link'].get('index')
    else:
        return None
# ---
def set_clone_source(self, slot_id: int, clone_source: jnp.ndarray | int) -> "SequenceTable":
        clone_sources = self.clone_sources.at["seq", slot_id].set(clone_source)
        return dataclasses.replace(self, clone_sources=clone_sources)
# ---
def compute_rbf_kernel_matrix(X):
    """Compute the RBF kernel matrix with sigma2 as the median pairwise
    distance.
    """
    sigma2 = np.median(pairwise_distances(X, metric='euclidean'))**2
    K = pairwise_kernels(X, X, metric='rbf', gamma=1.0/sigma2, n_jobs=-1)
    return K
# ---
def create(self, model: M) -> ModelAveraging[M]:
        pass
# ---
def ListEndpoints(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def case(context):
        """Check that the context has been set up."""
        assert context == {"squee": "kapow"}
# ---
def _add_row(self, id, address, reclen):
		if id in self._addresses: # Something's wrong here
			log.warning("Multiple instances of row %r found in %s" % (id, self.file.name))
		self._addresses[id] = (address, reclen)
# ---
def corofunc3():
            called[2] += 1

            async def corofunc():
                pass

            return event.ReturnValue(append_events=[evt3], schedule={corofunc()})
# ---
def format_archive_policy(ap):
    format_dict_list(ap, "definition")
    format_string_list(ap, "aggregation_methods")
# ---
def backwards(self, orm):

        # Deleting model 'Journey'
        db.delete_table('places_journey')

        # Deleting model 'ScheduledStop'
        db.delete_table('places_scheduledstop')
# ---
def compute_logprobs_fn(m, b, k):
                    return chunked_compute_logprobs(m, b, k, self.vocab_tile_size)
# ---
def on_terminate(hosts: list[str]) -> None:
        callback_hosts.extend(hosts)
# ---
def always(self):
        """Matcher will match everything and .files() will be empty --
        optimization might be possible."""
        return False
# ---
def test_show(self):
        resp = FakeResponse()
        self.type_action_controller.show(self.req, resp, fake.VOLUME_TYPE_ID)
        self.assertEqual({'id': fake.VOLUME_TYPE_ID,
                          'os-volume-type-access:is_public': True},
                         resp.obj['volume_type'])
# ---
def __init__(self, host='localhost', port=8125, enabled=True, prefix=''):
        self.addr = None
        self.enabled = enabled
        if enabled:
            self.set_address(host, port)
        self.prefix = prefix
        self.udp_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# ---
def friendly_time(msecs):
    secs, msecs = divmod(msecs, 1000)
    mins, secs = divmod(secs, 60)
    hours, mins = divmod(mins, 60)
    if hours:
        return '%dh%dm%ds' % (hours, mins, secs)
    elif mins:
        return '%dm%ds' % (mins, secs)
    elif secs:
        return '%ds%dms' % (secs, msecs)
    else:
        return '%.2fms' % msecs
# ---
def find_clusters(atimes):
    foo = Counter()
    bar = dict()
    for i in xrange(120, 3660, 10):
        clusters = get_clusters(atimes, i)
        cs = len(clusters)
        foo[cs] += 1

        # note first occurance of this cluster size.
        if cs not in bar:
            bar[cs] = i
        # print(len(atimes), i, cs)

    return bar[foo.most_common()[0][0]]
# ---
def bootstrap_config() -> config_pb2.BootstrapConfig:
    """Standard bootstrap configuration for tests."""
    return config_pb2.BootstrapConfig(
        controller_address="10.0.0.1:10000",
        worker_id="test-worker",
        worker_port=10001,
        docker_image="gcr.io/test/iris-worker:latest",
        cache_dir="/var/cache/iris",
    )
# ---
def compute_log_probs(model, example):
            model = trainer.mp.cast_to_compute(model)
            logprobs = model.compute_loss(example, key=None, reduction=None)
            # roll forward to get the loss for each predicted token
            logprobs = hax.roll(logprobs, 1, Pos)
            return logprobs.rearrange((EvalBatch, Pos)).array
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("x", Integer),
            Column("y", Integer),
            Column("z", String(50)),
        )
# ---
def get_test_contract(name):
    contract_path = os.path.abspath(
        os.path.join(os.path.dirname(__file__), "..", "smart_contracts", name)
    )
    contracts = compile_files_cwd([contract_path])

    return contract_path, contracts
# ---
def as_dict(self):
        return {'reason': self.value}
# ---
def __repr__(self):
        return ("{0.__class__.__name__}({0.pattern!r}, {0.fields!r}, "
                "{0.query_class.__name__})".format(self))
# ---
def __call__(self, batch: Sequence[BatchEncoding]) -> BatchEncoding:
        stacked = reduce(_stack_batch_encodings, batch)
        return stacked
# ---
def location(self):
        program = self.program()
        return program.program_family.name if program else None
# ---
def convert_to_cache(self, value, record, validate=True):
        if isinstance(value, dict):
            # special case, when an integer field is used as inverse for a one2many
            return value.get('id', False)
        return int(value or 0)
# ---
def get_reward(self, m, s):
        """
        Get reward for moving from state s to state (s + 1)
        """
        return self.rewards[m][s + 1][0]
# ---
def __repr__(self):
        return f"MapShardOp(fn={_get_fn_name(self.fn)})"
# ---
def _volumes(self):
        self._prepare_volumes()
        try:
            yield
        finally:
            self._teardown_volumes()
# ---
def output_code(self, m):
        text = m.group(2)
        return self.renderer.codespan(text)
# ---
def test_endswith_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e##fg", escape="#"), {9})
# ---
def get(self, item):
        """Return the element in the queue identical to `item`.

        Parameters
        ----------
        item :
            The element to search for.

        Returns
        -------
        The element in the queue identical to `item`. If the element
        was not found, None is returned.

        """
        try:
            index = self._queue.index(item)
            return self._queue[index]
        except Exception:
            return None
# ---
def add(self, n, values=None):
    self.update(self._seen_so_far + n, values)
# ---
def gen_md5(content_str):
    m = md5()
    m.update(content_str)
    return m.hexdigest()
# ---
def get_version():
    main_ns = {}
    version_path = convert_path('gpyfft/version.py')
    with open(version_path) as version_file:
        exec(version_file.read(), main_ns)
    version = main_ns['__version__']
    return version
# ---
def get_transactions(self, limit: int = 100) -> list[TransactionLog]:
        """Return recent transactions for debugging."""
        with self._lock:
            return list(self._transactions)[-limit:]
# ---
def _add_job(job_id, job):
    with _lock:
        if job_id in _jobs:
            raise JobExistsError("Job %r exists" % job_id)
        _jobs[job_id] = job
# ---
def test_do_setup(self):
        mox = self.mox
        drv = self._driver
        mox.StubOutWithMock(netapp_nfs.NetAppNFSDriver, 'do_setup')
        mox.StubOutWithMock(drv, '_get_client')
        mox.StubOutWithMock(drv, '_do_custom_setup')
        netapp_nfs.NetAppNFSDriver.do_setup(IgnoreArg())
        drv._get_client()
        drv._do_custom_setup(IgnoreArg())

        mox.ReplayAll()

        drv.do_setup(IsA(context.RequestContext))

        mox.VerifyAll()
# ---
def __getattr__(self, method_name: str) -> RayActorMethod:
        if method_name.startswith("_"):
            raise AttributeError(method_name)
        ray_method = getattr(self._actor_ref, method_name)
        return RayActorMethod(ray_method)
# ---
def test_mem_read_word(self):
        self.mda.video_ram[0x0000] = 0x41
        self.mda.video_ram[0x0001] = 0x08
        self.assertEqual(self.mda.mem_read_word(0x0000), 0x0841)
# ---
def write_artifacts(self, log_tree: LogTree):
        """Write the artifacts section from the log tree."""
        self._file.write("\n---\n\n")
        for line in log_tree.summary_lines():
            self._file.write(line + "\n")
        self._file.flush()
# ---
def test_do_setup(self):
        mox = self.mox
        drv = self._driver

        mox.StubOutWithMock(netapp_nfs.NetAppNFSDriver, 'do_setup')
        mox.StubOutWithMock(drv, '_get_client')
        mox.StubOutWithMock(drv, '_do_custom_setup')

        netapp_nfs.NetAppNFSDriver.do_setup(IgnoreArg())
        drv._get_client()
        drv._do_custom_setup(IgnoreArg())

        mox.ReplayAll()

        drv.do_setup(IsA(context.RequestContext))

        mox.VerifyAll()
# ---
def _unshaped_spec(store: ts.TensorStore) -> ts.Spec:
    spec = store.spec(retain_context=True)
    return spec
# ---
def test_impl(df):
            df['B'] = df.A.map(lambda a: 2 * a)
            return
# ---
def test_demo_notebook_hello_world_submit(demo_client: IrisClient) -> None:
    # Notebook cell snippet (verbatim structure).
    def hello_world():
        print("Hello from the cluster!")
        return 42

    job = demo_client.submit(
        entrypoint=Entrypoint.from_callable(hello_world),
        name="notebook-hello",
        resources=ResourceSpec(cpu=1, memory="512m"),
    )
    status = job.wait(timeout=30.0, raise_on_failure=False)
    assert status is not None
# ---
def teardown_class(cls):
        metadata.drop_all()
# ---
def test_perturb_operators_comparison(rng):
    source = "a > b"
    found = False
    for seed in range(50):
        r = random.Random(seed)
        result = perturb_operators(source, r, swap_prob=1.0)
        if result is not None:
            found = True
            ast.parse(result)
            break
    assert found
# ---
def build_train_lm_on_pod_config(self) -> TrainLmOnPodConfig:
        return TrainLmOnPodConfig(
            train_config=self.build_train_lm_config(),
            resources=self.build_pod_config(),
            output_path=this_output_path(),
        )
# ---
def __init__(self):
        """
        Construct empty object.
        """
        self.num_lines = 0
        self.remaining_lines = MAX_NUM_LINES
        self.lines = []
# ---
def test_count_add_capacity(self):
        """Count addition with consumed_capacity"""
        count = Count(4, 2, Capacity(3, 0))
        count2 = Count(5, 3, Capacity(2, 0))
        ret = count + count2
        self.assertEqual(ret, 9)
        self.assertEqual(ret.scanned_count, 5)
        self.assertEqual(ret.consumed_capacity.read, 5)
# ---
def __repr__(self):
        return f"GroupByOp(key={_get_fn_name(self.key_fn)})"
# ---
def test_rename_variables_no_names_returns_none(rng):
    # Only builtins and constants.
    source = "print(42)"
    result = rename_variables(source, rng)
    # "print" is protected, "42" is a constant â€” no names to rename.
    assert result is None
# ---
def test_classify_textfiles_to_db(mock_rfw, mock_rw, mock_jw):
    classify_documents.classify_textfiles_to_db(0, 'test')

    assert mock_rfw.called
    assert mock_rw.called
    assert mock_jw.called
# ---
def test_perturb_operators_swap_prob_zero_returns_none(rng):
    source = "a + b"
    result = perturb_operators(source, rng, swap_prob=0.0)
    assert result is None
# ---
def __init__(self, pool_size=2, strides=None, padding='valid'):
        super(LW_MaxPooling1D, self).__init__(pool_size, strides, padding)
# ---
def test_pspec_for_plain_array_uses_typeof_sharding():
    # In explicit sharding mode, jax.typeof carries sharding info for plain arrays.
    devices = jax.devices()
    mesh = Mesh(np.array(devices), (ResourceAxis.DATA,), axis_types=(AxisType.Explicit,))
    sharding = NamedSharding(mesh, PartitionSpec(ResourceAxis.DATA, None))
    array = jax.device_put(jnp.ones((len(devices), 2)), sharding)

    spec = pspec_for(array, resource_mapping={})
    assert spec == PartitionSpec(ResourceAxis.DATA, None)
# ---
def __getattr__(self, method_name: str) -> "ActorMethod":
        """Get a callable method wrapper for the actor."""
        raise NotImplementedError
# ---
def test_hashing_throughput(benchmark: Any, sample_batch: pa.RecordBatch, func: Callable, mode: str) -> None:
    # Use the sample_batch fixture (10k rows) and convert to bytes
    text_samples = [t.as_py().encode("utf-8") for t in sample_batch["text"]]

    def _run() -> list[Any]:
        if mode == "batch":
            return func(text_samples)
        return [func(x) for x in text_samples]

    benchmark(_run)
# ---
def unsupported_kernel(method, notebook, data):
    return {'username': SAAGIE_USERNAME}
# ---
def __init__(self, fget):
        self.fget = fget
        self.func_name = fget.__name__
# ---
def backend(request):
    """Parametrized fixture providing all backend types."""
    return request.param
# ---
def test_register_encoder(self):
        """Can register a custom encoder"""
        from datetime import datetime

        dynamizer = Dynamizer()
        dynamizer.register_encoder(datetime, lambda d, v: (STRING, v.isoformat()))
        now = datetime.utcnow()
        self.assertEqual(dynamizer.raw_encode(now), (STRING, now.isoformat()))
# ---
def __call__(self, model: M_con, *inputs: X, **input_kwargs) -> Scalar: ...
# ---
def close(self):
        self.__root.destroy()
        self.__root.quit()
# ---
def _scale_up_wrapper(stop_event):
            self._do_scale_up(group, ts, reason)
# ---
def assert_disk_type(self, image_meta, expected_disk_type):
        actual = vm_utils.VMHelper.determine_disk_image_type(image_meta)
        self.assertEqual(expected_disk_type, actual)
# ---
def dot(self, *args, **kwargs) -> "NamedArray":
        if "axis" in kwargs or len(args) == 0:
            return haliax.dot(self, *args, **kwargs)
        else:
            axis = args[0]
            args = args[1:]
            # We want to get the deprecation warning for this style
            return haliax.dot(axis, self, *args, **kwargs)
# ---
def tale_replacement(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'replacement',
        'purpose': 'characterization',
        'method': 'TALEN',
        'zygosity': 'heterozygous'
    }
# ---
def __repr__(self) -> str:
        return f"Job({self._job_id!r})"
# ---
def __le__(self, other: "Duration") -> bool:
        return self._ms <= other._ms
# ---
def test_actor_options_non_preemptible_pins_head_node():
    from fray.v2.ray_backend.backend import _actor_ray_options

    options = _actor_ray_options(ResourceConfig(preemptible=False))
    assert options["num_cpus"] == 1
    assert options["resources"] == {"head_node": 0.0001}
# ---
def create_multi_layer_neural_network(input_vars, out_dims, num_hidden_layers):

        num_hidden_neurons = 128

        hidden_layer = lambda: Dense(num_hidden_neurons, activation=cntk.ops.relu)
        output_layer = Dense(out_dims, activation=None)

        model = Sequential([LayerStack(num_hidden_layers, hidden_layer),
                            output_layer])(input_vars)
        return model
# ---
def format_sci(self, val):
        return f"{val:.0e}".replace("e-0", "e-").replace("e+0", "e+")
# ---
def list_units (self, utype=ANY) :
        """
        List IDs of data and/or compute units
        """

        raise Exception ("%s.list_units() is not implemented" % self.__class__.__name__)
# ---
def _gcloud_delete_tpu(project_id: str, zone: str, name: str) -> bool:
    cmd = [
        "gcloud",
        "compute",
        "tpus",
        "tpu-vm",
        "delete",
        name,
        f"--project={project_id}",
        f"--zone={zone}",
        "--quiet",
    ]
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        logger.warning("Failed to delete TPU %s in zone %s: %s", name, zone, result.stderr.strip())
        return False
    return True
# ---
def certificado(self):
        return self.__certificado
# ---
def regular_update(updates, nu_hat):
            # For subsequent steps, compute updates normally
            return jax.tree_util.tree_map(
                lambda m, v: None if m is None else m / (jnp.sqrt(v + eps_root) + eps),
                updates,
                nu_hat,
                is_leaf=lambda x: x is None,
            )
# ---
def _slice_cache_in_ray(cfg: SliceCacheConfig):
    logging.basicConfig(level=logging.INFO)
    logger.info(f"Starting slice cache with config: {cfg}")
    return _do_slice_cache(cfg)
# ---
def test_non_monotonic(self):
        """
        L{get_clock_info} only knows about the monotonic clock.
        """
        with pytest.raises(ValueError):
            get_clock_info("not monotonic")
# ---
def __hash__(self) -> int:
        return hash(self._epoch_ms)
# ---
def the_void_name_is_not_filled_by_filling(name, filling):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(rel.RelatedBuildingElement.Name == filling for rel in element.HasFillings):
        assert False, "A filling was found"
# ---
def __init__(self, *args, **kwargs):
        global scheduler
        scheduler = "Current scheduler is PyCOMPSs"
        self.task_instance = task(*args, **kwargs)
# ---
def match(self, item):
        return item.get(self.field) is None
# ---
def apply_gradients(model, grads):
        overwrites, grads = partition_for_grad_overwrite(grads)
        updates = jax.tree_util.tree_map(lambda g: -lr * g, grads)
        model = apply_updates(model, updates, overwrites)
        return model
# ---
def width(self):
        return self.__width
# ---
def test_host_state(self):
        stats = self.conn.get_host_stats()
        self.assertEquals(stats['disk_total'], 10000)
        self.assertEquals(stats['disk_used'], 20000)
        self.assertEquals(stats['host_memory_total'], 10)
        self.assertEquals(stats['host_memory_overhead'], 20)
        self.assertEquals(stats['host_memory_free'], 30)
        self.assertEquals(stats['host_memory_free_computed'], 40)
# ---
def __init__(self, model_name: str, attribute_name: str, k: int = 2, *args, **kwargs):
        self.model_name = model_name
        self.attribute_name = attribute_name
        self.model = self.load_model()
        self.k = k
# ---
def __init__(self, num_shards: int = 4, rows_per_shard: int = 10):
        self._num_shards = num_shards
        self._rows_per_shard = rows_per_shard
# ---
def __init__(self, geom, srs, clip=False):
        self.geom = geom
        self.bbox = geom.bounds
        self.srs = srs
        self.clip = clip
        self._prep_lock = threading.Lock()
        self._prepared_geom = None
        self._prepared_counter = 0
        self._prepared_max = 10000
# ---
def pop(self):
        assert isinstance(self.ast, ast.CharClass)
        last = self.ast.elems[-1]
        self.ast.elems = self.ast.elems[:-1]
        return last
# ---
def testFormatSourceShort(self):
    """Tests the _FormatSourceShort function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))

    source_short_string = test_helper._FormatSourceShort(
        event, event_data, event_data_stream)

    self.assertEqual(source_short_string, 'FILE')
# ---
def tunnel(
        self,
        controller_address: str,
        local_port: int | None = None,
        timeout: float | None = None,
        tunnel_logger: logging.Logger | None = None,
    ) -> AbstractContextManager[str]:
        """Return direct connection for local platform (no tunnel needed)."""
        return nullcontext(controller_address)
# ---
def parse():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument('--bootstrap', action='store_true')
    parser.add_argument('--no-shell-file', action='store_true')
    return parser.parse_args()
# ---
def clip(self, a_min=None, a_max=None) -> Any:  # pragma: no cover
        return haliax.clip(self, a_min=a_min, a_max=a_max)
# ---
def training_data(self) -> Iterator[DataExample]:
        train_dataset = _get_hendrycks_math_train()

        for idx, item in enumerate(train_dataset):
            raw_prompt = item["problem"]
            raw_answer = item["solution"]
            example_id = f"train_{idx}"

            yield self.clean_example(raw_prompt, raw_answer, example_id)
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["Olmo3Config"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfOlmo3Config,
        )
# ---
def convert_to_export(self, value, env):
        """ convert ``value`` from the cache to a valid value for export. The
            parameter ``env`` is given for managing translations.
        """
        if not value:
            return ''
        return value if env.context.get('export_raw_data') else ustr(value)
# ---
def from_now(cls, duration: "Duration") -> "Deadline":
        """Create deadline from a Duration offset from now."""
        return cls(time.monotonic() + duration.to_seconds())
# ---
def max_gen_toks(self):
        return self.leader.max_gen_toks
# ---
def display(self):
        if self.state == "visible":
            image(self.im, self.x, self.y)
# ---
def gram_schmidt(y):
    """ Modified Gram-Schmidt orthonormalization of the matrix y(n,n) """

    n = y.shape[0]
    if y.shape[1] != n:
        raise ValueError("Invalid shape: {}".format(y.shape))
    mo = np.zeros(n)

    # Main loop
    for i in range(n):
        # Remove component in direction i
        for j in range(i):
            esc = np.sum(y[j]*y[i])
            y[i] -= y[j]*esc

        # Normalization
        mo[i] = np.linalg.norm(y[i])
        y[i] /= mo[i]

    return mo
# ---
def filename(self):
        return self._filename
# ---
def __init__(self, name: str, count: int, job_id: Any):
        """Args:
        name: Actor name prefix
        count: Number of actors to discover
        job_id: JobId/JobName for the actor job
        """
        self._name = name
        self._count = count
        self._job_id = job_id
        self._handles: list[ActorHandle] = []
        self._discovered_names: set[str] = set()
# ---
def cross_entropy_loss(
    logits: NamedArray,
    Label: AxisSelector,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: AxisSelection = ...,
) -> NamedArray: ...
# ---
def vocab_size(self) -> int:
        """Vocabulary size derived from the tokenizer."""
        return get_vocab_size_for_tokenizer(self.tokenizer)
# ---
def the_collection_name1_is_in_the_collection_name2(name1, name2):
    assert bpy.data.collections.get(name2).children.get(name1)
# ---
def _default_trainer_for_run(run_name: str) -> TrainerConfig:
    try:
        trainer = DEFAULT_TRAINER_CONFIGS[run_name]
    except KeyError as exc:
        raise ValueError(
            f"No TrainerConfig known for run '{run_name}'. "
            "Update DEFAULT_TRAINER_CONFIGS in exp1803_convert_32b_phases.py with the training step for this run."
        ) from exc
    return deepcopy(trainer)
# ---
def shutdown(self, wait: bool = True) -> None:
        del wait
        self._remote_client.shutdown()
        self._manager.stop()
# ---
def _compute_value(self, records):
        """ Invoke the compute method on ``records``. """
        # initialize the fields to their corresponding null value in cache
        for field in self.computed_fields:
            records._cache[field] = field.null(records.env)
            records.env.computed[field].update(records._ids)
        self.compute(records)
        for field in self.computed_fields:
            records.env.computed[field].difference_update(records._ids)
# ---
def _mesh_axis_totals(self) -> Dict[str, int]:
        ici, dcn = self.mesh.axis_shapes(jax.device_count(), self.num_slices)
        return {name: ici.get(name, 1) * dcn.get(name, 1) for name in set(ici) | set(dcn)}
# ---
def _make_causal_mask(seq_len: int) -> Float[Array, "S S"]:
    """Create a causal attention mask.

    Returns a (seq_len, seq_len) boolean mask where True means "allowed to
    attend". Position i can attend to positions 0..i (inclusive).
    """
    return jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))
# ---
def create_index(self, index=None):
        if index is None:
            index = self.index
        try:
            get_indices(self.es).create(self.index)
        except elasticsearch.TransportError:
            pass
# ---
def extract_data(pipeline_response):
            deserialized = self._deserialize("ManagedInstanceQueryStatistics", pipeline_response)
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)
            return deserialized.next_link or None, iter(list_of_elem)
# ---
def test_visualize_shardings_runs(capsys):
    mesh = jax.sharding.Mesh(
        np.array(jax.devices()).reshape(-1, 1, 1),
        (ResourceAxis.DATA, ResourceAxis.MODEL, ResourceAxis.REPLICA),
    )
    with axis_mapping(resource_map), mesh:
        arr = hax.ones((Dim1, Dim2, Dim3))
        visualize_shardings(arr)

    out = capsys.readouterr().out
    assert "dim1" in out and "dim2" in out and "dim3" in out
# ---
def test_ar_loss_is_positive(params, tiny_cfg):
    batch_size, seq_len = 2, 16
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (batch_size, seq_len), 1, 100)
    loss_mask = jnp.ones((batch_size, seq_len))

    loss, _ = ar_loss(params, token_ids, loss_mask, tiny_cfg)
    assert float(loss) > 0
# ---
def logged(self):
        if len(self.doc.xpath('//b[text()="Session interrompue"]')) > 0:
            return False
        return True
# ---
def items():
        for key, value in d.items():
            if isinstance(value, dict):
                for subkey, subvalue in _flatten_nested_dict(value).items():
                    yield key + "/" + subkey, subvalue
            else:
                yield key, value
# ---
def calculate_sortino(self, mar=None):
        """
        http://en.wikipedia.org/wiki/Sortino_ratio
        """
        if mar is None:
            mar = self.treasury_period_return

        return sortino_ratio(self.algorithm_returns,
                             self.algorithm_period_returns,
                             mar)
# ---
def __post_init__(self):
        if len(self.ids) == 0:
            raise ValueError("PromptCompletion must have at least one token")

        # check that there is at least one token in the response
        if len(self.ids) <= self.prompt_length:
            raise ValueError(
                f"PromptCompletion must have strictly more tokens than the prompt length. Got {len(self.ids)} tokens"
                f" and prompt length {self.prompt_length}"
            )
# ---
def post_with_null_author():
    return make_post(with_author=False)
# ---
def example_reading_spec(self):
    label_key = "image/class/label"
    data_fields, data_items_to_decoders = (
        super(Image2TextProblem, self).example_reading_spec())
    data_fields[label_key] = tf.VarLenFeature(tf.int64)
    data_items_to_decoders["targets"] = contrib.slim().tfexample_decoder.Tensor(
        label_key)
    return data_fields, data_items_to_decoders
# ---
def do_fold(init, *extra_args, **extra_kwargs):
            carry = haliax.fold(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *extra_args, **extra_kwargs)
            return carry
# ---
def validation_set(self) -> Optional[ProcessedAudioCache]:
        return self.build_or_load_cache(self.validation_split)
# ---
def normalizerootdir(dir, funcname):
    if dir == ".":
        util.nouideprecwarn(
            "match.%s() no longer accepts '.', use '' instead." % funcname, "20190805"
        )
        return ""
    return dir
# ---
def remaining_seconds(self) -> float:
        """Get remaining seconds until deadline (0.0 if expired)."""
        return max(0.0, self._deadline - time.monotonic())
# ---
def test_deadline_remaining_time():
    """Deadline correctly reports remaining time."""
    deadline = Deadline.from_seconds(1.0)

    # Initially should have close to 1 second remaining
    remaining = deadline.remaining_seconds()
    assert 0.95 < remaining <= 1.0

    # After sleeping, remaining time should decrease
    time.sleep(0.1)
    remaining_after = deadline.remaining_seconds()
    assert remaining_after < remaining
    assert 0.85 < remaining_after < 0.95
# ---
def test_str_split(self):
        def test_impl(df):
            return df.A.str.split(',')

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D', 'G', '', 'g,f']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def __init__(self, config: config_pb2.IrisClusterConfig):
        self.config = config
        self._manual_config = config.controller.manual
        self._bootstrapped = False

        if not self._manual_config.host:
            raise RuntimeError("controller.manual.host is required for ManualController")

        port = self._manual_config.port or DEFAULT_CONTROLLER_PORT
        self.address = f"http://{self._manual_config.host}:{port}"
# ---
def __getitem__(self, key):
        if not utils.is_dict_like(key):
            raise TypeError('can only lookup dictionaries from Dataset.loc')
        return self.dataset.sel(**key)
# ---
def the_void_name_is_filled_by_filling(name, filling):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(rel.RelatedBuildingElement.Name == filling for rel in element.HasFillings):
        return True
    assert False, "No filling found"
# ---
def job_id(self) -> JobName | None:
        return self._job.job_id if self._job else None
# ---
def test_delete_missing_snapshot(self):
        drv = self._driver
        mox = self._prepare_delete_snapshot_mock(False)

        drv.delete_snapshot(FakeSnapshot())

        mox.VerifyAll()
# ---
def fsspec_get_atomic_directories(dir_path):
    """
    Get all directories under this directory that only contains files within them
    """
    subdirectories = []

    if fsspec_isdir(dir_path):
        for subdir in fsspec_get_curr_subdirectories(dir_path):
            if fsspec_dir_only_contains_files(subdir):
                subdirectories.append(subdir)
            else:
                subdirectories.extend(fsspec_get_atomic_directories(subdir))

    return subdirectories
# ---
def _max_params_for_budget(self, budget: float) -> float:
        """Compute max_params as a function of budget.

        Returns base_max_params for budgets <= base_max_params_budget,
        then scales with sqrt(budget) for larger budgets, capped at global_max_params.
        """
        scaling = self.base_max_params * math.sqrt(budget / self.base_max_params_budget)
        return min(max(self.base_max_params, scaling), self.global_max_params)
# ---
def test_add_to_aggregate_called(self):
        def fake_add_to_aggregate(context, aggregate, host):
            fake_add_to_aggregate.called = True
        self.stubs.Set(self.conn._pool,
                       "add_to_aggregate",
                       fake_add_to_aggregate)

        self.conn.add_to_aggregate(None, None, None)
        self.assertTrue(fake_add_to_aggregate.called)
# ---
def do_block_with_args(block: M, *args, **kwargs) -> OutputT_co:
                return fn(block, *args, **kwargs)
# ---
def _import_image(self, url=None, name=None, tag=None):
        ''' perform image import '''
        cmd = ['import-image']

        image = '{0}'.format(name)
        if tag:
            image += ':{0}'.format(tag)

        cmd.append(image)

        if url:
            cmd.append('--from={0}/{1}'.format(url, image))

        cmd.append('-n{0}'.format(self.namespace))

        cmd.append('--confirm')
        return self.openshift_cmd(cmd)
# ---
def _stop(self) -> threading.Event:
        return self._managed_thread.stop_event
# ---
def _Delete(self):
    """Delete the log group."""
    delete_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'delete-log-group',
        '--log-group-name', self.name
    ]
    vm_util.IssueCommand(delete_cmd, raise_on_failure=False)
# ---
def test_dropout(self):
    testing_utils.layer_test(
        keras.layers.Dropout, kwargs={'rate': 0.5}, input_shape=(3, 2))

    testing_utils.layer_test(
        keras.layers.Dropout,
        kwargs={'rate': 0.5,
                'noise_shape': [3, 1]},
        input_shape=(3, 2))
# ---
def lavita_allprocessed_to_dolma(row):
    try:
        return {
            "id": hashlib.sha256((row["instruction"] + row["input"] + row["output"]).encode("utf-8")).hexdigest(),
            "text": row["instruction"] + "\n\n" + "Context: \n" + row["input"] + "\n\n" + "Answer: \n" + row["output"],
            "source": "lavita/medical-qa-datasets/all-processed",
        }
    except Exception as e:
        print(e)
        return None
# ---
def _trigger_callback(self):
        self.callback()
# ---
def _download(self, gcs_path: str, local_path: Path) -> None:
        with fsspec.open(gcs_path, "rb") as src:
            with open(local_path, "wb") as dst:
                dst.write(src.read())
# ---
def __eq__(self, other):
        if not isinstance(other, MultiCoverage):
            return NotImplemented

        if self.bbox != other.bbox:
            return False

        if len(self.coverages) != len(other.coverages):
            return False

        for a, b in zip(self.coverages, other.coverages):
            if a != b:
                return False

        return True
# ---
def local_keys(self):
        '''
        Return a dict of local keys
        '''
        ret = {'local': []}
        fn_ = os.path.join(self.opts['pki_dir'], 'local.key')
        if os.path.isfile(fn_):
            ret['local'].append(fn_)
        return ret
# ---
def get_init_log(self, vm_id: str, tail: int | None = None) -> str:
        """Get initialization log for a VM."""
        ...
# ---
def isexact(self):
        """Matcher will match exactly the list of files in .files() --
        optimization might be possible."""
        return False
# ---
def iscomplex(a: A) -> A:
    return wrap_elemwise_unary(jnp.iscomplex, a)
# ---
def update_num(n):
      hasher.update(compat.as_bytes("%x" % n))
# ---
def __getitem__(self, idx: int) -> PageCacheT:
        return self.caches[idx]
# ---
def LaunchJob(self, request, context):
        """Job lifecycle"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def name(self) -> str | None:
        return self._thread.name
# ---
def test_lambda_output_shape_list_multiple_outputs(self):

    def lambda_fn(x):
      return x

    l = keras.layers.Lambda(lambda_fn, output_shape=[(10,), (20,)])
    output_shape = l.compute_output_shape([(10, 10), (10, 20)])
    self.assertAllEqual([(10, 10), (10, 20)], output_shape)
# ---
def test_augment_bank_preserves_originals(bank):
    original_entries: dict[str, set[str]] = {}
    for node_type, entries in bank.entries.items():
        original_entries[node_type] = {e.source for e in entries}

    augmented, _ = augment_bank_with_egraph(bank)

    for node_type, sources in original_entries.items():
        augmented_sources = {e.source for e in augmented.entries.get(node_type, [])}
        for s in sources:
            assert s in augmented_sources, f"Original entry missing: {s!r}"
# ---
def max_queued_tokens(self) -> int:
        return self.tokens.axis_size("position")
# ---
def __init__(self, notebook, job_data):
        self.notebook = notebook
        self.data = job_data
        self.platform_id = job_data['platform_id']
        self.capsule_type = job_data['capsule_code']
        self.id = job_data['id']
        self.name = job_data['name']
        self.last_run = None
# ---
def match(self, item):
        return self.value_match(self.pattern, item.get(self.field))
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step: Optional[int], commit: Optional[bool] = None):
        if step is not None:
            self.metrics[f"step_{step}"] = metrics
        else:
            self.metrics.update(metrics)
# ---
def test_number_set(self):
        """Store and retrieve a number set"""
        self.make_table()
        item = {
            "id": "a",
            "datas": set([1, 2, 3]),
        }
        self.dynamo.put_item("foobar", item)
        ret = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(ret, item)
# ---
def get_apartment_count():
        # type: () -> int
        return Apartment.select().count()
# ---
def update_secret(self, key, value):
        ''' update a secret'''
        if key in self.secrets:
            self.secrets[key] = value
        else:
            self.add_secret(key, value)

        return True
# ---
def _get_autoscaler_status(controller_url: str) -> vm_pb2.AutoscalerStatus:
    client = cluster_connect.ControllerServiceClientSync(controller_url)
    request = cluster_pb2.Controller.GetAutoscalerStatusRequest()
    return client.get_autoscaler_status(request).status
# ---
def evi(domain, b):
    '''Simple EVI based classifier'''
    #no_clouds = b['b3'].lte(2100).select(['sur_refl_b03'], ['b1'])
    criteria1 = b['EVI'].lte(0.3).And(b['LSWI'].subtract(b['EVI']).gte(0.05)).select(['sur_refl_b02'], ['b1'])
    criteria2 = b['EVI'].lte(0.05).And(b['LSWI'].lte(0.0)).select(['sur_refl_b02'], ['b1'])
    #return no_clouds.And(criteria1.Or(criteria2))
    return criteria1.Or(criteria2)
# ---
def test_addition_task_reward():
    task = AdditionTask()
    examples = task.generate_examples(10, np.random.default_rng(42))
    assert len(examples) == 10
    assert all("+" in ex["prompt"] for ex in examples)

    assert task.compute_reward("42", "42") == pytest.approx(1.0)
    assert task.compute_reward("42", "43") == pytest.approx(0.0)
    assert task.compute_reward("42", "-") == pytest.approx(0.0)
    assert task.compute_reward("42", "-2") == pytest.approx(0.0)
# ---
def ensure_first(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.rearrange((axis, ...))
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            return leaf
# ---
def mod(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.mod(x1, x2)
# ---
def __getitem__(cls, item: NamedArrayAxesSpec) -> typing.Any:
        axes = _parse_namedarray_axes(item)
        return typing.Annotated[NamedArray, axes]
# ---
def test_stop_not_inited(self):
        profiler.clean()
        profiler.stop()
# ---
def __list_handlers(self):
        """ Return a dict of { handler_name, method, ... }.
        All methods that do not being with an underscore will be exposed.
        We also make sure to not expose our register_rpc method.
        """
        handlers = {}
        for attr in dir(self):
            if self.__is_public_valid_method(attr):
                handlers[attr] = getattr(self, attr)
        return handlers
# ---
def output_filename(self, suffix: str = ".jsonl.gz") -> str:
        return f"{self.benchmark}_{self.date_range}{suffix}"
# ---
def stop(self,solverId):
    path = "{base}/{solverId}/stop".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.put(path,
                       params=self.params,
                       headers=self.headers)
    self.validateReply(req)
    return True
# ---
def __init__(self, text: str):
        self.text = text
# ---
def test_olmo2_param_counts_dont_change_with_seqlen():
    model = Olmo2LMHeadModel.init(hax.Axis("v", 2048), _get_olmo2_config(seq_len=128), key=random.PRNGKey(0))
    model2 = Olmo2LMHeadModel.init(hax.Axis("v", 2048), _get_olmo2_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def axis_indices(self, axis: AxisSelector) -> int | None:  # type: ignore
        ...
# ---
def __init__(self, host: str = "0.0.0.0", port: int = 9999):
        self.host = host
        self.port = port
        self.queues: dict[str, MemoryQueue] = {}
        self.app = self._create_app()

        import uvicorn

        config = uvicorn.Config(self.app, host=host, port=port, log_level="error", access_log=False)
        self.server = uvicorn.Server(config)
        self.server_thread: ServerThread | None = None
# ---
def debug_strings(self):
        print(self.strings)
# ---
def as_dict(self):
        dict = dataclasses.asdict(self)
        # remove Nones
        return {k: v for k, v in dict.items() if v is not None}
# ---
def open_shard(self, shard_name: str) -> Iterator[T_co]:
        return self.open_shard_at_row(shard_name, 0)
# ---
def __float__(self) -> float:
        """Coerce Metric to float outside of a JIT context."""
        return float(self.value())
# ---
def test_as_remote_kwargs_gpu_auto():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=GpuConfig(variant="auto", count=1))
    kwargs = as_remote_kwargs(config)
    assert kwargs["num_gpus"] == 1
    assert "accelerator_type" not in kwargs
# ---
def default_choice_name(cls) -> Optional[str]:
        return "adam"
# ---
def printStatusThread(q0, q1, q2, q3):
	strtime = time.time()
	while True:
		sys.stdout.write('\r\x1b[K')
		sys.stdout.write("urls:" + str(q0.qsize()) + " | ")
		sys.stdout.write("userids:" + str(q1.qsize()) + " | ")
		sys.stdout.write("user infos:" + str(q2.qsize()) + " | ")
		sys.stdout.write("manager infos:" + str(q3.qsize()))
		sys.stdout.flush()
		time.sleep(1)
# ---
def add_target_handle(self, layer):
        input_handle = layer.register_forward_hook(get_target_input)
        self.target_handle = input_handle
# ---
def test_not_true(self):
        expr = ~(col("flag") == True)  # noqa: E712
        assert expr.evaluate({"flag": True}) is False
        assert expr.evaluate({"flag": False}) is True
# ---
def test_spawn_not_enough_memory(self):
        self.assertRaises(exception.InsufficientFreeMemory,
                          self._test_spawn,
                          1, 2, 3, "4")
# ---
def get_spectrum(self):

		spectrum = []
		now = self.time()

		for time in self.channels:
			if now - time < .5:
				p = random.randint(-40, -20)
			else:
				p = random.randint(-90, -80)

			spectrum.append(p)

		return tuple(spectrum)
# ---
def get_advanced_inputs(desc, verbose):
    details = desc.get("details")
    if not verbose and isinstance(details, dict):
        return details.get("advancedInputs", [])
    return []
# ---
def set_mesh(mesh: MeshLike) -> ContextManager[None]:
    """Compatibility wrapper around `mesh_context` matching the JAX 0.7 API."""

    return mesh_context(mesh)
# ---
def prefix_info(lines, software, version, author, comment_mark='//'):
    """Prefix information to the given lines with given comment-mark.
    """
    prefix = ['%s Generated by the %s v%s' % (comment_mark,
              software, version)]
    prefix += ['%s    !author: %s' % (comment_mark, author)]
    prefix += ['%s    !trail: %s %s' % (comment_mark,
               os.path.basename(sys.argv[0]), ' '.join(sys.argv[1:]))]
    return prefix + lines
# ---
def __init__(self, xPos, yPos):
        super(Block, self).__init__(xPos, yPos)
        self.state = "visible"
# ---
def enable_chaos(
    key: str,
    failure_rate: float = 1.0,
    error: Exception | None = None,
    delay_seconds: float = 0.0,
    max_failures: int | None = None,
) -> None:
    _rules[key] = ChaosRule(
        failure_rate=failure_rate,
        error=error,
        delay_seconds=delay_seconds,
        max_failures=max_failures,
    )
# ---
def checkpoint_exists(repo_id: str, step: int, version_name: str) -> bool:
    """Check if a specific revision exists in a Hugging Face repository."""
    try:
        api = HfApi()
        commits = api.list_repo_commits(repo_id=repo_id)
        for commit in commits:
            if f"step {step}" in commit.title:
                return True
        return False
    except Exception:
        return False
# ---
def __add__(self, other):
        lst = []
        for k in self.lookup:
            lst.append((k, self.lookup[k]))
        for k in other.lookup:
            lst.append((k, other.lookup[k]))
        return Enumeration(lst)
# ---
def test_olmo2_decoder_layer(use_flash, num_kv_heads):
    config = _get_olmo2_config(use_flash=use_flash, num_kv_heads=num_kv_heads)
    key = random.PRNGKey(0)

    layer = Olmo2DecoderLayer.init(config=config, key=key)

    x, mask = _get_random_inputs(config)
    out = layer(x, mask)

    # Check output has correct shape
    assert out.array.shape == x.array.shape
    assert out.axes == x.axes
# ---
def get_logs(self, task_id: str, start_line: int = 0) -> list[cluster_pb2.Worker.LogEntry]: ...
# ---
def _drop_vars(self, names):
        self._assert_all_in_dataset(names)
        drop = set(names)
        drop |= set(k for k, v in iteritems(self._variables)
                    if any(name in v.dims for name in names))
        variables = OrderedDict((k, v) for k, v in iteritems(self._variables)
                                if k not in drop)
        coord_names = set(k for k in self._coord_names if k in variables)
        return self._replace_vars_and_dims(variables, coord_names)
# ---
def create_agent():
        return Agent.objects.create(
            type=AgentType.objects.create(main_type=MainAgentType.objects.create()),
            ref_code=RefCode.objects.create(
                country=Country.objects.get(iso='SE'),
                repository_code='repo',
            ),
            level_of_detail=0,
            record_status=0,
            script=0,
            language=Language.objects.get(iso_639_1='sv'),
            create_date=timezone.now(),
        )
# ---
def ready_count(self) -> int:
        """Number of VMs in READY state."""
        return sum(1 for v in self.vms if v.state == vm_pb2.VM_STATE_READY)
# ---
def run(self):
        cmd = '''
              docker run --rm -v rita_store:/rita/data  rita/test-python --inputfile {} --outputfile {}
        '''.format(os.path.join("/rita/data", os.path.basename(self.input().path)),
                   os.path.join("/rita/data", os.path.basename(self.output().path)))

        logger.debug(cmd)

        out = subprocess.call(cmd, shell=True)

        logger.debug(out)
# ---
def vars(self):  # pragma: no cover
        warnings.warn('the Dataset property `vars` has been deprecated; '
                      'use `data_vars` instead',
                      FutureWarning, stacklevel=2)
        return self.data_vars
# ---
def mask_fn(param, path):
            path_str = ".".join(path) if isinstance(path, (list, tuple)) else str(path)
            if "Embedding" in path_str:
                return "adam"
            elif "lm_head" in path_str:
                return "adamh"
            elif isinstance(param, Linear):
                # muonh for linear layers
                return dataclasses.replace(param, weight="muonh", bias="adam" if param.bias is not None else None)
            else:
                return "adam"
# ---


def fizz_buzz(n: int):
    """Return the number of times the digit 7 appears in integers less than n which are divisible by 11 or 13.
    >>> fizz_buzz(50)
    0
    >>> fizz_buzz(78)
    2
    >>> fizz_buzz(79)
    3
    """
    ns = []
    for i in range(n):
        if i % 11 == 0 or i % 13 == 0:
            ns.append(i)
    s = ''.join(list(map(str, ns)))
    ans = 0
    for c in s:
        ans += (c == '7')
    return ans
# ---
def canonicalize(self, step: ExecutorStep) -> ExecutorStep:
        """Multiple instances of `ExecutorStep` might have the same version."""
        return self.version_str_to_step[self.version_strs[step]]
# ---
def pytest_collection_modifyitems(config: Any, items: list[pytest.Item]) -> None:
    # If the --run-benchmark flag is set, do not skip anything.
    if config.getoption("--run-benchmark"):
        return
    # If the flag is not set, we check every test item
    skip_benchmark = pytest.mark.skip(reason="need --run-benchmark option to run")
    for item in items:
        if "benchmark" in item.fixturenames:
            item.add_marker(skip_benchmark)
# ---
def test_pspec_for_namedarray_with_missing_array():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        named = NamedArray(None, ("dim2", "dim3"))

        spec = pspec_for(named)

        assert spec == PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)
# ---
def remove(self, item):
        """Remove an element from the queue.

        Parameters
        ----------
        item :
            The element to remove.

        """
        self._queue.remove(item)
# ---
def _generate_nonce(ts, secret, salt=None, chars=string.hexdigits.upper()):
	# TODO: Add IP-address to nonce
	if not salt:
		try:
			rng = random.SystemRandom()
		except NotImplementedError:
			rng = random
		salt = ''.join(rng.choice(chars) for i in range(16))
	ctx = hashlib.md5(('%s:%s:%s' % (ts, salt, secret)).encode())
	return ('%s:%s:%s' % (ts, salt, ctx.hexdigest()))
# ---
def __init__(self, size):
        self._data = np.zeros((size,))
        self._capacity = size
        self._size = 0
# ---
def add_pilot (self, pid) :
        """
        add (Compute or Data)-Pilot(s) to the pool
        """

        raise Exception ("%s.add_pilot() is not implemented" % self.__class__.__name__)
# ---
def __init__(self, Vocab: hax.AxisSelector = "vocab"):
        self.Vocab = Vocab
# ---
def list_jobs(self) -> list[cluster_pb2.JobStatus]:
        request = cluster_pb2.Controller.ListJobsRequest()
        response = self._client.list_jobs(request)
        return list(response.jobs)
# ---
def worker_func(args):
    self = args[0]
    m = args[1]
    k = args[2]
    r = args[3]

    return (self.eval_func(m, k, r) -
            self.eval_func(m, k, self.rt) -
            self.temporal_diff_sum(m, k)) ** 2
# ---
def test_dupfile_on_textio():
    io = py.io.TextIO()
    f = capture.safe_text_dupfile(io, "wb")
    f.write("hello")
    assert io.getvalue() == "hello"
    assert not hasattr(f, 'name')
# ---
def broadcast_arrays_and_return_axes(
    *arrays: NamedOrNumeric, require_subset: bool = True, ensure_order: bool = True
) -> tuple[tuple[NamedOrNumeric, ...], tuple[Axis, ...]]: ...
# ---
def __bool__(self):
        return False
# ---
def convert_to_read(self, value, use_name_get=True):
        # Integer values greater than 2^31-1 are not supported in pure XMLRPC,
        # so we have to pass them as floats :-(
        if value and value > xmlrpclib.MAXINT:
            return float(value)
        return value
# ---
def h_fs_mkdir (_,path): os.mkdir(path)
# ---
def Embed(self) -> Axis:
        return self.config.Embed
# ---
def compute_loss(model: LmHeadModel, example: LmExample):
            with hax.axis_mapping(compute_axis_mapping):
                model = inference_mode(model, True)
                model = mp.cast_to_compute(model)
                return model.compute_next_token_loss(example, key=None)
# ---
def __setstate__(self, state):
        self.__init__(**state)
# ---
def match_entries_by_date_and_amount(self, threshold):
        self.selected_pane.match_entries_by_date_and_amount(threshold)
        self.import_table.refresh()
# ---
def create_loss_fn(self, reference_model: eqx.Module, train_model: eqx.Module) -> Callable:
        """Create the loss function for training."""
        ...
# ---
def test_cancel(self, cr, uid, ids, context=None):
        """ Test whether the move lines are canceled or not.
        @return: True or False
        """
        for pick in self.browse(cr, uid, ids, context=context):
            for move in pick.move_lines:
                if move.state not in ('cancel',):
                    return False
        return True
# ---
def main():
    if os.getenv("CI", None) is not None:
        logger.info("Skipping experiment execution on CI environment, needs HF access.")
        return

    runs = [
        build_config("130m"),
        build_config("300m"),
        build_config("520m"),
        build_config("1_2b"),
    ]

    steps = []
    for name, cfg in runs:
        cfg.print_run_info()
        steps.extend(default_speedrun(name, cfg))

    executor_main(steps=steps, description="AdamH speedruns (Chinchilla optimal)")
# ---
def testHarmonics(self):
        # Regression test for the 'harmonics' parameter
        tone = 100. # arbitrary frequency [Hz]
        freqs = [tone, tone*2, tone*3, tone*4]
        mags = [1]*4

        hpcpAlg = HPCP(minFrequency=50, maxFrequency=500, bandPreset=False, harmonics=3)
        hpcp = hpcpAlg(freqs, mags)
        expected = [0., 0., 0., 0.1340538263, 0., 0.2476127148, 0., 0., 0., 0., 1., 0.]
        self.assertAlmostEqualVector(hpcp, expected, 1e-4)
# ---
def params(self):
        param = {
            'x': 0,
            'y': 0,
            'w': 500,
            'h': 500,
            'title': '% Type Prog Title Here %',
        }
        return param
# ---
def print_user_desc(desc):
    print_field("ID", desc["id"])
    print_field("Name", desc["first"] + " " + ((desc["middle"] + " ") if desc["middle"] != '' else '') + desc["last"])
    if "email" in desc:
        print_field("Email", desc["email"])

    bill_to_label = "Default bill to"
    if "billTo" in desc:
        print_field(bill_to_label, desc["billTo"])

    if "appsInstalled" in desc:
        print_list_field("Apps installed", desc["appsInstalled"])
# ---
def device_mesh(self) -> Mesh:
        return self.config.device_mesh
# ---
def __init__(self, client):
        super(Model_LicenseInfo, self).__init__(client)
        Util.validate_type(client, "saklient.cloud.client.Client")
# ---
def confirmPopup(title, msg, answerCallback):
    content = ConfirmPopup(text=msg)
    content.bind(on_answer=answerCallback)
    popup = Popup(title=title,
                    content=content,
                    size_hint=(None, None),
                    size=(dp(600),dp(200)),
                    auto_dismiss= False)
    popup.open()
    return popup
# ---
def setUp(self):
        import time
        self.repo_path = tempfile.mkdtemp(prefix='selftest-buildhistory',
            dir=get_bb_var('TOPDIR'))

        self.repo = Repo.init(self.repo_path)
        self.test_file = "test"
        self.var_map = {}
# ---
def transform_dclm_hq(html: str) -> str:
    """Transform DCLM HQ HTML to markdown."""
    output = convert_page(
        html,
        extract_method="resiliparse",
        config=ResiliparseConfig(
            links=False,
            skip_elements=[],
            markdownify_config=HtmlToMarkdownConfig(
                include_images=False,
                include_links=False,
            ),
        ),
    )
    return output["content"]
# ---
def is_being_preempted(self) -> bool:
        """
        Check if the TPU slice is being preempted.
        This is a workaround for the fact that Ray doesn't expose this information directly.
        """
        return get_current_tpu_is_preempted()
# ---
def _field(self, name):
		"""
		Returns the field 'name'
		"""
		index = self.structure.index(name)
		return self.structure[index]
# ---
def can_be_scheduled(self) -> bool:
        """Check if task is ready to be scheduled.

        A task can be scheduled if:
        - It has no attempts yet (fresh task), or
        - Its current attempt is terminal AND it should retry
        """
        if not self.attempts:
            return True
        return self.attempts[-1].is_terminal() and not self.is_finished()
# ---
def result(self) -> bytes | None:
        """Serialized task result (cloudpickle), if available."""
        ...
# ---
def loss_ref(x_raw, w_raw):
        loss_ref, lse_ref = linear_softmax_cross_entropy_loss_reference(
            x_raw.reshape(6, 4),
            y.reshape(6),
            w_raw,
        )
        loss_ref = loss_ref + logsumexp_weight * (lse_ref**2)
        return loss_ref.mean()
# ---
def compute_hash(doc):
        content = doc["content"]
        return hashlib.md5(content.encode()).hexdigest()
# ---
def _delete(self):
        """
        Delete myself by my own id.
        As of 20170114 no other methods call me. You must do `foo._delete()`
        :return:
        """
        command = ['ec2', 'delete-security-group', '--region', self.region,
                   # '--dry-run',
                   '--group-id', self.id
                   ]
        bin_aws(command, decode_output=False)
        print('Deleted {0}'.format(command))  # TODO: Log(...)
        return True
# ---
def attach(self, **kwargs):
        pass
# ---
def _set_pitch(self, value):
        self._parameter1 = value
# ---
def info(self):
        raise NotImplementedError
# ---
def init(wrapped: hnn.Linear, r: int, alpha: float, dropout: float = 0.0, *, key):
        """
        Initializes a LoraLinear module.
        """
        lora = LowRankLinear.init(wrapped.In, wrapped.Out, r, alpha, dropout, key=key)
        return LoraLinear(wrapped, lora)
# ---
def delete_job(job_id):
    try:
        job = _get_job(job_id)
        _validate_job_finished(job)
        _remove_job(job_id)
    except ClientError as e:
        logging.info('Cannot delete job, error: %s', e)
        return errCode[e.err_name]
    return {'status': doneCode}
# ---
def __init__(self, Height: Axis, Width: Axis, begin, end, stride):
        super().__init__()
        self.Height = Height
        self.Width = Width
        self.begin = begin
        self.end = end
        self.stride = stride
# ---
def get_evals(self):
        return self.evals
# ---
def llama3_tokenizer():
    """Llama 3 tokenizer with chat template (uses tiktoken, not sentencepiece)."""
    return AutoTokenizer.from_pretrained("NousResearch/Meta-Llama-3-8B-Instruct")
# ---
def finger_all(self):
        '''
        Print out all fingerprints
        '''
        matches = self.key.finger('*')
        salt.output.display_output(
                matches,
                'key',
                self.opts)
# ---
def command(self, name, *args):
        """ Execute a raw command """
        args = [name.encode('utf-8')] + [ (arg if type(arg) is bytes else str(arg).encode('utf-8'))
                for arg in args if arg is not None ] + [None]
        _mpv_command(self.handle, (c_char_p*len(args))(*args))
# ---
def get_bit_length(self):
        """Return the number of bits that will be read"""
        return self._BitLength
# ---
def test_fused_cross_entropy_xla_matches_reference():
    x, w, y = _make_toy_inputs()

    loss = fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
        x.reshape(6, 4),
        y.reshape(6),
        w,
        reduction=None,
        logsumexp_weight=0.0,
        implementation="xla",
    )

    loss_ref, _ = linear_softmax_cross_entropy_loss_reference(
        x.reshape(6, 4),
        y.reshape(6),
        w,
    )

    assert jnp.allclose(loss, loss_ref, atol=1e-5, rtol=1e-5)
# ---
def test_skip_step_config_wrap_basic_setup():
    mock_optimizer = mock_optimizer_transform()
    skip_config = SkipStepConfig()

    wrapped_optimizer = skip_config.wrap(mock_optimizer)

    assert isinstance(wrapped_optimizer, optax.GradientTransformation)
    # Check if it has init and update attributes
    assert hasattr(wrapped_optimizer, "init")
    assert hasattr(wrapped_optimizer, "update")
# ---
def get(ref: NamedRef, idx: SliceSpec | EllipsisType = Ellipsis) -> NamedArray:
    """Functional helper equivalent to `ref[idx]`."""
    return ref[idx]
# ---
def asGenData(self):
        """ @rtype: GenData """
        impl_type = EnkfNode.cNamespace().get_impl_type(self)
        assert impl_type == ErtImplType.GEN_DATA

        return GenData.createCReference(self.valuePointer(), self)
# ---
def exitWalk(self):
        self.ignore(self.walkDoneEvent)
        self.walk.exit()
# ---
def lower(self, *args: Args.args, **kwargs: Args.kwargs) -> jax.stages.Lowered:
        raise NotImplementedError
# ---
def _flatten(metrics: Mapping[str, Any], prefix: str = "") -> dict[str, Any]:
    out: dict[str, Any] = {}
    for k, v in metrics.items():
        name = f"{prefix}/{k}" if prefix else k
        if isinstance(v, Mapping):
            out.update(_flatten(v, name))
        else:
            out[name] = v
    return out
# ---
def setUpClass(cls):
        if os.path.exists(db):
            os.remove(db)
        cls.conn = sqlite3.connect(db, detect_types=sqlite3.PARSE_DECLTYPES)
        st0 = option.CsvStore(kid='/base/')
        st0.merge_file(c1)
        st0.validate()
        cls.desc = st0.desc
# ---
def should_allow_eval(expr: str):
    # we don't want to try parsing unknown text or functions of more than two variables
    if count_unknown_letters_in_expr(expr) > 2:
        return False

    for bad_string in BAD_SUBSTRINGS:
        if bad_string in expr:
            return False

    return all(re.search(bad_regex, expr) is not None for bad_regex in BAD_REGEXES)
# ---
def default_validation_sets(tokenizer: str, base_path: str = "tokenized/") -> dict[str, TokenizerStep]:
    # Avoid circular dependencies
    # TODO: Will - break apart defaults a bit
    from experiments.evals.exp1600_uncheatable_evals import uncheatable_eval_tokenized

    validation_sets = dict(paloma_tokenized(base_path=base_path, tokenizer=tokenizer))
    validation_sets.update(uncheatable_eval_tokenized(base_path=base_path, tokenizer=tokenizer))
    return validation_sets
# ---
def test_get_last_historic(self):
        serial = "__TEST__"

        self.db.add_historic(serial, 0, 0)
        assert_equals(self.db.get_last_historic(serial), 0)

        self.db.add_historic(serial, 300, 0)
        assert_equals(self.db.get_last_historic(serial), 300)

        self.db.add_historic(serial, 3600, 0)
        assert_equals(self.db.get_last_historic(serial), 3600)

        self.db.add_historic(serial, 2000, 0)
        assert_equals(self.db.get_last_historic(serial), 3600)
# ---
def axis_size(self, axis: Sequence[AxisSelector]) -> tuple[int, ...]:  # type: ignore
        ...
# ---
def Embed(self) -> Axis:
        return self.hyena.Embed
# ---
def log_sigmoid(a: A) -> A:
    return wrap_elemwise_unary(jnn.log_sigmoid, a)
# ---
def __call__(self, x, *, key):
            return self.layers.fold(x, key=jax.random.split(key, self.layers.Block.size))
# ---
def GetIntValue( variable ):
  return int( vim.eval( variable ) )
# ---
def __init__(self, vocab_size: int, eos_id: int = 3):
        self.Vocab = Axis("vocab", vocab_size)
        self.eos = eos_id
# ---
def _maybe_override_auto_build_caches(config: TrainLmConfig, auto_build: bool) -> TrainLmConfig:
    data = config.data
    if data.auto_build_caches != auto_build:
        logger.info("Overriding auto_build_caches to %s", auto_build)
        data = dataclasses.replace(data, auto_build_caches=auto_build)
        config = replace(config, data=data)
    return config
# ---
def queue(request):
    """Parameterized fixture providing FileQueue and HttpQueue implementations."""
    if request.param == "file":
        with tempfile.TemporaryDirectory() as tmpdir:
            yield FileQueue(path=tmpdir)
    elif request.param == "http":
        with HttpQueueServer(host="127.0.0.1", port=9999) as server:
            yield server.new_queue("test-queue")
    else:
        raise ValueError(f"Unknown queue type: {request.param}")
# ---
def __init__(self, address: str = "auto", namespace: str | None = None):
        self._address = os.environ.get("RAY_ADDRESS", "auto") if address == "auto" else address
        if namespace is None:
            self._namespace = f"fray_{uuid.uuid4().hex[:8]}"
        else:
            self._namespace = namespace
        self._dashboard_address = self._get_dashboard_address()
        logger.info("RayClient connected to %s (namespace=%s)", self._address, self._namespace)
# ---
def load_apartment(apartment_id):
        # type: (int) -> Optional[ApartmentDTO]
        apartment_orm = Apartment.select().where(Apartment.id == apartment_id).first()
        if apartment_orm is None:
            return None
        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
        return apartment_dto
# ---
def with_segment_ids(
        self, q_segment_ids: Int[Array, "..."], kv_segment_ids: Int[Array, "..."] | None = None
    ) -> "AttentionMask":
        kv_ids = q_segment_ids if kv_segment_ids is None else kv_segment_ids
        return AttentionMask(
            is_causal=self.is_causal,
            segment_ids=(q_segment_ids, kv_ids),
            sliding_window=self.sliding_window,
        )
# ---
def setup(self):
        IfcStore.purge()
        bpy.ops.wm.read_homefile(app_template="")
        bpy.data.batch_remove(bpy.data.objects)
        bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)
        blenderbim.bim.handler.setDefaultProperties(None)
        bpy.ops.bim.create_project()
# ---
def __init__(self):
        self._min_known_len = 0
# ---
def print_key(self, match):
        self._call_all('print_key', match)
# ---
def counting_flat_map(self, path):
        self.flat_map_count += 1
        return load_file(path)
# ---
def test_pickling_and_unpickling_encoded_file():
    # See https://bitbucket.org/pytest-dev/pytest/pull-request/194
    # pickle.loads() raises infinite recursion if
    # EncodedFile.__getattr__ is not implemented properly
    ef = capture.EncodedFile(None, None)
    ef_as_str = pickle.dumps(ef)
    pickle.loads(ef_as_str)
# ---
def GetBoolValue( variable ):
  return bool( int( vim.eval( variable ) ) )
# ---
def __hash__(self):
        """Hash."""
        return hash(self.verb)
# ---
def _is_slice_like(value: Any) -> bool:
    return isinstance(value, (slice, range, HaliaxDSlice)) or is_pallas_dslice(value)
# ---
def is_empty(self, resource):
        args = self._es_args(resource)
        res = self.es.count(body={'query': {'match_all': {}}}, **args)
        return res.get('count', 0) == 0
# ---
def test_build_runtime_env_cpu_sets_jax_platforms():
    from fray.v2.ray_backend.backend import build_runtime_env

    request = JobRequest(
        name="cpu-test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=CpuConfig()),
    )
    env = build_runtime_env(request)
    assert env["env_vars"]["JAX_PLATFORMS"] == "cpu"
# ---
def test_simple_limit(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).limit(2),
            [(1, 1, 2), (2, 2, 3)],
        )
# ---
def pareto(key, shape: AxisSpec, b: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    b = broadcast_to(b, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.pareto(key, b.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def unprotect(self, tag: str) -> None:
        """No-op for local provider (no eviction)."""
        del tag
# ---
def assertBlobEqual(self, container_name, blob_name, expected_data):
        blob = self.bsc.get_blob_client(container_name, blob_name)
        actual_data = blob.download_blob()
        self.assertEqual(b"".join(list(actual_data.chunks())), expected_data)
# ---
def from_seconds(cls, epoch_seconds: float) -> "Timestamp":
        """Create timestamp from seconds since epoch."""
        return cls(int(epoch_seconds * 1000))
# ---
def poll(self) -> bool:
        """Return True if job is finished."""
        if self._job_id is None:
            return True
        return JobStatus.finished(self.cluster.poll(self._job_id).status)
# ---
def run_test(program: str, call_expr: str, expected: str) -> bool:
    """Execute a program and test case, return whether it passes."""
    try:
        namespace: dict = {}
        exec(program, namespace)
        result = eval(call_expr, namespace)
        return str(result) == expected
    except Exception:
        return False
# ---
def author(self):
        '''Name of the author. (optional)

        If there are multiple authors, pass a list of strings.

        To control the file-as and role attribute, use author objects instead
        of strings; file-as is an alternate form of the name used for sorting.
        For a description of the role attribute, see the docstring of the
        author class.'''
        if len(self._authors) == 1:
            return self._authors[0]
        return tuple([aut for aut in self._authors])
# ---
def __setitem__(self, key, value):
        self.subqueries[key] = value
# ---
def load_rollout_batch(pickle_file: str) -> RolloutBatch:
    """Load rollout batch from pickle file."""
    with open(pickle_file, "rb") as f:
        batch = pickle.load(f)
    return batch
# ---
def max_seqs(self) -> int:
        return self.seq_lens.axis_size("seq")
# ---
def count_and_extract(hash_key, items):
        """Reducer that counts items and extracts content from first item."""
        items_list = list(items)
        return {
            "hash": hash_key,
            "count": len(items_list),
            "content": items_list[0]["content"],
        }
# ---
def parameterize_with_configs(pattern, config_path=None):
    test_path = os.path.dirname(os.path.abspath(__file__))
    if config_path is None:
        config_path = os.path.join(test_path, "..", "config")

    configs = glob.glob(os.path.join(config_path, pattern))
    return pytest.mark.parametrize("config_file", configs, ids=lambda x: f"{os.path.basename(x)}")
# ---
def shutdown(self) -> None:
        self._executor.shutdown(wait=True)
# ---
def fix_init_kwarg(self, sender, args, kwargs, **signal_kwargs):
		# Anything passed in as self.name is assumed to come from a serializer and
		# will be treated as a json string.
		if self.name in kwargs:
			value = kwargs.pop(self.name)

			# Hack to handle the xml serializer's handling of "null"
			if value is None:
				value = 'null'

			kwargs[self.attname] = value
# ---
def test_preemptible_true_produces_no_constraints(self):
        resources = ResourceConfig(preemptible=True)
        constraints = convert_constraints(resources)
        assert constraints == []
# ---
def deduplicate_shard(items):
        seen = set()
        for item in items:
            key = item["id"]
            if key not in seen:
                seen.add(key)
                yield item
# ---
def test_wait_all_no_raise(client: LocalClient):
    h_ok = client.submit(JobRequest(name="ok", entrypoint=Entrypoint.from_callable(_noop)))
    h_fail = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    statuses = wait_all([h_ok, h_fail], raise_on_failure=False)
    assert statuses[0] == JobStatus.SUCCEEDED
    assert statuses[1] == JobStatus.FAILED
# ---
def test_jax_device_kind_to_fray_device_type(jax_device_kind, expected_fray_type):
    assert jax_device_kind_to_fray_device_type(jax_device_kind) == expected_fray_type
# ---
def _create_chained_picking(self, cr, uid, picking_name, picking, picking_type, moves_todo, context=None):
        picking_obj = self.pool.get('stock.picking')
        return picking_obj.create(cr, uid, self._prepare_chained_picking(cr, uid, picking_name, picking, picking_type, moves_todo, context=context))
# ---
def last_scale_down_ms(self) -> int:
        """Timestamp of last scale-down operation."""
        return self._last_scale_down.epoch_ms()
# ---
def kill_all():
    proc = multiprocessing.active_children()
    for p in proc:
        p.terminate()
    return 'killed all'
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"transformer": "model", "embeddings": None}
# ---
def delete_ports(self, inc_ports):
        ''' remove a port from a service '''
        if not isinstance(inc_ports, list):
            inc_ports = [inc_ports]

        ports = self.get(Service.port_path) or []

        if not ports:
            return True

        removed = False
        for inc_port in inc_ports:
            port = self.find_ports(inc_port)
            if port:
                ports.remove(port)
                removed = True

        return removed
# ---
def Vocab(self):
        return self.model.Vocab
# ---
def netloc(self):
        '''return username:password@hostname:port'''
        s = ''
        prefix = ''
        if self.username:
            s += self.username
            prefix = '@'

        if self.password:
            s += ":{}".format(self.password)
            prefix = '@'

        s += "{}{}".format(prefix, self.hostloc)
        return s
# ---
def test_equal_1(self):
        self.assertEqual(string_color('Jack'), '79CAE5')
# ---
def __init__(self, meth_name, action, handle):
        self.meth_name = meth_name
        self.handle = handle
        self.action = action
# ---
def get(self, ref):
        """Retrieve an object from Ray's object store."""
        return ray.get(ref)
# ---
def slice_count(self) -> int:
        """Total number of VM groups (regardless of state)."""
        with self._vm_groups_lock:
            return len(self._vm_groups)
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "RotaryEmbeddingsConfig":
        pass
# ---
def __post_init__(self) -> None:
        _ = self.inferred_head_dim
        if self.num_heads % self.num_kv_heads != 0:
            raise ValueError("num_heads must be divisible by num_kv_heads for grouped-query attention")
        if self.vocab_size <= 0:
            raise ValueError("vocab_size must be positive")
        if self.max_seq_len <= 0:
            raise ValueError("max_seq_len must be positive")
# ---
def switch_func(txn):
            txn.description, txn.payee = txn.payee, txn.description
# ---
def check_access_rule(self, cr, uid, ids, operation, context=None):
        #override in order to redirect the check of acces rules on the stock.picking object
        return self.pool.get('stock.picking').check_access_rule(cr, uid, ids, operation, context=context)
# ---
def __truediv__(self, other: str) -> "InputName":
        """Alias for `cd`. That looks more Pythonic."""
        return InputName(self, name=other)
# ---
def shutdown(self, graceful: bool = True) -> bool:
        """Shutdown worker container."""
        cmd = "docker stop iris-worker" if graceful else "docker kill iris-worker"
        try:
            self._conn.run(cmd, timeout=Duration.from_seconds(30))
            return True
        except Exception:
            return False
# ---
def __init__(self, tree, path: str, mode: str):
        self.path = path
        self.mode = mode
        self.tree = tree
# ---
def print_screen(self):
        """
        Return MAX_NUM_LINES lines.
        """
        return self.lines[-MAX_NUM_LINES:]
# ---
def process_dataset(config: ConversationToDolmaConfig):
    """Convert conversation format to Dolma format."""
    pipeline = (
        Dataset.from_files(f"{config.input_path}/**/*.jsonl.gz", empty_glob_ok=False)
        .flat_map(load_jsonl)
        .map(transform_conversation_to_dolma)
        .write_jsonl(f"{config.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )
    Backend.execute(pipeline)
# ---
def device_is_id(self):
        pattern = r'[A-Za-z0-9]{8}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{12}'
        if re.match(pattern, self.device):
            return True
        return False
# ---
def status(self) -> JobStatus:
        if self._terminated.is_set():
            return JobStatus.STOPPED
        if not self._future.done():
            return JobStatus.RUNNING
        exc = self._future.exception()
        if exc is not None:
            return JobStatus.FAILED
        return JobStatus.SUCCEEDED
# ---
def get_shard_source(self, split) -> ShardedDataSource[dict] | None:
        raise NotImplementedError
# ---
def write_xml(self):
        'Write the XML code for the OPF file.'
        metadata = ''
        for elem in self.meta:
            metadata += elem.write_xml()
        manif = ''
        spine = ''
        guide = ''
        for finfo in self.filelist:
            manif += finfo.manifest_entry()
            spine += finfo.spine_entry()
            guide += finfo.guide_entry()
        return self._opf.format(metadata, manif, spine, guide)
# ---
def section(self, title: str):
        """Write a markdown section header."""
        self.log("")
        self._file.write(f"\n## {title}\n\n")
        self._file.flush()
        print("=" * 60, flush=True)
        print(f" {title}", flush=True)
        print("=" * 60, flush=True)
        self.log("")
# ---
def softmax(a: A, axis: AxisSelection | None = None) -> A:
    return wrap_axiswise_call(jnn.softmax, a, axis=axis, single_axis_only=False)
# ---
def find_list_of_ids(self, resource, ids, client_projection=None):
        args = self._es_args(resource)
        return self._parse_hits(self.es.multi_get(ids, **args), resource)
# ---
def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> list[int]:
        raise NotImplementedError
# ---
def needs_space_before(self, other: "LatexNode") -> bool:
        # Macro needs space before text, another macro, or braced content
        return isinstance(other, (MacroNode, TextNode, BracedNode))
# ---
def average(self):
        if self._n == 0:
            return 0.0
        return self._elapsed / self._n
# ---
def setUp(self):
        self.parser = Parser()
# ---
def get_transactions(self, request: cluster__pb2.Controller.GetTransactionsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetTransactionsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def crispri(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'CRISPR'
    }
# ---
def the_object_name1_has_no_boolean_difference_by_name2(name1, name2):
    obj = the_object_name_exists(name1)
    for modifier in obj.modifiers:
        if modifier.type == "BOOLEAN" and modifier.object and modifier.object.name == name2:
            assert False, "A boolean was found"
# ---
def reconnect_p2p(self):
        """Tear down and bootstrap the P2P connection to the node.

        The node gets disconnected several times in this test. This helper
        method reconnects the p2p and restarts the network thread."""
        self.nodes[0].disconnect_p2ps()
        self.bootstrap_p2p()
# ---
def end_of_directory(cache_to_disc=True):
    xbmcplugin.endOfDirectory(int(sys.argv[1]), cacheToDisc=cache_to_disc)
# ---
def create_step():
        a = ExecutorStep(name="a", fn=fn, config=None)
        b = ExecutorStep(
            name="b",
            fn=fn,
            config=MyConfig(
                input_path=output_path_of(a, "sub"),
                output_path=this_output_path(),
                n=versioned(3),
                m=4,
            ),
        )
        return b
# ---
def shutdown(self) -> None:
        """Terminate the actor job."""
        client = self._get_client()
        client.terminate(self._job_id)
# ---
def validate_description(self):
		'''Clean HTML description if set'''
		if cint(frappe.db.get_single_value('Stock Settings', 'clean_description_html')):
			self.description = clean_html(self.description)
# ---
def call_process(self, message, stream_route, stream_id):
        """
        Handles pre-processing of packet and process work
        """
        self.process(message)
# ---
def _register_service(
    name: str,
    full_name: str,
    client_class: type,
) -> None:
    """Register a service in the global registry."""
    methods = _discover_methods_from_client(client_class)
    SERVICES[name] = ServiceInfo(
        name=name,
        full_name=full_name,
        client_class=client_class,
        methods=methods,
    )
# ---
def named(self, name):
        if not hasattr(self, name):
            colors = ', '.join(self.__colors())
            raise Exception('Unknown color %s. Available colors are: %s' % (name, colors))
        return getattr(self, name)
# ---
def _call_splash_attention(q_, k_, v_, sinks_):
            return jax.vmap(lambda q_b, k_b, v_b: kernel(q_b, k_b, v_b, sinks=sinks_), in_axes=(0, 0, 0))(q_, k_, v_)
# ---
def test_impl(df):
            return pd.DataFrame({'B': df.A.str.split(',')})
# ---
def _train_step(model, x, dy):
        def loss_fn(lin):
            y = lin(x)
            loss = y * dy.astype(y.dtype)
            return hax.sum(loss).scalar()

        grad_fn = eqx.filter_grad(loss_fn)
        grads = grad_fn(model)
        return apply_gradients(model, grads)
# ---
def next_interval(self) -> float:
        interval = self._initial * (self._factor**self._attempt)
        interval = min(interval, self._maximum)

        if self._jitter > 0:
            jitter = interval * self._jitter * (2 * random.random() - 1)
            interval = max(0.001, interval + jitter)

        self._attempt += 1
        return interval
# ---
def causal_loss_mask(Pos: Axis, prompt_length: Optional[int] = None) -> NamedArray:
        loss_weight = hax.logical_not(hax.nn.one_hot(-1, Pos, dtype=jnp.bool_))

        if prompt_length is not None:
            # don't predict the prompt tokens
            prompt_mask = hax.arange(Pos) >= prompt_length - 1
            loss_weight = hax.logical_and(loss_weight, prompt_mask)

        return loss_weight
# ---
def __contains__(self, key):
        try:
            self._fallback(key)
            return True
        except KeyError:
            return False
# ---
def tobytes(obj):
        if isinstance(obj, str):
            obj = obj.encode('UTF-8')
        assert isinstance(obj, bytes)
        return obj
# ---
def _receive_payload(self):
        payload = broadcast_shard(
            self._dummy_batch,
            hax.partitioning.infer_resource_partitions(self._dummy_batch),
        )
        return payload
# ---
def create_qwen_tokenizer():
    return AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
# ---
def BeginTransaction(self, request, context):
        """Begins a new transaction. This step can often be skipped:
    [Read][google.spanner.v1.Spanner.Read], [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] and
    [Commit][google.spanner.v1.Spanner.Commit] can begin a new transaction as a
    side-effect.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def cleanup(self) -> None:
        """Cleanup transfer server and thread pool."""
        logger.info("Cleaning up JAX transfer client")
        self.executor.shutdown(wait=True)
        self.transfer_server = None
# ---
def normalize_amplitude(self,z_data,cal_ampdata):
		return z_data/cal_ampdata
# ---
def join(self, *args, **kwargs):
        super().join(*args, **kwargs)
        if self._exception:
            raise self._exception
# ---
def cleanup(self) -> None:
        """Cleanup Flight client resources."""
        try:
            logger.info("Shutting down Arrow Flight client thread pool...")
            self._receive_pool.shutdown(wait=False, cancel_futures=False)
            logger.info("Thread pool shutdown completed")
        except Exception as e:
            logger.warning(f"Error shutting down thread pool: {e}")
# ---
def null(self, env):
        """ return the null value for this field in the given environment """
        return False
# ---
def test_dict(self):
        """Store and retrieve a dict"""
        self.make_table()
        data = {
            "i": 1,
            "s": "abc",
            "n": None,
            "l": ["a", 1, True],
            "b": False,
        }
        self.dynamo.put_item("foobar", {"id": "abc", "d": data})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["d"], data)
# ---
def prepared_registry(self):
        ''' prepared_registry property '''
        if not self.__prepared_registry:
            results = self.prepare_registry()
            if not results or ('returncode' in results and results['returncode'] != 0):
                raise RegistryException('Could not perform registry preparation. {}'.format(results))
            self.__prepared_registry = results

        return self.__prepared_registry
# ---
def run(self):
        cmd = '''
              docker run --rm -v rita_store:/rita/data  rita/test-r
        '''

        logger.debug(cmd)

        out = subprocess.check_output(cmd, shell=True)

        logger.debug(out)
# ---
def decodeNextSeg(self):
        # if we're not running send an instant kill switch.
        if ( not self.running ): return -1

        # try to grab a segment from the cache to decode.
        seg = None
        try:
            seg = self.cache.pop()
        except:
            pass

        if ( seg == None ) and ( self.all_decoded ):
            return -1
        return seg
# ---
def at(cls, timestamp: Timestamp, source: str, data: str) -> "LogLine":
        """Create a log line with an explicit timestamp."""
        return cls(
            timestamp=datetime.fromtimestamp(timestamp.epoch_seconds(), tz=timezone.utc),
            source=source,
            data=data,
        )
# ---
def test_empty_set_against_integer(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.in_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def test_repr_handles_lambdas():
    """Ensure anonymous lambdas work correctly."""
    op = FilterOp(lambda x: x > 0)
    assert repr(op) == "FilterOp(predicate=test_repr_handles_lambdas.<locals>.<lambda>)"
# ---
def bmarks():
    return_data = add_tags()
    return return_data
# ---
def __post_init__(self):
        assert isinstance(self.num_seqs, jnp.ndarray), "num_seqs must be a JAX ndarray"
# ---
def list_images_with_detail(self, params=None):
        """Returns a detailed list of images filtered by any parameters."""
        url = 'images/detail'
        if params:
            url += '?%s' % urllib.urlencode(params)

        resp, body = self.get(url)
        body = json.loads(body)
        self.validate_response(schema.list_images_details, resp, body)
        return service_client.ResponseBodyList(resp, body['images'])
# ---
def _stack_batch_encodings(a: BatchEncoding, b: BatchEncoding) -> BatchEncoding:
    """Stacks two batch encodings together, assuming that the keys are the same."""

    def _ensure_batched(x):
        if len(x) == 0:
            return list(x)
        elif isinstance(x[0], Sequence) or isinstance(x[0], np.ndarray):
            return list(x)
        else:
            return [x]

    return BatchEncoding({k: _ensure_batched(a[k]) + _ensure_batched(b[k]) for k in a.keys()})
# ---
def batch_schedule(self):
        return BatchSchedule(self.train_batch_size)
# ---
def test_with_trace(self, mock_start, mock_stop):

        with profiler.Trace("a", info="a1"):
            mock_start.assert_called_once_with("a", info="a1")
            mock_start.reset_mock()
            with profiler.Trace("b", info="b1"):
                mock_start.assert_called_once_with("b", info="b1")
            mock_stop.assert_called_once_with()
            mock_stop.reset_mock()
        mock_stop.assert_called_once_with()
# ---
def hook(*args):
            """Log the call into a list"""
            calls.append(args)
# ---
def filter_lidar_data_by_neighbourhood(in_pts,target_xy,radius):
    pts = np.zeros((0,in_pts.shape[1]))
    if in_pts.shape[0]>0:
        x,y,inside =  points_in_radius(in_pts[:,0],in_pts[:,1],target_xy[0],target_xy[1],radius)
        pts = in_pts[inside,:]
    else:
        print( "\t\t\t no points in neighbourhood")
    return pts
# ---
def __ge__(self, other: "Timestamp") -> bool:
        return self._epoch_ms >= other._epoch_ms
# ---
def _check_entered(self) -> None:
        if not self._entered:
            raise RuntimeError("JobGroup must be entered before calling run methods")
# ---
def test_cannot_scale_up_at_max_slices(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """can_scale_up() returns False when at max_slices."""
        discovered = [make_mock_vm_group(f"slice-{i}") for i in range(5)]
        manager = make_mock_vm_manager(vm_groups_to_discover=discovered)
        group = ScalingGroup(scale_group_config, manager)
        group.reconcile()

        assert group.slice_count() == 5  # max_slices
        assert not group.can_scale_up()
# ---
def _substitute_implementations():
        """Replaces implementation ids with input_types."""
        impls = {}
        for id_ in input_type['implementations']:
            type_ = input_types[id_]
            impls[type_['name']] = type_
        input_type['implementations'] = impls
# ---
def test_do_execute_w_replace(self):
        self._test_do_execute(True)
# ---
def _fix_a_slash_b(string):
    if len(string.split("/")) != 2:
        return string
    a = string.split("/")[0]
    b = string.split("/")[1]
    try:
        a = int(a)
        b = int(b)
        assert string == f"{a}/{b}"
        new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
        return new_string
    except BaseException:
        return string
# ---
def clear(self):
        self._items.clear()
        self.model.clear()
# ---
def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('BB', regex=False)
            return B.sum()
# ---
def test_is_stop_signal_no_match():
    # tail_tokens does not match stop_sequence
    tail_tokens = hax.named(jnp.array([5, 6, 7], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[1, 2, 3]], dtype=jnp.int32), axis=("seq", "position"))
    assert not is_stop_signal(tail_tokens, stop_sequences)
# ---
def default_attention_type() -> AttentionBackend:
    accelerator_type = jax.local_devices()[0].platform
    if accelerator_type == "gpu":
        return AttentionBackend.NVTE
    elif accelerator_type == "tpu":
        return AttentionBackend.SPLASH
    else:
        return AttentionBackend.JAX_FLASH
# ---
def canonicalize(value):
        if isinstance(value, dict):
            return {k: canonicalize(v) for k, v in sorted(value.items())}
        if isinstance(value, list):
            return [canonicalize(x) for x in value]
        if callable(value):
            return f"{value.__module__}.{value.__qualname__}"
        return value
# ---
def _setRateForVoiceType(self, voiceType, value):
        """Sets the speaking rate value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM
        - value: the rate value to set.
        """

        voiceACSS = self._getACSSForVoiceType(voiceType)
        voiceACSS[acss.ACSS.RATE] = value
        voiceACSS['established'] = True
# ---
def cancelButtonClicked(self, widget):
        """Signal handler for the "clicked" signal for the cancelButton
           GtkButton widget. The user has clicked the Cancel button.
           Don't write out the preferences. Destroy the configuration window.

        Arguments:
        - widget: the component that generated the signal.
        """

        self.windowClosed(widget)
        self.get_widget("orcaSetupWindow").destroy()
# ---
def __truediv__(self, other: str) -> "InputName":
        """Alias for `cd` that looks more Pythonic."""
        return self.cd(other)
# ---
def delete(self):
        self.fsm.requestFinalState()
        DistributedCCharBaseAI.DistributedCCharBaseAI.delete(self)
        self.lonelyDoneEvent = None
        self.lonely = None
        self.chattyDoneEvent = None
        self.chatty = None
        self.walkDoneEvent = None
        self.walk = None
        return
# ---
def test_synthetic_subtrees_deterministic():
    r1 = random.Random(99)
    r2 = random.Random(99)
    e1 = generate_synthetic_subtrees(r1, count_per_category=10)
    e2 = generate_synthetic_subtrees(r2, count_per_category=10)
    assert [e.source for e in e1] == [e.source for e in e2]
# ---
def _setup_regular(self, env):
        super(_Relational, self)._setup_regular(env)
        if self.comodel_name not in env.registry:
            _logger.warning("Field %s with unknown comodel_name %r"
                            % (self, self.comodel_name))
            self.comodel_name = '_unknown'
# ---
def exact(self, f):
        """Returns True if f is in .files()."""
        return f in self._fileset
# ---
def _rm(path):
    path = Path(path)
    if path.is_dir():
        shutil.rmtree(path, ignore_errors=True)
    elif path.is_file():
        os.remove(path)
    elif path.exists():
        raise RuntimeError(f"Remove failed. Path ({path}) is neither a directory nor a file.")
# ---
def model_type(self) -> Type["QwenLMHeadModel"]:
        return QwenLMHeadModel
# ---
def test_to_string(self):
        """Can convert from type id to type string."""
        self.assertEquals(
            vm_utils.ImageType.to_string(vm_utils.ImageType.KERNEL),
            vm_utils.ImageType.KERNEL_STR)
# ---
def norm_config(self) -> RmsNormConfig:
        """Return the normalization configuration for OLMo2."""
        return RmsNormConfig(
            eps=self.layer_norm_epsilon,
            use_weight=self.use_layer_norm_weight,
            use_bias=self.use_bias,
        )
# ---
def test_dialect_engine_options(self):
        engine = testing_engine("sqlite://")
        engine.dialect = Mock()
        e2 = engine.execution_options(foo="bar")
        eq_(
            engine.dialect.set_engine_execution_options.mock_calls,
            [call(e2, {"foo": "bar"})]
        )
# ---
def __init__(self, lang):
        '''lang must be a lower-case two-letter language code,
        optionally followed by a "-" and a upper-case two-letter country
        code. (e.g., "en", "en-US", "en-UK", "de", "de-DE", "de-AT")'''
        if self._lang_re.match(lang):
            self.tag = 'dc:language'
            self.text = lang
            self.attr = ()
        else:
            raise ValueError('invalid language format')
# ---
def test_conf_inline_ratelimiting(self):
        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):
            app = memcache.MemcacheMiddleware(
                FakeApp(),
                {'error_suppression_limit': '5',
                 'error_suppression_interval': '2.5'})
        self.assertEqual(app.memcache._error_limit_count, 5)
        self.assertEqual(app.memcache._error_limit_time, 2.5)
        self.assertEqual(app.memcache._error_limit_duration, 2.5)
# ---
def _select_0th(axis):
    def select_0th(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.take(axis, 0)
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            # other leaves don't matter
            return leaf

    return select_0th
# ---
def __init__(
        self,
        state: ControllerState,
        scheduler: SchedulerProtocol,
        bundle_prefix: str,
        log_buffer: LogBuffer | None = None,
    ):
        self._state = state
        self._scheduler = scheduler
        self._bundle_store = BundleStore(bundle_prefix)
        self._log_buffer = log_buffer
# ---
def __call__(self, x: NamedArray, group_sizes: NamedArray, *, key=None) -> NamedArray:
        k1, k2, k3 = maybe_rng_split(key, 3)
        hidden_states = self.w1(x, group_sizes, key=k1)
        hidden_states = self.act(hidden_states)
        hidden_states = hidden_states * self.w3(x, group_sizes, key=k3)
        outputs = self.w2(hidden_states, group_sizes, key=k2)
        return outputs
# ---
def test_gemma_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def get_logs(self, container_id: str, since: Timestamp | None = None) -> list[LogLine]:
        c = self._containers.get(container_id)
        if not c:
            return []
        if since:
            since_dt = datetime.fromtimestamp(since.epoch_seconds(), tz=timezone.utc)
            return [log for log in c._logs if log.timestamp > since_dt]
        return c._logs
# ---
def test_forward_output_shape(params, tiny_cfg):
    batch_size, seq_len = 2, 16
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (batch_size, seq_len), 1, 100)

    logits = forward(params, token_ids, tiny_cfg)
    assert logits.shape == (batch_size, seq_len, tiny_cfg.vocab_size)
# ---
def get_git_commit() -> str | None:
    """Return the git commit of the current branch (if it can be found)"""
    if os.path.exists(".git"):
        return os.popen("git rev-parse HEAD").read().strip()
    else:
        return None
# ---
def freeze(ref: NamedRef) -> NamedArray:
    """Freeze the reference and return its current contents."""
    return ref.value()
# ---
def sign(a: A) -> A:
    return wrap_elemwise_unary(jnp.sign, a)
# ---
def remove_biblinks(html: BeautifulSoup):
    # Remove the biblinks since we are removing the biblio
    biblinks = html.findAll("a", {"class": "ltx_ref"})
    for biblink in biblinks:
        # Removes reference links
        # biblink.decompose()
        # Removes linking but keeps text
        biblink.unwrap()
# ---
def get_directory_size(path):
    return sum([get_file_size(os.path.join(path, fastq)) for fastq in os.listdir(path)])
# ---
def http_error_401(self, *args, **kwds):
                self.parent.record("digest")
                urllib.request.HTTPDigestAuthHandler.http_error_401(self,
                                                             *args, **kwds)
# ---
def tokenizer_path() -> Path:
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        return Path(tokenizer.name_or_path)
    except Exception as e:  # noqa
        pytest.skip(f"Could not load tokenizer {MODEL_NAME}: {e}", allow_module_level=True)
        raise NotImplementedError("unreachable")
# ---
def allocate_for_seq(
        self,
        token_slot_ids: ht.i32[NamedArray, " position"],  # type: ignore[name-defined]
        token_pos_ids: ht.i32[NamedArray, " position"],  # type: ignore[name-defined]
    ) -> tuple["DecodeState", PageBatchInfo]:
        sequences, page_table, batch_info = self.sequences.allocate_for_seq(
            self.page_table, token_slot_ids, token_pos_ids
        )
        return dataclasses.replace(self, sequences=sequences, page_table=page_table), batch_info
# ---
def test_delete_permission(self):
        """
        Tests that staff cannot delete entries
        """
        self.assertFalse(self.creator_admin.has_delete_permission(self.request))
# ---
def send_css(filename):
    return static_file(filename, root='css')
# ---
def test_causal_mask_slicing():
    pos = hax.Axis("pos", 128)
    key_pos = pos.alias("key_pos")

    mask = AttentionMask.causal()

    mat_mask = mask.materialize(pos, key_pos)
    mat_sliced = mask.materialize(pos, key_pos, q_slice=hax.dslice(7, 16), k_slice=hax.dslice(24, 16))

    for i in range(16):
        for j in range(16):
            assert mat_sliced.array[i, j] == mat_mask.array[7 + i, 24 + j]
# ---
def signal_shutdown(self) -> None:
        logger.debug("Signalling shutdown")
        self._stop_event.set()
# ---
def __set__(self, record, value):
        raise TypeError("field 'id' cannot be assigned")
# ---
def reset(self):
        return self._reset()
# ---
def dispatch_events(self):
        """Poll the operating system event queue for new events and call
        attached event handlers.

        This method is provided for legacy applications targeting pyglet 1.0,
        and advanced applications that must integrate their event loop
        into another framework.

        Typical applications should use `pyglet.app.run`.
        """
        raise NotImplementedError('abstract')
# ---
def main():
    spec = ArgumentSpec()

    module = AnsibleModule(
        argument_spec=spec.argument_spec,
        supports_check_mode=spec.supports_check_mode,
        required_if=spec.required_if
    )

    try:
        mm = ModuleManager(module=module)
        results = mm.exec_module()
        module.exit_json(**results)
    except F5ModuleError as ex:
        module.fail_json(msg=str(ex))
# ---
def broadcast_arrays(
    *arrays: NamedArray, require_subset: bool = True, ensure_order: bool = True
) -> tuple[NamedArray, ...]: ...
# ---
def on_context_lost():
            """The window's GL context was lost.

            When the context is lost no more GL methods can be called until it
            is recreated.  This is a rare event, triggered perhaps by the user
            switching to an incompatible video mode.  When it occurs, an
            application will need to reload all objects (display lists, texture
            objects, shaders) as well as restore the GL state.

            :event:
            """
# ---
def around(a: A) -> A:
    return wrap_elemwise_unary(jnp.around, a)
# ---
def cli():
    """Iris Controller - Cluster control plane."""
    pass
# ---
def fake_vdi_resize(*args, **kwargs):
            raise Exception("This shouldn't be called")
# ---
def do_alloc(carry):
                return _alloc_pages_for_seq(i, carry)
# ---
def create_integration_entrypoint():
    """Create a simple test entrypoint for integration tests."""

    def test_fn():
        print("Hello from test task!")
        return 42

    return Entrypoint.from_callable(test_fn)
# ---
def linCongr(a, b, m):
        solutions = set()
        if (b % gcd(a, m) == 0):
                numSols = gcd(a, m)
                sol = (b * egcd(a, m)[0] / numSols) % m
                for i in xrange(0, numSols):
                        solutions.add((sol + m * i / numSols) % m)
        return solutions
# ---
def __init__(self, ser: serial.Serial, parent=None):
        super().__init__(parent)
        self.serial = ser
        self._quit = False
# ---
def show(self):
        super().show()
        self.inputLine.setFocus()
# ---
def softplus(a: A) -> A:
    return wrap_elemwise_unary(jnn.softplus, a)
# ---
def process_bind_param(self, value, dialect):
                raise Exception("nope")
# ---
def remove_router_interface_precommit(self, context, r_port_context):
        pass
# ---
def extract_ngrams(text: str, n: int, stride: int) -> Iterator[str]:
    """
    Extract n-grams from text based on config.
    """
    tokens: list[str] = text.split()

    for i in range(0, len(tokens) - n + 1, stride + 1):
        yield " ".join(tokens[i : i + n])
# ---
def __init__(self, query, explanation):
        if isinstance(query, list):
            query = " ".join(query)
        message = f"'{query}': {explanation}"
        super().__init__(message)
# ---
def __delattr__(self, name):
        """ Remove non-slot field attribute. """
        try:
            del self._attrs[name]
        except KeyError:
            raise AttributeError(name)
# ---
def _compute_tag_arrays(self):
        tag_arrays = []
        for dataset, tags in self.datasets:
            indexed = [self.tag_to_index[tag] for tag in tags]
            tags = np.zeros(self.Tag.size, dtype=np.int32)
            tags[indexed] = 1
            tags = hax.named(tags, self.Tag)

            tag_arrays.append(tags)
        return tag_arrays
# ---
def normalize_zdata(self,z_data,cal_z_data):
		return z_data/cal_z_data
# ---
def __post_init__(self):
        if self.hyena.max_seq_len != self.max_seq_len:
            object.__setattr__(self, "hyena", dataclasses.replace(self.hyena, max_seq_len=self.max_seq_len))
# ---
def testForElse(self):
    self.assertEqual((0, 'foo\nbar\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1,):
          print 'foo'
        else:
          print 'bar'""")))
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: Optional[str] = None
    ) -> HFCheckpointConverter["MistralConfig"]:  # type: ignore
        hf_model_path = "mistralai/Mistral-7B-v0.1" if ref_checkpoint is None else ref_checkpoint

        return HFCheckpointConverter(
            self,
            reference_checkpoint=hf_model_path,
            trust_remote_code=True,
            tokenizer=hf_model_path,
            HfConfigClass=HfMistralConfig,
        )
# ---
def getLoadedPlugins(): pass
# ---
def convert_tokens_to_ids(self, token):
        """Convert token string to token ID."""
        if isinstance(token, list):
            return [self.TOKENS.index(t) for t in token]
        return self.TOKENS.index(token)
# ---
def wait_unit (self, uids, state=[DONE, FAILED, CANCELED], timeout=-1.0) :
        """
        Wait for given unit(s) to enter given state
        """

        raise Exception ("%s.wait_unit() is not implemented" % self.__class__.__name__)
# ---
def TrainBatch(self):
        return self.config.TrainBatch
# ---
def additionally_the_object_name_is_selected(name):
    obj = bpy.context.scene.objects.get(name)
    if not obj:
        assert False, 'The object "{name}" could not be selected'
    bpy.context.view_layer.objects.active = obj
    obj.select_set(True)
# ---
def __rmatmul__(self, other):  # pragma: no cover
        raise ValueError("Matrix multiplication is too ambiguous with NamedArrays. Use dot instead.")
# ---
def success(self) -> bool:
        return self.worker is not None
# ---
def name(self):
    """Function name."""
    self._create_definition_if_needed()
    return self._func_name
# ---
def test_shift1(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.shift(1)
            return Ac.sum()

        hpat_func = self.jit(test_impl)
        n = 11
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def python_grad_func(self):
    """Python gradient function callable."""
    return self._python_grad_func
# ---
def normal_serialize(self):
        return super().serialize()
# ---
def the_object_name_is_not_voided_by_void(name, void):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    for rel in element.HasOpenings:
        if rel.RelatedOpeningElement.Name == void:
            assert False, "A void was found"
# ---
def _fold_in_key_vmap(key, indices):
    return jax.vmap(lambda i: jax.random.fold_in(key, i))(indices)
# ---
def getapiname(self):
		return 'rds.aliyuncs.com.CheckAccountNameAvailable.2014-08-15'
# ---
def message_post(self, *args, **kwargs):
        """Post the message on stock.picking to be able to see it in the form view when using the chatter"""
        return self.pool.get('stock.picking').message_post(*args, **kwargs)
# ---
def generate_job_name(command: list[str]) -> str:
    """Generate a job name from the command."""
    script_name = "job"
    for arg in command:
        path = Path(arg)
        if path.suffix == ".py":
            script_name = path.stem
            break

    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    username = getpass.getuser()
    return f"iris-run-{username}-{script_name}-{timestamp}"
# ---
def setSyntax(self, syntax):
        """Choose smart indentation algorithm according to syntax"""
        self._smartIndenter = self._chooseSmartIndenter(syntax)
# ---
def _substitute_default_descendant():
        """Replaces default descendant id with input_type."""
        id_ = input_type.get('default_descendant', None)
        if id_ is not None:
            input_type['default_descendant'] = input_types[id_]
# ---
def build(
        self,
        bundle_path: Path,
        dockerfile: str,
        job_id: str,
        task_logs: TaskLogs | None = None,
    ) -> BuildResult:
        del bundle_path, dockerfile, job_id, task_logs
        return BuildResult(
            image_tag="local:latest",
            build_time_ms=0,
            from_cache=True,
        )
# ---
def _around_frequency(self, frequency):
        # return ceil(frequency/self._calibration_step)*self._calibration_step
        return round(frequency / self._calibration_step) * self._calibration_step
# ---
def open_pager(self):
        "Open the selected item with the system's pager"
        data = self.get_selected_item()
        if data['type'] == 'Submission':
            text = '\n\n'.join((data['permalink'], data['text']))
            self.term.open_pager(text)
        elif data['type'] == 'Comment':
            text = '\n\n'.join((data['permalink'], data['body']))
            self.term.open_pager(text)
        else:
            self.term.flash()
# ---
def increment(self) -> int:
        """Increment and return new value."""
        self._value += 1
        return self._value
# ---
def run(self) -> None:
        uvicorn.run(self._app, host=self._host, port=self._port)
# ---
def tree_flatten(self) -> Any:
        return ((self.array,), self.main_axes)
# ---
def contribute_to_class(self, cls, name):
		super(JSONField, self).contribute_to_class(cls, name)
		setattr(cls, name, JSONDescriptor(self))
		models.signals.pre_init.connect(self.fix_init_kwarg, sender=cls)
# ---
def convert_to_cache(self, value, record, validate=True):
        return value or {}
# ---
def serve_weights(self, weight_id: int, model) -> None:
        """Serve weights to clients.

        Args:
            weight_id: Unique identifier for this weight update
            model: Levanter model parameters (PyTree of NamedArrays)
        """
        pass
# ---
def rad2deg(a: A) -> A:
    return wrap_elemwise_unary(jnp.rad2deg, a)
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        examples = []
        for _ in range(n_examples):
            prompt = "i like cats, i love cats, give me moar cats."
            num_cats = int(rng.integers(1, 5))
            answer = "cats" * num_cats + " love cats" * int(num_cats > 1)
            examples.append({"prompt": prompt, "answer": answer})
        return examples
# ---
def test_read_dataset_streaming(self, sample_data, tmpdir, ext, create_fn):
        """Test streaming reading for both JSONL.GZ and Parquet files"""
        input_file = os.path.join(tmpdir, f"test_input.{ext}")
        create_fn(sample_data, input_file)

        rows = list(read_dataset_streaming(input_file))

        assert len(rows) == len(sample_data)
        assert rows[0]["id"] == "doc1"
        assert rows[0]["text"] == sample_data[0]["text"]
# ---
def test_just_out_capture(self):
        with self.getcapture(out=True, err=False) as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            out, err = cap.readouterr()
        assert out == "hello"
        assert not err
# ---
def __post_init__(self):
        if self.hidden_dim % self.num_heads:
            raise ValueError(f"hidden_dim {self.hidden_dim} must be divisible by num_heads {self.num_heads}")
        if self.max_seq_len % self.num_blocks:
            raise ValueError(f"seq_len {self.max_seq_len} must be divisible by num_blocks {self.num_blocks}")
# ---
def test_after_end_date(self):
        """Test when create_time is after the end date"""
        self.assertFalse(check_create_time("2023-02-01 00:00:01 PST", "2023-01-01", "2023-01-31"))
# ---
def char_to_bytes(char):
    return bin(ord(char))
# ---
def _call_remote(*args, **kw):
                return ray.get(step.fn.remote(*args, **kw))
# ---
def ray_auth_secret(secret_override: str | None = None) -> str:
    """Return the Secret Manager secret name to use for Ray auth token retrieval."""
    return secret_override or DEFAULT_RAY_AUTH_TOKEN_SECRET
# ---
def set_size(self, width, height):
        """Resize the window.

        The behaviour is undefined if the window is not resizable, or if
        it is currently fullscreen.

        The window size does not include the border or title bar.

        :Parameters:
            `width` : int
                New width of the window, in pixels.
            `height` : int
                New height of the window, in pixels.

        """
        raise NotImplementedError('abstract')
# ---
def with_output_path(self, output_path: str) -> "ExecutorStep":
        """Return a copy of the step with the given output_path."""
        return replace(self, override_output_path=output_path)
# ---
def connect_client():
    """Connects to Mongo client"""
    try:
        return MongoClient(app.config['DB_HOST'], int(app.config['DB_PORT']))
    except errors.ConnectionFailure as e:
        raise e
# ---
def test_quantile_parallel(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def compute_logits(model: LmHeadModel, example: LmExample):
            model = mp.cast_to_compute(model)
            with hax.axis_mapping(compute_axis_mapping):
                activations = model.activations(example.tokens, key=None, attn_mask=example.attn_mask)
                head = model.get_lm_head()
                logits = hax.dot(activations, head, axis=model.Embed)
                return logits
# ---
def test_final_ellipsis():
    partial_order = ("apple", "banana", ...)
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("apple", "banana", "cherry")
# ---
def __call__(self, params: PyTree) -> jax.Array: ...
# ---
def match(self, c):
        return self.c == '.' or self.c == c
# ---
def eliminate_axes(axis_spec: PartialShapeDict, axes: AxisSelection) -> PartialShapeDict:  # type: ignore
    ...
# ---
def test_multiple_processes_cleanup():
    venv = TemporaryVenv()
    processes = []
    with venv:
        for _ in range(3):
            proc = venv.run_async([venv.python_path, "-c", "import time; time.sleep(100)"])
            processes.append(proc)

    # wait for processes to terminate.
    time.sleep(0.5)

    for proc in processes:
        print(proc, proc.poll())
        assert proc.poll() is not None
# ---
def format_dict_list(objs, field):
    objs[field] = "\n".join(
        "- " + ", ".join("%s: %s" % (k, v)
                         for k, v in elem.items())
        for elem in objs[field])
# ---
def passthrough(module, grad_input, grad_output):
    """No change made to gradients"""
    return None
# ---
def modInverse(a, m):
    x, y = egcd(a, m)
    if gcd(a, m) == 1:
        return x % m
# ---
def test_vmap_static_args():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Batch, Width, Depth))

    def vmap_fun(x, y):
        return x.sum(Width) if y else x

    selected = hax.vmap(vmap_fun, Batch)(named1, True)

    expected = hax.sum(named1, Width)

    assert jnp.all(jnp.equal(selected.array, expected.array))
    assert selected.axes == expected.axes
# ---
def _rotate_half(x: NamedArray, HeadSize: Axis) -> NamedArray:
    """Rotates half of the hidden dims of the input and concatenates them."""
    x1 = x[HeadSize, : HeadSize.size // 2]
    x2 = x[HeadSize, HeadSize.size // 2 :]
    out = hax.concatenate(HeadSize, (-x2, x1))
    return out
# ---
def __repr__(self) -> str:
        return f"~({self.child})"
# ---
def binsend(self, bindata):
		self.dispatcher(self.neighbor, bindata, self.frequency, self.bandwidth)
		time.sleep(self.send_delay)
# ---
def test_task_queue_fifo_order(job_request):
    """Tasks are returned in FIFO order."""
    state = ControllerState()

    req1 = job_request("job1")
    req2 = job_request("job2")
    submit_job(state, "j1", req1)
    submit_job(state, "j2", req2)

    pending = state.peek_pending_tasks()
    assert len(pending) == 2
    assert pending[0].job_id == JobName.root("j1")
    assert pending[1].job_id == JobName.root("j2")
# ---
def __repr__(self) -> str:
        return f"lit({self.value!r})"
# ---
def test_do_executemany_w_replace(self):
        self._test_do_executemany(True)
# ---
def _SortChunksByFile( chunks ):
  """Sort the members of the list |chunks| (which must be a list of dictionaries
  conforming to ycmd.responses.FixItChunk) by their filepath. Returns a new
  list in arbitrary order."""

  chunks_by_file = defaultdict( list )

  for chunk in chunks:
    filepath = chunk[ 'range' ][ 'start' ][ 'filepath' ]
    chunks_by_file[ filepath ].append( chunk )

  return chunks_by_file
# ---
def is_coscheduled(self) -> bool:
        """Whether this job uses coscheduling (all tasks assigned atomically)."""
        return self.request.HasField("coscheduling")
# ---
def __init__(self, array):
        self.array = array
# ---
def zeros(shape: AxisSpec, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to 0"""
    if dtype is None:
        dtype = jnp.float32
    return full(shape, 0, dtype)
# ---
def tearDown(self):
        super(SimpleInstanceTest, self).tearDown()
        CONF.network_label_regex = self.orig_conf
        CONF.ip_start = None
# ---
def test_worker_failed_maps_to_failed(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_WORKER_FAILED) == JobStatus.FAILED
# ---
def pop(self, lease_timeout: float = 60.0) -> Lease[T_co] | None:
        self._recover_expired_leases()
        if self.queue:
            item = self.queue.pop(0)
            lease_id = str(uuid.uuid4())
            timestamp = time.time()
            lease = Lease(item, lease_id, timestamp)
            self.leases[lease_id] = (item, timestamp, lease_timeout)
            return lease
        return None
# ---
def visit_mtext(self, element):
        text = element.get_text()
        return BracedNode(f"\\text{{{text}}}")
# ---
def _F(self, right: np.ndarray, key: np.uint64) -> np.ndarray:
        """A simple round function that mixes the right half.

        Operates modulo 2^(right_bits).
        """
        masked_right = right & self.R_mask
        return (masked_right * np.uint64(2654435761) + key) & self.R_mask
# ---
def _recover_expired_leases(self) -> None:
        """Move expired leases back to the front of the queue."""
        current_time = time.time()
        expired = []
        for lease_id, (_item, timestamp, timeout) in self.leases.items():
            if current_time - timestamp >= timeout:
                expired.append(lease_id)

        for lease_id in expired:
            item, _, _ = self.leases[lease_id]
            self.queue.insert(0, item)
            del self.leases[lease_id]
# ---
def test_beam_search_includes_initial_program(params, model_cfg, tokenizer):
    """The initial program should be among the candidates (it starts with score 0)."""
    source = "x = 1\n"
    results = beam_search(
        params=params,
        initial_programs=[source],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(4),
        beam_size=8,
        expansions_per_beam=2,
        max_depth=1,
    )
    sources = {c.source for c in results}
    assert source in sources
# ---
def test_attach_volume_not_available(self):
        server = dict(id='server001')
        volume = dict(id='volume001', status='error', attachments=[])

        with testtools.ExpectedException(
            openstack.cloud.OpenStackCloudException,
            "Volume %s is not available. Status is '%s'" % (
                volume['id'], volume['status'])
        ):
            self.cloud.attach_volume(server, volume)
        self.assertEqual(0, len(self.adapter.request_history))
# ---
def test_spatial_dropout_1d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout1D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4))
# ---
def _isnamedtupleinstance(x):
    t = type(x)
    b = t.__bases__
    if len(b) != 1 or b[0] is not tuple:
        return False
    f = getattr(t, "_fields", None)
    if not isinstance(f, tuple):
        return False
    return all(isinstance(n, str) for n in f)
# ---
def __init__(self):
        self.metrics: dict[str, Any] = {}
# ---
def send_img(filename):
    return static_file(filename, root='images')
# ---
def list_images(self, pattern: str) -> list[ImageInfo]: ...
# ---
def KeyPos(self) -> Axis:
        return self.Pos.alias("key_position")
# ---
def test_selector_adds_new_axis():
    B, S, V, T = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 5), Axis("step", 4)
    logits = hax.arange((B, S, V))
    idx = hax.arange((B, T), dtype=jnp.int32) % V.size
    out = logits["vocab", idx]
    assert set(out.axes) == {B, S, T}
    ref = jnp.transpose(_ref_gather(logits, V, idx), (0, 2, 1))
    assert jnp.array_equal(out.array, ref)
# ---
def rust_minhash_pipeline(batch: pa.RecordBatch) -> int:
    pipeline = [
        Transformation.CleanText(input_col="text", output_col="clean"),
        Transformation.MinHash(input_col="clean", output_col="sig", num_perms=286, ngram_size=5, seed=42),
        Transformation.MinHashLSH(input_col="sig", output_col="buckets", num_bands=26),
    ]
    res = dupekit.transform(batch, pipeline)
    return len(res)
# ---
def _add(a, b):
    return a + b
# ---
def POST(self):
        form = web.input(name="Nobody",greet="Hello")

        greeting = "%s,%s" % (form.greet,form.name)

        return render.index(greeting = greeting)
# ---
def __repr__(self):
        return f"TakePerShardOp(n={self.n})"
# ---
def _apply_replacements(text: str, replacements: dict[str, str]) -> str:
    updated = text
    for old, new in replacements.items():
        updated = updated.replace(old, new)
    return updated
# ---
def extract_version_from_path(gcs_path: str) -> str:
    """Extract the version name from a GCS path."""
    # Extract model name from path like "gs://marin-eu-west4/checkpoints/llama-8b-tootsie-0.001-19ad63/hf/"
    parts = gcs_path.strip("/").split("/")
    return parts[-3]
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> Optional[float]:
        # TODO: implement
        return None
# ---
def on_random_range_min_changed(self):
        self.ui.spinBoxRandomMaximum.setMinimum(self.ui.spinBoxRandomMinimum.value())
# ---
def _make_demo_config() -> config_pb2.IrisClusterConfig:
    config = config_pb2.IrisClusterConfig()
    cpu_sg = config.scale_groups["cpu"]
    cpu_sg.name = "cpu"
    cpu_sg.accelerator_type = config_pb2.ACCELERATOR_TYPE_CPU
    cpu_sg.min_slices = 0
    cpu_sg.max_slices = 1
    return IrisConfig(config).as_local().proto
# ---
def test_compile_2(self):
        compiler = PatternCompiler(pattern_set=dict(
            TEST=r'\w+'
        ))

        try:
            c1 = compiler.compile('$1{TEST}')
        except:
            self.assertTrue(1)

        c1 = compiler.compile('$1{TEST}', ['test'])
        self.assertEqual(c1, r'(?:(?P<test>(\w+)))')
# ---
def stop_cluster(ctx):
    """Stop cluster."""
    config_obj, config_path = ctx.obj.config_obj, ctx.obj.config_file
    if not config_obj or not config_path:
        print("Error: --config required for cluster commands", file=sys.stderr)
        sys.exit(1)

    _stop_cluster_internal(ctx.obj, config_obj, config_path)
    print("Cluster stopped successfully!")
# ---
def linebreak(self):
        """Rendering line break like ``<br>``."""
        if self.options.get('use_xhtml'):
            return '<br />\n'
        return '<br>\n'
# ---
def clear(self):
        self.cookies = {}
# ---
def vms(self) -> list[ManagedVm]:
        """Individual VM instances in this group."""
        ...
# ---
def __init__(self, window):
        from time import time
        from pyglet.text import Label
        self.label = Label('', x=10, y=10,
                           font_size=24, bold=True,
                           color=(127, 127, 127, 127))

        self.window = window
        self._window_flip = window.flip
        window.flip = self._hook_flip

        self.time = 0.0
        self.last_time = time()
        self.count = 0
# ---
def __post_init__(self):
        assert (
            self.num_heads % self.num_kv_heads == 0
        ), f"num_heads={self.num_heads} not divisible by num_kv_heads={self.num_kv_heads}."
# ---
def add_jobs(thread_id: int):
        try:
            barrier.wait()
            for i in range(jobs_per_thread):
                job_id = f"t{thread_id}_j{i}"
                req = job_request(f"job-{job_id}")
                submit_job(state, job_id, req)
        except Exception as e:
            errors.append(e)
# ---
def list_tasks(self, request: cluster__pb2.Worker.ListTasksRequest, ctx: RequestContext) -> cluster__pb2.Worker.ListTasksResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def server_url(self) -> str:
        if self.vllm_server is None:
            raise RuntimeError("vLLM server is not running in this environment.")
        return self.vllm_server.server_url
# ---
def logistic(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.logistic(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def parse_block_html(self, m):
        tag = m.group(1)
        if not tag:
            text = m.group(0)
            self.tokens.append({
                'type': 'close_html',
                'text': text
            })
        else:
            attr = m.group(2)
            text = m.group(3)
            self.tokens.append({
                'type': 'open_html',
                'tag': tag,
                'extra': attr,
                'text': text
            })
# ---
def __repr__(self):
        return f"ReduceOp(local={_get_fn_name(self.local_reducer)}, global={_get_fn_name(self.global_reducer)})"
# ---
def _create_volume(self, size='0'):
        """Create a volume object."""
        vol = {}
        vol['size'] = size
        vol['user_id'] = 'fake'
        vol['project_id'] = 'fake'
        vol['host'] = 'localhost'
        vol['availability_zone'] = FLAGS.storage_availability_zone
        vol['status'] = "creating"
        vol['attach_status'] = "detached"
        return db.volume_create(self.context, vol)
# ---
def read(self, cr, uid, ids, fields=None, context=None, load='_classic_read'):
        return self.pool.get('stock.picking').read(cr, uid, ids, fields=fields, context=context, load=load)
# ---
def environment_type(s):
    if s not in _VALID_ENVIRONMENTS:
      raise argparse.ArgumentTypeError(
          f'Invalid Environment specified: "{s}".')
    return s
# ---
def _round_to_power_of_two(x: float) -> int:
    """Round x UP to the nearest power of 2."""
    if x <= 1:
        return 1
    return 2 ** math.ceil(math.log2(x))
# ---
def upgrade(active_plugins=None, options=None):
    op.add_column('vnf_lcm_subscriptions',
                  sa.Column('tenant_id', sa.String(length=64),
                  nullable=False))

    op.add_column('vnf_lcm_op_occs',
                  sa.Column('tenant_id', sa.String(length=64),
                  nullable=False))
# ---
def test_single_expression_change():
    source = "x = 1 + 2\n"
    target = "x = 3 + 4\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1

    # Applying all edits should produce valid Python.
    current = source
    for edit in edits:
        m = edit.to_mutation(current)
        current = m.apply(current)
    ast.parse(current)
# ---
def get_task(self, task_id: JobName) -> ControllerTask | None:
        with self._lock:
            return self._tasks.get(task_id)
# ---
def _load_environment(self, lesson_id: str) -> MarinEnv:
        """Load environment from lesson ID."""
        if lesson_id in self._environments:
            return self._environments[lesson_id]

        lesson_config = self.config.curriculum_config.lessons[lesson_id]
        env = load_environment_from_spec(lesson_config.env_config)
        self._environments[lesson_id] = env
        return env
# ---
def test_t():
    param = hax.arange(Width, start=0.1)
    check_gen_is_equal(lambda k, s: jax.random.t(k, param.array, shape=s), lambda k, s: hax.random.t(k, s, param))

    check_gen_is_equal(lambda k, s: jax.random.t(k, 0.5, shape=s), lambda k, s: hax.random.t(k, s, 0.5))
# ---
def rint(a: A) -> A:
    return wrap_elemwise_unary(jnp.rint, a)
# ---
def __init__(self, app):
        self._app = app
        self._items = []
        self.model = MyMusicModel(app)
# ---
def query(question, default=None):
    "Ask the user a question and return the response"
    while True:
        if default:
            sys.stdout.write("{} [{}]: ".format(question, default))
        else:
            sys.stdout.write("%s: " % question)
        answer = sys.stdin.readline().strip().replace(" ", "_")
        if answer == "":
            if default:
                return default
        else:
            return answer
# ---
def transition_task_to_running(state: ControllerState, task: ControllerTask) -> None:
    """Transition a task to RUNNING state via event."""
    state.handle_event(
        TaskStateChangedEvent(
            task_id=task.task_id,
            new_state=cluster_pb2.TASK_STATE_RUNNING,
            attempt_id=task.current_attempt_id,
        )
    )
# ---
def from_submission_id(ray_job_id: str, name: str) -> "RayJobInfo":
        return RayJobInfo(ref=None, submission_id=ray_job_id, name=name)
# ---
def _makeglobrecursive(pat):
    """Make a glob pattern recursive by appending "/**" to it"""
    if pat.endswith("/") or not pat:
        return pat + "**"
    else:
        return pat + "/**"
# ---
def _get_lb_name(self, msg):
        # TODO(wenjianhn): utf-8 support, base64
        ##return "%s_%s" % (msg['project_id'],
        return "%s" % msg['uuid']
# ---
def Receptionist_Hears_Dialtone (self):
        self.Step (Message = "Receptionist hears dial-tone...")

        self.Log (Message = "Receptionist agent waits for dial-tone...")
        self.Receptionist.sip_phone.Wait_For_Dialtone ()
# ---
def f(llama_model, input_ids, mask):
        out = llama_model(input_ids, mask)
        return hax.sum(out).scalar()
# ---
def test_scan_not_0th_axis():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take(Width, 2)

    total, selected = hax.scan(scan_fun, Depth)(0.0, named1)

    assert jnp.all(jnp.isclose(total, jnp.sum(named1.array, axis=(0, 1, 2))))
    assert jnp.all(jnp.equal(selected.array, named1.take(Width, 2).rearrange(selected.axes).array))
# ---
def updateTable(self):
        pass
# ---
def get_numbers_as_list(midi_str):
    '''
    Translate each char into a number, return in a list.
    Used for reading data messages where each byte encodes
    a different discrete value.

    >>> get_numbers_as_list('\\x00\\x00\\x00\\x03')
    [0, 0, 0, 3]
    '''
    post = []
    for item in midi_str:
        if is_num(item):
            post.append(item)
        else:
            post.append(ord(item))
    return post
# ---
def without_axes(axis_spec: AxisSelection, to_remove: AxisSelection, allow_mismatched_sizes=False) -> AxisSelection:  # type: ignore
    """As eliminate_axes, but does not raise if any axis in to_remove is not present in axis_spec"""
# ---
def __getitem__(self, key):
        return self.docs[key]
# ---
def is_valid(x, invalid=INVALID):
    """
    Returns a boolean array indicating whether each token in the input is valid.
    A token is considered valid if it is not negative and not equal to INVALID.
    """
    return (x >= 0) & (x != invalid)
# ---
def __get__(self, record, owner):
        if record is None:
            return self         # the field is accessed through the class owner
        if not record:
            return False
        return record.ensure_one()._ids[0]
# ---
def _compute_related(self, records):
        """ Compute the related field ``self`` on ``records``. """
        for record in records:
            value = record
            # traverse the intermediate fields, and keep at most one record
            for name in self.related[:-1]:
                value = value[name][:1]
            record[self.name] = value[self.related[-1]]
# ---
def save(self, fs, node_id):
        assert isinstance(fs, EnkfFs)
        assert isinstance(node_id, NodeId)

        EnkfNode.cNamespace().store(self, fs, True, node_id)
# ---
def template_output(output):
    # output, output_cmd
    output = os.path.abspath(output)
    return "output\t{}".format(output), ["--output", output]
# ---
def test_unsupported_node_returns_empty():
    # A function call is not in our PyExpr sort.
    assert generate_expression_variants("f(x)") == []
# ---
def start(self) -> str:
        """Start controller, return address. Idempotent - returns existing if healthy."""
        ...
# ---
def training_sets(self) -> Mapping[str, ProcessedAudioCache]:
        doc_caches = self.build_caches("train")
        return doc_caches
# ---
def tree_unflatten(treedef, leaves):
    """
    Provided for consistency with tree_flatten.
    """
    return jax.tree_util.tree_unflatten(treedef, leaves)
# ---
def _direct_ssh(ssh_config: SshConfig, host: str) -> DirectSshConnection:
    return DirectSshConnection(
        host=host,
        user=ssh_config.user,
        port=ssh_config.port,
        key_file=ssh_config.key_file or None,
        connect_timeout=ssh_config.connect_timeout,
    )
# ---
def tearDown(self):
        removef(self.dbname)
        removef(self.bakname)
        super(SQLiteDBChecker, self).tearDown()
# ---
def __init__(self, request, policy):
		self.request = request
		self.policy = policy
# ---
def fresh_marin_tokenizer():
    try:
        base = load_llama3_tokenizer()
    except Exception as exc:
        pytest.skip(f"Could not load llama3 tokenizer: {exc}", allow_module_level=True)
    return create_marin_tokenizer(base)
# ---
def testGradientInput0(self):
    with self.test_session(use_gpu=False):
      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2],
                   dtype=tf.float64, name="x")
      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],
                   shape=[2, 4], dtype=tf.float64, name="y")
      m = tf.matmul(x, y, name="matmul")
      err = gc.ComputeGradientError(x, [3, 2], m, [3, 4])
    print("matmul input0 gradient err = ", err)
    self.assertLess(err, 1e-10)
# ---
def has_notes(self):
        '''Return True/False if this track has any note-on/note-off pairs defined.
        '''
        for event in self.events:
            if event.is_note_on():
                return True
        return False
# ---
def example_reading_spec(self):
    label_key = "image/class/label"
    data_fields, data_items_to_decoders = (
        super(Image2ClassProblem, self).example_reading_spec())
    data_fields[label_key] = tf.FixedLenFeature((1,), tf.int64)

    data_items_to_decoders["targets"] = contrib.slim().tfexample_decoder.Tensor(
        label_key)
    return data_fields, data_items_to_decoders
# ---
def __lshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.left_shift(self, other)
# ---
def _maybe_flatten(q, axes, name):
    if axes:
        q = q.flatten_axes(axes, name)
    else:
        q = q.broadcast_axis(Axis(name, 1))
    return q
# ---
def match(self, item):
        """Check whether this query matches a given Item. Can be used to
        perform queries on arbitrary sets of Items.
        """
        raise NotImplementedError
# ---
def test_with_scan():
    X = hax.Axis("x", 5)
    ref = hax.new_ref(hax.zeros(X))

    @jax.jit
    def foo(ref, xs):
        def scan_fn(_, x):
            ref_slice = ref.slice({"x": x})
            ref_slice[...] = (x * x).astype(ref_slice.dtype)
            return None, x * 2

        return hax.scan(scan_fn, X)(None, xs)[1]

    out = foo(ref, jnp.arange(X.size))

    assert jnp.all(ref.value().array == jnp.arange(X.size) ** 2)

    assert jnp.all(out == jnp.arange(X.size) * 2)
# ---
def forwards(self, orm):
        # Adding unique constraint on 'Vendeur', fields ['code_permanent']
        db.create_unique(u'encefal_vendeur', ['code_permanent'])
# ---
def format_temps(temps):
    return format_line(prefix='temps'.rjust(RJUST), values=temps)
# ---
def _check_worker_running(self, host: str) -> bool:
        """Check if a worker is healthy on the given host."""
        conn = self._create_ssh_connection(host)
        port = self._bootstrap_config.worker_port or 10001
        try:
            result = conn.run(f"curl -sf http://localhost:{port}/health", timeout=Duration.from_seconds(10))
            return result.returncode == 0
        except Exception:
            return False
# ---
def get_power_range(self):
		return self.power_range
# ---
def error_messages(self) -> list[str]:
        """Collect non-empty error messages from VMs."""
        return [v.init_error for v in self.vms if v.init_error]
# ---
def reset(self) -> "KvPageCache":
        """Return a reset version of this cache."""
        reset_pages = jnp.zeros_like(self.kv_pages.array)
        return dataclasses.replace(self, kv_pages=NamedArray(reset_pages, self.kv_pages.axes))
# ---
def get_lock_file_hash(lock_path):
    """Get a hash of the lock file for cache invalidation."""
    try:
        mtime = os.path.getmtime(lock_path)
        return hashlib.md5(f"{lock_path}:{mtime}".encode()).hexdigest()[:12]
    except OSError:
        return None
# ---
def the_object_name_has_a_body_of_value(name, value):
    assert the_object_name_exists(name).data.body == value
# ---
def __getstate__(self):
        return {"host": self.host, "port": self.port, "queue_name": self.queue_name}
# ---
def count_num_lines(path):
    with open(path) as path_inf:
        return sum(1 for line in path_inf)
# ---
def get_directory_friendly_dataset_name(hf_dataset_id: str) -> str:
    dataset_name = hf_dataset_id.replace("/", "--")
    dataset_name = dataset_name.replace(".", "-")
    dataset_name = dataset_name.replace("#", "-")
    return dataset_name
# ---
def compute(na, inp):
        return hax.nn.softmax(
            model(na, inp),
            axis=model.Vocab,
        )
# ---
def check_info(step_info: dict, step: ExecutorStep):
            assert step_info["name"] == step.name
            assert step_info["output_path"] == executor.output_paths[step]
            assert step_info["config"] == asdict_optional(executor.configs[step])
            assert step_info["version"] == executor.versions[step]
# ---
def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            self._stop_tokens = None
            self.max_tokens = 1024
# ---
def _validate_request(self, request):
        validate.check_tcp_request(request)
# ---
def backoff_until_ms(self) -> int:
        """Timestamp until which scale-up is blocked due to backoff."""
        return self._backoff_until.epoch_ms()
# ---
def test_binary_entrypoint(self):
        entry = Entrypoint.from_binary("python", ["-c", "print('hi')"])
        iris_entry = convert_entrypoint(entry)
        assert iris_entry.is_command
        assert iris_entry.command == ["python", "-c", "print('hi')"]
# ---
def get_db():
    """Connects to Mongo database"""
    if not hasattr(g, 'mongo_client'):
        g.mongo_client = connect_client()
        g.mongo_db = getattr(g.mongo_client, app.config['DB_NAME'])
        g.groups_collection = g.mongo_db[os.environ.get('DB_GROUPS_COLLECTION')]
    return g.mongo_db
# ---
def test_vhd(self):
        image_meta = {'id': 'a', 'disk_format': 'vhd'}
        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_VHD)
# ---
def test_deduplicate_all_unique(backend):
    """Test deduplication when all items are unique."""
    data = [{"id": i, "val": f"item_{i}"} for i in range(10)]

    ds = Dataset.from_list(data).deduplicate(key=lambda x: x["id"])

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 10
# ---
def validation_fn(model_weights, round_num):
    del round_num
    return evaluate_fn(model_weights, [cifar_test])
# ---
def map_svg():
    return make_map(request, format='svg')
# ---
def test_connection_available_accepts_duration_timeout():
    """connection_available works with Duration timeout (regression for TypeError)."""
    conn = FakeSshConnection()
    assert connection_available(conn, timeout=Duration.from_seconds(5)) is True
    assert conn.last_timeout == Duration.from_seconds(5)
# ---
def _flattened_spec(spec):
    out = []
    for s in spec:
        if isinstance(s, tuple):
            out.extend(s)
        elif s is None:
            pass
        else:
            out.append(s)

    return tuple(out)
# ---
def __init__(self, a, b):
        self.a = a
        self.b = b
# ---
def get_nick(self, nick):
        return self.data[nick.lower()]['nick']
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step, commit=None):
        del commit
        to_log = {}
        for k, v in metrics.items():
            to_log[k] = _convert_value_to_loggable_rec(v)

        import trackio

        trackio.log(to_log, step=step)
# ---
def output_double_emphasis(self, m):
        text = m.group(2) or m.group(1)
        text = self.output(text)
        return self.renderer.double_emphasis(text)
# ---
def inc_shared(ref_counts):
            def body(i, rc):
                page = src_pages["page", i]

                def inc(rc):
                    return rc.at["page", page].add(1)

                return jax.lax.cond(is_valid(page).scalar(), inc, lambda x: x, rc)

            limit = used_pages - jnp.where(is_boundary, 0, 1)
            return jax.lax.fori_loop(0, limit, body, ref_counts)
# ---
def setUp(self):
        for _index_name, index_class in settings.ELASTICSEARCH_INDEXES['default'].items():
            doctype = locate(index_class)
            alias_migration.setup_index(doctype)
# ---
def _get_stage_for_block(self, block_id: int) -> int:
        block_start = block_id * self.block_size
        stage_starts = np.array([start for start, _ in self.weight_stages])
        return max(0, np.searchsorted(stage_starts, block_start, side="right") - 1)
# ---
def should_display_status_to_user(self):
        """Whether or not the status from this attempt should be displayed to the user."""
        raise NotImplementedError
# ---
def put_number(num, length):
    '''
    Put a single number as a hex number at the end of a string `length` bytes long.

    >>> put_number(3, 4)
    b'\\x00\\x00\\x00\\x03'
    >>> put_number(0, 1)
    b'\\x00'
    '''
    lst = bytearray()

    for i in range(length):
        shift_bits = 8 * (length - 1 - i)
        this_num = (num >> shift_bits) & 0xFF
        lst.append(this_num)
    return bytes(lst)
# ---
def hash_binary(bin_path):
    blocksize = 65536
    hasher = hashlib.sha1()
    with open(bin_path, 'rb') as bin_file:
        buf = bin_file.read(blocksize)
        while len(buf) > 0:
            hasher.update(buf)
            buf = bin_file.read(blocksize)
    return hasher.hexdigest()
# ---
def test_perturb_operators_arithmetic(rng):
    source = "x + y"
    # Try many times since perturbation is probabilistic.
    found_different = False
    for seed in range(50):
        r = random.Random(seed)
        result = perturb_operators(source, r, swap_prob=1.0)
        if result is not None:
            found_different = True
            ast.parse(result)
            assert result != source
            break
    assert found_different, "Expected at least one operator perturbation"
# ---
def main() -> int:
    errors = _check_docs()
    if not errors:
        print("Docs source links: OK")
        return 0

    print("Docs source links: broken")
    for entry in errors:
        print(entry)
    return 1
# ---
def __init__(self, *args, **kwargs):
        super(User, self).__init__(*args, **kwargs)
        self.startup = None
        self.team_member = None
        self.profile = None
        self.user_finalist_roles = None
# ---
def worker(mock_bundle_cache, mock_image_cache, mock_runtime):
    """Create Worker with mocked dependencies."""
    config = WorkerConfig(
        port=0,
        port_range=(50000, 50100),
    )
    return Worker(
        config,
        bundle_provider=mock_bundle_cache,
        image_provider=mock_image_cache,
        container_runtime=mock_runtime,
    )
# ---
def test_vertices_equals_lat_lon(self):
        """Asserts that the "vertices" property is identical to the return
        value of to_lat_lon()."""
        assert_equal(self.polycircle.vertices, self.polycircle.to_lat_lon())
# ---
def playlist_clear(self):
        self.command('playlist_clear')
# ---
def main() -> None:
    args = parse_args()
    cfg = build_training_config(args)
    run_training(cfg, cache_dir=args.cache_dir)
# ---
def alive(self):
        return self.thread.is_alive() if self.thread else False
# ---
def test_cache(self):
        conn = testing.db.connect()
        cache = {}
        cached_conn = conn.execution_options(compiled_cache=cache)

        ins = users.insert()
        cached_conn.execute(ins, {'user_name':'u1'})
        cached_conn.execute(ins, {'user_name':'u2'})
        cached_conn.execute(ins, {'user_name':'u3'})
        assert len(cache) == 1
        eq_(conn.execute("select count(*) from users").scalar(), 3)
# ---
def __init__(
        self,
        controller_address: str,
        workspace: Path | None = None,
        bundle_gcs_path: str | None = None,
    ):
        logger.info(
            "FrayIrisClient connecting to %s (workspace=%s, bundle_gcs_path=%s)",
            controller_address,
            workspace,
            bundle_gcs_path,
        )
        self._iris = IrisClientLib.remote(controller_address, workspace=workspace, bundle_gcs_path=bundle_gcs_path)
# ---
def upload(request):
	#æ–‡ä»¶ä¸Šä¼ 
	return render(request, 'centres/upload.html')
# ---
def test_pass_different_length_seq(num_kv_heads):
    config = LlamaConfig(
        max_seq_len=64,
        hidden_dim=64,
        intermediate_dim=32,
        num_heads=2,
        num_kv_heads=num_kv_heads,
    )
    check_model_works_with_seqlen(LlamaLMHeadModel, config, 16)
# ---
def visit_BoolOp(self, node: ast.BoolOp) -> ast.BoolOp:
        node.op = self._maybe_swap(node.op, _BOOL_OPS)
        self.generic_visit(node)
        return node
# ---
def extend(self, batch: Sequence[T]):
        """
        Append a batch of data to the store.
        """
        jtu.tree_map(
            lambda writer, *xs: writer.extend([np.asarray(x) for x in xs]),
            self.tree,
            *batch,
            is_leaf=heuristic_is_leaf,
        )
# ---
def _resolve_save_reference_code(self, save_reference_code: Optional[bool]) -> bool:
        """Determine whether reference code should be bundled with the checkpoint."""
        #  the way we determine this is if the config class is in the HF package or not
        if save_reference_code is None:
            return not self.HfConfigClass.__module__.startswith("transformers.")

        return save_reference_code
# ---
def __repr__(self):
        return '<MultiCoverage %r: %r>' % (self.extent.llbbox, self.coverages)
# ---
def to_arrow(self, message):
        return self._parse_arrow_message(message)
# ---


def count_distinct_characters(string: str) -> int:
    """ Given a string, find out how many distinct characters (regardless of case) does it consist of
    >>> count_distinct_characters('xyzXYZ')
    3
    >>> count_distinct_characters('Jerry')
    4
    """
    return len(set(string.lower()))
# ---
def test_close_and_capture_again(testdir):
    testdir.makepyfile("""
        import os
        def test_close():
            os.close(1)
        def test_capture_again():
            os.write(1, b"hello\\n")
            assert 0
    """)
    result = testdir.runpytest_subprocess()
    result.stdout.fnmatch_lines("""
        *test_capture_again*
        *assert 0*
        *stdout*
        *hello*
    """)
# ---
def __init__(self, program, timeOfDay):
        super().__init__()
        self._program = program
        self._timeOfDay = timeOfDay
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        in_dtype = x.dtype
        x = x.astype(self.dtype)
        var = hax.mean(hax.square(x), axis=self.axis)
        inv = hax.rsqrt(var + self.eps)
        out = x * inv
        out = out.astype(in_dtype)

        if self.weight is not None:
            out = self.weight * out
        if self.bias is not None:
            out = out + self.bias
        return out
# ---
def test_host_maintenance_off(self):
        self._test_host_action(self.conn.host_maintenance_mode,
                               False, 'off_maintenance')
# ---
def compute_axis_mapping(self) -> ResourceMapping:
        return self.config.compute_axis_mapping
# ---
def VHeads(self) -> Axis:
        return Axis("v_heads", self.num_v_heads)
# ---
def launch(self, request: JobRequest) -> JobId:
        """Launch job on Ray cluster, returning job identifier."""
        logger.info("Launching job: %s", request.name)

        if isinstance(request.resources.device, TpuConfig):
            return self._launch_tpu_job(request)

        if request.entrypoint.binary_entrypoint is not None:
            return self._launch_binary_job(request)

        return self._launch_callable_job(request)
# ---
def to_string(value):
        """ Convert a :class:`datetime` value into the format expected by the ORM. """
        return value.strftime(DATETIME_FORMAT) if value else False
# ---
def __init__(self, subtype, msg=None):
        if msg is None:
            msg = "An error occured for subtype {}".format(subtype)
        super(PhylotyperError, self).__init__(msg)
        self.subtype = subtype
# ---
def condition(self):
        return len(self.el) >= 5 and not self.el.get('id', '').startswith('libelleLong')
# ---
def to_safe_token(self) -> str:
        """Return a filesystem/tag-safe token derived from this name."""
        return "job__" + "__".join(self._parts)
# ---
def create_tx(self, spend_tx, n, value, script=CScript([OP_TRUE, OP_DROP] * 15 + [OP_TRUE])):
        return create_tx_with_script(spend_tx, n, amount=value, script_pub_key=script)
# ---
def tick(self, ts: int | None = None) -> None:
        """Advance all VM group state transitions.

        Call this to simulate time passing and VMs completing boot/init.
        """
        ts = ts or Timestamp.now().epoch_ms()
        with self._lock:
            for fake_vm_group in self._slices.values():
                fake_vm_group.tick(ts)
# ---
def testFormatInode(self):
    """Tests the _FormatInode function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    inode_string = test_helper._FormatInode(
        event, event_data, event_data_stream)
    self.assertEqual(inode_string, '-')
# ---
def test_dicts_loop_io(benchmark: Any, small_parquet_path: str) -> None:
    """
    Python End-to-End: Read File -> List[dict] -> Loop calling Rust per item -> List[dict].
    Slowest Python approach (Baseline for worst case).
    """

    def _pipeline() -> int:
        docs = pq.read_table(small_parquet_path).to_pylist()
        return len([dupekit.process_dicts_loop(doc) for doc in docs])

    assert benchmark(_pipeline) > 0
# ---
def test_status_transitions(client: LocalClient):
    handle = client.submit(JobRequest(name="slow", entrypoint=Entrypoint.from_callable(_sleep_then_succeed)))
    # Should be running or pending initially
    initial = handle.status()
    assert initial in (JobStatus.PENDING, JobStatus.RUNNING)
    handle.wait()
    assert handle.status() == JobStatus.SUCCEEDED
# ---
def __call__(self, array: NamedArray, axis: AxisSelector | None = None, **kwargs) -> NamedArray: ...
# ---
def loss_fn(self) -> WrappedLossFunction:
        """
        Wrapped loss function that always returns (loss, metrics_dict).
        Casts the model to compute precision and sets the context axis mapping to compute.
        """
        return WrappedLossFunction(
            self._raw_loss_function,
            self.mp,
            self.compute_axis_mapping,
        )
# ---
def send_acknowledgement_notification(acknowledgement):
    subject = _("%s has acknowledged you on Villages.cc") % (
        acknowledgement.payer)
    send_notification(subject, acknowledgement.payer, acknowledgement.recipient,
                      'acknowledgement_notification_email.txt',
                      {'acknowledgement': acknowledgement})
# ---
def allocate(self, count: int = 1) -> list[int]:
        with self._lock:
            ports = []
            for _ in range(count):
                port = self._find_free_port()
                self._allocated.add(port)
                ports.append(port)
            return ports
# ---
def fn(config: MyConfig | None):
        append_log(log, config)
# ---
def conjugate(a: A) -> A:
    return wrap_elemwise_unary(jnp.conjugate, a)
# ---
def utf8_to_bytearray(x):
    return bytearray(x, 'utf-8')
# ---
def Embed(self) -> Axis:
        return Axis(name="embed_dim", size=self.d_model)
# ---
def sqrt(a: A) -> A:
    return wrap_elemwise_unary(jnp.sqrt, a)
# ---
def cursor_execute(
                self,
                execute,
                cursor,
                statement,
                parameters,
                context,
                executemany,
                ):
                cursor_stmts.append((str(statement), parameters, None))
                return execute(cursor, statement, parameters, context)
# ---
def get_github_token() -> str:
    """Get GitHub token from environment variable."""
    token = os.getenv("MARIN_CI_TOKEN")

    if not token:
        logging.error("MARIN_CI_TOKEN environment variable not set")
        logging.error("Create a fine-grained PAT with repo:actions scope")
        sys.exit(1)

    return token
# ---
def main(islinput, inputfile, pluginData, globalData):
	currentIndex = 0
	for item in islinput:
		item = normalizeEnter(item) #Deletes not wanted line breaks in order to prevent the problem we have with Markdown.
		islinput[currentIndex] = item
		currentIndex += 1
	return islinput, pluginData, globalData
# ---
def area(self):
        return math.pi * self.radius ** 2
# ---
def __init__(self, maxlen: int = 5000):
        self._buffer: deque[BufferedLogRecord] = deque(maxlen=maxlen)
        self._lock = Lock()
# ---
def test_append_single_rank(cache_metadata):
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=1, dtype=jnp.float32, cache_metadata=cache_metadata)

        data = jnp.array([1.0, 2.0, 3.0])
        builder.append(data)

        assert len(builder) == 1

        result = builder[0]
        assert jnp.all(result == data)
# ---
def get_version():
    return addon.getAddonInfo('version')
# ---
def __enter__(self):
        self.start()
# ---
def __init__(self, connection):
                self.connection = connection
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        global _global_tracker
        _global_tracker = self.old_tracker
# ---
def _build_cmd(self, command: str) -> list[str]:
        return [
            "gcloud",
            "compute",
            "tpus",
            "tpu-vm",
            "ssh",
            self.vm_id,
            f"--zone={self._zone}",
            f"--project={self.project_id}",
            f"--worker={self.worker_index}",
            "--quiet",
            "--command",
            command,
        ]
# ---
def conjugate(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.conjugate(), self.axes)
# ---
def __gt__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "gt")
# ---
def __str__(self):
        return f'[{self.start}, {self.end})'
# ---
def update_fn(updates, state, params=None, **extra_args):
        del params

        def _clip_fn(u):
            clip_denom = hax.maximum(1.0, hax.sqrt(hax.mean(u * u)) / threshold)
            return u / clip_denom

        updates = scan_aware_tree_map(_clip_fn, updates)
        return updates, state
# ---
def crt(congruences):
        x = 0
        M = 1
        for i in xrange(len(congruences)):
                M *= congruences[i][2]
                congruences[i] = reduceCongr(congruences[i][0], congruences[i][1], congruences[i][2])

        for j in xrange(len(congruences)):
                m = congruences[j][2]
                if gcd(m, M/m) != 1:
                        return None
                x += congruences[j][1] * modInverse(M/m, m) * M / m

        return x % M
# ---
def raise_parse_error(message: str, expression: str, pos: int | tuple[int, int] | None) -> NoReturn:
    """Raise a ValueError with a message and the position in the expression."""
    fmt = f"Error while parsing:\n    {expression}"
    if pos is not None:
        if isinstance(pos, int):
            fmt += f'\n    {" " * pos}^'
        else:
            fmt += f"\n    {' ' * pos[0]}{'^' * max(1, pos[1] - pos[0])}"

    fmt += f"\n{message}"

    raise ValueError(fmt)
# ---
def test_dialect_conn_options(self):
        engine = testing_engine("sqlite://", options=dict(_initialize=False))
        engine.dialect = Mock()
        conn = engine.connect()
        c2 = conn.execution_options(foo="bar")
        eq_(
            engine.dialect.set_connection_execution_options.mock_calls,
            [call(c2, {"foo": "bar"})]
        )
# ---
def fun_wrapped(dynamic_donated, dynamic_reserved, static):
        dynamic = eqx.combine(dynamic_donated, dynamic_reserved)
        dynamic_fun, dynamic_spec = dynamic
        static_fun, static_spec = static

        fun = hashable_combine(dynamic_fun, static_fun)
        args, kwargs = hashable_combine(dynamic_spec, static_spec)
        out = fun(*args, **kwargs)
        out_dynamic, out_static = hashable_partition(out, is_array)
        return out_dynamic, Static(out_static)
# ---
def data_received(self, data):
        """Called with snippets received from the serial port"""
        raise NotImplementedError
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        return vm_pb2.SliceInfo(
            slice_id=self._group_id,
            scale_group=self._scale_group,
            created_at=self._created_at.to_proto(),
            vms=[vm.info for vm in self._managed_vms],
        )
# ---
def initialize(config: TrainerConfig | AllConfig):
    """Initializes jax, logging, setting the run name/id in the process. Also initializes tracking and saves config
    as hyperparameters and an artifact"""
    if isinstance(config, TrainerConfig):
        trainer_config = config
    else:
        trainer_config = config.trainer

    trainer_config.initialize()
    levanter.tracker.log_configuration(config)
# ---
def test_optimize_consecutive_maps():
    """Consecutive maps should be fused into a single stage."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            MapOp(lambda x: x + 1),
            MapOp(lambda x: x * 3),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    assert len(plan.stages[0].operations) == 1
    fused_op = plan.stages[0].operations[0]
    assert isinstance(fused_op, Map)
# ---
def main(config: ConvertLingolyToDolmaConfig) -> None:
    """CLI entrypoint."""
    convert_lingoly_to_dolma(config)
# ---
def _convert_objectGUID(item):
    item = uuid.UUID("{{{0!s}}}".format(item)).bytes_le
    item = escape_bytes(item)
    return item
# ---
def test_copy_experiment(self):
        """ Tests that copy_experiment creates a new experiment """
        experiment = self.create_test_experiment()
        num_experiments = Experiment.objects.count()
        url = reverse("ab_testing_tool_copy_experiment", args=(experiment.id,))
        response = self.client.post(url, follow=True)
        self.assertOkay(response)
        self.assertEqual(Experiment.objects.count(), num_experiments + 1)
# ---
def bar_open(self): pass
# ---
def format_buffers(buffers):
    return format_line(prefix='buffers'.rjust(RJUST), values=buffers)
# ---
def print_all(self):
        '''
        Print out all managed keys
        '''
        self.print_key('*')
# ---
def reset(self) -> None:
        try:
            os.unlink(self._path)
        except FileNotFoundError:
            pass
# ---
def parameter_axis_mapping(self) -> ResourceMapping:
        return self.mesh.resolved_param_mapping
# ---
def __init__(self, rname, namespace, kubeconfig, registry_options):
        super(RegistryConfig, self).__init__(rname, namespace, kubeconfig, registry_options)
# ---
def _materialize_sharded_tensor_from_host_array(
    array: np.ndarray,
    sharding: jax.sharding.Sharding,
) -> jax.Array:
    indices_map = sharding.devices_indices_map(array.shape)
    local_devices = sharding.addressable_devices
    per_device_arrays = []
    for device in local_devices:
        indices = tuple(indices_map[device])
        per_device_arrays.append(jax.device_put(array[indices], device))
    return jax.make_array_from_single_device_arrays(array.shape, sharding, per_device_arrays)
# ---
def test_run_streaming_with_retry_success_first_attempt():
    """run_streaming_with_retry succeeds on first attempt."""
    conn = MagicMock()
    conn.run_streaming.return_value = make_fake_popen()
    lines_received: list[str] = []
    result = run_streaming_with_retry(conn, "bootstrap script", max_retries=3, on_line=lines_received.append)
    assert result.returncode == 0
    assert len(lines_received) > 0
# ---
def rearrange(self, expression: str, **bindings: AxisSelector | int) -> "NamedArray":
        """See [haliax.rearrange][] for details."""
        pass
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        return dir in self._dirs
# ---
def __repr__(self):
        return f"JoinOp(type={self.join_type})"
# ---
def test_clear(self):
        storage = MemcachedStorage("memcached://localhost:22122")
        limiter = FixedWindowRateLimiter(storage)
        per_min = RateLimitItemPerMinute(1)
        limiter.hit(per_min)
        assert not limiter.hit(per_min)
        limiter.clear(per_min)
        assert limiter.hit(per_min)
# ---
def allow_cancel(self, cr, uid, ids, context=None):
        for pick in self.browse(cr, uid, ids, context=context):
            if not pick.move_lines:
                return True
            for move in pick.move_lines:
                if move.state == 'done':
                    raise osv.except_osv(_('Error!'), _('You cannot cancel the picking as some moves have been done. You should cancel the picking lines.'))
        return True
# ---
def get_choice_tokens(self, choice):
            return np.array([ord(c) for c in choice.message.content], dtype=np.int32)
# ---
def get_obj(self, index: int) -> T:
        return self._index_to_obj[index]
# ---
def test_linear_has_no_function_leaves_by_default():
    H, C, W, E = hax.make_axes(H=10, C=12, W=14, E=16)

    hax_linear = hax.nn.Linear.init((H, C, W), E, key=jrandom.PRNGKey(0))
    assert all(not isinstance(v, Callable) for v in jax.tree_util.tree_leaves(hax_linear))
# ---
def busy_loop():
        while True:
            print("test")
# ---
def step_impl(context):
    context.execute_steps(u'''
        given I open History dialog
    ''')
    history = context.browser.find_element_by_id("HistoryPopup")
    entries = history.find_elements_by_xpath('.//li[not(@data-clone-template)]')
    assert len(entries) > 0, "There are no entries in the history"
    item = entries[0]
    item.find_elements_by_xpath('.//*[@data-share-item]')[0].click()
# ---
def _get_data(self):
        return self._parameter1
# ---
def _raw(self, name):
		"""
		Returns the raw value from field 'name'
		"""
		index = self.structure.index(name)
		return self[index]
# ---
def flatten_axes(self, old_axes: AxisSelection, new_axis: AxisSelector) -> "NamedArray":  # pragma: no cover
        return haliax.flatten_axes(self, old_axes=old_axes, new_axis=new_axis)
# ---
def test_parse_to_value_exception(self):
        text = 'not important'

        with mock.patch.object(yaml, 'load') as yaml_loader:
            yaml_loader.side_effect = self.raised_exception

            self.assertRaises(ValueError,
                              template_format.parse, text)
# ---
def _log_libtpu_args_once():
    global _LOGGED_LIBTPU_ARGS
    if not _LOGGED_LIBTPU_ARGS:
        args = os.environ.get("LIBTPU_INIT_ARGS", "<unset>")
        _logger.info("LIBTPU_INIT_ARGS (worker): %s", args)
        _LOGGED_LIBTPU_ARGS = True
# ---
def mock_requests_get(url, **kwargs):
        from unittest.mock import Mock

        response = Mock()
        response.status_code = 200
        response.headers = {"content-length": str(len(compressed_data))}
        response.raw = BytesIO(compressed_data)
        return response
# ---
def loraize(model: M, config: LoraConfig, key: jax.random.PRNGKey) -> M:
    """
    Applies LoRA transform to the given model by replacing Linear layers that match the given pattern with LoraLinear layers.
    """
    return _loraize(model, config, key, "", batch_dims=())
# ---
def check_health(self) -> bool:
        """Check if worker is healthy via health endpoint."""
        port = self._bootstrap_config.worker_port or 10001
        try:
            result = self._conn.run(f"curl -sf http://localhost:{port}/health", timeout=Duration.from_seconds(10))
            return result.returncode == 0
        except Exception:
            return False
# ---
def format_match(match):
    print(match.match_number)
    print(match.team_names)
    print(match.team_numbers)
    return '{}: {} ({}) & {} ({}) vs. {} ({}) & {} ({})'.format(
        match.match_number,
        match.team_names[0], match.team_numbers[0],
        match.team_names[1], match.team_numbers[1],
        match.team_names[2], match.team_numbers[2],
        match.team_names[3], match.team_numbers[3],
        )
# ---
def QLoraSize(self) -> Axis:
        return Axis("q_lora_rank", self.q_lora_rank)
# ---
def to_ms(self) -> int:
        """Convert to milliseconds."""
        return self._ms
# ---
def UnregisterEndpoint(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def finger(self, match):
        '''
        Print out the fingerprints for the matched keys

        :param str match: A string to match against. i.e. 'web*'
        '''
        matches = self.key.finger(match)
        salt.output.display_output(
                matches,
                'key',
                self.opts)
# ---
def test_profiler_get_shorten_id(self):
        uuid_id = "4e3e0ec6-2938-40b1-8504-09eb1d4b0dee"
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        result = prof.get_shorten_id(uuid_id)
        expected = "850409eb1d4b0dee"
        self.assertEqual(expected, result)
# ---
def get_plugin_url(queries):
    try:
        query = urllib.urlencode(queries)
    except UnicodeEncodeError:
        for k in queries:
            if isinstance(queries[k], unicode):
                queries[k] = queries[k].encode('utf-8')
        query = urllib.urlencode(queries)

    return sys.argv[0] + '?' + query
# ---
def _detect_resource_type() -> Literal["tpu", "gpu", "unknown"]:
    if _detect_tpu_environment():
        return "tpu"
    if _detect_nvidia_gpu_environment():
        return "gpu"
    return "unknown"
# ---
def get_mapping(self, index, doc_type=None):
        return get_indices(self.es).get_mapping(index=index, doc_type=doc_type)
# ---
def __eq__(self, other):
        if not isinstance(other, TwoStageConfig):
            return False
        return hash(self) == hash(other)
# ---
def __init__(self):
        self._count = 0
# ---
def filter_eval_shape(*args, **kwargs):
    import warnings

    warnings.warn(
        "filter_eval_shape is deprecated, use eqx.filter_eval_shape instead",
        DeprecationWarning,
    )
    return eqx.filter_eval_shape(*args, **kwargs)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """CPU FLOPS not tracked."""
        raise NotImplementedError("CPU FLOPS not available")
# ---
def test_edit_to_mutation_roundtrip():
    edit = Edit(
        source_start=4,
        source_end=9,
        target_fragment="world",
        node_type="Name",
        stmt_count=0,
    )
    source = "say hello there"
    mutation = edit.to_mutation(source)
    assert mutation.apply(source) == "say world there"
    assert mutation.original == "hello"
# ---
def fast_job(n):
            return n * 2
# ---
def onlySpeakDisplayedTextToggled(self, widget):
        """Signal handler for the "toggled" signal for the GtkCheckButton
        onlySpeakDisplayedText. In addition to updating the preferences,
        set the sensitivity of the contextOptionsGrid.

        Arguments:
        - widget: the component that generated the signal.
        """

        enable = widget.get_active()
        self.prefsDict["onlySpeakDisplayedText"] = enable
        self.get_widget("contextOptionsGrid").set_sensitive(not enable)
# ---
def find_open_port() -> int:
    """Find an open port on localhost."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("", 0))
        return s.getsockname()[1]
# ---
def fill_customer_code(self):
		""" Append all the customer codes and insert into "customer_code" field of item table """
		cust_code = []
		for d in self.get('customer_items'):
			cust_code.append(d.ref_code)
		self.customer_code = ','.join(cust_code)
# ---
def softmax(data, axis):
    r"""Computes softmax.

    .. math:: \text{softmax}(x)_i = \frac{exp(x_i)}{\sum_j exp(x_j)}

    .. note::
        This operator can be optimized away for inference.

    Parameters
    ----------
    data: relay.Expr
        The input data to the operator.

    axis: int
        The axis to sum over when computing softmax
    """

    return _make.softmax(data, axis)
# ---
def isOK( self ):
    return self.valid
# ---
def process_latex_matrices(text: str) -> str:
    """Convert matrix environments to simple bracket notation."""

    def matrix_replacer(content: str) -> str:
        return f"[{content}]"

    for env in ["matrix", "pmatrix", "bmatrix", "vmatrix", "Vmatrix"]:
        text = replace_latex_environment(text, env, matrix_replacer)

    return text
# ---
def status(self) -> TaskState:
        """Current task state (PENDING, RUNNING, SUCCEEDED, etc.)."""
        ...
# ---
def __call__(
        self,
        lhs,
        rhs,
        dimension_numbers,
        precision: PrecisionLike = None,
        preferred_element_type: DTypeLike | None = None,
        **kwargs,
    ) -> jnp.ndarray: ...
# ---
def __init__(
        self,
        coordinator: OctoprintDataUpdateCoordinator,
        tool: str,
        temp_type: str,
        device_id: str,
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, f"{temp_type} {tool} temp", device_id)
        self._temp_type = temp_type
        self._api_tool = tool
# ---
def __call__(
        self,
        lhs,
        rhs,
        dimension_numbers,
        precision,
        preferred_element_type=None,
        **kwargs,
    ):
        cfg = aqt_config.set_context(self.cfg, jrandom.PRNGKey(42), train_step=None)
        return cfg(lhs, rhs, dimension_numbers, precision, preferred_element_type)
# ---
def cos(a: A) -> A:
    return wrap_elemwise_unary(jnp.cos, a)
# ---
def getcommissioninfo(self, data):
        if data._name in self.comminfo:
            return self.comminfo[data._name]

        return self.comminfo[None]
# ---
def process_bind_param(self, value, dialect):
                raise MyException("nope")
# ---
def p_translation_unit(self, p):
        """
        translation_unit : translate_task
                         | translation_unit translate_task
                         |
        """
        pass
# ---
def get_words_on_cluster(self, cluster):
        return self.vocab[self.clusters == cluster]
# ---
def setUp(self):
        self.cj = cangjie.Cangjie(self.version, self.language)
# ---
def unescape(s):
    s = s.replace("&lt;", "<")
    s = s.replace("&gt;", ">")
    # this has to be last:
    s = s.replace("&amp;", "&")
    return s
# ---
def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig)
# ---
def value_match(cls, pattern, value):
        return pattern == value
# ---
def LoadFromFile(self, file):
        self.fs, self.s = wav.read(file)
        self.sLength, self.nChans = self.s.shape
# ---
def h_fs_system(_,path,eltName='',cwd=None):
        import subprocess as sp
        import shlex
        data=sp.Popen(shlex.split(path),cwd=cwd,stdout=sp.PIPE, stderr=sp.PIPE).communicate()
        _.ws.send(json.dumps({"method":"fs_system","result":[path,data,eltName]}));
        pass
# ---
def _get_logs(self, limit: int = 50) -> list[LogEntry]:
        with self._logs_lock:
            return list(self._logs)[-limit:]
# ---
def testAssignMultiple(self):
    self.assertEqual((0, 'baz baz\n'), _GrumpRun(textwrap.dedent("""\
        foo = bar = 'baz'
        print foo, bar""")))
# ---
def init_log(self, tail: int | None = None) -> str:
        return ""
# ---
def build(self, ctx: LrScheduleContext):
        return _inv_sqrt_decay_schedule(ctx.learning_rate, ctx.min_lr, ctx.warmup_steps, self.timescale)
# ---
def getradio(id) :
    db = cherrypy.session['database']
    if id.isdigit() :
        sql = "select radio, genre, url from Radio where id=%s" % id
    else:
        sql = "select radio, genre, url from Radio where url=%s" % id
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
    except:
        rows = [('Not Found', '', '')]

    rows = cur.fetchone()
    if rows == None:
        rows = ('Not Found', '', '')

    con.close()

    return rows
# ---
def dequeue():
    try:
        dequeue_task()
    except Exception as e:
        return make_response(dumps(dict(status=e.message)), 202)

    return make_response(dumps(dict(status="ok")), 202)
# ---
def nanprod(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanprod, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def run(self):
        self.call_count += 1
        print(f"Running {self.fn_id}, call count: {self.call_count}")
        if self.call_count < self.preempt_until_n_calls:
            raise TimeoutError("Simulated preemption via TimeoutError")

        return np.zeros(1)
# ---
def get_team_code(full_name):
    for code, name in team_mapping.items():
        if name == full_name:
            return code
    return full_name
# ---
def test_find_path_multi_step():
    source = "a = 1\nb = 2\nc = 3\n"
    target = "a = 10\nb = 20\nc = 30\n"
    path = find_path(source, target)
    assert len(path) >= 1

    current = source
    for mutation in path:
        current = mutation.apply(current)
    ast.parse(current)
# ---
def tearDown(self):
        super(TestMisc, self).tearDown()
        self.dynamo.default_return_capacity = False
# ---
def test_edit_experiment_view(self):
        """ Tests edit_experiment template renders when authenticated """
        experiment = self.create_test_experiment()
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(experiment.id,)))
        self.assertTemplateUsed(response, "ab_tool/edit_experiment.html")
# ---
def __add_fuzzing_range(self):
        start = self.ui.sBAddRangeStart.value()
        end = self.ui.sBAddRangeEnd.value()
        step = self.ui.sBAddRangeStep.value()
        self.fuzz_table_model.add_range(start, end + 1, step)
# ---
def torch_loss(model, input_ids) -> torch.Tensor:
            return model(input_ids, labels=input_ids)[0]
# ---
def is_being_preempted(self) -> bool:
        return get_current_tpu_is_preempted()
# ---
def an_empty_ifc_project():
    bpy.ops.bim.create_project()
# ---
def terminate(self) -> None:
        self._job.terminate()
# ---
def test_connection_available_returns_false_on_os_error():
    """connection_available returns False on OSError."""
    conn = MagicMock()
    conn.run.side_effect = OSError("Connection refused")
    assert connection_available(conn) is False
# ---
def test_df_input(self):
        def test_impl(df):
            return df.B.sum()

        n = 121
        df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))
# ---
def chip_count(self) -> int:
        """Total number of GPU chips."""
        return self.count
# ---
def __init__(self, repeat: MinimumOfRepetition):
        self.repeat = repeat
        self.max = []
# ---
def get_cumulative_difficulty(self):
        with self.lock:
            last_block_metadata = self._state.get_block_metadata(self._last_block.headerhash)
            return last_block_metadata.cumulative_difficulty
# ---
def update(self: S, new_model: M, step: int) -> S:
        del step
        # 1 - beta because increment_update expects the weight of the new model
        return dataclasses.replace(self, model=optax.incremental_update(new_model, self.model, 1 - self.beta))
# ---
def inner(x):
        # x is a BatchTracer with batch_dim=0, aval shape (hidden,)
        na = hax.NamedArray(x, (Hidden,))
        return hax.auto_sharded(na)
# ---
def test_is_success(self):
        self.assertFalse(status.is_success(199))
        self.assertFalse(status.is_success(300))

        for i in range(200, 299):
            self.assertTrue(status.is_success(i))
# ---
def _write(path: Path, records: list[dict]) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        with gzip.open(path, "wt", encoding="utf-8") as handle:
            for record in records:
                handle.write(json.dumps(record))
                handle.write("\n")
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "ToyLmHeadModel":
        del key
        if new_size != self.Vocab.size:
            raise NotImplementedError("ToyLmHeadModel.resize_vocab only supports a no-op resize.")
        return self
# ---
def step_impl(context, username, field, value):
    user = context.user_service.exists(username)

    if user is not None:
        user[field] = value
        context.user_service.update(user.to_json())
    else:
        raise UserNotFound(username, "User was not found")
# ---
def year_field_is_required_present(self):
        """
        :rtype: bool
        """
        return self._is_element_present(AddMoviePageLocators.YEAR_INPUT_ERROR_LOCATOR)
# ---
def count(self) -> int:
        return self._count
# ---
def __init__(self, content=None):
        ''' Constructor for deploymentconfig '''
        if not content:
            content = DeploymentConfig.default_deployment_config

        super(DeploymentConfig, self).__init__(content=content)
# ---
def __init__(self, value):
        self.value = value
# ---
def norm_config(self) -> LayerNormConfigBase:
        return RmsNormConfig(
            use_weight=self.use_layer_norm_weight,
            use_bias=self.use_bias,
            eps=self.layer_norm_epsilon,
        )
# ---
def corofunc():
            called[0] = True
# ---
def __init__(self, docs: List[List[T]]):
        self.docs = docs
# ---
def set_item_default(item_code, company, fieldname, value):
	item = frappe.get_cached_doc('Item', item_code)

	for d in item.item_defaults:
		if d.company == company:
			if not d.get(fieldname):
				frappe.db.set_value(d.doctype, d.name, fieldname, value)
			return

	# no row found, add a new row for the company
	d = item.append('item_defaults', {fieldname: value, "company": company})
	d.db_insert()
	item.clear_cache()
# ---
def setUp(self):
        self.oFile = vhdlFile.vhdlFile(lFile)
        self.assertIsNone(eError)
        self.oFile.set_indent_map(dIndentMap)
# ---
def __init__(self, difficulty: str = "medium"):
        pass
# ---
def add_cookies(self, clist):
        for c in clist:
            name = c.split("\t")[5]
            self.cookies[name] = c
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        pass
# ---
def filter_stackexchange(config: FilterStackExchangeConfig):
    """Filter StackExchange data by vote threshold and remove duplicates."""
    pipeline = (
        Dataset.from_files(f"{config.input_path}/*.jsonl.gz")
        .flat_map(lambda path: _process_file_with_filtering(path, config))
        .write_jsonl(f"{config.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )

    Backend.execute(pipeline)
# ---
def test_register_route(self):
        # Ensure about route behaves correctly.
        response = self.client.get("/register", follow_redirects=True)
        self.assertIn(b"<h1>Register</h1>\n", response.data)
# ---
def testPrintFunction(self):
    want = "abc\n123\nabc 123\nabcx123\nabc 123 "
    self.assertEqual((0, want), _GrumpRun(textwrap.dedent("""\
        "module docstring is ok to proceed __future__"
        from __future__ import print_function
        print('abc')
        print(123)
        print('abc', 123)
        print('abc', 123, sep='x')
        print('abc', 123, end=' ')""")))
# ---
def test_list_migrations_in_flavor_resize_situation(self):
        """Admin can get the migrations list containing the resized server"""
        server = self.create_test_server(wait_until="ACTIVE")
        server_id = server['id']

        self.resize_server(server_id, self.flavor_ref_alt)

        body = self.client.list_migrations()['migrations']

        instance_uuids = [x['instance_uuid'] for x in body]
        self.assertIn(server_id, instance_uuids)
# ---
def __init_options(self):
        options_file = '/etc/func/modules/'+self.__class__.__name__+'.conf'
        self.options = read_config(options_file, self.Config)
        return
# ---
def test_capturing_done_simple(self):
        with self.getcapture() as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            out, err = cap.readouterr()
        assert out == "hello"
        assert err == "world"
# ---
def _get_one_lb_info(self, line_all, line_index, line_total):
        value = []

        for i in range(line_index, line_total):
            line = line_all[i]

            if line.startswith('\t'):
                value.append(line)
            elif line.startswith('listen'):
                return i, value

        return line_total - 1, value
# ---
def _is_scalar(v) -> bool:
    return isinstance(v, numbers.Number) or (isinstance(v, np.ndarray | jax.Array) and v.ndim == 0)
# ---
def build_eval_harness_config(self) -> LmEvalHarnessConfig:
        if self.eval_harness_tasks is None:
            return None
        return LmEvalHarnessConfig(task_spec=convert_to_levanter_task_config(self.eval_harness_tasks))
# ---
def __init__(self, **kwargs):
        env = Environment(loader=PackageLoader('hotzenplotz.worker','templates'))
        self.template =  env.get_template('cron')
        self.dir_path = None
# ---
def default_get(self, cr, uid, fields_list, context=None):
        # merge defaults from stock.picking with possible defaults defined on stock.picking.out
        defaults = self.pool['stock.picking'].default_get(cr, uid, fields_list, context=context)
        out_defaults = super(stock_picking_out, self).default_get(cr, uid, fields_list, context=context)
        defaults.update(out_defaults)
        return defaults
# ---
def activation_genetic_modification_2(testapp, lab, award):
    return{
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'activation',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def unstacked(self) -> Sequence[M]:
        """
        Returns the unstacked version of this module. This is useful for logging or saving checkpoints.

        """
        ...
# ---
def __init__(self, dataset):
        self._dataset = dataset
# ---
def _compare_eqx_and_haliax(hax_mod: eqx.Module, eqx_mod: eqx.Module):
    def f(x: NamedArray, *args, **kwargs):
        unnamed_x = x.array
        hax_out = hax_mod(x, *args, **kwargs)  # type: ignore
        eqx_out = eqx_mod(unnamed_x, *args, **kwargs)  # type: ignore

        assert jnp.allclose(hax_out.array, eqx_out)
        return hax_out

    return f
# ---
def genetic_modification_source(testapp, lab, award, source, gene):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'introduced_gene': gene['@id'],
        'purpose': 'expression',
        'method': 'CRISPR',
        'reagents': [
            {
                'source': source['@id'],
                'identifier': 'sigma:ABC123'
            }
        ]
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def reset(self):
        self.flat_map_count = 0
        self.map_count = 0
        self.processed_ids = []
# ---
def main():
    """Script entry point."""
    if os.name != 'nt':
        return 1
    return print_banner(**vars(parse()))
# ---
def _compute_block_assignment(base_ids, index, key):
    rng = jax.random.fold_in(key, index)
    permuted_ids = jax.random.permutation(rng, base_ids)
    return permuted_ids
# ---
def __init__(self):
        self.priority = 1
        self.language = ['en']
        self.domains = ['coolmoviezone.online']
        self.base_link = 'https://coolmoviezone.online'
        self.scraper = cfscrape.create_scraper()
# ---
def key(item):
            field_val = item.get(self.field, '')
            if self.case_insensitive and isinstance(field_val, str):
                field_val = field_val.lower()
            return field_val
# ---
def scheduler():
    """
    A pytest fixture that sets up a BatchScheduler with the following schedule:
      - Use batch size 32 until step 1000
      - Then batch size 64 until step 100000
      - Then batch size 128 forever
    """
    schedule = [
        ScheduleStep(start=0, value=32),
        ScheduleStep(start=1000, value=64),
        ScheduleStep(start=100000, value=128),
    ]
    return BatchSchedule(schedule)
# ---
def test_rate_limiter_throttles():
    """RateLimiter prevents running too frequently."""
    limiter = RateLimiter(interval_seconds=0.2)
    assert limiter.should_run()
    assert not limiter.should_run()
    time.sleep(0.25)
    assert limiter.should_run()
# ---
def __init__(self, examples):
        self.examples = examples
# ---
def get_timestamps(self, backend, dn):
        return ObjectBackendRegistry.backends[backend].get_timestamps(dn)
# ---
def structure(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> Any:
    """Alias for :func:`haliax.tree_util.tree_structure` matching :func:`jax.tree.structure`."""

    return tree_util.tree_structure(tree, is_leaf=is_leaf)
# ---
def num_queued_tokens(self) -> jax.Array:
        """Expose current queued token count from ``TokenQueue``."""
        return self.tqueue.num_queued_tokens
# ---
def _get_or_create_queue(queue_name: str, maxlen: int | None = None) -> "InMemoryRolloutQueue":
    """Get or create a named in-memory queue."""
    if queue_name not in _MEMORY_QUEUES:
        _MEMORY_QUEUES[queue_name] = InMemoryRolloutQueue(maxlen=maxlen)
    return _MEMORY_QUEUES[queue_name]
# ---
def set_image_metadata(self, image_id, meta):
        """Sets the metadata for an image."""
        post_body = json.dumps({'metadata': meta})
        resp, body = self.put('images/%s/metadata' % str(image_id), post_body)
        body = json.loads(body)
        self.validate_response(schema.image_metadata, resp, body)
        return service_client.ResponseBody(resp, body['metadata'])
# ---
def global_stats(shard_stats):
        stats_list = list(shard_stats)
        return {
            "sum": sum(s["sum"] for s in stats_list),
            "count": sum(s["count"] for s in stats_list),
            "min": min(s["min"] for s in stats_list),
            "max": max(s["max"] for s in stats_list),
        }
# ---
def get_field_from_jbor(thing):
    '''
    :returns: Output field name from a JBOR

    Assumes :func:`is_job_ref` evaluates to True
    '''
    if '$dnanexus_link' in thing:
        return thing['$dnanexus_link']['field']
    else:
        return thing['field']
# ---
def _all_input_axes(arrays):
    return ensure_tuple(functools.reduce(union_axes, (a.axes for a in arrays), ()))
# ---
def i_add_a_cube_of_size_size_at_location(size, location):
    bpy.ops.mesh.primitive_cube_add(size=float(size), location=[float(co) for co in location.split(",")])
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if self._prefix and dir in self._roots:
            return "all"
        return (
            dir in self._roots
            or dir in self._dirs
            or any(parentdir in self._roots for parentdir in util.finddirs(dir))
        )
# ---
def test_run_inference_raises_for_empty_glob(tmp_path):
    config = InferenceConfig(
        input_path=str(tmp_path),
        output_path=str(tmp_path / "out"),
        model_name="compression",
        model_type="compression",
        attribute_name="test",
    )

    with pytest.raises(FileNotFoundError):
        run_inference(config)
# ---
def get_device_type(device: cluster_pb2.DeviceConfig) -> str:
    """Extract device type from config."""
    if device.HasField("cpu"):
        return "cpu"
    elif device.HasField("gpu"):
        return "gpu"
    elif device.HasField("tpu"):
        return "tpu"
    return "cpu"
# ---
def _render_path_elem(x):
    match x:
        case jtu.DictKey(key):
            return f"{key}"
        case jtu.GetAttrKey(key):
            return f"{key}"
        case jtu.SequenceKey(i):
            return f"{i}"
        case jtu.FlattenedIndexKey(i):
            return f"{i}"
        case _:
            return str(x)
# ---
def __repr__(self):
        return f"{self.__class__.__name__}()"
# ---
def Caller_Places_Call (self, Number):
        self.Step (Message = "Caller places call to " + str (Number) + "...")

        self.Log (Message = "Dialling through caller agent...")
        self.Caller.dial (Number)
# ---
def _dashboard(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Iris Worker", "/static/worker/app.js"))
# ---
def fit_baseline_amp(self,z_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		'''
		return self._baseline_als(np.absolute(z_data),lam,p,niter=niter)
# ---
def convert_to_export(self, value, env):
        if not value:
            return ''
        return self.from_string(value) if env.context.get('export_raw_data') else ustr(value)
# ---
def auth(self):
        return self._auth
# ---
def __init__(self, dataset: SyncDataset[T_co]):
        super().__init__()
        self.dataset = dataset
# ---
def rights(self):
        del self._rights
# ---
def __contains__(self, key):
        """The 'in' operator will return true or false depending on whether
        'key' is an array in the dataset or not.
        """
        return key in self._variables
# ---
def test_spatial_dropout_2d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout2D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4, 5))

    testing_utils.layer_test(
        keras.layers.SpatialDropout2D,
        kwargs={'rate': 0.5, 'data_format': 'channels_first'},
        input_shape=(2, 3, 4, 5))
# ---
def __getitem__(self, item):
		if isinstance(item, slice):
			keys = sorted(self._addresses.keys())[item]
			return [self[k] for k in keys]

		if item not in self._values:
			self._parse_row(item)

		return self._values[item]
# ---
def __call__(self, L):
        """Get positional embeddings for the first L positions.

        Args:
            L: Length to get embeddings for

        Returns:
            Tuple of (z, t) embeddings limited to length L
        """
        if L > self.PosPerBlock.size:
            raise ValueError(f"Requested length {L} > max size {self.PosPerBlock.size}")

        return self.z.slice(self.PosPerBlock, length=L), self.t.slice(self.PosPerBlock, length=L)
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' and '.join(str(r) for r in self.rules)
# ---
def validation_sets(self) -> Mapping[str, AsyncDataset[np.ndarray]]:
        pass
# ---
def get_stack(self, start_index, stack_size):
        end_index = len(self.examplers) - stack_size
        if end_index < 0:
            stack = list(self.examplers) + [self.examplers[-1] for _ in range(-end_index)]
        else:
            start_index = min(start_index, end_index)
            stack = [self.examplers[i + start_index] for i in range(stack_size)]
        return np.stack(stack, axis=-1)
# ---
def get_vm(self, vm_id: str) -> vm_pb2.VmInfo | None:
        """Get VM info by ID."""
        vm = self._vm_registry.get_vm(vm_id)
        return vm.info if vm else None
# ---
def test_bank_has_return(bank):
    assert bank.has_type("Return"), "Should extract Return statements"
# ---
def processing_func(input_file_path):
        attr_file_paths = (
            [
                rebase_file_path(config.input_doc_path, input_file_path, input_attr_path)
                for input_attr_path in config.input_attr_paths
            ]
            if config.input_attr_paths is not None
            else []
        )
        return create_dataset_shard(input_file_path, config, attr_file_paths)
# ---
def update(self, rsc, **kw):
        for k, v in kw.items():
            setattr(rsc, k, v)
        return rsc.save()
# ---
def _quick_task_job(task_id: int):
    """Quick job that sleeps and returns."""
    import time as time_module

    time_module.sleep(2.0)
    print(f"Task {task_id} completed")
    return task_id
# ---
def newImage(self, row=0, col=0, filename=""):
    print("%d.%d > %s" % (row, col, filename))
# ---
def __init__(self, driver):
        """Creates a new ActionChains.
        Args:
            driver: The WebDriver instance which performs user actions.
        """
        self._driver = driver
        self._actions = []
# ---
def do_get_name(self, event):
        self.name = configurator.get_team_name(self.number)
# ---
def from_ms(cls, timeout_ms: int) -> "Deadline":
        """Create deadline from milliseconds in the future."""
        return cls(time.monotonic() + timeout_ms / 1000.0)
# ---
def secureheaders():
    headers = cherrypy.response.headers
    headers['X-Frame-Options'] = 'DENY'
    headers['X-XSS-Protection'] = '1; mode=block'
    headers['Content-Security-Policy'] = "default-src='self'"
# ---
def metadata(self) -> dict[str, Any]:
        return {
            "tokenizer": self.tokenizer.name_or_path,
            "vocab_size": len(self.tokenizer),
            "return_attention_mask": self.return_attention_mask,
            "padding": self.padding,
            "max_length": self.max_length,
            "append_bos": self._need_to_add_bos,
            "append_eos": self._need_to_add_eos,
        }
# ---
def test_filter_expression_logical_and(backend):
    """Test filter with logical AND expression."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"a": 1, "b": 2},
            {"a": -1, "b": 3},
            {"a": 2, "b": -1},
            {"a": -1, "b": -1},
        ]
    ).filter((col("a") > 0) & (col("b") > 0))

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == {"a": 1, "b": 2}
# ---
def list_jobs(self, request: cluster__pb2.Controller.ListJobsRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListJobsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def test_missing_logprobs_raises(inference_ctx):
    """Test that missing logprobs raises ValueError."""
    choice = Choice(
        finish_reason="stop",
        index=0,
        message=ChatCompletionMessage(role="assistant", content="test"),
        logprobs=None,
    )

    with pytest.raises(ValueError, match="missing logprobs"):
        inference_ctx.response_tokens_from_choice(choice)

    with pytest.raises(ValueError, match="missing logprobs"):
        inference_ctx.logprobs_from_choice(choice)
# ---
def comma_cooldown_mixture(*, tokenizer: str = llama3_tokenizer, permutation_type="feistel"):
    """LmMixtureDatasetConfig for the cooldown stage."""
    tokenized = common_pile_tokenized(tokenizer=tokenizer)
    components = {f"common_pile/{dataset}": tokenized[f"common_pile/{dataset}"] for dataset in COMMON_PILE_DATASETS}
    return lm_mixture_data_config(
        components=components,
        weights=COMMA_COOLDOWN_MIXTURE_WEIGHTS,
        permutation_type=permutation_type,
    )
# ---
def find_project_root(file_path, marker_files):
    """Walk up from file_path looking for project root markers."""
    current = os.path.dirname(os.path.abspath(file_path))
    for _ in range(10):  # Max 10 levels up
        for marker in marker_files:
            if os.path.exists(os.path.join(current, marker)):
                return current
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent
    return None
# ---
def render_short_timestamp(timestamp):
    return str(datetime.datetime.fromtimestamp(timestamp//1000))
# ---
def __init__(self, value: int):
        self.value = value
        self.call_count = 0
# ---
def rearrange(self, *args, **kwargs) -> "NamedArray":  # pragma: no cover
        """See [haliax.rearrange][] for details."""
        return haliax.rearrange(self, *args, **kwargs)
# ---
def flatten(array: NamedArray, new_axis_name: AxisSelector) -> NamedArray:
    """
    Returns a flattened view of the array, with all axes merged into one. Aliax for [haliax.ravel][]
    """
    return ravel(array, new_axis_name)
# ---
def encode_text(text: str):
    tokens = current_tokenizer.encode(text)
    for token in tokens:
        console.print(f"{token} ", end="")
    console.print()
# ---
def test_count_basic(backend):
    """Test basic count operation."""
    ds = Dataset.from_list(range(100)).count()
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == 100
# ---
def scan_via(
        self, fn: Callable[..., tuple[CarryT, OutputT_co]], *, unroll: int | bool | None = None
    ) -> Callable[P, tuple[CarryT, OutputT_co]]: ...
# ---
def __init__(self, dict_):
        super(self.__class__, self).__init__(dict_)
# ---
def zeros_like(x: Arrayish) -> "RunningMean":
        return RunningMean(x * 0.0, x * 0.0)
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "google/gemma-3-1b-pt"
    ) -> HFCheckpointConverter["Gemma3Config"]:  # type: ignore
        return HFCheckpointConverter(
            self,
            reference_checkpoint=ref_checkpoint,
            trust_remote_code=True,
            HfConfigClass=HfGemma3Config,
        )
# ---
def testEmpty(self):
        gen = VectorInput([])
        tcToTotal = sTCToTotal()
        p = Pool()

        gen.data >> tcToTotal.envelope
        tcToTotal.TCToTotal >> (p, 'lowlevel.tctototal')

        run(gen)

        self.assertRaises(KeyError, lambda: p['lowlevel.tctototal'])
# ---
def go(t):
    o = option.Option(**{'handle': t, 'type': t})
    o.validate()
    return o
# ---
def test_compute_ray_retry_count(max_failure, max_preemption, expected):
    from fray.v2.ray_backend.backend import compute_ray_retry_count

    request = JobRequest(
        name="test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        max_retries_failure=max_failure,
        max_retries_preemption=max_preemption,
    )
    assert compute_ray_retry_count(request) == expected
# ---
def testLowFrequency(self):
        hpcp = HPCP(minFrequency=100, maxFrequency=1000)([99], [1])
        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
# ---
def __init__(self, logits):
        self._logits = logits
# ---
def keys(self):
        """ @rtype: StringList """
        return SummaryKeyMatcher.cNamespace().keys(self)
# ---
def controller_url(self) -> str:
        if self._remote_url:
            return self._remote_url
        if self._manager:
            url = self._manager.controller.discover()
            if url:
                return url
        raise RuntimeError("No controller URL available. Call __enter__ first.")
# ---
def load(self):
        pass
# ---
def wait(self):
        if self._thread is not None and self._thread.is_alive():
            self._thread.join()
# ---
def test_status(self):
        eq_(self.record.status, [])
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        self.last_timeout = timeout
        return self._run_result
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        visit1 = self._m1.visitdir(dir)
        if visit1 == "all":
            return self._m2.visitdir(dir)
        # bool() because visit1=True + visit2='all' should not be 'all'
        return bool(visit1 and self._m2.visitdir(dir))
# ---
def job_request():
    return cluster_pb2.Controller.LaunchJobRequest(
        name=JobName.root("test-job").to_wire(),
        entrypoint=_make_test_entrypoint(),
        resources=cluster_pb2.ResourceSpecProto(cpu=2, memory_bytes=4 * 1024**3),
        environment=cluster_pb2.EnvironmentConfig(),
        replicas=1,
    )
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> ActorHandle:
        group = self.create_actor_group(actor_class, *args, name=name, count=1, resources=resources, **kwargs)
        return group.wait_ready()[0]
# ---
def func():
            pass
# ---
def main():
    sol = Solution()
    print(sol.threeSumClosest([-111, -111, 3, 6, 7, 16, 17, 18, 19], 13))
    return 0
# ---
def test_rolling2(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            df['moving average'] = df.A.rolling(window=5, center=True).mean()
            return df['moving average'].sum()

        hpat_func = self.jit(test_impl)
        n = 121
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def fsspec_isdir(dir_path):
    """
    Check if a path is a directory in fsspec filesystem.
    """
    fs, _ = fsspec.core.url_to_fs(dir_path)
    return fs.isdir(dir_path)
# ---
def __eq__(self, other): return True
# ---
def decode_sequence(tokenizer, tensor: Sequence[int]) -> str:
    return tokenizer.decode(list(tensor), skip_special_tokens=False)
# ---
def test_impl(df):
            return df.B.sum()
# ---
def _reset_CC(self):
        """reset CC back to defaults"""
        self.CC.p_h = default_physiological_p_h
        self.CC.p_mg = default_physiological_p_mg
        self.CC.temperature = default_physiological_temperature
        self.CC.ionic_strength = default_physiological_ionic_strength
# ---
def __init__(self, **kwds):
#        LOG.debug("extensions constructor: %s" % kwds)
        super(Extensions, self).__init__(**kwds)
# ---
def loss_default(x_raw, w_raw):
        return fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
            x_raw,
            y,
            w_raw,
            reduction="mean",
            logsumexp_weight=logsumexp_weight,
            block_sizes=block_sizes,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
        )
# ---
def test_bound_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).offset(bindparam("o")),
            [(3, 3, 4), (4, 4, 5)],
            params={"o": 2},
        )
# ---
def test_synthetic_subtrees_have_correct_types(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=10)
    for entry in entries:
        assert entry.node_type in ("Return", "If", "For", "Assign", "BinOp", "Call", "Compare", "UnaryOp")
# ---
def __init__(
        self,
        config: WeightTransferConfig,
        axis_mapping: ResourceMapping | None = None,
        mesh: Mesh | None = None,
    ):
        self.config = config
        self.checkpoint_queue = deque()
        self.axis_mapping = axis_mapping
        self.mesh = mesh
        self.metrics = WeightTransferServerMetrics()
# ---
def __repr__(self):
        return ("<{0.__class__.__name__}"
                " id={0.entity_id!r}"
                " type={0.entity_type!r}"
                " ident={0.ident!r}>").format(self)
# ---
def _get_policy_uuid(conn, policy_name):
    policy_query = sa.sql.select([policy_table.c.uuid]).where(
        policy_table.c.name == policy_name
    )

    for policy in conn.execute(policy_query).fetchall():
        return policy[0]
# ---
def __init__(self, field, ascending=True, case_insensitive=True):
        self.field = field
        self.ascending = ascending
        self.case_insensitive = case_insensitive
# ---
def test_stdfd_functional(self, testdir):
        reprec = testdir.inline_runsource("""
            def test_hello(capfd):
                import os
                os.write(1, "42".encode('ascii'))
                out, err = capfd.readouterr()
                assert out.startswith("42")
                capfd.close()
        """)
        reprec.assertoutcome(passed=1)
# ---
def testInt32Random(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.int32)
      y = self._randMatrix(k, m, np.int32)
      self._testCpuMatmul(x, y)
# ---
def quit_watch_later(self, code=None):
        self.command('quit_watch_later', code)
# ---
def _apply_logit_soft_cap(logits: jax.Array, logit_soft_cap: Optional[float]) -> jax.Array:
    if logit_soft_cap is None:
        return logits
    return jnp.tanh(logits / logit_soft_cap) * logit_soft_cap
# ---
def __init__(self, comodel_name=None, inverse_name=None, string=None, **kwargs):
        super(One2many, self).__init__(
            comodel_name=comodel_name,
            inverse_name=inverse_name,
            string=string,
            **kwargs
        )
# ---
def test_brackets_unbalanced():
    assert not brackets_balanced("(a + b")
    assert not brackets_balanced("a + b)")
    assert not brackets_balanced("[1, 2)")
# ---
def _deshape(x):
        if is_jax_array_like(x) and x.shape != ():
            return x.reshape([length, *x.shape[2:]])
        else:
            return x
# ---
def test_do_execute_no_params_wo_replace(self):
        self._test_do_execute_no_params(False)
# ---
def get_block_depth(cls, block_structure, block_key):
        """
        Return the precalculated depth of a block within the block_structure:

        Arguments:
            block_structure: a BlockStructure instance
            block_key: the key of the block whose depth we want to know

        Returns:
            int
        """
        return block_structure.get_transformer_block_field(
            block_key,
            cls,
            cls.BLOCK_DEPTH,
        )
# ---
def _write(filename, contents):
        ''' Actually write the file contents to disk. This helps with mocking. '''

        tmp_filename = filename + '.yedit'

        with open(tmp_filename, 'w') as yfd:
            yfd.write(contents)

        os.rename(tmp_filename, filename)
# ---
def is_big_name(item):
            return len(item) > 4
# ---
def n_conceptos(self):
        return self.__n_conceptos
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df['B'] = df.A.map(lambda a: 2 * a)
            return df.B.sum()
# ---
def long_job(s):
            s.wait()
# ---
def get_mac():
    """
    Gets a random mac address.
    """
    mac = ("%02x:%02x:%02x:%02x:%02x:%02x" %
                    (random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff)))
    return mac
# ---
def vpnservice_updated(self, context, **kwargs):
        """Vpnservice updated rpc handler

        VPN Service Driver will call this method
        when vpnservices updated.
        Then this method start sync with server.
        """
        router = kwargs.get('router', None)
        self.sync(context, [router] if router else [])
# ---
def convert_td(self, el, text, convert_as_inline):
        if convert_as_inline:
            return text + " "
        colspan = 1
        if "colspan" in el.attrs:
            colspan = _try_convert_int(el["colspan"], 1)

        return " " + text.strip().replace("\n", " ") + " |" * colspan
# ---
def output_paragraph(self):
        return self.renderer.paragraph(self.inline(self.token['text']))
# ---
def sources_add(self):
        data = bottle.request.body.getvalue().decode('utf-8')
        node, spec = make_spec(data, self.config)
        self.config['sources'].append(node)
        self.ndb.connect_source(node, spec)
# ---
def __init__(self, pool: ActorPool[T]):
        self._pool = pool
# ---
def validate_conversion_factor(self):
		check_list = []
		for d in self.get('uoms'):
			if cstr(d.uom) in check_list:
				frappe.throw(
					_("Unit of Measure {0} has been entered more than once in Conversion Factor Table").format(d.uom))
			else:
				check_list.append(cstr(d.uom))

			if d.uom and cstr(d.uom) == cstr(self.stock_uom) and flt(d.conversion_factor) != 1:
				frappe.throw(
					_("Conversion factor for default Unit of Measure must be 1 in row {0}").format(d.idx))
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)
        return ScaleByMuonHState(momentum_buffer=momentum_buffer)
# ---
def foo2(x):
            return hax.shard(x, resource_map)
# ---
def test_shutdown_prevents_new_submissions(self, local_client):
        """After shutdown, submit() raises RuntimeError."""
        config = WorkerPoolConfig(
            num_workers=1,
            resources=ResourceSpec(cpu=1, memory="512m"),
        )

        pool = WorkerPool(local_client, config, timeout=30.0)
        pool.__enter__()

        pool.shutdown(wait=False)

        with pytest.raises(RuntimeError, match="shutdown"):
            pool.submit(lambda: 42)
# ---
def serve(self):
        try:
            logger.info(f"Starting Levanter inference server on {self.config.host}:{self.config.port}")
            self._server = uvicorn.Server(uvicorn.Config(self.app, host=self.config.host, port=self.config.port))
            self._server.run()
        finally:
            self.shutdown()
# ---
def decode_step_callback(iteration: int):
            """Called at each decode iteration in the engine."""
            # Use the iteration number as the step for profiling
            saved_step = self._current_step
            self._current_step = iteration
            self._handle_profiler_step()
            self._current_step = saved_step
# ---
def on_fuzzing_range_start_changed(self, value: int):
        self.ui.sBAddRangeEnd.setMinimum(value)
        self.ui.sBAddRangeStep.setMaximum(self.ui.sBAddRangeEnd.value() - value)
# ---
def entry(self,codeBibtex):
        return codeBibtex[self.label]
# ---
def test_getitem_bool_series(self):
        def test_impl(df):
            return df['A'][df['B']].values

        hpat_func = self.jit(test_impl)
        df = pd.DataFrame({'A': [1, 2, 3], 'B': [True, False, True]})
        np.testing.assert_array_equal(test_impl(df), hpat_func(df))
# ---
def test_registry_unregister_removes_vm(registry: VmRegistry):
    """Unregistering a VM removes it from the registry."""
    vm = MagicMock()
    vm.info = vm_pb2.VmInfo(vm_id="test-vm-001")

    registry.register(vm)
    registry.unregister("test-vm-001")

    assert registry.vm_count() == 0
    assert registry.get_vm("test-vm-001") is None
# ---
def _remove_endpoints_for_task(self, task_id: JobName) -> list[ControllerEndpoint]:
        """Remove all endpoints associated with a task."""
        endpoint_ids = list(self._endpoints_by_task.get(task_id, []))
        removed = []
        for eid in endpoint_ids:
            endpoint = self._endpoints.pop(eid, None)
            if endpoint:
                removed.append(endpoint)
        self._endpoints_by_task.pop(task_id, None)
        return removed
# ---
def test_get_task_not_found(client):
    """Test GetTaskStatus RPC with nonexistent task returns error."""
    response = rpc_post(
        client,
        "GetTaskStatus",
        {"taskId": JobName.root("nonexistent").task(0).to_wire()},
    )
    assert response.status_code != 200
# ---
def resolve_axis(self, axis: AxisSelector) -> Axis: ...
# ---
def save(self, commit=False):
        usuario = super(UsuarioForm, self).save(commit)

        # Cria User
        u = User.objects.create(username=usuario.username, email=usuario.email)
        u.set_password(self.cleaned_data['password'])
        u.is_active = True
        u.groups.add(get_or_create_grupo(self.cleaned_data['tipo'].descricao))

        u.save()
        usuario.user = u
        usuario.save()
        return usuario
# ---
def dummy(*args, **kwargs):
            pass
# ---
def __init__(self, path, json_data):
        if path is None:
            path = 'Untitled.ipynb'
        if json_data is None:
            json_data = json.dumps({
                'cells': [],
                'metadata': {'kernelspec': {'name': 'python3'}}})
        self.path = path
        self.json = json.loads(json_data)
        # In cached instances, current_job is already defined.
        if not hasattr(self, 'current_job'):
            self.current_job = None
# ---
def health_check(self, request: actor__pb2.Empty, ctx: RequestContext) -> actor__pb2.HealthResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def start(self):
        self.init()
# ---
def chip_count(self) -> int:
        """Total accelerator chips across all replicas/slices."""
        return self.device.chip_count() * self.replicas
# ---
def test_sniff_format_for_parquet():

    import pyarrow as pa
    import pyarrow.parquet as pq

    with tempfile.NamedTemporaryFile(suffix=".parquet") as f:
        table = pa.table({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})
        pq.write_table(table, f.name)
        f.flush()

        assert _sniff_format_for_dataset(f.name) == ".parquet"
# ---
def fsspec_size(file_path: str) -> int:
    """Get file size (in bytes) of a file on an `fsspec` filesystem."""
    fs = fsspec.core.url_to_fs(file_path)[0]

    return fs.size(file_path)
# ---
def __init__(
        self,
        raw_fn: ComputeLossFunction,
        mp: jmp.Policy,
        compute_axis_mapping: ResourceMapping,
    ):
        """
        Args:
            raw_fn: The underlying loss function
            mp: Mixed precision policy for casting
            compute_axis_mapping: Axis mapping for compute
        """
        self._raw_fn = raw_fn
        self._mp = mp
        self._compute_axis_mapping = compute_axis_mapping
# ---
def is_finite(self) -> bool:
        return all(dataset.is_finite() for dataset, _ in self.datasets)
# ---
def email(self):
        """
        Shortcut property for finding the e-mail address or bot URL.
        """
        if "profile" in self._raw:
            email = self._raw["profile"].get("email")
        elif "bot_url" in self._raw:
            email = self._raw["bot_url"]
        else:
            email = None
        if not email:
            logging.debug("No email found for %s", self._raw.get("name"))
        return email
# ---
def __init__(self, l):
                self.l = l
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "WhisperModel":
        new_decoder = self.decoder.resize_vocab(new_size, key)
        return dataclasses.replace(self, decoder=new_decoder)
# ---
def testInt32Basic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.int32)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.int32)
    self._testCpuMatmul(x, y)
# ---
def __setitem__(self, name, value, file_local=False):
        """ Get an option value """
        prefix = 'file-local-options/' if file_local else 'options/'
        return self._set_property(prefix+name, value)
# ---
def __init__(self, host: str, port: int, queue_name: str):
        self.host = host
        self.port = port
        self.queue_name = queue_name
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
            raise RuntimeError("This should not be called")
# ---
def stop(self) -> None:
        if self.server:
            self.server.shutdown()
        if self.thread:
            self.thread.join(timeout=5)
# ---
def details_template_name(self):
        return 'include/python_job_details.html'
# ---
def from_list(items: list[T]) -> Dataset[T]:
        """Create a dataset from a list."""
        return Dataset(items)
# ---
def _canonicalize_batch(batch: Union[dict, list[dict]]) -> list[dict]:
    if isinstance(batch, pa.RecordBatch):
        batch = dict_from_record_batch(batch)

    if isinstance(batch, dict):
        return _to_list_of_dicts(batch)
    else:
        return batch
# ---
def upload_face_image(self, img):
        raise NotImplementedError
# ---
def cosh(a: A) -> A:
    return wrap_elemwise_unary(jnp.cosh, a)
# ---
def __le__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.less_equal(self, other)
# ---
def get_final_weighted_score(self):
        return self.final_weighted
# ---
def resize_embeddings(self, new_size: int, key: Optional[PRNGKeyArray] = None):
        new_weights = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, token_embeddings=new_weights)
# ---
def do_init(self, match):
        self.lc.publish('Match/Init', match.encode())
# ---
def test_xxh3_128_vector():
    # Catch un-intentional regressions
    assert hash_xxh3_128(b"hello") == 241804000618833338782870102822322583576
# ---
def before(self, other: "Timestamp") -> bool:
        """Check if this timestamp is before another."""
        return self._epoch_ms < other._epoch_ms
# ---
def __getitem__(self, key):
            if key in self:
                return super(defaultdict, self).__getitem__(key)
            else:
                return self.__factory()
# ---
def columnas():
        return ["Emisor","Fecha_CFDI","Tipo","RFC_Emisor","Folio_fiscal","Folio","Receptor",
                "RFC_Receptor", "Subtotal","IEPS","IVA","Ret IVA","Ret ISR","TC","Total"]
# ---
def option_info(self, name):
        return self._get_property('option-info/'+name)
# ---
def test_load_env_vars_invalid_key():
    """Test error on key with = sign."""
    with pytest.raises(ValueError, match="cannot contain '='"):
        load_env_vars([["KEY=VALUE"]])
# ---
def action_done(self, cr, uid, ids, context=None):
        """Changes picking state to done.

        This method is called at the end of the workflow by the activity "done".
        @return: True
        """
        self.write(cr, uid, ids, {'state': 'done', 'date_done': time.strftime('%Y-%m-%d %H:%M:%S')})
        return True
# ---
def create(self, request, *args, **kwargs):
        raise MethodNotAllowed(self.action)
# ---
def __add__(self, other: "RunningMean"):
        return self.add(other.mean, other.total)
# ---
def __len__(self):
                return len(self.l)
# ---
def debug_print(self, prefix: str = ""):

        def callback(self):
            print(f"{prefix}JitScheduler State:")
            print(f"{prefix}Queued Tokens: {self.queued_tokens}")
            print(f"{prefix}Queued Slot IDs: {self.queued_slot_ids}")
            print(f"{prefix}Num Queued Tokens: {self.num_queued_tokens}")

        jax.experimental.io_callback(callback, None, ordered=True, self=self)
# ---
def test_with_unflatten_flatten():
    Z = Axis("Z", B.size * C.size)
    assert einops_rearrange(zq, "(Q: B H) d w c -> d (Z: B c) w H", H=H).axes == (D, Z, W, H)
    # make sure the values are right too
    z_t = (
        zq.array.reshape((B.size, H.size, D.size, W.size, C.size))
        .transpose((2, 0, 4, 3, 1))
        .reshape((D.size, Z.size, W.size, H.size))
    )
    assert (einops_rearrange(zq, "(Q: B H) d w c -> d (Z: B c) w H", H=H).array == z_t).all()
# ---
def terminate(self, job_id: JobId) -> None:
        """Terminate all replicas of a job."""
        self._get_job(job_id).cleanup()
# ---
def test_check_share_in_use_incorrect_host(self):
        drv = self._driver
        mox = self.mox
        mox.StubOutWithMock(utils, 'resolve_hostname')
        utils.resolve_hostname(IgnoreArg()).AndRaise(Exception())
        mox.ReplayAll()
        share = drv._check_share_in_use('incorrect:8989', '/dir')
        mox.VerifyAll()
        if share:
            self.fail('Unexpected share detected.')
# ---
def makefile(self, mode='r', bufsize=-1):
        self._makefile_refs += 1
        return socket._fileobject(self, mode, bufsize, close=True)
# ---
def _toy_example(Batch: Axis, Pos: Axis, Vocab: Axis, *, key: PRNGKeyArray) -> LmExample:
    tokens = hax.random.randint(key, (Batch, Pos), 0, Vocab.size)
    loss_weight = hax.ones((Batch, Pos), dtype=jnp.float32).at[Pos, Pos.size - 1].set(0.0)
    return LmExample(tokens=tokens, loss_weight=loss_weight, attn_mask=AttentionMask.causal())
# ---
def test_partial_setup_failure(self, testdir):
        p = testdir.makepyfile("""
            def test_hello(capsys, missingarg):
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*test_partial_setup_failure*",
            "*1 error*",
        ])
# ---
def testFormatFilename(self):
    """Tests the _FormatFilename function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    filename_string = test_helper._FormatFilename(
        event, event_data, event_data_stream)
    self.assertEqual(filename_string, 'log/syslog.1')
# ---
def mock_open(path, mode="rb"):
            if path in files:
                return io.BytesIO(files[path])
            raise FileNotFoundError(f"File not found: {path}")
# ---
def process_latex_sqrt(text: str) -> str:
    """Convert \\sqrt{x} or \\sqrt x to sqrt(x)."""
    return replace_latex_command(text, "sqrt", lambda args: f"sqrt({args[0]})" if args else "sqrt", max_args=1)
# ---
def datasource_from_hf(id: str, *, split, **kwargs) -> ShardedDataSource[dict]:
    """
    Create a ShardedDataset from a HuggingFace dataset. Arguments are passed to load_dataset.
    """
    return WrappedHFDataSource(id, split=split, **kwargs)
# ---
def _get_current_tpu_pod_type() -> str:
    """Return the TPU pod type for the current node across Ray versions."""

    if hasattr(TPUAcceleratorManager, "_get_current_node_tpu_pod_type"):
        return TPUAcceleratorManager._get_current_node_tpu_pod_type()
    if hasattr(TPUAcceleratorManager, "get_current_node_tpu_pod_type"):
        return TPUAcceleratorManager.get_current_node_tpu_pod_type()
    raise AttributeError("TPUAcceleratorManager is missing TPU pod type helpers")
# ---
def __call__(self, x, *, key):  # pragma: no cover - unused in this test
            return x + self.array + self.static
# ---
def accept_all(self, include_rejected=False):
        '''
        Accept all keys

        :param bool include_rejected: Whether or not to accept a matched key that was formerly rejected
        '''
        self.accept('*', include_rejected=include_rejected)
# ---
def discover_vm_groups(self) -> list[FakeVmGroup]:
        """No-op for fake - returns empty list."""
        return []
# ---
def _fully_replicated_sharding(mesh):
    return hax.partitioning.sharding_for_axis((), {}, mesh)
# ---
def func_name(self):
            pass
# ---
def poke(self, context):
        logging.info(
            'Checking if the time ({0}) has come'.format(self.target_time))
        return datetime.now().time() > self.target_time
# ---
def is_wandb_available():
    try:
        import wandb
    except ImportError:
        return False
    return wandb is not None and wandb.run is not None
# ---
def __init__(self, m1, m2):
        super(differencematcher, self).__init__(m1._root, m1._cwd)
        self._m1 = m1
        self._m2 = m2
        self.bad = m1.bad
        self.traversedir = m1.traversedir
# ---
def device_port(self):
        if self._values['device_port'] is None:
            return None
        return int(self._values['device_port'])
# ---
def get_all_regions() -> list[str]:
    """Extract unique regions from TPU_ZONES_CONFIG."""
    regions = set()
    for zone in TPU_ZONES_CONFIG.keys():
        # Zone format: us-west4-a -> region: us-west4
        region = zone.rsplit("-", 1)[0]
        regions.add(region)
    return sorted(regions)
# ---
def parse_row(self, data, reclen=0):
		"""
		Assign data to a DBRow instance
		"""
		return DBRow(self, data=data, reclen=reclen)
# ---
def __init_log(self):
        log = logger.Logger()
        self.logger = log.logger
# ---
def year(self):
        return self.__year
# ---
def flatten_for_export(self: Mod) -> Mod:
        if isinstance(self.axis, hax.Axis):
            return self

        if self.weight is not None:
            weight = self.weight.flatten("__OUT")
        else:
            weight = None

        if self.bias is not None:
            bias = self.bias.flatten("__OUT")
        else:
            bias = None

        return dataclasses.replace(self, weight=weight, bias=bias, axis=hax.flatten_axes(self.axis, "__OUT"))
# ---
def get(self):
        return self.value
# ---
def test_vmap_error_for_incorrectly_specified_args():
    class Module(eqx.Module):
        # this should usually be declared static, but we're simulating a user error
        field: Axis

        def __call__(self, x):
            return x.sum(self.field)

    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)

    hax.vmap(lambda a: Module(a), Batch)(Width)
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        """Total peak FLOP/s across all GPUs."""
        return self.device_flops(dtype) * self.count
# ---
def main():
    ragged = build_config(use_gmm=False, name="ragged-dot")
    gmm = build_config(use_gmm=True, name="grouped-matmul")
    executor_main(steps=default_speedrun("pranshu_mixtral_moe_compare_ragged", ragged))
    executor_main(steps=default_speedrun("pranshu_mixtral_moe_compare_gmm", gmm))
# ---
def reset(self):
        return DecodeState.init(
            page_table=self.page_table.reset(),
            pad_token_id=self.pad_token_id,
            max_stop_seqs=self.stop_tokens.shape["stop_seq"] if self.stop_tokens is not None else 0,
            max_stop_tokens=self.stop_tokens.shape["position"] if self.stop_tokens is not None else 0,
            max_queued_tokens=self.tqueue.max_queued_tokens,
        )
# ---
def usage():
	"""usage de la ligne de commande"""
	print ("usage : " + sys.argv[0] + "-h --help -s --server someurl.com -u --user login -p --password password")
# ---
def _test_syntax(self, cfile_path):
        LOG.info('Testing the new puppet configuration file')
        cmd = "puppet parser validate %s" % cfile_path

        try:
            utils.execute(cmd)
        except exception.ProcessExecutionError as e:
            LOG.warn('Did not pass the configuration syntax test: %s', e)
            raise
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        raise NotImplementedError("CPU FLOPS not available")
# ---
def visitdir(self, dir):
        return False
# ---
def convert_to_read(self, value, use_name_get=True):
        """ convert ``value`` from the cache to a value as returned by method
            :meth:`BaseModel.read`

            :param bool use_name_get: when True, value's diplay name will
                be computed using :meth:`BaseModel.name_get`, if relevant
                for the field
        """
        return False if value is None else value
# ---
def get_fai(b):
    '''Just the internals of the FAI method'''
    return b['b2'].subtract(b['b1'].add(b['b5'].subtract(b['b1']).multiply((859.0 - 645) / (1240 - 645)))).select(['sur_refl_b02'], ['b1'])
# ---
def format_headrooms(headrooms):
    return format_line(prefix='headrooms'.rjust(RJUST), values=headrooms)
# ---
def local_tag(self) -> str:
        return f"{self.image_name}:{self.version}"
# ---
def __init__(self, surfaces):
        self.surfaces = surfaces
# ---
def modified(self, records):
        # Invalidate cache for self.inverse_fields, too. Note that recomputation
        # of fields that depend on self.inverse_fields is already covered by the
        # triggers (see above).
        spec = super(_Relational, self).modified(records)
        for invf in self.inverse_fields:
            spec.append((invf, None))
        return spec
# ---
def _strip_properly_formatted_commas(expr: str):
    # We want to be careful because we don't want to strip tuple commas
    p1 = re.compile("(\\d)(,)(\\d\\d\\d)($|\\D)")
    while True:
        next_expr = p1.sub("\\1\\3\\4", expr)
        if next_expr == expr:
            break
        expr = next_expr
    return next_expr
# ---
def setUp(self):
        super(NetappDirectCmodeNfsDriverOnlyTestCase, self).setUp()
        self._custom_setup()
# ---
def elapsed(self):
        return self._elapsed
# ---
def __str__(self):
        return self.text
# ---
def __init__(self,occurrence):
        self.label = occurrence[0]
# ---
def relu(a: A) -> A:
    return wrap_elemwise_unary(jnn.relu, a)
# ---
def __unicode__(self):
		return self.contenido
# ---
def inference_server(trainer_config, baby_llama_config, loaded_model):
    """Create an InferenceServer instance."""
    model, tokenizer = loaded_model
    with trainer_config.use_device_mesh(), hax.axis_mapping(trainer_config.compute_axis_mapping):
        return InferenceServer.create(baby_llama_config, model, tokenizer)
# ---
def unlink(self, cr, uid, ids, context=None):
        if context is None:
            context = {}
        ctx = context.copy()
        for move in self.browse(cr, uid, ids, context=context):
            if move.state != 'draft' and not ctx.get('call_unlink', False):
                raise osv.except_osv(_('User Error!'), _('You can only delete draft moves.'))
        return super(stock_move, self).unlink(
            cr, uid, ids, context=ctx)
# ---
def to_state_dict(tree: PyTree, prefix: str | None = None) -> StateDict:
        warnings.warn("Ignore all int8 states (if any) for now.")
        return {}
# ---
def shard_names(self) -> Sequence[str]:
            return []
# ---
def ListJobs(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def use_cpu_device():
    """Temporarily sets the default device to CPU"""
    cpu = jax.local_devices(backend="cpu")[0]
    with jax.default_device(cpu):
        yield cpu
# ---
def run_all_tests(marin_tokenizer: PreTrainedTokenizer):
    """Run all tests on the modified tokenizer."""
    special_tokens_injection_check(marin_tokenizer)
    chat_template_checks(marin_tokenizer)
# ---
def extract_id(row: dict, corpus_type: str) -> str:
    """Extract ID from row based on corpus type.

    Recursively navigates nested structures as defined in CORPUS_TYPE_TO_ID_GUIDE."""
    guide = CORPUS_TYPE_TO_ID_GUIDE[corpus_type]

    # grab the key, then navigate nested if needed
    val = row[guide["key"]]

    while "nested" in guide:
        nested = guide["nested"]
        assert isinstance(nested, dict)
        val = val[nested["key"]]
        guide = nested

    return val
# ---
def _find_acl_templates(conn, acl_templates):
    acl_template_ids = []
    for acl_template in acl_templates:
        acl_template_id = _find_acl_template(conn, acl_template)
        if acl_template_id:
            acl_template_ids.append(acl_template_id)
    return acl_template_ids
# ---
def _get_container_ids() -> set[str]:
    """Get all container IDs (running and stopped)."""
    result = subprocess.run(["docker", "ps", "-aq"], capture_output=True, text=True, check=False)
    return set(result.stdout.strip().split()) if result.stdout.strip() else set()
# ---
def __init__(self, context, sock):
        self._context = context
        self._sock = sock
        self._connection = OpenSSL.SSL.Connection(context, sock)
        self._makefile_refs = 0
# ---
def max_gen_toks(self) -> int:
        """Backward compatibility property for max_gen_toks."""
        return self._generation_kwargs.get("max_gen_toks", 256)
# ---
def approve(self, request, *args, **kwargs):
        response = super().update(request, *args, **kwargs)
        instance = self.get_object()
        instance.approve(processor=self.request.user)
        return response
# ---
def initialized_wrapped_optimizer():
    mock_opt = mock_optimizer_transform()
    skip_conf = SkipStepConfig(rolling_interval_length=10)  # smaller interval for tests
    wrapped_opt = skip_conf.wrap(mock_opt)
    dummy_params = {"w": jnp.array([1.0, 2.0, 3.0]), "b": jnp.array([0.5])}
    initial_state = wrapped_opt.init(dummy_params)
    return wrapped_opt, initial_state, dummy_params, skip_conf
# ---
def visit_mfrac(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            num = self._visit(children[0])
            den = self._visit(children[1])
            return BracedNode(f"\\frac{{{num}}}{{{den}}}")
        return TextNode("")
# ---
def from_job(cls, job):
        """Create runner from RLJob."""

        train_config, _ = job.to_worker_configs()
        return cls(train_config)
# ---
def asr_model_type(cls) -> Type["ASRMixin"]:
        pass
# ---
def test_is_redirect(self):
        self.assertFalse(status.is_redirect(299))
        self.assertFalse(status.is_redirect(400))

        for i in range(300, 399):
            self.assertTrue(status.is_redirect(i))
# ---
def __init__(self, location: str, config: WeightTransferConfig):
        super().__init__(location)
        self.config = config
        self._weights_store = {}
        self._latest_weight_id = None
        self._lock = threading.Lock()
        self._location = location
# ---
def test_admin_contacts(self):
        eq_(self.record.admin_contacts.__class__.__name__, 'list')
        eq_(self.record.admin_contacts, [])
# ---
def test_as_remote_kwargs_gpu():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=GpuConfig(variant="H100", count=2))
    kwargs = as_remote_kwargs(config)
    assert kwargs["num_gpus"] == 2
    assert kwargs["accelerator_type"] == "H100"
# ---
def brailleFlashTimeValueChanged(self, widget):
        self.prefsDict["brailleFlashTime"] = widget.get_value_as_int() * 1000
# ---
def test_comparison_boundary_shift():
    variants = generate_expression_variants("x < y")
    assert "x <= y" in variants
# ---


def fib(n: int):
    """Return n-th Fibonacci number.
    >>> fib(10)
    55
    >>> fib(1)
    1
    >>> fib(8)
    21
    """
    if n == 0:
        return 0
    if n == 1:
        return 1
    return fib(n - 1) + fib(n - 2)
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        examples = []
        for _ in range(n_examples):
            word, opposite = self.OPPOSITES[rng.integers(len(self.OPPOSITES))]
            prompt = f"Opposite of {word}? One word:"
            answer = opposite
            examples.append({"prompt": prompt, "answer": answer})
        return examples
# ---
def __repr__(self):
        return "<unionmatcher matchers=%r>" % self._matchers
# ---
def blocking_wait(coro):
    """
    This will only work if there are fewer than 10 levels of nested coroutines...
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop is not None and loop.is_running():
        future = _executor.submit(lambda: asyncio.run(coro))
        return future.result()
    else:
        return asyncio.run(coro)
# ---
def reciprocal(a: A) -> A:
    return wrap_elemwise_unary(jnp.reciprocal, a)
# ---
def _construct_direct(cls, variables, coord_names, dims, attrs,
                          file_obj=None):
        """Shortcut around __init__ for internal use when we want to skip
        costly validation
        """
        obj = object.__new__(cls)
        obj._variables = variables
        obj._coord_names = coord_names
        obj._dims = dims
        obj._attrs = attrs
        obj._file_obj = file_obj
        return obj
# ---
def handle(self):
		run_root = self.__backend_mgr.get_run_root(self.__task.data['backend'], self.__task.data['id'])

		main_root = os.path.join(run_root, 'main')

		safe_rmdir(main_root)
		safe_mkdir(main_root)

		self.__create_input(main_root)
		self.__create_context(main_root)
		self.__create_action(main_root)
		self.__create_navigator(main_root)
		self.__create_bootstrap(main_root)

		launcher_param = self.__create_launcher(run_root)

		self.__submit(launcher_param)
# ---
def read(self, path):
        raise Exception('read called with %r' % path)
# ---
def EvalBatch(self):
        return self.config.EvalBatch
# ---
def getReplicaStatus( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndict = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.getReplicaStatus( lfndict )
# ---
def pages_per_seq(self) -> int:
        return self._pages_per_seq
# ---
def setUpTestData(cls):
        cls.url = reverse('search-list')
        Feature.objects.create(name='archival descriptions', enabled=True)

        org_group_type = GroupType.objects.create(codename='organization')
        cls.group = Group.objects.create(group_type=org_group_type)
        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)
        cls.archive_type = TagVersionType.objects.create(name='archive', archive_type=True)
# ---
def num_slices(self):
        """number of nodes"""
        return max(getattr(device, "slice_index", 0) for device in jax.devices()) + 1
# ---
def test_cpu_defaults(self):
        resources = ResourceConfig()
        spec = convert_resources(resources)
        assert spec.cpu == 1
        assert spec.memory == "128m"
        assert spec.disk == "1g"
        assert spec.device is None
# ---
def _compute_unpermuted_ids(self, counts_per_block):
        unpermuted_ids = np.zeros(int(counts_per_block.sum()), dtype=np.int64)
        start = 0
        for i, dsname in enumerate(self.dataset_index):
            count = counts_per_block[i]
            unpermuted_ids[start : start + count] = (i << 16) + np.arange(count)
            start += count
        return unpermuted_ids
# ---
def test_nested_field(self):
        expr = col("meta")["score"]
        assert expr.evaluate({"meta": {"score": 0.8}}) == 0.8
# ---
def TryJumpLocationInOpenedTab( filename, line, column ):
  filepath = os.path.realpath( filename )

  for tab in vim.tabpages:
    for win in tab.windows:
      if win.buffer.name == filepath:
        vim.current.tabpage = tab
        vim.current.window = win
        vim.current.window.cursor = ( line, column - 1 )

        # Center the screen on the jumped-to location
        vim.command( 'normal! zz' )
        return True
  # 'filename' is not opened in any tab pages
  return False
# ---
def test_reset_twice_error(self):
        with self.getcapture() as cap:
            print("hello")
            out, err = cap.readouterr()
        pytest.raises(ValueError, cap.stop_capturing)
        assert out == "hello\n"
        assert not err
# ---
def try_parse(value):
    try:    return int(value)
    except: return { 'true': True, 'false': False }.get(value.lower(), value)
# ---
def union_axes(a1: AxisSpec, a2: ShapeDict) -> ShapeDict: ...
# ---
def test_ckpt_path_with_input_name():
    input_name = make_input_name("checkpoints/step-555000", "myrun")
    assert ckpt_path_to_step_name(input_name) == "myrun-555000"
# ---
def _cleanup_all_iris_containers(self) -> None:
        """Remove all iris-managed containers at startup.

        This handles crash recovery cleanly without tracking complexity.
        """
        removed = self._runtime.remove_all_iris_containers()
        if removed > 0:
            logger.info("Startup cleanup: removed %d iris containers", removed)
# ---
def name_get(self, cr, uid, ids, context=None):
        # always return the full hierarchical name
        res = self._complete_name(cr, uid, ids, 'complete_name', None, context=context)
        return res.items()
# ---
def log_summary(self, metrics: dict[str, Any]):
        for k, v in metrics.items():
            if _is_scalar(v):
                self.writer.add_scalar(k, v, global_step=None)
            elif isinstance(v, str):
                self.writer.add_text(k, v, global_step=None)
            else:
                pylogger.error(f"Unsupported metric type: {type(v)} for key {k}")
# ---
def scenario(function):
    def subfunction(self):
        run(function(self))

    return subfunction
# ---
def has_active_lock(self) -> bool:
        """Check if any worker has an active (non-stale) lock."""
        _, lock_data = self._read_lock_with_generation()
        return lock_data is not None and not lock_data.is_stale()
# ---
def test_mem_write_byte_calls_char_generator_top_left(self):
        self.mda.mem_write_byte(0x0000, 0x41)
        self.assertEqual(self.cg.last_blit, (None, (0, 0), 0x41, MDA_GREEN, MDA_BLACK))
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> ActorHandle:
        """Create a named actor instance. Returns a handle immediately."""
        ...
# ---
def validate(self, task):
        """Check that 'driver_info' contains IPMI credentials.

        Validates whether the 'driver_info' property of the supplied
        task's node contains the required credentials information.

        :param task: a task from TaskManager.
        :raises: InvalidParameterValue if required IPMI parameters
            are missing.
        :raises: MissingParameterValue if a required parameter is missing.

        """
        _parse_driver_info(task.node)
# ---
def test_find_span_end_invalid_python():
    end = _find_span_end("def (broken", 0)
    assert end is None
# ---
def vm_name(zone: str) -> str:
    """Generate VM name from zone using fun naming."""
    return f"{config.TPU_VM_PREFIX}-{generate_fun_name(zone)}"
# ---
def stop(self):
        """Stop the event loop and join the thread."""
        if self._loop is None:
            return

        self._loop.call_soon_threadsafe(self._loop.stop)
        if self._thread is not None:
            self._thread.join(timeout=5)
        self._loop = None
        self._thread = None
# ---
def test_list(self):
        """Store and retrieve a list"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "abc", "l": ["a", 1, False]})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["l"], ["a", 1, False])
# ---
def workspace_path(self) -> str:
        """Path to the workspace root directory (temp dir root)."""
        if not self._temp_dir:
            raise RuntimeError("TemporaryVenv must be entered before accessing workspace_path")
        return self._temp_dir.name
# ---
def test_bundle_creator_uses_fallback_when_git_unavailable(workspace):
    with patch("iris.cluster.client.bundle._get_git_non_ignored_files", return_value=None):
        creator = BundleCreator(workspace)
        bundle_bytes = creator.create_bundle()

    with zipfile.ZipFile(io.BytesIO(bundle_bytes)) as zf:
        names = zf.namelist()
        assert "pyproject.toml" in names
        assert "src/main.py" in names
        assert not any("__pycache__" in n for n in names)
# ---
def year(self):
        program = self.program()
        return program.start_date.year if program else None
# ---

def iscube(a):
    '''
    Write a function that takes an integer a and returns True
    if this ingeger is a cube of some integer number.
    Note: you may assume the input is always valid.
    Examples:
    iscube(1) ==> True
    iscube(2) ==> False
    iscube(-1) ==> True
    iscube(64) ==> True
    iscube(0) ==> True
    iscube(180) ==> False
    '''
    a = abs(a)
    return int(round(a ** (1. / 3))) ** 3 == a
# ---
def argmin(array: NamedArray, axis: AxisSelector | None) -> NamedArray:
    return wrap_reduction_call(jnp.argmin, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def dev_shards(self):
    raise NotImplementedError()
# ---
def clear_record(self):
        self.recordWidget.clear()
# ---
def fray_default_job_ctx(ctx: JobContext):
    """Set the default job context for the duration of the context.

    Examples:
        >>> ctx = create_job_ctx("threadpool", max_workers=8)
        >>> with fray_default_job_ctx(ctx):
        ...     results = execute(ds)
    """
    old_ctx = _job_context.get()
    _job_context.set(ctx)
    try:
        yield ctx
    finally:
        _job_context.set(old_ctx)
# ---
def testFloatBasic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.float32)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def __init__(self):
        self._start_time = time.time()
        self._elapsed = 0.0
        self._n = 0
# ---
def _define_methods(self, methods):
        for spec in methods:
            if len(spec) == 2: name, action = spec
            else: name, action = spec, None
            meth = FakeMethod(name, action, self.handle)
            setattr(self.__class__, name, meth)
# ---
def cmd(self):
        return '''
               docker run --rm --env AWS_ACCESS_KEY_ID={} --env AWS_SECRET_ACCESS_KEY={} rita/download-rita --year {} --month {} --data_path {}/{}
        '''.format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, self.year, self.month, self.root_path, self.raw_path)
# ---
def __str__(self):
        return '%s (%s)' % (self.name, self.sno.short_name)
# ---
def test_one_step_edit_invalid_source_returns_none():
    assert one_step_edit("not valid{{{", "x = 1\n") is None
# ---
def __init__(self, models, config):
        self.models = models
        self.config = config
        self.es = Elasticsearch(
            hosts=self.config.hosts,
            **self.config.get("client_options", {})
        )

        self.types = AttributeDict()
# ---
def get_toplevel_xid():
    if app.window.get_window():
        try:
            return app.window.get_window().get_xid()
        except AttributeError:  # non x11
            pass
    return 0
# ---
def convert_to_display_name(self, value, record=None):
        """ convert ``value`` from the cache to a suitable display name. """
        return ustr(value)
# ---
def teardown():
            """Delete the temporary files created in `setup()`."""
            shutil.rmtree(context.temp_dir)
# ---
def replacer(match):
        old_link = match.group(0)
        new_link = f"https://wandb.ai/marin-community{match.group(1)}"
        replacements.append((old_link, new_link))
        return new_link
# ---
def __init__(self, exception):
        self.exception = exception
# ---
def nancumprod(a: NamedArray, axis: AxisSelector, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.nancumprod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nancumprod.html)
    """
    return wrap_axiswise_call(jnp.nancumprod, a, axis, dtype=dtype, single_axis_only=True)
# ---
def test_simple(self, tmpfile):
        fd = tmpfile.fileno()
        cap = capture.FDCapture(fd)
        data = tobytes("hello")
        os.write(fd, data)
        s = cap.snap()
        cap.done()
        assert not s
        cap = capture.FDCapture(fd)
        cap.start()
        os.write(fd, data)
        s = cap.snap()
        cap.done()
        assert s == "hello"
# ---
def push(self, item: T_co) -> None:
        self.queue.append(item)
# ---
def gpt2_tokenizer():
    """GPT-2 tokenizer without chat template (for fallback testing)."""
    return AutoTokenizer.from_pretrained("gpt2")
# ---
def __repr__(self):
        return '<GeomCoverage %r: %r>' % (self.extent.llbbox, self.geom)
# ---
def test_metric_value(reduction, value, count, expected):
    """Metric.value() applies correct reduction."""
    m = Metric(_value=value, _count=count, reduction=reduction)
    assert jnp.allclose(m.value(), expected)
# ---
def check():
            try:
                re.compile(regex_edit.text())
            except re.error as e:
                raise OptionsCheckError(_("Regex Error"), string_(e))
# ---
def delete(id) :
    db = cherrypy.session['database']
    try:
        con = lite.connect( db )
        cur = con.cursor()
        sql =  "DELETE from Radio WHERE id = '%s'" % (id)
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def Vocab(self) -> Axis:
        return Axis("vocab", len(self.tokenizer))
# ---
def skip_if_module_missing(module: str):
    def try_import_module(module):
        try:
            __import__(module)
        except ImportError:
            return False
        else:
            return True

    return pytest.mark.skipif(not try_import_module(module), reason=f"{module} not installed")
# ---
def quit(self):
        self._quit = True
# ---
def test_worker_crash_mid_task(cluster):
    """Worker task monitor crashes mid-task. Task fails, controller detects
    via heartbeat reconciliation or report_task_state."""
    _url, client = cluster
    # task_monitor chaos kills the monitoring loop â€” task fails with error
    enable_chaos("worker.task_monitor", failure_rate=1.0)
    job = submit(client, _quick, "crash-mid-task")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_FAILED
# ---
def backwards(self, orm):
        # Removing unique constraint on 'Package', fields ['name', 'url']
        db.delete_unique(u'api_package', ['name', 'url'])

        # Deleting model 'Package'
        db.delete_table(u'api_package')
# ---
def view(self, name):
        ret = []
        obj = getattr(self.ndb, name)
        for line in obj.dump():
            ret.append(line)
        return bottle.template('{{!ret}}', ret=json.dumps(ret))
# ---
def test_registered_on_defaults_to_datetime(self):
        # Ensure that registered_on is a datetime.
        with self.client:
            self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="admin_user"),
                follow_redirects=True,
            )
            user = User.query.filter_by(email="ad@min.com").first()
            self.assertIsInstance(user.registered_on, datetime.datetime)
# ---
def read(self, bufsiz, flags=0):
        return self.recv(bufsiz, flags)
# ---
def caption(self):
        """The window caption (title).  Read-only.

        :type: str
        """
        return self._caption
# ---
def __get__(self, obj, cls=None):
        if not obj:
            return self
        return getattr(obj, self.slot, None)
# ---
def vm_manager(
        self,
        group_config: config_pb2.ScaleGroupConfig,
        vm_factory: TrackedVmFactory,
        *,
        dry_run: bool = False,
    ) -> VmManagerProtocol:
        raise NotImplementedError("Local platform uses LocalController autoscaler")
# ---
def locked_call():
                with self._lock:
                    return self._method(*args, **kwargs)
# ---
def user_phone(self):
        return self._get_profile().phone
# ---
def __len__(self):
        return len(iter(self))
# ---
def __getitem__(self, ind):
        return self.lines[ind]
# ---
def find_speedrun_results(base_path: str) -> list[str]:
    fs = fsspec.filesystem(base_path.split("://", 1)[0] if "://" in base_path else "file")
    pattern = f"{base_path}/**/speedrun_results.json"
    all_results = fs.glob(pattern)

    # Filter out excluded speedruns by checking the run name (directory name)
    return [path for path in all_results if Path(path).parent.name not in EXCLUDED_SPEEDRUNS]
# ---
def test_missing_elements_errors():
    partial_order = ("qux", ...)
    candidates = ("banana", "apple", "cherry")
    with pytest.raises(ValueError):
        rearrange_for_partial_order(partial_order, candidates)
# ---
def KHeads(self) -> Axis:
        return Axis("k_heads", self.num_k_heads)
# ---
def check_xsrf_cookie(self):
        return
# ---
def update_demand(self, demand: int) -> None:
        """Update current demand."""
        self._current_demand = demand
        self._peak_demand = max(self._peak_demand, demand)
# ---
def visit_Compare(self, node: ast.Compare) -> ast.Compare:
        node.ops = [self._maybe_swap(op, _COMPARE_OPS) for op in node.ops]
        self.generic_visit(node)
        return node
# ---
def _console_pwfile_path(uuid):
    """Return the file path for storing the ipmi password for a console."""
    file_name = "%(uuid)s.pw" % {'uuid': uuid}
    return os.path.join(CONF.tempdir, file_name)
# ---
def resource_type(self):
        """Returns the primary type of resource this client works with."""
        return 'image'
# ---
def test_dupfile_on_bytesio():
    io = py.io.BytesIO()
    f = capture.safe_text_dupfile(io, "wb")
    f.write("hello")
    assert io.getvalue() == b"hello"
    assert 'BytesIO object' in f.name
# ---
def base_token_offset(self) -> int:
        """First base vocabulary token ID."""
        return 3 + self.num_position_tokens
# ---
def test_basic_actor_call():
    """Test basic actor method calls work correctly."""
    server = ActorServer(host="127.0.0.1")
    server.register("calc", Calculator())
    port = server.serve_background()

    try:
        resolver = FixedResolver({"calc": f"http://127.0.0.1:{port}"})
        client = ActorClient(resolver, "calc")
        assert client.add(2, 3) == 5
        assert client.multiply(4, 5) == 20
    finally:
        server.stop()
# ---
def __init__(self):
        self._calls = []
# ---
def test_bool(self):
        """Store and retrieve a boolean"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "abc", "b": True})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["b"], True)
        self.assertTrue(isinstance(item["b"], bool))
# ---
def cli(ctx: click.Context) -> None:
    """Benchmark Zephyr deduplication pipeline."""
    if ctx.invoked_subcommand is None:
        # Default to benchmark command with default args
        ctx.invoke(benchmark)
# ---
def test_add_dicts_base_case(self):
        """add_dict where one argument is None returns the other"""
        f = object()
        self.assertEqual(add_dicts(f, None), f)
        self.assertEqual(add_dicts(None, f), f)
# ---
def testSmallMinMaxRange(self):
        self.assertConfigureFails(HPCP(), {'bandPreset':False, 'maxFrequency':200, 'minFrequency':1})
# ---
def to_raw_shape(shape: Union[ShapeSpec, NamedShapeSpec]) -> Optional[Tuple[int, ...]]:
    if isinstance(shape, ShapeDtypeStruct):
        return shape.shape
    else:
        raw = shape.shape
        if raw is None:
            return None
        return tuple(ax.size for ax in raw)
# ---
def test_function_body_difference():
    source = "def f(x):\n    return x + 1\n"
    target = "def f(x):\n    return x * 2\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1

    # The edit should target the body, not the whole function.
    for edit in edits:
        assert edit.node_type != "FunctionDef", "Should target a specific sub-expression, not the whole function"
# ---
def new_ref(value: NamedArray) -> NamedRef:
    """Construct a `NamedRef` from a `NamedArray`."""
    if isinstance(value, NamedArray):
        base_axes = value.axes
        impl = jax.new_ref(value.array)
    else:
        raise TypeError("new_ref only supports NamedArray inputs")

    prefix = tuple(slice(None) for _ in base_axes)
    return NamedRef(impl, base_axes, prefix)
# ---
def with_prefix(prefix: str, leaf: None) -> str: ...
# ---
def wrapper(*args, **kwargs):
            with _remove_tpu_lockfile_on_exit_cm():
                return fn(*args, **kwargs)
# ---
def __init__(self, track, time=None, channel=None):
        MidiEvent.__init__(self, track, time=time, channel=channel)
        self.type_ = "DeltaTime"
# ---
def _refresh_target_selection(self):
        if not self.panes:
            return
        target = self.selected_pane.selected_target
        self._selected_target_index = 0
        if target is not None:
            try:
                self._selected_target_index = self.target_accounts.index(target) + 1
            except ValueError:
                pass
# ---
def pytest_addoption(parser):
    parser.addoption(
        "--use-docker",
        action="store_true",
        default=False,
        help="Use real Docker containers instead of in-process mocks",
    )
    parser.addoption(
        "--update-snapshots",
        action="store_true",
        default=False,
        help="Update golden snapshot files instead of comparing",
    )
# ---
def set_time(value):
    global test_time
    test_time = value
    log.debug("Time now set to : %d" % test_time)
# ---
def matplotlib_pyplot():
  import matplotlib  # pylint: disable=g-import-not-at-top
  matplotlib.use("agg")
  import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top
  return plt
# ---
def _expandsets(kindpats, ctx):
    """Returns the kindpats list with the 'set' patterns expanded."""
    fset = set()
    other = []

    for kind, pat, source in kindpats:
        if kind == "set":
            if not ctx:
                raise error.ProgrammingError("fileset expression with no " "context")
            s = ctx.getfileset(pat)
            fset.update(s)
            continue
        other.append((kind, pat, source))
    return fset, other
# ---
def _is_valid_nonce(nonce, secret):
	comp = nonce.split(':')
	if len(comp) != 3:
		return False
	calc_nonce = _generate_nonce(comp[0], secret, comp[1])
	if nonce == calc_nonce:
		return True
	return False
# ---
def _apply_failing_clock_call(name, errno_value):
        calls = []

        def _failing_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            monkeypatch.setattr(_api.ffi, "errno", errno.EINVAL)
            return -1

        monkeypatch.setattr(_api, name, _failing_clock_call)

        return calls
# ---
def output_exemplar(self):
        return self.exemplar
# ---
def cursor_execute(conn, cursor, statement, parameters,
                                context, executemany):
            cursor_stmts.append((str(statement), parameters, None))
# ---
def _pack_requests(
    requests: list[Instance],
    tokenizer: HfTokenizer,
    Pos: hax.Axis,
    max_pack_size: int,
) -> list[LmExample]:
    packed_iterator = _iterate_tokenized_requests(requests, tokenizer, Pos.size, batch_size=128)
    # TODO: use a better packing algorithm?
    return greedy_pack_prompt_completions(
        Pos,
        packed_iterator,
        max_segments_per_example=max_pack_size,
        pad_token=tokenizer.pad_token_id,
    )
# ---
def disable_noisy_loggers():
    """Disable verbose INFO logging from inference engine and HTTP clients."""
    noisy_loggers = [
        "levanter.inference.engine",
        "levanter.inference.openai",
        "httpx",
        "uvicorn.access",
    ]
    for logger_name in noisy_loggers:
        logging.getLogger(logger_name).setLevel(logging.WARNING)
# ---
def test_from_list(sample_data, backend):
    """Test creating dataset from list."""
    ds = Dataset.from_list(sample_data)
    assert list(Backend.execute(ds, context=backend)) == sample_data
# ---
def __init__(self):
        'Initialize.'
        self.meta = []
        self.filelist = []
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "@"
# ---
def __getitem__(self, key):
        return self.subqueries[key]
# ---
def _print_rejected(matches, after_match):
            if self.key.REJ in after_match:
                rejected = sorted(
                    set(after_match[self.key.REJ]).difference(
                        set(matches.get(self.key.REJ, []))
                    )
                )
                for key in rejected:
                    print('Key for minion {0} rejected.'.format(key))
# ---
def init_block(i):
                (block_args, block_kwargs) = haliax.tree_util.tree_map(
                    functools.partial(BlockSeq._slice_out, Block, i), (args, kwargs)
                )
                return module.init(*block_args, **block_kwargs)
# ---
def split(self, axis: AxisSelector, new_axes: Sequence[Axis]) -> Sequence["NamedArray"]:  # pragma: no cover
        return haliax.split(self, axis=axis, new_axes=new_axes)
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "Gpt2LMHeadModel":
        new_embeddings = self.embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, embeddings=new_embeddings)
# ---
def reference_impl_batched(x_batched: jax.Array) -> jax.Array:
    """Reference implementation (oracle).

    Replace this with a readable, correct baseline for your kernel. This should operate on batched inputs.
    """

    return jnp.tanh(x_batched)
# ---
def fn(data):
            mod = MyModule()
            return mod.array
# ---
def list_all_endpoints(self) -> list[ControllerEndpoint]:
        """Return all registered endpoints."""
        with self._lock:
            return list(self._endpoints.values())
# ---
def userdatabase(user) :
    db = database
    if not os.path.isfile(db):
        return None
    return db
# ---
def current_label_start(self):
        if self.current_label and self.message:
            return self.message.get_label_range(self.current_label, self.proto_view, False)[0]
        else:
            return -1
# ---
def _inject_implicit_mixed_number(step: str):
    """
    Automatically make a mixed number evalable
    e.g. 7 3/4 => 7+3/4
    """
    p1 = re.compile("([0-9]) +([0-9])")
    step = p1.sub("\\1+\\2", step)  ## implicit mults
    return step
# ---
def find_one_raw(self, resource, _id):
        args = self._es_args(resource)
        hit = self.es.get(id=_id, **args)
        return self._parse_hits({'hits': {'hits': [hit]}}, resource).first()
# ---
def gen_codes():
            """Generate the 702 possible input codes"""
            # First, the 1-character codes
            for c in string.ascii_lowercase:
                yield c

            # Next, the 2-characters-with-wildcard codes
            for t in itertools.product(string.ascii_lowercase, repeat=2):
                yield '*'.join(t)
# ---
def show_logs(ctx, tail):
    """View cluster logs."""
    log_command = f"tail -n {tail} -f /tmp/ray/session_latest/logs/monitor*"
    subprocess.run(
        _maybe_add_ray_verbose(ctx.obj, ["ray", "exec", ctx.obj.config_file, log_command]),
        check=True,
    )
# ---
def forget(self, request):
		return self.match(request).forget(request)
# ---
def device_variant(self) -> str | None:
        """Device variant from worker metadata."""
        return get_device_variant(self.metadata.device)
# ---
def test_collate_order_by(self):
        collation = testing.requires.get_order_by_collation(testing.config)

        self._assert_result(
            select([self.tables.some_table]).order_by(
                self.tables.some_table.c.data.collate(collation).asc()
            ),
            [(1, "collate data1"), (2, "collate data2")],
        )
# ---
def unbound_on_Server(host, port,
                              use_ssl,
                              connect_timeout, *a, **kwargs):
            return self._on_Server(host, port,
                              use_ssl,
                              connect_timeout, *a, **kwargs)
# ---
def __init__(self):
                self.inner = MyModuleInit()
# ---
def build_map(hashes: list[str], ids: list[str]) -> dict[str, Any]:
    """
    Builds a duplicate map where ~50% of items are marked as canonical and 50% as duplicates.
    """
    dup_map = {}
    for i, (h, rec_id) in enumerate(zip(hashes, ids, strict=True)):
        # Every other element is treated as a duplicate of "other_canonical_id"
        canon = rec_id if i % 2 != 0 else "other_canonical_id"
        dup_map[h] = {"canonical": canon}
    return dup_map
# ---
def go_account(self, form, idx, idroot):
        form = self.get_form(name=form)
        form['indiceCompte'] = idx
        form['idRacine'] = idroot
        form.submit()
# ---
def __init__(self, driver):
        super(AddMoviePage, self).__init__(driver)
        self.nav = NavBlock(driver)
# ---
def execute(
                self,
                conn,
                execute,
                clauseelement,
                *multiparams,
                **params
                ):
                stmts.append((str(clauseelement), params, multiparams))
                return execute(clauseelement, *multiparams, **params)
# ---
def as_async_dataset(self) -> "AsyncDataset[T_co]":
        raise NotImplementedError("...")
# ---
def no_alloc(state):
                    # No-op; leave index INVALID so downstream gets INVALID destinations
                    state = eqx.error_if(state, jnp.zeros(()) < 4, "INVALID!")
                    return state
# ---
def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "boing"}
# ---
def write_batch(self, batch: RolloutBatch) -> None:
        """Write batch to memory queue, blocking if full."""
        self._queue.push(batch)
# ---
def test_simple_many_check_open_files(self, testdir):
        with lsof_check():
            with testdir.makepyfile("").open('wb+') as tmpfile:
                self.test_simple_many(tmpfile)
# ---
def unembed_active_scale(self):
        return 1 / hax.axis_size(self.Embed)
# ---
def sink_attention_jax_flash(
    query,
    key,
    value,
    sinks,
    sm_scale: float = 0.125,
    sliding_window: int | None = None,
    start_q: int = 0,
    *,
    block_size: int = 64,
):
    return sink_attention(
        query,
        key,
        value,
        sinks,
        sm_scale,
        sliding_window,
        start_q,
        attn_backend=AttentionBackend.JAX_FLASH,
        block_size=block_size,
        inference=True,
    )
# ---
def update_floatingip_postcommit(self, context, fip_context):
        pass
# ---
def config(self):
        return self.backbone.config
# ---
def ip(self):
        if not hasattr(self, "_socket"):
            self._get_address()
        return self._ip
# ---
def test_calculo_horas_voadas(self):
        s_horas = {
            'h_diurno': '6:40',
            'h_noturno': '6:47',
            'h_total_voo': '13:27',
            'h_faixa2': '0:00',
            'h_sobreaviso': '40:00',
            'h_reserva': '29:13'
        }
        self.assertEqual(self.escala.soma_horas(), s_horas)
# ---
def tokenizer():
    return TreeDiffusionTokenizer(max_seq_len=64)
# ---
def test_actor_thread_safety(job_context):
    actor = job_context.create_actor(SimpleActor, 0)

    futures = [actor.increment.remote(1) for _ in range(100)]
    [job_context.get(f) for f in futures]

    final_value = actor.get_value.remote()
    assert job_context.get(final_value) == 100
# ---
def create_tmpfile_copy(inc_file):
        '''create a temporary copy of a file'''
        tmpfile = Utils.create_tmpfile('lib_openshift-')
        Utils._write(tmpfile, open(inc_file).read())

        # Cleanup the tmpfile
        atexit.register(Utils.cleanup, [tmpfile])

        return tmpfile
# ---
def validate_edit(source: str, mutation: Mutation) -> bool:
    """Check whether applying a mutation produces valid Python.

    Args:
        source: Current program source.
        mutation: Proposed edit.

    Returns:
        True if the edited program parses successfully.
    """
    edited = mutation.apply(source)
    try:
        ast.parse(edited)
        return True
    except SyntaxError:
        return False
# ---
def start(self):
        """Start worker in background thread."""
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
# ---
def slice(
        self, axis: AxisSelector, new_axis: AxisSelector | None = None, start: int = 0, length: int | None = None
    ) -> "NamedArray": ...
# ---
def inferred_head_dim(self) -> int:
        """Get per-head dimension."""
        if self.head_dim is not None:
            return self.head_dim
        return self.hidden_dim // self.num_heads
# ---
def test_load_math_environment():
    """Test loading MathEnv via EnvConfig."""
    config = EnvConfig(env_class="marin.rl.environments.math_env.MathEnv", env_args={"seed": 42})

    env = load_environment_from_spec(config)

    assert isinstance(env, MathEnv)
    assert len(env.train_examples) > 0
    assert len(env.eval_examples) > 0
# ---
def __init__(self, *args, **kwargs):
        super(UsuarioEditForm, self).__init__(*args, **kwargs)
        self.fields['primeiro_telefone'].widget.attrs['class'] = 'telefone'
        self.fields['segundo_telefone'].widget.attrs['class'] = 'telefone'
# ---
def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"
# ---
def _custom_setup(self):
        self._driver = netapp_nfs.NetAppDirect7modeNfsDriver(
            configuration=create_configuration())
# ---
def get_bundle(self, gcs_path: str, expected_hash: str | None = None) -> Path:
        del gcs_path, expected_hash
        return self._bundle_path
# ---
def rights(self):
        'Copyright/licensing information. (optional)'
        try:
            return self.opt_meta['rights']
        except KeyError:
            return None
# ---
def okPopup(title, msg, answerCallback):
    content = OkPopup(text=msg)
    content.bind(on_ok=answerCallback)
    popup = Popup(title=title,
                    content=content,
                    size_hint=(None, None),
                    size=(dp(600),dp(200)),
                    auto_dismiss= False)
    popup.open()
    return popup
# ---
def __iter__(self):
        return BackgroundIterator(self._producer_fn, self.max_capacity)
# ---
def map(self, fn: Callable[[T_co], U]) -> "ShardedDataSource[U]":
        return _MappedShardedDataSource(self, fn)
# ---
def expm1(a: A) -> A:
    return wrap_elemwise_unary(jnp.expm1, a)
# ---
def is_slow(self):
        """Indicate whether this query is *slow*, meaning that it cannot
        be executed in SQL and must be executed in Python.
        """
        return False
# ---
def _default_lm_head_fn(params: PyTree) -> jax.Array:
    return params.output_proj
# ---
def now(cls) -> "Timestamp":
        """Create timestamp for current time."""
        return cls(_now_ms())
# ---
def _node_list(cls):
        return next((os.environ[o] for o in _NODE_LIST_CHOICES if o in os.environ), None)
# ---
def __init__(self, renderer, rules=None, **kwargs):
        self.renderer = renderer
        self.links = {}
        self.footnotes = {}
        self.footnote_index = 0

        if not rules:
            rules = self.grammar_class()

        kwargs.update(self.renderer.options)
        if kwargs.get('hard_wrap'):
            rules.hard_wrap()

        self.rules = rules

        self._in_link = False
        self._in_footnote = False
        self._parse_inline_html = kwargs.get('parse_inline_html')
# ---
def pad_token_id(self) -> int:
        return 0
# ---
def item(self, *args, **kwargs):  # pragma: no cover - exercised via test assertions
            raise AssertionError("item should not be called on tracer-like indices")
# ---
def test_tokenizer():
    return create_test_tokenizer()
# ---
def __pow__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.power(self, other)
# ---
def valuePointer(self):
        return EnkfNode.cNamespace().value_ptr(self)
# ---
def test_lambda_output_shape_tuple_with_none(self):

    def lambda_fn(x):
      return x

    l = keras.layers.Lambda(lambda_fn, output_shape=(None, 10))
    output_shape = l.compute_output_shape((5, 10, 20))
    self.assertAllEqual([5, None, 10], output_shape.as_list())
# ---
def _fields(self, resource):
        """Get projection fields for given resource."""
        datasource = self._datasource(resource)
        keys = datasource[2].keys()
        return ','.join(keys) + ','.join([config.LAST_UPDATED, config.DATE_CREATED])
# ---
def cache_dir(tmp_path):
    """Create a temporary cache directory."""
    cache = tmp_path / "cache"
    cache.mkdir()
    return cache
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> KvPageCache:
        return self.self_attn.empty_page_cache(spec, dtype=dtype)
# ---
def _getVolumeForVoiceType(self, voiceType):
        """Gets the volume (gain) value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM

        Returns the volume (gain) value for the given voice type, or
        None if not set.
        """

        return self._getKeyValueForVoiceType(voiceType, acss.ACSS.GAIN)
# ---
def test_fix_rule_300(self):
        oRule = iteration_scheme.rule_300()

        oRule.fix(self.oFile)

        lActual = self.oFile.get_lines()

        self.assertEqual(lExpected, lActual)

        oRule.analyze(self.oFile)
        self.assertEqual(oRule.violations, [])
# ---
def pathJoin(parent, base):
    if parent.endswith('/'):
        return parent + base
    return parent + '/' + base
# ---
def test_get_lines_with_very_long_string():
        assert len(get_lines("a"*(4*LINEWIDTH-1))) == 4
# ---
def test_single_statement_replacement():
    source = "return a + b\n"
    target = "return a * b\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1
    assert edits[0].node_type in ("BinOp", "Return")
# ---
def test_filter_nested_field(backend):
    """Test filter with nested field access."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"id": 1, "meta": {"score": 0.9}},
            {"id": 2, "meta": {"score": 0.3}},
            {"id": 3, "meta": {"score": 0.7}},
        ]
    ).filter(col("meta")["score"] > 0.5)

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert all(r["meta"]["score"] > 0.5 for r in results)
# ---
def convert_to_cache(self, value, record, validate=True):
        """ convert ``value`` to the cache level in ``env``; ``value`` may come from
            an assignment, or have the format of methods :meth:`BaseModel.read`
            or :meth:`BaseModel.write`

            :param record: the target record for the assignment, or an empty recordset

            :param bool validate: when True, field-specific validation of
                ``value`` will be performed
        """
        return value
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {
            "mlp": None,
            "mlp_ln": "final_layer_norm",
            "attn_ln": "self_attn_layer_norm",
            "encoder_attn_ln": "encoder_attn_layer_norm",
        }
# ---
def test_read(self):
        config = read_config('config')
        self.assertEqual(config['cmus_host'], 'raspberry')
        self.assertEqual(config['cmus_passwd'], 'PaSsWd')
        self.assertEqual(config['app_host'], 'localhost')
        self.assertEqual(config['app_port'], '8080')
# ---
def output_shape_fn(input_shape):
      return input_shape
# ---
def question_suffix(cls) -> str:
        return " Write your answer in \\boxed{} format."
# ---
def _show(x):
        if isinstance(x, NamedArray):
            arr = x.array
            axes = x.axes
        else:
            arr = x
            axes = None

        def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass

        jax.debug.inspect_array_sharding(arr, callback=cb)
        return x
# ---
def _default_wandb_entity() -> str:
    if _wandb_mode_disabled() or not os.getenv("WANDB_API_KEY"):
        return WANDB_ENTITY
    return wandb.Api().default_entity
# ---
def list_instances(self, project: str, zone: str) -> list[dict]: ...
# ---
def the_object_name_is_at_location(name, location):
    obj_location = the_object_name_exists(name).location
    assert (
        obj_location - Vector([float(co) for co in location.split(",")])
    ).length < 0.1, f"Object is at {obj_location}"
# ---
def _filter_gen(stream: Iterator, predicate: Callable) -> Iterator:
    for item in stream:
        if predicate(item):
            yield item
# ---
def on_lower_bound_checked_changed(self):
        if self.ui.checkBoxLowerBound.isChecked():
            self.ui.spinBoxLowerBound.setEnabled(True)
            self.ui.spinBoxBoundaryNumber.setEnabled(True)
        elif not self.ui.checkBoxUpperBound.isChecked():
            self.ui.spinBoxLowerBound.setEnabled(False)
            self.ui.spinBoxBoundaryNumber.setEnabled(False)
        else:
            self.ui.spinBoxLowerBound.setEnabled(False)
# ---
def get_cookie(self, name):
        return self.parse_cookie(name)
# ---
def Embed(self) -> Axis:
        return self.token_embeddings.Embed
# ---
def open(self, filename, attrib="rb"):
        '''
        Open a MIDI file path for reading or writing.

        For writing to a MIDI file, `attrib` should be "wb".
        '''
        if attrib not in ['rb', 'wb']:
            raise MidiException('cannot read or write unless in binary mode, not:', attrib)
        self.file = open(filename, attrib)
# ---
def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4'), (2, 100, 200, 'ext4')]
# ---
def config(self):
        return self.model.config
# ---
def tree_structure(tree, is_leaf=None):
    """
    Version of [jax.tree_util.tree_structure][] that automatically treats NamedArrays as leaves.
    """
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: is_leaf(x) or is_named_array(x)

    return jax.tree_util.tree_structure(tree, is_leaf=is_leaf)
# ---
def user_title(self):
        return self._get_title_and_company()['title']
# ---
def create_process(self, process_id, vpnservice, namespace):
        return fedora_strongswan_ipsec.FedoraStrongSwanProcess(
            self.conf,
            process_id,
            vpnservice,
            namespace)
# ---
def send_keys(self, *keys_to_send):
        """Sends keys to current focused element.
        Args:
            keys_to_send: The keys to send.
        """
        self._actions.append(lambda:
            self._driver.switch_to_active_element().send_keys(*keys_to_send))
        return self
# ---
def to_list(x):
  """Normalizes a list/tensor into a list.

  If a tensor is passed, we return
  a list of size 1 containing the tensor.

  Arguments:
      x: target object to be normalized.

  Returns:
      A list.
  """
  if isinstance(x, list):
    return x
  return [x]
# ---
def __getstate__(self) -> dict:
        # Only serialize the endpoint name - client is lazily resolved
        return {"endpoint_name": self._endpoint_name}
# ---
def encode_cmd(text: str, tokenizer: str | None):
    """Encode text to tokens."""
    if tokenizer:
        load_tokenizer(tokenizer)
    encode_text(text)
# ---
def testFutureUnicodeLiterals(self):
    want = "u'foo'\n"
    self.assertEqual((0, want), _GrumpRun(textwrap.dedent("""\
        from __future__ import unicode_literals
        print repr('foo')""")))
# ---
def ensure_roundtrip(td_str, expected_seconds):
        # we don't enforce that the output is the same as the input,
        # but we do enforce that it can be parsed to the same timedelta
        td = parse_timedelta(td_str)
        assert td.total_seconds() == expected_seconds
        assert parse_timedelta(encode_timedelta(td)) == td, f"Failed to roundtrip {td_str}: {encode_timedelta(td)}"
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        effective_timeout = timeout if timeout is not None else 86400.0
        try:
            self._job.wait(timeout=effective_timeout, raise_on_failure=raise_on_failure)
        except Exception:
            if raise_on_failure:
                raise
        return self.status()
# ---
def parent(self) -> "JobName | None":
        """Get parent job name, or None if this is a root job."""
        if len(self._parts) == 1:
            return None
        return JobName(self._parts[:-1])
# ---
def test_create_experiment_view_unauthorized(self):
        """ Tests edit_experiment template does not render for url 'create_experiment'
            when unauthorized """
        self.set_roles([])
        response = self.client.get(reverse("ab_testing_tool_create_experiment"), follow=True)
        self.assertTemplateNotUsed(response, "ab_tool/create_experiment.html")
        self.assertTemplateUsed(response, "ab_tool/not_authorized.html")
# ---
def JumpToPreviewWindow():
  """ Jump the vim cursor to the preview window, which must be active. Returns
  boolean indicating if the cursor ended up in the preview window """
  vim.command( 'silent! wincmd P' )
  return vim.current.window.options[ 'previewwindow' ]
# ---
def _batchified_shape(Batch, leaf: hax.NamedArray | Array) -> ShapeSpec | NamedShapeSpec:
    if is_named_array(leaf):
        return NamedShapeSpec((Batch,) + leaf.axes, leaf.dtype)
    else:
        return ShapeSpec((Batch.size,) + tuple(leaf.shape), leaf.dtype)
# ---
def log_step_info_inner(step: StepInfo):
        metrics = {"train/loss": step.loss, "global_step": step.step}
        if total_steps:
            metrics["run_progress"] = step.step / total_steps
        log_optimizer_hyperparams(step.opt_state, step=step.step, prefix="optim")
        levanter.tracker.log(metrics, step=step.step)
# ---
def hook(mod):
    if sys.version[0] > '1':
        for i in range(len(mod.imports)-1, -1, -1):
            if mod.imports[i][0] == 'strop':
                del mod.imports[i]
    return mod
# ---
def _apply_timespec(name, goal_timespec):
        calls = []

        def _fake_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            timespec[0] = goal_timespec[0]
            return 0

        monkeypatch.setattr(_api, name, _fake_clock_call)

        return calls
# ---
def dot(
        self,
        *args: "NamedArray",
        axis: AxisSelection | None,
        precision: PrecisionLike = None,
        dot_general=jax.lax.dot_general,
    ) -> "NamedArray": ...
# ---
def get_writer(self, name: str, description: str) -> Path:
        """Register and return path for a log artifact. Creates parent dirs."""
        path = self._root / name
        path.parent.mkdir(parents=True, exist_ok=True)
        self._artifacts.append(LogArtifact(path=path, description=description))
        return path
# ---
def set_level(self,
                  level,
                  console_only=False):
        """
        Defines the logging level (from standard logging module) for log messages.
        :param level:           Level of logging for the file logger.
        :param console_only:    [Optional] If True then the file logger will not be affected.
        """
        self.queue.put(dill.dumps(SetLevelCommand(level=level,
                                                  console_only=console_only)))
# ---
def __new__(cls, path, json):
        if path in cls.CACHE:
            return cls.CACHE[path]
        cls.CACHE[path] = new = super(Notebook, cls).__new__(cls)
        return new
# ---
def _test_host_action(self, method, action, expected=None):
        result = method('host', action)
        if not expected:
            expected = action
        self.assertEqual(result, expected)
# ---
def test_profiler_hmac(self):
        hmac = "secret"
        prof = profiler._Profiler(hmac, base_id="1", parent_id="2")
        self.assertEqual(hmac, prof.hmac_key)
# ---
def test_create_actor_with_args(client: LocalClient):
    actor = client.create_actor(Counter, 42, name="counter")
    assert actor.get.remote().result() == 42
# ---
def __init__(self, proc1, proc2):
        self._procs = (proc1, proc2)
        self._stdout = proc2.stdout
# ---
def default_choice_name(cls) -> str | None:
        return "text"
# ---
def list(cls, params=None):
        if params is None:
            params = dict()
        response = cls(Api.call('sales/list_sales', params))
        return response.sale_summary
# ---
def __contains__(self, key):
        return id(key) in self._data
# ---
def clone_pages_from(self, src, dest) -> "DecodeState":
        """
        Clone kv_pages from src slot to dest slot.
        """
        sequences, page_table = self.sequences.clone_pages_from(self.page_table, src, dest)
        return dataclasses.replace(self, sequences=sequences, page_table=page_table)
# ---
def get_committed_resources(self) -> tuple[int, int, int]:
        """Return committed (cpu, memory_bytes, gpu_count) for this worker."""
        return (self.committed_cpu, self.committed_mem, self.committed_gpu)
# ---
def threeSumClosest(self, nums: List[int], target: int) -> Optional[int]:
        res = None
        nums = sorted(nums)

        for i in range(len(nums)):
            for j in range(i + 1, len(nums)):
                res = bsearch(nums, j + 1, len(nums) - 1, res, i, j, target)
        return res
# ---
def active_scale(self):
        return 1 / hax.axis_size(self.In)
# ---
def terminate(self, job_id: JobName) -> None:
        """Terminate a running job.

        Args:
            job_id: Job ID to terminate
        """
        self._cluster_client.terminate_job(job_id)
# ---
def on_btn_add_row_clicked(self):
        self.current_label.add_fuzz_value()
        self.fuzz_table_model.update()
# ---
def log_step_info(total_steps: Optional[int]):
    def log_step_info_inner(step: StepInfo):
        metrics = {"train/loss": step.loss, "global_step": step.step}
        if total_steps:
            metrics["run_progress"] = step.step / total_steps
        log_optimizer_hyperparams(step.opt_state, step=step.step, prefix="optim")
        levanter.tracker.log(metrics, step=step.step)

    return log_step_info_inner
# ---
def _effective_pack(component: DatasetComponent) -> bool | int | Literal["pad"]:
    if component.pack is not None:
        return component.pack
    fmt = component.format
    if isinstance(fmt, TextLmDatasetFormat):
        return False
    if isinstance(fmt, ChatLmDatasetFormat):
        return True if fmt.pack is None else fmt.pack
    return False
# ---
def reset_backoff(self) -> None:
        """Reset backoff state (typically after successful operation)."""
        self._consecutive_failures = 0
        self._backoff_until = Timestamp.from_ms(0)
# ---
def should_display_status_to_user(self):
        """Whether or not the status from this attempt should be displayed to the user."""
        return False
# ---
def on_move(x, y):
            """The window was moved.

            :Parameters:
                `x` : int
                    Distance from the left edge of the screen to the left edge
                    of the window.
                `y` : int
                    Distance from the top edge of the screen to the top edge of
                    the window.  Note that this is one of few methods in pyglet
                    which use a Y-down coordinate system.

            :event:
            """
# ---
def run_mbpp_test(program: str, test_assert: str, setup_code: str = "") -> bool:
    """Execute an MBPP assert-based test case against a program."""
    try:
        namespace: dict = {}
        if setup_code:
            exec(setup_code, namespace)
        exec(program, namespace)
        exec(test_assert, namespace)
        return True
    except Exception:
        return False
# ---
def __call__(
        self,
        lhs,
        rhs,
        dimension_numbers,
        precision: PrecisionLike = None,
        preferred_element_type: DTypeLike | None = None,
        **kwargs,
    ) -> jnp.ndarray:
        return jax.lax.dot_general(lhs, rhs, dimension_numbers, precision, preferred_element_type, **kwargs)
# ---
def non_tracking(self, context, **kwargs):
        router = kwargs.get('router', None)
        process_id = router['id']
        self.destroy_process(process_id)
        if process_id in self.routers:
            del self.routers[process_id]
# ---
def __init__(self, rollout_worker_config):
        super().__init__(rollout_worker_config)
        self.rollout_worker_config = rollout_worker_config

        # Metrics
        self.rollouts_generated = 0
        self.weight_transfers = 0
# ---
def to_proto(self) -> cluster_pb2.Entrypoint:
        """Convert to protobuf representation."""
        proto = cluster_pb2.Entrypoint()
        if self._callable_bytes is not None:
            proto.callable = self._callable_bytes
        elif self.command is not None:
            proto.command.argv[:] = self.command
        return proto
# ---
def __repr__(self):
        return "<subdirmatcher path=%r, matcher=%r>" % (self._path, self._matcher)
# ---
def rust_mark_paragraph_duplicates(
    batch: pa.RecordBatch, text_col: str, id_col: str, dup_map: dict, attribute_name: str
) -> pa.RecordBatch:
    # The mark function only needs the batch, map, and attr name.
    # text_col and id_col are ignored but kept for signature compatibility with the benchmark.
    return dupekit.mark_paragraph_duplicates(batch, dup_map, attribute_name)
# ---
def age_ms(self) -> int:
        """Get age of this timestamp in milliseconds."""
        return _now_ms() - self._epoch_ms
# ---
def atomIndexInResidue(residue):
  """ list of atom index in residue """
  index=[]
  for a in list(residue.atoms()):
    index.append(a.index)
  return index
# ---
def initialize(self, base_block):
        self.vtx = copy.deepcopy(base_block.vtx)
        self.hashMerkleRoot = self.calc_merkle_root()
# ---
def delete_previous_logs_browser():
    cmd = 'rm -rf browserlogs/*'
    cr.run_command(cmd)
# ---
def __init__(self, writer: "SummaryWriter"):
        self.writer = writer
# ---
def stop_event(self) -> threading.Event:
        return self._stop_event
# ---
def find_result(results, _name):
        ''' Find the specified result by name'''
        rval = None
        for result in results:
            if 'metadata' in result and result['metadata']['name'] == _name:
                rval = result
                break

        return rval
# ---
def non_caching_cycle(iterable):
    """Like itertools.cycle, but doesn't cache the iterable."""
    while True:
        yield from iterable
# ---
def loglikelihood_rolling(self, requests) -> List[Tuple[float]]:
        raise NotImplementedError()
# ---
def prepared_registry(self, data):
        ''' setter method for prepared_registry attribute '''
        self.__prepared_registry = data
# ---
def stop(self) -> None:
        """No-op: FakeVm has no background threads to stop."""
# ---
def test_corrupted_character():
    assert corrupted_character('{([(<{}[<>[]}>{[]{[(<()>')[0] == '}'
    assert corrupted_character('[[<[([]))<([[{}[[()]]]')[0] == ')'
    assert corrupted_character('[{[{({}]{}}([{[{{{}}([]')[0] == ']'
    assert corrupted_character('[<(<(<(<{}))><([]([]()')[0] == ')'
    assert corrupted_character('<{([([[(<>()){}]>(<<{{')[0] == '>'
# ---
def _failing_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            monkeypatch.setattr(_api.ffi, "errno", errno.EINVAL)
            return -1
# ---
def _to_job_id_str(self, job_or_id) -> str:
        """Convert Job object or string to job_id string."""
        if isinstance(job_or_id, str):
            return (
                JobName.from_string(job_or_id).to_wire()
                if job_or_id.startswith("/")
                else JobName.root(job_or_id).to_wire()
            )
        # Assume it's a Job object
        return str(job_or_id.job_id)
# ---
def GetLogs(region, stream_name, group_name, token=None):
  """Fetches the JSON formatted log stream starting at the token."""
  get_cmd = util.AWS_PREFIX + [
      '--region', region,
      'logs', 'get-log-events',
      '--start-from-head',
      '--log-group-name', group_name,
      '--log-stream-name', stream_name,
  ]
  if token:
    get_cmd.extend(['--next-token', token])
  stdout, _, _ = vm_util.IssueCommand(get_cmd)
  return json.loads(stdout)
# ---
def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Kill all tracked processes and clean up venv directory."""
        self._job_group.__exit__(exc_type, exc_val, exc_tb)
        try:
            self._temp_dir.cleanup()
        except Exception as e:
            logger.warning(f"Failed to cleanup temporary directory: {e}")
# ---
def create_test_logprobs(text: str):
    """Create logprobs content for a response text."""
    from openai.types.chat.chat_completion_chunk import ChoiceLogprobsLogprob

    logprobs_content = []
    for c in text:
        logprobs_content.append(
            ChoiceLogprobsLogprob(
                token=c,
                logprob=-1.0,
                bytes=[ord(c)],
                top_logprobs=[],
            )
        )
    return ChoiceLogprobs(content=logprobs_content)
# ---
def _detect_tpu_environment() -> bool:
    """Detects whether the TPU environment variable TPU_NAME is set and non-empty."""
    return bool(os.environ.get("TPU_NAME"))
# ---
def from_ms(cls, epoch_ms: int) -> "Timestamp":
        """Create timestamp from milliseconds since epoch."""
        return cls(epoch_ms)
# ---
def test_propagate_option_engine_to_connection(self):
        e1 = testing_engine("sqlite://",
                        options=dict(execution_options={"foo": "bar"}))
        e2 = e1.execution_options(bat="hoho")
        c1 = e1.connect()
        c2 = e2.connect()
        eq_(c1._execution_options, {"foo": "bar"})
        eq_(c2._execution_options, {"foo": "bar", "bat": "hoho"})
# ---
def __init__(self, urls):
        super().__init__(urls)
# ---
def __init__(self, job_id, command):
        self._id = job_id
        self._command = command
        self._thread = None

        self._status = STATUS.STARTING
        self._description = ''
        self._disk_progress = 0
        self._disk_count = 1
        self._current_disk = 1
        self._aborted = False
        self._proc = None
# ---
def request(self, method, url, body=None, headers=None):
        self.method = method
        self.selector = url
        if headers is not None:
            self.req_headers += headers.items()
        self.req_headers.sort()
        if body:
            self.data = body
        if self.raise_on_endheaders:
            import socket
            raise socket.error()
# ---
def testGetFormattedField(self):
    """Tests the GetFormattedField function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = TestFieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    zone_string = test_helper.GetFormattedField(
        'zone', event, event_data, event_data_stream, None)
    self.assertEqual(zone_string, 'UTC')
# ---
def build(self, axis: AxisSpec) -> RmsNorm:
        return RmsNorm.init(axis, eps=self.eps, use_weight=self.use_weight, use_bias=self.use_bias)
# ---
def _parse_arrow_schema(self):
        raise NotImplementedError("Not implemented.")
# ---
def _slugify(value: str) -> str:
    slug = SLUGIFY_PATTERN.sub("_", value.lower()).strip("_")
    return slug or "dataset"
# ---
def _is_parseable(source: str, node_type: str) -> bool:
    """Check that a code fragment parses back successfully.

    Statements are parsed as-is. Expressions are wrapped in an assignment
    to form a valid statement for ast.parse().
    """
    try:
        if node_type in STATEMENT_TYPES:
            ast.parse(source)
        else:
            # Expressions need a statement wrapper to parse.
            ast.parse(f"__x = {source}")
        return True
    except SyntaxError:
        return False
# ---
def from_batch(items: Sequence[np.ndarray], item_rank: Optional[int] = None) -> "PreparedBatch":
        data, offsets, shapes = _prepare_batch(items, item_rank)
        return PreparedBatch(data, offsets, shapes)
# ---
def manifest_entry(self):
        'Write the XML element for the manifest.'
        return _make_xml_elem('item', '',
          [
            ('href', self.name),
            ('id', self.ident),
            ('media-type', self.media_type)
          ])
# ---
def test_basic_rearrange_unordered():
    assert einops_rearrange(z, "{B D H W C} -> B H W D C").axes == (B, H, W, D, C)
    z_t = z.array.transpose((0, 2, 3, 1, 4))
    assert (einops_rearrange(z, "{B D H W C} -> B H W D C").array == z_t).all()

    assert einops_rearrange(z, "{C W H D B} -> B H W D C").axes == (B, H, W, D, C)
    assert (einops_rearrange(z, "{C W H D B} -> B H W D C").array == z_t).all()
# ---
def test_pending(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_PENDING) == JobStatus.PENDING
# ---
def __mod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.mod(self, other)
# ---
def switch_to(self):
        """Make this window the current OpenGL rendering context.

        Only one OpenGL context can be active at a time.  This method sets
        the current window's context to be current.  You should use this
        method in preference to `pyglet.gl.Context.set_current`, as it may
        perform additional initialisation functions.
        """
        raise NotImplementedError('abstract')
# ---
def _iter_speedrun_results(paths: list[Path]) -> Iterable[Path]:
    for path in paths:
        if path.is_dir():
            yield from path.rglob("speedrun_results.json")
        elif path.name == "speedrun_results.json":
            yield path
# ---
def endorsement(request, endorsement_id):
    endorsement = get_object_or_404(Endorsement, pk=endorsement_id)
    return locals()
# ---
def shutdown(self, wait: bool = True) -> None:
        """Shutdown the client.

        Args:
            wait: If True, wait for pending jobs to complete (local mode only)
        """
        self._cluster_client.shutdown(wait=wait)
# ---
def log_data_to_wandb(data):
    # Generate the week ID
    week_id = int(datetime.now().strftime("%W"))  # e.g., "02"
    project_name = "marin-monitoring"
    id_ = "weekly-metrics-final-final"
    wandb.init(project=project_name, id=id_, resume="allow")
    # Log the data for this week
    log_weekly_data(data=data, week_id=week_id, project_name=project_name, id_=id)
    wandb.finish()
# ---
def client_with_autoscaler(service_with_autoscaler):
    """Dashboard test client with autoscaler enabled."""
    dashboard = ControllerDashboard(service_with_autoscaler)
    return TestClient(dashboard._app)
# ---
def __init__(
            self, table, partition_name, schema="default",
            mysql_conn_id="metastore_mysql",
            *args, **kwargs):

        self.partition_name = partition_name
        self.table = table
        self.schema = schema
        self.first_poke = True
        self.conn_id = mysql_conn_id
        super(SqlSensor, self).__init__(*args, **kwargs)
# ---
def test_entrypoint_from_callable_resolve_roundtrip():
    ep = Entrypoint.from_callable(_add, 3, b=4)
    fn, args, kwargs = ep.resolve()
    assert fn(*args, **kwargs) == 7
# ---
def test_complex_arithmetic(self):
        # (a + b) * c
        expr = (col("a") + col("b")) * col("c")
        assert expr.evaluate({"a": 2, "b": 3, "c": 4}) == 20
# ---
def diff_offsets(offsets: np.ndarray):
            # fine to mutate since we have a copy
            # the array store has the number of rows in the 0th offset
            offsets[0] = 0
            return offsets[1:] - offsets[:-1]
# ---
def with_cpu(**kwargs) -> ResourceConfig:
        return ResourceConfig(device=CpuConfig(), **kwargs)
# ---
def enc8(x):
	if (x > 255):
		raise Exception("The integer %d cannot be encoded on 8 bits." % x)
	else:
		return x
# ---
def tracker(name):
            def go(conn, cursor, statement, parameters, context, executemany):
                canary.append((statement, context))
            return go
# ---
def list_all(self):
        self._call_all('list_all')
# ---
def is_dir_of_checkpoints(path):
    fs = fsspec.filesystem("gcs")
    # if the children are named like step-XXXXX, then it's a checkpoint directory
    children = fs.ls(path)
    return any("step-" in child for child in children)
# ---
def to_state_dict(self, prefix: Optional[str] = None) -> StateDict:
        # weight can be None for certain filtering things like LoRA
        scaled = dataclasses.replace(
            self, weight=self.weight * self.reparam.active_scale if self.weight is not None else None
        )
        return default_eqx_module_to_state_dict(scaled, prefix)
# ---
def model_type(self):  # noqa: D401
        return Qwen3LMHeadModel
# ---
def __init__(self):
        self._calls = CallList()
        self._server_mock = None
        self.directory = []
        self.exception = None
        self.reset()
# ---
def vm_ops(self) -> PlatformOps: ...
# ---
def test_load_file_auto_detects_vortex(self, sync_backend, vortex_file, tmp_path):
        """Test that load_file() auto-detects vortex format."""
        output_pattern = str(tmp_path / "output-{shard:05d}.jsonl.gz")

        ds = Dataset.from_files(str(vortex_file)).load_file().filter(lambda r: r["id"] < 10).write_jsonl(output_pattern)

        results = list(Backend.execute(ds, context=sync_backend))
        assert len(results) == 1
# ---
def _to_expr(value: Any) -> Expr:
    """Convert a value to an expression if not already one."""
    if isinstance(value, Expr):
        return value
    return LiteralExpr(value)
# ---
def mkdirs(path):
    """Create a directory and any necessary parent directories."""
    fs, path = fsspec.core.url_to_fs(path)
    fs.makedirs(path, exist_ok=True)
# ---
def cluster_update_configs(ctx):
    """Update all cluster configuration files from templates."""
    print("Updating cluster configuration files...")
    update_cluster_configs("infra/")
    print("Cluster configurations updated successfully!")
# ---
def __getattr__(self, method_name: str) -> ActorMethod: ...
# ---
def partition(self, tensor):
        """Partition tensor into blocks."""
        assert tensor.shape == self._shape
        tensors = [tensor]
        for i, indices in self._splits:
            tensors_local = []
            for t in tensors:
                tensors_local.extend(jnp.split(t, indices_or_sections=indices, axis=i))
            tensors = tensors_local
        return tuple(tensors)
# ---
def on_btn_del_row_clicked(self):
        min_row, max_row, _, _ = self.ui.tblFuzzingValues.selection_range()
        self.delete_lines(min_row, max_row)
# ---
def test_get_job_status_not_found(service):
    """Verify get_job_status raises ConnectError for unknown job."""
    request = cluster_pb2.Controller.GetJobStatusRequest(job_id=JobName.root("nonexistent").to_wire())

    with pytest.raises(ConnectError) as exc_info:
        service.get_job_status(request, None)

    assert exc_info.value.code == Code.NOT_FOUND
    assert "nonexistent" in exc_info.value.message
# ---
def test_actual_sizeof():
    d1 = {"a": 1, "b": 2}
    d2 = {"a": "this is a string", "b": "this is another string"}

    assert actual_sizeof(d1) < actual_sizeof(d2)
# ---
def __init__(self, entity_id, entity_type, ident):
        self.entity_id = int(entity_id)
        self.entity_type = entity_type
        self.ident = ident
# ---
def url(self) -> str: ...
# ---
def _jit_paged_decode(attn, x, pos_ids, cache: KvPageCache, binfo: PageBatchInfo) -> tuple[NamedArray, KvPageCache]:
    return attn.paged_decode(x, cache, binfo, pos_ids=pos_ids, key=jrandom.PRNGKey(2))
# ---
def test_format_shard_path_normalizes_double_slashes():
    """Test that double slashes are normalized in paths."""
    # Simulate what happens when output_path has trailing slash
    pattern = "gs://bucket/path//data-{shard:05d}-of-{total:05d}.jsonl"
    result = format_shard_path(pattern, 0, 10)
    assert result == "gs://bucket/path/data-00000-of-00010.jsonl"
    assert "//" not in result.replace("://", "")
# ---
def __init__(self, epoch_ms: int):
        self._epoch_ms = epoch_ms
# ---
def __init__(self, name=None, display_name=None, internal_name=None, to_be_exported_for_shoebox=None):
        super(Dimension, self).__init__()
        self.name = name
        self.display_name = display_name
        self.internal_name = internal_name
        self.to_be_exported_for_shoebox = to_be_exported_for_shoebox
# ---
def __init__(self, *args, **kwargs):
        Signature.__init__(self, *args, **kwargs)
# ---
def _maybe_fold_in_key(self, key, index):
        if key is not None:
            key = jax.random.fold_in(key, index)
        return key
# ---
def check_ruff(files: list[pathlib.Path], fix: bool) -> int:
    if not files:
        return 0

    click.echo("\nRuff linter:")
    args = ["uvx", "ruff@0.14.3", "check"]
    if fix:
        args.extend(["--fix", "--exit-non-zero-on-fix"])

    file_args = [str(f.relative_to(ROOT_DIR)) for f in files]
    args.extend(file_args)

    return run_cmd(args).returncode
# ---
def __init__(self, model_name: str, attribute_name: str, model_type: str | None, *args, **kwargs):
        super().__init__(model_name, attribute_name, model_type, *args, **kwargs)
# ---
def test_backup(self):
        assert os.path.exists(self.bakname)
        backup = open(self.bakname).read()
        assert_equals(self.original, backup)
# ---
def on_content(self, instance, value):
        Clock.schedule_once(lambda dt: self.ids.content.add_widget(value))
# ---
def create_job_base_data(data):
    return {
        'platform_id': data['saagie-platform'],
        'category': 'processing',
        'name': data['job-name'],
        'description': data['description'],
        'current': {
            'cpu': data['cpu'],
            'disk': data['disk'],
            'memory': data['ram'],
            'isInternalSubDomain': False,
            'isInternalPort': False,
            'options': {}
        }
    }
# ---
def testSmallMinRange(self):
        self.assertConfigureFails(HPCP(), {'minFrequency':1, 'splitFrequency':200})
# ---
def __init__(self, comodel_name=None, relation=None, column1=None, column2=None,
                 string=None, **kwargs):
        super(Many2many, self).__init__(
            comodel_name=comodel_name,
            relation=relation,
            column1=column1,
            column2=column2,
            string=string,
            **kwargs
        )
# ---
def _build_adjacency(node_id: RecordId, links: Iterator[tuple[RecordId, RecordId]]) -> CCNode:
    all_links = list(links)
    return CCNode(
        node_id=all_links[0][0],
        adjacency_list=list(set([link[1]["record_id_norm"] for link in all_links])),
        # init with own id as component
        component_id=node_id,
        changed=True,
    )
# ---
def clause(self):
        clause, subvals = self.subquery.clause()
        if clause:
            return f'not ({clause})', subvals
        else:
            # If there is no clause, there is nothing to negate. All the logic
            # is handled by match() for slow queries.
            return clause, subvals
# ---
def trapz(y, x=None, axis=-1):
    return _Ntrapz(y, x, axis=axis)
# ---
def click(self, on_element=None):
        """Clicks an element.
        Args:
            on_element: The element to click.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.CLICK, {'button': 0}))
        return self
# ---
def jbor_to_str(val):
    ans = get_job_from_jbor(val) + ':' + get_field_from_jbor(val)
    index = get_index_from_jbor(val)
    if index is not None:
        ans += "." + str(index)
    return ans
# ---
def test_position_from_token_non_position(tok):
    with pytest.raises(ValueError):
        tok.position_from_token(tok.sos_token_id)
# ---
def test_perturb_operators_no_ops_returns_none(rng):
    source = "x = 42"
    result = perturb_operators(source, rng, swap_prob=1.0)
    assert result is None
# ---
def set_learning_factor(self, learning_factor):
        assert(learning_factor >= 0 and learning_factor <= 1)
        self.lf = learning_factor
# ---
def date(self):
        '''Publication date. (optional)

        Must be given in "YYYY[-MM[-DD]]" format.'''
        try:
            return self.opt_meta['date']
        except KeyError:
            return None
# ---
def contains(self, bbox, srs):
        bbox = self._geom_in_coverage_srs(bbox, srs)
        with self._prep_lock:
            return self.prepared_geom.contains(bbox)
# ---
def body_finish(self):
        self.body = b"".join(self.body)
# ---
def remove_biblio(html: BeautifulSoup):
    # Remove the biblio since there is a lot of noise
    biblio = html.findAll("section", {"id": "bib"})
    for bib in biblio:
        bib.decompose()
# ---
def vm_count(appliance, metrics_tbl, mgmt_system_id):
    return bool(appliance.db.client.session.query(metrics_tbl).filter(
        metrics_tbl.parent_ems_id == mgmt_system_id).filter(
        metrics_tbl.resource_type == "VmOrTemplate").count()
    )
# ---
def arcsin(a: A) -> A:
    return wrap_elemwise_unary(jnp.arcsin, a)
# ---
def from_proto(cls, proto: "time_pb2.Duration") -> "Duration":
        """Create from proto Duration message."""
        return cls(proto.milliseconds)
# ---
def equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.equal](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.equal.html)
    """
    return jnp.equal(x1, x2)
# ---
def write_and_read(ref):
        ref[{"x": 1}] = 4.2
        return ref[{"x": 1}]
# ---
def _compute_top2_gap_on_device(logit_fn, model, batch: B, Vocab) -> jnp.ndarray:
    with jax.named_scope("logits"):
        logits = logit_fn(model, batch)
    gaps = top2_gap_from_logits(logits, axis=Vocab)
    return gaps.flatten("token").array
# ---
def test_olmo3_sliding_window_config():
    """Test sliding window configuration."""
    config = _get_olmo3_config(sliding_window=64)
    assert config.sliding_window == 64

    hf_config = config.to_hf_config(vocab_size=50304)
    assert hf_config.sliding_window == 64
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Estimated Finish Time", device_id)
# ---
def obj_fun(trainable_model):
            model = eqx.combine(trainable_model, state.model)
            with hax.axis_mapping(self.compute_axis_mapping):
                model = self.mp.cast_to_compute(model)
                result = self._raw_loss_function(model, *batch, **batch_kwargs, key=key)
                # result is (loss, metrics) tuple
                loss_for_opt, _metrics = result
                return loss_for_opt.scalar()
# ---
def bmarks():
    return_data = get_edit_tags()
    return return_data
# ---
def _get_nnx_key_name(split_key: list[str]) -> str:
    """
    Determine the NNX key name from the split Levanter key.
    If the key ends in 'bias', append '_bias' to the parameter name.
    Otherwise (e.g. 'weight'), use the parameter name directly.
    """
    key_name = split_key[-2]
    if split_key[-1] == "bias":
        key_name = f"{key_name}_bias"
    return key_name
# ---
def read_volts(self, channel):
        """Read the ADC value from channel and convert to volts, assuming that Vref is set correctly. """
        return self._Vref * self.read_adc(channel) / self.get_value_max()
# ---
def filter_by(self, key, value, multiple=False):
        Util.validate_type(key, "str")
        Util.validate_type(multiple, "bool")
        return self._filter_by(key, value, multiple)
# ---
def _make_padding_example(ex: Ex) -> Ex:
    with local_cpu_mesh():
        return tree_zeros_like(ex)
# ---
def increment(self, amount: int = 1) -> int:
        self._value += amount
        return self._value
# ---
def __init__(self, opts):
        self.opts = opts
        kind = self.opts.get('__role', '')  # application kind
        if kind not in kinds.APPL_KINDS:
            emsg = ("Invalid application kind = '{0}'.".format(kind))
            log.error(emsg + '\n')
            raise ValueError(emsg)
        self.event = salt.utils.event.get_event(
                kind,
                opts['sock_dir'],
                opts['transport'],
                opts=opts,
                listen=False)
# ---
def test_pspec_for_plain_array_axis_names():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        mod = ArrayModule(jnp.ones((Dim2.size, Dim3.size)))

        specs: ArrayModule = pspec_for(mod)

        assert specs.arr == PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)
# ---
def test_core_event_deco(self):
        @event.core_event
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.priority is event.Priority.CORE
# ---
def output_nolink(self, m):
        key = _keyify(m.group(1))
        if key not in self.links:
            return None
        ret = self.links[key]
        return self._process_link(m, ret['link'], ret['title'])
# ---
def get_drinker_list():
    data = urllib2.urlopen("http://192.168.11.5:8080/drinkcounter/get_datas/").read().split("\n")
    drinkers = []

    for data_row in data:
        if data_row == '': continue

        fields = data_row.split('|')

        drinker = Drinker()
        drinker.id = int(fields[0])
        drinker.name = fields[1]
        drinker.drinks = int(fields[2])
        drinker.prom = float(fields[3])
        drinker.idle = fields[4]

        drinkers.append(drinker)

    return drinkers
# ---
def create(self, model: M) -> EmaDecaySqrtModelAveraging[M]:
        return EmaDecaySqrtModelAveraging(
            model=model,
            beta=self.beta,
            switch_step=self.switch_step,
            decay_steps=self.decay_steps,
        )
# ---
def _go(conn):
            assert_raises_message(
                MyException,
                "nope",
                conn.execute,
                    select([1]).\
                        where(
                            column('foo') == literal('bar', MyType())
                        )
            )
# ---
def from_single_ref(ref: Any, context: JobContext, idx: int, count: int) -> Shard:
        """Wrap a single ref as a Shard.

        Args:
            ref: Reference to wrap (type depends on context)
            context: Execution context for get operations
            idx: Shard index
            count: Number of items in the ref

        Returns:
            Shard containing the single ref
        """
        return Shard(idx=idx, chunks=[Chunk(count=count, data=ref)], context=context)
# ---
def _expand_and_check_shape(expected_len: int, spec: T | Sequence[T], name: str) -> tuple[T, ...]:
    spec = ensure_tuple(spec)
    if len(spec) == 1:
        spec = spec * expected_len
    if len(spec) != expected_len:
        raise ValueError(f"Expected {expected_len} elements for {name}, got {len(spec)}")

    return spec
# ---
def _record_id(record: dict) -> str:
    if "id" in record:
        return record["id"]
    else:
        # compute hash of the msgspec serialization of the record
        s = msgspec.msgpack.encode(record, order="deterministic")
        return str(_bloom_hash(s))
# ---
def _fake_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            timespec[0] = goal_timespec[0]
            return 0
# ---
def test_get_job_status_returns_status(service, job_request):
    """Verify get_job_status returns correct status for launched job."""
    service.launch_job(job_request("test-job"), None)

    request = cluster_pb2.Controller.GetJobStatusRequest(job_id=JobName.root("test-job").to_wire())
    response = service.get_job_status(request, None)

    assert response.job.job_id == JobName.root("test-job").to_wire()
    assert response.job.state == cluster_pb2.JOB_STATE_PENDING
# ---
def createLink( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def send_font(filename):
    return static_file(filename, root='fonts')
# ---
def set_dirty(self):
        self.dirty = True
# ---
def test_order_by_selectable_in_unions(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2).order_by(table.c.id)
        s2 = select([table]).where(table.c.id == 3).order_by(table.c.id)

        u1 = union(s1, s2).limit(2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def generate_fun_name(zone: str) -> str:
    """
    Generate a fun VM name in wandb style.

    Format: {verb}-{noun}-{zone}-{random_id}
    Example: running-tiger-us-west4-a-7x3k
    """
    import random

    verb = random.choice(VERBS)
    noun = random.choice(NOUNS)
    random_id = generate_random_id()
    return f"{verb}-{noun}-{zone}-{random_id}"
# ---
def flatten_axes(array: NamedArray, old_axes: AxisSelection, new_axis: AxisSelector) -> NamedArray:
    pass
# ---
def _shape_bucket(b: int, h: int, v: int) -> Optional[str]:
    for bucket in SHAPE_BUCKETS:
        if bucket.matches(b, h, v):
            return bucket.name
    return None
# ---
def _transform(item, meta=dataset_meta):
            return wrap_transform(item, meta, cfg)
# ---
def run_gcloud_command(cmd: list[str], **kwargs) -> subprocess.CompletedProcess:
    """Run a gcloud command with error handling."""
    try:
        logger.info(f"Running {' '.join(cmd)}")
        return subprocess.run(cmd, check=True, capture_output=True, text=True, **kwargs)
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"gcloud command failed: {' '.join(cmd)}\nError: {e.stderr}") from e
# ---
def local_cpu_mesh():
    """Temporarily sets the default device to CPU and creates a mesh with a single CPU device"""
    cpu = jax.local_devices(backend="cpu")[0]
    mesh = create_mesh_from_axis_specs(
        ici_axes={
            ResourceAxis.REPLICA: 1,
            ResourceAxis.DATA: 1,
            ResourceAxis.MODEL: 1,
            ResourceAxis.CONTEXT: 1,
        },
        dcn_axes={},
        devices=[cpu],
    )
    with use_cpu_device(), haliax.partitioning.set_mesh(mesh):
        yield mesh
# ---
def set_content(content):
    xbmcplugin.setContent(int(sys.argv[1]), content)
# ---
def tearDown(self):
        Engine.dispatch._clear()
        Engine._has_events = False
# ---
def test_axis_shapes_multiple_absorbers_error():
    cfg = MeshConfig(axes={"data": -1, "model": -1})
    with pytest.raises(ValueError):
        cfg.axis_shapes(num_devices=8, num_slices=1)
# ---
def _change_state(self, state):
        """ Helper method for changing state """
        self.table_entry.state = state
        self.creator_admin.save_model(self.request, self.table_entry, None, True)
# ---
def __init__(self):
        self._server_info = None
# ---
def mk_LayerNorm(self, axis: AxisSpec) -> LayerNormBase:
        return hnn.RmsNorm.init(
            axis, eps=self.layer_norm_epsilon, use_weight=self.use_layer_norm_weight, use_bias=self.use_bias
        )
# ---
def footnote_ref(self, key, index):
        """Rendering the ref anchor of a footnote.

        :param key: identity key for the footnote.
        :param index: the index count of current footnote.
        """
        html = (
            '<sup class="footnote-ref" id="fnref-%s">'
            '<a href="#fn-%s" rel="footnote">%d</a></sup>'
        ) % (escape(key), escape(key), index)
        return html
# ---
def _add_block(self, block, batch=None, check_stale=True) -> (bool, bool):
        self.trigger_miner = False

        block_size_limit = self.get_block_size_limit(block)
        if block_size_limit and block.size > block_size_limit:
            logger.info('Block Size greater than threshold limit %s > %s', block.size, block_size_limit)
            return False, False

        return self._try_branch_add_block(block, batch, check_stale)
# ---
def test_horizontal_retrace_toggles(self):
        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF0)
        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF1)
        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF0)
# ---
def test_kml_equals_lon_lat(self):
        """Asserts that the return value of to_kml() property is identical to
        the return value of to_lon_lat()."""
        assert_equal(self.polycircle.to_kml(), self.polycircle.to_lon_lat())
# ---
def do_block(carry: CarryT, block: M, *args, **kwargs) -> CarryT:
            return fn(block, carry, *args, **kwargs)
# ---
def obj_code(self):
                if Field('label')(self) == "LIQUIDITES":
                    return 'XX-liquidity'
                code = CleanText(TableCell('code'))(self)
                return code if is_isin_valid(code) else NotAvailable
# ---
def fsspec_dir_only_contains_files(dir_path):
    """
    Check if a directory only contains files in a fsspec filesystem.
    """
    fs, _ = fsspec.core.url_to_fs(dir_path)
    ls_res = fs.ls(dir_path, detail=True)
    if len(ls_res) == 0:
        return False
    return all(item["type"] == "file" for item in ls_res)
# ---
def resource_setup(cls):
        super(VolumesActionsTest, cls).resource_setup()

        # Create a test shared volume for attach/detach tests
        cls.volume = cls.create_volume()
# ---
def body(i, cmap):
            tid = target_slot_ids["position", i].scalar()

            def do(c):
                return c.at["seq", tid].set(INVALID)

            return jax.lax.cond(is_valid(tid), do, lambda c: c, cmap)
# ---
def __iter__(self):
        return (key for key in self._dataset._variables
                if key not in self._dataset._coord_names)
# ---
def skip_if_no_soundlibs(f):
    return pytest.mark.skipif(not has_soundlibs(), reason="soundfile/librosa not installed")(f)
# ---
def create_process(self, process_id, vpnservice, namespace):
        return ipsec.OpenSwanProcess(
            self.conf,
            process_id,
            vpnservice,
            namespace)
# ---
def bitwise_right_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_right_shift](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_right_shift.html)
    """
    return jnp.bitwise_right_shift(x1, x2)
# ---
def from_job_id(cls, job_id: JobName) -> "Namespace":
        """Derive namespace from hierarchical job ID.

        The namespace is the first component of the job ID hierarchy.
        For example:
            JobName.from_string("/abc123/worker-0") -> Namespace("abc123")

        Args:
            job_id: Hierarchical job ID

        Returns:
            Namespace derived from root job ID

        Raises:
            ValueError: If job_id is empty
        """
        return cls(job_id.namespace)
# ---
def _fileset(self):
        return set(self._files)
# ---
def config(self):
        return self.encoder.config
# ---
def _stop_daemon(self):
        ovn_nbctl = self._get_ovn_controller(self.install_method)
        ovn_nbctl.stop_daemon()
# ---
def __init__(self):
        self.explore_rate = self.MAX_EXPLORATION_RATE
        self.brain = Brain()
        self.memory = Memory(self.MEMORY_CAPACITY)
        self.steps = 0
# ---
def absent(self):
        if self.exists():
            return self.remove()
        return False
# ---
def prepopulate(self):
        pass
# ---
def __init__(self):

        player.__init__(self)
        self.experiences = deque()
# ---
def mesh_axis_specs(self) -> List[str]:
        """Materialized mesh axis names; validates mesh config."""
        ici, dcn = self.mesh.axis_shapes(jax.device_count(), self.num_slices)
        return list(ici.keys() | dcn.keys())
# ---
def test_capsysbinary(self, testdir):
        reprec = testdir.inline_runsource("""
            def test_hello(capsysbinary):
                import sys
                # some likely un-decodable bytes
                sys.stdout.buffer.write(b'\\xfe\\x98\\x20')
                out, err = capsysbinary.readouterr()
                assert out == b'\\xfe\\x98\\x20'
                assert err == b''
        """)
        reprec.assertoutcome(passed=1)
# ---
def autoscaler(self) -> "Autoscaler | None":
        """The autoscaler instance, if autoscaling is enabled."""
        return self._autoscaler
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            Ac = df.A.cumsum()
            return Ac.sum()
# ---
def as_formatted_date(self) -> str:
        """Format as ISO 8601 string in UTC."""
        dt = datetime.fromtimestamp(self.epoch_seconds(), tz=timezone.utc)
        return dt.isoformat()
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "data": "collate data1"},
                {"id": 2, "data": "collate data2"},
            ],
        )
# ---
def go(*args, **kw):
                canary1.append(name)
# ---
def _process_loop(self):
        """Worker thread processes tokens from queue."""
        while True:
            token = self._queue.get()
            try:
                self._handle_token(token)
            except Exception as e:
                print(f"[{self.actor_id}] Error processing token: {e}")
# ---
def _deDuplicate(results):
        found = dict()
        deDuped = list()
        for entry in results:
            dn = entry.get("dn")
            if not dn in found:
                found[dn] = 1
                deDuped.append(entry)

        return deDuped
# ---
def __call__(self, target, cred):
        """Check the policy.

        Requires that all rules accept in order to return True.
        """

        for rule in self.rules:
            if not rule(target, cred):
                return False

        return True
# ---
def init(In: hax.Axis, Out: hax.Axis, Mid: hax.Axis, *, key):
        w_in = hax.random.normal(key, hax.concat_axis_specs(In, Mid)) * 0.02
        w_out = hax.random.normal(key, hax.concat_axis_specs(Mid, Out)) * 0.02
        return Mlp(w_in, w_out, In, Out, Mid)
# ---
def mock_requests_get(url, **kwargs):
        from unittest.mock import Mock

        response = Mock()
        response.status_code = 200
        if "file1" in url:
            response.headers = {"content-length": str(len(file1_data))}
            response.raw = BytesIO(file1_data)
        else:
            response.headers = {"content-length": str(len(file2_data))}
            response.raw = BytesIO(file2_data)
        return response
# ---
def pack_next_sequence(self, max_tokens: int) -> tuple["DecodeState", PackedSequence]:  # type: ignore[name-defined]
        """Forward ``pack_next_sequence`` to ``TokenQueue`` and return updated ``DecodeState`` plus the ``PackedSequence``."""
        new_tqueue, packed = self.tqueue.pack_next_sequence(max_tokens)
        return dataclasses.replace(self, tqueue=new_tqueue), packed
# ---
def test_retrieval(self, expected, actual):
        assert expected == actual
# ---
def display(self):
        image(self.im, self.x, self.y)
# ---
def reset(self) -> None:
        """Reset rate limiter to allow immediate run."""
        self._last_run = None
# ---
def test_vm_detail_page_escapes_vm_id(client):
    """VM detail page escapes the VM ID to prevent XSS."""
    response = client.get('/vm/"onmouseover="alert(1)')
    assert response.status_code == 200
    assert "onmouseover" not in response.text or "&quot;" in response.text
# ---
def forwards(self, orm):
        # Adding field 'Idea.color'
        db.add_column(u'brainstorming_idea', 'color',
                      self.gf('django.db.models.fields.CharField')(default='', max_length=100, blank=True),
                      keep_default=False)
# ---
def _filter_blacklist(package):
        blacklist = ["-i", "#", "Python==", "python-lambda=="]
        return all(package.startswith(entry) is False for entry in blacklist)
# ---
def __init__(self):
        self.transfer_server_address = None  # Actual JAX transfer server address
        self._requested_transfers = []
        self._lock = asyncio.Lock()
        self._latest_weight_id = None
        self._pending_completion = {}
        self._transfer_id = 0
# ---
def module_org():
    return entities.Organization().create()
# ---
def d_attn(qkv, fn):
        q, k, v = qkv
        x_out = fn(QPos, KPos, Key, q, k, v, mask=mask)
        return (x_out * x_out).mean().scalar()
# ---
def test_formatted_string_table(datadir):
    datadir.join('reader').chdir()
    src = 'shared-strings-rich.xml'
    with open(src) as content:
        assert read_string_table(content.read()) == [
            'Welcome', 'to the best shop in town', "     let's play "]
# ---
def delete_image(self, image_id):
        """Deletes the provided image."""
        resp, body = self.delete("images/%s" % str(image_id))
        self.validate_response(schema.delete, resp, body)
        return service_client.ResponseBody(resp, body)
# ---
def test_sharded_tree_size_shape_dtype_struct_with_named_sharding():
    mesh = jax.sharding.AbstractMesh((4,), ("data",))
    spec = jax.sharding.PartitionSpec("data", None)
    sharding = jax.sharding.NamedSharding(mesh, spec)

    struct = jax.ShapeDtypeStruct((8, 4), jnp.float32, sharding=sharding)

    per_device_bytes = sharded_tree_size(struct, mesh=mesh)

    assert per_device_bytes == (8 * 4 * jnp.dtype(jnp.float32).itemsize) // 4
# ---
def test_init_options(self, mocker):
        constructor = mocker.spy(pymemcache.client, "PooledClient")
        assert storage_from_string(self.storage_url, connect_timeout=1).check()
        assert constructor.call_args[1]["connect_timeout"] == 1
# ---
def _is_lora_compatible_module(module):
    # TODO: more modules
    return isinstance(module, hnn.Linear)
# ---
def get_group(self, name: str) -> ScalingGroup | None:
        """Get a scale group by name."""
        return self._groups.get(name)
# ---
def test_maybe_reduce_loss_weighted_sum():
    Batch, Pos = hax.make_axes(Batch=2, Pos=2)
    arr = hax.named(jnp.array([[1.0, 2.0], [3.0, 4.0]]), (Batch, Pos))
    weights = hax.named(jnp.array([[0.5, 1.5], [1.0, 0.0]], dtype=jnp.float32), (Batch, Pos))

    reduced = maybe_reduce_loss(arr, hax.sum, Pos, where=None, weight=weights)

    expected = jnp.array(
        [
            1.0 * 0.5 + 2.0 * 1.5,
            3.0 * 1.0 + 4.0 * 0.0,
        ]
    )
    assert jnp.allclose(reduced.array, expected)
# ---
def check_master(self):
        '''
        Log if the master is not running

        :rtype: bool
        :return: Whether or not the master is running
        '''
        if not os.path.exists(
                os.path.join(
                    self.opts['sock_dir'],
                    'publish_pull.ipc'
                    )
                ):
            return False
        return True
# ---
def test_permutation_creates_valid_instance():
    length = 100
    prng_key = jrandom.PRNGKey(0)
    permutation = LcgPermutation(length, prng_key)
    assert permutation.length == length
    assert 0 < permutation.a < length
    assert 0 <= permutation.b < length
# ---
def with_output(self, x, static1, *, static2):
            assert static1 == 1.0
            assert static2 is False
            out = x + self.w + static1
            return out, 2 * self.w
# ---
def loss_and_metrics(params: GrugModelParameters, batch: dict[str, jax.Array]):
        logits = forward(params, batch["tokens"], model_cfg, mask=None)
        loss = cross_entropy_loss(logits, batch["labels"])
        metrics = {"loss": loss, "ppl": jnp.exp(loss)}
        return loss, metrics
# ---
def _presentMessage(self, text, interrupt=False):
        """If the text field is not None, presents the given text, optionally
        interrupting anything currently being spoken.

        Arguments:
        - text: the text to present
        - interrupt: if True, interrupt any speech currently being spoken
        """

        self.script.speakMessage(text, interrupt=interrupt)
        try:
            self.script.displayBrailleMessage(text, flashTime=-1)
        except:
            pass
# ---
def mock_open_for_remote(path, mode="r", **kwargs):
        # Intercept only the remote paths file download
        if "data.commoncrawl.org" in path and "data-jsonl.paths.gz" in path:
            return paths_file.open("rb")
        return original_open(path, mode, **kwargs)
# ---
def test_disclaimer(self):
        eq_(self.record.disclaimer, None)
# ---
def the_object_name_is_not_a_filling(name):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(element.FillsVoids):
        assert False, "A filling was found"
# ---
def serialize(self, with_witness=False):
        r = b""
        r += super(CBlock, self).serialize()
        r += struct.pack("<BQ", 255, len(self.vtx))
        for tx in self.vtx:
            if with_witness:
                r += tx.serialize_with_witness()
            else:
                r += tx.serialize_without_witness()
        return r
# ---
def _get_next_endpoint(self) -> ResolvedEndpoint:
        """Get the next endpoint in round-robin order.

        Thread-safe: uses a lock to protect the endpoint index.
        """
        endpoints = self._resolve().endpoints
        with self._lock:
            if not endpoints:
                raise RuntimeError(f"No endpoints for '{self._name}'")
            endpoint = endpoints[self._endpoint_index % len(endpoints)]
            self._endpoint_index += 1
            return endpoint
# ---
def test___cmp__ne(self):
        self._test__cmp__(
            lambda left, right: left != right,
            (
                True,
                False,
                False,
                True,
                False,
                True,
                PY3,
                True,
                True,
                True,
            ),
            '!='
        )
# ---
def test_remove(self):
        prio_set_list = event._PrioritizedSetList()
        obj = (1,)

        prio_set_list.add(1, obj)
        assert prio_set_list
        prio_set_list.remove(obj)
        assert not prio_set_list

        with pytest.raises(ValueError) as excinfo:
            prio_set_list.remove(obj)
        excinfo.match(r"can not be found")
# ---
def __getitem__(self, key):
        return self._data[id(key)][1]
# ---
def __init__(self, ssh_config: SshConfig, bootstrap: config_pb2.BootstrapConfig):
        self._ssh_config = ssh_config
        self._bootstrap = bootstrap
# ---
def __init__(
        self,
        dataset: AsyncDataset[T],
        fn: MapFunction[Sequence[U]],
        *extra_args,
        **extra_kwargs,
    ):
        super().__init__()
        self.dataset = dataset
        self.fn = fn
        self._extra_args = extra_args
        self._extra_kwargs = extra_kwargs
# ---
def lista_valores(self):
        v  = [self.__emisornombre,self.__fechatimbrado, self.__tipo, self.__emisorrfc ]
        v += [self.__uuid, self.__folio, self.__receptornombre, self.__receptorrfc ]
        v += [self.__subtotal, self.__trieps, self.__triva]
        v += [self.__retiva, self.__retisr, self.__tcambio, self.__total]
        return v
# ---
def unsliced(self) -> "NamedRef":
        """Return a view of the original reference without staged selectors."""
        full_prefix = tuple(slice(None) for _ in self._axes)
        if self._prefix == full_prefix:
            return self
        return NamedRef(self._ref, self._axes, full_prefix)
# ---
def visitdir(self, dir):
        m1dir = self.m1.visitdir(dir)
        m2dir = self.m2.visitdir(dir)

        # if both matchers return "all" then we know for sure we don't need
        # to visit this directory. Same if all matchers return False. In all
        # other case we have to visit a directory.
        if m1dir == "all" and m2dir == "all":
            return False
        if not m1dir and not m2dir:
            return False
        return True
# ---
def progress(self):
        '''
        progress is part of multiple disk_progress its
        flat and not 100% accurate - each disk take its
        portion ie if we have 2 disks the first will take
        0-50 and the second 50-100
        '''
        completed = (self._disk_count - 1) * 100
        return (completed + self._disk_progress) / self._disk_count
# ---
def _step(key: jax.Array) -> dict[str, jax.Array]:
        tokens = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)
        return {"tokens": tokens[:, :-1], "labels": tokens[:, 1:]}
# ---
def test_equal_3(self):
        self.assertEqual(string_color('Joshua Smith'), '8F00FB')
# ---
def all(
        self, axis: AxisSelection | None = None, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.all(self, axis=axis, where=where)
# ---
def test_connect_to_host_without_session(self):
        """Can connect to a dynamo host without passing in a session"""
        conn = DynamoDBConnection.connect("us-west-1", host="localhost")
        self.assertIsNotNone(conn.host)
# ---
def __init__(self):
        self.stdin_path = '/dev/null'
        self.stdout_path = settings.LOG_PATH + '/webapp.log'
        self.stderr_path = settings.LOG_PATH + '/webapp.log'
        self.pidfile_path = settings.PID_PATH + '/webapp.pid'
        self.pidfile_timeout = 5
# ---
def from_proto(cls, proto: cluster_pb2.Worker.LogEntry) -> "LogEntry":
        return cls(
            timestamp=Timestamp.from_proto(proto.timestamp),
            source=proto.source,
            data=proto.data,
        )
# ---
def find_free_port(start_port: int = 9000, max_attempts: int = 1000) -> int:
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(("", port))
                return port
        except OSError:
            continue
    raise RuntimeError(f"No free port found in range {start_port}-{start_port + max_attempts}")
# ---
def dict_to_str(d):
    """
    Given a dictionary d, return a string with
    each entry in the form 'key: value' and entries
    separated by newlines.
    """
    vals = []
    for k in d.keys():
        vals.append('{}: {}'.format(k, d[k]))
    v = '\n'.join(vals)
    return v
# ---
def testImportNativeType(self):
    self.assertEqual((0, "<type 'Duration'>\n"), _GrumpRun(textwrap.dedent("""\
        from "__go__/time" import Duration
        print Duration""")))
# ---
def __del__(self):
        """ Cleanup the GPIO before being destroyed """
        if self._MOSI > 0:
            GPIO.cleanup(self._CS_bar)
            GPIO.cleanup(self._CLK)
            GPIO.cleanup(self._MOSI)
            GPIO.cleanup(self._MISO)
# ---
def _encode_options(options):
        return ','.join('{}={}'.format(str(key), str(val)) for key, val in options.items())
# ---
def __call__(self, indices: np.ndarray) -> np.ndarray:
        """Apply the permutation to an array of integers.

        Args:
            indices: An array of integers to be permuted.

        Returns:
            The permuted value(s).
        """
        ...
# ---
def ring_buffer():
    return LogRingBuffer(maxlen=10)
# ---
def testDeleteSubscript(self):
    self.assertEqual((0, '{}\n'), _GrumpRun(textwrap.dedent("""\
        foo = {'bar': 'baz'}
        del foo['bar']
        print foo""")))
# ---
def testExprNameGlobal(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        foo = 42
        foo""")))
# ---
def causal(cls, *, sliding_window: int | None = None) -> "AttentionMask":
        return cls(is_causal=True, sliding_window=sliding_window)
# ---
def release(self, lease: Lease[T_co]) -> None:
        """Release a lease and requeue the item for reprocessing."""
        ...
# ---
def handle404(error):
    return '<H1>Ooops, its not here<BR>'
# ---
def shouldnt_run_fn(cfg: DummyCfg):
    raise RuntimeError("This function should not run.")
# ---
def to_dict(self):
        return {
            'ident': self.ident,
            'entity_id': self.entity_id,
            'entity_type': self.entity_type, }
# ---
def replace_dot_general(x):
        if isinstance(x, DefaultDotGeneralOp):
            return PreciseDotGeneralOp()
        return x
# ---
def job_id(self) -> str:
        return str(self._job.job_id)
# ---
def assert_span(span, operation, parent=None):
    assert span.operation_name == 'MySQLdb:' + operation
    assert span.tags.get(tags.SPAN_KIND) == tags.SPAN_KIND_RPC_CLIENT
    if parent:
        assert span.parent_id == parent.context.span_id
        assert span.context.trace_id == parent.context.trace_id
    else:
        assert span.parent_id is None
# ---
def test_and_both_true(self):
        expr = (col("a") > 0) & (col("b") > 0)
        assert expr.evaluate({"a": 1, "b": 1}) is True
# ---
def _parse_periods(pattern):
    """Parse a string containing two dates separated by two dots (..).
    Return a pair of `Period` objects.
    """
    parts = pattern.split('..', 1)
    if len(parts) == 1:
        instant = Period.parse(parts[0])
        return (instant, instant)
    else:
        start = Period.parse(parts[0])
        end = Period.parse(parts[1])
        return (start, end)
# ---
def name_get(self, cr, uid, ids, context=None):
        res = []
        for line in self.browse(cr, uid, ids, context=context):
            name = line.location_id.name+' > '+line.location_dest_id.name
            # optional prefixes
            if line.product_id.code:
                name = line.product_id.code + ': ' + name
            if line.picking_id.origin:
                name = line.picking_id.origin + '/ ' + name
            res.append((line.id, name))
        return res
# ---
def convert_to_onchange(self, value):
        return value.id
# ---
def has_len(self) -> bool:
        """
        Whether the data store currently has a known length. If this returns False, then the length of the data store
        may change in the future.
        """
        pass
# ---
def test_repeat_vector(self):
    testing_utils.layer_test(
        keras.layers.RepeatVector, kwargs={'n': 3}, input_shape=(3, 2))
# ---
def native_value(self):
        """Return sensor state."""
        job: OctoprintJobInfo = self.coordinator.data["job"]
        if not job:
            return None

        if not (state := job.progress.completion):
            return 0

        return round(state, 2)
# ---
def sources(self) -> Mapping[str, LmDatasetSourceConfigBase]:
        sources: dict[str, LmDatasetSourceConfigBase] = {}
        for name, comp in self.components.items():
            if isinstance(comp, DatasetComponent) and comp.source is not None:
                sources[name] = comp.source
        return sources
# ---
def test_nofiles(self):
        self.assertEqual(0, len(tecautils.filterImages([], self.conf)))
# ---
def device_address(self):
        if self._values['managed']:
            return None
        return self._values['device_address']
# ---
def _make_candidate(source: str, score: float = 0.0) -> BeamCandidate:
    return BeamCandidate(source=source, score=score, depth=0, edits=())
# ---
def exit_submission(self):
        "Close the submission and return to the subreddit page"

        self.active = False
# ---
def __getitem__(self, field):
        return getattr(self, field, None)
# ---
def scan_fun(acc, x, static1, *, static2):
        assert static1 is True
        assert static2 is False
        return acc + jnp.sum(x.array), x.take(Width, 2)
# ---
def fold(self, *args, **kwargs): ...
# ---
def testClassDefWithVar(self):
    self.assertEqual((0, 'abc\n'), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          bar = 'abc'
        print Foo.bar""")))
# ---
def vmap_fun2(x):
        return x.sum(Width).array
# ---
def count(self, eventtype):
        """Tells how many events have been counted of the specified type

        Args:
            eventtype (:obj:`baroque.entities.eventtype.EventType`): the type of events to be counted

        Returns:
            int

        """
        return self.events_count_by_type.get(type(eventtype), 0)
# ---
def convert_to_display_name(self, value, record=None):
        raise NotImplementedError()
# ---
def __nameToWordnetCode(self, name):
        """
        It returns the wordnet code for a given language name
        """
        if not self.__isLanguageAvailable(language_name=name):
            raise Exception("Wordnet code not found for the language name %s " % name)
        name = name.lower()
        languageShortCode = AVAILABLE_LANGUAGES_NAMES[name]

        wordnetCode = self.__shortCodeToWordnetCode(code=languageShortCode)
        return wordnetCode
# ---
def square(a: A) -> A:
    return wrap_elemwise_unary(jnp.square, a)
# ---
def allocate_ports():
        ports = allocator.allocate(count=5)
        results.append(ports)
# ---
def test_1D_Var_len(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n), 'B': np.arange(n) + 1.0})
            df1 = df[df.A > 5]
            return len(df1.B)

        hpat_func = self.jit(test_impl)
        n = 11
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def test_weibull_min():
    scale = hax.arange(Width, start=0.1)
    concentration = hax.arange(Height, start=0.1)

    check_gen_is_equal(
        lambda k, s: jax.random.weibull_min(
            k, scale.array.reshape(1, -1), concentration.array.reshape(-1, 1), shape=s
        ),
        lambda k, s: hax.random.weibull_min(k, s, scale, concentration),
    )
# ---
def _update_current_step(info: levanter.callbacks.StepInfo):
            self.replay_buffer.set_current_step(info.step)
# ---
def _create_from_content(self, rname, content):
        '''create a temporary file and then call oc create on it'''
        fname = Utils.create_tmpfile(rname + '-')
        yed = Yedit(fname, content=content)
        yed.write()

        atexit.register(Utils.cleanup, [fname])

        return self._create(fname)
# ---
def disconnect(self):
        self.worker.quit()
        self.serial.close()
        self.connectAction.setText(self.tr("Connect"))
        self.connectAction.setIcon(QIcon(pixmap("network-connect-3.png")))
        self.serialdlgAction.setEnabled(True)
        self.connectionstateLabel.setText(self.tr("Not connected"))
        self._connected = False
        self.objectexplorer.refresh()
# ---
def test_count_add_none_capacity(self):
        """Count addition with one None consumed_capacity"""
        cap = Capacity(3, 0)
        count = Count(4, 2)
        count2 = Count(5, 3, cap)
        ret = count + count2
        self.assertEqual(ret, 9)
        self.assertEqual(ret.scanned_count, 5)
        self.assertEqual(ret.consumed_capacity, cap)
# ---
def absolute(a: A) -> A:
    return wrap_elemwise_unary(jnp.absolute, a)
# ---
def _initialize_actors(self):
        """Initialize the minimum number of actors."""
        logger.info(f"Initializing {self.min_actors} actors...")
        for _ in range(self.min_actors):
            self._create_and_register_actor()
        logger.info(f"Initialized {len(self.actors)} actors")
# ---
def test_validate_success_login_form(self):
        # Ensure correct data validates.
        form = LoginForm(email="ad@min.com", password="admin_user")
        self.assertTrue(form.validate())
# ---
def get_init_log(self, vm_id: str, tail: int | None = None) -> str:
        """Get initialization log for a VM."""
        return self._vm_registry.get_init_log(vm_id, tail)
# ---
def test_exceed_max_pack_size():
    Pos = hax.Axis("pos", size=10)
    packer = SequencePacker(Pos=Pos, max_pack_size=2, pad_token=0)

    packer.add_example([1, 2, 3], [1, 1, 1])
    packer.add_example([4, 5, 6], [1, 1, 1])

    with pytest.raises(ValueError, match="Too many segments"):
        packer.add_example([7, 8], [1, 1])
# ---
def test_shard_map_decorator_no_kwargs():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    @hax.shard_map
    def fn(x):
        return x - 1

    x = hax.ones(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), hax.partitioning.set_mesh(mesh):
        out = fn(x)

    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array - 1)
# ---
def test_gpu_device(self):
        resources = ResourceConfig(device=GpuConfig(variant="H100", count=8))
        spec = convert_resources(resources)
        assert spec.device is not None
        assert spec.device.HasField("gpu")
        assert spec.device.gpu.variant == "H100"
        assert spec.device.gpu.count == 8
# ---
def test_gemma_param_counts_dont_change_with_seqlen():
    model = GemmaLMHeadModel.init(hax.Axis("v", 2048), _get_gemma_config(seq_len=128), key=random.PRNGKey(0))
    model2 = GemmaLMHeadModel.init(hax.Axis("v", 2048), _get_gemma_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def fn_pass(config: MyConfig | None):
        append_log(log, config)
# ---
def remember(self, request, principal, *kw):
		return []
# ---
def test_submit_callable_failure(client: LocalClient):
    handle = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    status = handle.wait(raise_on_failure=False)
    assert status == JobStatus.FAILED
# ---
def present(self):
        if self.exists():
            return False
        return self.create()
# ---
def __len__(self):
        return len(self._dict)
# ---
def trainer_config():
    """Create a basic trainer config for tests."""
    return TrainerConfig(
        id="test-run",
        checkpointer=CheckpointerConfig(),
        ray=RayConfig(),
    )
# ---
def repeat(
    a: NamedArray, repeats: int | jnp.ndarray, axis: AxisSelector, total_repeat_length: int | None = None
) -> NamedArray:
    """Version of [jax.numpy.repeat][] that returns a NamedArray"""
    index = a.axis_indices(axis)
    if index is None:
        raise ValueError(f"Axis {axis} not found in array {a}")

    return named(
        jnp.repeat(a.array, repeats, axis=index, total_repeat_length=total_repeat_length),
        a.axes[:index] + (axis_name(axis),) + a.axes[index + 1 :],
    )
# ---
def test_save_config_success(self):
        resp = self.xe.save_config()
        self.assertEqual(204, resp.status_code)
# ---
def controller_address_metadata_key(label_prefix: str) -> str:
    """Metadata key for the controller address for a given prefix."""
    return f"iris-controller-address-{label_prefix}"
# ---
def transform_conversation_to_dolma(row: dict):
    dolma_row = row
    text = ""
    for message in dolma_row["messages"]:
        text += message["role"] + ": "
        text += message["content"] + "\n\n"

    text = text.strip()

    dolma_row["text"] = text
    del dolma_row["messages"]
    return dolma_row
# ---
def _needs_long_sequence_workaround(self):
        if isinstance(self.tokenizer, PreTrainedTokenizerFast):
            normalizer = self.tokenizer.backend_tokenizer.normalizer
            if normalizer is None:
                return False
            return isinstance(normalizer, (normalizers.Replace, normalizers.Sequence))
        return False
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.sorts == other.sorts
# ---
def test_init_capturing(self):
        capouter = StdCaptureFD()
        try:
            capman = CaptureManager("fd")
            capman.start_global_capturing()
            pytest.raises(AssertionError, "capman.start_global_capturing()")
            capman.stop_global_capturing()
        finally:
            capouter.stop_capturing()
# ---
def modal(method, notebook, data):
    return {}
# ---
def _remove_right_units(string):
    # "\\text{ " only ever occurs (at least in the val set) when describing units
    if "\\text{ " in string:
        splits = string.split("\\text{ ")
        assert len(splits) == 2
        return splits[0]
    else:
        return string
# ---
def test_iterators_are_a_type(self):
        it = iter(range(1,6))

        total = 0

        for num in it:
            total += num

        self.assertEqual(15 , total)
# ---
def test_async_handler(self):
        @event.event(enable=False)
        async def on_async_test(self):
            pass

        assert hasattr(on_async_test, '_h_info')
        h_info = on_async_test._h_info
        assert h_info.event_name == 'async_test'
        assert h_info.handler is on_async_test
        assert h_info.priority is event.Priority.DEFAULT
        assert not h_info.should_enable
        assert h_info.is_async
# ---
def foo_open(self): pass
# ---
def test_less_equal(self):
        expr = col("score") <= 100
        assert expr.evaluate({"score": 50}) is True
        assert expr.evaluate({"score": 100}) is True
        assert expr.evaluate({"score": 150}) is False
# ---
def _end_message_token(self) -> int:
        (token,) = self.tokenizer.encode("<|eot_id|>", add_special_tokens=False)
        return token
# ---
def __wrapped__(self):
        return self._fn
# ---
def save_utf8_file(fn, lines):
    """Save string lines into an UTF8 text files.
    """
    with open(fn, "w") as out_file:
        out_file.write("\n".join(lines).encode("utf-8"))
# ---
def remove(self, resource, lookup=None):
        args = self._es_args(resource)
        if lookup:
            try:
                return self.es.delete(id=lookup.get('_id'), refresh=True, **args)
            except elasticsearch.NotFoundError:
                return
        else:
            query = {'query': {'match_all': {}}}
            return self.es.delete_by_query(body=query, **args)
# ---
def cluster_restart(ctx):
    """Restart cluster by stopping then starting."""
    ctx.invoke(cluster_stop)
    click.echo("")
    ctx.invoke(cluster_start)
# ---
def __init__(self, side):
        self.side = side
# ---
def _wrap_in_buffered_base(fileobj):
    """
    Wraps a file-like object in a BufferedIOBase object.
    This is necessary because HF's upload_folder function expects a BufferedIOBase object.
    """
    if isinstance(fileobj, io.BufferedIOBase):
        return fileobj
    else:
        return io.BufferedReader(fileobj)
# ---
def node_types(self) -> list[str]:
        return sorted(self.entries.keys())
# ---
def zero_update(updates, nu_hat):
            # For the first step, return zero updates
            return jax.tree_util.tree_map(
                lambda u: None if u is None else jnp.zeros_like(u),
                updates,
                is_leaf=lambda x: x is None,
            )
# ---
def defineBrowserAgent(uiname, uiversion):
    class AppURLopener(urllib.FancyURLopener):
        version = "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)"
        #version = uiname + " " + uiversion + " / " + sys.platform
    urllib._urlopener = AppURLopener()
# ---
def GetBufferNumberForFilename( filename, open_file_if_needed = True ):
  return GetIntValue( u"bufnr('{0}', {1})".format(
      EscapeForVim( os.path.realpath( filename ) ),
      int( open_file_if_needed ) ) )
# ---
def scan_compatible_fn(carry, *args, **kwargs):
        return fn(carry, *args, **kwargs), None
# ---
def toggle_osd(self):
        self.command('osd')
# ---
def class_method(cls, arg):
        return arg
# ---
def _get_current_step_save_interval(self, step):
        # binary search for the correct interval
        # we assume that the intervals are sorted by until
        current_policy = next(filter(lambda p: p.until is None or p.until >= step, self.step_policies), None)
        if current_policy is None:
            return None
        return current_policy.every
# ---
def _health(self, _request: Request) -> JSONResponse:
        """Simple health check endpoint for bootstrap and load balancers."""
        return JSONResponse({"status": "healthy"})
# ---
def task_index(self) -> int | None:
        """If this is a task (last component is numeric), return the index."""
        try:
            return int(self._parts[-1])
        except ValueError:
            return None
# ---
def _compute_full(Vocab, pred_embeddings, pred_lm_head, true_ids):
    logits_full = hax.dot(pred_embeddings, pred_lm_head, axis="embed")
    target_y_full = hax.nn.one_hot(true_ids, Vocab, dtype=pred_embeddings.dtype)
    loss_full, sumexp_full = cross_entropy_loss_and_log_normalizers(logits_full, Vocab, target_y_full)
    return loss_full, sumexp_full
# ---
def _parse_arrow_message(self, message):
        self._parse_arrow_schema()

        return pyarrow.ipc.read_record_batch(
            pyarrow.py_buffer(message.arrow_record_batch.serialized_record_batch),
            self._schema,
        )
# ---
def vm_ops(self) -> PlatformOps:
        return _LocalPlatformOps()
# ---
def unsize_axes(axis_spec: AxisSelection) -> AxisSelection: ...
# ---
def __init__(self, policy_file=None, rules=None, default_rule=None):
        self.rules = Rules(rules)
        self.default_rule = default_rule or CONF.policy_default_rule

        self.policy_path = None
        self.policy_file = policy_file or CONF.policy_file
# ---
def test_map_shard_error_propagation(backend):
    """Test that exceptions in map_shard functions propagate correctly."""

    def failing_generator(items):
        for item in items:
            if item == 3:
                raise ValueError("Test error")
            yield item

    ds = Dataset.from_list([list(range(1, 6))]).flat_map(lambda x: x).map_shard(failing_generator)

    with pytest.raises(ValueError, match="Test error"):
        list(Backend.execute(ds, context=backend))
# ---
def init(self):
        if None not in self.comminfo.keys():
            self.comminfo = dict({None: self.p.commission})

        self.startingcash = self.cash = self.p.cash

        self.orders = list()  # will only be appending
        self.pending = collections.deque()  # popleft and append(right)

        self.positions = collections.defaultdict(Position)
        self.notifs = collections.deque()
# ---
def test_shard_map_multiple_args():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    def fn(a, b):
        return a + b

    sm = hax.shard_map(fn, mesh=mesh, check_rep=False)
    x = hax.ones(Dim)
    y = hax.arange(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), mesh:
        out = sm(x, y)

    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array + y.array)
# ---
def log_softmax(a: A, axis: AxisSelection | None = None) -> A:
    return wrap_axiswise_call(jnn.log_softmax, a, axis=axis, single_axis_only=False)
# ---
def testFloatRandom(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.float32)
      y = self._randMatrix(k, m, np.float32)
      self._testCpuMatmul(x, y)
      self._testGpuMatmul(x, y)
# ---
def get_frequency_range(self):
		return self.frequency_range
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[T]:
        for doc in self.source.open_shard_at_row(shard_name, row):
            yield self.fn(doc)
# ---
def test_label_detection_with_error_response(self, annotator_client_mock):
        # Given
        detect_text_method = annotator_client_mock.label_detection
        detect_text_method.return_value = AnnotateImageResponse(
            error={"code": 3, "message": "test error message"}
        )

        # When
        with pytest.raises(AirflowException) as ctx:
            self.hook.label_detection(image=DETECT_TEST_IMAGE)

        err = ctx.value
        assert "test error message" in str(err)
# ---
def _set_match_panel(self, match, team_idx, panel_idx):
        match.team_numbers[team_idx] = self.team_panels[panel_idx].number
        match.team_names[team_idx] = self.team_panels[panel_idx].name
# ---
def __init__(self):
        """Initialize the ParseState."""

        self.tokens = []
        self.values = []
# ---
def __bool__(self) -> bool:  # pragma: no cover
        return bool(self.array)
# ---
def __init__(self, rule):
        msg = _("Policy doesn't allow %s to be performed.") % rule
        super(PolicyNotAuthorized, self).__init__(msg)
# ---
def attention_config(self) -> AttentionConfig:
        """Convert this MistralConfig to an AttentionConfig for use with Attention."""
        return AttentionConfig(
            Embed=self.Embed,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            use_bias=self.use_bias,
            upcast_attn=self.upcast_attn,
            attn_backend=self.attn_backend,
            flash_attention_block_size=self.flash_attention_block_size,
            rope=self.rope,
        )
# ---
def _is_float(num: str) -> bool:
    try:
        float(num)
        return True
    except (ValueError, OverflowError):
        return False
# ---
def is_idle(self) -> bool:
        return len(self.running_task_ids) == 0
# ---
def test_rowproxy_is_sequence(self):
        import collections
        from sqlalchemy.engine import RowProxy

        row = RowProxy(object(), ['value'], [None], {'key'
                         : (None, None, 0), 0: (None, None, 0)})
        assert isinstance(row, collections.Sequence)
# ---
def _run_async_task(self, coro):
        if not self.loop.is_running() or not self.thread.is_alive():
            raise StopIteration
        try:
            future = asyncio.run_coroutine_threadsafe(coro, self.loop)
            return future.result()
        except (RuntimeError, asyncio.CancelledError):
            raise StopIteration
# ---
def probe(self) -> cluster_pb2.WorkerMetadata: ...
# ---
def grad_func_name(self):
    """Its gradient function's name."""
    return self._grad_func.name if self._grad_func else None
# ---
def parse_gcs_url(gcs_url):
    """Parse the GCS URL and return the bucket name and prefix path."""
    match = re.match(r"gs://([^/]+)/(.+)", gcs_url)
    if match:
        bucket_name, path_prefix = match.groups()
        return bucket_name, path_prefix
    else:
        raise ValueError(f"Invalid GCS URL format: {gcs_url}")
# ---
def _multislice_info_to_env_vars(multislice: MultisliceInfo) -> dict[str, str]:
    if multislice is not None:
        mxla_env = {
            "MEGASCALE_COORDINATOR_ADDRESS": f"{multislice.coordinator_ip}:{multislice.port}",
            "MEGASCALE_NUM_SLICES": str(multislice.num_slices),
            "MEGASCALE_PORT": f"{multislice.port}",
            "MEGASCALE_SLICE_ID": str(multislice.slice_id),
        }
    else:
        mxla_env = {}
    return mxla_env
# ---
def quick_job():
        return 42
# ---
def _loglikelihood_tokens(self, requests, disable_tqdm: bool = False):
        raise NotImplementedError("_loglikelihood_tokens is not yet supported")
# ---
def __lt__(self, other: "Timestamp") -> bool:
        return self._epoch_ms < other._epoch_ms
# ---


def change_base(x: int, base: int):
    """Change numerical base of input number x to base.
    return string representation after the conversion.
    base numbers are less than 10.
    >>> change_base(8, 3)
    '22'
    >>> change_base(8, 2)
    '1000'
    >>> change_base(7, 2)
    '111'
    """
    ret = ""
    while x > 0:
        ret = str(x % base) + ret
        x //= base
    return ret
# ---
def test_load_env_vars_single_key():
    """Test env var with no value (empty string)."""
    result = load_env_vars([["KEY_ONLY"]])
    assert result["KEY_ONLY"] == ""
# ---
def min(
        self, axis: AxisSelection | None = None, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.min(self, axis=axis, where=where)
# ---
def get_fp8_max(fp8_dtype, out_dtype):
    assert fp8_dtype in (jnp.float8_e4m3fn, jnp.float8_e5m2)
    return jnp.finfo(fp8_dtype).max.astype(out_dtype)
# ---
def test_ema_phase():
    ma = EmaDecaySqrtModelAveraging(model=_dummy(0.0), beta=0.5, switch_step=5, decay_steps=10)
    ma = ma.update(_dummy(1.0), step=0)
    assert jnp.allclose(ma.model["p"], 1.0)
    assert jnp.isclose(ma.tot_weight, 0.5)
# ---
def characterization_insertion_transfection(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['stable transfection'],
        'modified_site_nonspecific': 'random',
        'introduced_elements': 'synthesized DNA'
    }
# ---
def oswritebytes(fd, obj):
    os.write(fd, tobytes(obj))
# ---
def bmarks():
    return_data = get_bmarks()
    return return_data
# ---
def Postprocessing (self):
        self.Log ("Incoming calls test case: Cleaning up after test...")

        if not self.Caller is None:
            self.Caller.release ()
        if not self.Receptionist is None:
            self.Receptionist.release ()
        if not self.Receptionist_2 is None:
            self.Receptionist_2.release ()
        if not self.Callee is None:
            self.Callee.release ()
# ---
def remove_all_iris_containers(self) -> int:
        count = len(self._containers)
        self._containers.clear()
        return count
# ---
def is_named_or_shaped_array_like(x):
    return (is_jax_array_like(x) and x.ndim >= 1) or is_named_array(x)
# ---
def play_n_episodes(player, predfunc, nr):
    logger.info("Start evaluation: ")
    for k in range(nr):
        if k != 0:
            player.restart_episode()
        score = play_one_episode(player, predfunc)
        print("{}/{}, score={}".format(k, nr, score))
# ---
def supportsSeeking(self):
		return hasattr(self.file, "seek")
# ---
def _flush_while_waiting(event):
    def flush_stdout():
        sys.stdout.flush()
        sys.stderr.flush()
        time.sleep(5)
        while not event.is_set():
            print("Waiting...", flush=True)
            print("\n", file=sys.stderr, flush=True)
            time.sleep(5)

    thread = threading.Thread(target=flush_stdout)
    thread.start()
# ---
def sl(start, size):
            return hax.ds(start, size)
# ---
def temp_cache_dir(tmp_path):
    """Create a temporary cache directory."""
    cache_dir = tmp_path / "cache"
    cache_dir.mkdir()
    return cache_dir
# ---
def SearchInCurrentBuffer( pattern ):
  """ Returns the 1-indexed line on which the pattern matches
  (going UP from the current position) or 0 if not found """
  return GetIntValue( "search('{0}', 'Wcnb')".format( EscapeForVim( pattern )))
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "embed_tokens", "position_embeddings": "embed_positions"}
# ---
def __call__(
        self,
        array: NamedArray,
        axis: AxisSelection | None = None,
        where: NamedArray | None = None,
        **kwargs,
    ) -> NamedArray: ...
# ---
def init(cls, Vocab: Axis, config: MistralConfig, *, key) -> "MistralLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = LlamaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return MistralLMHeadModel(transformer, embeddings, lm_head)
# ---
def test_spawn_with_network_qos(self):
        self._create_instance()
        for vif_ref in xenapi_fake.get_all('VIF'):
            vif_rec = xenapi_fake.get_record('VIF', vif_ref)
            self.assertEquals(vif_rec['qos_algorithm_type'], 'ratelimit')
            self.assertEquals(vif_rec['qos_algorithm_params']['kbps'],
                              str(3 * 1024))
# ---
def to_dict(self):
        """
        Convert the TaskConfig to a dictionary, excluding None values.

        Returns:
            Dictionary representation of the task configuration
        """
        base_dict = dataclasses.asdict(self)
        return {k: v for k, v in base_dict.items() if v is not None}
# ---
def _pipeline() -> int:
        return len([dupekit.process_dicts_loop(d) for d in in_memory_table.to_pylist()])
# ---
def test_scalar_updated_slice():
    # Base case: scalar start on a 1D array
    Seq = hax.Axis("seq", 5)
    arr = hax.arange((Seq,), dtype=int)
    # replace positions 2 and 3 with [100, 101]
    upd = hax.named([100, 101], "seq")

    result = updated_slice(arr, {"seq": 2}, upd)
    # expect [0,1,100,101,4]
    assert np.array_equal(result.array, np.array([0, 1, 100, 101, 4]))
# ---
def can_perform_swap(self):
        index = self.swap_type_list.selected_index
        if index == SwapType.DayMonth:
            return self._can_swap_date_fields(DAY, MONTH)
        elif index == SwapType.MonthYear:
            return self._can_swap_date_fields(MONTH, YEAR)
        elif index == SwapType.DayYear:
            return self._can_swap_date_fields(DAY, YEAR)
        else:
            return True
# ---
def __getitem__(self, key):
        if key not in self._dataset._coord_names:
            return self._dataset[key]
        else:
            raise KeyError(key)
# ---
from typing import List


def remove_duplicates(numbers: List[int]) -> List[int]:
    """ From a list of integers, remove all elements that occur more than once.
    Keep order of elements left the same as in the input.
    >>> remove_duplicates([1, 2, 3, 2, 4])
    [1, 3, 4]
    """
    import collections
    c = collections.Counter(numbers)
    return [n for n in numbers if c[n] <= 1]
# ---
def peek(self) -> T_co | None:
        """View the next available item without acquiring a lease."""
        ...
# ---
def test_health_check_rpc(self, real_service):
        """Test HealthCheck RPC returns healthy status."""
        ctx = Mock(spec=RequestContext)

        response = real_service.health_check(cluster_pb2.Empty(), ctx)

        assert response.healthy
        assert response.uptime.milliseconds >= 0
# ---
def _scale_up(self, count: int):
        """Add new actors to the pool."""
        logger.info(f"Scaling up: adding {count} new actors")
        for _ in range(count):
            self._create_and_register_actor()
        logger.info(f"Pool size now: {len(self.actors)} actors")
# ---
def path(self) -> str:
        """Returns the URL path to mount the application to when serving multiple applications."""
        return "/iris.cluster.ControllerService"
# ---
def tags(self):
        return self.tag_to_index.keys()
# ---
def stop(self, wait: bool = True):
        self._stop_event.set()
        # I'm getting an error that the thread is threading.current_thread(), which seems impossible
        if self.thread is not None and wait and self.thread != threading.current_thread():
            self.thread.join()
# ---
def get_readme():
    dirname = os.path.dirname(os.path.abspath(__file__))
    with open(os.path.join(dirname, "README.md"), "r") as fp:
        long_description = fp.read()
    return long_description
# ---
def asGenKw(self):
        """ @rtype: GenKw """
        impl_type = EnkfNode.cNamespace().get_impl_type(self)
        assert impl_type == ErtImplType.GEN_KW

        return GenKw.createCReference(self.valuePointer(), self)
# ---
def test_endswith_autoescape_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e%fg", autoescape=True, escape="#"), {6})
        self._test(col.endswith("e#fg", autoescape=True, escape="#"), {9})
# ---
def genetic_modification_2(lab, award):
    return {
        'modification_type': 'deletion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'modification_description': 'some description',
        'modification_zygocity': 'homozygous',
        'modification_purpose': 'tagging',
        'modification_treatments': [],
        'modification_genome_coordinates': [{
            'chromosome': '11',
            'start': 5309435,
            'end': 5309451
            }]
    }
# ---
def __init__(self, run_result: subprocess.CompletedProcess | None = None):
        self._run_result = run_result or subprocess.CompletedProcess(args=[], returncode=0, stdout="ok", stderr="")
        self.last_timeout: Duration | None = None
# ---
def bitwise_or(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_or](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_or.html)
    """
    return jnp.bitwise_or(x1, x2)
# ---
def _bump_params(params):
        return jax.tree.map(lambda x: x + jnp.ones_like(x) * 0.001, params)
# ---
def __iter__(self):
        return iter(self._dict)
# ---
def test_action_registry(self):
        """Do some sanity checks on the action registry."""
        self.assertEqual(
            len(action_registry.Registry.get_all_actions()), 3)
# ---
def __getitem__(self,a):
        return self.arguments[a]
# ---
def fail(returncode, e):
    sys.stderr.write("ERROR: %s\n" % e)
    sys.exit(returncode)
# ---
def test_axis_shapes_multiple_dcn_absorbers_error():
    cfg = MeshConfig(dcn_axes={"replica_dcn": -1, "other": -1})
    with pytest.raises(ValueError):
        cfg.axis_shapes(num_devices=8, num_slices=2)
# ---
def set_instances(self, instances: list[dict]) -> None:
        self._instances = instances
# ---
def local_cluster():
    yield LocalCluster(LocalClusterConfig(use_isolated_env=False))
# ---
def _install_package(self, db_version):
        sudo("pkg_add postgresql%s-server" %db_version)
        sudo("pkg_add postgresql%s-replicationtools" %db_version)
        sudo("svcadm enable postgresql")
# ---
def compute_num_layers(self, hidden_size: int) -> int:
        """Compute number of layers from hidden size using the depth-width formula."""
        hs_pow = math.log2(hidden_size)
        return round(
            hidden_size
            / (self.base_hidden_layer_ratio + (hs_pow * self.layer_scaling_factor) - self.layer_formula_offset)
        )
# ---
def from_read_rows_response(message):
        schema_type = message._pb.WhichOneof("schema")
        if schema_type == "avro_schema":
            return _AvroStreamParser(message)
        elif schema_type == "arrow_schema":
            return _ArrowStreamParser(message)
        else:
            raise TypeError(
                "Unsupported schema type in message: {0}".format(schema_type)
            )
# ---
def stack_tree(batch_name, individual_datums):
    def _stack_leaves_unchecked(*leaves):
        if is_named_array(leaves[0]):
            return hax.stack(batch_name, leaves)
        else:
            return jnp.stack(leaves)

    return jax.tree.map(_stack_leaves_unchecked, *individual_datums, is_leaf=is_named_array)
# ---
def corofunc2():
            called[1] += 1
            return event.ReturnValue(insert_events=[evt3], append_events=[evt2])
# ---
def is_finished(self):
        return True
# ---
def launch_job(self, request: cluster__pb2.Controller.LaunchJobRequest, ctx: RequestContext) -> cluster__pb2.Controller.LaunchJobResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def reject(self, request, *args, **kwargs):
        instance = self.get_object()
        serializer = self.get_serializer(instance)
        instance.reject(processor=request.user)
        return Response(serializer.data)
# ---
def __init__(self, bindings=None):
        if bindings is None:
            self.bindings = {}
        else:
            self.bindings = {**bindings}
# ---
def test_connect_to_region(self):
        """Can connect to a dynamo region"""
        conn = DynamoDBConnection.connect("us-west-1")
        self.assertIsNotNone(conn.host)
# ---
def test_special_tokens_injection(marin_tokenizer: PreTrainedTokenizer):
    """Test that special tokens are correctly replaced."""
    special_tokens_injection_check(marin_tokenizer)
# ---
def __call__(self, x: hax.NamedArray) -> hax.NamedArray:
        return hax.sin(self.freq * x)
# ---
def output_exemplar(self):
        return AudioTextDict_exemplar
# ---
def test_same_expr_same_hash(self):
        expr1 = col("score") > 0.5
        expr2 = col("score") > 0.5
        assert hash(expr1) == hash(expr2)
# ---
def _run() -> list[Any]:
        if mode == "batch":
            return func(text_samples)
        return [func(x) for x in text_samples]
# ---
def __init__(
        self,
        cache_dir: Path,
        registry: str,
        max_images: int = 100,
    ):
        self._cache_dir = cache_dir / "images"
        self._registry = registry
        self._max_images = max_images
        self._cache_dir.mkdir(parents=True, exist_ok=True)
        self._docker = DockerImageBuilder(registry)
        # Refcount of images currently in use by jobs (should not be evicted)
        self._image_refcounts: dict[str, int] = {}
# ---
def build(
        self,
        context: Path,
        dockerfile_content: str,
        tag: str,
        task_logs: TaskLogs | None = None,
    ) -> None: ...
# ---
def fold_via(
        self, fn: FoldFunction[M, P, CarryT], *, unroll: int | bool | None = None
    ) -> Callable[Concatenate[CarryT, P], CarryT]: ...
# ---
def _fetch_all_from_queue(q: queue.Queue, timeout: float) -> List:
    """Fetch all items from `q` which arrive within `timeout` seconds."""
    deadline = time.time() + timeout
    items = []
    while time.time() < deadline:
        try:
            item = q.get(timeout=max(0, deadline - time.time()))
            items.append(item)
        except queue.Empty:
            break
    return items
# ---
def bombardment_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'nucleic_acid_delivery_method': ['bombardment']
    }
# ---
def testAssertMsg(self):
    want = (0, "AssertionError('foo',)\n")
    self.assertEqual(want, _GrumpRun(textwrap.dedent("""\
        try:
          assert False, 'foo'
        except AssertionError as e:
          print repr(e)""")))
# ---
def mathml_to_markdown(mathml_node):
    converter = MathMLToLatex()
    return converter.convert(mathml_node)
# ---
def _delete_vpn_processes(self, sync_router_ids, vpn_router_ids):
        for process_id in sync_router_ids:
            if process_id not in vpn_router_ids:
                self.destroy_process(process_id)
# ---
def _fsdp_impl(fn: F, parameter_mapping, compute_mapping):
    return named_jit(
        fn, in_axis_resources=parameter_mapping, out_axis_resources=parameter_mapping, axis_resources=compute_mapping
    )
# ---
def default_run_id():
    """Generate a run ID for wandb and continuation.

    Wandb expects a base36 encoded ID of exactly 8 lowercase characters
    or it won't generate a display name."""
    rng_bytes = os.urandom(16)
    run_id = base64.b32encode(rng_bytes)[:8].lower()
    run_id = run_id.decode("utf-8")
    assert len(run_id) == 8
    for char in run_id:
        assert char in "abcdefghijklmnopqrstuvwxyz0123456789"
    return run_id
# ---
def test_auto_picking(self, cr, uid, ids):
        # TODO: Check locations to see if in the same location ?
        return True
# ---
def axis_spec_to_tuple(axis_spec: PartialShapeDict) -> tuple[AxisSelector, ...]: ...
# ---
def stop(self) -> None:
        """Stop all container threads managed by this VM manager."""
        self._threads.stop(timeout=Duration.from_seconds(5.0))
# ---
def store_weights(self, weight_id: int, params_dict: dict[str, tuple[pa.Schema, Sequence[pa.RecordBatch]]]) -> None:
        with self._lock:
            # remove all other weights
            self._weights_store.clear()
            self._weights_store[weight_id] = params_dict
            self._latest_weight_id = weight_id
# ---
def job_id(self) -> str: ...
# ---
def test_binary_force_string(self):
        """Binary must wrap a string type"""
        with self.assertRaises(TypeError):
            Binary(2)
# ---
def can_accept_demand(self, timestamp: Timestamp | None = None) -> bool:
        """Whether this group can accept demand for waterfall routing."""
        return self.availability(timestamp).status == GroupAvailability.AVAILABLE
# ---
def genetic_modification_8(lab, award):
    return {
        'purpose': 'analysis',
        'category': 'interference',
        'award': award['uuid'],
        'lab': lab['uuid'],
        "method": "CRISPR",
    }
# ---
def __set__(self, instance, value):
		instance.__dict__[self.field.name] = value
		setattr(instance, self.field.attname, json.dumps(value))
# ---
def setUpTestData(cls):
        cls.url = reverse('search-list')
        Feature.objects.create(name='archival descriptions', enabled=True)
        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)
        cls.security_levels = [1, 2, 3, 4, 5]
# ---
def sync_backend(request):
    """Backend fixture for sync and threadpool backends."""
    return create_job_ctx(request.param, max_workers=2)
# ---
def _set_velocity(self, value):
        self._parameter2 = value
# ---
def __init__(self, resistance, voltage):
        self._resistance = resistance
        self.voltage = voltage
# ---
def __call__(self, x: NamedArray, group_sizes: NamedArray, *, key=None) -> NamedArray:
        k1, k2, k3 = maybe_rng_split(key, 3)

        w1_output = self.w1(x, group_sizes, key=k1)

        activated = self.act(w1_output)

        w3_output = self.w3(x, group_sizes, key=k3)

        gated = activated * w3_output

        final_output = self.w2(gated, group_sizes, key=k2)

        return final_output
# ---
def task_func():
        print("hello from callable")
# ---
def test_capsyscapfd(self, testdir):
        p = testdir.makepyfile("""
            def test_one(capsys, capfd):
                pass
            def test_two(capfd, capsys):
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*ERROR*setup*test_one*",
            "E*capfd*capsys*same*time*",
            "*ERROR*setup*test_two*",
            "E*capsys*capfd*same*time*",
            "*2 error*"])
# ---
def generator(self):
        """Generate pages from specified page interval."""
        for page_number in self.page_number_gen():
            title = '{page_ns}:{prefix}/{number}'.format(
                page_ns=self._page_ns,
                prefix=self._prefix,
                number=page_number)
            page = ProofreadPage(self._index.site, title)
            page.page_number = page_number  # remember page number in djvu file
            yield page
# ---
def test_best_of_n_returns_candidates(params, model_cfg, tokenizer):
    """Best-of-N should return at least one candidate."""
    results = best_of_n(
        params=params,
        source="x = 1 + 2\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(0),
        n=4,
        max_depth=2,
        temperature=1.0,
    )
    assert len(results) > 0
    assert all(isinstance(c, BeamCandidate) for c in results)
# ---
def _full_next_token_loss(logits: jax.Array, token_ids: jax.Array, loss_weight: jax.Array) -> jax.Array:
    labels = jnp.concatenate([token_ids[:, 1:], token_ids[:, :1] * 0], axis=1).astype(jnp.int32)
    log_probs = jax.nn.log_softmax(logits, axis=-1)
    nll = -jnp.take_along_axis(log_probs, labels[..., None], axis=-1)[..., 0]
    nll = nll * loss_weight
    denom = jnp.sum(loss_weight)
    return jnp.sum(nll) / jnp.maximum(denom, jnp.array(1.0, dtype=nll.dtype))
# ---
def restore_view(self):
        self.import_table.columns.restore_columns()
# ---
def add_hook(self, fn: Callback, *, every: int = 1): ...
# ---
def _hf_sleep_with_jitter(delay: float, max_delay: float) -> tuple[float, float]:
    jitter = random.uniform(0.5, 1.5)
    sleep_seconds = min(delay * jitter, max_delay)
    time.sleep(sleep_seconds)
    next_delay = min(delay * 2, max_delay)
    return sleep_seconds, next_delay
# ---
def test_do_execute_no_params_w_replace(self):
        self._test_do_execute_no_params(True)
# ---
def _has_expert_details(self):
        if self._is_expert():
            profile = self._get_profile()
            return True if profile.title or profile.company else False
# ---
def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str, ...]:
        return ()
# ---
def create_vm_group_side_effect(tags: dict[str, str] | None = None) -> MagicMock:
        # Generate a unique slice ID based on call count
        slice_id = f"new-slice-{len(manager.create_vm_group.call_args_list)}"
        mock = make_mock_vm_group(slice_id)
        mock.tags = tags  # Store tags so tests can verify if needed
        return mock
# ---
def matches(step: ExecutorStep) -> bool:
            # track which regexes have been used
            for i, regex in enumerate(regexes):
                if regex.search(step.name):
                    used_regexes.add(i)
                    return True

            return False
# ---
def yaml_dict(self):
        ''' getter method for yaml_dict '''
        return self.__yaml_dict
# ---
def _get_scheduling_strategy(self, resource_config: ResourceConfig | None):
        if resource_config is None:
            return None
        return get_scheduling_strategy(resource_config)
# ---
def unique_id(self):
        """Return the unique id of this switch."""
        return self._unique_id
# ---
def __enter__(self) -> "ActorPool[T]":
        return self
# ---
def open_door(self, content, door_card):
        return self._do_open
# ---
def delete_single_tpu(node: dict[str, Any]) -> str | None:
        node_name = node.get("name", "").split("/")[-1]
        try:
            delete_tpu_node(node_name, project, zone, quiet=True)
            logger.info(f"Terminated TPU node: {node_name}")
            return node_name
        except Exception as e:
            logger.error(f"Failed to terminate TPU node {node_name}: {e}")
            return None
# ---
def body(i, buf):
        p = t_pages[i]
        s = t_slots[i]
        ins = kv_ev[i][None, None, :, :]
        return lax.dynamic_update_slice(buf, ins, (p, s, 0, 0))
# ---
def tokenizer_name(self) -> str:
        """Return a string identifier for the tokenizer/chat template."""
        if hasattr(self.tokenizer, "name_or_path"):
            return self.tokenizer.name_or_path
        elif hasattr(self.tokenizer, "model_name"):
            return self.tokenizer.model_name
        else:
            return "unknown_tokenizer"
# ---
def _rotary_cache(seq_len: int, head_dim: int, rope: RotaryConfig) -> tuple[Float[Array, "S D"], Float[Array, "S D"]]:
    half_dim = head_dim // 2
    inv_freq = 1.0 / (rope.theta ** (jnp.arange(0, half_dim, dtype=jnp.float32) / half_dim))
    positions = jnp.arange(seq_len, dtype=jnp.float32)
    angles = positions[:, None] * inv_freq[None, :]
    cos = jnp.cos(angles)
    sin = jnp.sin(angles)
    return cos, sin
# ---
def test_fn(model_weights):
    return evaluate_fn(model_weights, [cifar_test])
# ---
def test_init(self):
        """
        The initializer updates the instance's C{__dict__} with its
        keyword arguments.
        """
        namespace = _api._SimpleNamespace(x=1)
        assert namespace.x == 1
# ---
def value_match(cls, pattern, value):
        """Determine whether the value matches the pattern. Both
        arguments are strings.
        """
        raise NotImplementedError()
# ---
def test_connect_error_with_traceback_populates_timestamp() -> None:
    try:
        raise ValueError("boom")
    except ValueError as exc:
        err = connect_error_with_traceback(Code.INTERNAL, "Error launching job", exc=exc)

    details = extract_error_details(err)
    assert details is not None
    assert details.exception_type.endswith("ValueError")
    assert details.timestamp.epoch_ms > 0
# ---
def test_wait_for_condition_timeout() -> None:
    """Test wait_for_condition raises TimeoutError when condition never becomes true."""
    import pytest

    start = time.monotonic()
    with pytest.raises(TimeoutError, match="did not become true within"):
        wait_for_condition(lambda: False, timeout=Duration.from_seconds(0.1))
    elapsed = time.monotonic() - start

    # Should wait approximately the timeout duration
    assert 0.09 < elapsed < 0.2
# ---
def play_one_episode(player, func, verbose=False):
    def f(s):
        spc = player.get_action_space()
        act = func([[s]])[0][0].argmax()
        if random.random() < 0.001:
            act = spc.sample()
        if verbose:
            print(act)
        return act
    return np.mean(player.play_one_episode(f))
# ---
def reset(self) -> "PageTable":
        ref_counts = hax.full_like(self.page_ref_counts, 0)
        return PageTable(ref_counts, self.page_size, self._max_seqs, self._pages_per_seq)
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[List[int]]:
        # parse the shard name to get the shard number
        shard_num = int(shard_name.split("_")[1])
        return ([shard_num * 10 + i] * 10 for i in range(row, 10))
# ---
def stop(self) -> None:
        """No-op: FakeVmManager has no background threads to stop."""
        pass
# ---
def render_timestamp(timestamp):
    return datetime.datetime.fromtimestamp(timestamp//1000).ctime()
# ---
def _num_train_steps(*, param_count: int, batch_size: int, max_seq_len: int, tpp: int = 20) -> int:
    total_tokens = param_count * tpp
    return max(1, total_tokens // (batch_size * max_seq_len))
# ---
def __call__(self, q: NamedArray, position_ids: NamedArray) -> NamedArray:
        inv_freq_llama = self._compute_inv_freq_llama()
        freqs = position_ids * inv_freq_llama.broadcast_axis(position_ids.axes)
        emb = hax.concatenate(self.HeadDim, (freqs, freqs))
        cos = hax.cos(emb).astype(q.dtype)
        sin = hax.sin(emb).astype(q.dtype)

        q_embed = q * cos + _rotate_half(q, self.HeadDim) * sin
        return q_embed
# ---
def decorator(request, *args, **kwargs):
            if not request.user.has_perm(self.perm):
                raise PermissionRequired(self.perm)
            return view_func(request, *args, **kwargs)
# ---
def writer(self) -> "InMemoryRolloutWriter":
        """Create a writer for this queue."""
        return InMemoryRolloutWriter(self)
# ---
def test_generative_engine_event_dispatch_hasevents(self):
        def l1(*arg, **kw):
            pass
        eng = create_engine(testing.db.url)
        assert not eng._has_events
        event.listen(eng, "before_execute", l1)
        eng2 = eng.execution_options(foo='bar')
        assert eng2._has_events
# ---
def astype(self, dtype) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.astype(dtype), self.axes)
# ---
def _make_starttag(tag, attrs):
    'Write a starttag.'
    out = '<' + tag
    for key in attrs:
        out += ' {}="{}"'.format(key, html.escape(attrs[key]))
    out += '>'
    return out
# ---
def toggle_connect(self):
        if self.serial.isOpen():
            self.disconnect()
        else:
            self.connect()
# ---
def make_route(self):
		if not self.route:
			return cstr(frappe.db.get_value('Item Group', self.item_group,
					'route')) + '/' + self.scrub((self.item_name if self.item_name else self.item_code) + '-' + random_string(5))
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "openai/whisper-base"
    ) -> HFCheckpointConverter["WhisperModel"]:  # type: ignore
        return HFCheckpointConverter(self, reference_checkpoint=ref_checkpoint, ignore_prefix="model")
# ---
def check_raise(self):
        if self._exception:
            raise self._exception
# ---
def heuristic_is_leaf(x):
    if isinstance(x, list):
        return jnp.isscalar(x[0])
    else:
        return False
# ---
def test_terminate():
    """Terminate marks a job as STOPPED even if the underlying thread is still running."""
    stop = threading.Event()

    def hang():
        stop.wait(10)

    c = LocalClient(max_threads=4)
    handle = c.submit(JobRequest(name="hang", entrypoint=Entrypoint.from_callable(hang)))
    time.sleep(0.05)
    handle.terminate()
    assert handle.status() == JobStatus.STOPPED
    stop.set()  # unblock the thread so the executor can shut down
    c.shutdown(wait=True)
# ---
def test_without_args(self, mock_start, mock_stop):
        self.assertEqual((1, 2), trace_hide_args_func(1, i=2))
        expected_info = {
            "function": {
                "name": "osprofiler.tests.unit.test_profiler"
                        ".trace_hide_args_func"
            }
        }
        mock_start.assert_called_once_with("hide_args", info=expected_info)
        mock_stop.assert_called_once_with()
# ---
def leaderboard():
    """Leaderboard page for SciNet"""
    get_db()
    groups = get_groups(g.groups_collection)
    return render_template("leaderboard.html", groups=groups)
# ---
def nextafter(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.nextafter(x1, x2)
# ---
def test_utils_to_probs_raises():
    with pytest.raises(RuntimeError):
        mnl.utils_to_probs(
            pd.DataFrame([[1, 2, np.inf, 3]]))
# ---
def test_named_param_annotation():
    def foo(x: f32[NamedArray, "batch embed"]):  # type: ignore  # noqa: F722
        pass

    axes = typing.get_args(typing.get_type_hints(foo, include_extras=True)["x"])[1]
    assert axes.before == ("batch", "embed")
# ---
def __init__(self,occurrence):
        Occurrence.__init__(self,occurrence.name,occurrence.arguments,as_written=occurrence.as_written,position=occurrence.position)
        self.occurrence = occurrence
        self.filename = self.occurrence[0]
        self.input_paths=InputPaths()
        self._file_content=None
# ---
def check_short_name(short_name_raw: str) -> str:
    short_name = short_name_raw.strip()
    if len(short_name) == 0:
        raise JsonableError(_("Bad name or username"))
    return short_name
# ---
def test_cluster_job_success(cluster):
    request = JobRequest(
        name="success-test-job",
        entrypoint=Entrypoint.from_callable(lambda: None),
        environment=EnvironmentConfig.create(),
    )

    job_id = cluster.launch(request)
    info = cluster.wait(job_id)

    assert info.status == "succeeded"
    assert info.error_message is None
# ---
def wait(self, futures: list, num_returns: int = 1) -> tuple[list, list]:
        """Wait for Ray futures to complete."""
        # NOTE: fetch_local=False is paramount to avoid copying the data to the Zephyr
        # driver node, especially for the data futures.
        ready, pending = ray.wait(futures, num_returns=num_returns, fetch_local=False)
        return list(ready), list(pending)
# ---
def on_remove_duplicates_state_changed(self):
        self.fuzz_table_model.remove_duplicates = self.ui.chkBRemoveDuplicates.isChecked()
        self.fuzz_table_model.update()
        self.remove_duplicates()
# ---
def test_correct_output(self):
        hfmt = HtmlFormatter(nowrap=True)
        houtfile = StringIO.StringIO()
        hfmt.format(tokensource, houtfile)

        nfmt = NullFormatter()
        noutfile = StringIO.StringIO()
        nfmt.format(tokensource, noutfile)

        stripped_html = re.sub('<.*?>', '', houtfile.getvalue())
        escaped_text = escape_html(noutfile.getvalue())
        self.assertEquals(stripped_html, escaped_text)
# ---
def create_manual_group(vms: list[MagicMock]) -> VmGroupProtocol:
        from iris.time_utils import Timestamp

        return ManualVmGroup(
            group_id="test-slice-001",
            scale_group="test-group",
            vms=vms,
            vm_registry=registry,
            created_at=Timestamp.from_ms(1234567890),
        )
# ---
def iter_from_step(self, start_from_batch: int | None = None):
        # sometimes we pass in an array for the start_from_batch, so we need to check for that
        start_from_batch = int(start_from_batch) if start_from_batch is not None else None
        return DataLoaderIterator(self, start_from_batch=start_from_batch)
# ---
def __eq__(self, other):
        return self._dict == other._dict
# ---
def push_to_gcp(local_id, project_id, region, repository) -> str:
    """Pushes a local Docker image to Artifact Registry."""
    configure_gcp_docker(project_id, region, repository)

    artifact_repo = f"{region}-docker.pkg.dev/{project_id}/{repository}"

    full_image_name = f"{artifact_repo}/{local_id}"
    _run(["docker", "tag", local_id, full_image_name])
    _run(["docker", "push", full_image_name])

    return f"{artifact_repo}/{local_id}"
# ---
def __iter__(self):
        """
        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.
        :return: The logger instance.
        """
        return self
# ---
def init(cls, Vocab: Axis, config: GemmaConfig, *, key) -> "GemmaLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = GemmaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return GemmaLMHeadModel(transformer, embeddings, lm_head)
# ---
def counting_fn(x):
        nonlocal call_count
        call_count += 1
        return x * 2
# ---
def test_successor(self):
        try:
            self.test_patterns.create(Successor)
            raise Exception("Recursive structure did not explode.")
        except RuntimeError as re:
            assert str(re).startswith("maximum recursion depth")
# ---
def test_map_shard_empty_result(backend):
    """Test map_shard that filters everything out."""

    def filter_all(items):
        for _ in items:
            pass  # Consume but don't yield
        return iter([])  # Return empty iterator

    ds = Dataset.from_list([list(range(1, 6))]).flat_map(lambda x: x).map_shard(filter_all)
    result = list(Backend.execute(ds, context=backend))
    assert result == []
# ---
def default():
        return DefaultDotGeneralOp.init()
# ---
def test_beam_search_sorted_by_score(params, model_cfg, tokenizer):
    """Results should be sorted: edited candidates first, then by score."""
    results = beam_search(
        params=params,
        initial_programs=["x = 1 + 2\n", "y = 3\n"],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(1),
        beam_size=4,
        expansions_per_beam=2,
        max_depth=2,
    )
    keys = [(c.depth > 0, c.score) for c in results]
    assert keys == sorted(keys, reverse=True)
# ---
def _flatten_module(module):
        if isinstance(module, ModuleWithStateDictSerialization):
            module = module.flatten_for_export()
            module = scan_aware_tree_map(
                _flatten_module,
                module,
                is_leaf=lambda x: x is not module and isinstance(x, ModuleWithStateDictSerialization),
            )
        return module
# ---
def process_file(input_path):
        """Read file and return transformed record."""
        with open(input_path) as f:
            content = f.read()
        # Return a single record with uppercased content
        return {"content": content.upper(), "path": input_path}
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None:
        return None
# ---
def __post_init__(self):
        if self.a <= 0 or self.b >= 0:
            raise ValueError("Power schedule expects a > 0 and b < 0")
# ---
def lit(value: Any) -> LiteralExpr:
    """Create a literal value expression.

    Example:
        >>> col("a") + lit(10)
        (col('a') + lit(10))
    """
    return LiteralExpr(value)
# ---
def test_expr_as_dict_key(self):
        expr = col("score") > 0.5
        d = {expr: "value"}
        assert d[expr] == "value"
# ---
def __setstate__(self, state: dict) -> None:
        self._endpoint_name = state["endpoint_name"]
        self._client = None
# ---
def __eq__(self, other):
        if other is None:
            return False
        return self.platform_id == other.platform_id and self.id == other.id
# ---
def test_get_task_status_not_found(service, worker, request_context):
    """Test get_task_status raises NOT_FOUND for nonexistent task."""
    status_request = cluster_pb2.Worker.GetTaskStatusRequest(task_id=JobName.root("nonexistent").task(0).to_wire())

    with pytest.raises(ConnectError) as exc_info:
        service.get_task_status(status_request, request_context)

    assert exc_info.value.code == Code.NOT_FOUND
    assert "nonexistent" in str(exc_info.value)
# ---
def test_iris_config_missing_file(tmp_path):
    """Test error on missing config file."""
    with pytest.raises(FileNotFoundError):
        IrisConfig.load(tmp_path / "nonexistent.yaml")
# ---
def current_label_end(self):
        if self.current_label and self.message:
            return self.message.get_label_range(self.current_label, self.proto_view, False)[1]
        else:
            return -1
# ---
def sample_batch(parquet_file: str) -> pa.RecordBatch:
    """
    Loads a single batch (10k rows) for algorithm benchmarks (hashing, dedupe logic).
    Columns are restricted to ensure we have 'text' and 'id'.
    """
    pf = pq.ParquetFile(parquet_file)
    # Ensure we get necessary columns if they exist, though 'iter_batches' defaults to all.
    return next(pf.iter_batches(batch_size=10_000))
# ---
def make(self):
        comps = {}
        for name, component in self.components.items():
            comps[name] = eval(component['class'])(config=eval(component['config']))
        return Compound(linkages=self.linkages, **comps)
# ---
def __init__(self):
                self.scalar = jnp.zeros(
                    (),
                )
# ---
def __add__(self, other):
        assert self.n_classes == other.n_classes, "Number of classes does not match"
        return MulticlassContingencyTable(self.table + other.table,
                                          n_classes=self.n_classes,
                                          class_names=self.class_names)
# ---
def prepare_sqlite(self):
        self.dbname = "__testdb__smadata2_%s_.sqlite" % self.__class__.__name__
        self.bakname = self.dbname + ".bak"

        # Start with a blank slate
        removef(self.dbname)
        removef(self.bakname)

        self.prepopulate()

        if os.path.exists(self.dbname):
            self.original = open(self.dbname).read()
        else:
            self.original = None
# ---
def go(*args, **kw):
                canary2.append(name)
# ---
def project_back(
    grad: Array,
    Q: List[Union[Array, None]],
    precision: jax.lax.PrecisionLike = jax.lax.Precision.HIGHEST,
) -> Array:
    for mat in Q:
        if mat is not None:  # noqa: SIM108
            grad = jnp.tensordot(
                grad,
                mat,
                axes=((0,), (1,)),
                precision=precision,
            )
        else:
            grad = jnp.moveaxis(grad, 0, -1)

    return grad
# ---
def make_bug_cache_key(self, repository, bug_id):
        """Returns a key to use when caching fetched bug information."""
        return 'repository-%s-bug-%s' % (repository.pk, bug_id)
# ---
def tiny_cfg():
    return TreeDiffusionConfig(
        vocab_size=128,
        hidden_dim=64,
        intermediate_dim=128,
        num_layers=2,
        num_heads=4,
        num_kv_heads=4,
        max_seq_len=64,
    )
# ---
def test_mean_pool3d():
    _x = jnp.arange(64).reshape(4, 4, 4)
    x = hax.named(_x, ("H", "W", "D"))
    output = mean_pool((hax.Axis("H", 1), hax.Axis("W", 3), hax.Axis("D", 1)), x, stride=2)

    answer = jnp.array([[[4, 6]], [[36, 38]]])

    assert jnp.all(output.array == answer)
# ---
def loss_function(model: LmHeadModel, example: LmExample, *, key=None):
        return model.compute_next_token_loss(example, key=key, logsumexp_weight=config.z_loss_weight)
# ---
def test_transaction_engine_ctx_rollback(self):
        fn = self._trans_rollback_fn()
        ctx = testing.db.begin()
        assert_raises_message(
            Exception,
            "breakage",
            testing.run_as_contextmanager, ctx, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def test_empty_homogeneous_tuples(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.y).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def __getattr__(self, attr):
        if attr not in self.lookup:
            raise AttributeError
        return self.lookup[attr]
# ---
def withdraw(self, amount):
        if self.balance - amount < self.minimum_balance:
            print('Sorry, minimum balance must be maintained.')
        else:
            BankAccount.withdraw(self, amount)
# ---
def force_assign(self, cr, uid, ids, context=None):
        """ Changes the state to assigned.
        @return: True
        """
        self.write(cr, uid, ids, {'state': 'assigned'})
        wf_service = netsvc.LocalService('workflow')
        for move in self.browse(cr, uid, ids, context):
            if move.picking_id:
                wf_service.trg_write(uid, 'stock.picking', move.picking_id.id, cr)
        return True
# ---
def exists_env_key(self, key):
        ''' return whether a key, value  pair exists '''
        results = self.get_env_vars()
        if not results:
            return False

        for result in results:
            if result['name'] == key:
                return True

        return False
# ---
def build_asr(self, Vocab: Axis, *, key: PRNGKeyArray) -> "ASRMixin":
        return self.asr_model_type.init(Vocab, self, key=key)
# ---
def ravel(array: NamedArray, new_axis_name: AxisSelector) -> NamedArray:
    """
    Returns a flattened view of the array, with all axes merged into one
    """
    flattened = flatten_axes(array, array.axes, new_axis_name)
    return flattened
# ---
def logs_tail(self, handle: VllmServerHandle, *, max_lines: int = 200) -> str:
        return _docker_logs_tail(handle.docker_container_name, max_lines=max_lines)
# ---
def polyval(p: NamedArray | ArrayLike, x: NamedOrNumeric) -> NamedOrNumeric:
    """Named version of [jax.numpy.polyval][].

    When ``x`` is a [haliax.NamedArray][], the returned array reuses ``x``'s axes.
    Otherwise a regular :mod:`jax.numpy` array is returned.
    """

    arr_p, arr_x = unwrap_namedarrays(p, x)
    result = jnp.polyval(arr_p, arr_x)
    if isinstance(x, NamedArray):
        return NamedArray(result, x.axes)
    else:
        return result
# ---
def date(selector):
                return DateGuesser(Regexp(CleanText(selector), r'\w+ (\d{2}/\d{2})'), Env('date_guesser')) | Transaction.Date(selector)
# ---
def _workflow_trigger(self, cr, uid, ids, trigger, context=None):
        #override in order to trigger the workflow of stock.picking at the end of create, write and unlink operation
        #instead of it's own workflow (which is not existing)
        return self.pool.get('stock.picking')._workflow_trigger(cr, uid, ids, trigger, context=context)
# ---
def switch_func(txn):
            txn.date = swapped_date(txn.date, first, second)
# ---
def setupUi(self, Dialog):
        Dialog.setObjectName(_fromUtf8("Dialog"))
        Dialog.resize(1000, 400)

        self.gridLayout = QtGui.QGridLayout(Dialog)
        self.gridLayout.setObjectName(_fromUtf8("gridLayout"))

        # list of Events
        self.prepare_form(Dialog)

        self.retranslateUi(Dialog)
        QtCore.QMetaObject.connectSlotsByName(Dialog)
# ---
def __init__(
        self,
        source: ShardedDataSource[T_co],
        fn: Callable[[list[T_co]], Iterable[U]],
        batch_size,
        num_cpus=1,
        num_gpus=0,
        output_exemplar=None,
        **resources,
    ):
        self.source = source
        self._transform = _BatchMapTransform(
            fn, batch_size, num_cpus, num_gpus, resources, output_exemplar=output_exemplar
        )
# ---
def main():
#    utils.drop_privileges()
    mapper = NoiseMapper()
    mapper.run()
# ---
def get_worker(self, worker_id: WorkerId) -> ControllerWorker | None:
        with self._lock:
            return self._workers.get(worker_id)
# ---
def _reduce_gen(shard: Any, key_fn: Callable, reducer_fn: Callable) -> Iterator:
    for key, items_iter in _merge_sorted_chunks(shard, key_fn):
        yield reducer_fn(key, items_iter)
# ---
def __init__(self, actor_ref: ray.actor.ActorHandle):
        # Store under a mangled name so __getattr__ doesn't recurse
        object.__setattr__(self, "_actor_ref", actor_ref)
# ---
def GetSession(self, request, context):
        """Gets a session. Returns `NOT_FOUND` if the session does not exist.
    This is mainly useful for determining whether a session is still
    alive.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def test_sequence_table_reserve_and_release_slot():
    pt = _make_table()
    sequences = SequenceTable.init(pt.max_seqs, pt.pages_per_seq, pt.page_size)

    sequences, slot_arr = sequences.reserve_slot()
    slot = int(slot_arr)
    assert slot == 0
    assert bool(sequences.used_mask.array[slot])

    sequences = sequences.release_slot(slot)
    assert not bool(sequences.used_mask.array[0])
# ---
def do_load(self, e):
        self.remote.do_load(self.clear_first.GetValue())
# ---
def _process_sensor(sensor_data):
    sensor_data_fields = sensor_data.split('\n')
    sensor_data_dict = {}
    for field in sensor_data_fields:
        if not field:
            continue
        kv_value = field.split(':')
        if len(kv_value) != 2:
            continue
        sensor_data_dict[kv_value[0].strip()] = kv_value[1].strip()

    return sensor_data_dict
# ---
def __init__(self, bot):
        self.bot = bot
        """
        imLink = http://services.runescape.com/m=hiscore_ironman/index_lite.ws?player=
        nmLink = http://services.runescape.com/m=hiscore/index_lite.ws?player=
        """
# ---


def add(x: int, y: int):
    """Add two numbers x and y
    >>> add(2, 3)
    5
    >>> add(5, 7)
    12
    """
    return x + y
# ---
def is_invalid(x, invalid=INVALID):
    return (x < 0) | (x == invalid)
# ---
def map(self, *args, **kwargs):
        return map(*args, **kwargs)
# ---
def __getitem__(self, key):
        return self._dict[key]
# ---
def get_es(url):
    o = urlparse(url)
    es = elasticsearch.Elasticsearch(hosts=[{'host': o.hostname, 'port': o.port}])
    es.transport.serializer = ElasticJSONSerializer()
    return es
# ---
def sub_step(self, skip):
        self.command('sub_step', skip)
# ---
def __post_init__(self):
        self.thread = None
        self.stop_flag = threading.Event()
# ---
def test_transaction_connection_fn_commit(self):
        fn = self._trans_fn()
        conn = testing.db.connect()
        conn.transaction(fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def finger_all(self):
        '''
        Return fingerprints for all keys
        '''
        ret = {}
        for status, keys in six.iteritems(self.list_keys()):
            ret[status] = {}
            for key in keys:
                if status == 'local':
                    path = os.path.join(self.opts['pki_dir'], key)
                else:
                    path = os.path.join(self.opts['pki_dir'], status, key)
                ret[status][key] = self._get_key_finger(path)
        return ret
# ---
def shard_names(self) -> Sequence[str]:
            return [f"shard_{i}" for i in range(4)]
# ---
def crispr_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'method': 'CRISPR'
    }
# ---
def _convert_token_to_id(self, token: str) -> int:
        return int(token)
# ---
def target(stop_event: threading.Event) -> None:
            ctx.run(self._run, stop_event)
# ---
def release(self, on_element):
        """Releasing a held mouse button.
        Args:
            on_element: The element to mouse up.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.MOUSE_UP, {}))
        return self
# ---
def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LmDatasetSourceConfigBase:
        """
        Create a Levanter dataset source config from this config and the actual output path.
        """
        pass
# ---
def cancel_units (self, uids) :
        """
        Cancel given unit(s)
        """

        raise Exception ("%s.cancel_unit() is not implemented" % self.__class__.__name__)
# ---
def embed(self, input_ids, *args):
        return self.token_embeddings(input_ids)
# ---
def test_impl(A, B):
            df = pd.DataFrame({'A': A, 'B': B})
            df2 = df.groupby('A', as_index=False)['B'].sum()
            # TODO: fix handling of df setitem to force match of array dists
            # probably with a new node that is appended to the end of basic block
            # df2['C'] = np.full(len(df2.B), 3, np.int8)
            # TODO: full_like for Series
            df2['C'] = np.full_like(df2.B.values, 3, np.int8)
            return df2
# ---
def get_all_address_state(self):
        with self.lock:
            return self._state.get_all_address_state()
# ---
def title(self):
        '''Title of the ebook. (mandatory)

        If this property is left unset, it defaults to "Untitled".'''
        try:
            return self._title
        except AttributeError:
            self.title = 'Untitled'
            return self._title
# ---
def _stop_task() -> None:
            attempt.should_stop = True
            if attempt.container_id:
                try:
                    self._runtime.kill(attempt.container_id, force=True)
                except RuntimeError:
                    pass
# ---
def _updateOrcaModifier(self):
        combobox = self.get_widget("orcaModifierComboBox")
        keystring = ", ".join(self.prefsDict["orcaModifierKeys"])
        combobox.set_active(self.getComboBoxIndex(combobox, keystring))
# ---
def __gt__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.greater(self, other)
# ---
def fetch_task_logs(self, request: cluster__pb2.Worker.FetchTaskLogsRequest, ctx: RequestContext) -> cluster__pb2.Worker.FetchTaskLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def EscapeForVim( text ):
  return ToUnicode( text.replace( "'", "''" ) )
# ---
def test_iter(self):
        prio_set_list = event._PrioritizedSetList()
        objs = [(i,) for i in range(5)]
        for i, obj in enumerate(objs):
            prio_set_list.add(-i, obj)

        for i, set_ in enumerate(prio_set_list):
            assert set_ == (-i, {objs[i]})
# ---
def withdraw(self, amount):
        self.balance -= amount
        return self.balance
# ---
def __contains__(self, obj: T) -> bool:
        return obj in self._obj_to_index
# ---
def Vocab(self) -> Axis:
        return self.decoder.embeddings.Vocab
# ---
def handle_request(self, request):
        res = super(System, self).handle_request(request)
        if res is not None: return res
# ---
def __iter__(self):
        return iter(self._queue)
# ---
def scan_for_old(ng):
    nodes = [n for n in ng.nodes if n.bl_idname in old_bl_idnames]
    for node in nodes:
        mark_old(node)
# ---
def _wrapped_hist(arr):
        return _single_shard_histogram(arr, bin_edges=bins, reduce_mesh=flattened_spec)
# ---
def eval_model(evaluator: "TaggedEvaluator", model: LmHeadModel, prefix: str = "") -> dict[str, float]:
    with levanter.tracker.capture_time() as time_fn:
        result = evaluator.evaluate(model)
    log_dict = _construct_log_dict(evaluator, result, time_fn(), prefix=prefix)
    return log_dict
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> LocalActorHandle:
        """Create an in-process actor, returning a handle immediately."""
        group = self.create_actor_group(actor_class, *args, name=name, count=1, resources=resources, **kwargs)
        return group.wait_ready()[0]
# ---
def get_lines(string):
    """
    Return list of lines extracted from string.
    """
    line_list = string.split('\n')

    new_list = []
    for l in line_list:
        new_list += [l[i*LINEWIDTH:(i+1)*LINEWIDTH] for i in range(len(l) // LINEWIDTH + 1)]

    return new_list
# ---
def nanstd(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanstd, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def clear(self):
        """Clears Enforcer rules, policy's cache and policy's path."""
        self.set_rules({})
        self.policy_path = None
# ---
def do_vmap(init: CarryT, *args, **kwargs) -> OutputT_co:
            # Apply fn to each block independently
            outputs = []
            for block in self.blocks:
                output = fn(block, init, *args, **kwargs)
                outputs.append(output)

            # Stack the outputs
            stacked_out = haliax.tree_util.tree_map(lambda *x: haliax.stack(self.Block, x), *outputs)
            return stacked_out
# ---
def test_add_event_after_connect(self):
        # new feature as of #2978
        canary = Mock()
        e1 = create_engine(config.db_url)
        assert not e1._has_events

        conn = e1.connect()

        event.listen(e1, "before_execute", canary.be1)
        conn.execute(select([1]))

        eq_(canary.be1.call_count, 1)

        conn._branch().execute(select([1]))
        eq_(canary.be1.call_count, 2)
# ---
def deleted_user(id):
    """
    Create a User object for a deleted user.
    """
    deleted_user = {
        "id": id,
        "name": "deleted-" + id,
        "deleted": True,
        "is_bot": False,
        "is_app_user": False,
    }
    return User(deleted_user)
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
            # parse the shard name to get the shard number
            shard_num = int(shard_name.split("_")[1])
            for i in range(10):
                if i == self.crash_point:
                    raise _CustomException(f"Crashing at {shard_num} {i} {self.crash_point}")
                if i >= row:
                    yield [shard_num * 10 + i] * 10
# ---
def add_hook(self, fn: Optional[Callable[[StepInfo], Any] | Callback | JitCallback] = None, *, every: int = 1):
        return self.hooks.add_hook(fn, every=every)
# ---
def handle(self, *args, **options):

        if options['profile_file']:
            profiler = Profile()
            profiler.runcall(self._handle, *args, **options)
            profiler.dump_stats(options['profile_file'])
        else:
            self._handle(*args, **options)
# ---
def is_terminal(self) -> bool:
        """True if VM is in a terminal state."""
        return self.info.state in (
            vm_pb2.VM_STATE_READY,
            vm_pb2.VM_STATE_FAILED,
            vm_pb2.VM_STATE_TERMINATED,
            vm_pb2.VM_STATE_PREEMPTED,
        )
# ---
def _flavor_clean_up(self, flavor_id):
        try:
            self.admin_flavors_client.delete_flavor(flavor_id)
            self.admin_flavors_client.wait_for_resource_deletion(flavor_id)
        except exceptions.NotFound:
            pass
# ---
def __init__(self, docker_image: str | None, docker_run_args: list[str] | None) -> None:
        self._docker_image = docker_image
        self._docker_run_args = docker_run_args
# ---
def ListTasks(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def test_engine_connect(self):
        engine = engines.testing_engine()

        tracker = Mock()
        event.listen(engine, "engine_connect", tracker)

        c1 = engine.connect()
        c2 = c1._branch()
        c1.close()
        eq_(
            tracker.mock_calls,
            [call(c1, False), call(c2, True)]
        )
# ---
def test_profiler_get_base_id_unset_case(self, mock_generate_uuid):
        mock_generate_uuid.return_value = "42"
        prof = profiler._Profiler("secret")
        self.assertEqual(prof.get_base_id(), "42")
        self.assertEqual(prof.get_parent_id(), "42")
# ---
def populateComboBox(self, combobox, items):
        """Populates the combobox with the items provided.

        Arguments:
        - combobox: the GtkComboBox to populate
        - items: the list of strings with which to populate it
        """

        model = Gtk.ListStore(str)
        for item in items:
            model.append([item])
        combobox.set_model(model)
# ---
def test_attentionmask_materialize_causal():
    mask = AttentionMask.causal()
    allowed = mask.materialize_mask(4, 4)
    expected = jnp.array(
        [
            [True, False, False, False],
            [True, True, False, False],
            [True, True, True, False],
            [True, True, True, True],
        ],
        dtype=bool,
    )
    assert allowed is not None
    assert allowed.shape == (4, 4)
    assert jnp.array_equal(allowed, expected)
# ---
def path(self) -> str:
        return self._path
# ---
def __xor__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_xor(self, other)
# ---
def test_deduplicate_all_duplicates(backend):
    """Test deduplication when all items have same key."""
    data = [{"id": 1, "val": f"item_{i}"} for i in range(10)]

    ds = Dataset.from_list(data).deduplicate(key=lambda x: x["id"])

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0]["id"] == 1
# ---
def intersects(self, bbox, srs):
        bbox = self._bbox_in_coverage_srs(bbox, srs)
        return bbox_intersects(self.bbox, bbox)
# ---
def get_url(self):
        '''return the dsn back into url form'''
        return urlparse.urlunparse((
            self.scheme,
            self.netloc,
            self.path,
            self.params,
            self.query_str,
            self.fragment,
        ))
# ---
def __getitem__(self, index):
        """Get the value at the given index.

        Parameters
        ----------
        index : int
            The index into the array.

        """
        return self._data[index]
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Start Time", device_id)
# ---
def default_hf_config(self) -> HfConfig:
        return self.hf_config_from_hf_checkpoint(None)
# ---
def _component_cache_dir(name: str, component: DatasetComponent, default_root: str | None) -> str:
    base = component.cache_dir if component.cache_dir is not None else default_root
    if base is None:
        raise ValueError(f"No cache_dir provided for component {name}")
    if component.cache_dir is None:
        return os.path.join(base, name)
    return base
# ---
def accumulate_x_grad():
        res = jax.lax.dot_general(
            xw_scratch_ref[...],
            w_ref[...],
            (((1,), (1,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        x_read_future.wait()
        x_grad_tile_ref[...] += res
        x_write_future.start()
# ---
def timerules():
    """Adds, Updates or deletes time rule for the given element"""
    rules = request.get_json()

    if len(rules) == 0:
        raise Exception("No elements in the list")

    for rule in rules:
        if 'id' not in rule:
            rule['id'] = None

    home_services.save_time_rules(rules)
    return 'OK'
# ---
def test_available(self):
        eq_(self.record.available, True)
# ---
def run_debug(scenario, blend_filepath=None):
    try:
        result = run(scenario)
    except Exception as e:
        if blend_filepath:
            bpy.ops.wm.save_as_mainfile(filepath=blend_filepath)
        assert False, e
    if blend_filepath:
        bpy.ops.wm.save_as_mainfile(filepath=blend_filepath)
    return result
# ---
def _inventory_line_hook(self, cr, uid, inventory_line, move_vals):
        """ Creates a stock move from an inventory line
        @param inventory_line:
        @param move_vals:
        @return:
        """
        return self.pool.get('stock.move').create(cr, uid, move_vals)
# ---
def test_is_informational(self):
        self.assertFalse(status.is_informational(99))
        self.assertFalse(status.is_informational(200))

        for i in range(100, 199):
            self.assertTrue(status.is_informational(i))
# ---
def _kill_old_container(name):
    try:
        logger.info(f"Killing old container {name}")
        _run_command("sudo", "docker", "rm", "-f", name)
    except subprocess.CalledProcessError:
        pass
# ---
def icon(self):
        """Icon to use in the frontend, if any."""
        return self._icon
# ---
def Vocab(self) -> Axis:
        return self.embeddings.Vocab
# ---
def _make_allocator(max_pages=8, max_seqs=2, page_size=4, pages_per_seq=3):
    pt = PageTable.init(max_pages, max_seqs, page_size, pages_per_seq)
    sequences = SequenceTable.init(max_seqs, pages_per_seq, page_size)
    return sequences, pt
# ---
def time_until_next(self) -> float:
        """Get seconds until next allowed run (0.0 if can run now)."""
        if self._last_run is None:
            return 0.0
        elapsed = time.monotonic() - self._last_run
        return max(0.0, self._interval - elapsed)
# ---
def __str__(self):
        return "NoncentralTDistr(df={0},mu={1})#{2}".format(self.df, self.mu, self.id())
# ---
def testIfElif(self):
    self.assertEqual((0, 'foo\nbar\n'), _GrumpRun(textwrap.dedent("""\
        if True:
          print 'foo'
        elif False:
          print 'bar'
        if False:
          print 'foo'
        elif True:
          print 'bar'""")))
# ---
def on_signal(sig, frame):
        click.echo("\nClosing tunnel...")
        stop.set()
# ---
def __len__(self):
        return len(self._data)
# ---
def __divmod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.divmod(self, other)
# ---
def __init__(self, pluginname, account=None):
        self.cookies = {}
        self.plugin = pluginname
        self.account = account
# ---
def get_value_from_remote(obj):  # Gather
    obj = compss_wait_on(obj)
    return obj
# ---
def image_url(self):
        return self._get_profile().image_url()
# ---
def with_cpu(**kwargs: Any) -> ResourceConfig:
        return ResourceConfig(device=CpuConfig(), **kwargs)
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        """Block until the job completes."""
        if self._ref is not None:
            return self._wait_ref(timeout, raise_on_failure)
        return self._wait_submission(timeout, raise_on_failure)
# ---
def tree_unflatten(cls, aux_data, children):
        inner_state = jax.tree.unflatten(aux_data, children[:-5])

        return cls(
            inner_opt_state=inner_state,
            losses=children[-5],
            grad_norms=children[-4],
            valid_mask=children[-3],
            current_idx=children[-2],
            count=children[-1],
        )
# ---
def FiletypesForBuffer( buffer_object ):
  # NOTE: Getting &ft for other buffers only works when the buffer has been
  # visited by the user at least once, which is true for modified buffers
  return GetBufferOption( buffer_object, 'ft' ).split( '.' )
# ---
def get_line(self, n):
        """
        Return the line with index n.
        """
        if n < self.num_lines:
            return self.lines[n]
        else:
            raise IndexError("Line index out of range.")
# ---
def _run_with_lockfile():
        with remove_tpu_lockfile_on_exit():
            convert_task()
# ---
def redirect(h, req, url=to_url):
            h.http_error_302(req, MockFile(), 302, "Blah",
                             MockHeaders({"location": url}))
# ---
def _shard_name_to_url_mapping(self):
        return _mk_shard_name_mapping(self.urls)
# ---
def test_resolve_audio_pointer():
    AudioTextUrlDataSource.resolve_audio_pointer("https://ccrma.stanford.edu/~jos/mp3/trumpet.mp3", 16_000)
# ---
def _convert_example(inputs: AudioTextDict) -> "AudioTextExample":
            tokens = hax.named(inputs["input_ids"], self.TextPos)
            audio_features = hax.named(inputs["input_features"], self.AudioPos)
            out = AudioTextExample.init(audio_features, tokens, ignore_id=self.ignore_id)
            out = jax.lax.with_sharding_constraint(out, sharding)
            return out
# ---
def controller_metadata_key(label_prefix: str) -> str:
    """Metadata key to mark a VM as the controller for a given prefix."""
    return f"iris-controller-{label_prefix}"
# ---
def rollouts_to_training_batch(rollouts, max_tokens=32, pad_token_id=0):
    """Helper function to convert rollouts to training batch for testing."""
    return train_batch.create_training_batch_from_rollouts(rollouts, max_tokens, pad_token_id)
# ---
def next(self, psm: PSM):
        if psm.char in string.hexdigits:
            self.pattern.pattern += psm.char
            count = len(self.pattern.pattern)
            return self.prev if count >= 2 else self
        else:
            psm.error = "expected ASCII hexadecimal character"
# ---
def _str_to_int(x: str) -> int:
    x = x.replace(",", "")
    x_float = float(x)
    return int(x_float)
# ---
def __init__(
            self,
            filepath,
            hdfs_conn_id='hdfs_default',
            *args, **kwargs):
        super(HdfsSensor, self).__init__(*args, **kwargs)
        self.filepath = filepath
        self.hdfs_conn_id = hdfs_conn_id
# ---
def f(p):
            token_ids = jnp.zeros((batch, seq), dtype=jnp.int32)
            token_ids = jax.sharding.reshard(token_ids, Pbatch)
            loss_weight = jnp.ones((batch, seq), dtype=jnp.float32)
            loss_weight = jax.sharding.reshard(loss_weight, Pbatch)
            return loss_fn(p, token_ids, loss_weight, cfg, mask=AttentionMask.causal(), reduction="mean")
# ---
def skip_if_checkpoint_not_accessible(path: str):
    def try_load_path(path):
        try:
            fs, path_to_open = _get_fs_and_plain_path(path)
            fs.open(path_to_open, "rb")
        except Exception:
            return False
        else:
            return True

    return pytest.mark.skipif(not try_load_path(path), reason="Checkpoint not accessible")
# ---
def drop_vars(self, *names):  # pragma: no cover
        warnings.warn('the Dataset method `drop_vars` has been deprecated; '
                      'use `drop` instead',
                      FutureWarning, stacklevel=2)
        return self.drop(names)
# ---
def the_object_name_is_selected(name):
    i_deselect_all_objects()
    additionally_the_object_name_is_selected(name)
# ---
def __add_random_fuzzing_values(self):
        n = self.ui.spinBoxNumberRandom.value()
        minimum = self.ui.spinBoxRandomMinimum.value()
        maximum = self.ui.spinBoxRandomMaximum.value()
        self.fuzz_table_model.add_random(n, minimum, maximum)
# ---
def test_just_return_first_item_found(self):
        def is_big_name(item):
            return len(item) > 4

        names = ["Jim", "Bill", "Clarence", "Doug", "Eli"]
        name = None

        iterator = filter(is_big_name, names)
        try:
            name = next(iterator)
        except StopIteration:
            msg = 'Ran out of big names'

        self.assertEqual("Clarence", name)
# ---
def unschedule_router_precommit(self, context, router_context):
        pass
# ---
def skip_if_no_torch(f):
    return pytest.mark.skipif(not has_torch(), reason="torch not installed")(f)
# ---
def rsqrt(a: A) -> A:
    return wrap_elemwise_unary(jax.lax.rsqrt, a)
# ---
def _binding_name(callback_or_cmd):
        return 'py_kb_{:016x}'.format(hash(callback_or_cmd)&0xffffffffffffffff)
# ---
def setTitle(self, title):
        self.__title = title
# ---
def mod_ndwi_learned(domain, b):
    if domain.unflooded_domain == None:
        print('No unflooded training domain provided.')
        return None
    unflooded_b = modis_utilities.compute_modis_indices(domain.unflooded_domain)
    water_mask  = modis_utilities.get_permanent_water_mask()
    threshold   = modis_utilities.compute_binary_threshold(get_mod_ndwi(unflooded_b), water_mask, domain.bounds)
    return mod_ndwi(domain, b, threshold)
# ---
def isusable(self, word):
        """Returns a value indicating if the given word should be kept as a
        suggestion for autocomplete."""
        return len(word) > self.comp_len + 2
# ---
def __repr__(self):
        return "Grid with %i knots"%self.nbknots
# ---
def _product_virtual_get(self, cr, uid, id, product_ids=False, context=None, states=None):
        if states is None:
            states = ['done']
        return self._product_all_get(cr, uid, id, product_ids, context, ['confirmed', 'waiting', 'assigned', 'done'])
# ---
def root():

    return 'Started a background process with PID ' + str(backProc.pid) + " is running: " + str(backProc.is_alive())
# ---
def test_lambda_output_shape_function(self):
    def get_output_shape(input_shape):
      return 1 * input_shape

    l = keras.layers.Lambda(lambda x: x + 1, output_shape=get_output_shape)
    l(keras.backend.variable(np.ones((1, 1))))
    self.assertEqual('lambda', l.get_config()['output_shape_type'])
# ---
def dec_refcounts_for_seq(pages_row, ref_counts):
            valid = is_valid(pages_row)

            def body(i, rc):
                def dec(rc):
                    page = pages_row["page", i].scalar()
                    return rc.at["page", page].add(-1)

                return jax.lax.cond(valid["page", i].scalar(), dec, lambda x: x, rc)

            updated = jax.lax.fori_loop(0, pages_row.axis_size("page"), body, ref_counts)
            return hax.maximum(updated, hax.zeros_like(updated))
# ---
def __call__(self, batch: Sequence[Sequence[int]]):
            raise RuntimeError("exc")
# ---
def num_position_tokens(self) -> int:
        return self.max_seq_len
# ---
def Exists(self):
    """Returns True if the log group exists."""
    describe_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'describe-log-groups',
        '--log-group-name-prefix', self.name,
        '--no-paginate'
    ]
    stdout, _, _ = vm_util.IssueCommand(describe_cmd)
    log_groups = json.loads(stdout)['logGroups']
    group = next((group for group in log_groups
                  if group['logGroupName'] == self.name), None)
    return bool(group)
# ---
def __init__(self, subtype, data, msg=None):
        m = "Database is missing data {} for {}".format(data, subtype)
        super(PhylotyperError, self).__init__(subtype, m)
        self.data = data
# ---
def lora_state_dict(model: M, prefix: Optional[str] = DEFAULT_DICT_PREFIX) -> StateDict:
    """
    Returns a state dict of the LoRA parameters of the given model without other parameters.
    This method attempts to return a state dict compatible with PEFT's import method.
    """
    state_dict = to_torch_compatible_state_dict(filter_lora_params(model), prefix=prefix)
    return {k: v for k, v in state_dict.items() if v is not None}
# ---
def test___doc__(self):
        self.assertEqual(
            ctds.Parameter.__doc__,
            '''\
Parameter(value, output=False)

Explicitly define a parameter for :py:meth:`.callproc`,
:py:meth:`.execute`, or :py:meth:`.executemany`. This is necessary
to indicate whether a parameter is *SQL* `OUTPUT` or `INPUT/OUTPUT`
parameter.

:param object value: The parameter's value.
:param bool output: Is the parameter an output parameter.
'''
        )
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[False] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[False] = ...,
) -> NamedArray: ...
# ---
def test_utils_to_probs(utilities, test_data):
    probs = mnl.utils_to_probs(utilities)
    pdt.assert_frame_equal(probs, test_data['probabilities'])
# ---
def test_launch_job_rejects_duplicate_name(service, job_request):
    """Verify launch_job rejects duplicate job names."""
    request = job_request("duplicate-job")

    response = service.launch_job(request, None)
    assert response.job_id == JobName.root("duplicate-job").to_wire()

    with pytest.raises(ConnectError) as exc_info:
        service.launch_job(request, None)

    assert exc_info.value.code == Code.ALREADY_EXISTS
    assert JobName.root("duplicate-job").to_wire() in exc_info.value.message
# ---
def without_axes(axis_spec: Sequence[AxisSelector], to_remove: AxisSelection, allow_mismatched_sizes=False) -> tuple[AxisSpec, ...]:  # type: ignore
    ...
# ---
def _inv_sqrt_decay_schedule(lr: float, min_lr: float, warmup_steps: int, timescale: float = 10000):
    def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / jnp.sqrt(jnp.maximum(count + warmup_steps, 1) / timescale))
        return jnp.maximum(lr * decay, min_lr)

    return schedule
# ---
def get_client(self) -> IrisClient:
        if self._rpc_client is None:
            self._rpc_client = IrisClient.remote(
                f"http://127.0.0.1:{self._controller_port}",
                workspace=Path(__file__).parent.parent.parent,  # lib/iris
            )
        return self._rpc_client
# ---
def actual_head_size(self):
        """Returns the actual head size based on the head_dim or calculated from hidden_dim and num_heads."""
        if self.head_dim is not None:
            return self.head_dim
        return self.hidden_dim // self.num_heads
# ---
def vms(self) -> list[FakeVm]:
        """Get VMs in this VM group.

        Returns FakeVm instances (not ManagedVm). The autoscaler uses this
        to find workers by worker_id, so FakeVm must have a compatible interface.
        """
        return list(self._vms)
# ---
def set_rebus_controller(self, rebus_controller):
        self.rebus_controller = rebus_controller
# ---
def llama_small_config() -> HFCompatConfig:
    hf_config = AutoConfig.from_pretrained(MODEL_NAME)
    hf_converter = HFCheckpointConverter.from_hf(MODEL_NAME)
    lev_config = hf_converter.config_from_hf_config(hf_config)
    return dataclasses.replace(lev_config, max_seq_len=MAX_INPUT_TOKENS + MAX_OUTPUT_TOKENS, tokenizer=MODEL_TOKENIZER)
# ---
def unsize_axes(axis_spec: PartialShapeDict) -> PartialShapeDict: ...
# ---
def assumed_state(self):
        """State is assumed, if no template given."""
        return self._template is None
# ---
def store(self) -> TreeStore[T_co]:
        return self._store
# ---
def _pspec_for(self, shape_spec: ShapeSpec | NamedShapeSpec) -> PartitionSpec:
        if isinstance(shape_spec, ShapeSpec):  # type: ignore
            batch_name = hax.partitioning.physical_axis_name(self.dl.batch_axis_name, self.dl.axis_resources)
            return PartitionSpec(batch_name, *((None,) * (len(shape_spec.shape) - 1)))
        else:
            return hax.partitioning.pspec_for_axis(shape_spec.shape, self.dl.axis_resources)
# ---
def test_impl_2(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.rolling(7).sum()
            return Ac.sum()
# ---
def has_error(self):
        return CleanText('//span[@id="id_error_msg"]')(self.doc)
# ---
def epoch_ms(self) -> int:
        """Get milliseconds since epoch."""
        return self._epoch_ms
# ---
def zone(self) -> str:
        return ""
# ---
def message_subscribe(self, *args, **kwargs):
        """Send the subscribe action on stock.picking model as it uses _name in request"""
        return self.pool.get('stock.picking').message_subscribe(*args, **kwargs)
# ---
def remove(self):
        self._set_changed_options()
        if self.module.check_mode:
            return True
        self.remove_from_device()
        if self.exists():
            raise F5ModuleError("Failed to delete the resource.")
        # Artificial sleeping to wait for remote licensing (on BIG-IP) to complete
        #
        # This should be something that BIG-IQ can do natively in 6.1-ish time.
        time.sleep(60)
        return True
# ---
def read(self):
        return ''
# ---
def astype(self, dtype):
        return PreparedBatch(self.data.astype(dtype), self.offsets, self.shapes)
# ---
def init_optimizer_for_trainables(optimizer, trainable_model):
    """
    Initializes the optimizer state for the trainable parameters of the model.
    """
    _, trainable = partition_for_grad_overwrite(trainable_model)  # doesn't make a huge difference, but saves some ram
    opt_state = optimizer.init(trainable)
    return opt_state
# ---
def test_list_type_access_public(self):
        """Querying os-volume-type-access on public type should return 404."""
        req = fakes.HTTPRequest.blank('/v2/%s/types/os-volume-type-access' %
                                      fake.PROJECT_ID,
                                      use_admin_context=True)
        self.assertRaises(webob.exc.HTTPNotFound,
                          self.type_access_controller.index,
                          req, fake.VOLUME_TYPE2_ID)
# ---
def exists_env_value(self, key, value):
        ''' return whether a key, value  pair exists '''
        results = self.get_env_vars()
        if not results:
            return False

        for result in results:
            if result['name'] == key and result['value'] == value:
                return True

        return False
# ---
def test_entrypoint_command():
    ep = Entrypoint.from_command("echo", "hello")
    assert ep.is_command
    assert not ep.is_callable
    assert ep.command == ["echo", "hello"]
# ---
def _getRateForVoiceType(self, voiceType):
        """Gets the speaking rate value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM

        Returns the rate value for the given voice type, or None if
        not set.
        """

        return self._getKeyValueForVoiceType(voiceType, acss.ACSS.RATE)
# ---
def broadcast_qs(_, ps, q, s):
            stack_n = ps[0]
            if partition_grads_into_blocks:
                # add leading dim for stacked partitions
                q = jax.tree.map(lambda x: jnp.repeat(jnp.expand_dims(x, 0), stack_n, axis=0), q)
            if s > 0:
                # add leading dim if we're scanning this layer
                q = jax.tree.map(lambda d: jnp.repeat(jnp.expand_dims(d, 0), s, axis=0), q)
            return q
# ---
def get_external_ips(self):
        ''' get a list of external_ips '''
        return self.get(Service.external_ips) or []
# ---
def __call__(self, x: NamedArray, attn_mask: Optional[AttentionMask | NamedArray], *, key=None) -> NamedArray:
        keys = hax.jax_utils.maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.blocks.fold(x, attn_mask, hax.arange(self.config.Layers), key=keys)
        x = self.ln_f(x)

        return x
# ---
def LineAndColumnNumbersClamped( line_num, column_num ):
  new_line_num = line_num
  new_column_num = column_num

  max_line = len( vim.current.buffer )
  if line_num and line_num > max_line:
    new_line_num = max_line

  max_column = len( vim.current.buffer[ new_line_num - 1 ] )
  if column_num and column_num > max_column:
    new_column_num = max_column

  return new_line_num, new_column_num
# ---
def render (self):
    with Page (self.canvas) as pg:
      for obj in self.objects:
        coords = pg.next (obj.asset)
        with XlateFrame (self.canvas, obj.tile_type, *coords,
                         inset_by = "margin"):
          # print ("Obj: ", obj.asset)
          obj.render ()
# ---
def do_block(carry: CarryT, block: M, *args, **kwargs) -> tuple[CarryT, OutputT_co]:
            carry, output = fn(block, carry, *args, **kwargs)
            return carry, output
# ---
def i_deselect_all_objects():
    bpy.context.view_layer.objects.active = None
    bpy.ops.object.select_all(action="DESELECT")
# ---
def test_keyboardinterrupt_disables_capturing(self, testdir):
        p = testdir.makepyfile("""
            def test_hello(capfd):
                import os
                os.write(1, str(42).encode('ascii'))
                raise KeyboardInterrupt()
        """)
        result = testdir.runpytest_subprocess(p)
        result.stdout.fnmatch_lines([
            "*KeyboardInterrupt*"
        ])
        assert result.ret == 2
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        if not hasattr(self, "_tracker_cm"):
            raise RuntimeError("This tracker is not set as the global tracker")
        self._tracker_cm.__exit__(exc_type, exc_val, exc_tb)
        delattr(self, "_tracker_cm")
# ---
def remove_worker(self, worker_id: WorkerId) -> ControllerWorker | None:
        with self._lock:
            return self._workers.pop(worker_id, None)
# ---
def skip_if_not_enough_devices(count: int):
    return pytest.mark.skipif(len(jax.devices()) < count, reason=f"Not enough devices ({len(jax.devices())})")
# ---
def subfunction(self):
        run_debug(function(self))
# ---
def vf_env():
    """Create a real verifiers SingleTurnEnv with example dataset."""
    dataset = vf.load_example_dataset("gsm8k", n=2)
    return vf.SingleTurnEnv(dataset=dataset)
# ---
def test_writeorg(self, tmpfile):
        data1, data2 = tobytes("foo"), tobytes("bar")
        cap = capture.FDCapture(tmpfile.fileno())
        cap.start()
        tmpfile.write(data1)
        tmpfile.flush()
        cap.writeorg(data2)
        scap = cap.snap()
        cap.done()
        assert scap == totext(data1)
        with open(tmpfile.name, 'rb') as stmp_file:
            stmp = stmp_file.read()
            assert stmp == data2
# ---
def with_tpu(tpu_type: str, *, slice_count: int = 1, **kwargs: Any) -> ResourceConfig:
        device = TpuConfig(variant=tpu_type)
        return ResourceConfig(device=device, replicas=slice_count, **kwargs)
# ---
def click_edit_button(self):
        """
        :rtype: EditMoviePage
        """
        self._click(BrowseMoviePageLocators.EDIT_BUTTON_LOCATOR)
        return EditMoviePage(self._driver)
# ---
def doMonteCarloNP(pointa, pointb, weights, nopoint):
    #print "weights ", weight
    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    #print dataset_Monte
    return getMisMatches(dataset_Monte, weights)
# ---
def discnav(self, command):
        self.command('discnav', command)
# ---
def read(self):
        ''' read from file '''
        # check if it exists
        if self.filename is None or not self.file_exists():
            return None

        contents = None
        with open(self.filename) as yfd:
            contents = yfd.read()

        return contents
# ---
def test_wait_for_condition_immediate() -> None:
    """Test wait_for_condition returns immediately when condition is already true."""
    start = time.monotonic()
    wait_for_condition(lambda: True, timeout=Duration.from_seconds(1.0))
    elapsed = time.monotonic() - start

    # Should return almost immediately
    assert elapsed < 0.1
# ---
def __init__(self, loggers: List[Tracker]):
        self.loggers = loggers
# ---
def __repr__(self):
        return f"MapOp(fn={_get_fn_name(self.fn)})"
# ---
def rearrange(self, axes: Sequence[AxisSelector | EllipsisType]) -> "NamedArray":
        """See [haliax.rearrange][] for details."""
        pass
# ---
def get_env_vars(self):
        '''return a environment variables '''
        return self.get(DeploymentConfig.env_path) or []
# ---
def remove(self, tag: str) -> None:
        subprocess.run(
            ["docker", "rmi", tag],
            capture_output=True,
            check=False,
        )
# ---
def __repr__(self):
        return f"EraShufflingDataset({repr(self.dataset)}, era_length={self.era_length})"
# ---
def from_array(array: jax.Array, num_bins: int = 31) -> "Histogram":
        array = array.ravel()
        min = array.min()
        max = array.max()
        num = array.size
        sum = array.sum()
        sum_squares = (array**2).sum()
        counts, edges = jax.numpy.histogram(array, bins=num_bins)
        return Histogram(min, max, num, sum, sum_squares, edges, counts)
# ---
def parse_lheading(self, m):
        """Parse setext heading."""
        self.tokens.append({
            'type': 'heading',
            'level': 1 if m.group(2) == '=' else 2,
            'text': m.group(1),
        })
# ---
def initSequences(controller):
    mysqlsteps = [
             {'title': 'Adding MySQL manifest entries',
              'functions':[createmanifest]}
    ]
    controller.addSequence("Installing MySQL", [], [], mysqlsteps)
# ---
def __post_init__(self):
        if self.update_rms_clipping is not None and self.update_rms_clipping <= 0:
            raise ValueError("update_rms_clipping must be a positive number or None.")

        if self.clip_update_norm is not None and self.update_rms_clipping is not None:
            raise ValueError("Cannot use both update_rms_clipping and clip_update_norm at the same time.")
# ---
def find_location(self):
        for course, score in self.courses:
            location = str( course.rec["BUILDING"] )+ " " + str( course.rec["ROOM"] )
            # just need to find the first one, so break after this happens
            break
        return location
# ---
def log(self, metrics: Mapping[str, Any], *, step: int | None = None):
        self._run.log(metrics, step=step)
# ---
def write_batch(self, batch: BatchResult):
        if isinstance(batch, pa.RecordBatch):
            batch = dict_from_record_batch(batch)

        cbatch = _canonicalize_batch(batch)  # type: ignore[arg-type]
        self._tree_store.extend(cbatch)
# ---
def test_get_lines_with_short_string():
        assert len(get_lines("a"*(LINEWIDTH-1))) == 1
# ---
def init(key):
        return SimpleModel(hax.random.normal(key, (Embed,)))
# ---
def setUp(self):
        super(IntegrationTestAnalyzers, self).setUp()

        self.api = SuperSearchWithFields(config=self.config)
        self.now = datetimeutil.utc_now()
# ---
def test_asdict_excluding_simple():
    """Test asdict_excluding with a simple dataclass."""
    config = NestedConfig(name="test", value=42)
    result = asdict_excluding(config, exclude={"runtime_env"})
    assert result == {"name": "test", "value": 42}
    assert "runtime_env" not in result
# ---
def _calculate_bytes_per_token_type(self, tokenizer: HfTokenizer) -> Optional[hax.NamedArray]:
        if tokenizer is None:
            return None
        else:
            # calculate the number of bytes in each token
            Vocab = hax.Axis("vocab", len(tokenizer.get_vocab()))
            bytes = np.ndarray((Vocab.size,), dtype=np.int32)

            for i in range(Vocab.size):
                bytes[i] = byte_length_of_token(tokenizer, i)

            return hax.named(jnp.array(bytes), Vocab)
# ---
def show_recordsettings(self):
        dlg = CSVSettingsDialog(self)
        dlg.exec_()
# ---
def __call__(self, view_func):
        def decorator(request, *args, **kwargs):
            if not request.user.has_perm(self.perm):
                raise PermissionRequired(self.perm)
            return view_func(request, *args, **kwargs)
        return checks_permissions(self.error_view)(decorator)
# ---
def __rpow__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.power(other, self)
# ---
def copy_clfftdll_to_package():
    import shutil
    shutil.copy(
        os.path.join(CLFFT_DIR, 'bin', 'clFFT.dll'),
        'gpyfft')

    shutil.copy(
        os.path.join(CLFFT_DIR, 'bin', 'StatTimer.dll'),
        'gpyfft')
    print("copied clFFT.dll, StatTimer.dll")
# ---
def _resolve_bindings(array, bindings: Mapping[str, Axis | str | int]) -> AliasTable:
    b: dict[str, AxisSelector] = {}
    for name, selector in bindings.items():
        if isinstance(selector, str):
            try:
                selector = array.resolve_axis(selector)
            except ValueError:
                pass
        elif isinstance(selector, int):
            selector = Axis(name, selector)
        assert not isinstance(selector, int)
        b[name] = selector
    return AliasTable(b)
# ---
def _global_router_name(self, hosting_device_id, logical=False):
        if logical is True:
            return cisco_constants.LOGICAL_ROUTER_ROLE_NAME
        else:
            return '%s-%s' % (cisco_constants.ROUTER_ROLE_NAME_PREFIX,
                              hosting_device_id[-cisco_constants.ROLE_ID_LEN:])
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_index: bool = False,
    return_inverse: bool = False,
    return_counts: bool = False,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> NamedArray | tuple[NamedArray, ...]: ...
# ---
def LatentSize(self) -> Axis:
        return Axis("latent", self.kv_lora_rank)
# ---
def standardize_config(config: ServerConfig) -> ServerConfig:
    """Replace relative paths with absolute paths, except for cloud storage paths."""
    absolute_root_paths = [resolve_path(path) for path in config.root_paths]
    return replace(config, root_paths=absolute_root_paths)
# ---
def order_clause(self):
        """Generates a SQL fragment to be used in a ORDER BY clause, or
        None if no fragment is used (i.e., this is a slow sort).
        """
        return None
# ---
def run_with_env(_env=replica_env):
                    with temporary_env_vars(_env):
                        return callable_ep.callable(*callable_ep.args, **callable_ep.kwargs)
# ---
def to_hf_batched(x):
        if len(x) == 0:
            return list(x)
        elif isinstance(x[0], Sequence) or isinstance(x[0], np.ndarray):
            if all(len(y) == len(x[0]) for y in x):
                return np.stack(x)
            else:
                return list(x)
        else:
            return x
# ---
def test_scale_up_creates_and_tracks_vm_group(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """scale_up() creates a VM group via VmManager and tracks it."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        new_vm_group = group.scale_up()

        manager.create_vm_group.assert_called_once()
        assert group.slice_count() == 1
        assert new_vm_group in group.vm_groups()
# ---
def test_max_variants_limit():
    variants = generate_expression_variants("a + b", max_variants=2)
    assert len(variants) <= 2
# ---
def to_hf_config(self) -> tuple[float, dict]:
        return self.theta, {
            "rope_type": "yarn",
            "factor": self.factor,
            "beta_fast": self.beta_fast,
            "beta_slow": self.beta_slow,
            "original_max_position_embeddings": self.original_max_position_embeddings,
            "mscale": self.mscale,
        }
# ---
def random_ascii_string(length):
    random = SystemRandom()
    return ''.join([random.choice(UNICODE_ASCII_CHARACTERS) for x in range(length)])
# ---
def testImportFutureLateRaises(self):
    regexp = 'from __future__ imports must occur at the beginning of the file'
    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,
                            'foo = bar\nfrom __future__ import print_function')
# ---
def get_ts_diff(utc_str1: str, utc_str2: str) -> float:
    # Parse the UTC string to a datetime object
    utc_dt1 = datetime.strptime(utc_str1, "%Y-%m-%dT%H:%M:%S")
    utc_dt2 = datetime.strptime(utc_str2, "%Y-%m-%dT%H:%M:%S")
    diff = utc_dt2 - utc_dt1
    diff_seconds = diff.total_seconds()
    diff_hrs = diff_seconds / 3600.0
    return diff_hrs
# ---
def clear(self):
        self.exc = None
        self.tb = None
# ---
def test_equal_4(self):
        self.assertEqual(string_color('Hayden Smith'), '7E00EE')
# ---
def test_new_websocket_client_novnc_token_invalid(self, check_token):
        check_token.return_value = False

        self.wh.path = "http://127.0.0.1/"
        self.wh.headers.getheader.return_value = "token=XXX"

        self.assertRaises(exception.InvalidToken,
                          self.wh.new_websocket_client)
        check_token.assert_called_with(mock.ANY, token="XXX")
# ---
def setReplicaStatus( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndict = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.setReplicaStatus( lfndict )
# ---
def test_binary_bytes(self):
        """Store and retrieve bytes as a binary"""
        self.make_table()
        data = {"a": 1, "b": 2}
        self.dynamo.put_item("foobar", {"id": "a", "data": Binary(dumps(data))})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(loads(item["data"].value), data)
# ---
def get(self):
		return ord(self.read(1))
# ---
def mk_LayerNorm(self, axis: AxisSpec) -> hnn.RmsNorm:
        return self.norm_config.build(axis)
# ---
def split_image_and_tag(docker_base_image):
    if ":" in docker_base_image:
        base_image, base_tag = docker_base_image.rsplit(":", 1)
    else:
        base_image = docker_base_image
        base_tag = "latest"
    return base_image, base_tag
# ---
def test_scan_reports_eqx_module_field_path():
    Height = Axis("Height", 2)

    class Foo(eqx.Module):
        my_array: jnp.ndarray

    foo = Foo(jnp.zeros((Height.size - 1, 3)))

    def f(c, foo):
        return c, foo.my_array

    with pytest.raises(ValueError) as e:
        hax.scan(f, Height)(0, foo)

    assert "foo.my_array" in str(e.value)
# ---
def method1(self, a, b, c=10):
        return a + b + c
# ---
def name(cls):
        return "blocks_api:block_depth"
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> Optional[float]:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.hidden_dim * self.mlp_scale,
            num_layers=self.num_layers,
            num_kv_heads=self.num_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def current(self):
        return self.voltage / self.resistance
# ---
def __iter__(self):
        return iter(self.subqueries)
# ---
def bitwise_count(a: A) -> A:
    return wrap_elemwise_unary(jnp.bitwise_count, a)
# ---
def get_actor_pool_name(self) -> str:
        assert self._slice_info
        return f"slice {self._slice_info.slice_name}"
# ---
def __post_init__(self):
        assert not (self.fp8 and self.int8), "Cannot use FP8 and INT8 quantization at the same time."
# ---
def default_error_handler(ec, *args):
        return ValueError(_mpv_error_string(ec).decode('utf-8'), ec, *args)
# ---
def loss_ref(x_raw, w_raw, y_raw):
        loss_val, lse_val = linear_softmax_cross_entropy_loss_reference(
            x_raw,
            y_raw,
            w_raw,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
        )
        loss_val = loss_val + logsumexp_weight * (lse_val**2)
        return loss_val.mean()
# ---
def convert_issue_to_xml(issue):
    return f"""<issue>
        <number>{issue.number}</number>
        <title>{issue.title}</title>
        <state>{issue.state}</state>
        <created_at>{issue.created_at}</created_at>
        <updated_at>{issue.updated_at}</updated_at>
        <labels>{','.join([label.name for label in issue.labels])}</labels>
        <url>{issue.html_url}</url>
        <body>\n{issue.body}</body>
        <comments>{issue.comments}</comments>
    </issue>\n"""
# ---
def send_config_change_event(msg, error=EventError.ErrorTypes.NO_ERROR, pubsub=INJECTED):
        # type: (str, Dict[str, Any], PubSub) -> None
        event = EsafeEvent(EsafeEvent.Types.CONFIG_CHANGE, {'type': 'apartment', 'msg': msg}, error=error)
        pubsub.publish_esafe_event(PubSub.EsafeTopics.CONFIG, event)
# ---
def i_get_the_project(step, resource):
    resource = world.api.get_project(resource)
    world.status = resource['code']
    assert world.status == HTTP_OK
    world.project = resource['object']
# ---
def decodes():
    pass
# ---
def test_simple_fail_second_start(self, tmpfile):
        fd = tmpfile.fileno()
        cap = capture.FDCapture(fd)
        cap.done()
        pytest.raises(ValueError, cap.start)
# ---
def schedule_router_precommit(self, context, router_context):
        pass
# ---
def flush(self):
        """
        Flushes the remaining messages and progress bars state by forcing redraw. Can be useful if you want to be sure
        that a message or progress has been updated in display at a given moment in code, like when you are exiting an
        application or doing some kind of synchronized operations.
        """
        self.queue.put(dill.dumps(FlushCommand()))
# ---
def get_task_status(
        self,
        request: cluster_pb2.Worker.GetTaskStatusRequest,
    ) -> cluster_pb2.TaskStatus: ...
# ---
def _get_key_finger(self, path):
        '''
        Return a sha256 kingerprint for the key
        '''
        with salt.utils.fopen(path, 'r') as fp_:
            keydata = self.serial.loads(fp_.read())
            key = 'pub: {0}\nverify: {1}'.format(
                    keydata['pub'],
                    keydata['verify'])
        return hashlib.sha256(key).hexdigest()
# ---
def generate_data(self, data_dir, tmp_dir, task_id=-1):
    generator_utils.generate_dataset_and_shuffle(
        self.generator(data_dir, tmp_dir, True),
        self.training_filepaths(data_dir, self.train_shards, shuffled=False),
        self.generator(data_dir, tmp_dir, False),
        self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))
# ---
def test_voter_get_requests_redirected_to_index(self):
        self.client.logout()
        self.client.login(username='user0', password='voter')

        response = self.client.get(reverse('results-export'), follow=True)
        self.assertRedirects(response, reverse('index'))
# ---
def load_checkpoint(
        self,
        state: M,
        path: Optional[PathLike] = None,
        *,
        discover_latest: bool = True,
        axis_mapping: Optional[haliax.partitioning.ResourceMapping] = None,
        mesh: Optional[haliax.partitioning.Mesh] = None,
    ) -> Optional[M]:
        if path is None:
            path = self.base_path
        return load_checkpoint(state, path, discover_latest=discover_latest, axis_mapping=axis_mapping, mesh=mesh)
# ---
def quit():
        app_lock.signal()
# ---
def name(self):
        return EnkfNode.cNamespace().get_name(self)
# ---
def get_task(self, task_id: str) -> TaskInfo | None:
        """Get a task by ID.

        Returns TaskInfo view (implemented by TaskAttempt) to decouple callers
        from execution internals.
        """
        return self._tasks.get(task_id)
# ---
def new(self, **kwargs):
        ctx = Ctx()
        ctx._parent = self
        for name, value in kwargs.iteritems():
            setattr(ctx, name, value)
        return ctx
# ---
def write_input(
    output_dir: Path,
    num_docs: int,
    words_per_doc: int,
    num_input_files: int,
) -> None:
    """Generate and write input files for benchmarking."""
    output_dir.mkdir(parents=True, exist_ok=True)
    setup_input_files(num_docs, words_per_doc, num_input_files, str(output_dir))
    print(f"\nInput files written to: {output_dir}")
# ---
def saveActiveProfile(newProfile = True):
            if newProfile:
                activeProfileIter = self.profilesComboModel.append(profile)
                self.profilesCombo.set_active_iter(activeProfileIter)

            self.prefsDict['profile'] = profile
            self.prefsDict['activeProfile'] = profile
            self.saveBasicSettings()
            self.writeUserPreferences()
# ---
def vmap_fun(x):
        return x.sum(Width)
# ---
def timetest():
        return str(time.time())
# ---
def test_activation(self):
    # with string argument
    testing_utils.layer_test(
        keras.layers.Activation,
        kwargs={'activation': 'relu'},
        input_shape=(3, 2))

    # with function argument
    testing_utils.layer_test(
        keras.layers.Activation,
        kwargs={'activation': keras.backend.relu},
        input_shape=(3, 2))
# ---
def do_main():
  fix_args()

  updateCB.newExec()
  target = PARAMS["TARGET"]
  if not(PARAMS["PUT_RANDOM"]):
    photowall(target)
  else:
    random_wall(target)
# ---
def _handle_image(self, attrs):
        'Returns the alt text of an image tag.'
        try:
            return attrs['alt']
        except KeyError:
            return ''
# ---
def test_conf_extra_no_section(self):
        with mock.patch.object(memcache, 'ConfigParser',
                               get_config_parser(section='foobar')):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '127.0.0.1:11211')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, False)
        self.assertEqual(
            app.memcache._client_cache['127.0.0.1:11211'].max_size, 2)
# ---
def retrieve_detail(item):
    '''Retrive detaill for news item
    return is tuple (id, title, url, text)
    '''
    url = item[2]
    html = urlopen(url)
    soup = BeautifulSoup(html, 'html.parser')
    detail = soup.find(class_='divNewsDetailsText')
    detail = detail.get_text()
    _list  = list(item)
    _list.insert(3, detail)
    item = tuple(_list)
    return item
# ---
def test_decode_token_specials(tok):
    assert tok.decode_token(tok.pad_token_id) == ""
    assert tok.decode_token(tok.sos_token_id) == "<SOS>"
    assert tok.decode_token(tok.eos_token_id) == "<EOS>"
# ---
def open_shard_at_row(self, shard_name: str, row: int):
        return self.docs[row:]
# ---
def join_path(output_path: str, name: str | None) -> str:
        return os.path.join(output_path, name) if name else output_path
# ---
def bitwise_invert(a: A) -> A:
    return wrap_elemwise_unary(jnp.bitwise_invert, a)
# ---
def flip(self):
        """Swap the OpenGL front and back buffers.

        Call this method on a double-buffered window to update the
        visible display with the back buffer.  The contents of the back buffer
        is undefined after this operation.

        Windows are double-buffered by default.  This method is called
        automatically by `EventLoop` after the `on_draw` event.
        """
        raise NotImplementedError('abstract')
# ---
def init_ui(self, main_view):
        self.main_view = main_view
        self.init_hotkeys()
# ---
def test_limit_offset_nobinds(self):
        """test that 'literal binds' mode works - no bound params."""

        table = self.tables.some_table
        stmt = select([table]).order_by(table.c.id).limit(2).offset(1)
        sql = stmt.compile(
            dialect=config.db.dialect, compile_kwargs={"literal_binds": True}
        )
        sql = str(sql)

        self._assert_result(sql, [(2, 2, 3), (3, 3, 4)])
# ---
def remove_widget(self, widget):
        """Remove a widget (currently only L{TextBox}s are accepted) from
            the list of widgets to do auto-correction for.
            """
        if isinstance(widget, TextBox) and widget in self.widgets:
            self._remove_textbox(widget)
# ---
def status(self) -> cluster_pb2.JobStatus:
        """Get current job status.

        Returns:
            JobStatus proto with current state, task counts, and error info
        """
        return self._client._cluster_client.get_job_status(self._job_id)
# ---
def capsule_type_chooser(method, notebook, data):
    return {'username': SAAGIE_USERNAME}
# ---
def test_filter_passing_empty_input():
    passing = filter_passing([], ["assert True"])
    assert passing == []
# ---
def add_service(
    name: str,
    user_profile: UserProfile,
    base_url: Optional[str] = None,
    interface: Optional[int] = None,
    token: Optional[str] = None,
) -> None:
    Service.objects.create(
        name=name, user_profile=user_profile, base_url=base_url, interface=interface, token=token
    )
# ---
def f(idx):
        return named1["H", idx]
# ---
def removef(filename):
    try:
        os.remove(filename)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise
# ---
def test_health_endpoint_empty_cluster(client):
    """Health endpoint returns ok for empty cluster (no workers, no jobs)."""
    resp = client.get("/health")

    assert resp.status_code == 200
    data = resp.json()
    assert data["status"] == "ok"
    assert data["workers"] == 0
    assert data["jobs"] == 0
# ---
def execute(self):
        raise NotImplementedError("Subclass must implement this")
# ---
def arccos(a: A) -> A:
    return wrap_elemwise_unary(jnp.arccos, a)
# ---
def env_for_accel(self, accel_type: str) -> dict[str, str]:

        base_env = self.env.copy()

        if "-" in accel_type:
            base_env.update(self.accel_env.get(accel_type.split("-")[0], {}))

        if accel_type in self.accel_env:
            base_env.update(self.accel_env[accel_type])

        return base_env
# ---
def filename(self):
        pass
# ---
def test_count_with_filter(backend):
    """Test count with filter operation."""
    ds = Dataset.from_list(range(100)).filter(lambda x: x % 2 == 0).count()
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == 50
# ---
def _has_validation_set(self):
        if len(self.validation_urls) > 0:
            return True

        if self.id is not None:
            dataset = datasets.load_dataset(
                self.id, name=self.name, streaming=self.stream, split=self.validation_split
            )
            try:
                next(iter(dataset))
                return True
            except StopIteration:
                return False

        return False
# ---
def test_impl(df):
            B = df.A.str.replace('AB*', 'EE', regex=True)
            return B
# ---
def clear_queue(self) -> None:
        """Clear all batches from the queue (for testing/debugging)."""
        pattern = f"{self.path}/*"
        files = self.fs.glob(pattern)

        for file_path in files:
            self.fs.delete(file_path)

        self._batch_counter = 0

        logger.info(f"Cleared queue at {self.path}")
# ---
def list_workers(ctx):
    """List Ray workers."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        result = subprocess.check_output(
            ["ray", "list", "workers", "--format=json", f"--limit={1000}"],
            text=True,
            timeout=60,
        )
        print(json.dumps(json.loads(result), indent=2))
# ---
def print_key(self, match):
        '''
        Print out a single key

        :param str match: A string to match against. i.e. 'web*'
        '''
        matches = self.key.key_str(match)
        salt.output.display_output(
                matches,
                'key',
                self.opts)
# ---
def t_newline(self, t):
        r'\n+'
        t.lexer.lineno += len(t.value)
# ---
def total_entries(self) -> int:
        return sum(len(v) for v in self.entries.values())
# ---
def test_blake2_compliance():
    inputs = [
        b"",
        b"hello",
        b"The quick brown fox jumps over the lazy dog",
        b"\x00\xff" * 100,
    ]

    for data in inputs:
        rust_result_list = hash_blake2(data)
        rust_result_bytes = bytes(rust_result_list)
        assert len(rust_result_bytes) == 64

        # Parity with python
        py_result_bytes = hashlib.blake2b(data).digest()
        assert rust_result_bytes == py_result_bytes
# ---
def test_find_path_respects_max_steps():
    source = "a = 1\nb = 2\nc = 3\n"
    target = "a = 10\nb = 20\nc = 30\n"
    path = find_path(source, target, max_steps=1)
    assert len(path) <= 1
# ---
def tff_model_fn() -> tff.learning.Model:
    return tff.learning.from_keras_model(
        keras_model=model_builder(),
        input_spec=input_spec,
        loss=loss_builder(),
        metrics=metrics_builder())
# ---
def concat_axes(a1: AxisSelection, a2: AxisSelection) -> AxisSelection:
    pass
# ---
def bmarks():
    return_data = do_edit()
    return return_data
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        pass
# ---
def num_seqs(self) -> int:
        return sum(req.n_generations for req in self)
# ---
def test_terminate_job_not_found(service):
    """Verify terminate_job raises ConnectError for unknown job."""
    request = cluster_pb2.Controller.TerminateJobRequest(job_id=JobName.root("nonexistent").to_wire())

    with pytest.raises(ConnectError) as exc_info:
        service.terminate_job(request, None)

    assert exc_info.value.code == Code.NOT_FOUND
    assert "nonexistent" in exc_info.value.message
# ---
def append_log(path: str, obj: dataclass):
    with open(path, "a") as f:
        print(json.dumps(asdict(obj) if obj else None), file=f)
# ---
def device_username(self):
        if self._values['managed']:
            return None
        return self._values['device_username']
# ---
def test_format_shard_path_basic():
    """Test basic path formatting with placeholders."""
    pattern = "output/data-{shard:05d}-of-{total:05d}.jsonl"
    result = format_shard_path(pattern, 0, 10)
    assert result == "output/data-00000-of-00010.jsonl"
# ---
def vocab_size(self):
    """Number of pixel values."""
    return 256
# ---
def test_greater_than(self):
        """Test that cmp_version compares a as greater than b"""
        self.assertTrue(vmops.cmp_version('1.2.3.5', '1.2.3.4') > 0)
# ---
def __init__(self):
        self.verbose: bool = False
        self.config_file: str | None = None
        self.config_obj: RayClusterConfig | None = None
        self.tpu_name: str | None = None
        self.config_data: dict | None = None
# ---
def _save(self):
		for name in self._values:
			index = self.structure.index(name)
			col = self.structure[index]
			self[index] = col.from_python(self._values[name])
# ---
def test_get_insert_cmd(self):
        print(get_insert_cmd(go('Integer'), base_col_def))
        print(get_insert_cmd(go('String'), base_col_def))
        print(get_insert_cmd(go('Point'), base_col_def))
        print(get_insert_cmd(go('Role'), base_col_def))
        print(get_insert_cmd(go('RoleIO'), base_col_def))
        print(get_insert_cmd(go('Log'), base_col_def))
        print(get_insert_cmd(go('Meta'), base_col_def))
# ---
def false_heading(self, elem):
        '''Handle a "false heading", i.e., text that appears in heading
        tags in the source even though it is not a chapter heading.'''
        elem.attrs['class'] = 'getebook-false-h'
        elem.tag = 'p'
        self.handle_elem(elem)
# ---
def loss_fast_batched(v):
        return jnp.sum(template_op(v))
# ---
def test_killed_maps_to_stopped(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_KILLED) == JobStatus.STOPPED
# ---
def _go(conn):
            assert_raises_message(
                tsa.exc.StatementError,
                r"nope \(original cause: Exception: nope\) u?'SELECT 1 ",
                conn.execute,
                    select([1]).\
                        where(
                            column('foo') == literal('bar', MyType())
                        )
            )
# ---
def __init__(self, handles: list[ActorHandle], jobs: list[LocalJobHandle]):
        self._handles = handles
        self._jobs = jobs
        self._yielded = False
# ---
def linspace(
    axis: AxisSelector, *, start: float, stop: float, endpoint: bool = True, dtype: DTypeLike | None = None
) -> NamedArray:
    """
    Version of jnp.linspace that returns a NamedArray.
    If `axis` is a string, the default number of samples (50, per numpy) will be used.
    """
    if isinstance(axis, str):
        axis = Axis(axis, 50)
    return NamedArray(jnp.linspace(start, stop, axis.size, endpoint=endpoint, dtype=dtype), (axis,))
# ---
def _merge_bloom(bloom_files: Iterator[str]):
        merged_bloom = Bloom(config.estimated_doc_count, config.false_positive_rate)
        for bloom_file_path in bloom_files:
            fs, path = fsspec.url_to_fs(bloom_file_path)
            with fs.open(path, "rb") as f:
                bloom_bytes = f.read()
            shard_bloom = Bloom.load_bytes(bloom_bytes)
            merged_bloom.update(shard_bloom)
        yield merged_bloom.save_bytes()
# ---
def unload(self):
        """Unload the inference model to free up resources."""
        self.inference_context.unload()
# ---
def GetBufferFiletypes( bufnr ):
  command = 'getbufvar({0}, "&ft")'.format( bufnr )
  return VimExpressionToPythonType( command ).split( '.' )
# ---
def __init__(self, async_iter):
        self.async_iter = async_iter
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self._exhausted = False
# ---
def get_timeline_data(doctype, name):
	'''returns timeline data based on stock ledger entry'''
	out = {}
	items = dict(frappe.db.sql('''select posting_date, count(*)
		from `tabStock Ledger Entry` where item_code=%s
			and posting_date > date_sub(curdate(), interval 1 year)
			group by posting_date''', name))

	for date, count in iteritems(items):
		timestamp = get_timestamp(date)
		out.update({timestamp: count})

	return out
# ---
def _materialize_mask_slice(mask, i, j, QPos, KPos, block_size):
    return materialize_mask(
        mask,
        QPos,
        KPos,
        q_slice=hax.ds.block(i, block_size),
        k_slice=hax.ds.block(j, block_size),
    )
# ---
def to_option_list(self, ascommalist=''):
        '''return all options as a string
           if ascommalist is set to the name of a key, and
           the value of that key is a dict, format the dict
           as a list of comma delimited key=value pairs'''
        return self.stringify(ascommalist)
# ---
def compute(input):
            model_output = model(input, attn_mask=attn_mask)
            return hax.nn.softmax(model_output, axis=model.Vocab)
# ---
def build(self, ctx: LrScheduleContext):
        return optax.constant_schedule(ctx.learning_rate)
# ---
def fake_remove_from_aggregate(context, aggregate, host):
            fake_remove_from_aggregate.called = True
# ---
def trace_hide_args_func(a, i=10):
    return (a, i)
# ---
def test_relu_squared_scalar(use_jit):
    f = hax.nn.relu_squared
    if use_jit:
        f = jax.jit(f)

    x = 5.0
    expected = 25.0
    actual = f(x)
    assert jnp.allclose(actual, expected)

    x_neg = -5.0
    expected_neg = 0.0
    actual_neg = f(x_neg)
    assert jnp.allclose(actual_neg, expected_neg)
# ---
def scanned_f(*args, **kwargs):
        return scan_preconfig(None, *args, **kwargs)[1]
# ---
def get_autoscaler_status(self) -> cluster_pb2.Controller.GetAutoscalerStatusResponse:
        return self._remote_client.get_autoscaler_status()
# ---
def stock_ledger_created(self):
		if not hasattr(self, '_stock_ledger_created'):
			self._stock_ledger_created = len(frappe.db.sql("""select name from `tabStock Ledger Entry`
				where item_code = %s limit 1""", self.name))
		return self._stock_ledger_created
# ---
def get_gpu_count(device: cluster_pb2.DeviceConfig) -> int:
    """Extract GPU count from config."""
    if device.HasField("gpu"):
        return device.gpu.count or 1
    return 0
# ---
def test_create(self):
        resp = FakeResponse()
        self.type_action_controller.create(self.req, {}, resp)
        self.assertEqual({'id': fake.VOLUME_TYPE_ID,
                          'os-volume-type-access:is_public': True},
                         resp.obj['volume_type'])
# ---
def _make(cpu: int = 10, memory_bytes: int = 10 * 1024**3) -> cluster_pb2.ResourceSpecProto:
        return cluster_pb2.ResourceSpecProto(cpu=cpu, memory_bytes=memory_bytes, disk_bytes=10 * 1024**3)
# ---
def __init__(self, matchers):
        m1 = matchers[0]
        super(unionmatcher, self).__init__(m1._root, m1._cwd)
        self.traversedir = m1.traversedir
        self._matchers = matchers
# ---
def config_with_special_chars() -> config_pb2.BootstrapConfig:
    """Config with values containing braces and special characters."""
    return config_pb2.BootstrapConfig(
        docker_image="gcr.io/test/iris:v1.0-{tag}",
        cache_dir="/cache/{project}/iris",
        worker_port=10001,
        env_vars={
            "MESSAGE": "Hello {world}!",
            "JSON": '{"key": "value"}',
        },
    )
# ---
def _reset_worker_state(self) -> None:
        """Reset worker state: wipe all containers and clear tracking."""
        logger.info("Resetting worker state")

        # Clear task tracking
        with self._lock:
            self._tasks.clear()

        # Wipe ALL iris containers (simple, no tracking needed)
        self._cleanup_all_iris_containers()

        logger.info("Worker state reset complete")
# ---
def _split(self, path):
        if "/" in path:
            subdir, rest = path.split("/", 1)
        else:
            subdir, rest = path, ""
        if not subdir:
            raise error.ProgrammingError("path cannot be absolute")
        return subdir, rest
# ---
def animation(name):
    url = bilibili(name)
    try:
        result = 'bilibiliæœ€åŽæ›´æ–°:ç¬¬'+url[-1][0]+'é›†'+url[-1][1]
        return result
    except IndexError:
        return 'ä»€ä¹ˆéƒ½æ‰¾ä¸åˆ°ï¼'
# ---
def test_corrupted_character_stack():
    assert corrupted_character('[({(<(())[]>[[{[]{<()<>>')[1] == ['}', '}', ']', ']', ')', '}', ')', ']']
# ---
def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('AB*', regex=True)
            return B.sum()
# ---

def digitSum(s):
    """Task
    Write a function that takes a string as input and returns the sum of the upper characters only'
    ASCII codes.

    Examples:
        digitSum("") => 0
        digitSum("abAB") => 131
        digitSum("abcCd") => 67
        digitSum("helloE") => 69
        digitSum("woArBld") => 131
        digitSum("aAaaaXa") => 153
    """
    if s == "": return 0
    return sum(ord(char) if char.isupper() else 0 for char in s)
# ---
def l1(*arg, **kw):
            pass
# ---
def define(self, dimensions):
                return {d: ('middle', 2, 2) for d in dimensions}
# ---
def check_for_unexpected_keys(name, input_dict, expected_values):
  unknown = set(input_dict.keys()).difference(expected_values)
  if unknown:
    raise ValueError('Unknown entries in {} dictionary: {}. Only expected '
                     'following keys: {}'.format(name, list(unknown),
                                                 expected_values))
# ---
def mlp(block: TreeDiffusionBlockParams, x: Float[Array, "B S D"]) -> Float[Array, "B S D"]:
    """SwiGLU MLP."""
    gate = jnp.einsum("bsh,hm->bsm", x, block.mlp_gate)
    up = jnp.einsum("bsh,hm->bsm", x, block.mlp_up)
    activated = jax.nn.silu(gate) * up
    return jnp.einsum("bsm,mh->bsh", activated, block.mlp_down)
# ---
def make_state(key):
            model = MLP(in_size=2, out_size=1, width_size=2, depth=3, key=key)
            opt_state = optim.init(arrays_only(model))

            return model, opt_state, key
# ---
def bar(x):
            return x
# ---
def test_log(monkeypatch):
    monkeypatch.setenv("HF_HUB_OFFLINE", "1")
    run = trackio.init(project="test-log")
    tracker = TrackioTracker(run)
    tracker.log({"float": 2.0}, step=0)
    tracker.log({"str": "test"}, step=0)
    tracker.log({"scalar_jax_array": jnp.array(3.0)}, step=0)
    tracker.log({"scalar_np_array": np.array(3.0)}, step=0)
    tracker.log({"histogram": Histogram.from_array(jnp.array([1.0, 2.0, 3.0]))}, step=0)
    trackio.finish()
# ---
def top_level_startup_industry(self):
        industry = (
            self.startup.primary_industry if self._get_startup() else None)
        return industry.parent if industry and industry.parent else industry
# ---
def commalist(propval=''):
    return str(propval).split(',')
# ---
def get_preset(name: str) -> ModelPreset:
    """Get a preset by name.

    Args:
        name: Preset name.

    Returns:
        ModelPreset instance.

    Raises:
        ValueError: If preset name is unknown.
    """
    if name not in PRESETS:
        raise ValueError(f"Unknown preset: {name}. Available: {list(PRESETS.keys())}")
    return PRESETS[name]()
# ---
def to_mutation(self, source: str) -> Mutation:
        """Convert to a Mutation that can be applied to the source."""
        return Mutation(
            start=self.source_start,
            end=self.source_end,
            replacement=self.target_fragment,
            node_type=self.node_type,
            original=source[self.source_start : self.source_end],
        )
# ---
def bump_seq_len_to_next_page(self, seq_id: int) -> "DecodeState":
        sequences = self.sequences.bump_seq_len_to_next_page(seq_id)
        return dataclasses.replace(self, sequences=sequences)
# ---
def axis_spec_to_tuple(axis_spec: ShapeDict) -> tuple[Axis, ...]: ...
# ---
def with_output(self, x, y, z, *, static1, static2):
            assert static1 is True
            assert static2 is False
            out = x + self.w + y + z
            return out, 2 * self.w + y
# ---
def config(self) -> HackableTransformerConfig:
        return self.transformer.config
# ---
def test_one_network_label(self):
        CONF.network_label_regex = 'public'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(['15.123.123.123'], ip)
# ---
def convert_eval_to_dolma(cfg: ConvertEvalToDolmaConfig):
    pipeline = (
        Dataset.from_files(f"{cfg.input_path}/**/*.jsonl.gz")
        .flat_map(load_jsonl)
        .map(map_row)
        .write_jsonl(f"{cfg.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )
    Backend.execute(pipeline)
# ---
def multiple(context):
        """A fixture to be invoked multiple times."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "kapow", "boing": "thunk"}

        return setup, teardown
# ---
def launch(
        model: ModelConfig,
        evals: list[EvalTaskConfig],
        output_path: str,
        max_eval_instances: int | None = None,
        wandb_tags: list[str] | None = None,
    ) -> None:
        if configure_logging:
            import logging

            logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", force=True)
        evaluator.evaluate(model, evals, output_path, max_eval_instances, wandb_tags)
# ---
def isexact(self):
        return True
# ---
def __init__(self, bbox, srs):
        self.bbox = bbox
        self.srs = srs
        self.geom = None
# ---
def test_register_plugin(self, dispatcher):
        name = "some_name"

        class AClass:
            @event.event(name)
            def handler(self):
                pass

            @event.event(name)
            async def hander(self):
                pass

        obj = AClass()
        h_insts = dispatcher.register_plugin(obj)
        assert len(dispatcher.event_map) == 1
        assert len(h_insts) == 2
        for h_inst in h_insts:
            assert h_inst in dispatcher.event_map[name]
# ---
def is_null(self) -> IsNullExpr:
        return IsNullExpr(self)
# ---
def unbind(self, existing, imported):
        [match] = [m for m in self.matches if m[0] is existing and m[1] is imported]
        match[1] = None
        self.matches.append([None, imported])
        self._sort_matches()
# ---
def setUp(self):
        super(NetappDirectCmodeNfsDriverTestCase, self).setUp()
        self._custom_setup()
# ---
def test_convert_page_with_resiliparse(sample_html_simple):
    config = ResiliparseConfig()
    result = convert_page(sample_html_simple, url=None, extract_method="resiliparse", config=config)

    assert "content" in result
    assert "title" in result
    assert "html" in result
    assert result["content"] is not None
    assert len(result["content"]) > 0
# ---
def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()
# ---
def predict(self, documents: list[str]):
        return self.model.predict(documents, k=self.k)
# ---
def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        return parse_response_for_stop_token(response, self.tokenizer, self._end_message_token)
# ---
def test_multislice_one_slice_fails():
    """3. Run a function where one slice fails, verify retries and eventual failure."""
    num_slices = 2
    tpu_type = "v5litepod-4"

    with pytest.raises(RayTaskError) as excinfo:
        run_on_pod(
            fail_on_slice_0_fn,
            tpu_type,
            num_slices=num_slices,
            max_retries_failure=2,  # low retry
            max_retries_preemption=1,
        )

    assert "DeliberatelyRaisedException" in str(excinfo.value)
# ---
def count_statements(node: ast.AST) -> int:
    """Count the number of statement nodes in an AST subtree.

    This is the Python analogue of the paper's 'sigma' (primitive count).
    Used to control the size of extracted and replaced subtrees.
    """
    count = 0
    for child in ast.walk(node):
        if isinstance(child, ast.stmt):
            count += 1
    return count
# ---
def total_attempts(self) -> int:
        """Total number of retries (failure + preemption retries)."""
        return self.failure_count + self.preemption_count
# ---
def feature_encoders(self, data_dir):
    if self.is_character_level:
      encoder = text_encoder.ByteTextEncoder()
    else:
      vocab_filename = os.path.join(
          data_dir, self.vocab_problem.vocab_filename)
      encoder = text_encoder.SubwordTextEncoder(vocab_filename)
    input_encoder = text_encoder.ImageEncoder(channels=self.num_channels)
    return {"inputs": input_encoder, "targets": encoder}
# ---
def join(self, timeout=5):
        """Wait for thread completion."""
        if self.thread:
            self.thread.join(timeout)
# ---
def poll(self, job_id: JobId) -> JobInfo:
        return self._get_job(job_id).get_info()
# ---
def vmap_via(self, fn: Callable[..., OutputT_co]) -> Callable[..., OutputT_co]: ...
# ---
def predict(self, s):
        return self.model.eval([s])
# ---
def __init__(self, name: str = "root") -> None:
        self._name = name
        self._threads: list[ManagedThread] = []
        self._children: list[ThreadContainer] = []
        self._executors: list[ThreadPoolExecutor] = []
        self._lock = threading.Lock()
# ---
def max_stop_seq_len(self) -> int:
        """Maximum number of stop sequences for each sequence."""
        if self.stop_tokens is None:
            return 0
        return self.stop_tokens.axis_size("position")
# ---
def Vocab(self) -> Axis:
        return self.token_embeddings.Vocab
# ---
def dummyPreprocessInput(image):
    image -= 127.5
    return image
# ---
def test_perturb_operators_valid_python(rng):
    source = "def f(a, b):\n    if a > b:\n        return a + b\n    return a - b"
    for seed in range(20):
        r = random.Random(seed)
        result = perturb_operators(source, r, swap_prob=0.5)
        if result is not None:
            ast.parse(result)
# ---
def evaluate(self, record: dict) -> bool:
        return self.child.evaluate(record) is None
# ---
def __init__(self, func=lambda x: x):
        super(PriorityQueue, self).__init__()

        self.func = func
# ---
def hrule(self):
        """Rendering method for ``<hr>`` tag."""
        if self.options.get('use_xhtml'):
            return '<hr />\n'
        return '<hr>\n'
# ---
def run_command(cmd, timeout=5):
    """Run a command and return output, or None on failure."""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout,
            shell=True
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except (subprocess.TimeoutExpired, OSError, Exception):
        pass
    return None
# ---
def __dir__(self):
		result = self.__dict__.keys()
		result.extend(self.structure.column_names)
		return result
# ---
def test_impl(df):
            B = df.A.str.split(',')
            return B
# ---
def assert_same_version(**kwargs):
            output_path = get_output_path(**(defaults | kwargs))
            assert output_path == default_output_path
# ---
def _roll_and_update(amax_h, update):
        return jnp.roll(amax_h, shift=-1, axis=0).at[0].set(update)
# ---
def _read_all(directory: Path, pattern: str = "*.jsonl.gz") -> list[dict]:
        records = []
        for file_path in sorted(directory.glob(pattern)):
            with gzip.open(file_path, "rt", encoding="utf-8") as handle:
                for line in handle:
                    if line.strip():
                        records.append(json.loads(line))
        return records
# ---
def UnplaceSignInBuffer( buffer_number, sign_id ):
  if buffer_number < 0:
    return
  vim.command(
    'try | exec "sign unplace {0} buffer={1}" | catch /E158/ | endtry'.format(
        sign_id, buffer_number ) )
# ---
def test_hashing(benchmark: Any, sample_batch: pa.RecordBatch, granularity: str, backend: str) -> None:
    """Benchmark the hash generation step."""
    func = PROCESS_FUNCS[(granularity, backend)]
    benchmark.group = f"{granularity.title()}: Hash Generation"
    benchmark(func, sample_batch, "text", "id")
# ---
def scan_fun(acc, z, x):
        return (acc + z * x).scalar(), x * z
# ---
def get_diff(b):
    '''Just the internals of the difference method'''
    return b['b2'].subtract(b['b1']).select(['sur_refl_b02'], ['b1'])
# ---
def bad(self, f, msg):
        self._matcher.bad(self._path + "/" + f, msg)
# ---
def get_logs(self, container_id: str, since: "Timestamp | None" = None) -> list[LogLine]: ...
# ---
def get_indexable(self):
        raise NotImplementedError
# ---
def get_console(self, task):
        """Get the type and connection information about the console."""
        driver_info = _parse_driver_info(task.node)
        url = console_utils.get_shellinabox_console_url(driver_info['port'])
        return {'type': 'shellinabox', 'url': url}
# ---
def create_superuser(self, email, password, **extra_fields):
        return self._create_user(email, password, True, True,
                                 **extra_fields)
# ---
def __contains__(self, key):
        return (key in self._dataset._variables
                and key not in self._dataset._coord_names)
# ---
def slice_any_failed(slice_info: vm_pb2.SliceInfo) -> bool:
    """Compute any_failed from vms[] in proto."""
    return any(vm.state in (vm_pb2.VM_STATE_FAILED, vm_pb2.VM_STATE_PREEMPTED) for vm in slice_info.vms)
# ---
def __init__(self, rule):
        """Initialize the 'not' check.

        :param rule: The rule to negate.  Must be a Check.
        """

        self.rule = rule
# ---
def start(self):
        self.thread.start()
# ---
def format_differences(differences):
    return format_line(prefix='differences'.rjust(RJUST), values=differences)
# ---
def get_status(self) -> vm_pb2.AutoscalerStatus:
        """Build status for the status API."""
        from iris.rpc import time_pb2

        return vm_pb2.AutoscalerStatus(
            groups=[g.to_status() for g in self._groups.values()],
            current_demand={g.name: g.current_demand for g in self._groups.values()},
            last_evaluation=time_pb2.Timestamp(epoch_ms=0),  # Controlled by controller now
            recent_actions=list(self._action_log),
        )
# ---
def main(cfg: TrainFasttextClassifierConfig):
    train(cfg)
# ---
def __init__(self, config: RolloutTrackerConfig, run_id: str):
        self._run = wandb.init(
            entity=config.entity,
            project=config.project,
            name=config.name,
            tags=config.tags,
            id=run_id,
            resume="allow",
            mode=config.mode,
        )
# ---
def test_binary(self):
        """Store and retrieve a binary"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "data": Binary("abc")})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["data"].value, b"abc")
# ---
def rust_mark_document_duplicates(
    batch: pa.RecordBatch, text_col: str, id_col: str, dup_map: dict, attribute_name: str
) -> pa.RecordBatch:
    # The mark function only needs the batch, map, and attr name.
    # text_col and id_col are ignored but kept for signature compatibility with the benchmark.
    return dupekit.mark_document_duplicates(batch, dup_map, attribute_name)
# ---
def record(self, info):
                self.recorded.append(info)
# ---
def is_well_known_file_type(self, wkf, name):
        '''Check if a structure has the expected name.

        Parameters
        ----------
        wkf : dict
            A well-known-file structure with nested type information.
        name : string
            The expected type name

        See Also
        --------
        read_json: where this helper function is used.
        '''
        try:
            return wkf['well_known_file_type']['name'] == name
        except:
            return False
# ---
def update_status(self, context, status):
        """Update local status.

            This method call updates status attribute of
            VPNServices.
        """
        cctxt = self.client.prepare()
        return cctxt.call(context, 'update_status', status=status)
# ---
def sub_seek(self, skip):
        self.command('sub_seek', skip)
# ---
def optimized_func(self, r):
        result = 0
        M = len(self.data)
        pool = Pool(processes=4)

        for m in range(M):
            Nm = self.data[m].shape[0] - 1

            k_args = range(Nm + 1)
            self_args = [self] * len(k_args)
            m_args = [m] * len(k_args)
            r_args = [r] * len(k_args)

            result += sum(pool.map(worker_func,
                                   zip(self_args, m_args, k_args, r_args)))

        return result
# ---
def convert_to_display_name(self, value, record=None):
        return ustr(value and value.display_name)
# ---
def _compute_entropy_on_device(logit_fn, model, batch: B, Vocab) -> jnp.ndarray:
    with jax.named_scope("logits"):
        logits = logit_fn(model, batch)
    entropies = entropy_from_logits(logits, axis=Vocab)
    return entropies.flatten("token").array
# ---
def test_iterating_with_next(self):
        stages = iter(['alpha','beta','gamma'])

        try:
            self.assertEqual('alpha', next(stages))
            next(stages)
            self.assertEqual('gamma', next(stages))
            next(stages)
        except StopIteration as ex:
            err_msg = 'Ran out of iterations'

        self.assertRegex(err_msg, 'Ran out')
# ---
def output_reflink(self, m):
        key = _keyify(m.group(2) or m.group(1))
        if key not in self.links:
            return None
        ret = self.links[key]
        return self._process_link(m, ret['link'], ret['title'])
# ---
def test_not_comparison(self):
        expr = ~(col("score") > 50)
        assert expr.evaluate({"score": 60}) is False
        assert expr.evaluate({"score": 40}) is True
# ---
def leaves(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> Sequence[Any]:
    """Alias for :func:`haliax.tree_util.tree_leaves` matching :func:`jax.tree.leaves`."""

    return tree_util.tree_leaves(tree, is_leaf=is_leaf)
# ---
def read_all_available(self) -> list[RolloutBatch]:
        """Read all currently available batches without blocking.

        Returns:
            List of all available batches (may be empty).
        """
        pass
# ---
def vm_count(self) -> int:
        return get_tpu_topology(self.variant).vm_count
# ---
def string_match(cls, pattern, value):
        return pattern.lower() in value.lower()
# ---
def isnan(a: A) -> A:
    return wrap_elemwise_unary(jnp.isnan, a)
# ---
def captured_inputs(self):
    """Returns the list of implicitly captured inputs."""
    self._create_definition_if_needed()
    return self._extra_inputs
# ---
def set_current_cluster(cluster: Cluster) -> None:
    _cluster_context.set(cluster)
# ---
def accumulate_w_grad():
        res = jax.lax.dot_general(
            x_ref[...],
            xw_scratch_ref[...],
            (((0,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        w_read_future.wait()
        w_grad_tile_ref[...] += res
        w_write_future.start()
# ---
def _submit_simple_job(client: IrisClient, tpu_type: str) -> cluster_pb2.JobStatus:
    def hello():
        print("Hello from validation job!")
        return 42

    job = client.submit(
        entrypoint=Entrypoint.from_callable(hello),
        name="validate-hello",
        resources=ResourceSpec(device=tpu_device(tpu_type)),
        environment=EnvironmentSpec(),
    )
    return job.wait(timeout=DEFAULT_VALIDATION_TIMEOUT, raise_on_failure=False)
# ---
def available_tpus(self) -> int:
        """Available TPU chip count after subtracting committed resources."""
        return get_tpu_chip_count(self.metadata.device) - self.committed_tpu
# ---
def output_exemplar(self) -> dict[str, Sequence[int]]:
        return {"data": np.array([0], dtype=np.int64)}
# ---
def __init__(self, config: InferenceReplConfig):
        self.config = config
        self.server = None
        self.model_name = None

        self.commands: Dict[str, Callable] = {
            "load": self.load,
            "unload": self.unload,
            "chat": self.chat,
            "complete": self.complete,
            "batch": self.batch,
            "help": self.show_help,
            "serve": self.serve,
        }
# ---
def is_show_uncategorized(self, request):
        """Return the result of the "?show_uncategorized" query string param"""

        show_uncategorized = request.GET.get('show_uncategorized', False)
        if show_uncategorized is True or show_uncategorized == 'true':
            return True
        return False
# ---
def less(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.less](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.less.html)
    """
    return jnp.less(x1, x2)
# ---
def tree_flatten(self) -> Any:
        return ((self.array,), self.axis_names)
# ---
def script_message(self, *args):
        self.command('script_message', *args)
# ---
def eliminate_axes(axis_spec: Axis | Sequence[Axis], axes: AxisSelection) -> tuple[Axis, ...]:  # type: ignore
    ...
# ---
def __enter__(self):
        self.server_thread = ServerThread(self.server, self.host, self.port)
        self.server_thread.start()
        return self
# ---
def test_is_cloneable_share_goodformat5(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://netapp.com/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def apartment_id_exists(apartment_id):
        # type: (int) -> bool
        apartments = ApartmentController.load_apartments()
        ids = (x.id for x in apartments)
        return apartment_id in ids
# ---
def test_unrecognized_type(self):
        """Dynamizer throws error on unrecognized type"""
        value = {
            "ASDF": "abc",
        }
        with self.assertRaises(TypeError):
            self.dynamo.dynamizer.decode(value)
# ---
def test_wait_all_mixed_failure(client: LocalClient):
    h_ok = client.submit(JobRequest(name="ok", entrypoint=Entrypoint.from_callable(_noop)))
    h_fail = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    with pytest.raises(JobFailed):
        wait_all([h_ok, h_fail], raise_on_failure=True)
# ---
def fake_resize_part_and_fs(dev, start, old, new):
            marker["partition_called"] = True
# ---
def _join_key(prefix: str, key: str) -> str:
    if prefix:
        return f"{prefix}.{key}"
    return key
# ---
def test_position_token_id_roundtrip(tok):
    for pos in [0, 1, 100, 511]:
        tid = tok.position_token_id(pos)
        assert tok.is_position_token(tid)
        assert tok.position_from_token(tid) == pos
# ---
def body(i, state):
            ref_counts, sequences = state

            def do(state):
                ref_counts, sequences = state
                pages_row = sequences.page_indices["seq", i]
                ref_counts = dec_refcounts_for_seq(pages_row, ref_counts)
                sequences = sequences.release_slot(i)
                return ref_counts, sequences

            return jax.lax.cond(finished_mask[i], do, lambda s: s, (ref_counts, sequences))
# ---
def _log_profiler_artifact(self):
        """Log profiler artifact to the tracker."""
        levanter.tracker.current_tracker().log_artifact(self.profiler_config.profile_path, type="jax_profile")
# ---
def match_like(templ_leaf, tree_leaf):
        if templ_leaf is None:
            return None
        else:
            if tree_leaf is None:
                warnings.warn(f"Template has a non-None value where tree is None. Template value: {templ_leaf}")
            return tree_leaf
# ---
def vm_state_name(state: int) -> str:
    """Return enum name like 'VM_STATE_READY'."""
    try:
        return _VM_STATE.values_by_number[state].name
    except KeyError:
        return f"UNKNOWN({state})"
# ---
def _dirs(self):
        return set(util.dirs(self._fileset))
# ---
def checkpoint_path(self) -> str:
        checkpoint_path = self.config.load_checkpoint_path
        if checkpoint_path is None:
            checkpoint_path = self.config.checkpointer.expanded_path(self.run_id)
        return checkpoint_path
# ---
def __hash__(self):
        return hash(tuple(self.sorts))
# ---
def addStringFromLineEdit(self):
        text = self.inputLine.text()
        if not text:
            return
        try:
            self.model.add(text)
        except AlreadyThereException:
            self.app.show_message("Expression already in the list.")
            return
        except Exception as e:
            self.app.show_message(f"Expression is invalid: {e}")
            return
        self.inputLine.clear()
# ---
def setUp(self):
        self.app = memcache.MemcacheMiddleware(FakeApp(), {})
# ---
def finalist_user_roles(self):
        if not self.user_finalist_roles:
            finalist_roles = BaseUserRole.FINALIST_USER_ROLES
            self.user_finalist_roles = self.programrolegrant_set.filter(
                program_role__user_role__name__in=finalist_roles
            ).values_list('program_role__name', flat=True).distinct()
        return list(self.user_finalist_roles)
# ---
def _substitute_key_type():
        """Replaces key type with input_type."""
        # pylint: disable=unused-variable, invalid-name
        for __, value in input_type['keys'].items():
            value['type'] = input_types[value['type']]
# ---
def test_engine_title_set():
    engine = salt.engines.Engine({}, "foobar.start", {}, {}, {}, {}, name="foobar")
    with patch("salt.utils.process.appendproctitle", MagicMock()) as mm:
        with pytest.raises(KeyError):
            # The method does not exist so a KeyError will be raised.
            engine.run()
        mm.assert_called_with("foobar")
# ---
def test_available_when_no_constraints(self, unbounded_config: config_pb2.ScaleGroupConfig):
        """Group is AVAILABLE when not in backoff, quota ok, and under capacity."""
        from iris.cluster.vm.scaling_group import GroupAvailability

        manager = make_mock_vm_manager()
        group = ScalingGroup(unbounded_config, manager)

        state = group.availability()
        assert state.status == GroupAvailability.AVAILABLE
# ---
def get_image_metadata_item(self, image_id, key):
        """Returns the value for a specific image metadata key."""
        resp, body = self.get("images/%s/metadata/%s" % (str(image_id), key))
        body = json.loads(body)
        self.validate_response(schema.image_meta_item, resp, body)
        return service_client.ResponseBody(resp, body['meta'])
# ---
def test_flatten_scalar_channels(self):
    testing_utils.layer_test(
        keras.layers.Flatten, kwargs={}, input_shape=(3,))

    # Test channels_first
    inputs = np.random.random((10,)).astype('float32')
    outputs = testing_utils.layer_test(
        keras.layers.Flatten,
        kwargs={'data_format': 'channels_first'},
        input_data=inputs)
    target_outputs = np.expand_dims(inputs, -1)
    self.assertAllClose(outputs, target_outputs)
# ---
def perform_create(self, serializer):
        instance = serializer.save()
        instance.open(applicant=self.request.user)
# ---
def run_training_mode(args):
    """Run in training worker mode."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger = logging.getLogger("training_worker")

    logger.info("Starting training worker mode...")
    cleanup()

    worker_config = llama_small_training_worker_config(CHECKPOINT_DIR, str(RUN_ID))
    worker = TrainWorker(
        config=worker_config,
    )

    worker.train()
    logger.info("Training worker completed")
# ---
def get_orthogonal_matrix(GG: List[Union[Array, None]], epsilon: float) -> List[Union[Array, None]]:
    Q: List[Union[Array, None]] = []
    for gg in GG:
        if gg is None:
            Q.append(None)
        else:
            _, eigh = jnp.linalg.eigh(gg + epsilon * jnp.eye(gg.shape[0]))
            Q.append(jnp.flip(eigh, axis=1))
    return Q
# ---
def error(status, message=""):
    if message:
        data = json.dumps(dict(message=message))
    else:
        data=""
    return make_response(data, status)
# ---
def array_value(self, decode_str=False):
        return [ self.values[i].node_value(decode_str) for i in range(self.num) ]
# ---
def _safe_sharding_constraint(x, sharding):
    if sharding is None:
        return x
    else:
        return with_sharding_constraint(x, sharding)
# ---
def test_hmac_secret_is_text(self, mocked_render_template):
        config.init('kinto.ini', 'postgresql')
        args, kwargs = list(mocked_render_template.call_args)
        self.assertEquals(type(kwargs['secret']), six.text_type)
# ---
def __init__(self, func, queue):
            super(Worker, self).__init__()
            self._func = func
            self.q = queue
# ---
def visit_mtable(self, element):
        rows = []
        for child in element.children:
            if child.name == "mtr":
                cells = []
                for cell in child.children:
                    if cell.name == "mtd":
                        cells.append(str(self._visit(cell)))
                rows.append(" & ".join(cells))
        content = " \\\\ ".join(rows)
        return BracedNode(f"\\begin{{matrix}} {content} \\end{{matrix}}")
# ---
def finish(self):
        excs = []
        for tracker in self.loggers:
            try:
                tracker.finish()
            except Exception as e:
                excs.append(e)

        if excs:
            raise RuntimeError("Errors occurred when finishing trackers") from excs[0]
# ---
def _count_params(state_dict: dict) -> int:
    """Count total parameters in a state dict."""
    total = 0
    for value in state_dict.values():
        if hasattr(value, "size"):
            total += value.size
        elif hasattr(value, "shape"):
            total += int(np.prod(value.shape))
    return total
# ---
def bottom_left_corner3d(self):
        return self.edge_points3d[3]
# ---
def transform_to(self, srs):
        if srs == self.srs:
            return self

        bbox = self.srs.transform_bbox_to(srs, self.bbox)
        return BBOXCoverage(bbox, srs)
# ---
def axis_spec_to_tuple(axis_spec: AxisSelection) -> tuple[AxisSelector, ...]: ...
# ---
def get_all_pool_members(self) -> list[ActorPoolMember]:
        return self._actor_pool.copy()
# ---
def __init__(self, root, cwd, files, badfn=None):
        super(exactmatcher, self).__init__(root, cwd, badfn)

        if isinstance(files, list):
            self._files = files
        else:
            self._files = list(files)
# ---
def __call__(self, target, creds, enforcer):
        """Check an individual match.

        Matches look like:

            tenant:%(tenant_id)s
            role:compute:admin
        """

        # TODO(termie): do dict inspection via dot syntax
        match = self.match % target
        if self.kind in creds:
            return match == six.text_type(creds[self.kind])
        return False
# ---
def _get_client(self, url: str) -> ActorServiceClientSync:
        if self._client is None or self._client_url != url:
            self._client = ActorServiceClientSync(
                address=url,
                timeout_ms=int(self._timeout * 1000),
            )
            self._client_url = url
        return self._client
# ---
def parse(self, data):
        # reset list
        self.todo = Todo()
        return self.parser.parse(data)
# ---
def read(self, path):
            return True
# ---
def test_alter_disconnect_to_false(self):
        self._test_alter_disconnect(True, False)
        self._test_alter_disconnect(False, False)
# ---
def __init__(self, instance: Any):
        self._instance = instance
        self._executor = ThreadPoolExecutor(max_workers=16)
# ---
def clean(c):
    """Remove generated files"""
    if os.path.isdir(CONFIG['deploy_path']):
        shutil.rmtree(CONFIG['deploy_path'])
        os.makedirs(CONFIG['deploy_path'])
# ---
def _call_all(self, fun, *args):
        '''
        Call the given function on all backend keys
        '''
        for kback in self.keys:
            print(kback)
            getattr(self.keys[kback], fun)(*args)
# ---
def comments():
    return [make_comment() for _ in range(3)]
# ---
def __init__(self, driver):
        super(BrowseMoviePage, self).__init__(driver)
        self.nav = NavBlock(driver)
# ---
def chronos(monkeypatch):
    """Virtual time fixture - makes time.sleep() controllable for fast tests."""
    clock = VirtualClock()

    # Patch time module
    monkeypatch.setattr(time, "time", clock.time)
    monkeypatch.setattr(time, "monotonic", clock.time)
    monkeypatch.setattr(time, "sleep", clock.sleep)

    return clock
# ---
def test_encoder_missing(self):
        """If no encoder is found, raise ValueError"""
        from datetime import datetime

        dynamizer = Dynamizer()
        with self.assertRaises(ValueError):
            dynamizer.encode(datetime.utcnow())
# ---
def reload_old(ng=False):
    if ng:
        bl_idnames = {n.bl_idname for n in ng.nodes if n.bl_idname in old_bl_idnames}
        for bl_id in bl_idnames:
            mod = register_old(bl_id)
            if mod:
                importlib.reload(mod)
            else:
                print("Couldn't reload {}".format(bl_id))
    else:
        for ng in bpy.data.node_groups:
            reload_old(ng)
# ---
def shutdown(self, wait: bool = True) -> None:
        del wait
# ---
def reset(git_path, module, dest):
    '''
    Resets the index and working tree to HEAD.
    Discards any changes to tracked files in working
    tree since that commit.
    '''
    cmd = "%s reset --hard HEAD" % (git_path,)
    return module.run_command(cmd, check_rc=True, cwd=dest)
# ---
def register(
        self,
        name: str,
        address: str,
        metadata: dict[str, str] | None = None,
    ) -> str:
        """Register an endpoint for actor discovery.

        Args:
            name: Actor name for discovery
            address: Address where actor is listening (host:port)
            metadata: Optional metadata for the endpoint

        Returns:
            Unique endpoint ID for later unregistration
        """
        ...
# ---
def get_cluster_hosts(self) -> [Host]:
        return list(self.deployment.hosts)[1:]
# ---
def _roots(kindpats):
    """Returns root directories to match recursively from the given patterns."""
    roots, dirs = _patternrootsanddirs(kindpats)
    return roots
# ---
def GetCurrentBufferNumber():
  return vim.current.buffer.number
# ---
def test_is_stop_signal_partial_match_with_padding():
    # stop_sequence is shorter and left padded with INVALID
    tail_tokens = hax.named(jnp.array([5, 6, 7], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[INVALID, 6, 7]], dtype=jnp.int32), axis=("seq", "position"))
    assert is_stop_signal(tail_tokens, stop_sequences)
# ---
def _is_url_like(path):
    return urllib.parse.urlparse(path).scheme != ""
# ---
def save_apartment(self, apartment_dto, send_event=True):
        # type: (ApartmentDTO, bool) -> ApartmentDTO
        self._check_rebus_ids(apartment_dto)
        apartment_orm = ApartmentMapper.dto_to_orm(apartment_dto)
        apartment_orm.save()
        if send_event:
            ApartmentController.send_config_change_event('save')
        return ApartmentMapper.orm_to_dto(apartment_orm)
# ---
def test_multiple_empty_sets(self):
        # test that any anonymous aliasing used by the dialect
        # is fine with duplicates
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.in_(bindparam("q", expanding=True)))
            .where(table.c.y.in_(bindparam("p", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": [], "p": []})
# ---
def __exit__(self, *_):
        self.shutdown(wait=False)
# ---
def stop(self, handle: VllmServerHandle) -> None:
        container_name = handle.docker_container_name
        subprocess.run(["docker", "rm", "-f", container_name], check=False, capture_output=True, text=True)
# ---
def test_create_actor_and_call_remote(client: LocalClient):
    actor = client.create_actor(Counter, name="counter")
    result = actor.increment.remote(5).result()
    assert result == 5
    assert actor.get.remote().result() == 5
# ---
def current_thread_local_mapping():
    """
    Get the current thread-local resource mapping, or None if there is no resource mapping set.
    :return:
    """
    if _mapping_holder.thread_data is None:
        return None
    if not hasattr(_mapping_holder.thread_data, "resource_mapping"):
        return None

    return _mapping_holder.thread_data.resource_mapping
# ---


def sort_third(l: list):
    """This function takes a list l and returns a list l' such that
    l' is identical to l in the indicies that are not divisible by three, while its values at the indicies that are divisible by three are equal
    to the values of the corresponding indicies of l, but sorted.
    >>> sort_third([1, 2, 3])
    [1, 2, 3]
    >>> sort_third([5, 6, 3, 4, 8, 9, 2])
    [2, 6, 3, 4, 8, 9, 5]
    """
    l = list(l)
    l[::3] = sorted(l[::3])
    return l
# ---
def poke(self, context):
        c = airflow.hooks.webhdfs_hook.WebHDFSHook(self.webhdfs_conn_id)
        logging.info(
            'Poking for file {self.filepath} '.format(**locals()))
        return c.check_for_path(hdfs_path=self.filepath)
# ---
def action_assign_wkf(self, cr, uid, ids, context=None):
        """ Changes picking state to assigned.
        @return: True
        """
        self.write(cr, uid, ids, {'state': 'assigned'})
        return True
# ---
def __get__(self, obj, cls):
        if obj is None:
            return None
        value = self.fget(obj)
        setattr(obj, self.func_name, value)
        return value
# ---
def test_created_on(self):
        eq_(self.record.created_on, None)
# ---
def add(self, source: str, data: str, timestamp: Timestamp | None = None) -> None:
        if timestamp:
            self.lines.append(LogLine.at(timestamp, source, data))
        else:
            self.lines.append(LogLine.now(source, data))
# ---
def test_load_mock_environment():
    """Test loading MockEnv via EnvConfig."""
    config = EnvConfig(env_class="marin.rl.environments.mock_env.MockEnv", env_args={"task_type": "cats", "seed": 42})

    env = load_environment_from_spec(config)

    assert isinstance(env, MockEnv)
    assert env.task_type == "cats"
    assert len(env.train_examples) > 0
    assert len(env.eval_examples) > 0
# ---
def define_tables(cls, metadata):
        Table(
            "is_distinct_test",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("col_a", Integer, nullable=True),
            Column("col_b", Integer, nullable=True),
        )
# ---
def test_corrupt_program_single_step(bank):
    source = CORPUS[3]  # add -- very short
    rng = random.Random(42)

    corrupted, mutations = corrupt_program(source, num_steps=1, bank=bank, rng=rng)

    assert len(mutations) <= 1
    if mutations:
        assert corrupted != source
# ---
def to_jax_shape(shape):
    from haliax.core import Axis, ensure_tuple

    shape = ensure_tuple(shape)
    return tuple(axis.size if isinstance(axis, Axis) else axis for axis in shape)
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_inverse: typing.Literal[True],
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]: ...
# ---


def sum_to_n(n: int):
    """sum_to_n is a function that sums numbers from 1 to n.
    >>> sum_to_n(30)
    465
    >>> sum_to_n(100)
    5050
    >>> sum_to_n(5)
    15
    >>> sum_to_n(10)
    55
    >>> sum_to_n(1)
    1
    """
    return sum(range(n + 1))
# ---
def log_summary(metrics: dict[str, Any]):
    """
     Log summary metrics to the global tracker.

    Args:
         metrics: Metrics to log
    """
    global _global_tracker
    if _global_tracker is None:
        warnings.warn("No global tracker set")
        return

    _global_tracker.log_summary(metrics)
# ---
def has_soundlibs():
    try:
        import librosa  # noqa F401
        import soundfile  # noqa F401

        return True
    except ImportError:
        return False
# ---
def define_class_with_no_name():
            @six.add_metaclass(profiler.TracedMeta)
            class FakeTraceWithMetaclassNoName(FakeTracedCls):
                pass
# ---
def __init__(self, manager: ClusterManager, remote_client: RemoteClusterClient):
        self._manager = manager
        self._remote_client = remote_client
# ---
def add_children(graph, parent_id, d, level=0):

    blue = "#6b6bd1"
    white = "#fdfefd"
    green = "#33a333"
    colours = [blue, white, green] * 3

    for class_, children in d.items():
        colour = colours[level]
        child_label = class_
        child_id = parent_id + "_" + class_
        add_child(graph, child_id, child_label, parent_id, colour)
        add_children(graph, child_id, children, level+1)
# ---
def LogPABotError(message):
    _pabotlog.error(message)
# ---
def foo():
            return hax.zeros((Dim1, Dim2, Dim3))
# ---
def _parse_arrow_schema(self):
        if self._schema:
            return

        self._schema = pyarrow.ipc.read_schema(
            pyarrow.py_buffer(self._first_message.arrow_schema.serialized_schema)
        )
        self._column_names = [field.name for field in self._schema]
        self._first_message = None
# ---
def pre_jit(x):
        if is_source:
            inp = x
        else:
            inp = np.zeros_like(x)
        inp = np.expand_dims(inp, axis=0)
        return host_local_array_to_global_array(inp, global_mesh, pspec)
# ---
def object_list_uid(object_list):
  """Creates a single string from object ids."""
  object_list = nest.flatten(object_list)
  return ', '.join([str(abs(id(x))) for x in object_list])
# ---
def utilities(choosers_dm, spec, test_data):
    utils = choosers_dm.dot(spec).astype('float')
    return pd.DataFrame(
        utils.as_matrix().reshape(test_data['probabilities'].shape),
        columns=test_data['probabilities'].columns)
# ---
def __repr__(self):
        return_str = "<MidiTrack %d -- %d events\n" % (self.index, len(self.events))
        for event in self.events:
            return_str = return_str + "    " + event.__repr__() + "\n"
        return return_str + "  >"
# ---
def test_on_start_date(self):
        """Test when create_time is exactly on the start date"""
        self.assertTrue(check_create_time("2023-01-01 00:00:00 PST", "2023-01-01", "2023-01-31"))
# ---
def slow_fn(x):
        time.sleep(0.2)
        return x * 2
# ---
def on_delivery(self, err, msg):
        if err:
            print('Delivery report: Failed sending message {0}'.format(msg.value()))
            print(err)
            # We could retry sending the message
        else:
            print('Message produced, offset: {0}'.format(msg.offset()))
# ---
def scaling_action_name(action: int) -> str:
    """Return enum name like 'SCALING_ACTION_SCALE_UP'."""
    try:
        return _SCALING_ACTION.values_by_number[action].name
    except KeyError:
        return f"UNKNOWN({action})"
# ---
def _bbox_in_coverage_srs(self, bbox, srs):
        if srs != self.srs:
            bbox = srs.transform_bbox_to(self.srs, bbox)
        return bbox
# ---
def __init__(self,
                 config,
                 debug):
        ''' Constructor for OCVersion '''
        super(OCVersion, self).__init__(None, config)
        self.debug = debug
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> float:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def __le__(self, other: "Timestamp") -> bool:
        return self._epoch_ms <= other._epoch_ms
# ---
def __call__(self, x):
            return x + self.weight
# ---
def _ax_name(ax: AxisSelector) -> str:
        if isinstance(ax, Axis):
            return ax.name
        else:
            return ax
# ---
def spec(self) -> PageTableSpec:
        return PageTableSpec(num_pages=self.num_pages, page_size=self.page_size)
# ---
def default(self, value):
        """Convert mongo.ObjectId."""
        if isinstance(value, ObjectId):
            return str(value)
        return super(ElasticJSONSerializer, self).default(value)
# ---
def max_queued_tokens(self) -> int:
        """Maximum number of tokens that can be buffered in the queue."""
        return self.queued_tokens.axis_size("position")
# ---
def _matches_target(key_path, config: QuantizationConfig) -> bool:
    if not key_path:
        key = ""
    else:
        key = _key_path_to_str(key_path[-1:])

    if config.targets is None:
        return True
    if isinstance(config.targets, list):
        return key in config.targets

    import re

    key_path_str = _key_path_to_str(key_path)
    return re.match(config.targets, key_path_str) is not None
# ---
def find_external_ips(self, inc_external_ip):
        ''' find a specific external IP '''
        val = None
        try:
            idx = self.get_external_ips().index(inc_external_ip)
            val = self.get_external_ips()[idx]
        except ValueError:
            pass

        return val
# ---
def __repr__(self):
        return "<patternmatcher patterns=%r>" % self._pats
# ---
def convert_pre(self, el, text, convert_as_inline):
        if not text:
            return ""
        code_language = self.options["code_language"]

        if self.options["code_language_callback"]:
            code_language = self.options["code_language_callback"](el) or code_language

        if "```" in text:  # have to use <pre>
            return f"\n<pre><code>{text}</code></pre>\n"
        else:
            return f"\n```{code_language}\n{text}\n```\n"
# ---
def score_syntax_errors(program_lines):
    points = {')': 3, ']': 57, '}': 1197, '>': 25137}
    s = 0
    scores_auto = []

    for line in program_lines:
        corrupted, stack = corrupted_character(line)

        if corrupted:
            s += points[corrupted]
        else:
            scores_auto.append(score_autocomplete(stack))

    return s, sorted(scores_auto)[floor(len(scores_auto)/2)]
# ---
def list_tasks(
        self,
        request: cluster_pb2.Worker.ListTasksRequest,
    ) -> cluster_pb2.Worker.ListTasksResponse: ...
# ---
def test_dontreadfrominput_buffer_python3():
    from _pytest.capture import DontReadFromInput
    f = DontReadFromInput()
    fb = f.buffer
    assert not fb.isatty()
    pytest.raises(IOError, fb.read)
    pytest.raises(IOError, fb.readlines)
    pytest.raises(IOError, iter, fb)
    pytest.raises(ValueError, fb.fileno)
    f.close()
# ---
def __init__(self):
        self._auth = None
        self._agent_pid = None
        self._ssh_auth_re = re.compile(_SSH_AUTH_RE)
# ---
def the_object_name_exists(name):
    obj = bpy.data.objects.get(name)
    if not obj:
        assert False, f'The object "{name}" does not exist'
    return obj
# ---
def test_callable_entrypoint(self):
        entry = Entrypoint.from_callable(_dummy_fn, args=(42,))
        iris_entry = convert_entrypoint(entry)
        assert iris_entry.is_callable
# ---
def resistance(self):
        return self._resistance
# ---
def healthy(self) -> bool:
        return not self.is_being_preempted()
# ---
def _make_prefix_absolute_path(prefix, override_path):
    if _is_relative_path(override_path):
        override_path = os.path.join(prefix, override_path)
    return override_path
# ---
def fn(x):
        return {
            "expanded": hax.broadcast_axis(x, D),
            "twice": x + x,
        }
# ---
def _write(filename, contents):
        ''' Actually write the file contents to disk. This helps with mocking. '''

        with open(filename, 'w') as sfd:
            sfd.write(contents)
# ---
def remove(self, container_id: str) -> None: ...
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        self.run.config.update(_convert_value_to_loggable_rec(hparams), allow_val_change=True)
# ---
def loss_ref_batched(v):
        return jnp.sum(reference_impl_batched(v))
# ---
def is_terminal(self) -> bool:
        """Check if this attempt is in a terminal state."""
        return self.state in TERMINAL_TASK_STATES
# ---
def _ingest_outputs(self, outputs: _DecodeOutputs | None) -> int:
        """Drain device outputs into host results and apply host-side release.

        Returns the number of tokens appended to results. No-op if outputs is None.
        """
        if outputs is None:
            return 0
        appended = self._extract_outputs(outputs)
        return appended
# ---
def max_seqs(self) -> int:
        return self._max_seqs
# ---
def test_do_execute_wo_replace(self):
        self._test_do_execute(False)
# ---
def add_hook(self, fn: JitCallback, *, every: int = 1): ...
# ---
def init(In, Out, key):
            up_proj = hax.nn.Linear.init(In, Out, key=key)
            down_proj = hax.nn.Linear.init(Out, In, key=key)
            return Block(up_proj, down_proj)
# ---
def test_passing_invalid_ip_address_throws_exception(self):
        with pytest.raises(InvalidIpAddressException):
            line = Line(line_contents=get_updated_line_contents({'ip_address': 'foobar'}))
# ---
def get_volume_mounts(self):
        '''return volume mount information '''
        return self.get_volumes(mounts=True)
# ---


def is_prime(n):
    """Return true if a given number is prime, and false otherwise.
    >>> is_prime(6)
    False
    >>> is_prime(101)
    True
    >>> is_prime(11)
    True
    >>> is_prime(13441)
    True
    >>> is_prime(61)
    True
    >>> is_prime(4)
    False
    >>> is_prime(1)
    False
    """
    if n < 2:
        return False
    for k in range(2, n - 1):
        if n % k == 0:
            return False
    return True
# ---
def test_filter(sample_data, backend):
    """Test filtering dataset."""
    ds = Dataset.from_list(sample_data).filter(lambda x: x % 2 == 0)
    assert list(Backend.execute(ds, context=backend)) == [2, 4, 6, 8, 10]
# ---
def parse_def_links(self, m):
        key = _keyify(m.group(1))
        self.def_links[key] = {
            'link': m.group(2),
            'title': m.group(3),
        }
# ---
def to_hf_config(self) -> tuple[float, dict | None]:
        """Returns the rope_theta and config dict for the HF config."""
        pass
# ---
def addSummaryKey(self, key):
        assert isinstance(key, str)
        return SummaryKeyMatcher.cNamespace().add_key(self, key)
# ---
def test_decode_source_skips_specials(tok):
    ids = [tok.sos_token_id, tok.encode_char("h"), tok.encode_char("i"), tok.eos_token_id]
    assert tok.decode_source(ids) == "hi"
# ---
def map_png():
    return make_map(request, format='png')
# ---
def get_ova_info(ova_path):
    ns = {'ovf': _OVF_NS, 'rasd': _RASD_NS}

    try:
        root = ET.fromstring(_read_ovf_from_ova(ova_path))
    except ET.ParseError as e:
        raise V2VError('Error reading ovf from ova, position: %r' % e.position)

    vm = {}
    _add_general_ovf_info(vm, root, ns, ova_path)
    _add_disks_ovf_info(vm, root, ns)
    _add_networks_ovf_info(vm, root, ns)

    return response.success(vmList=vm)
# ---
def cross_entropy_loss(logits: jax.Array, labels: jax.Array) -> jax.Array:
    log_probs = jax.nn.log_softmax(logits, axis=-1)
    gathered = jnp.take_along_axis(log_probs, labels[..., None], axis=-1)
    return -jnp.mean(gathered)
# ---
def definition(self):
    """Function definition proto."""
    self._create_definition_if_needed()
    if self._c_func:
      with c_api_util.tf_buffer() as buf:
        c_api.TF_FunctionToFunctionDef(self._c_func.func, buf)
        fdef = function_pb2.FunctionDef()
        proto_data = c_api.TF_GetBuffer(buf)
        fdef.ParseFromString(compat.as_bytes(proto_data))
      return fdef
    return self._definition
# ---
def flatten(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> tuple[Sequence[Any], Any]:
    """Alias for :func:`haliax.tree_util.tree_flatten` matching :func:`jax.tree.flatten`."""

    return tree_util.tree_flatten(tree, is_leaf=is_leaf)
# ---
def weibull_min(key, shape: AxisSpec, scale: NamedOrNumeric, concentration: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    scale = broadcast_to(scale, shape)
    concentration = broadcast_to(concentration, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.weibull_min(key, scale.array, concentration.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def get_list_of_block_numbers(item):
    """ Creates a list of block numbers of the given list/single event"""
    if isinstance(item, list):
        return [element["blockNumber"] for element in item]

    if isinstance(item, dict):
        block_number = item["blockNumber"]
        return [block_number]

    return list()
# ---
def _map(example: dict) -> LmExample:
                loss_weight = example[loss_weights_key]
                loss_weight = self.loss_weight_transform(loss_weight)
                return _create_lm_example(example[input_ids_key], loss_weight)
# ---
def get_properties(self):
        return COMMON_PROPERTIES
# ---
def where(
    condition: NamedArray,
    *,
    fill_value: int,
    new_axis: Axis,
) -> tuple[NamedArray, ...]: ...
# ---
def job_id(self) -> JobId | None:
        return self._job_id
# ---
def rust_compute_paragraph_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> pa.RecordBatch:
    pipeline = [
        dupekit.Transformation.SplitParagraphs(text_col=text_col, id_col=id_col),
        dupekit.Transformation.Hash(input_col="paragraph_text", output_col="hash", algo=dupekit.HashAlgorithm.Xxh3_128),
    ]
    return dupekit.transform(batch, pipeline)
# ---
def test_minhash_dimensions():
    """Test that MinHash output has correct dimensions."""
    texts = ["doc one", "doc two"]
    num_perms = 128
    batch = pa.RecordBatch.from_pydict({"text": texts})
    pipeline = [Transformation.MinHash(input_col="text", output_col="sig", num_perms=num_perms, ngram_size=3, seed=42)]
    sigs = transform(batch, pipeline)["sig"]
    for sig in sigs:
        assert len(sig.as_py()) == num_perms
        assert all(isinstance(x, int) for x in sig.as_py())
# ---
def markdownify_kwargs(self) -> dict:
        exclude = {*list(self.resiliparse_kwargs.keys()), "prepend_title"}
        return {f.name: getattr(self, f.name) for f in fields(self.markdownify_config) if f.name not in exclude}
# ---
def fmod(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.fmod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fmod.html)
    """
    return jnp.fmod(x1, x2)
# ---
def Embed(self) -> Axis:
        return Axis("embed", self.embed_dim)
# ---
def test_bundle(tmp_path):
    """Create a test bundle and return file:// path."""
    return create_test_bundle(tmp_path)
# ---
def get_dartmouth(b):
    A = 500
    B = 2500
    return b['b2'].add(A).divide(b['b1'].add(B)).select(['sur_refl_b02'], ['b1'])
# ---
def _get_step(path: str) -> str:
        # make sure it looks like step-{train_step_number}
        g = re.match(r"step-(\d+)/?$", path)
        if g is None:
            raise ValueError(f"Invalid path: {path}")

        return g.group(1)
# ---
def reset_testable(self):
        self.reset_count += 1
# ---
def test_registered(self):
        eq_(self.record.registered, False)
# ---
def test_zero_deadline_expires_immediately():
    """Deadline with zero timeout expires immediately."""
    deadline = Deadline.from_ms(0)
    assert deadline.expired()
    assert deadline.remaining_ms() == 0
    assert deadline.remaining_seconds() == 0.0
# ---
def get_shard_dir(dir_name: os.PathLike, subset_name: str | None, split: str) -> os.PathLike | str:
    """Creates a new path with the subset and split names.
    e.g., create_subset_name('gs://thisserver/testfolder-a982374', 'subset', 'train') -> 'gs://thisserver/testfolder-a982374/subset/train'
    """
    if (subset_name == "default") or (subset_name is None):
        return os.path.join(dir_name, split)
    return os.path.join(dir_name, subset_name, split)
# ---
def _load_files(self, dataset_code):
        url = "http://www.bea.gov/national/nipaweb/GetCSV.asp?GetWhat=SS_Data/Section1All_xls.zip&Section=2"
        self.register_url(url,
                          self.DATASETS[dataset_code]["filepath"])
# ---
def on_deactivate():
            """The window was deactivated.

            This event can be triggered by clicking on another application
            window.  When a window is deactivated it no longer has the
            keyboard focus.

            :event:
            """
# ---
def __init__(self, config):
        self.config = config

        # State tracking
        self.worker = None
        self.thread = None
        self.result_queue = queue.Queue(maxsize=1)
# ---
def RunTask(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __init__(self):
        self._start = time.monotonic()
# ---
def max_seqs(self) -> int:
        """Number of sequences in the buffer."""
        return self.tokens.axis_size("seq")
# ---
def reraise(self):
        if self.ex is not None:
            raise self.ex.with_traceback(self.tb.as_traceback())
        else:
            raise Exception("Process failed with no exception").with_traceback(self.tb.as_traceback())
# ---
def read_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()
# ---
def subtotal(self):
        return self.__subtotal
# ---
def test_response_tokens_from_choice(inference_ctx, llama3_tokenizer):
    """Test extracting token IDs from Choice using BPE round-trip."""
    response_text = "The answer is 42"
    choice = create_choice_with_logprobs(llama3_tokenizer, response_text)

    tokens = inference_ctx.response_tokens_from_choice(choice)

    # Should match tokenizer's encoding
    expected_tokens = llama3_tokenizer.encode(response_text, add_special_tokens=False)
    np.testing.assert_array_equal(tokens, expected_tokens)
# ---
def test_succeeded(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_SUCCEEDED) == JobStatus.SUCCEEDED
# ---
def _do_jit_log(metrics, *, step=None):
    try:
        if _global_tracker is None:
            warnings.warn("No global tracker set")
        else:
            _global_tracker.log(metrics, step=step, commit=False)
    except Exception:
        logger.exception("Error logging metrics")
# ---
def is_axis_compatible(ax1: AxisSelector, ax2: AxisSelector):
    """
    Returns true if the two axes are compatible, meaning they have the same name and, if both are Axis, the same size
    """
    if isinstance(ax1, str):
        if isinstance(ax2, str):
            return ax1 == ax2
        return ax1 == ax2.name
    if isinstance(ax2, str):
        return ax1.name == ax2
    return ax1.name == ax2.name
# ---
def _make_dummy_batch(EvalBatch, EvalPos):
    dummy_batch = hax.vmap(LmExample.causal, EvalBatch)(
        hax.zeros(EvalPos, dtype=jnp.int32),
        loss_weight=hax.zeros(EvalPos, dtype=jnp.float32),
        segment_ids=hax.zeros(EvalPos, dtype=jnp.int32),
    )
    out = hax.shard(dummy_batch, {})
    return out
# ---
def test_connection_as_ctx(self):
        fn = self._trans_fn()
        ctx = testing.db.connect()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        # autocommit is on
        self._assert_fn(5, value=8)
# ---
def _set_data(self, value):
        if value is not None and not isinstance(value, bytes):
            if isinstance(value, str):
                value = value.encode('utf-8')
        self._parameter1 = value
# ---
def _loss_function(model, batch, key):
            return loss_fn(model, batch, key)
# ---
def mark_dup_documents(docs: Iterator[dict]) -> Iterator[dict]:
        fuzzy_dup_map = _load_fuzzy_dupe_map_shard(fuzzy_dup_shards)

        for doc in docs:
            is_fuzzy_dup = fuzzy_dup_map.get(doc["id"], False)
            doc["attributes"] = doc.get("attributes", {})
            assert DedupMode.FUZZY_DOCUMENT not in doc["attributes"]
            doc["attributes"][str(DedupMode.FUZZY_DOCUMENT)] = is_fuzzy_dup
            yield doc
# ---
def device_password(self):
        if self._values['device_password'] is None:
            return None
        return self._values['device_password']
# ---
def dict(self):
		"""
		Return a dict of the row as colname: value
		"""
		return dict(zip(self.structure.column_names, self))
# ---
def updLine(self, row, tmpLine):
    #print("--- %d ---" % row)
    pass
# ---
def baseline_func_amp(self,z_data,f_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		returns the baseline as a function
		the points in between the datapoints are computed by cubic interpolation
		'''
		return interp1d(f_data, self._baseline_als(np.absolute(z_data),lam,p,niter=niter), kind='cubic')
# ---
def test_position_tokens_dont_overlap_base(tok):
    """Position tokens and base vocab tokens should not overlap."""
    pos_range = set(range(tok.position_token_offset, tok.position_token_offset + tok.num_position_tokens))
    base_range = set(range(tok.base_token_offset, tok.base_token_offset + tok.base_vocab_size))
    assert not pos_range & base_range
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: AttentionMask | NamedArray | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        return self.transformer(self.embeddings.embed(input_ids), attn_mask=attn_mask, key=key, pos_ids=pos_ids)
# ---
def append(self, data: np.ndarray):
        self.extend([data])
# ---
def _multislice_info_from_head(head: SliceInfo, slice_id: int, num_slices: int) -> MultisliceInfo:
    return MultisliceInfo(
        coordinator_ip=head.ip_address,
        slice_id=slice_id,
        num_slices=num_slices,
        port=8081,
    )
# ---
def __str__(self):
        return f"{self.__class__.__name__}(shape={self.shape})"
# ---
def map_fn(p):
        if isinstance(p, hax.nn.Linear):
            if p.weight is None:
                return p
            return f(p)
        elif or_else is not None:
            return or_else(p)
        else:
            return p
# ---
def _convert_ray_status(ray_status: RayJobStatus) -> JobStatus:
    mapping = {
        RayJobStatus.PENDING: JobStatus.PENDING,
        RayJobStatus.RUNNING: JobStatus.RUNNING,
        RayJobStatus.SUCCEEDED: JobStatus.SUCCEEDED,
        RayJobStatus.FAILED: JobStatus.FAILED,
        RayJobStatus.STOPPED: JobStatus.STOPPED,
    }
    return mapping.get(ray_status, JobStatus.FAILED)
# ---
def test_alter_disconnect_to_true(self):
        self._test_alter_disconnect(False, True)
        self._test_alter_disconnect(True, True)
# ---
def test_hf_gpt2_roundtrip_fa():
    hf_config = HfGpt2Config.from_pretrained("gpt2")
    config = Gpt2Config.from_hf_config(hf_config)
    config = dataclasses.replace(config, use_flash_attention=True, flash_attention_block_size=128)
    _roundtrip_compare_gpt2_checkpoint("gpt2", None, config=config)
# ---
def slice_id(self) -> str:
        return self._group_id
# ---
def is_mysql_running():
    try:
        import MySQLdb
        with MySQLdb.connect(host='127.0.0.1', user='root'):
            pass
        return True
    except:
        return False
# ---
def recurPowerNew(base, exp):

    # Base case is when exp = 0
    if exp <= 0:
        return 1

    # Recursive Call
    elif exp % 2 == 0:
        return recurPowerNew(base*base, exp/2)

    return base * recurPowerNew(base, exp - 1)
# ---
def test_open(self):
        self.db = smadata2.db.SQLiteDatabase(self.dbname)
# ---
def __init__(self, pool_size=2, strides=None, padding='valid'):
        super(LW_AveragePooling1D, self).__init__(pool_size, strides, padding)
# ---
def callback(response):
            current = response.context['upload_stream_current']
            total = response.context['data_stream_total']
            if current is not None:
                progress.append((current, total))
# ---
def is_slow(self):
        return True
# ---
def __init__(self, docs: List[T]):
        self.docs = docs
# ---
def _partitions(lst):
    """Generate all partitions of a list."""
    if not lst:
        yield [[]]
    else:
        for i in range(len(lst)):
            for part in _partitions(lst[i + 1 :]):
                yield [lst[: i + 1]] + part
# ---
def push(self, item):
        """Push an element to the end of the queue.

        Parameters
        ----------
        item :
            The element to append.

        """
        self._queue.append(item)
# ---
def generate_id(n=16):
    alphabet = string.ascii_letters + string.digits
    return ''.join(random.choice(alphabet) for _ in range(n))
# ---
def test_trim_exact_length_no_change():
    """Test that array of exact length is unchanged."""
    ary = np.array([1, 2, 3, 4, 5], dtype=np.int32)
    result = train_batch.trim_and_pad(ary, max_seq_len=5, pad_to=5, padding_value=999)

    np.testing.assert_array_equal(result, ary)
# ---
def output_linebreak(self, m):
        return self.renderer.linebreak()
# ---
def remove(self, thread: ManagedThread) -> None:
        """Remove a thread from this container.

        Called automatically when threads complete. Thread-safe.
        """
        with self._lock:
            try:
                self._threads.remove(thread)
            except ValueError:
                # Already removed, that's fine
                pass
# ---
def echo(self, msg: str) -> str:
        return f"echo: {msg}"
# ---
def append(self, record: BufferedLogRecord) -> None:
        with self._lock:
            self._buffer.append(record)
# ---
def obj_code_type(self):
                return Investment.CODE_TYPE_ISIN if is_isin_valid(Field('code')(self)) else NotAvailable
# ---
def _matches(self, **kwargs):
		for k, v in kwargs.items():
			if not self._query(k, v):
				return False
		return True
# ---
def convert_to_cache(self, value, record, validate=True):
        if not value:
            return False
        if isinstance(value, basestring):
            if validate:
                # force parsing for validation
                self.from_string(value)
            value = value[:DATETIME_LENGTH]
            if len(value) == DATE_LENGTH:
                value += " 00:00:00"
            return value
        return self.to_string(value)
# ---
def port(self) -> int:
        """Actual bound port (may differ from config if port=0 was specified)."""
        if self._server and self._server.servers:
            # Get actual port from the first server socket
            sockets = self._server.servers[0].sockets
            if sockets:
                return sockets[0].getsockname()[1]
        return self._config.port
# ---
def metadata(self) -> Dict[str, Any]:
        """Any metadata that changes the behavior of this processor."""
        raise NotImplementedError
# ---
def init(config: HackableTransformerConfig, *, key):
        S = Stacked  # use BlockSeq for non-homogeneous layers
        layers = S.init(config.Layers, HackableDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config, key=shaped_rng_split(key, config.num_layers)
        )
        return HackableTransformer(config, layers, config.mk_LayerNorm(config.Embed))
# ---
def run(self, coro: Coroutine[Any, Any, T]) -> T:
        """
        Run an async function from sync code and wait for result.

        Args:
            coro: Coroutine to run (e.g., async_func(args))

        Returns:
            Result of the coroutine
        """
        if self._loop is None:
            raise RuntimeError("AsyncBridge not started. Call start() first.")

        future = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return future.result()
# ---
def test_copy_experiment_unauthorized(self):
        """ Tests that copy_experiment fails when unauthorized """
        self.set_roles([])
        experiment = self.create_test_experiment()
        url = reverse("ab_testing_tool_copy_experiment", args=(experiment.id,))
        response = self.client.post(url, follow=True)
        self.assertTemplateUsed(response, "ab_tool/not_authorized.html")
# ---
def test_beam_candidate_with_edits():
    m = Mutation(start=4, end=5, replacement="2", node_type="Constant", original="1")
    c = BeamCandidate(source="x = 2\n", score=-1.0, depth=1, edits=(m,))
    assert len(c.edits) == 1
    assert c.edits[0].replacement == "2"
# ---
def __delitem__(self, key):
        del self._data[id(key)]
# ---
def to_str(val):
  return val
# ---
def patched_start(self):
        captured_manager.append(self)
        return original_start(self)
# ---
def test_parse_no_version_format(self):
        yaml = ''
        self._parse_template(yaml, 'Template format version not found')
        yaml2 = '''Parameters: {}
Mappings: {}
Resources: {}
Outputs: {}
'''
        self._parse_template(yaml2, 'Template format version not found')
# ---
def __hash__(self):
        return 0
# ---
def pending(self) -> int:
        return len(self.leases)
# ---
def depth(self):
        return self.__depth
# ---
def test_allocated_ports_are_usable(allocator):
    """Test that allocated ports can actually be bound."""
    ports = allocator.allocate(count=3)

    for port in ports:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(("", port))
# ---
def set_mouse_cursor(self, cursor=None):
        """Change the appearance of the mouse cursor.

        The appearance of the mouse cursor is only changed while it is
        within this window.

        :Parameters:
            `cursor` : `MouseCursor`
                The cursor to set, or None to restore the default cursor.

        """
        if cursor is None:
            cursor = DefaultMouseCursor()
        self._mouse_cursor = cursor
        self.set_mouse_platform_visible()
# ---
def has_pending_call(self, ar):
        """
        Returns true if the call (keyed by the AsyncResult returned by _routing_call) is still pending.
        """
        for _, qar, _, _, _, _ in self._ctrl_queue.queue:
            if qar == ar:
                return True

        return False
# ---
def setServer( self, url ):
    self.serverURL = url
# ---
def release_slot(self, slot_id: int) -> "DecodeState":
        sequences = self.sequences.release_slot(slot_id)
        return dataclasses.replace(self, sequences=sequences)
# ---
def chip_count(self) -> int:
        """CPU has no accelerator chips."""
        return 0
# ---
def __eq__(self, other):
        if not isinstance(other, GeomCoverage):
            return NotImplemented

        if self.srs != other.srs:
            return False

        if self.bbox != other.bbox:
            return False

        if not self.geom.equals(other.geom):
            return False

        return True
# ---
def render(self, view_name, notebook, data=None, method='GET', **kwargs):
        if data is None:
            data = {}
        try:
            view = views[view_name]
        except KeyError:
            raise ResponseError(404)
        template_data = view(method, notebook, data, **kwargs)
        if isinstance(template_data, tuple):
            template_name, template_data = template_data
        else:
            template_name = view.__name__ + '.html'
        return template_name, template_data
# ---
def load_data(self):
        try:
            f = open('data/er_nick-data.csv', 'rt')
            reader = csv.reader(f)
            for nick_irc,id,nick_er,level,strength,rank_points,citizenship in reader:
                self.data[nick_irc] = {'id': int(id), 'nick': nick_er, 'level': int(level), 'strength': float(strength), 'rank_points': int(rank_points), 'citizenship': citizenship}
            f.close()
        except:
            pass
# ---
def loss_fast(v):
        return jnp.sum(template_op(v))
# ---
def KeyPos(self) -> Axis:
        return self.config.KeyPos
# ---
def build(c):
    """Build local version of site"""
    c.run('pelican -s pelicanconf.py')
# ---
def close(self):
        self._file.close()
# ---
def _dummy_fn(x: int) -> int:
    return x + 1
# ---
def load(cls, config_path: Path | str) -> "IrisConfig":
        """Load IrisConfig from YAML file.

        Args:
            config_path: Path to YAML configuration file

        Returns:
            IrisConfig with defaults applied and validated
        """
        proto = load_config(config_path)
        return cls(proto)
# ---
def test_engine_module_name():
    engine = salt.engines.Engine({}, "foobar.start", {}, {}, {}, {}, name="foobar")
    assert engine.name == "foobar"
# ---
def check_files(context):
        """Return the number of present and absent files."""
        present = 0
        absent = 0
        for filename in context.filenames:
            if os.path.exists(os.path.join(context.temp_dir, filename)):
                present += 1
            else:
                absent += 1
        return context.temp_dir, present, absent
# ---
def setUp(self):
        super(XenAPIDetermineDiskImageTestCase, self).setUp()
        glance_stubs.stubout_glance_client(self.stubs)

        class FakeInstance(object):
            pass

        self.fake_instance = FakeInstance()
        self.fake_instance.id = 42
        self.fake_instance.os_type = 'linux'
        self.fake_instance.architecture = 'x86-64'
# ---
def _load_model():
            return load_model_from_checkpoint(
                checkpoint=config.initial_checkpoint,
                model_config=config.model,
                trainer_config=config.trainer,
                vocab_axis=Vocab,
                tokenizer=self.tokenizer,
                mesh=config.trainer.device_mesh,
                axis_mapping=self.config.trainer.parameter_axis_mapping,
                key=model_key,
            )
# ---
def output_exemplar(self):
        return self.transforms[-1].output_exemplar
# ---
def parse_table(self, m):
        item = self._process_table(m)

        cells = re.sub(r'(?: *\| *)?\n$', '', m.group(3))
        cells = cells.split('\n')
        for i, v in enumerate(cells):
            v = re.sub(r'^ *\| *| *\| *$', '', v)
            cells[i] = re.split(r' *\| *', v)

        item['cells'] = cells
        self.tokens.append(item)
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.field == other.field and self.pattern == other.pattern
# ---
def avatarEnterNextState(self):
        if len(self.nearbyAvatars) == 1:
            if self.fsm.getCurrentState().getName() != 'Walk':
                self.fsm.request('Chatty')
            else:
                self.notify.debug('avatarEnterNextState: in walk state')
        else:
            self.notify.debug('avatarEnterNextState: num avatars: ' + str(len(self.nearbyAvatars)))
# ---
def _add_tensor_and_parents(self, tensor):
    op = self._add_op_and_parents(tensor.op)
    return op.outputs[tensor.value_index]
# ---
def shard_loss(x_in, w_in, y_in):
            loss = linear_softmax_cross_entropy_loss(
                x_in,
                y_in,
                w_in,
                reduction=None,
                implementation="mosaic_tpu",
            )
            local_sum = jnp.sum(loss)
            total_sum = jax.lax.psum(local_sum, "data")
            total_denom = jax.lax.psum(x_in.shape[0], "data")
            return total_sum / total_denom
# ---
def BufferModified( buffer_object ):
  return bool( int( GetBufferOption( buffer_object, 'mod' ) ) )
# ---
def fn(input_ids, mask):
        return MistralLMHeadModel.init(Vocab=Vocab, config=mistral_config, key=random.PRNGKey(0))(input_ids, mask)
# ---
def _tokenize(self, text, **kwargs):
        tokens = np.fromstring(text, dtype=int, sep=" ")
        return tokens
# ---
def shuffle_ds(ds, k):
            if self.shuffle is True:
                ds = ds.shuffle(k, perm_type=perm_type)
            elif isinstance(self.shuffle, int) and self.shuffle > 0:
                ds = ds.era_shuffle(self.shuffle, key=k, perm_type=perm_type)
            return ds
# ---
def __init__(self, raw_data):
        self._raw = raw_data
# ---
def test_wf_improved(self):
        G = nx.union(self.P4, nx.path_graph([4, 5, 6]))
        c = nx.closeness_centrality(G)
        cwf = nx.closeness_centrality(G, wf_improved=False)
        res = {0: 0.25, 1: 0.375, 2: 0.375, 3: 0.25, 4: 0.222, 5: 0.333, 6: 0.222}
        wf_res = {0: 0.5, 1: 0.75, 2: 0.75, 3: 0.5, 4: 0.667, 5: 1.0, 6: 0.667}
        for n in G:
            assert almost_equal(c[n], res[n], places=3)
            assert almost_equal(cwf[n], wf_res[n], places=3)
# ---
def __post_init__(self):
        if " " in self.name:
            raise ValueError("Job name must not contain spaces")
# ---
def alibi_attention_bias(Heads: Axis, KPos: Axis, bias_max: float = 8, dtype=jnp.float32) -> NamedArray:
    """
    Creates an attention bias for alibi attention.

    :param KPos: Axis of (key) sequence length
    :param Heads: Axis of heads
    :return: NamedArray of shape (Heads, KPos)
    """
    slopes = haliax.named(np.array(_get_alibi_slopes(Heads.size, bias_max)), Heads)
    positions = haliax.arange(KPos).broadcast_axis(Heads)

    biases = slopes * positions
    return biases.astype(dtype)
# ---
def on_job_fetch_update(self, progress):
        self.main_view.update_job_fetch_progress_dialog(progress)
# ---
def run(self):

        logger.info('starting webapp')
        logger.info('hosted at %s' % settings.WEBAPP_IP)
        logger.info('running on port %d' % settings.WEBAPP_PORT)

        app.run(settings.WEBAPP_IP, settings.WEBAPP_PORT)
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "%s:%s" % (self.kind, self.match)
# ---
def unflatten_from_export(self: Mod, template: Mod) -> Mod:
        """
        Unflatten the module after import from torch.

        Template has the proper structure (e.g. articulated named axes) but the values are meaningless.
        """
        del template
        return self
# ---
def loop():
    return asyncio.get_event_loop()
# ---
def _add_log(self, cluster: str, level: str, message: str, details: str | None = None) -> None:
        entry = LogEntry(
            timestamp=datetime.now(),
            cluster=cluster,
            level=level,
            message=message,
            details=details,
        )
        with self._logs_lock:
            self._logs.append(entry)
# ---
def __init__(self, controller_url: str, log_tree: LogTree, logger: SmokeTestLogger):
        self._client = ControllerServiceClientSync(controller_url)
        self._scheduling_dir = log_tree.get_dir("scheduling", "Task scheduling snapshots")
        self._logger = logger
        self._stop_event = threading.Event()
        self._thread: threading.Thread | None = None
        self._tracked_jobs: set[str] = set()
# ---
def testAugAssign(self):
    self.assertEqual((0, '42\n'), _GrumpRun(textwrap.dedent("""\
        foo = 41
        foo += 1
        print foo""")))
# ---
def main():
    pass
# ---
def test_dtype_and_axes_annotation():
    def foo(x: f32["batch embed"]):  # type: ignore  # noqa: F722
        pass

    ann = typing.get_args(typing.get_type_hints(foo, include_extras=True)["x"])
    assert ann[0] is NamedArray
    spec = ann[1]
    assert spec.dtype == jnp.float32
    assert spec.before == ("batch", "embed")
# ---
def init(config: MistralConfig, *, key) -> "MixtralTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, MixtralDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return MixtralTransformer(config, layers, ln_f)
# ---
def triu(array: NamedArray, axis1: Axis, axis2: Axis, k=0) -> NamedArray:
    """Compute the upper triangular part of an array along two named axes."""
    array = array.rearrange((..., axis1, axis2))

    inner = jnp.triu(array.array, k=k)
    return NamedArray(inner, array.axes)
# ---
def get_time():
    return test_time
# ---
def test_str_split_bool_index(self):
        def test_impl(df):
            C = df.A.str.split(',')
            return C[df.B == 'aa']

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D'], 'B': ['aa', 'bb']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def get_screens(self):
            """Get the available screens.

            A typical multi-monitor workstation comprises one `Display` with
            multiple `Screen` s.  This method returns a list of screens which
            can be enumerated to select one for full-screen display.

            For the purposes of creating an OpenGL config, the default screen
            will suffice.

            :rtype: list of `Screen`
            """
            raise NotImplementedError('deprecated')
# ---
def get_status_path(output_path: str) -> str:
    """Return the path of the status file associated with `output_path`."""
    return os.path.join(output_path, ".executor_status")
# ---
def get_volumes(self, mounts=False):
        '''return volume mount information '''
        if mounts:
            return self.get(DeploymentConfig.volume_mounts_path) or []

        return self.get(DeploymentConfig.volumes_path) or []
# ---
def __str__(self):
        """
        Imprime el cfdi en el siguiente orden
        emisor, fecha de timbrado, tipo de comprobante, rfc emisor, uuid,_
        receptor, rfc receptor, subtotal, ieps, iva, retiva, retisr, tc, total
        """
        respuesta = '\t'.join( map(str, self.lista_valores))
        return respuesta
# ---
def _run_test_in_subprocess(code: str, test: str, queue: multiprocessing.Queue) -> None:
    """Execute a single test in a subprocess. Must be module-level for pickling."""
    try:
        exec_globals: dict = {}
        exec(code, exec_globals)
        exec(test, exec_globals)
        queue.put(True)
    except Exception:
        queue.put(False)
# ---
def make_tokenizer(tokenizer: str | PreTrainedTokenizer) -> PreTrainedTokenizer:
    if isinstance(tokenizer, str):
        return AutoTokenizer.from_pretrained(tokenizer)
    return tokenizer
# ---
def local_stats(items):
        items_list = list(items)
        if not items_list:
            return {"sum": 0, "count": 0, "min": float("inf"), "max": float("-inf")}
        return {
            "sum": sum(items_list),
            "count": len(items_list),
            "min": min(items_list),
            "max": max(items_list),
        }
# ---
def __init__ (self, canvas, objects):
    self.canvas = canvas
    self.objects = objects
# ---
def test_permutation_with_single_index_returns_correct_value(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    index = 5
    result = permutation(index)
    assert isinstance(result, int)
    assert result != index
# ---
def join_key(prefix, k):
    if k is None:
        return prefix
    return f"{prefix}.{k}" if prefix else k
# ---
def get_job_status(self, job_id: JobName) -> cluster_pb2.JobStatus:
        return self._remote_client.get_job_status(job_id)
# ---
def guides_transduction_GM(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'nucleic_acid_delivery_method': ['transduction'],
        'introduced_elements': 'gRNAs and CRISPR machinery',
        'MOI': 'high',
        'guide_type': 'sgRNA'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def start(self) -> None:
        pass
# ---
def build_pot(srcdir, project_id, sources):
    # Must be relative paths
    sources = [os.path.join('C', source) for source in sources]
    outfile = os.path.join(srcdir, project_id + '.pot')
    subprocess.call(['itstool', '-o', outfile] + sources)
# ---
def intersection(self, bbox, srs):
        bbox = self._geom_in_coverage_srs(bbox, srs)
        return GeomCoverage(self.geom.intersection(bbox), self.srs)
# ---
def __init__(self, c):
        self.c = c
        self.is_star = False
# ---
def make_token_dataset(cache: TreeCache[dict], *, seq_len: int) -> TokenSeqDataset:
    """Thin wrapper so callers don't touch Levanter internals directly."""

    return TokenSeqDataset(cache, seq_len)
# ---
def _unary_op(f):
        @functools.wraps(f)
        def func(self, *args, **kwargs):
            ds = self.coords.to_dataset()
            for k in self.data_vars:
                ds._variables[k] = f(self._variables[k], *args, **kwargs)
            return ds
        return func
# ---
def mean(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.mean(self, axis=axis, dtype=dtype, where=where)
# ---

def prime_length(string):
    """Write a function that takes a string and returns True if the string
    length is a prime number or False otherwise
    Examples
    prime_length('Hello') == True
    prime_length('abcdcba') == True
    prime_length('kittens') == True
    prime_length('orange') == False
    """
    l = len(string)
    if l == 0 or l == 1:
        return False
    for i in range(2, l):
        if l % i == 0:
            return False
    return True
# ---
def _lm_head_from_sink_params(params: dict) -> jax.Array:
    return params["core"].output_proj
# ---
def add(self, x: Arrayish, total: Arrayish) -> "RunningMean":
        delta = x - self.mean
        # careful: total and self.total can be 0
        new_total = self.total + total
        ratio = hax.where(new_total, total / new_total, 0.0)
        new_mean = self.mean + delta * ratio
        new_total = self.total + total
        return RunningMean(new_mean, new_total)
# ---
def latest_weight_id(self) -> int | None:
        """
        Returns the latest weight ID that has been transferred.
        """
        return self._latest_weight_id
# ---
def prefix(self):
        return self._matcher.prefix() and not self._always
# ---
def convert_to_cache(self, value, record, validate=True):
        if value is None or value is False:
            return False
        return ustr(value)[:self.size]
# ---
def __repr__(self):
        return f"WriteOp(type={self.writer_type}, pattern={self.output_pattern})"
# ---
def test_preemptible_false_adds_constraint(self):
        resources = ResourceConfig(preemptible=False)
        constraints = convert_constraints(resources)
        assert len(constraints) == 1
        c = constraints[0]
        assert c.key == "preemptible"
        assert c.value == "false"
# ---
def flops_per_token(self, vocab_size: int, context_length: int):
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def test_poly_int_der():
    C = Axis("C", 4)
    p = hax.named([1.0, 0.0, -2.0, 1.0], (C,))

    d = hax.polyder(p)
    i = hax.polyint(p)

    assert jnp.allclose(d.array, jnp.polyder(p.array))
    assert jnp.allclose(i.array, jnp.polyint(p.array))
    assert d.axes[0] == C.resize(d.array.shape[0])
    assert i.axes[0] == C.resize(i.array.shape[0])
# ---
def _print_accepted(matches, after_match):
            if self.key.ACC in after_match:
                accepted = sorted(
                    set(after_match[self.key.ACC]).difference(
                        set(matches.get(self.key.ACC, []))
                    )
                )
                for key in accepted:
                    print('Key for minion {0} accepted.'.format(key))
# ---
def _get_account_analytic_invoice(self, cr, uid, picking, move_line):
        return False
# ---
def has_torch():
    try:
        import torch  # noqa F401

        return True
    except ImportError:
        return False
# ---
def test_rule_300(self):
        oRule = iteration_scheme.rule_300()
        self.assertTrue(oRule)
        self.assertEqual(oRule.name, 'iteration_scheme')
        self.assertEqual(oRule.identifier, '300')

        lExpected = [13, 17]

        oRule.analyze(self.oFile)
        self.assertEqual(lExpected, utils.extract_violation_lines_from_violation_object(oRule.violations))
# ---
def patkind(pattern, default=None):
    """If pattern is 'kind:pat' with a known kind, return kind."""
    return _patsplit(pattern, default)[0]
# ---
def imag(a: A) -> A:
    return wrap_elemwise_unary(jnp.imag, a)
# ---


def how_many_times(string: str, substring: str) -> int:
    """ Find how many times a given substring can be found in the original string. Count overlaping cases.
    >>> how_many_times('', 'a')
    0
    >>> how_many_times('aaa', 'a')
    3
    >>> how_many_times('aaaa', 'aa')
    3
    """
    times = 0

    for i in range(len(string) - len(substring) + 1):
        if string[i:i+len(substring)] == substring:
            times += 1

    return times
# ---
def ray_cluster():
    from fray.cluster.ray import RayCluster

    if not ray.is_initialized():
        logging.info("Initializing Ray cluster")
        ray.init(
            address="local",
            num_cpus=8,
            ignore_reinit_error=True,
            logging_level="info",
            log_to_driver=True,
            resources={"head_node": 1},
        )
    yield RayCluster()
# ---
def __float__(self) -> float:  # pragma: no cover
        return float(self.array)
# ---
def _path_to_step_name(path: str) -> str:
    """
    Converts a path pointing to a levanter checkpoint into a name we can use as an id for the viz step
    """
    # we want llama-8b-tootsie-phase2-730000
    components = path.split("/")
    step = components[-2].split("-")[-1]
    name = components[-4].split("/")[-1]
    return f"analysis/viz-compare/{name}-{step}"
# ---
def has_active_session():
    """Check if there's an active chainlink session."""
    result = run_chainlink(["session", "status"])
    if result and "Session #" in result and "(started" in result:
        return True
    return False
# ---
def _vm_exists_in_db(self, uuid):
        try:
            models.VM.objects.get_one(filters={'uuid': uuid})
            return True
        except exceptions.RecordNotFound:
            return False
# ---
def __str__(self):
        return f"RunningMean(mean={self.mean}, total={self.total})"
# ---
def vref(self, vr):
        self._Vref = vr
# ---
def md5_for_str(content):
    hash_md5 = hashlib.md5()
    hash_md5.update(content.encode())
    return str(hash_md5.hexdigest())
# ---
def get_choice_logprobs(self, choice):
            return np.full(len(choice.message.content), -1.0, dtype=np.float32)
# ---
def wrap_key(key):
        if prefix:
            return f"{prefix}/{key}"
        return key
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.query_class == other.query_class
# ---
def __init__(self, requested_depth=None):
        self.requested_depth = requested_depth
# ---
def run_chainlink(args):
    """Run a chainlink command and return output."""
    try:
        result = subprocess.run(
            ["chainlink"] + args,
            capture_output=True,
            text=True,
            timeout=5
        )
        return result.stdout.strip() if result.returncode == 0 else None
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        return None
# ---
def is_administrator_role(role: int) -> bool:
    return role in {UserProfile.ROLE_REALM_ADMINISTRATOR, UserProfile.ROLE_REALM_OWNER}
# ---
def registry(self) -> VmRegistry:
        """Access the underlying registry."""
        return self._registry
# ---
def test_worker_sequential_jobs(cluster):
    """Sequential jobs verify reconciliation works across job boundaries.
    Worker state is consistent between tasks."""
    _url, client = cluster
    for i in range(3):
        job = submit(client, _quick, f"seq-{i}")
        status = wait(client, job, timeout=30)
        assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def ndim(a: A) -> A:
    return wrap_elemwise_unary(jnp.ndim, a)
# ---
def _wrap_check(self, _p1, check, _p2):
        """Turn parenthesized expressions into a 'check' token."""

        return [('check', check)]
# ---
def std(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.std, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> float | None:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def zip_logs():
    if platform.system() == "Windows":
        cmd = "Compress-Archive logs logs-$(date +%Y-%m-%d-%H%M).zip"
        subprocess.call(["powershell.exe", cmd])

    elif platform.system() == "Linux" or platform.system() == "Darwin":
        cmd = "zip -vr logs-$(date +%Y-%m-%d-%H%M).zip" + " logs/"
        cr.run_command(cmd)
# ---
def max(x, axis=0):
    return _Nmax(x, axis)
# ---
def set_test_params(self):
        self.num_nodes = 1
        self.setup_clean_chain = True
        self.extra_args = [[]]
# ---
def create(self, run_id) -> Checkpointer:
        keeps = [CheckpointInterval(**k) for k in self.keep]
        return Checkpointer(
            base_path=self.expanded_path(run_id),
            save_interval=self.save_interval,
            step_policies=keeps,
            delete_old_temp_checkpoints=self.delete_old_temp_checkpoints,
        )
# ---
def __init__(self, mapping: dict[str, str]):
        self.mapping = mapping
# ---
def test_pass_different_length_seq(num_kv_heads):
    config = GemmaConfig(
        max_seq_len=64,
        hidden_dim=64,
        intermediate_dim=32,
        num_heads=2,
        num_kv_heads=num_kv_heads,
        use_flash_attention=True,
        head_dim=4,
    )
    check_model_works_with_seqlen(GemmaLMHeadModel, config, 16)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.pct_change(1)
            return Ac.sum()
# ---
def block_quote(self, text):
        """Rendering <blockquote> with the given text.

        :param text: text content of the blockquote.
        """
        return '<blockquote>%s\n</blockquote>\n' % text.rstrip('\n')
# ---
def _get_fn_name(fn: Any) -> str:
    """Safely get a function name, handling partials and callables."""
    # Unwrap partials to find the underlying function name
    while hasattr(fn, "func"):
        fn = fn.func
    return getattr(fn, "__qualname__", getattr(fn, "__name__", str(fn)))
# ---
def __create_bootstrap(self, main_root):
		bootstrap_dir = os.path.join(main_root, 'bootstrap')
		safe_mkdir(bootstrap_dir)

		bootstrap = self.__config_mgr.bootstrap()
		self.__bootstrap_mgr.create_bootstrap(bootstrap, bootstrap_dir)
# ---
def GetJobStatus(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def timeout_config() -> config_pb2.TimeoutConfig:
    """Timeout configuration for tests."""
    from iris.time_utils import Duration

    timeout_cfg = config_pb2.TimeoutConfig()
    timeout_cfg.boot_timeout.CopyFrom(Duration.from_seconds(5).to_proto())
    timeout_cfg.init_timeout.CopyFrom(Duration.from_seconds(10).to_proto())
    timeout_cfg.ssh_poll_interval.CopyFrom(Duration.from_seconds(1).to_proto())
    return timeout_cfg
# ---
def test_shift2(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.pct_change(1)
            return Ac.sum()

        hpat_func = self.jit(test_impl)
        n = 11
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def load_jupyter_server_extension(nb_app):
    web_app = nb_app.web_app
    base_url = web_app.settings['base_url']

    route_pattern = url_path_join(base_url, '/saagie')
    web_app.add_handlers('.*$', [(route_pattern, SaagieHandler)])

    route_pattern = url_path_join(base_url, '/saagie/check')
    web_app.add_handlers('.*$', [(route_pattern, SaagieCheckHandler)])
# ---
def test_check_health_passes_duration_to_run():
    """check_health passes Duration to conn.run without TypeError."""
    conn = FakeSshConnection()
    result = check_health(conn, port=10001)
    assert result.healthy is True
    assert conn.last_timeout == Duration.from_seconds(10)
# ---
def remove_authors(html: BeautifulSoup):
    # Remove authors since we only care about information after first section
    authors = html.findAll("div", {"class": "ltx_authors"})
    for author in authors:
        author.decompose()
        section = author.previous_sibling
        while section:
            new_section = section.previous_sibling
            section.decompose()
            section = new_section
    return html
# ---
def add_external_ips(self, inc_external_ips):
        ''' add an external_ip to the external_ips list '''
        if not isinstance(inc_external_ips, list):
            inc_external_ips = [inc_external_ips]

        external_ips = self.get_external_ips()
        if not external_ips:
            self.put(Service.external_ips, inc_external_ips)
        else:
            external_ips.extend(inc_external_ips)

        return True
# ---
def failing_fn():
    print("Executing failing_fn. This should fail.")
    raise deliberately_raised_exception
# ---
def test_log_summary(monkeypatch):
    monkeypatch.setenv("HF_HUB_OFFLINE", "1")
    run = trackio.init(project="test-log-summary")
    tracker = TrackioTracker(run)
    tracker.log_summary({"float": 2.0})
    tracker.log_summary({"str": "test"})
    tracker.log_summary({"scalar_jax_array": jnp.array(3.0)})
    tracker.log_summary({"scalar_np_array": np.array(3.0)})
    trackio.finish()
# ---
def vmap_fun(x, y):
        return x.sum(Width) if y else x
# ---
def scan_fun(acc, z, x):
        out = acc + z * x, x * z
        return out
# ---
def build(self, Vocab: Axis, *, key: PRNGKeyArray) -> "LmT":
        return self.model_type.init(Vocab, self, key=key)
# ---
def __init__(self):
                self.finish_revert_migration_called = False
# ---
def HiddenEnabled( buffer_object ):
  return bool( int( GetBufferOption( buffer_object, 'hid' ) ) )
# ---
def _to_list(arr) -> list:
    """Convert array-like object to list, handling both JAX arrays and Python lists."""
    if isinstance(arr, list):
        return arr
    elif hasattr(arr, "tolist"):
        return arr.tolist()
    else:
        # Fallback for other array types
        return list(arr)
# ---
def route(method, path):
    def decorator(f):
        f.http_route = path
        f.http_method = method
        return f
    return decorator
# ---
def test_transaction_engine_fn_rollback(self):
        fn = self._trans_rollback_fn()
        assert_raises_message(
            Exception,
            "breakage",
            testing.db.transaction, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def stop(self) -> None:
        """Terminate controller GCE VM."""
        self._delete_vm()
# ---
def target_space_id(self):
    raise NotImplementedError()
# ---
def load_tokenizer_cmd(model_name: str):
    """Load a tokenizer and start REPL."""
    if load_tokenizer(model_name):
        click.Context(cli).invoke(repl_cmd)
# ---
def remainder(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.remainder(x1, x2)
# ---
def getMisMatchesQP(data, clf):
    #print(data)
    data_x = np.c_[data[:,0], data[:,1]]
    results = clf.predict(data_x)
    #print(np.sign(results))
    print("mismatch ", float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data))
    print("score ", clf.score(data_x, data[:,2]))

    return float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data)
# ---
def exists(results, _name):
        ''' Check to see if the results include the name '''
        if not results:
            return False

        if Utils.find_result(results, _name):
            return True

        return False
# ---
def create_colors_list():
    colors_list = []
    for color in plt.cm.tab10(np.linspace(0, 1, 10))[:-1]:
        colors_list.append(tuple(color))
    colors_list.append("black")
    for color in plt.cm.Set2(np.linspace(0, 1, 8)):
        colors_list.append(tuple(color))
    for color in plt.cm.Set3(np.linspace(0, 1, 12)):
        colors_list.append(tuple(color))
    return colors_list
# ---
def device_is_name(self):
        if not self.device_is_address and not self.device_is_id:
            return True
        return False
# ---
def create_bad_vbd(vm_ref, vdi_ref):
            vbd_rec = {'VM': vm_ref,
               'VDI': vdi_ref,
               'userdevice': 'fake',
               'currently_attached': False}
            vbd_ref = xenapi_fake._create_object('VBD', vbd_rec)
            xenapi_fake.after_VBD_create(vbd_ref, vbd_rec)
            return vbd_ref
# ---
def nanmin(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.nanmin, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def test_named_ref_is_pytree_leaf():
    X = hax.Axis("x", 3)
    ref = hax.new_ref(hax.zeros(X))

    leaves = jax.tree_util.tree_leaves(ref)
    assert len(leaves) == 1
    assert leaves[0] is ref._ref

    structure = jax.tree_util.tree_structure(ref)
    rebuilt = jax.tree_util.tree_unflatten(structure, leaves)

    assert isinstance(rebuilt, hax.NamedRef)
    assert rebuilt.axes == ref.axes
    assert rebuilt._prefix == ref._prefix
# ---
def l2(*arg, **kw):
            canary.append("l2")
# ---
def SetVariableValue( variable, value ):
  vim.command( "let {0} = {1}".format( variable, json.dumps( value ) ) )
# ---
def sub_add(self, filename):
        self.command('sub_add', filename.encode(fs_enc))
# ---
def cleanTransformation( self, transID, rpc = '', url = '', timeout = None ):
    """ Clean the transformation, and set the status parameter (doing it here, for easier extensibility)
    """
    # Cleaning
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    res = rpcClient.cleanTransformation( transID )
    if not res['OK']:
      return res
    # Setting the status
    return self.setTransformationParameter( transID, 'Status', 'TransformationCleaned' )
# ---
def assign(dest):
                page_idx = pos_id // self.page_size
                page_offset = pos_id % self.page_size
                page = page_indices["seq", seq_id, "page", page_idx]
                dest_value = hax.where(is_invalid(page), INVALID, page * self.page_size + page_offset)
                return dest.at["position", i].set(dest_value)
# ---
def float_power(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.float_power](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.float_power.html)
    """
    return jnp.float_power(x1, x2)
# ---
def __rfloordiv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.floor_divide(other, self)
# ---
def _serialize_and_commit(self, cache_dir):
        path = os.path.join(cache_dir, LEDGER_FILE_NAME)
        return _serialize_json_and_commit(path, self)
# ---
def _coerce_integer_like_index(value: Any):
        if isinstance(value, np.integer):
            return int(value)
        if type(value) is np.ndarray and value.shape == () and jnp.issubdtype(value.dtype, jnp.integer):
            return int(value.item())
        return value
# ---
def ensure_tuple(x: Sequence[T] | T) -> tuple[T, ...]:
    if isinstance(x, str):
        return (x,)  # type: ignore
    elif isinstance(x, Sequence):
        return tuple(x)
    return (x,)
# ---
def _str_is_int(x: str) -> bool:
    try:
        x = _strip_properly_formatted_commas(x)
        x = float(x)
        return abs(x - round(x)) <= 1e-7
    except BaseException:
        return False
# ---
def test_impl(n):
            S1 = pd.Series(np.ones(n))
            S2 = pd.Series(np.random.ranf(n))
            df = pd.DataFrame({'A': S1, 'B': S2})
            return df.A.sum()
# ---
def count(self, **kwargs):
        """Get hits count."""
        return int(self.hits['hits']['total'])
# ---
def fold_step(block: FoldModule, carry: hax.NamedArray) -> hax.NamedArray:
        return block(carry)
# ---
def get_output_shape(input_shape):
      return 1 * input_shape
# ---
def subscribe(ch, method, properties, body):
        """
        prints the body message. It's the default callback method
        :param ch: keep null
        :param method: keep null
        :param properties: keep null
        :param body: the message
        :return:
        """
# ---
def __repr__(self):
        return "<recursivematcher %r>" % self._matcher
# ---
def _diag_mask(Ci: Axis, Cj: Axis) -> NamedArray:
    ii = hax.arange(Ci)
    jj = hax.arange(Cj)
    I = ii.broadcast_axis(Cj)
    J = jj.broadcast_axis(Ci)
    return cast(NamedArray, I == J)
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        return 1 / math.sqrt(hax.axis_size(In))
# ---
def make_author():
    return Author(
        id=fake.random_int(),
        first_name=fake.first_name(),
        last_name=fake.last_name(),
        twitter=fake.domain_word(),
    )
# ---
def BufferIsVisible( buffer_number ):
  if buffer_number < 0:
    return False
  window_number = GetIntValue( "bufwinnr({0})".format( buffer_number ) )
  return window_number != -1
# ---
def get_ls_l_desc_fields():
    return {
        'id': True,
        'class': True,
        'folder': True,
        'length': True,
        'modified': True,
        'name': True,
        'project': True,
        'size': True,
        'state': True
    }
# ---
def dec(rc):
                page = seq_pages["page", i].scalar()
                return rc.at["page", page].add(-1)
# ---
def __init__(self, seq_len):
        super().__init__()
        self.seq_len = seq_len
        self.begin = 0
        self.end = 256
        self.stride = 1
# ---
def test_impl(df):
            A = df.A.str.split(',')
            B = pd.Series(list(itertools.chain(*A)))
            return B
# ---
def output_footnote(self, m):
        key = _keyify(m.group(1))
        if key not in self.footnotes:
            return None
        if self.footnotes[key]:
            return None
        self.footnote_index += 1
        self.footnotes[key] = self.footnote_index
        return self.renderer.footnote_ref(key, self.footnote_index)
# ---
def reg_nick_write(self, nick, id):
        if(nick.lower() in self.data.keys()):
            self.data[nick.lower()]['id'] = int(id)
        else:
            self.data[nick.lower()] = {'id': int(id), 'nick': nick, 'level': 1, 'strength': 0, 'rank_points': 0, 'citizenship': ''}

        self.update_data(nick.lower())
# ---
def encodes(text):

    bext = text.encode(encoding="utf-8")
    enc_bext = codecs.encode(bext, "hex_codec")

    return enc_bext.decode("utf-8")
# ---
def select(x):
        if isinstance(x, NamedArray):
            return x["seq", idx]
        elif is_jax_array_like(x):
            return x[idx]
        else:
            raise TypeError(f"Unexpected type in seq_params: {type(x)}")
# ---
def __initialize_manager(self):
		self.__config_mgr	= self.__manager.load_config_manager()

		self.__backend_mgr   = self.__manager.load_backend_manager()
		self.__bootstrap_mgr = self.__manager.load_bootstrap_manager()
		self.__navigator_mgr = self.__manager.load_navigator_manager()
		self.__context_mgr  = self.__manager.load_context_manager()
		self.__action_mgr	= self.__manager.load_action_manager()
		self.__launcher_mgr  = self.__manager.load_launcher_manager()
# ---
def test_is_stop_signal_exact_match():
    # tail_tokens matches stop_sequence exactly
    tail_tokens = hax.named(jnp.array([5, 6, 7], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[5, 6, 7]], dtype=jnp.int32), axis=("seq", "position"))
    assert is_stop_signal(tail_tokens, stop_sequences)
# ---
def clause(self):
        return '0', ()
# ---
def __copy__(self):
        return self.copy(deep=False)
# ---
def descobreAtaqueUsado(self, atksXML, pkmn):
		for i in range(0, len(atksXML)):
			id = int(atksXML[i].find('id').text) - 1
			ppXML = int(atksXML[i].find('power_points').text)
			pp = pkmn.getAtks(id).getPpAtual()

			if (pp != ppXML):
				pkmn.getAtks(id).decreasePp()
				return id

		return id
# ---
def test_ar_loss_zero_mask_gives_zero_loss(params, tiny_cfg):
    batch_size, seq_len = 2, 16
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (batch_size, seq_len), 1, 100)
    loss_mask = jnp.zeros((batch_size, seq_len))

    loss, _ = ar_loss(params, token_ids, loss_mask, tiny_cfg)
    assert float(loss) == 0.0
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        self.metrics["hparams"] = hparams
# ---
def layer(x_arg, weights_arg, bias_arg):
            return with_sharding_constraint(jnp.dot(x_arg, weights_arg) + bias_arg, P())
# ---
def storage_config(request, tmp_path):
    """Parametrized fixture for different storage types."""
    if request.param == "memory":
        return RolloutStorageConfig(storage_type=StorageType.IN_MEMORY, queue_name="test_queue")
    else:
        return RolloutStorageConfig(storage_type=StorageType.FILE, path=str(tmp_path / "rollout_storage"))
# ---
def retrying_create_commit(*args, **kwargs):
    return create_commit(*args, **kwargs)
# ---
def _f32(x):
            return np.asarray(x, dtype=np.float32)
# ---
def update_po(srcdir, project_id, langs):
    potfile = os.path.join(srcdir, project_id + '.pot')
    for lang in langs:
        pofile = os.path.join(srcdir, lang, lang + '.po')
        subprocess.call(['msgmerge', '-q', '-o', pofile, pofile, potfile])
# ---
def init(config: AttentionConfig, *, key) -> "AttentionWithSink":
        base = Attention.init(config, key=key)
        sinks = hax.zeros((config.KVHeads, config.QHeadsPerGroup), dtype=jnp.float32)
        return AttentionWithSink(
            base.config,
            base.q_proj,
            base.k_proj,
            base.v_proj,
            base.o_proj,
            base.q_norm,
            base.k_norm,
            base.rot_embs,
            sinks,
        )
# ---
def JumpToPreviousWindow():
  """ Jump the vim cursor to its previous window position """
  vim.command( 'silent! wincmd p' )
# ---
def __init__(self, notebook, platform_data):
        self.notebook = notebook
        self.id = platform_data['id']
        self.name = platform_data['name']
        self.capsule_types = {c['code'] for c in platform_data['capsules']}
# ---
def maybe_chunk(name, var, chunks):
            chunks = selkeys(chunks, var.dims)
            if not chunks:
                chunks = None
            if var.ndim > 0:
                return var.chunk(chunks, name=name, lock=lock)
            else:
                return var
# ---
def __call__(self, carry: hax.NamedArray) -> tuple[hax.NamedArray, hax.NamedArray]:
            updated = carry + self.weight
            return updated, updated
# ---
def output_loose_item(self):
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'list_item_end':
            body += self.tok()
        return self.renderer.list_item(body)
# ---
def _getPitchForVoiceType(self, voiceType):
        """Gets the pitch value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM

        Returns the pitch value for the given voice type, or None if
        not set.
        """

        return self._getKeyValueForVoiceType(voiceType,
                                             acss.ACSS.AVERAGE_PITCH)
# ---
def in_qdq_bwd(compute_dtype, res, g):
    new_scale, new_history = res
    q_g = g
    return q_g, new_scale, new_history
# ---
def string_match(cls, pattern, value):
        return pattern.lower() == value.lower()
# ---
def fail_once_jax_fn() -> None:
        # Check if we should fail BEFORE initializing JAX/TPU
        # This avoids claiming the TPU on the first attempt
        count = ray.get(counter_actor.count.remote())
        ray.get(counter_actor.increment.remote())
        if count == 0:
            raise DeliberatelyRaisedException(f"Failing deliberately because count is {count}")

        # Only do JAX work after we know we won't fail
        result = simple_jax_fn()
        return result
# ---
def from_iterable(iterable: Iterable[T]) -> Dataset[T]:
        """Create a dataset from any iterable."""
        return Dataset(iterable)
# ---
def setUp(self):
        super(BaseSystemTest, self).setUp()
        # Clear out any pre-existing tables
        for tablename in self.dynamo.list_tables():
            self.dynamo.delete_table(tablename)
# ---
def asdict_optional(obj):
            return asdict(obj) if obj else None
# ---
def output_text(self):
        return self.renderer.paragraph(self.tok_text())
# ---
def exponential(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.exponential(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def __returnOK( self, lfn ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    successful = {}
    for lfn in res['Value'].keys():
      successful[lfn] = True
    resDict = {'Successful':successful, 'Failed':{}}
    return S_OK( resDict )
# ---
def test_wrong_nodes_prev_cc_raises(self):
        with pytest.raises(nx.NetworkXError):
            G = self.undirected_G.copy()
            edge = self.pick_add_edge(G)
            insert = True
            prev_cc = self.undirected_G_cc.copy()
            num_nodes = len(prev_cc)
            prev_cc.pop(0)
            prev_cc[num_nodes] = 0.5
            nx.incremental_closeness_centrality(G, edge, prev_cc, insert)
# ---
def segComplete(self, seg):
        if ( seg == None ): return

        if ( seg.data ):
            data_size = len("".join(seg.data))

            current_time = time.time()
            if ( (current_time - self.speedTime) > 1 ):
                self.status.kbps = self.speedCounter
                self.speedCounter = 0
                self.speedTime = current_time
            else:
                self.speedCounter += (data_size/1024)

            self.cache.append(seg)
# ---
def highlight_value(key, value):
        if key in highlight_fields:
            return YELLOW() + value + ENDC()
        else:
            return value
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[True] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[True] = ...,
) -> tuple[NamedArray, Array, Array, Array, Array]: ...
# ---
def test_passing_non_parseable_status_throws_exception(self):
        with pytest.raises(InvalidStatusException):
            Line(line_contents=get_updated_line_contents({'status': 'foobar'}))
# ---
def copy_template(self, template, dest, mode=0o644):
        "Copy template to dest in workflowdir with mode"
        path = os.path.join(self.workflowdir, dest)
        t = self.jinja.get_template(template)
        t.stream(**self.__dict__).dump(path)
        os.chmod(path, mode)
# ---
def test_blake3_vector():
    # Catch un-intentional regressions
    assert bytes(hash_blake3(b"hello")).hex() == "ea8f163db38682925e4491c5e58d4bb3506ef8c14eb78a86e908c5624a67200f"
# ---
def test_nested_field_with_comparison(self):
        expr = col("meta")["score"] > 0.5
        assert expr.evaluate({"meta": {"score": 0.8}}) is True
        assert expr.evaluate({"meta": {"score": 0.2}}) is False
# ---
def test_conf_from_extra_conf_bad_max_conn(self):
        with mock.patch.object(memcache, 'ConfigParser', get_config_parser(
                memcache_max_connections='bad42')):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '1.2.3.4:5')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, True)
        self.assertEqual(
            app.memcache._client_cache['1.2.3.4:5'].max_size, 2)
# ---
def _search_related(self, records, operator, value):
        """ Determine the domain to search on field ``self``. """
        return [('.'.join(self.related), operator, value)]
# ---
def init(*, key):
            k1, k2 = jax.random.split(key)
            first = hnn.Linear.init(In, Mid, key=k1, out_first=True)
            second = hnn.Linear.init(Mid, In, key=k2, out_first=True)
            return Module(first, second)
# ---
def matchfn(self, f):
        return self._matcher.matches(f)
# ---
def __call__(self, text):
        return self.parse(text)
# ---
def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.host = 'localhost'
        self.port = 8000
# ---
def datasets():
    ds1 = ListAsyncDataset([1, 2, 3, 4, 5])
    ds2 = ListAsyncDataset([10, 20, 30, 40, 50])
    ds3 = ListAsyncDataset([100, 200, 300, 400, 500])
    ds1.finalize()
    ds2.finalize()
    ds3.finalize()
    return {"ds1": ds1, "ds2": ds2, "ds3": ds3}
# ---
def _update_file(path: Path, *, dry_run: bool) -> bool:
    with path.open() as f:
        data = json.load(f)

    changed = False
    for entry in data.get("runs", []):
        run_info = entry.get("run_info")
        if isinstance(run_info, dict) and _correct_run_info(run_info):
            changed = True

    if changed and not dry_run:
        with path.open("w") as f:
            json.dump(data, f, indent=2, sort_keys=True)

    return changed
# ---
def test_conf_from_extra_conf(self):
        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '1.2.3.4:5')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, True)
        self.assertEqual(
            app.memcache._client_cache['1.2.3.4:5'].max_size, 4)
# ---
def testFunctionDef(self):
    self.assertEqual((0, 'bar baz\n'), _GrumpRun(textwrap.dedent("""\
        def foo(a, b):
          print a, b
        foo('bar', 'baz')""")))
# ---
def initialize(self, io_loop=None, num_threads=10):
        from concurrent.futures import ThreadPoolExecutor
        super(ThreadedResolver, self).initialize(
            io_loop=io_loop, executor=ThreadPoolExecutor(num_threads))
# ---
def compute_svm_subjects(K, y, n_folds=5):
    """
    """
    cv = KFold(len(K)/2, n_folds)
    scores = np.zeros(n_folds)
    for i, (train, test) in enumerate(cv):
        train_ids = np.concatenate((train, len(K)/2+train))
        test_ids = np.concatenate((test, len(K)/2+test))
        clf = SVC(kernel='precomputed')
        clf.fit(K[train_ids, :][:, train_ids], y[train_ids])
        scores[i] = clf.score(K[test_ids, :][:, train_ids], y[test_ids])

    return scores.mean()
# ---
def __repr__(self):
        return self.node + "\nREF: " + str(self.references) + "\nChildren: " + str(self.children.keys()) + "\n"
# ---
def init(cls, Vocab: Axis, config: Gpt2HyenaConfig, *, key) -> "Gpt2HyenaModel":
        k_t, k_embeddings = jrandom.split(key, 2)
        backbone = Gpt2HyenaBackbone.init(config, key=k_t)
        embeddings = Gpt2Embeddings.init(
            Vocab,
            # Our config type has everything it needs, but is not a subclass of Gpt2Config
            config,  # type: ignore
            key=k_embeddings,
        )

        return Gpt2HyenaModel(backbone, embeddings)
# ---
def test_can_pack_simple_case():
    Pos = hax.Axis("pos", size=10)
    packer = SequencePacker(Pos=Pos, max_pack_size=2, pad_token=0)

    assert packer.can_pack([1, 2, 3]) is True
    packer.add_example(ids=[1, 2, 3], loss_weight=[1, 1, 1])
    assert packer.can_pack([4, 5]) is True
    assert packer.can_pack(list(range(6, 16))) is False
# ---
def format_fans(fans):
    return format_line(prefix='fans'.rjust(RJUST), values=fans)
# ---
def get_prep_value(self, value):
		return ','.join(value)
# ---
def enterLonely(self):
        self.lonely.enter()
        self.acceptOnce(self.lonelyDoneEvent, self.__decideNextState)
# ---
def __repr__(self):
        return "<{0}: {1} {2}>".format(
            self.__class__.__name__, self.method, self.path
        )
# ---
def test_impl(n):
            df = pd.DataFrame({'A': ['aa', 'bb', 'aa', 'cc', 'cc']})
            return df.A.nunique()
# ---
def put(self, obj: Any) -> Any:
        """Identity operation - no serialization needed."""
        return obj
# ---
def from_spec(cls, query_params: dict[str, list[str]]) -> "RayCluster":
        namespace = None
        config_path = None
        address = "auto"

        if "namespace" in query_params:
            namespace = query_params["namespace"][0]

        if "cluster" in query_params:
            config_path = find_config_by_region(query_params["cluster"][0])

        return cls(address=address, config_path=config_path, namespace=namespace)
# ---
def named_jit(
    fn: Callable[Args, R],
    axis_resources: ResourceMapping | None = None,
    *,
    in_axis_resources: ResourceMapping | None = None,
    out_axis_resources: ResourceMapping | None = None,
    donate_args: PyTree | None = None,
    donate_kwargs: PyTree | None = None,
    # args from jit
    keep_unused: bool = False,
    backend: str | None = None,
    inline: bool | None = None,
) -> WrappedCallable[Args, R]: ...
# ---
def add_words(self, words, update=True):
        """Add a word or words to the list of words to auto-complete."""
        for word in words:
            if self.isusable(word):
                self._word_freq[word] += 1
        if update:
            self._update_word_list()
# ---
def __init__(self, methods):
        self._define_methods(methods)
# ---
def show_savecfg_dlg(self):
        filename, _ = QFileDialog.getSaveFileName(
            self, self.tr("Save configuration file..."),
            directory=os.path.expanduser("~"),
            filter="Json file (*.json)"
        )

        if filename:
            self.filename = filename
            self.save_file()
# ---
def test_impl(df):
            B = df.A.str.split(',')
            C = pd.to_numeric(B.str.get(1), errors='coerce')
            return C
# ---
def testReturn(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          return 'bar'
        print foo()""")))
# ---
def __repr__(self):
        return "Average SAS model with %i atoms"%len(self.atoms)
# ---
def __call__(self, x: NamedArray, mask: Optional[AttentionMask | NamedArray], layer_idx, *, key):
        k1, k2, k3, k4 = haliax.jax_utils.maybe_rng_split(key, 4)

        attn_output = self.attn(self.ln_1(x), mask=mask, layer_idx=layer_idx, key=k1)
        attn_output = self.resid_dropout(attn_output, key=k2)
        x = x + attn_output

        ff_output = self.mlp(self.ln_2(x), key=k3)
        ff_output = self.resid_dropout(ff_output, key=k4)
        x = x + ff_output

        return x
# ---
def test_uses_defaults_when_cluster_ssh_config_empty(self):
        """get_ssh_config uses built-in defaults when cluster config empty."""
        from iris.time_utils import Duration

        config = config_pb2.IrisClusterConfig()

        ssh_config = get_ssh_config(config)

        assert ssh_config.user == "root"
        assert ssh_config.key_file is None
        assert ssh_config.port == 22
        assert ssh_config.connect_timeout == Duration.from_seconds(30)
# ---
def HR_knockout(lab, award, target):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'knockout',
        'purpose': 'repression',
        'method': 'homologous recombination',
        'modified_site_by_target_id': target['@id']
    }
# ---
def failing_job():
            raise ValueError("intentional failure")
# ---
def format_ports(ports):
    return format_line(prefix='ports'.rjust(RJUST), values=ports)
# ---
def stack_converter(st):
        return [lookup[element] for element in st[::-1]]
# ---
def create_contamination_column(pd_tool_bins):
    pd_tool_bins['newcolumn'] = 1 - pd_tool_bins['precision_bp']
# ---
def load_dt_model(pickle_model):
    """
    Retrieve model using Pickle binary format.

    :param string pickle_model: location of Pickle model
    :return: Pickle model for re-use
    :rtype: object
    """
    return pickle.loads(pickle_model)
# ---
def ongoing_process():
        while True:
            for item in range(1, 101):
                yield item
# ---
def update_element():
    """Updates single element with all new values received from the user application"""
    received_element = request.get_json()
    home_services.update_element(received_element)
    return 'OK'
# ---
def job_context(request, ray_cluster):
    if request.param == "sync":
        return SyncContext()
    elif request.param == "threadpool":
        return ThreadContext(max_workers=2)

    return RayContext()
# ---
def or_(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def _thrift_cmd(self):
    cmd = [self._thrift_binary]
    thrift_import = 'thrift_import={}'.format(self.get_options().thrift_import)
    gen_options = self.get_options().gen_options
    if gen_options:
      gen_options += ',' + thrift_import
    else:
      gen_options = thrift_import
    cmd.extend(('--gen', 'go:{}'.format(gen_options)))

    if self.get_options().strict:
      cmd.append('-strict')
    if self.get_options().level == 'debug':
      cmd.append('-verbose')
    return cmd
# ---
def parse_hrule(self, m):
        self.tokens.append({'type': 'hrule'})
# ---
def get_value_max(self):
        """Return the maximum value possible for an ADC read"""
        return 2 ** self._BitLength - 1
# ---
def is_overwrite_with_gradient(v):
        return isinstance(v, OverwriteWithGradient)
# ---
def delete(self, rsc):
        return rsc.delete()
# ---
def scale_group_config() -> config_pb2.ScaleGroupConfig:
    """A standard scale group configuration for tests."""
    return config_pb2.ScaleGroupConfig(
        name="test-group",
        min_slices=1,
        max_slices=5,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def test_deploy(self, cav, ue):
        s3 = boto.connect_s3()
        s3.create_bucket("laterpay-rubberjack-ebdeploy")  # FIXME Remove hardcoded bucket name

        with tempfile.NamedTemporaryFile() as tmp:
            result = CliRunner().invoke(rubberjack, ['deploy', tmp.name], catch_exceptions=False)

            self.assertEquals(result.exit_code, 0, result.output)
# ---
def main():
    try:
        conf = config.get_config_object()
        paste_file = conf.find_file(conf.paste_deploy.config_file)
        wsgi_app = os_pastedeploy.paste_deploy_app(paste_file,
                                                   'staccato-api',
                                                   conf)
        server = os_wsgi.Service(wsgi_app, conf.bind_port)
        server.start()
        server.wait()
    except RuntimeError as e:
        fail(1, e)
# ---
def target_fn(queue, args, kwargs):
        try:
            # Call the original function
            result = underlying_function(*args, **kwargs)
            queue.put((True, result))  # Success, put the result
        except Exception as e:
            # Capture and return the full traceback in case of an exception
            exc_info = sys.exc_info()
            exception_info = ExceptionInfo(ex=e, tb=tblib.Traceback(exc_info[2]))
            queue.put((False, exception_info))
# ---
def create_process(self, process_id, vpnservice, namespace):
        return strongswan_ipsec.StrongSwanProcess(
            self.conf,
            process_id,
            vpnservice,
            namespace)
# ---
def test_get_host(self):
        self.assertEqual("www.python.org", self.get.get_host())
# ---
def gcd(a, b):
    while b != 0:
        (a, b) = (b, a % b)
    return a
# ---
def __init__(self, cache_dir: Path, max_bundles: int = 100):
        self._cache_dir = cache_dir
        self._bundles_dir = cache_dir / "bundles"
        self._extracts_dir = cache_dir / "extracts"
        self._max_bundles = max_bundles
        self._extract_locks: dict[str, threading.Lock] = defaultdict(threading.Lock)

        self._bundles_dir.mkdir(parents=True, exist_ok=True)
        self._extracts_dir.mkdir(parents=True, exist_ok=True)
# ---
def test_encode_decode_source_roundtrip(tok):
    source = "x = 1 + 2\n"
    ids = tok.encode_source(source)
    decoded = tok.decode_source(ids)
    assert decoded == source
# ---
def validate_cancelled_item(item_code, docstatus=None, verbose=1):
	if docstatus is None:
		docstatus = frappe.db.get_value("Item", item_code, "docstatus")

	if docstatus == 2:
		msg = _("Item {0} is cancelled").format(item_code)
		_msgprint(msg, verbose)
# ---
def _make_einsum_spec(arrays, out_axes):
    name_mappings_for_einsum: dict[str, str] = {}
    used_letters: set[str] = set()
    spec = ""
    for operand in arrays:
        if len(spec):
            spec += ","
        for axis in operand.axes:
            letter = _assign_letter_to_name(axis.name, name_mappings_for_einsum, used_letters)
            spec += letter
    spec += "->"
    for out in out_axes:
        letter = name_mappings_for_einsum[axis_name(out)]
        spec += letter
    return spec
# ---
def validate_name_with_item_group(self):
		# causes problem with tree build
		if frappe.db.exists("Item Group", self.name):
			frappe.throw(
				_("An Item Group exists with same name, please change the item name or rename the item group"))
# ---
def dispatcher(self):
        return event.EventDispatcher()
# ---
def resolve_all_mro(cls, name, reverse=False):
    """ Return the (successively overridden) values of attribute ``name`` in ``cls``
        in mro order, or inverse mro order if ``reverse`` is true.
    """
    klasses = reversed(cls.__mro__) if reverse else cls.__mro__
    for klass in klasses:
        if name in klass.__dict__:
            yield klass.__dict__[name]
# ---
def validate_retain_sample(self):
		if self.retain_sample and not frappe.db.get_single_value('Stock Settings', 'sample_retention_warehouse'):
			frappe.throw(_("Please select Sample Retention Warehouse in Stock Settings first"))
		if self.retain_sample and not self.has_batch_no:
			frappe.throw(_(" {0} Retain Sample is based on batch, please check Has Batch No to retain sample of item").format(
				self.item_code))
# ---
def add_hook(self, fn: Callable[[StepInfo], Any], *, every: int = 1): ...
# ---
def finish(self):
        """
        Finish the tracker. This is called when the tracker is no longer needed. This can, e.g.,
        force a commit of all metrics.
        """
        pass
# ---
def _out_first(self):
        """
        Returns: bool: Whether the output axes are first in the weight matrix
        """
        # We do it this way because of scan layers
        if isinstance(self.Out, hax.Axis):
            return self.weight.axes[-1] != self.Out
        else:
            return self.weight.axes[-len(self.Out) :] != self.Out
# ---
def test_add(self):
        expr = col("a") + col("b")
        assert expr.evaluate({"a": 10, "b": 5}) == 15
# ---
def parameter_axis_mapping(self) -> ResourceMapping:
        return self.config.parameter_axis_mapping
# ---
def train_worker_task():
            with remove_tpu_lockfile_on_exit():
                logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", force=True)
                worker = TrainWorker(config=train_worker_config)
                worker.train()
# ---
def clause(self):
        return self.clause_with_joiner('and')
# ---
def get_block_size_limit(self, block: Block):
        with self.lock:
            return self._state.get_block_size_limit(block)
# ---
def multiply(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.multiply(x1, x2)
# ---
def test_run_streaming_with_retry_calls_on_line_callback():
    """run_streaming_with_retry calls on_line callback for each line."""
    expected_lines = ["line one", "line two", "line three"]
    conn = MagicMock()
    conn.run_streaming.return_value = make_fake_popen(expected_lines)
    lines_received: list[str] = []
    run_streaming_with_retry(conn, "bootstrap script", on_line=lines_received.append)
    assert lines_received == expected_lines
# ---
def kebab_to_pascal(name: str) -> str:
    """Convert kebab-case to PascalCase."""
    return "".join(word.capitalize() for word in name.split("-"))
# ---
def test_permutation_is_deterministic(PermutationClass):
    length = 4
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    indices = np.arange(length, dtype=np.uint64)
    results = permutation(indices)
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    results2 = permutation(indices)
    assert jnp.all(results == results2)
# ---
def BatchCreateSessions(self, request, context):
        """Creates multiple new sessions.

    This API can be used to initialize a session cache on the clients.
    See https://goo.gl/TgSFN2 for best practices on session cache management.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def processTeamQueue():
	"""
	This should be called periodically to check and schedule jobs pending for the
	particular team
	"""
	pass
# ---
def test_pjit_class_init():
    with axis_mapping(resource_map):
        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 2), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModuleInit)()

        assert mod.named.array.shape == (Dim2.size, Dim3.size)

        assert mod.unnamed1.shape == ()
        assert mod.named2.array.shape == (Dim3.size,)
# ---
def chip_count(self) -> int:
        return 0
# ---
def test_non_callable(self):
        with pytest.raises(TypeError) as excinfo:
            event.event(123)
        excinfo.match(r"Expected string, callable or None as first argument")

        with pytest.raises(TypeError) as excinfo:
            event.event("name")([])
        excinfo.match(r"Callable must be a function \(`def`\)"
                      r" or coroutine function \(`async def`\)")
# ---
def parse_user_command_line(command: str) -> dict[str, str]:
    """Extract interesting parts from a user command line."""
    parts = command.strip().split()
    entrypoint = None
    for part in parts:
        if Path(part).exists() and "/python" not in part:
            entrypoint = Path(part).name.split(".")[0]
            return {"entrypoint": entrypoint}

    if parts and entrypoint is None:
        entrypoint = parts[0]
    else:
        entrypoint = "unknown"

    return {"entrypoint": entrypoint}
# ---
def pytest_addoption(parser: Any) -> None:
    parser.addoption("--run-benchmark", action="store_true", default=False, help="run benchmark tests")
# ---
def __init__(self, cluster: LocalClusterClient | RemoteClusterClient, namespace: Namespace | None = None):
        self._cluster = cluster
        self._namespace = namespace
# ---
def get(self):
        raise self.exception
# ---
def dartmouth(domain, b, threshold=None):
    '''A flood detection method from the Dartmouth Flood Observatory.

        This method is a refinement of the simple b2-b1 detection method.
    '''
    if threshold == None:
        threshold = float(domain.algorithm_params['dartmouth_threshold'])
    return get_dartmouth(b).lte(threshold)
# ---
def peek(self) -> Any | None:
        with httpx.Client() as client:
            response = client.get(f"http://{self.host}:{self.port}/queues/{self.queue_name}/peek")
            if response.status_code == 200:
                return pickle.loads(response.json()["payload"])
            return None
# ---
def __call__(self, f):
        return self.task_instance.__call__(f)
# ---
def test_works_after_dispose_testing_engine(self):
        eng = engines.testing_engine()
        for i in range(3):
            eq_(eng.scalar(select([1])), 1)
            eng.dispose()
# ---
def test_dicts_loop(benchmark: Any, in_memory_table: pa.Table) -> None:
    """
    Benchmarks Python Memory -> List[dict] -> Python Loop calls Rust per item -> List[dict].
    """

    def _pipeline() -> int:
        return len([dupekit.process_dicts_loop(d) for d in in_memory_table.to_pylist()])

    assert benchmark(_pipeline) > 0
# ---
def _fn(a):
        return jnp.square(jnn.relu(a))
# ---
def test_inv_sqrt_decay_schedule():
    optimizer = AdamConfig(
        learning_rate=1e-3,
        weight_decay=0.0,
        warmup=0.1,
        min_lr_ratio=0.1,
        lr_schedule="inv_sqrt",
        cycles=None,
    )

    sched_fn = optimizer.lr_scheduler(100_000)

    # Warmup phase
    assert np.isclose(sched_fn(0), 0.0)
    assert np.isclose(sched_fn(5000), 0.5e-3)

    # Decay phase: our invsqrt has a non configurable, very long period
    assert sched_fn(50000) < sched_fn(30000)
# ---
def key_str_all(self):
        '''
        Return all managed key strings
        '''
        ret = {}
        for status, keys in six.iteritems(self.list_keys()):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                ret[status][key] = self._get_key_str(key, status)
        return ret
# ---
def cond(state: tuple[GenState, _DecodeOutputs, jax.Array]):
        _gen_state, _outputs, step = state
        return (
            (step < max_rounds)
            & (_gen_state.decode_state.num_queued_tokens > 0)
            & (~hax.all(_gen_state.decode_state.finished)).scalar()
        )
# ---
def load(self, fs, node_id):
        if not self.tryLoad(fs, node_id):
            raise Exception("Could not load node: %s iens: %d report: %d" % (self.name(), node_id.iens, node_id.report_step))
# ---
def append_for(module, suffix):
        # Modules have to be appended to the existing mysql.pp
        # otherwise pp will fail for some of them saying that
        # Mysql::Config definition is missing.
        template = "mysql_%s_%s.pp" % (module, suffix)
        manifestdata.append(getManifestTemplate(template))
# ---
def clause(self):
        if self.fast:
            return self.col_clause()
        else:
            # Matching a flexattr. This is a slow query.
            return None, ()
# ---
def test_mem_read_byte_off_screen(self):
        self.assertEqual(self.mda.mem_read_byte(4000), 0x00)
# ---
def _setup_logger():
    logging.basicConfig(level=logging.INFO)
# ---
def multiply(self, a: int, b: int) -> int:
        return a * b
# ---
def state(self) -> ControllerState:
        return self._state
# ---
def post(self, request):
        data = request.data
        serializer = AuthenticateSerializer(data=data)
        if serializer.is_valid(raise_exception=True):
            new_date = serializer.data
            return response.Response(new_date,status=status.HTTP_200_OK)
        return response.Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
# ---
def __rmul__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.multiply(other, self)
# ---
def _core_plugin(self):
        return bc.get_plugin()
# ---
def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1.two, df2.two])
            return (A3 == 'foo').sum()
# ---
def listeEuler(f, x0, y0, pas, n):
    x, y, L = x0, y0, []
    for k in range(n):
        L += [(x, y)]
        x += pas
        y += pas * f(x, y)
    return L
# ---
def _parse_template(self, tmpl_str, msg_str):
        parse_ex = self.assertRaises(ValueError,
                                     template_format.parse,
                                     tmpl_str)
        self.assertIn(msg_str, six.text_type(parse_ex))
# ---
def less_equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.less_equal](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.less_equal.html)
    """
    return jnp.less_equal(x1, x2)
# ---
def on_update(self):
		invalidate_cache_for_item(self)
		self.validate_name_with_item_group()
		self.update_variants()
		self.update_item_price()
		self.update_template_item()
# ---
def _read_jsonl_gz(path: Path) -> list[dict]:
    """Read records from a gzipped JSONL file."""
    records = []
    with gzip.open(path, "rt", encoding="utf-8") as handle:
        for line in handle:
            if line.strip():
                records.append(json.loads(line))
    return records
# ---
def _(self):
        return self.translator.ugettext
# ---
def _compute_shard_names(self):
        dataset = self._load_dataset()
        if isinstance(dataset, datasets.IterableDataset):
            try:
                return [str(i) for i in range(dataset.n_shards)]
            except NotImplementedError:
                return ["data"]
        else:
            return ["data"]
# ---
def inner(x):
        # x.axes == (Hidden,) but x.array is a batched tracer with leading batch_dim
        return hax.auto_sharded(x)
# ---
def get_describe_filters(self):
        return {
            "Filters": [
                {"Name": "tag:Name", "Values": [self.resource.name]},
                {
                    "Name": "instance-state-name",
                    "Values": [
                        "pending",
                        "running",
                        "shutting-down",
                        " stopping",
                        "stopped",
                    ],
                },
            ]
        }
# ---
def test_cold_migration(self):
        """Test cold migrating server and then confirm the migration"""
        self._test_cold_migrate_server(revert=False)
# ---
def transform_web(html: str) -> str:
    """Transform web HTML to markdown using resiliparse."""
    output = convert_page(html, extract_method="resiliparse", config=ResiliparseConfig())
    return output["content"]
# ---
def cluster_list_configs(ctx):
    """List available cluster configurations."""
    configs = list_available_configs()
    if not configs:
        print("No cluster configurations found in infra/")
        return

    print("Available cluster configurations:")
    for config_path in configs:
        print(f"  {config_path}")
# ---

def unique_digits(x):
    """Given a list of positive integers x. return a sorted list of all
    elements that hasn't any even digit.

    Note: Returned list should be sorted in increasing order.

    For example:
    >>> unique_digits([15, 33, 1422, 1])
    [1, 15, 33]
    >>> unique_digits([152, 323, 1422, 10])
    []
    """
    odd_digit_elements = []
    for i in x:
        if all (int(c) % 2 == 1 for c in str(i)):
            odd_digit_elements.append(i)
    return sorted(odd_digit_elements)
# ---
def from_iterable(caches: Iterable[PageCacheT]) -> "ListCache[PageCacheT]":
        return ListCache(tuple(caches))
# ---
def _partition_trainable_params(model, filter):
    """
    Partitions the model into trainable and non-trainable parameters. This is used internally
    for the gradient calculation and checkpointing, but you can also use it to filter out params for logging
    or something.

    Returns:
        trainable, non-trainable
    """

    filter = make_floating_point_trainable_filter(filter)
    return eqx.partition(model, filter, is_leaf=lambda x: isinstance(x, haliax.NamedArray))
# ---
def _decode_tokens_pretty(tok, ids):
    # we want to make sure we don't have any weird characters in the output
    # so we'll decode the tokens and then escape them
    if hasattr(tok, "convert_ids_to_tokens"):
        return [str(t) for t in tok.convert_ids_to_tokens(ids)]
    else:
        return [str(t) for t in tok.decode(ids)]
# ---
def remove_job(self, index):
        job_num = int(self.main_view.ui.jobs_tab_widget.tabText(index)[1:])
        self.main_model.jobs.pop(job_num, None)
        self.main_view.remove_tab(index)
# ---
def click_remove_button(self):
        """
        :rtype: HomePage
        """
        self._click(EditMoviePageLocators.REMOVE_BUTTON_LOCATOR)
        self.alert_accept()
        from .home import HomePage
        return HomePage(self._driver)
# ---
def area(self):
        raise NotImplementedError
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Invoke the method synchronously (blocking)."""
        ...
# ---
def test_exponential():
    check_gen_is_equal(lambda k, s: jax.random.exponential(k, s), lambda k, s: hax.random.exponential(k, s))
# ---
def config(self):
        return self.embeddings.config
# ---
def trim_and_pad(ary: np.ndarray, max_seq_len: int, pad_to: int, padding_value: int | float) -> np.ndarray:
    """Trim to max_seq_len and pad to pad_to."""
    ary = ary[:max_seq_len]
    if pad_to < len(ary):
        raise ValueError(f"pad_to ({pad_to}) must be >= trimmed length ({len(ary)})")
    ary = np.pad(ary, (0, pad_to - len(ary)), mode="constant", constant_values=padding_value)
    return ary
# ---
def fetch_startup_logs(self, tail_lines: int = 100) -> str | None:
        """Fetch startup script logs for debugging. Returns None if unavailable."""
        ...
# ---
def degrees(a: A) -> A:
    return wrap_elemwise_unary(jnp.degrees, a)
# ---
def convert_to_read(self, value, use_name_get=True):
        return "%s,%s" % (value._name, value.id) if value else False
# ---
def test_transaction_connection_ctx_commit(self):
        fn = self._trans_fn(True)
        conn = testing.db.connect()
        ctx = conn.begin()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def broadcaster():
        while True:
            with lock:
                received = broadcast_one_to_all(status["val"], is_source=is_leader)
                status["val"] = received
                if received != 0:
                    break

            time.sleep(10)

        return status["val"]
# ---
def sample_input_files(tmp_path):
    """Create standard sample input files for skip_existing tests."""
    input_dir = tmp_path / "input"
    input_dir.mkdir()
    for i in range(3):
        with open(input_dir / f"input-{i}.jsonl", "w") as f:
            f.write(f'{{"id": {i}}}\n')
    return input_dir
# ---
def partition_lora_params(params: M) -> Tuple[M, M]:
    """
    Partitions the given parameter tree into base/non-LoRA parameters and non-LoRA parameters.

    Returns:
        (base_params, lora_params)
    """
    lora_params, base_params = eqx.partition(params, is_lora_param, is_leaf=is_lora_param)
    return base_params, lora_params
# ---
def __init__(self, op, leaving):
      self.op = op
      self.leaving = leaving
# ---
def testRaiseInstance(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        try:
          raise RuntimeError('foo')
          print 'bad'
        except RuntimeError as e:
          print e""")))
# ---
def _clear_mesh(mesh):
        del mesh
# ---
def now(*args):
        """ Return the current day and time in the format expected by the ORM.
            This function may be used to compute default values.
        """
        return datetime.now().strftime(DATETIME_FORMAT)
# ---
def __getattr__(self, method_name: str) -> _IrisActorMethod:
        if method_name.startswith("_"):
            raise AttributeError(method_name)
        return _IrisActorMethod(self, method_name)
# ---
def sleep_seconds(n: int):
        time.sleep(n)
# ---
def get_extra_vars():
  """Returns the captured variables by the function.

  Returns:
    If the default graph is being used to define a function, the
    returned list of variables are those created inside the function
    body so far. Otherwise, returns an empty list.
  """
  g = ops.get_default_graph()
  if isinstance(g, _FuncGraph):
    return g.extra_vars
  else:
    return []
# ---
def make_widgets(self, main_frame):
        pass
# ---
def get(self, ref: Any) -> Any:
        """Get result, handling GeneratorFuture, Future objects and plain values."""
        if isinstance(ref, GeneratorFuture):
            return ref.result()
        if isinstance(ref, Future):
            return ref.result()
        return ref
# ---
def __next__(self) -> T:
        start = time.perf_counter()
        item = next(self.items)
        self.this_load_time = time.perf_counter() - start
        self.total_time += self.this_load_time
        return item
# ---
def result(self) -> Any:
        return self._result
# ---
def test_equal_2(self):
        self.assertEqual(string_color('Joshua'), '6A10D6')
# ---
def _setup_regular(self, env):
        super(Float, self)._setup_regular(env)
        self._setup_digits(env)
# ---
def test_bool(self):
        prio_set_list = event._PrioritizedSetList()

        assert bool(prio_set_list) is False

        prio_set_list.add(0, None)
        assert bool(prio_set_list) is True
# ---
def fpik(inp, pref="FPIK"):
            vals = list(inp)
            vals = [(v if v != 2 else (2 if sum(vals) < 4 else 1)) for v in vals]
            for i, p in enumerate(pref):
                yield '"' + p + '": ' + str(vals[i])
# ---
def __init__(self):
        self._balance = 0
        self.__id = generate_id()
# ---
def do_vmap(*args, **kwargs) -> OutputT_co:
            # Create a function that captures the additional arguments
            def do_block_with_args(block: M, *args, **kwargs) -> OutputT_co:
                return fn(block, *args, **kwargs)

            return haliax.vmap(do_block_with_args, self.Block)(self.stacked, *args, **kwargs)
# ---
def wait_ready(self, count: int | None = None, timeout: float = 300.0) -> list[ActorHandle]: ...
# ---
def device_compatible(job_device_type: str, worker_device_type: str) -> bool:
    """Check if a job's device requirement is compatible with a worker's device.

    CPU jobs can run on any worker since every host has a CPU.
    Accelerator jobs (GPU, TPU) require the specific hardware.
    """
    if job_device_type == "cpu":
        return True
    return job_device_type == worker_device_type
# ---
def post_delete(request, slug=None):
    if not request.user.is_staff or not request.user.is_superuser:
        raise Http404
    instance = get_object_or_404(Post, slug=slug)
    instance.delete()
    messages.success(request, "Successfully deleted")
    return redirect("posts:list")
# ---
def hparams(self, defaults, unused_model_hparams):
    p = defaults
    p.modality = {"inputs": modalities.ModalityType.IMAGE,
                  "targets": modalities.ModalityType.SYMBOL}
    p.vocab_size = {"inputs": 256,
                    "targets": self._encoders["targets"].vocab_size}
    p.batch_size_multiplier = 256
    p.loss_multiplier = 1.0
    p.input_space_id = problem.SpaceID.IMAGE
    p.target_space_id = self.target_space_id
# ---
def revert_seek(self):
        self.command('revert_seek');
# ---
def PlaceSign( sign_id, line_num, buffer_num, is_error = True ):
  # libclang can give us diagnostics that point "outside" the file; Vim borks
  # on these.
  if line_num < 1:
    line_num = 1

  sign_name = 'YcmError' if is_error else 'YcmWarning'
  vim.command( 'sign place {0} name={1} line={2} buffer={3}'.format(
    sign_id, sign_name, line_num, buffer_num ) )
# ---
def delete_vf(self, context, vf):
        """remove a vf from the virtual_function_list
        if the vf does not exist, ignore it
        """
        for idx, exist_vf in self.virtual_function_list:
            if base.obj_equal_prims(vf, exist_vf):
                removed_vf = self.virtual_function_list.pop(idx)
                removed_vf.destroy(context)
                return
        LOG.warning("The removing vf does not exist!")
# ---
def _merge_dict(self, other, overwrite_vars, compat, join):
    other = _align_variables(other, join='outer')

    alignable = [k for k, v in other.items() if hasattr(v, 'indexes')]
    aligned = partial_align(self, *[other[a] for a in alignable],
                            join=join, copy=False, exclude=overwrite_vars)

    aligned_self = aligned[0]

    other = OrderedDict(other)
    other.update(zip(alignable, aligned[1:]))

    return _merge_expand(aligned_self, other, overwrite_vars, compat)
# ---
def __eq__(self, other: object) -> bool:
        if not isinstance(other, Duration):
            return NotImplemented
        return self._ms == other._ms
# ---
def list_iris_containers(self, all_states: bool = True) -> list[str]:
        del all_states
        return list(self._containers.keys())
# ---
def push(self, item: T) -> None:
        timestamp = time.time()
        unique_id = uuid.uuid4()
        filename = f"{timestamp:.6f}_{unique_id}.pkl"

        with self.fs.open(self.pending_dir / filename, "wb") as f:
            pickle.dump(item, f)
# ---
def test_get_all_bw_usage_in_failure_case(self):
        """Test that get_all_bw_usage returns an empty list when metrics
        compilation failed.  c.f. bug #910045.
        """
        class testinstance(object):
            def __init__(self):
                self.name = "instance-0001"
                self.uuid = "1-2-3-4-5"

        result = self.conn.get_all_bw_usage([testinstance()],
                datetime.datetime.utcnow())
        self.assertEqual(result, [])
# ---
def test_magic_table_props(self):
        """Table can look up properties on response object"""
        hash_key = DynamoKey("id")
        self.dynamo.create_table("foobar", hash_key=hash_key)
        ret = self.dynamo.describe_table("foobar")
        assert ret is not None
        self.assertEqual(ret.item_count, ret["ItemCount"])
        with self.assertRaises(KeyError):
            self.assertIsNotNone(ret["Missing"])
# ---
def h_fs_get(_,path,eltName=''):
        from stat import S_ISDIR
        data = (escape(open(path).read())
                if not S_ISDIR(os.stat(path).st_mode)
                else [(p,S_ISDIR(os.stat(path+'/'+p).st_mode))
                        for p in os.listdir(path)])
        _.ws.send(json.dumps({"method":"fs_get","result":[path,data,eltName]}))
        pass
# ---
def cifar_image_augmentation(images):
  """Image augmentation suitable for CIFAR-10/100.

  As described in https://arxiv.org/pdf/1608.06993v3.pdf (page 5).

  Args:
    images: a Tensor.
  Returns:
    Tensor of the same shape as images.
  """
  images = tf.image.resize_image_with_crop_or_pad(images, 40, 40)
  images = tf.random_crop(images, [32, 32, 3])
  images = tf.image.random_flip_left_right(images)
  return images
# ---
def test_can_supply_filename_None(self):
        out = StringIO()
        c_log, f_log, formatter = logging_support.configure_logging(out, None)
        self.assertEqual(None, f_log)
# ---
def copy_dir(self, src, dest):
        # self.mkdir(dest)
        if not src.startswith("/"):
            src = os.path.join(self.sharedir, src)
        try:
            dest = os.path.join(self.workflowdir, dest)
            shutil.copytree(src, dest)
        except OSError as exc:  # python >2.5
            if exc.errno == errno.ENOTDIR:
                shutil.copy(src, dest)
            else:
                raise
# ---
def test_axis_shapes_inherit_defaults_and_absorb():
    cfg = MeshConfig(axes={"model": 2})
    ici, dcn = cfg.axis_shapes(num_devices=8, num_slices=1)
    # data should absorb remaining ICI after replica=1, model=2 -> data = 4
    assert ici == {"data": 4, "replica": 1, "model": 2}
    # replica_dcn should absorb all slices by default
    assert dcn == {"replica_dcn": 1}
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids)
        x = self.norm(x)
        return x
# ---
def _reconnect(self):
        """Reconnect to the ReadRows stream using the most recent offset."""
        while True:
            try:
                self._wrapped = self._client.read_rows(
                    read_stream=self._name,
                    offset=self._offset,
                    **self._read_rows_kwargs
                )
                break
            except Exception as exc:
                if not self._resource_exhausted_exception_is_retryable(exc):
                    raise
# ---
def test_encrypt_newlines_inside_message(self):
        self._test_encryption('Message\nwith\ninterior\nnewlines.')
# ---
def set_node(self, node):
        self.node = node
# ---
def is_team_member(self):
        return True if self._get_member() else False
# ---
def unshard(x: jax.Array) -> jax.Array:
    return reshard(x, P((None,)))
# ---
def _async_checkpoint_remover(self):
        while True:
            checkpoint = self._async_checkpoint_remover_queue.get(block=True)
            self._checkpoint_being_removed = checkpoint
            self._do_rm_checkpoint(checkpoint)
            self._checkpoint_being_removed = None
# ---
def laplace(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.laplace(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def power(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.power(x1, x2)
# ---
def from_seconds(cls, seconds: float) -> "Duration":
        """Create duration from seconds."""
        return cls(int(seconds * 1000))
# ---
def test_per_connection_plus_engine(self):
        canary = Mock()
        e1 = testing_engine(config.db_url)

        event.listen(e1, "before_execute", canary.be1)

        conn = e1.connect()
        event.listen(conn, "before_execute", canary.be2)
        conn.execute(select([1]))

        eq_(canary.be1.call_count, 1)
        eq_(canary.be2.call_count, 1)

        conn._branch().execute(select([1]))
        eq_(canary.be1.call_count, 2)
        eq_(canary.be2.call_count, 2)
# ---
def _pipeline() -> int:
        docs = [dupekit.Document(row["id"], row["text"]) for row in in_memory_table.to_pylist()]
        return len(dupekit.process_rust_structs(docs))
# ---
def run_streaming(self, command: str) -> subprocess.Popen:
        return subprocess.Popen(
            self._build_cmd(command),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
        )
# ---
def get_location(self):
        """Return the current position of the window.

        :rtype: (int, int)
        :return: The distances of the left and top edges from their respective
            edges on the virtual desktop, in pixels.
        """
        raise NotImplementedError('abstract')
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "RotaryEmbeddingsConfig":
        return Llama3RotaryEmbeddingsConfig(
            theta=rope_theta,
            factor=config.get("factor", 8.0),
            low_freq_factor=config.get("low_freq_factor", 1.0),
            high_freq_factor=config.get("high_freq_factor", 4.0),
            original_max_position_embeddings=config.get("original_max_position_embeddings", 8192),
        )
# ---
def __module_api_version(self):
        return self.api_version
# ---
def test_new_websocket_client(self, check_token):
        check_token.return_value = {
            'host': 'node1',
            'port': '10000'
        }
        self.wh.socket.return_value = '<socket>'
        self.wh.path = "ws://127.0.0.1/?token=123-456-789"

        self.wh.new_websocket_client()

        check_token.assert_called_with(mock.ANY, token="123-456-789")
        self.wh.socket.assert_called_with('node1', 10000, connect=True)
        self.wh.do_proxy.assert_called_with('<socket>')
# ---
def test_host_startup(self):
        self.assertRaises(NotImplementedError,
                          self.conn.host_power_action, 'host', 'startup')
# ---
def chip_count(self) -> int:
        return self.count
# ---
def local_rope(self) -> RotaryEmbeddingsConfig:
        """Local RoPE config used for Gemma-3's alternating local attention."""
        return dataclasses.replace(self.rope, theta=self.rope_local_base_freq)
# ---
def next(self, psm: PSM):
        if psm.char.isalpha() or psm.char == "_":
            self.parent.g.group.name += psm.char
            return self
        elif psm.char == ">":
            return self.parent
        else:
            psm.error = 'expected a letter, "_" or ">"'
# ---
def positive(a: A) -> A:
    return wrap_elemwise_unary(jnp.positive, a)
# ---
def make_loss_weight(id, prompt_length):
        loss_weight = (np.arange(len(id)) >= prompt_length - 1).astype(np.float32)
        loss_weight[-1] = 0
        return loss_weight
# ---
def on_resize(width, height):
            """The window was resized.

            The window will have the GL context when this event is dispatched;
            there is no need to call `switch_to` in this handler.

            :Parameters:
                `width` : int
                    The new width of the window, in pixels.
                `height` : int
                    The new height of the window, in pixels.

            :event:
            """
# ---
def init(layer_idx, *, key):
            k1, k2 = jax.random.split(key)
            up = hax.nn.Linear.init(Embed, Up, key=k1)
            down = hax.nn.Linear.init(Up, Embed, key=k2)

            up = dataclasses.replace(up, weight=up.weight + layer_idx)  # type: ignore
            down = dataclasses.replace(down, weight=down.weight + layer_idx)  # type: ignore

            return Module(up=up, down=down)
# ---
def foo(x: f32["batch embed"]):  # type: ignore  # noqa: F722
        pass
# ---
def list_jobs(self) -> list[JobInfo]:
        """List all jobs managed by this cluster.

        Returns:
            List of job information for all jobs (running, completed, and failed)
        """
        ...
# ---
def dirty(self, value):
        self._dirty = value
        self.refresh_window_title()
# ---
def convert_to_cache(self, value, record, validate=True):
        if value is None or value is False:
            return False
        if validate and self.sanitize:
            return html_sanitize(value, strip_style=self.strip_style)
        return value
# ---
def validate_item_defaults(self):
		companies = list(set([row.company for row in self.item_defaults]))

		if len(companies) != len(self.item_defaults):
			frappe.throw(_("Cannot set multiple Item Defaults for a company."))
# ---
def parse_timedelta(td_str) -> timedelta:
    td = timedelta(seconds=pytimeparse.parse(td_str))
    if td.total_seconds() < 0:
        raise ValueError("Cannot encode negative timedelta")  # not worth the trouble

    return td
# ---
def test_invalid_programs_are_skipped():
    """Programs with syntax errors should be skipped, not crash."""
    programs = [
        "def good():\n    return 1\n",
        "def bad(\n",  # syntax error
        "also bad{{{",  # syntax error
        "def also_good(x):\n    return x + 1\n",
    ]
    bank = SubtreeBank.from_corpus(programs)
    assert bank.total_entries > 0
# ---
def __init__(self, content):
        '''Service constructor'''
        super(Service, self).__init__(content=content)
# ---
def read_batch(self, timeout: float | None = None) -> RolloutBatch | None:
        """Read a single batch with optional timeout.

        Args:
            timeout: Maximum time to wait in seconds. None means no wait.

        Returns:
            RolloutBatch if available, None if timeout or no batches.
        """
        pass
# ---
def to_slice(self) -> slice:
        return slice(self.start, self.start + self.size)
# ---
def _normalize_unroll(unroll: int | bool | None, block_size: int) -> int | bool:
    """Convert user-provided ``unroll`` values into something understood by ``jax.lax.scan``."""

    if unroll is None:
        return 1

    if isinstance(unroll, bool):
        return unroll

    resolved = int(unroll)
    if resolved < 1:
        raise ValueError(f"unroll must be >= 1; got {resolved}.")

    return resolved
# ---
def __hash__(self):
        if self._hash is None:
            self._hash = hash(frozenset(self._dict.items()))
        return self._hash
# ---
def __init__(self, fichier, player):
        self.fichier = fichier
        self.grille = self.getFirstGrid()
        self.best_hit = 0
        self.players = player
# ---
def _match_greater_than_or_equal(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) >= str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def _get_random_inputs(config: GemmaConfig):
    Embed = config.Embed
    Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()
    return x, mask
# ---
def get_job_tasks(self, job_id: JobName) -> list[ControllerTask]:
        """Get all tasks for a job."""
        with self._lock:
            task_ids = self._tasks_by_job.get(job_id, [])
            return [self._tasks[tid] for tid in task_ids if tid in self._tasks]
# ---
def on_key_press(self, symbol, modifiers):
        """Default on_key_press handler."""
        if symbol == key.ESCAPE and not (modifiers & ~(key.MOD_NUMLOCK |
                                                       key.MOD_CAPSLOCK |
                                                       key.MOD_SCROLLLOCK)):
            self.dispatch_event('on_close')
# ---
def _debug(m):
        print >> sys.stderr, m
# ---
def test_count_repr(self):
        """Count repr"""
        count = Count(0, 0)
        self.assertEqual(repr(count), "Count(0)")
# ---
def e(inp, dif):
            for i in inp:
                yield 2 if i == 10 else (1 if i >= dif else 0)
# ---
def _metric_unflatten(reduction: ReductionType, children):
    """Unflatten Metric for JAX."""
    value, count = children
    return Metric(_value=value, _count=count, reduction=reduction)
# ---
def config(self):
        @dataclass
        class Config:
            model_name: str = "test-model"

        return Config()
# ---
def get_output_shape_for(self, input_shape):
        return (input_shape[0], input_shape[2])
# ---
def __del__(self):
        if self.tb:
            self.hndl(self.cls, self.tb)
# ---
def format_directions(directions):
    return format_line(prefix='directions'.rjust(RJUST), values=directions)
# ---
def _get_local_data_browser_port(default: int = 5000) -> int:
    # looks for the port in the local data browser config file
    config_path = _find_data_browser_local_conf()
    if config_path is None:
        return default

    try:
        with config_path.open() as fp:
            for line in fp:
                match = _LOCAL_DATA_BROWSER_PORT_RE.match(line)
                if match:
                    return int(match.group(1))
    except OSError:
        return default

    return default
# ---
def __repr__(self) -> str:
        return f"Timestamp({self.as_formatted_date()})"
# ---
def yaml_dict(self, value):
        ''' setter method for yaml_dict '''
        self.__yaml_dict = value
# ---
def counting_sample_batch(lesson_id, n_examples, n_generations, mode, rng):
            batch_data, metrics = original_sample_batch(lesson_id, n_examples, n_generations, mode, rng)
            if batch_data is None or metrics is None:
                return None, None
            self._track_rollout_generation()
            # Add metadata about rollout
            metrics["rollout_number"] = self.rollouts_generated
            return batch_data, metrics
# ---
def tearDown(self):
        self.master.delete()
        self.master_status.delete()
        self.datastore.delete()
        self.datastore_version.delete()
        models.create_nova_client = self.safe_nova_client
        super(TestReplication, self).tearDown()
# ---
def test_reinitialize_some_tokens_empty_list(local_gpt2_tokenizer):
    tokenizer = local_gpt2_tokenizer
    model = None

    with pytest.raises(ValueError, match="No tokens to reinitialize"):
        reinitialize_some_tokens(model, tokenizer, [], jax.random.PRNGKey(0))
# ---
def test_profiler_get_base_id(self):
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        self.assertEqual(prof.get_base_id(), "1")
# ---
def default_window_title(self):
		"""ã†ã•ãŽã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¿ã‚¤ãƒˆãƒ«(ãƒ©ãƒ™ãƒ«)ã‚’å¿œç­”ã™ã‚‹ã€‚"""
		if TRACE: print(__name__), self.default_window_title.__doc__

		return "Stanford Bunny"
# ---
def __init__(self,**kwargs):
        self.register_event_type('on_answer')
        super(EditorPopup,self).__init__(**kwargs)
# ---
def list_pilots (self, ptype=ANY) :
        """
        List IDs of data and/or compute pilots
        """

        raise Exception ("%s.list_pilots() is not implemented" % self.__class__.__name__)
# ---
def peirce_skill_score(self):
        """
        Multiclass Peirce Skill Score (also Hanssen and Kuipers score, True Skill Score)
        """
        n = float(self.table.sum())
        nf = self.table.sum(axis=1)
        no = self.table.sum(axis=0)
        correct = float(self.table.trace())
        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (no * no).sum() / n ** 2)
# ---
def inc_qps():
    pass
# ---
def test_hash_verification_failure(temp_cache_dir, test_bundle):
    """Test that hash verification fails with incorrect hash."""
    cache = BundleCache(temp_cache_dir)

    file_url = f"file://{test_bundle}"

    # Use wrong hash
    wrong_hash = "a" * 64

    # Should raise ValueError
    with pytest.raises(ValueError, match="Bundle hash mismatch"):
        cache.get_bundle(file_url, expected_hash=wrong_hash)
# ---
def test_rademacher():
    check_gen_is_equal(lambda k, s: jax.random.rademacher(k, s), lambda k, s: hax.random.rademacher(k, s))
# ---
def test_bound_in_two_tuple(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.y).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(
            stmt, [(2,), (3,), (4,)], params={"q": [(2, 3), (3, 4), (4, 5)]}
        )
# ---
def _start_db_server(self, db_version):
        sudo('svcadm enable postgresql')
# ---
def contains(self, bbox, srs):
        bbox = self._bbox_in_coverage_srs(bbox, srs)
        return bbox_contains(self.bbox, bbox)
# ---
def getPlugins(): pass
# ---
def __init__(self):
        self.lexer = lex.lex(module=self)
# ---
def on_close(self):
        """Default on_close handler."""
        self.has_exit = True
        from pyglet import app
        if app.event_loop.is_running:
            self.close()
# ---
def from_string(s: str) -> "RepoRef":
        if "@" not in s:
            return RepoRef(s)
        model_name_or_path, revision = s.split("@")
        return RepoRef(model_name_or_path, revision)
# ---
def get_user(self, email):
        try:
            return User.objects.get(email=email)
        except User.DoesNotExist:
            return None
# ---
def setUp(self):
        super(CsrfTests, self).setUp()
        # The CSRF library uses the time, so we mock it out.
        self.time_mock = mock.Mock()
        csrf.time = self.time_mock
        self.time_mock.time = mock.Mock(return_value=MOCKED_TIME)
        # The handler tests need a WSGIApplication.
        app = webapp2.WSGIApplication([('/', self.TestHandler)])
        self.testapp = webtest.TestApp(app)
# ---
def _dt_struct_to_array(struct):
        if not isinstance(struct, ShapeDtypeStruct):
            return struct
        return jnp.zeros(struct.shape, struct.dtype)
# ---
def tobytes(self, order="C") -> Any:  # pragma: no cover
        return self.array.tobytes(order=order)
# ---
def test_nested_field_missing(self):
        expr = col("meta")["score"]
        assert expr.evaluate({"meta": {"other": 1}}) is None
# ---
def on_line(line: str) -> None:
            logger.info("[%s] %s", vm_name, line)
# ---
def orcaModifierChanged(self, widget):
        """Signal handler for the changed signal for the orcaModifierComboBox
           Set the 'orcaModifierKeys' preference to the new value.

        Arguments:
        - widget: the component that generated the signal.
        """

        model = widget.get_model()
        myIter = widget.get_active_iter()
        orcaModifier = model[myIter][0]
        self.prefsDict["orcaModifierKeys"] = orcaModifier.split(', ')
# ---
def backend(request):
    """Parametrized fixture providing all job contexts for testing."""
    return request.param
# ---
def slice(self, *args, **kwargs) -> "NamedArray":  # pragma: no cover
        return haliax.slice(self, *args, **kwargs)
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        return ScaleByAdamHState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)
# ---
def get_endpoint(self, template, *args):
        return template % ((self.service_port,) + tuple(args))
# ---
def can_access_delivery_email(user_profile: UserProfile) -> bool:
    realm = user_profile.realm
    if realm.email_address_visibility == Realm.EMAIL_ADDRESS_VISIBILITY_ADMINS:
        return user_profile.is_realm_admin

    if realm.email_address_visibility == Realm.EMAIL_ADDRESS_VISIBILITY_MODERATORS:
        return user_profile.is_realm_admin or user_profile.is_moderator

    return False
# ---
def resolve(self, url):
        return url
# ---
def list_slices(self, group_config: config_pb2.ScaleGroupConfig) -> list[str]: ...
# ---
def transform(self, x, scalar):
            return x + self.w + scalar
# ---
def null(self, env):
        return env[self.comodel_name]
# ---
def render_template(template_name, context={}):
    template = env.get_template(template_name)
    context.update(
        SETTINGS=settings,
        now=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        version='.'.join(map(str, __VERSION__)))
    return template.render(**context)
# ---
def get_item_defaults(item_code, company):
	item = frappe.get_cached_doc('Item', item_code)

	out = item.as_dict()

	for d in item.item_defaults:
		if d.company == company:
			row = copy.deepcopy(d.as_dict())
			row.pop("name")
			out.update(row)
	return out
# ---
def reset_table_style(self):
        if self._row_matched:
            self._row_matched = False
            self.model.reset_rows_highlight()
        self.table.refresh()
# ---
def close(self):
        """Close the event loop and thread gracefully."""
        if self.loop.is_running():
            self.loop.call_soon_threadsafe(self.loop.stop)
        self.thread.join()
        self.loop.close()
# ---
def to_python(self, value):
		if not value:
			return []

		if isinstance(value, list):
			return value

		return value.split(',')
# ---
def stop_recording(self):
        self.recording_enabled = False
        self.startrecordingAction.setEnabled(True)
        self.stoprecordingAction.setEnabled(False)
# ---
def test_mem_write_word_at_top_left(self):
        self.mda.mem_write_word(0x0000, 0x0841) # 'A' with intensity.
        self.assertEqual(self.mda.video_ram[0x0000], 0x41)
        self.assertEqual(self.mda.video_ram[0x0001], 0x08)
        self.assertEqual(self.cg.last_blit, (None, (0, 0), 0x41, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def genetic_modification_7_addgene_source(testapp):
    item = {
        'name': 'addgene',
        'title': 'Addgene',
        'status': 'released'
    }
    return testapp.post_json('/source', item).json['@graph'][0]
# ---
def __init__(self, verb, description):
        """ Initialize EventType.

        :verb: Scim verb
        :description: HR description text
        """
        self.verb = verb
        self.description = description
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetDepartmentsRow(), 3)
# ---
def __getstate__(self) -> dict:
        # Serialize only the discovery parameters - discovery state resets on deserialize
        return {
            "name": self._name,
            "count": self._count,
            "job_id": self._job_id,
        }
# ---
def test_slice_ref_with_dslice():
    Layer = hax.Axis("layer", 10)
    Head = hax.Axis("head", 4)
    cache_ref = hax.new_ref(hax.zeros((Layer, Head)))

    sub = cache_ref.slice({"layer": hax.ds(3, 3)})
    assert sub.axes == (Layer.resize(3), Head)

    sub[{"layer": 1, "head": slice(None)}] = hax.ones(Head)

    value = cache_ref.value()
    assert jnp.allclose(value[{"layer": 4}].array, jnp.ones((Head.size,), dtype=jnp.float32))
    assert isinstance(sub._prefix[0], type(dslice(0, 1)))
# ---
def iter_chunks(self) -> Iterator[list]:
        """Iterate over chunks (each chunk is a list of items)."""
        for chunk in self.chunks:
            data = self.context.get(chunk.data)
            yield data
# ---
def register_worker(
    state: ControllerState,
    worker_id: str,
    address: str,
    metadata: cluster_pb2.WorkerMetadata,
) -> WorkerId:
    """Register a worker via event."""
    wid = WorkerId(worker_id)
    state.handle_event(
        WorkerRegisteredEvent(
            worker_id=wid,
            address=address,
            metadata=metadata,
            timestamp=Timestamp.now(),
        )
    )
    return wid
# ---
def test_capturing_readouterr_decode_error_handling(self):
        with self.getcapture() as cap:
            # triggered a internal error in pytest
            print('\xa6')
            out, err = cap.readouterr()
        assert out == py.builtin._totext('\ufffd\n', 'unicode-escape')
# ---
def signbit(a: A) -> A:
    return wrap_elemwise_unary(jnp.signbit, a)
# ---
def capture_time():
    start = time.perf_counter()
    done = False

    def fn():
        if done:
            return end - start
        else:
            return time.perf_counter() - start

    yield fn
    end = time.perf_counter()
    done = True
# ---
def test_parse_chat_completion_logprobs(tokenizer):
    """Test logprob extraction."""
    token_strs = ["Hello", "Ä world"]
    logprobs = [-0.123, -0.456]

    chat_completion = create_mock_chat_completion_with_logprobs("Hello world", token_strs, logprobs)

    parsed_logprobs = parse_chat_completion_logprobs(chat_completion)

    assert len(parsed_logprobs) == 2
    assert parsed_logprobs == logprobs
# ---
def __getitem__(self, key):
        return self._fallback(key)
# ---
def link(self, link, title, text):
        """Rendering a given link with content and title.

        :param link: href link for ``<a>`` tag.
        :param title: title content for `title` attribute.
        :param text: text content for description.
        """
        link = escape_link(link)
        if not title:
            return '<a href="%s">%s</a>' % (link, text)
        title = escape(title, quote=True)
        return '<a href="%s" title="%s">%s</a>' % (link, title, text)
# ---
def next_sleep_interval() -> float:
    now = get_now(datetime.timezone.utc)
    target = now.replace(second=0) + WAKEUP_INTERVAL
    return (target - now).total_seconds()
# ---
def __repr__(self):
        return "%s.%s" % (self.model_name, self.name)
# ---
def to_named_array(self):
        return NamedArray(self.array, self.main_axes)
# ---
def _slice_out(Block, i, x):
        if isinstance(x, haliax.core.NamedArray):
            if haliax.selects_axis(x.axes, Block):
                return x[Block, i]
            else:
                return x
        elif haliax.jax_utils.is_jax_array_like(x):
            return x[i]
        else:
            return x
# ---
def load_tokenizer(model_name: str) -> bool:
    """Load a tokenizer and set it as current."""
    global current_tokenizer
    console.print(f"[blue]Loading {model_name}...[/blue]")
    current_tokenizer = AutoTokenizer.from_pretrained(model_name)
    console.print(f"[green]âœ“ Loaded {model_name}[/green]")
# ---
def norm_config(self) -> LayerNormConfigBase:
        """Get the normalization configuration for Gemma."""
        return GemmaNormConfig(
            eps=self.layer_norm_epsilon,
            use_weight=True,  # GemmaRMSNorm requires use_weight=True
            use_bias=False,  # GemmaRMSNorm doesn't support bias
        )
# ---
def test_deadline_from_now_with_duration():
    """Deadline.from_now works with Duration."""
    deadline = Deadline.from_now(Duration.from_ms(100))
    assert not deadline.expired()
    time.sleep(0.15)
    assert deadline.expired()
# ---
def to_state_dict(self, prefix: Optional[str] = None) -> StateDict:
        w = [self.w1.weight, self.w2.weight, self.w3.weight]
        out = {}

        num_experts = self.w1.Experts.size
        for i in range(num_experts):
            for j in range(3):
                key = f"{prefix}.{i}.w{j + 1}.weight"
                val = w[j]["experts", i].array
                # out[key] = val
                out[key] = jnp.swapaxes(val, -1, -2)

        return out
# ---
def ravel(self, new_axis_name: AxisSelector) -> "NamedArray":  # pragma: no cover
        return haliax.ravel(self, new_axis_name=new_axis_name)
# ---
def find_chainlink_dir():
    """Find the .chainlink directory by walking up from cwd."""
    current = os.getcwd()
    for _ in range(10):
        candidate = os.path.join(current, '.chainlink')
        if os.path.isdir(candidate):
            return candidate
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent
    return None
# ---
def __repr__(self):
        return "<exactmatcher files=%r>" % self._files
# ---
def cli(local: bool, rounds: int, delay: float, rounds_positional: int | None):
    """Token-passing actor test."""
    # Positional arg takes precedence (for job submission compatibility)
    actual_rounds = rounds_positional if rounds_positional is not None else rounds

    if local:
        success = run_local(actual_rounds, delay)
        raise SystemExit(0 if success else 1)
    else:
        # When run as a job, main() will use iris_ctx() for context
        main(actual_rounds, delay)
# ---
def try_load_path(path):
        try:
            fs, path_to_open = _get_fs_and_plain_path(path)
            fs.open(path_to_open, "rb")
        except Exception:
            return False
        else:
            return True
# ---
def __setitem__(self, index: int, obj: T):
        self._index_to_obj[index] = obj
        self._obj_to_index[obj] = index
# ---
def GetBufferChangedTick( bufnr ):
  return GetIntValue( 'getbufvar({0}, "changedtick")'.format( bufnr ) )
# ---
def getContainedObjectInterface(self):
        return IPublication
# ---
def user_to_email(user_profile: UserProfile) -> str:
        return user_profile.email.lower()
# ---
def _loop(self):
        while True:
            try:
                self.lc.handle()
            except Exception as ex:
                print('Got exception while handling lcm message', ex)
# ---
def test_find_image_in_cache_no_shares(self):
        drv = self._driver
        drv._mounted_shares = []
        result = drv._find_image_in_cache('image_id')
        if not result:
            pass
        else:
            self.fail('Return result is unexpected')
# ---
def find_free_port() -> int:
    """Find an available port."""
    with socket.socket() as s:
        s.bind(("", 0))
        return s.getsockname()[1]
# ---
def _get_mistral_config(use_flash=False, num_kv_heads=4) -> MistralConfig:
    return MistralConfig(
        num_layers=2,
        max_seq_len=128,
        hidden_dim=16,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def __hash__(self) -> int:
        """Hash based on repr for use in sets/dicts."""
        return hash(repr(self))
# ---
def call(self) -> "_PoolCallProxy[T]":
        return _PoolCallProxy(self)
# ---
def _is_checkpoint_dir(path):
        for checkpoint_dir_pattern in is_checkpoint_dir_pattern:
            if not fsspec_exists(os.path.join(path, checkpoint_dir_pattern)):
                return False
        return True
# ---
def teardown(self):
        testing.db.execute(users.delete())
# ---
def __repr__(self) -> str:
        return f"Timer(elapsed={self.elapsed_seconds():.3f}s)"
# ---
def test_capturing_error_recursive(self):
        with self.getcapture() as cap1:
            print("cap1")
            with self.getcapture() as cap2:
                print("cap2")
                out2, err2 = cap2.readouterr()
                out1, err1 = cap1.readouterr()
        assert out1 == "cap1\n"
        assert out2 == "cap2\n"
# ---
def fn(config: MyConfig | None):
        print(config.input_path, os.path.exists(config.input_path), flush=True)
        if os.path.exists(config.input_path):
            raise Exception("Failed")
        else:
            append_log(log, config)
# ---
def reload_model(self, model: LmHeadModel | None, state_dict: dict) -> LmHeadModel | None:
        assert model is not None or state_dict is not None, "Either model or state_dict must be provided"
        if model is None and state_dict is not None:
            with hax.set_mesh(self.mesh), hax.axis_mapping(self.axis_mapping):
                model = update_model(model, state_dict)

        self._inference_server.reload(lambda _: model)
        return model
# ---
def __goForAWalk(self, task):
        self.notify.debug('going for a walk')
        self.fsm.request('Walk')
        return Task.done
# ---
def filename(self, value=""):
        self._filename = value
        self.refresh_window_title()
# ---
def test_ring_buffer_fifo_eviction(ring_buffer):
    """Oldest records are evicted when buffer is full."""
    for i in range(15):
        ring_buffer.append(BufferedLogRecord(timestamp=float(i), level="INFO", logger_name="test", message=f"msg-{i}"))
    results = ring_buffer.query()
    assert len(results) == 10
    assert results[0].message == "msg-5"
    assert results[-1].message == "msg-14"
# ---
def preprocessing(text, tab=4):
    text = _newline_pattern.sub('\n', text)
    text = text.expandtabs(tab)
    text = text.replace('\u00a0', ' ')
    text = text.replace('\u2424', '\n')
    pattern = re.compile(r'^ +$', re.M)
    return pattern.sub('', text)
# ---
def test_get_by_id(self):
        # Ensure id is correct for the current/logged in user.
        with self.client:
            self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="admin_user"),
                follow_redirects=True,
            )
            self.assertTrue(current_user.id == 1)
# ---
def setUp(self):
        super(XenAPIDiffieHellmanTestCase, self).setUp()
        self.alice = vmops.SimpleDH()
        self.bob = vmops.SimpleDH()
# ---
def test_exists():
    """`fix.with_fixture` function exists"""
    assert isinstance(with_fixture, FunctionType)
# ---
def withdraw(self, amount):
        if not isinstance(amount, int):
            raise ValueError
        if self._balance < amount:
            raise WithdrawError(amount)
        self._balance -= amount
        return self._balance
# ---
def screenshot(self, includes='subtitles', mode='single'):
        self.command('screenshot', includes, mode)
# ---
def should_run(self) -> bool:
        """Check if enough time has passed; updates last run time if True."""
        now = time.monotonic()
        if self._last_run is None or (now - self._last_run >= self._interval):
            self._last_run = now
            return True
        return False
# ---
def user_twitter_handle(self):
        return self._get_profile().twitter_handle
# ---
def load_apartment_by_mailbox_id(mailbox_id):
        # type: (int) -> Optional[ApartmentDTO]
        apartment_orm = Apartment.select().where(Apartment.mailbox_rebus_id == mailbox_id).first()
        if apartment_orm is None:
            return None
        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
        return apartment_dto
# ---
def print_field(label, value):
    if get_delimiter() is not None:
        sys.stdout.write(label + get_delimiter() + value + '\n')
    else:
        sys.stdout.write(
            label + " " * (FIELD_NAME_WIDTH-len(label)) + fill(value,
                                                               subsequent_indent=' '*FIELD_NAME_WIDTH,
                                                               width_adjustment=-FIELD_NAME_WIDTH) +
            '\n')
# ---
def log(self, metrics: Mapping[str, Any], *, step: Optional[int], commit: Optional[bool] = None):
        del commit
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "log",
                "step": step,
                "metrics": metrics,
            }
        )
        self.logger.info(json.dumps(record))
        if step is not None:
            self._last_metrics.update(_flatten(metrics))
# ---
def col_clause(self):
        pattern = (self.pattern
                   .replace('\\', '\\\\')
                   .replace('%', '\\%')
                   .replace('_', '\\_'))
        search = '%' + pattern + '%'
        clause = self.field + " like ? escape '\\'"
        subvals = [search]
        return clause, subvals
# ---
def do_viz_lm(config: LevanterVizLmConfig) -> None:
    """
    Visualizes log probabilities of a language model.

    Args:
        config (VizLmConfig): The configuration for visualizing log probabilities.
    """
    # Levanter can read `gs://` checkpoints directly via fsspec/tensorstore, and HF
    # checkpoints via fsspec as well. Avoid staging large directories locally.
    execute_in_subprocess(viz_lm_main, (config,), {})
# ---
def get_driver(dsn_string):
    driver, args, kwargs = parse_dsn(dsn_string)
    return driver(*args, **kwargs)
# ---
def _run_dashboard(host: str, port: int):
    """Run the FastAPI dashboard server."""
    uvicorn.run(app, host=host, port=port, log_level="info")
# ---
def _teardown_volumes(self):
        for drive in self._prepared_volumes:
            try:
                self._irs.teardownImage(drive['domainID'],
                                        drive['poolID'],
                                        drive['imageID'])
            except Exception as e:
                logging.error('Job %r error tearing down drive: %s',
                              self._vmid, e)
# ---
def _is_subsequence(needle, haystack):
    needle_i = 0
    haystack_j = 0
    while needle_i < len(needle) and haystack_j < len(haystack):
        if needle[needle_i] == haystack[haystack_j]:
            needle_i += 1
        haystack_j += 1

    if needle_i < len(needle):
        return False
    return True
# ---
def test_warmup_and_cosine_decay():
    optimizer = AdamConfig(
        learning_rate=1e-2,
        weight_decay=0.0,
        warmup=0.1,  # 10% of steps
        min_lr_ratio=0.1,
        lr_schedule="cosine",
        cycles=None,
    )

    sched_fn = optimizer.lr_scheduler(1000)

    # Warmup phase
    assert np.isclose(sched_fn(0), 0.0)
    assert np.isclose(sched_fn(50), 0.5e-2)
    assert np.isclose(sched_fn(100), 1e-2)

    # Decay phase
    assert np.isclose(sched_fn(999), 1e-3, atol=1e-5)
# ---
def make_client() -> JobSubmissionClient:
    """Create a JobSubmissionClient based on environment variables."""
    address = os.environ.get("RAY_ADDRESS", REMOTE_DASHBOARD_URL)
    # Always pass an explicit HTTP dashboard URL. If Ray has to infer the Jobs
    # API endpoint (e.g. from a `ray://...` address), it can resolve to the head
    # node's internal `webui_url`, which isn't reachable from a developer laptop
    # when using SSH port forwarding.
    return JobSubmissionClient(address)
# ---
def __str__(self):
        return "%s.%s" % (self.model_name, self.name)
# ---
def fn(config: MyConfig | None):
        pass
# ---
def main(*args):
    _args = ()
    for arg in args:
        if isinstance(arg, Path):
            _args += (str(arg),)
        else:
            _args += (arg,)
    _main(_args)
# ---
def configurable_default(cls):
        return BlockingResolver
# ---
def __init__(
        self,
        coordinator: OctoprintDataUpdateCoordinator,
        sensor_type: str,
        device_id: str,
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator)
        self._device_id = device_id
        self._attr_name = f"OctoPrint {sensor_type}"
        self._attr_unique_id = f"{sensor_type}-{device_id}"
# ---
def _building_job():
    """Simple job for testing BUILDING state (used with chaos delay injection)."""
    return "ok"
# ---
def __repr__(self):
        return "<treematcher rules=%r>" % self._rules
# ---
def set_db_records():
    affiemp = set_uw_account("affiemp")

    javerage = set_uw_account("javerage")

    ellen = set_uw_account("ellen")

    staff = set_uw_account("staff")
    staff.set_disable()

    retiree = set_uw_account("retiree")

    tyler = set_uw_account("faculty")

    leftuw = set_uw_account("leftuw")
    leftuw.set_terminate_date()

    testid = set_uw_account("testid")
# ---
def _docker_logs_tail(container_name: str, *, max_lines: int = 200) -> str:
    result = subprocess.run(
        ["docker", "logs", "--tail", str(max_lines), container_name],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        stderr = (result.stderr or "").strip()
        return f"<failed to read docker logs for {container_name}: {stderr}>"
    return result.stdout
# ---
def test_get_task_logs_not_found_for_missing_job(client):
    """GetTaskLogs returns NOT_FOUND when the job doesn't exist."""
    resp = client.post(
        "/iris.cluster.ControllerService/GetTaskLogs",
        json={"taskId": JobName.root("nonexistent").task(0).to_wire()},
        headers={"Content-Type": "application/json"},
    )
    assert resp.status_code != 200
    assert "not found" in resp.text.lower()
# ---
def shutdown(self) -> None:
        """Terminate all local actors."""
        for job in self._jobs:
            job.terminate()
        for handle in self._handles:
            handle._executor.shutdown(wait=False)
# ---
def named_call(*, name: str | None = None) -> Callable[[F], F]: ...
# ---
def hardcoded(path: str) -> "InputName":
        """
        Sometimes we want to specify a path that is not part of the pipeline but is still relative to the prefix.
        Try to use this sparingly.
        """
        return InputName(None, name=path)
# ---
def testFormatMACB(self):
    """Tests the _FormatMACB function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    macb_string = test_helper._FormatMACB(event, event_data, event_data_stream)
    self.assertEqual(macb_string, '..C.')
# ---
def test_entrypoint_params_cpu():
    from fray.v2.ray_backend.backend import get_entrypoint_params

    request = JobRequest(
        name="cpu-job",
        entrypoint=Entrypoint.from_binary("echo", ["hello"]),
        resources=ResourceConfig(cpu=4, ram="2g"),
    )
    params = get_entrypoint_params(request)
    assert params["entrypoint_num_cpus"] == 4.0
    assert "entrypoint_num_gpus" not in params
    assert params["entrypoint_memory"] > 0
# ---
def size(self):
		if hasattr(self.file, "size"):
			return self.file.size()
		elif isinstance(self.file, file):
			from os.path import getsize
			return getsize(self.file.name)
		raise NotImplementedError
# ---
def register_message_handler(self, target, handler):
        self._message_handlers[target] = handler
# ---
def loss_ref(x_raw, w_raw):
        loss_val, lse_val = linear_softmax_cross_entropy_loss_reference(
            x_raw,
            y,
            w_raw,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
        )
        loss_val = loss_val + logsumexp_weight * (lse_val**2)
        return loss_val.mean()
# ---
def _stack_leaves_unchecked(*leaves):
        if is_named_array(leaves[0]):
            return hax.stack(batch_name, leaves)
        else:
            return jnp.stack(leaves)
# ---
def sign(x):
    return 1 if x > 0 else -1
# ---
def loss_fn(p):
        loss, _ = ar_loss(p, token_ids, loss_mask, tiny_cfg)
        return loss
# ---
def reload(self) -> str:
        """Re-run bootstrap on existing host.

        For ManualController this is the same as start() since there's no VM
        to preserve - we just re-run the bootstrap script.
        """
        return self.start()
# ---
def visit_arg(self, node: ast.arg) -> ast.arg:
        if node.arg in self.mapping:
            return ast.arg(arg=self.mapping[node.arg], annotation=node.annotation)
        return node
# ---
def test_is_empty(self):
        assert not os.path.exists(self.dbname)
# ---
def git_version(git_path, module):
    """return the installed version of git"""
    cmd = "%s --version" % git_path
    (rc, out, err) = module.run_command(cmd)
    if rc != 0:
        # one could fail_json here, but the version info is not that important,
        # so let's try to fail only on actual git commands
        return None
    rematch = re.search('git version (.*)$', to_native(out))
    if not rematch:
        return None
    return LooseVersion(rematch.groups()[0])
# ---
def loss_fn(mlp, x):
        return mlp(x).mean().scalar(), {}
# ---
def block_html(self, html):
        """Rendering block level pure html content.

        :param html: text content of the html snippet.
        """
        if self.options.get('skip_style') and \
           html.lower().startswith('<style'):
            return ''
        if self.options.get('escape'):
            return escape(html)
        return html
# ---
def _get_ha_binding(self, context, router_id):
        with context.session.begin(subtransactions=True):
            query = context.session.query(ha_db.RouterHASetting)
            query = query.filter(
                ha_db.RouterHASetting.router_id == router_id)
            return query.first()
# ---
def _resolve_config_path(config_path: str) -> Path:
    path = Path(config_path)
    if not path.is_absolute():
        path = ROOT / path
    return path
# ---
def _get_gemma3_config(use_flash=False, num_kv_heads=4, seq_len=128) -> Gemma3Config:
    from levanter.models.gemma import Gemma3Config

    return Gemma3Config(
        max_seq_len=seq_len,
        hidden_dim=16,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
        head_dim=4,
        query_pre_attn_scalar=4,
        sliding_window=seq_len,
    )
# ---
def _get_mixtral_config(use_flash=False, num_kv_heads=4, seq_len=128) -> MixtralConfig:
    return MixtralConfig(
        max_seq_len=seq_len,
        hidden_dim=16,
        intermediate_dim=32,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---


def largest_divisor(n: int) -> int:
    """ For a given number n, find the largest number that divides n evenly, smaller than n
    >>> largest_divisor(15)
    5
    """
    for i in reversed(range(n)):
        if n % i == 0:
            return i
# ---
def list_workers(self, request: cluster__pb2.Controller.ListWorkersRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListWorkersResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def compute_blake2_hex(text: str) -> str:
    return bytes(hash_blake2(text.encode("utf-8"))).hex()
# ---
def test_tree_diff_syntax_error_returns_empty():
    assert tree_diff("invalid{{{", "x = 1\n") == []
    assert tree_diff("x = 1\n", "invalid{{{") == []
# ---
def __getitem__(self, item):
        return self.store[item]
# ---
def loadPics(self):
        self.standing = loadImage("gripe_stand.png")
        self.falling = loadImage("grfalling.png")
        for i in range(8):
            imageName = "gr" + str(i) + ".png"
            self.walkR.append(loadImage(imageName))
        for i in range(8):
            imageName = "gl" + str(i) + ".png"
            self.walkL.append(loadImage(imageName))
# ---
def reconcile(self) -> None:
        """Discover and adopt existing VM groups from the cloud.

        Called once at startup to recover state from a previous controller.
        """
        with self._vm_groups_lock:
            for vm_group in self._vm_manager.discover_vm_groups():
                self._vm_groups[vm_group.group_id] = vm_group
# ---
def show_serialdlg(self):
        dlg = SerialDialog(self.settings, self)
        dlg.exec_()
# ---
def _parse_gcs_path(self, path: str) -> tuple[str, str]:
        """Parse gs://bucket/path into (bucket, blob_path)."""
        path = path[5:]  # Remove gs:// prefix
        bucket, _, blob_path = path.partition("/")
        return (bucket, blob_path)
# ---
def finger_all(self):
        self._call_all('finger_all')
# ---
def setfunc(index):
            self.view.set_swap_button_enabled(self.can_perform_swap())
# ---
def _createPostamble(self):
        """
        """
        ex = []
        ex.append('M30\n') # End of Program, rewind
        return ex
# ---
def worker(worker_id: int):
        try:
            for i in range(num_operations):
                vm = MagicMock()
                vm_id = f"vm-{worker_id}-{i}"
                vm.info = vm_pb2.VmInfo(vm_id=vm_id)

                registry.register(vm)
                registry.unregister(vm_id)
        except Exception as e:
            errors.append(e)
# ---
def fai(domain, b, threshold=None):
    ''' Floating Algae Index. Method from paper: Feng, Hu, Chen, Cai, Tian, Gan,
    Assessment of inundation changes of Poyang Lake using MODIS observations
    between 2000 and 2010. Remote Sensing of Environment, 2012.
    '''
    if threshold == None:
        threshold = float(domain.algorithm_params['fai_threshold'])
    return get_fai(b).lte(threshold)
# ---
def test_all_variants_valid_python():
    exprs = ["a + b", "x < 0", "a and b", "a + b * c", "x > y"]
    for expr in exprs:
        variants = generate_expression_variants(expr)
        for v in variants:
            try:
                ast.parse(v, mode="eval")
            except SyntaxError:
                pytest.fail(f"Invalid variant {v!r} from {expr!r}")
# ---
def execute(conn, clauseelement, multiparams, params):
            canary.append('execute')
            return clauseelement, multiparams, params
# ---
def _on_store_loaded(self, storecontroller):
        self.autocomp.add_words_from_units(storecontroller.get_store().get_units())

        if hasattr(self, '_cursor_changed_id'):
            self.store_cursor.disconnect(self._cursor_changed_id)
        self.store_cursor = storecontroller.cursor
        self._cursor_changed_id = self.store_cursor.connect('cursor-changed', self._on_cursor_change)
        self._on_cursor_change(self.store_cursor)
# ---
def num_cpus(self) -> int:
        if self.override_resources is not None:
            cpus = self.override_resources.get("num_cpus", None)
            if cpus is not None:
                return cpus
        return num_cpus_used_by_tokenizer(self.tokenizer)
# ---
def create_container(self, config: ContainerConfig) -> str: ...
# ---
def __init__(self, slot):
        """ Creates a new datetime descriptor.

        :param str slot:
            The attribute name where the actual value is stored.
        """
        self.slot = slot
# ---
def finalize(self):
        common.replace_in_file(self.lua_file, 'local out_file = "' +
                               self.results_file + '"',
                               'local out_file = ""')
        # destroy neighbor stacks
        for stack_name in self.neighbor_stack_names:
            common.DEPLOYMENT_UNIT.destroy_heat_template(stack_name)
        self.neighbor_stack_names = list()
# ---
def execute(self, hook):
        """
        Executes the check.

        :param hook: The name of the hook being run.
        :type hook: :class:`str`
        :returns: ``True`` if the check passed, ``False`` if not.
        :rtype: :class:`bool`

        """
        pass
# ---
def enabled(self):
        if not app.player.paused:
            self.plugin_on_unpaused()
# ---
def test_poly():
    R = Axis("R", 3)
    roots = hax.named([1.0, 2.0, 3.0], (R,))
    coeffs = hax.poly(roots)
    assert jnp.allclose(coeffs.array, jnp.poly(roots.array))
    assert coeffs.axes[0] == R.resize(coeffs.array.shape[0])
# ---
def batch_axis_name(self) -> str | None:
        return self.mesh.batch_axis_name
# ---
def __enter__(self) -> Autoscaler:
        return self
# ---
def isreal(a: A) -> A:
    return wrap_elemwise_unary(jnp.isreal, a)
# ---
def ready_count(self) -> int:
        """Number of actors that are available for RPC."""
        return len(self._handles)
# ---
def _description_searchable(self):
        return bool(self.store or self.search or (self.column and self.column._fnct_search))
# ---
def get_code_cells(self):
        return [cell['source'] for cell in self.json['cells']
                if cell['cell_type'] == 'code']
# ---
def test_count_division(self):
        """Count division"""
        count = Count(4, 2)
        self.assertEqual(count / 2, 2)
# ---
def OpenFileInPreviewWindow( filename ):
  """ Open the supplied filename in the preview window """
  vim.command( 'silent! pedit! ' + filename )
# ---
def get_slice(self, slice_id: str) -> FakeVmGroup | None:
        """Get a specific VM group by ID."""
        with self._lock:
            return self._slices.get(slice_id)
# ---
def push(self, batch: RolloutBatch) -> None:
        """Push batch to queue, blocking if full (when bounded)."""
        with self._not_full:
            if self._maxlen is not None:
                while len(self._queue) >= self._maxlen:
                    self._not_full.wait()
            self._queue.append(batch)
            self._not_empty.notify()
# ---
def _deps(self):
    thrift_import_target = self.get_options().thrift_import_target
    thrift_imports = self.context.resolve(thrift_import_target)
    return thrift_imports
# ---
def init(config: GemmaConfig, *, key) -> "GemmaTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, GemmaDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return GemmaTransformer(config, layers, ln_f)
# ---
def is_lora_param(node):
    return isinstance(node, LowRankLinear)
# ---
def clause(self):
        return '1', ()
# ---
def overlay_add(self, overlay_id, x, y, file_or_fd, offset, fmt, w, h, stride):
        self.command('overlay_add', overlay_id, x, y, file_or_fd, offset, fmt, w, h, stride)
# ---
def open(self, req, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
        self.req, self.data, self.timeout = req, data, timeout
# ---
def signup():
    # Create new user
    new_user = User()
    new_user.name = request.form['name']
    new_user.email = request.form['email']
    new_user.password = sha1(request.form['password']).hexdigest()
    new_user.token = str(uuid.uuid4())
    new_user.save()
    return JSON(message='User created successfully')
# ---
def decorate_as_label(self, label_type, labels):
        allure_label_marker = '{prefix}.{label_type}'.format(prefix=ALLURE_LABEL_PREFIX, label_type=label_type)
        allure_label = getattr(pytest.mark, allure_label_marker)
        return allure_label(*labels, label_type=label_type)
# ---
def test_host_shutdown(self):
        self._test_host_action(self.conn.host_power_action, 'shutdown')
# ---
def blockIndentation(block):
            text = block.text()
            return text[:len(text) - len(text.lstrip())]
# ---
def setUp(self):
    self.schedule = schedule_parser.Schedule()
    self.schedule.Parse(SCHEDULE_PATH)
# ---
def resize_vocab(self, new_size: int, key: PRNGKeyArray | None = None) -> "GrugWrapper":
        raise NotImplementedError("GrugWrapper does not yet support resizing the vocabulary.")
# ---
def get_gpg_fingerprint(output):
    """Return a fingerprint of the primary key.

    Ref:
    https://git.gnupg.org/cgi-bin/gitweb.cgi?p=gnupg.git;a=blob;f=doc/DETAILS;hb=HEAD#l482
    """
    for line in output.splitlines():
        data = line.split()
        if data[1] != 'VALIDSIG':
            continue

        # if signed with a subkey, this contains the primary key fingerprint
        data_id = 11 if len(data) == 11 else 2
        return data[data_id]
# ---
def _mk(remat_policy: bool | str | "ScanCheckpointPolicy") -> "ScanCheckpointPolicy":
        if isinstance(remat_policy, ScanCheckpointPolicy):
            return remat_policy
        else:
            return ScanCheckpointPolicy.from_bool_or_str(remat_policy)
# ---
def _default_jax_compilation_cache_dir() -> str:
    marin_prefix = os.environ.get("MARIN_PREFIX")
    if marin_prefix:
        return os.path.join(marin_prefix, "compilation-cache")
    return "/tmp/marin-jax-compilation-cache"
# ---
def _default_move_type(self, cr, uid, context=None):
        """ Gets default type of move
        @return: type
        """
        if context is None:
            context = {}
        picking_type = context.get('picking_type')
        type = 'internal'
        if picking_type == 'in':
            type = 'in'
        elif picking_type == 'out':
            type = 'out'
        return type
# ---
def objective(params):
        a, b, c = params
        pred = a * L**2 + b * L + c
        residuals = y - pred
        return jnp.sum(huber(residuals))
# ---
def run():
    """

    """
    # figure 1 from dudko 2008
    data = Example_Data.Dudko2008Fig1_Probabilities()
    PlotFit(data,"../Out/Dudko2008_Fig1")
    # figure 2 frm dudko 2008
    data = Example_Data.Dudko2008Fig2_Probabilities()
    PlotFit(data,"../Out/Dudko2008_Fig2")
# ---
def isneginf(a: A) -> A:
    return wrap_elemwise_unary(jnp.isneginf, a)
# ---
def fullscreen(self):
        """True if the window is currently fullscreen.  Read-only.

        :type: bool
        """
        return self._fullscreen
# ---
def cd(self, name: str) -> "InputName":
        return InputName(self.step, name=os.path.join(self.name, name) if self.name else name)
# ---
def Tournament(self):
        return self.__data['cls_tournament']
# ---
def __init__(self):
        super(KanboardShell, self).__init__(
            description='Kanboard Command Line Client',
            version=app_version.VersionInfo('kanboard_cli').version_string(),
            command_manager=commandmanager.CommandManager('kanboard.cli'),
            deferred_help=True)
        self.client = None
        self.is_super_user = True
# ---
def get_curr_value(invalue, val_type):
        '''return the current value'''
        if invalue is None:
            return None

        curr_value = invalue
        if val_type == 'yaml':
            curr_value = yaml.load(invalue)
        elif val_type == 'json':
            curr_value = json.loads(invalue)

        return curr_value
# ---
def __delete__(self, obj):
        if hasattr(obj, self.slot):
            delattr(obj, self.slot)
# ---
def find_repo_root(start: Path | None = None) -> Path:
    """Return the nearest parent containing a .git directory."""
    here = (start or Path(__file__)).resolve()
    for p in [here] + list(here.parents):
        if (p / ".git").exists():
            return p
    return Path.cwd()
# ---
def _tree_random_like(rng_key: chex.PRNGKey, target_tree: chex.ArrayTree, dtype=None) -> chex.ArrayTree:
    # adopted from optax
    tree_def = jax.tree.structure(target_tree)
    keys = jax.random.split(rng_key, tree_def.num_leaves)
    keys_tree = jax.tree.unflatten(tree_def, keys)
    return jax.tree.map(
        lambda target_array, key: jax.random.normal(
            key, target_array.shape, dtype if dtype is not None else target_array.dtype
        ),
        target_tree,
        keys_tree,
    )
# ---
def matches_target(self, key_path):
        if isinstance(self.target_modules, str):
            compiled = re.compile(self.target_modules)
            return compiled.match(key_path) is not None
        elif self.target_modules is None:
            return True
        else:
            return any(key_path.endswith(target) for target in self.target_modules)
# ---
def trainables_only(model, filter):
    """
    Filters out non-trainable parameters from the model. This is used internally to
    for the optimizer state and to compute gradients, but you can also use it to filter out
    params for logging or something.
    """
    return _partition_trainable_params(model, filter)[0]
# ---
def addExclusions(nonbondedforce, pairlist):
  """ add nonbonded exclusions between pairs """
  for i,j in pairlist:
    nonbondedforce.addExclusion(i,j)
# ---
def release(queue_name: str, lease_id: str = Body(...), timestamp: float = Body(...)):
            if queue_name not in self.queues:
                return Response(status_code=404)
            lease = Lease(item=None, lease_id=lease_id, timestamp=timestamp)
            self.queues[queue_name].release(lease)
            return {"status": "ok"}
# ---


def common(l1: list, l2: list):
    """Return sorted unique common elements for two lists.
    >>> common([1, 4, 3, 34, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121])
    [1, 5, 653]
    >>> common([5, 3, 2, 8], [3, 2])
    [2, 3]

    """
    ret = set()
    for e1 in l1:
        for e2 in l2:
            if e1 == e2:
                ret.add(e1)
    return sorted(list(ret))
# ---
def is_too_large(size: int) -> bool:
    return server.config.max_size is not None and size > server.config.max_size
# ---
def playlist_remove(self, index='current'):
        self.command('playlist_remove', index)
# ---
def mock_render_to_string(template_name, context):
    """Return a string that encodes template_name and context"""
    return str((template_name, context))
# ---
def gen_signature(self, privkey, pubkey, sig_path):
        '''
        Generate master public-key-signature
        '''
        return salt.crypt.gen_signature(privkey,
                                        pubkey,
                                        sig_path)
# ---
def HealthCheck(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def var(
        self, axis: AxisSelection | None = None, dtype=None, ddof=0, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.var(self, axis=axis, dtype=dtype, ddof=ddof, where=where)
# ---
def get_available_workers(self) -> list[ControllerWorker]:
        with self._lock:
            return [w for w in self._workers.values() if w.healthy]
# ---
def name(dtype):
            if hasattr(dtype, "name"):
                return dtype.name
            elif hasattr(dtype, "dtype"):
                return name(dtype.dtype)
# ---
def _get_gemma_config(use_flash=False, num_kv_heads=4, seq_len=128) -> GemmaConfig:
    return GemmaConfig(
        max_seq_len=seq_len,
        hidden_dim=16,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
        head_dim=4,
    )
# ---
def get_directory_friendly_name(name: str) -> str:
    """Convert a huggingface repo name to a directory friendly name."""
    return name.replace("/", "--").replace(".", "-").replace("#", "-")
# ---
def norm_config(self) -> RmsNormConfig:
        return RmsNormConfig(
            eps=self.layer_norm_epsilon,
            use_weight=self.use_layer_norm_weight,
            use_bias=self.use_bias,
        )
# ---
def __contains__(self, key):
        return SummaryKeyMatcher.cNamespace().match_key(self, key)
# ---
def publisher(self, val):
        self.opt_meta['publisher'] = _EpubMeta('dc:publisher', '' + val)
# ---
def _call_fn(self, indices: Sequence[int], items):
        if "key" in self._extra_kwargs:
            key = self._maybe_fold_in_key(self._extra_kwargs["key"], indices)
            kwargs = {**self._extra_kwargs, "key": key}
        else:
            kwargs = self._extra_kwargs
        return self.fn(items, *self._extra_args, **kwargs)
# ---
def dim2loss(d, dim0=max_dim):
        """A heuristic map from dim to loss with the least loss occurs at dim0."""
        loss = 0
        if d < dim0:
            loss += np.log2(dim0 / d)
            too_small = dim0 / 8
            if d < too_small:
                loss += 100 * np.log2(too_small / d)
        else:
            loss += 10 * np.log2(d / dim0)
            too_large = 8 * dim0
            if d > too_large:
                loss += 1000 * np.log2(d / too_large)
        return loss
# ---
def fold(
    fn: Callable[[Carry, X], Carry],
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable[[Carry, PyTree[X]], Carry]: ...
# ---
def foreign_tasks(self, tasks, person, roles):
        """List of other instructors' tasks, per event."""
        return [
            task.event.task_set.filter(role__in=roles)
                               .exclude(person=person)
                               .select_related('person')
            for task in tasks
        ]
# ---
def test_combined_scenarios():
    """Tests with multiple special characters."""
    input_text = r"""This is *bold* and _italic_.
    # Header 1
    - List item 1
    + List item 2
    This has [brackets] and \backslashes\.
    """
    expected_text = r"""This is \*bold\* and \_italic\_.
    \# Header 1
    \- List item 1
    \+ List item 2
    This has \[brackets\] and \backslashes\\.
    """
    assert expected_text == minimal_markdown_escape(input_text)
# ---
def testFloatRandomTransposeBoth(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(k, n, np.float32)
      y = self._randMatrix(m, k, np.float32)
      self._testCpuMatmul(x, y, True, True)
      self._testGpuMatmul(x, y, True, True)
# ---
def handleNode(currentNodeInAction, referenceNodeNow, referencesToCheck, patchMessageReferenceNode):
    for reference in referencesToCheck[:] :
        if reference in referenceNodeNow.get_children() :
            referencesToCheck.remove(reference)
            return patchMessageReferenceNode[reference]
    if len(referencesToCheck) == 0 :
        referenceNodeNow.get_children()[currentNodeInAction.get_node()] = currentNodeInAction
# ---
def __getattr__(self, attr):
		if attr in self.structure:
			return self._get_value(attr)

		if attr in self.structure._abstractions: # Union abstractions etc
			field, func = self.structure._abstractions[attr]
			return func(field, self)

		if "__" in attr:
			return self._query(attr)

		return super(DBRow, self).__getattribute__(attr)
# ---
def _logs_page(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Iris Logs", "/static/worker/logs-page.js"))
# ---
def loadlist(self, playlist, mode='replace'):
        self.command('loadlist', playlist.encode(fs_enc), mode)
# ---
def run(self, fn: Callable, *args, name: str | None = None) -> Any:
        """Execute a function with arguments and return a future.

        Args:
            fn: Function to execute
            *args: Arguments to pass to function
            name: Optional task name for debugging/monitoring (used by RayContext)

        Returns:
            Future representing the execution (type depends on context)
        """
        ...
# ---
def test_logical_or(self):
        expr = (col("a") > 0) | (col("b") > 0)
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def uniquePairs(index):
  """ list of unique, internal pairs """
  return list(combinations( range(index[0],index[-1]+1),2 ) )
# ---
def is_scanned_with_axis(leaf):
        if is_named_array(leaf):
            return selects_axis(leaf.axes, axis) and is_scanned(leaf)
        else:
            return is_scanned(leaf)
# ---
def all_ready(self) -> bool:
        """True if all VMs in the group are in READY state."""
        return all(v.state == vm_pb2.VM_STATE_READY for v in self.vms)
# ---
def __init__(self, max_workers: int):
        """Initialize thread pool context.

        Args:
            max_workers: Maximum number of worker threads
        """
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self._actors: dict[str, Any] = {}  # name -> instance
        self._actor_locks: dict[str, threading.Lock] = {}  # per-actor locks
        self._actors_lock = threading.Lock()
# ---
def test_get_lines_with_long_text_string():
        text = "This is a test string, which should simulate real text. The command should" \
         + " correctly split this text into two lines."
        LINEWIDTH = 80
        correct_lines = [text[:LINEWIDTH], text[LINEWIDTH:]]
        assert len(get_lines(text)) == len(text) // LINEWIDTH + 1
        assert get_lines(text) == correct_lines
# ---
def tree_unflatten(cls, aux, tree: Any) -> Any:
        assert len(tree) == 1
        return cls(tree[0], main_axes=aux)
# ---
def tracker2(name):
            def go(*args, **kw):
                canary2.append(name)
            return go
# ---
def update_fn(updates, state, params=None):
        new_state = {"count": state["count"] + 1}
        # simple transformation: negate gradients
        transformed_updates = jax.tree_util.tree_map(lambda g: -g, updates)
        return transformed_updates, new_state
# ---
def mark_requesting(self, timestamp: Timestamp, timeout: Duration) -> None:
        """Mark this group as REQUESTING (scale-up in progress).

        Args:
            timestamp: Current timestamp
            timeout: How long to stay in REQUESTING state
        """
        self._requesting_until = timestamp.add(timeout)
# ---
def __getitem__(self, key: str) -> FieldAccessExpr:
        return FieldAccessExpr(self, key)
# ---
def _make_abstract_mesh(*, data: int, model: int) -> AbstractMesh:
    return AbstractMesh(
        axis_sizes=(data, model),
        axis_names=("data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit),
    )
# ---
def extract_seed(prng_key) -> int:
    """Extract an integer seed from either a JAX PRNG key or an integer."""
    if isinstance(prng_key, int):
        return prng_key
    # It's a JAX key - extract seed using JAX
    return jax.random.randint(prng_key, (), 0, 1_000_000).item()
# ---
def exp2(a: A) -> A:
    return wrap_elemwise_unary(jnp.exp2, a)
# ---
def start_recording(self):
        self.recording_enabled = True
        self.startrecordingAction.setEnabled(False)
        self.stoprecordingAction.setEnabled(True)
# ---
def task_index(self) -> int:
        return self.task_id.require_task()[1]
# ---
def num_shards(self) -> int:
        """Total number of logical shards in the plan.

        This counts unique shard_idx values, not total SourceItems. When
        intra-shard parallelism is enabled, multiple SourceItems may belong
        to the same logical shard.
        """
        if not self.source_items:
            return 0
        return len({item.shard_idx for item in self.source_items})
# ---
def __repr__(self) -> str:
        return f"Duration({self.to_seconds():.3f}s)"
# ---
from typing import List


def filter_by_substring(strings: List[str], substring: str) -> List[str]:
    """ Filter an input list of strings only for ones that contain given substring
    >>> filter_by_substring([], 'a')
    []
    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')
    ['abc', 'bacd', 'array']
    """
    return [x for x in strings if substring in x]
# ---
def make_snapshot(
    vm_id: str = "vm-0",
    state: vm_pb2.VmState = vm_pb2.VM_STATE_READY,
    address: str = "10.0.0.1",
    init_phase: str = "",
    init_error: str = "",
) -> VmSnapshot:
    """Create a VmSnapshot for testing."""
    return VmSnapshot(
        vm_id=vm_id,
        state=state,
        address=address,
        init_phase=init_phase,
        init_error=init_error,
    )
# ---

def generate_integers(a, b):
    """
    Given two positive integers a and b, return the even digits between a
    and b, in ascending order.

    For example:
    generate_integers(2, 8) => [2, 4, 6, 8]
    generate_integers(8, 2) => [2, 4, 6, 8]
    generate_integers(10, 14) => []
    """
    lower = max(2, min(a, b))
    upper = min(8, max(a, b))

    return [i for i in range(lower, upper+1) if i % 2 == 0]
# ---
def __init__(self):
        super(FIFOQueue, self).__init__()
# ---
def increment_counting(self, event):
        """Counts an event

        Args:
            event (:obj:`baroque.entities.event.Event`): the event to be counted

        """
        assert isinstance(event, Event)
        self.events_count += 1
        t = type(event.type)
        if t in self.events_count_by_type:
            self.events_count_by_type[t] += 1
        else:
            self.events_count_by_type[t] = 1
# ---
def add_env_value(self, key, value):
        ''' add key, value pair to env array '''
        rval = False
        env = self.get_env_vars()
        if env:
            env.append({'name': key, 'value': value})
            rval = True
        else:
            result = self.put(DeploymentConfig.env_path, {'name': key, 'value': value})
            rval = result[0]

        return rval
# ---
def url_dequery(url):
    """Return a URL with the query component removed.

    :param url: URL to dequery.
    :type url: str
    :rtype: str
    """
    url = urlparse(url)
    return urlunparse((url.scheme,
                                url.netloc,
                                url.path,
                                url.params,
                                '',
                                url.fragment))
# ---
def name(self):
        return self.__data['str_name']
# ---
def _ensure_scalar(x: hax.types.Scalar | hax.NamedArray) -> hax.types.Scalar:
    if isinstance(x, hax.NamedArray):
        return x.scalar()
    else:
        return x
# ---
def enroll_student(self, email, password, course):
        """
        Student login and enroll for the course
        """
        self.login(email, password)
        self.enroll(course, verify=True)
# ---
def notnull_errcheck(res, func, *args):
    if res is None:
        raise RuntimeError('Underspecified error in MPV when calling {} with args {!r}: NULL pointer returned.'\
                'Please consult your local debugger.'.format(func.__name__, args))
    return res
# ---
def test_mem_write_byte_attribute_before_char(self):
        self.mda.mem_write_byte(3999, MDA_ATTR_INTENSITY)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x00, MDA_BRIGHT_GREEN, MDA_BLACK))
        self.mda.mem_write_byte(3998, 0xFF)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def testImportGrumpy(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        from "__go__/grumpy" import Assert
        Assert(__frame__(), True, 'bad')""")))
# ---
def _log_samples_hook(info: levanter.callbacks.StepInfo):
            rollouts = self.data_loader._last_rollouts
            if rollouts is not None:
                self._log_samples(trainer, info.step, rollouts)
# ---
def convert_to_local_time(utc_str: str) -> str:
    # Parse the UTC string to a datetime object
    utc_dt = datetime.strptime(utc_str, "%Y-%m-%dT%H:%M:%S")

    # Set the timezone to UTC
    utc_dt = utc_dt.replace(tzinfo=pytz.UTC)

    # Convert to Pacific Time
    pacific_tz = pytz.timezone("America/Los_Angeles")
    pacific_dt = utc_dt.astimezone(pacific_tz)

    # Format the result as a string (optional)
    pacific_str = pacific_dt.strftime("%Y-%m-%d %H:%M:%S %Z")
    return pacific_str
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        # https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_rope_utils.py#L307
        # Porting that to JAX/Haliax:
        return Llama3RotaryEmbeddings.init(HeadSize, self)
# ---
def reset(self):
        self._count = 0
        self.requests = []
# ---
def groups(self) -> dict[str, ScalingGroup]:
        """All scale groups."""
        return self._groups
# ---
def comment(self, params=None):
        if params is None:
            params = dict()
        params['sale_id'] = self.sale_id
        return Sale(Api.call('sales/create_comment', params))
# ---
def __exit__(self, *args) -> None:
        self.shutdown()
# ---
def testFormatUsername(self):
    """Tests the _FormatUsername function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    username_string = test_helper._FormatUsername(
        event, event_data, event_data_stream)
    self.assertEqual(username_string, '-')
# ---
def log(self, message: str, level: str = "INFO"):
        now = datetime.now()
        elapsed = time.monotonic() - self._start_time
        line = f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] [{elapsed:8.1f}s] [{level}] {message}"
        print(line, flush=True)
        self._file.write(line + "\n")
        self._file.flush()
# ---
def _thrift_binary(self):
    thrift_binary = ThriftBinary.Factory.scoped_instance(self).create()
    return thrift_binary.path
# ---
def close_pane(self, index):
        was_selected = index == self.selected_pane_index
        del self.panes[index]
        if not self.panes:
            self.view.close()
            return
        self._selected_pane_index = min(self._selected_pane_index, len(self.panes) - 1)
        if was_selected:
            self._update_selected_pane()
# ---
def cumprod(x, axis=0):
    return _Ncumprod(x, axis)
# ---
def test_write_parquet_file_empty():
    """Test writing an empty parquet file."""
    with tempfile.TemporaryDirectory() as tmpdir:
        output_path = str(Path(tmpdir) / "empty.parquet")
        records = []

        result = write_parquet_file(records, output_path)

        assert result["path"] == output_path
        assert result["count"] == 0
        assert Path(output_path).exists()

        table = pq.read_table(output_path)
        assert len(table) == 0
# ---
def log_summary(self, metrics: typing.Mapping[str, Any]):
        self.run.summary.update(_convert_value_to_loggable_rec(metrics))
# ---
def gameboard(size, seed=""):
        if size > 20:
            size = 20
        return render_template("gameboard.html", size=size, seed=seed)
# ---
def _extend_and_expr(self, and_expr, _and, check):
        """Extend an 'and_expr' by adding one more check."""

        return [('and_expr', and_expr.add_check(check))]
# ---
def fix_args():
  global get_next_file

  if PARAMS["PATH"][-1] != "/":
    PARAMS["PATH"] += "/"

  if PARAMS["FORCE_NO_VFS"]:
    PARAMS["USE_VFS"]
  elif PARAMS["FORCE_NO_VFS"]:
    PARAMS["USE_VFS"]
  else:
    PARAMS["USE_VFS"] = path_is_jnetfs(PARAMS["PATH"])

  if not PARAMS["USE_VFS"]:
    get_next_file = GetFileDir(PARAMS["PICK_RANDOM"]).get_next_file
  else:
    get_next_file = get_next_file_vfs
# ---
def _update_block_number_mapping(self, block, batch):
        block_number_mapping = qrl_pb2.BlockNumberMapping(headerhash=block.headerhash,
                                                          prev_headerhash=block.prev_headerhash)
        self._state.put_block_number_mapping(block.block_number, block_number_mapping, batch)
# ---
def decodeSuccess(self, seg):
        self.status.current_bytes += seg.size
        self.segments_finished.append(seg.msgid)
        if ( (len(self.segments_finished)+len(self.segments_aborted)) >= len(self.segment_list) ):
            self.all_decoded = True
# ---
def plot_adjusted_rand_index_vs_assigned_bps(colors, summary_per_query, labels, output_dir, rank=None):
    plot_summary(colors,
                 summary_per_query,
                 labels,
                 output_dir,
                 rank,
                 'p',
                 'ari_vs_assigned_bps',
                 'Adjusted Rand index',
                 'Percentage of binned base pairs')
# ---
def get_children(self, job_id: JobName) -> list[ControllerJob]:
        with self._lock:
            return [job for job in self._jobs.values() if job.parent_job_id == job_id]
# ---
def mean(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.mean, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype)
# ---
def test_connect_to_region_creds(self):
        """Can connect to a dynamo region with credentials"""
        conn = DynamoDBConnection.connect(
            "us-west-1", access_key="abc", secret_key="12345"
        )
        self.assertIsNotNone(conn.host)
# ---
def test_fn():
        print("Hello from test task!")
        return 42
# ---
def resolve_path(path: str) -> str:
    """Resolve a path to an absolute path, except for cloud storage paths."""
    return path if path.startswith(CLOUD_STORAGE_PREFIXES) else os.path.realpath(path)
# ---
def __list_methods(self):
        return self.__list_handlers().keys() + self.__base_methods.keys()
# ---
def read(self):
        '''
        Read and parse MIDI data stored in a file.
        '''
        self.readstr(self.file.read())
# ---
def create_router_postcommit(self, context, router_context):
        pass
# ---
def handle_shutdown(_signum, _frame):
        logger.info("Shutdown signal received, stopping controller...")
        controller.stop()
        logger.info("Controller stopped")
        stop_event.set()
# ---
def clearCache(self):
        mt.utils.rmdir(self.cache_path)
# ---
def setup(self, memcached, memcached_cluster):
        self.storage_url = "memcached://localhost:22122"
# ---
def test_get_logs_nonexistent_task(worker):
    """Test getting logs for nonexistent task returns empty list."""
    logs = worker.get_logs(JobName.root("nonexistent-task").task(0).to_wire())
    assert logs == []
# ---
def test_cauchy():
    check_gen_is_equal(lambda k, s: jax.random.cauchy(k, s), lambda k, s: hax.random.cauchy(k, s))
# ---
def _to_unbatched_named_array(axis_to_strip: AxisSelector):
    def to_unbatched_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            if selects_axis(leaf.main_axes, axis_to_strip):
                return leaf.strip_axis(axis_to_strip)
            else:
                return leaf.to_named_array()
        else:
            return leaf

    return to_unbatched_named_array
# ---
def testForElseBreakNotNested(self):
    self.assertRaisesRegexp(
        util.ParseError, "'continue' not in loop",
        _ParseAndVisit, 'for i in (1,):\n  pass\nelse:\n  continue')
# ---
def is_not_a_branch(git_path, module, dest):
    branches = get_branches(git_path, module, dest)
    for branch in branches:
        if branch.startswith('* ') and ('no branch' in branch or 'detached from' in branch or 'detached at' in branch):
            return True
    return False
# ---
def is_set(self) -> bool:
        return os.path.exists(self._path)
# ---
def test_mem_write_word_at_bottom_right(self):
        self.mda.mem_write_word(3998, 0x085A) # 'Z' with intensity.
        self.assertEqual(self.mda.video_ram[3998], 0x5A)
        self.assertEqual(self.mda.video_ram[3999], 0x08)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x5A, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def _relative_positions(seg_ids):
        idx = jnp.arange(seg_ids.shape[0])
        is_start = jnp.concatenate([jnp.array([True]), seg_ids[1:] != seg_ids[:-1]])
        start_idx = idx * is_start.astype(idx.dtype)
        seg_start = jax.lax.associative_scan(jnp.maximum, start_idx)
        return idx - seg_start
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"stacked": None}
# ---
def test_select_partial_columns(backend):
    """Test select with columns that don't exist in all records."""
    ds = Dataset.from_list(
        [
            {"id": 1, "name": "alice"},
            {"id": 2, "name": "bob", "score": 60},
        ]
    ).select("id", "score")

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert results[0] == {"id": 1}
    assert results[1] == {"id": 2, "score": 60}
# ---
def PlotFit(data,BaseName):
    fig = Example_Data.PlotHistograms(data)
    fig.savefig(BaseName + "_Histogram.png")
    fig = Example_Data.PlotLifetimesAndFit(data)
    fig.savefig(BaseName + "_Lifetimes.png")
# ---
def setUp(self):
        super().setUp()

        self.user = User.objects.create()
        permission = Permission.objects.get(codename='search')
        self.user.user_permissions.add(permission)
        self.client.force_authenticate(user=self.user)
# ---
def __init__(self,name,arguments,as_written="",position=0):
        self.arguments = arguments
        self.number_of_arguments = len(arguments)
        self.name = name
        self.as_written = as_written
        self.arguments_list = arguments
        self.position = position
# ---
def build(self, ctx: LrScheduleContext) -> Callable:
        raise NotImplementedError
# ---
def is_finished(self) -> bool:
        return self.state in (
            cluster_pb2.JOB_STATE_SUCCEEDED,
            cluster_pb2.JOB_STATE_FAILED,
            cluster_pb2.JOB_STATE_KILLED,
            cluster_pb2.JOB_STATE_UNSCHEDULABLE,
        )
# ---
def prep_signature(self):
        self._call_all('prep_signature')
# ---
def KHeadDim(self) -> Axis:
        return Axis("k_head_dim", self.head_k_dim)
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> ListCache[KvPageCache]:
        """
        Creates an initial cache for this model. Note that in order to create a decoder state, you
        need to couple the KvPageCache to the PageTable's state with a BatchInfo object.
        """
        return hax.auto_sharded(self.transformer.initial_cache(spec, dtype=dtype))
# ---
def load_requests(requests_file: str) -> list[dict[str, Any]]:
    """Load requests from JSON file."""
    logger.info(f"Loading requests from {requests_file}")
    with open(requests_file, "r") as f:
        requests = json.load(f)
    logger.info(f"Loaded {len(requests)} chat completion requests")
    return requests
# ---
def test_sharded_tree_size_tuple_axis_partition_spec():
    mesh = jax.sharding.AbstractMesh((2, 2), ("data", "model"))
    spec = jax.sharding.PartitionSpec(("data", "model"), None)
    sharding = jax.sharding.NamedSharding(mesh, spec)

    struct = jax.ShapeDtypeStruct((8, 4), jnp.float32, sharding=sharding)

    per_device_bytes = sharded_tree_size(struct, mesh=mesh)

    assert per_device_bytes == (8 * 4 * jnp.dtype(jnp.float32).itemsize) // 4
# ---
def intersects(self, bbox, srs):
        return any(c.intersects(bbox, srs) for c in self.coverages)
# ---
def ensure_versioned(value: VersionedValue[T_co] | T_co) -> VersionedValue[T_co]:
    """
    Ensure that the value is wrapped in a VersionedValue. If it is already wrapped, return it as is.
    """
    return value if isinstance(value, VersionedValue) else VersionedValue(value)
# ---
def __init__(self, num_shards: int = 4):
        self._num_shards = num_shards
# ---
def go(*arg, **kw):
                    canary.append(fn.__name__)
                    return fn(*arg, **kw)
# ---
def execute(conn, *args, **kw):
            canary.append('execute')
# ---
def Embed(self) -> Axis:
        return Axis("embed", self.grug_config.hidden_dim)
# ---
def get_logger(section, name):
    """
    Fetches a logger.

    Arguments:
        section (string): The section the logger is attributed to.
        name (string): The name of the logger.

    Returns:
        The logger corresponding to the section and name provided.
    """
    section_name = LoggingSection.reverse_mapping[section].lower()

    logger = logging.getLogger('htresearch.{0}.{1}'.format(section_name, name))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

    return logger
# ---
def __init__(self, connection):
            self.standard = self.Standard(connection)
# ---
def __enter__(self) -> "JobGroup":
        self._entered = True
        return self
# ---
def fit_baseline_phase(self,z_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		'''
		return self._baseline_als(np.angle(z_data),lam,p,niter=niter)
# ---
def _on_worker_heartbeat(self, txn: TransactionLog, event: WorkerHeartbeatEvent) -> None:
        worker = self._workers[event.worker_id]
        worker.last_heartbeat = event.timestamp
        worker.healthy = True
        worker.consecutive_failures = 0
        txn.log("heartbeat", event.worker_id)
# ---
def select_if_missing(missing_leaf, new_value):
            if isinstance(missing_leaf, jax.ShapeDtypeStruct):
                return new_value
            else:
                return None
# ---
def __mul__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "mul")
# ---
def JOB_STATES(state):
    if state == 'failed':
        return BOLD() + RED() + state + ENDC()
    elif state == 'done':
        return BOLD() + GREEN() + state + ENDC()
    elif state in ['running', 'in_progress']:
        return GREEN() + state + ENDC()
    elif state == 'partially_failed':
        return RED() + state + ENDC()
    else:
        return YELLOW() + state + ENDC()
# ---
def get_concurrency(cfg):
    """Return the Reserved Concurrent Executions if present in the config"""
    concurrency = int(cfg.get("concurrency", 0))
    return max(0, concurrency)
# ---
def select_0th(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.take(axis, 0)
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            # other leaves don't matter
            return leaf
# ---
def test_check_share_in_use_invalid_conn(self):
        drv = self._driver
        share = drv._check_share_in_use(':8989', '/dir')
        if share:
            self.fail('Unexpected share detected.')
# ---
def restart(self) -> str:
        """Stop then start controller."""
        ...
# ---
def _handle_func(name, args, restype, errcheck, ctx=MpvHandle):
    func = getattr(backend, name)
    func.argtypes = [ctx] + args if ctx else args
    if restype is not None:
        func.restype = restype
    if errcheck is not None:
        func.errcheck = errcheck
    globals()['_'+name] = func
# ---
def disruption_genetic_modification(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'CRISPR cutting',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def test_startswith_autoescape_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab%c", autoescape=True, escape="#"), {3})
        self._test(col.startswith("ab#c", autoescape=True, escape="#"), {7})
# ---
def poke_partition(partition):

            schema, table, partition = self.parse_partition_name(partition)

            logging.info(
                'Poking for {schema}.{table}/{partition}'.format(**locals())
            )
            return self.hook.check_for_named_partition(
                schema, table, partition)
# ---
def terminate(self) -> None:
        """Terminate this job."""
        self._client._cluster_client.terminate_job(self._job_id)
# ---
def test_select_exists_false(self, connection):
        stuff = self.tables.stuff
        eq_(
            connection.execute(
                select([literal(1)]).where(
                    exists().where(stuff.c.data == "no data")
                )
            ).fetchall(),
            [],
        )
# ---
def __init__(
        self,
        config: config_pb2.IrisClusterConfig,
        threads: ThreadContainer | None = None,
    ):
        self._config = config
        self._threads = threads if threads is not None else get_thread_container()
        self._controller: ControllerProtocol | None = None
# ---
def read(*paths):
    """Build a file path from *paths* and return the contents."""
    with open(os.path.join(*paths), 'r') as f:
        return f.read()
# ---
def has_value(self, attr):
        if attr in self.reverse_lookup:
            return True
        return False
# ---
def build(self, axis: AxisSpec) -> LayerNorm:
        return LayerNorm.init(axis, eps=self.eps, use_weight=self.use_weight, use_bias=self.use_bias)
# ---
def test_stage_name():
    """PhysicalStage.stage_name() generates descriptive names from operations."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            FilterOp(lambda x: x > 5),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    assert plan.stages[0].stage_name() == "Map"
# ---
def test_score_candidate_partial_pass():
    candidate = _make_candidate("def f(x):\n    return x\n")
    tests = ["assert f(1) == 1", "assert f(2) == 3"]
    result = score_candidate(candidate, tests)
    assert result.tests_passed == 1
    assert result.tests_total == 2
    assert 0.0 < result.test_pass_rate < 1.0
# ---
def page_number_gen(self):
        """Generate pages numbers from specified page intervals."""
        last = 0
        for start, end in sorted(self._pages):
            start = max(last, start)
            last = end + 1
            yield from range(start, last)
# ---
def poll(self) -> int | None:
        if self.is_alive():
            return None
        # Return non-zero exit code if thread raised an exception
        return 1 if self._exception is not None else 0
# ---
def left_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.left_shift](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.left_shift.html)
    """
    return jnp.left_shift(x1, x2)
# ---
def clause(self):
        return self.clause_with_joiner('or')
# ---
def writemypid(pidfile):
    pid = str(os.getpid())
    with open(pidfile, 'w') as f:
        f.write(pid)
    f.close
# ---
def _get_storage_domain_path(self, path):
        '''
        prepareImage returns /prefix/sdUUID/images/imgUUID/volUUID
        we need storage domain absolute path so we go up 3 levels
        '''
        return path.rsplit(os.sep, 3)[0]
# ---
def out_first(self):
        """
        Returns: bool: Whether the output axes are first in the weight matrix
        """
        # We do it this way because of scan layers
        if isinstance(self.Out, hax.Axis):
            return self.weight.axes[-1] != self.Out
        else:
            return self.weight.axes[-len(self.Out) :] != self.Out
# ---
def _validate_accelerator_types(config: config_pb2.IrisClusterConfig) -> None:
    """Validate that scale groups have explicit accelerator types."""
    for name, sg_config in config.scale_groups.items():
        if sg_config.accelerator_type == config_pb2.ACCELERATOR_TYPE_UNSPECIFIED:
            raise ValueError(f"Scale group '{name}' must set accelerator_type to cpu, gpu, or tpu.")
# ---
def post(self):
            pass
# ---
def test_forward_output_dtype_is_float32(params, tiny_cfg):
    """Logits should always be float32 regardless of compute_dtype."""
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (1, 8), 1, 100)
    logits = forward(params, token_ids, tiny_cfg)
    assert logits.dtype == jnp.float32
# ---
def test_contains_autoescape_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b%cd", autoescape=True, escape="#"), {3})
        self._test(col.contains("b#cd", autoescape=True, escape="#"), {7})
# ---
def clear_cmd():
    """Clear the screen."""
    import os

    os.system("clear" if os.name == "posix" else "cls")
# ---
def setUp(self):
        self.latitude = 32.074322
        self.longitude = 34.792081
        self.radius_meters = 100
        self.number_of_vertices = 36
        self.polycircle = \
            polycircles.Polycircle(latitude=self.latitude,
                                   longitude=self.longitude,
                                   radius=self.radius_meters,
                                   number_of_vertices=self.number_of_vertices)
# ---
def get_autoscaler_status(
        self,
        request: cluster_pb2.Controller.GetAutoscalerStatusRequest,
        ctx: Any,
    ) -> cluster_pb2.Controller.GetAutoscalerStatusResponse:
        """Get current autoscaler status."""
        autoscaler = self._scheduler.autoscaler
        if not autoscaler:
            return cluster_pb2.Controller.GetAutoscalerStatusResponse(status=vm_pb2.AutoscalerStatus())

        return cluster_pb2.Controller.GetAutoscalerStatusResponse(status=autoscaler.get_status())
# ---
def saved_fd(fd):
    new_fd = os.dup(fd)
    try:
        yield
    finally:
        os.dup2(new_fd, fd)
        os.close(new_fd)
# ---
def _dnsname_to_pat(dn):
        pats = []
        for frag in dn.split(r'.'):
            if frag == '*':
                # When '*' is a fragment by itself, it matches a non-empty dotless
                # fragment.
                pats.append('[^.]+')
            else:
                # Otherwise, '*' matches any dotless fragment.
                frag = re.escape(frag)
                pats.append(frag.replace(r'\*', '[^.]*'))
        return re.compile(r'\A' + r'\.'.join(pats) + r'\Z', re.IGNORECASE)
# ---
def get_img_info(self, format):
        class img_info(object):
            def __init__(self, fmt):
                self.file_format = fmt

        return img_info(format)
# ---
def action_explode(self, cr, uid, moves, context=None):
        """Hook to allow other modules to split the moves of a picking."""
        return moves
# ---
def __default(self, param):
        attr1 = getattr(self.want, param)
        try:
            attr2 = getattr(self.have, param)
            if attr1 != attr2:
                return attr1
        except AttributeError:
            return attr1
# ---
def __init__(self, ray_options: dict | None = None):
        """Initialize Ray context.

        Args:
            ray_options: Options to pass to ray.remote() (e.g., memory, num_cpus, num_gpus)
        """
        self.ray_options = ray_options or {}
# ---
def _tqdm_logging_one_time_setup():
    global _did_tqdm_logging_one_time_setup
    if _did_tqdm_logging_one_time_setup:
        return
    _did_tqdm_logging_one_time_setup = True
    tqdm_logging.set_log_rate(timedelta(seconds=60))
# ---
def abort(self):
        self._status = STATUS.ABORTED
        logging.info('Job %r aborting...', self._id)
        self._abort()
# ---
def _msgprint(msg, verbose):
	if verbose:
		msgprint(msg, raise_exception=True)
	else:
		raise frappe.ValidationError(msg)
# ---
def lt(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def create_actor(self) -> ActorHandle:
        return SliceActor.options(resources={f"TPU-{self._tpu_type}-head": 1}).remote()
# ---
def test_non_dict_parent(self):
        expr = col("meta")["score"]
        assert expr.evaluate({"meta": "not a dict"}) is None
# ---
def hypot(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.hypot](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.hypot.html)
    """
    return jnp.hypot(x1, x2)
# ---
def _str_is_int(x_str: str) -> bool:
    try:
        x_str = _strip_properly_formatted_commas(x_str)
        x = float(x_str)
        return abs(x - int(round(x))) <= 1e-7
    except (ValueError, OverflowError):
        return False
# ---
def key():
    return jax.random.PRNGKey(42)
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[dict]:
        url = self._shard_name_to_url_mapping[shard_name]
        i = 0
        with fsspec.open(url, "r", compression="infer") as f:
            # TODO: would be nice if we could seek faster than this. Right now, all we do is skip json parsing
            # which is not nothing, but not ideal.
            for line in f:
                if i >= row:
                    yield json.loads(line)
                i += 1
# ---
def is_on_mac_metal():
    return jax.devices()[0].platform.lower() == "metal"
# ---
def _all_done() -> bool:
            for rid, n_kids in expected_children.items():
                kid_map = self.results.get(rid, {})
                for cid in range(n_kids):
                    dr = kid_map.get(cid)
                    if dr is None or not dr.done:
                        return False
            return True
# ---
def uadd(cls, operand: "PyExpr") -> "PyExpr": ...
# ---
def run_server():
        uvicorn_server.run()
# ---
def url(self):
        return urlunparse(
            (self.scheme, self.host, self.path, None, self.query_string, None)
        )
# ---
def default_choice_name(cls) -> str | None:
        return "cached"
# ---
def local_client():
    """Create a IrisClient for testing."""
    config = LocalClientConfig(max_workers=2)
    with IrisClient.local(config) as client:
        yield client
# ---
def __str__(self):
        return f"IdentityMap({list(str(x) for x in self._data.values())})"
# ---
def sub_reload(self, sub_id=None):
        self.command('sub_reload', sub_id)
# ---
def setUp(self):
        self.client.login(username='admin', password='root')
# ---
def arctanh(a: A) -> A:
    return wrap_elemwise_unary(jnp.arctanh, a)
# ---
def hang():
        stop.wait(10)
# ---
def post_backlog_processing(self, context):
        pass
# ---
def test_empty_insert(self):
        """test that execute() interprets [] as a list with no params"""

        testing.db.execute(users_autoinc.insert().
                    values(user_name=bindparam('name', None)), [])
        eq_(testing.db.execute(users_autoinc.select()).fetchall(), [(1, None)])
# ---
def _set_pdeathsig_preexec():
    """Use prctl(PR_SET_PDEATHSIG, SIGKILL) to kill subprocess if parent dies."""
    PR_SET_PDEATHSIG = 1
    try:
        libc = ctypes.CDLL(ctypes.util.find_library("c"), use_errno=True)
        if libc.prctl(PR_SET_PDEATHSIG, signal.SIGKILL) != 0:
            errno = ctypes.get_errno()
            logger.warning(f"Failed to set parent death signal: errno {errno}")
    except Exception as e:
        logger.info(f"Could not set parent death signal: {e}")
# ---
def due_date(self):
        """ Returns a date object of the todo's due date. """
        return self.get_date(config().tag_due())
# ---
def __init__(self, producer_fn: Callable[[], Iterator[Ex] | AsyncIterator[Ex]], max_capacity: int | None):
        super().__init__(producer_fn, max_capacity)
# ---
def test_named_ref_scalar_update():
    X = hax.Axis("x", 5)
    ref = hax.new_ref(hax.zeros(X))
    ref[{"x": 2}] = 3.14
    assert pytest.approx(ref.value()[{"x": 2}].array.item()) == 3.14
# ---
def test_write_desc(self):
        s = option.SqlStore()
        s.cursor = self.conn.cursor()
        s.write_desc(self.desc)
        print('READING')
        r = s.read_tree()
        print(r)
        print('print(tree\n', print_tree(r))
        print('WRITING AGAIN')
        s.write_tree(r)
        print("READING AGAIN")
        r = s.read_tree()
        print(r)
        print('print(tree2\n', print_tree(r))
# ---
def exists(self):
        '''does the object exist?'''
        if self.deploymentconfig and self.service:
            return True

        return False
# ---
def getName(self):
        return "NoncChi2({0},{1})".format(self.df, self.lmbda)
# ---
def _default(self):
		"""
		Change all fields to their default values
		"""
		del self[:]
		self._values = {}
		for col in self.structure:
			char = col.char
			if col.dyn:
				self.append(None)
			elif char == "s":
				self.append("")
			elif char == "f":
				self.append(0.0)
			else:
				self.append(0)
# ---
def check_permissions(self, request):
        if not settings.SECURITY_COMMAND_EXECUTION and request.user.is_common_user:
            return self.permission_denied(request, "Command execution disabled")
        return super().check_permissions(request)
# ---
def test_two_contiguous_selectors():
    B, X, Y = Axis("batch", 3), Axis("x", 5), Axis("y", 7)
    a = hax.arange((B, X, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a["x", ix, "y", iy]
    assert out.axes == (B,)
    ref = a.array[jnp.arange(3), ix.array, iy.array]
    assert jnp.array_equal(out.array, ref)
# ---
def clear_requesting(self) -> None:
        """Clear REQUESTING state (scale-up completed or failed)."""
        self._requesting_until = Timestamp.from_ms(0)
# ---
def slow_create(tags=None):
            time.sleep(0.2)
            result = original_create(tags)
            create_completed.append(True)
            return result
# ---
def get_archive_object_tar(self):
        '''
        return tarfile object and its members
        '''
        tfile = tarfile.open(name=self.filename)
        members = tfile.getnames()
        return tfile, members
# ---
def _send_message(self, message):
        assert jax.process_index() == 0
        out = broadcast_shard(jnp.array(message), PartitionSpec())
        return out
# ---
def v5p8_scale_group() -> config_pb2.ScaleGroupConfig:
    """Single-host TPU scale group (v5p-8)."""
    return config_pb2.ScaleGroupConfig(
        name="tpu-v5p-8",
        min_slices=0,
        max_slices=10,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def vm_count(self) -> int:
        """Total number of VMs in the group."""
        return len(self.vms)
# ---
def __init__(self, remote, letter, number, name, colour, *args, **kwargs):
        super(TeamPanel, self).__init__(*args, **kwargs)
        self.remote = remote
        self.InitUI(letter, number, name, colour)
# ---
def soft_sign(a: A) -> A:
    return wrap_elemwise_unary(jnn.soft_sign, a)
# ---
def test_binary_set(self):
        """Store and retrieve a binary set"""
        self.make_table()
        item = {
            "id": "a",
            "datas": set([Binary("a"), Binary("b")]),
        }
        self.dynamo.put_item("foobar", item)
        ret = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(ret, item)
# ---
def wait_until_finished(self):
        self._manager.wait_until_finished()
        if jax.process_index() == 0:
            while self._checkpoint_being_removed is not None or not self._async_checkpoint_remover_queue.empty():
                time.sleep(0.2)
# ---
def title(self, val):
        # If val is not a string, raise TypeError now rather than later.
        self._title = _EpubMeta('dc:title', '' + val)
# ---
def start_response(*args):
    pass
# ---
def __repr__(self):
        return f'MultipleSort({self.sorts!r})'
# ---
def terminate(self):
        """
        Tells the logger process to exit immediately. If you do not call 'flush' method before, you may lose some
        messages of progresses that have not been displayed yet. This method blocks until logger process has stopped.
        """
        self.queue.put(dill.dumps(ExitCommand()))

        if self.process:
            self.process.join()
# ---
def batch_axis_at_step(self, step: int):
        size = self.rounded_batch_size_at_step(step)

        return hax.Axis(self.batch_axis_name, size)
# ---
def stop(self):
        self._patcher.stop()
        self._patcher2.stop()
        self._server_mock = None
# ---
def start_container(self, container_id: str) -> None: ...
# ---
def mock_load_datasets(config):
        return [DatasetWithMetaData(mock_dataset, "subset1", "train", "main")]
# ---
def test_olmo3_attention_layer_type_detection(layer_idx):
    """Test that attention correctly detects its layer type."""
    config = _get_olmo3_config()
    attention = _get_olmo3_attention(config, layer_idx=layer_idx, key=random.PRNGKey(0))

    expected_sliding = (layer_idx + 1) % 4 != 0
    if expected_sliding:
        assert attention.config.sliding_window == config.sliding_window
    else:
        assert attention.config.sliding_window is None
# ---
def get_tpu_topology(tpu_type: str) -> TpuTopologyInfo:
    """Get TPU topology by type name."""
    for config in TPU_TOPOLOGIES:
        if config.name == tpu_type:
            return config
    raise ValueError(f"Unknown TPU type: {tpu_type}")
# ---
def transgene_insertion(testapp, lab, award, ctcf):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'in vivo enhancer characterization',
        'nucleic_acid_delivery_method': ['mouse pronuclear microinjection'],
        'modified_site_by_gene_id': ctcf['@id'],
        'introduced_sequence': 'ATCGTA'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def make_state(key):
        model = MLP(in_size=2, out_size=1, width_size=2, depth=3, key=key)
        optim = optax.adam(1e-4)
        opt_state = optim.init(arrays_only(model))

        return model, opt_state, key
# ---
def apply_update(m_hat, v_hat):
            return m_hat / (jnp.sqrt(v_hat) + epsilon)
# ---
def eos_token_id(self) -> int:
        return 2
# ---
def __getattribute__(self, key):
                fn = object.__getattribute__(self, key)
                def go(*arg, **kw):
                    canary.append(fn.__name__)
                    return fn(*arg, **kw)
                return go
# ---
def _make_xml_elem(tag, text, attr = []):
    'Write a flat xml element.'
    out = '    <' + tag
    for (key, val) in attr:
        out += ' {}="{}"'.format(key, val)
    if text:
        out += '>{}</{}>\n'.format(text, tag)
    else:
        out += ' />\n'
    return out
# ---
def _zero_if_array_else_none(x: Any) -> ResolvedUnnamedAxisSpec:
    return 0 if is_jax_array_like(x) else None
# ---
def test_freeze_returns_named_array():
    X = hax.Axis("x", 3)
    ref = hax.new_ref(hax.arange(X))
    frozen = hax.freeze(ref)
    assert isinstance(frozen, hax.NamedArray)
    assert frozen.axes == ref.axes
    assert jnp.allclose(frozen.array, ref.value().array)
# ---
def add_dims_to_spec(qss, sds):
            if partition_grads_into_blocks:
                qss = jax.tree.map(lambda qs: PartitionSpec(*((None,) + qs)), qss)
            if sds is not None:
                qss = jax.tree.map(lambda qs: PartitionSpec(*(sds + qs)), qss)
            return qss
# ---
def name(self):
        return self.name_ctrl.GetValue()
# ---
def test_pspec_for_plain_array_axis_names_nested_module():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        mod = NestedArrayModule(ArrayModule(jnp.ones((Dim2.size, Dim3.size))))

        specs: NestedArrayModule = pspec_for(mod)

        assert specs.inner.arr == PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)
# ---
def test_comparison_flip():
    variants = generate_expression_variants("x < y")
    assert "y > x" in variants
# ---
def get_properties(self):
        d = COMMON_PROPERTIES.copy()
        d.update(CONSOLE_PROPERTIES)
        return d
# ---
def _get_disk_format(self):
        fmt = self._vminfo.get('format', 'raw').lower()
        return "qcow2" if fmt == "cow" else fmt
# ---
def fsspec_cpdir(dir_path: str, target_path: str) -> None:
    """
    Recursively copies all contents of dir_path to target_path.

    Args:
        dir_path (str): The path of the directory to copy.
        target_path (str): The target path.
    """

    fs = fsspec.core.get_fs_token_paths(target_path, mode="wb")[0]
    fs.put(os.path.join(dir_path, "*"), target_path, recursive=True)
# ---
def T(self):
        return self.transpose()
# ---
def logs(ctx, job_id):
    """Stream logs from a job."""
    cluster = ctx.obj["cluster"]
    try:
        job_info = cluster.monitor(job_id)
        click.echo(f"Job status: {job_info.status}")
    except KeyboardInterrupt:
        click.echo("\nInterrupted.")
# ---
def __getitem__(self, key):
        return self._raw[key]
# ---
def _power_off(driver_info):
    """Turn the power OFF for this node.

    :param driver_info: the ipmitool parameters for accessing a node.
    :returns: one of ironic.common.states POWER_OFF or ERROR.
    :raises: IPMIFailure on an error from ipmitool (from _power_status call).

    """
    return _set_and_wait(states.POWER_OFF, driver_info)
# ---
def mock_the_cursor(cursor, *arg):
            arg[-1].get_result_proxy = Mock(return_value=Mock(context=arg[-1]))
            return retval
# ---
def test_iris_not_detected_when_no_context():
    """Should not detect Iris when get_iris_ctx() returns None."""
    with patch("iris.client.client.get_iris_ctx", return_value=None):
        with patch("ray.is_initialized", return_value=False):
            client = current_client()
            assert isinstance(client, LocalClient)
# ---
def default():
        return CacheOptions()
# ---
def module_custom_product(module_org):
    return entities.Product(organization=module_org).create()
# ---
def policy_encode(policy: jmp.Policy):
        def name(dtype):
            if hasattr(dtype, "name"):
                return dtype.name
            elif hasattr(dtype, "dtype"):
                return name(dtype.dtype)

        out = f"compute={name(policy.compute_dtype)},params={name(policy.param_dtype)},output={name(policy.output_dtype)}"
        assert jmp.get_policy(out) == policy
        return out
# ---
def get_batch(self, indices: Sequence[int] | np.ndarray) -> Sequence[T_co]:
        pass
# ---
def stop(self):
        self.running = False
        self.articleDecoder.stop()
        for thread in self.threads:
            thread.stop()
        self.clearCache()
# ---
def test_get_root_on_create(self):
        root_on_create_val = Instance.get_root_on_create(
            'redis')
        self.assertFalse(root_on_create_val)
# ---
def p_start(self, p):
        "start : translation_unit"
        p[0] = self.todo
# ---
def quick_gelu(x):
    return x * sigmoid(1.702 * x)
# ---
def test_neutron_resolved(self):
        self.compare_stacks('Neutron.template', 'Neutron.yaml', {})
# ---
def slice_id(self) -> str:
        return self._slice_id
# ---
def keys(self):
		return self._addresses.keys()
# ---
def test_capture_binary_output(testdir):
    testdir.makepyfile(r"""
        import pytest

        def test_a():
            import sys
            import subprocess
            subprocess.call([sys.executable, __file__])

        def test_foo():
            import os;os.write(1, b'\xc3')

        if __name__ == '__main__':
            test_foo()
        """)
    result = testdir.runpytest('--assert=plain')
    result.assert_outcomes(passed=2)
# ---
def admin_url(self):
        return get_absolute_saagie_url('/#/manager/%s/job/%s'
                                       % (self.platform_id, self.id))
# ---
def submit(self, request: JobRequest) -> JobHandle:
        """Submit a job for execution. Returns immediately."""
        ...
# ---
def join_fn(left: Iterator, right: Iterator) -> Iterator:
        return _sorted_merge_join(left, right, left_key_fn, right_key_fn, combiner_fn, join_type)
# ---
def divide(self, a: int, b: int) -> float:
        return a / b
# ---
def __init__(self):
        try:
            _check_option_support(['timing', 'single_bridge', 'dual_bridge'])
        except OSError:
            raise exception.DriverLoadError(
                driver=self.__class__.__name__,
                reason=_("Unable to locate usable ipmitool command in "
                         "the system path when checking ipmitool version"))
        _check_temp_dir()
# ---
def start(self):
        self.thread.start()
        self._wait_for_server()
# ---
def status_represent(index):
            if index == None:
                return "Unknown" # @todo: use messages (internationalize)
            else:
                return import_upload_status[index]
# ---
def build_request(method_info: MethodInfo, json_str: str | None, kwargs: dict[str, Any]) -> Message:
    """Build a protobuf request message from JSON or keyword arguments."""
    if json_str:
        data = json.loads(json_str)
    else:
        data = kwargs

    return json_format.ParseDict(data, method_info.input_type())
# ---
def __init__(self, title, info):
        self.title = title
        self.info = info
# ---
def get_selector(self):
        ''' get the service selector'''
        return self.get(Service.selector_path) or {}
# ---
def status(self) -> VmGroupStatus:
        """Current status computed from VM states."""
        ...
# ---
def testExprNameLocal(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        foo = 42
        def bar():
          foo
        bar()""")))
# ---
def __gt__(self, other: "Timestamp") -> bool:
        return self._epoch_ms > other._epoch_ms
# ---
def output_heading(self):
        return self.renderer.header(
            self.inline(self.token['text']),
            self.token['level'],
            self.token['text'],
        )
# ---
def ensure_tensorboard_available(executable: str) -> None:
    if os.path.sep in executable or executable.startswith("."):
        if not Path(executable).exists():
            raise FileNotFoundError(f"TensorBoard executable '{executable}' was not found.")
        return

    if shutil.which(executable) is None:
        raise FileNotFoundError(
            f"TensorBoard executable '{executable}' not found on PATH. Use --tensorboard to point to it explicitly."
        )
# ---
def _create_blob(self):
        blob_name = self._get_blob_reference()
        blob = self.bsc.get_blob_client(self.container_name, blob_name)
        blob.upload_blob(b'')
        return blob
# ---
def check_format(self, sample_str: str) -> bool:
        try:
            _ = extract_boxed(sample_str)
            return True
        except ValueError:
            return False
# ---
def delete_file(file_path):
    compss_delete_file(file_path)
# ---
def main():
    pass
# ---
def relu_squared(x: A) -> A:
    """ReLU squared activation function. jnp.square(jnp.maximum(0, x))"""

    def _fn(a):
        return jnp.square(jnn.relu(a))

    return typing.cast(A, wrap_elemwise_unary(_fn, x))
# ---
def get_task_status(self, task_name: JobName) -> cluster_pb2.TaskStatus:
        """Get status of a specific task within a job.

        Args:
            task_name: Full task name (/job/.../index)

        Returns:
            TaskStatus proto for the requested task
        """
        task_name.require_task()
        request = cluster_pb2.Controller.GetTaskStatusRequest(task_id=task_name.to_wire())
        response = self._client.get_task_status(request)
        return response.task
# ---
def col_clause(self):
        return self.field + " = ?", [self.pattern]
# ---
def crispr_deletion_1(testapp, lab, award, target):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'deletion',
        'purpose': 'repression',
        'method': 'CRISPR',
        'modified_site_by_target_id': target['@id'],
        'guide_rna_sequences': ['ACCGGAGA']
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def setDepth(self, depth):
        self.__depth = depth
# ---
def get_internal_type(self):
		return "TextField"
# ---
def _get(delta, start):
            p = start
            while delta:
                p = p.next
                delta -= 1
            return p.val
# ---
def local_reducer(rows: Iterator[dict], attr_name: str = attr_name, attr_label: str | None = attr_label) -> DDSketch:
        """Build DDSketch from rows in a single shard."""
        sketch = DDSketch()
        for row in rows:
            attributes = row["attributes"]
            value = attributes[attr_name][attr_label] if attr_label else attributes[attr_name]
            sketch.add(value)
        return sketch
# ---
def tag(self, client, tags, images, **kwds):
        if tags is None:
            tags = ['latest']
        for image in images:
            self.getImage(image).tag(client, tags, **kwds)
# ---
def stdout(self):
        return self._stdout
# ---
def global_data_indices_by_device_for_step(self, step: int) -> dict[jax.Device, range]:
        local_indices = self.local_data_indices_by_device_for_step(step)
        offset = self.scheduler.global_data_offset_by_step(step)

        return {device: range(offset + r.start, offset + r.stop, r.step) for device, r in local_indices.items()}
# ---
def testDoubleRandom(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.float64)
      y = self._randMatrix(k, m, np.float64)
      self._testCpuMatmul(x, y)
# ---
def test_spawn_glance(self):
        stubs.stubout_fetch_image_glance_disk(self.stubs)
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_MACHINE,
                         glance_stubs.FakeGlance.IMAGE_KERNEL,
                         glance_stubs.FakeGlance.IMAGE_RAMDISK)
        self.check_vm_params_for_linux_with_external_kernel()
# ---
def notify_task_update(self, request: cluster__pb2.Controller.NotifyTaskUpdateRequest, ctx: RequestContext) -> cluster__pb2.Empty:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def start(self):
        self._thread = concurrent.thread(self._run, name="v2v/" + self._id[:8])
        self._thread.start()
# ---
def test_get_host_unquote(self):
        req = Request("http://www.%70ython.org/")
        self.assertEqual("www.python.org", req.get_host())
# ---
def check_master(self):
        '''
        Log if the master is not running
        NOT YET IMPLEMENTED
        '''
        return True
# ---
def imag(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.imag, self.axes)
# ---
def get_server_mock(self):
        return self._server_mock
# ---
def find_version(filename):
    _version_re = re.compile(r"__version__ = '(.*)'")
    for line in open(filename):
        version_match = _version_re.match(line)
        if version_match:
            return version_match.group(1)
# ---
def test_format_shard_path_basename_placeholder():
    """Test that {basename} placeholder works."""
    pattern = "output/{basename}-{shard:05d}.jsonl"
    result = format_shard_path(pattern, 3, 10)
    assert result == "output/shard_3-00003.jsonl"
# ---
def deposit(self, amount):
        self.balance += amount
        return self.balance
# ---
def test_loggamma():
    a = hax.arange(Width, start=0.1)

    check_gen_is_equal(
        lambda k, s: jax.random.loggamma(k, a.array.reshape(1, -1), shape=s), lambda k, s: hax.random.loggamma(k, s, a)
    )
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> RayActorHandle:
        """Create a single Ray actor and return a handle immediately."""
        group = self.create_actor_group(actor_class, *args, name=name, count=1, resources=resources, **kwargs)
        return group.wait_ready()[0]
# ---
def __ge__(self, other: "Duration") -> bool:
        return self._ms >= other._ms
# ---
def advance_time(delta_seconds):
        nonlocal fake_now
        fake_now += timedelta(seconds=delta_seconds)
# ---
def test_reduce_sum(backend):
    """Test basic sum reduction."""
    ds = Dataset.from_list(range(100)).reduce(sum)
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == sum(range(100))
# ---
def total_host_count(self) -> int:
        """Total number of hosts in the pool."""
        return len(self._hosts)
# ---
def _dashboard(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Iris Controller", "/static/controller/app.js"))
# ---
def __repr__(self):
        return '%s(width=%d, height=%d)' % \
            (self.__class__.__name__, self.width, self.height)
# ---
def is_scalar(input_type):
    """Returns True if input_type is scalar."""
    return input_type['base_type'] in SCALAR
# ---
def test_actor_unnamed_isolation(job_context):
    """Test that unnamed actors are isolated instances."""
    actor1 = job_context.create_actor(SimpleActor, 10)
    actor2 = job_context.create_actor(SimpleActor, 20)

    job_context.get(actor1.increment.remote(5))
    job_context.get(actor2.increment.remote(3))

    assert job_context.get(actor1.get_value.remote()) == 15
    assert job_context.get(actor2.get_value.remote()) == 23
# ---
def host_count(appliance, metrics_tbl, mgmt_system_id):
    return bool(appliance.db.client.session.query(metrics_tbl).filter(
        metrics_tbl.parent_ems_id == mgmt_system_id).filter(
        metrics_tbl.resource_type == "Host").count()
    )
# ---
def read_config():
    with open('config.cfg', 'r') as cfg_file:
        lines = cfg_file.readlines()
        lines = [
            line.strip().replace(' ', '').split('=')
            for line in lines
            if line.strip() and '=' in line
        ]
        cfg = {key:try_parse(value) for key,value in lines}
        return cfg
# ---
def SetFittingHeightForCurrentWindow():
  window_width = GetIntValue( 'winwidth( 0 )' )
  fitting_height = 0
  for line in vim.current.buffer:
    fitting_height += len( line ) // window_width + 1
  vim.command( '{0}wincmd _'.format( fitting_height ) )
# ---
def validate_seed_results(self, results: list[tuple[str, str]]) -> bool:
        """Validate that seed jobs completed as expected."""
        expected = ["JOB_STATE_SUCCEEDED"] * len(results)
        actual = [r[1] for r in results]
        return actual == expected
# ---
def weight_transfer_config(transfer_mode):
    """Create weight transfer config for the specified mode."""
    with tempfile.TemporaryDirectory() as temp_dir:
        config = WeightTransferConfig(
            mode=transfer_mode,
            sync_interval_steps=1,
            checkpoint_dir=temp_dir,
        )
        yield config
# ---
def __repr__(self):
        return "<differencematcher m1=%r, m2=%r>" % (self._m1, self._m2)
# ---
def _make_not_expr(self, _not, check):
        """Invert the result of another check."""

        return [('check', NotCheck(check))]
# ---
def limit_fastq(fastq_gen, num_sequences=1000):
    for i in range(num_sequences):
        try:
            yield next(fastq_gen)
        except StopIteration:
            return
# ---
def docker_cleanup_scope():
    """Cleans up all Docker containers and images created during the test."""
    with _docker_cleanup(cleanup_images=True):
        yield
# ---
def num_cpus(self) -> int:
        return num_cpus_used_by_tokenizer(self.tokenizer)
# ---
def info(self):
        return {}
# ---
def __module_version(self):
        return self.version
# ---
def worker(idx: int):
        barrier.wait()
        results[idx] = actor.increment.remote(1).result()
# ---
def __init__(self,**kwargs):
        self.register_event_type('on_ok')
        super(OkPopup,self).__init__(**kwargs)
# ---
def activate(self):
        exc = self.exc
        if exc is not None:
            self.exc = None
            self.tb = traceback.format_exception(exc.__class__, exc,
                                                 exc.__traceback__)
# ---
def test_aggregate(self):
        yield self.check_aggregate_range, 0, 24*3600
        yield self.check_aggregate_range, 8*3600, 20*3600
        yield self.check_aggregate_range, 13*3600, 14*3600
# ---
def set_configuration(self, frequency, bandwidth, power):
		self.frequency = frequency
		self.bandwidth = bandwidth
# ---
def append(self, record: BufferedLogRecord) -> None: ...
# ---
def test_package_installation():
    with TemporaryVenv(pip_install_args=["six"]) as venv:
        result = venv.run(
            [venv.python_path, "-c", "import six; print(six.__version__)"],
            capture_output=True,
            text=True,
        )
        assert result.returncode == 0
        assert len(result.stdout.strip()) > 0
# ---
def screenshot_to_file(self, filename, includes='subtitles'):
        self.command('screenshot_to_file', filename.encode(fs_enc), includes)
# ---
def test_is_client_error(self):
        self.assertFalse(status.is_client_error(399))
        self.assertFalse(status.is_client_error(500))

        for i in range(400, 499):
            self.assertTrue(status.is_client_error(i))
# ---
def __init__(self, handles: list[ActorHandle]):
        self._handles = handles
        self._yielded = False
# ---
def registry() -> VmRegistry:
    """Create a fresh VmRegistry for testing."""
    return VmRegistry()
# ---
def __call__(self, x, y, *, key):
            return x + self.array + self.static + hax.random.normal(key, x.axes), x * 2 + y
# ---
def attention(
    q: Float[Array, "B Q Hq D"],
    k: Float[Array, "B K Hkv D"],
    v: Float[Array, "B K Hkv D"],
    mask: AttentionMask | Bool[Array, "B Q K"] | Float[Array, "B Q K"] | None,
) -> Float[Array, "B Q Hq D"]:
    if jax.default_backend() == "tpu":
        if isinstance(mask, jax.Array):
            return reference_attention(q, k, v, mask, logits_dtype=jnp.float32)
        return _tpu_splash_attention(q, k, v, mask)
    return reference_attention(q, k, v, mask, logits_dtype=jnp.float32)
# ---
def create_log():
    # Note that different steps cannot share variables
    with tempfile.NamedTemporaryFile(prefix="executor-log-") as f:
        return f.name
# ---
def filter_all(items):
        for _ in items:
            pass  # Consume but don't yield
        return iter([])
# ---
def get_experiment_url(self) -> str:
        """Return the URL where the experiment can be viewed."""
        if self.prefix.startswith("gs://"):
            host = "https://marin.community/data-browser"
        else:
            host = f"http://localhost:{_get_local_data_browser_port()}"

        return host + "/experiment?path=" + urllib.parse.quote(self.executor_info_path)
# ---
def code_inline(node: RenderTreeNode, context: RenderContext) -> str:
    code = node.content
    all_chars_are_whitespace = not code.strip()
    longest_backtick_seq = longest_consecutive_sequence(code, "`")
    if longest_backtick_seq:
        separator = "`" * (longest_backtick_seq + 1)
        return f"{separator} {code} {separator}"
    if code.startswith(" ") and code.endswith(" ") and not all_chars_are_whitespace:
        return f"` {code} `"
    return f"`{code}`"
# ---
def test_asdict_excluding_invalid():
    """Test asdict_excluding with non-dataclass input."""
    with pytest.raises(ValueError, match="Only dataclasses are supported"):
        asdict_excluding({"key": "value"}, exclude=set())
# ---
def release(self, lease: Lease[T_co]) -> None:
        if lease.lease_id in self.leases:
            item, _, _ = self.leases[lease.lease_id]
            self.queue.insert(0, item)
            del self.leases[lease.lease_id]
# ---
def test_max_subtree_stmts():
    """Large subtrees should be excluded."""
    programs = [SAMPLE_PROGRAMS[2]]  # merge_sort has many statements.
    bank = SubtreeBank.from_corpus(programs, max_subtree_stmts=2)
    for entries in bank.entries.values():
        for entry in entries:
            assert entry.stmt_count <= 2
# ---
def _noop():
    pass
# ---
def _apply(x: Float[Array, "B S H D"]) -> Float[Array, "B S H D"]:
        x1, x2 = jnp.split(x, 2, axis=-1)
        return jnp.concatenate([x1 * cos - x2 * sin, x2 * cos + x1 * sin], axis=-1)
# ---
def _custom_setup(self):
        kwargs = {}
        kwargs['netapp_mode'] = 'proxy'
        kwargs['configuration'] = create_configuration()
        self._driver = netapp_nfs.NetAppDirectCmodeNfsDriver(**kwargs)
# ---
def batch_flatten(data):
    """BatchFlatten.

    This operator flattens all the dimensions except for the batch dimension.
    which results a 2D output.

    For data with shape ``(d1, d2, ..., dk)``
    batch_flatten(data) returns reshaped output of shape ``(d1, d2*...*dk)``.


    Parameters
    ----------
    data : relay.Expr
        The input data to the operator.

    Returns
    -------
    result: relay.Expr
        The Flattened result.
    """
    return _make.batch_flatten(data)
# ---
def release(self, lease: Lease[T]) -> None:
        timestamp = 0.0
        unique_id = uuid.uuid4()
        filename = f"{timestamp:.6f}_{unique_id}.pkl"

        try:
            self.fs.mv(self.processing_dir / lease.lease_id, self.pending_dir / filename)
        except FileNotFoundError:
            raise ValueError(f"Invalid lease: {lease.lease_id} not found during release") from None
# ---
def direct_fn(pred):
        pred_embeddings, pred_lm_head = pred
        logits = hax.dot(pred_embeddings, pred_lm_head, axis="embed", preferred_element_type=pred_embeddings.dtype)
        target_y = hax.nn.one_hot(true_ids, Vocab, dtype=pred_embeddings.dtype)
        loss, _ = cross_entropy_loss_and_log_normalizers(logits, Vocab, target_y)
        return loss.mean().scalar()
# ---
def unsize_axes(axis_spec: PartialShapeDict, to_unsize: AxisSelection) -> PartialShapeDict: ...
# ---
def actual_head_size(self) -> int:
        return self.head_dim or (self.hidden_dim // self.num_heads)
# ---
def to_dataframe(self, message, dtypes=None):
        raise NotImplementedError("Not implemented.")
# ---
def OnQuit(self, e):
        self.Close()
# ---
def visit_mover(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            over = self._visit(children[1])
            return BracedNode(f"\\overset{{{over}}}{{{base}}}")
        return TextNode("")
# ---
def __str__(self):
        return f"EraShufflingDataset({str(self.dataset)})"
# ---
def callback(self):
            print(f"{prefix}JitScheduler State:")
            print(f"{prefix}Queued Tokens: {self.queued_tokens}")
            print(f"{prefix}Queued Slot IDs: {self.queued_slot_ids}")
            print(f"{prefix}Num Queued Tokens: {self.num_queued_tokens}")
# ---
def get_network_id(self, runner):
        # FIXME: We can save on some steps if we only do this once
        obj = runner.get_plan(self.adapts).describe_object()
        return obj.get("VpcId", None)
# ---
def test_encrypt_many_newlines_at_end(self):
        self._test_encryption('Message with lotsa newlines.\n\n\n')
# ---
def kv_pages(self) -> ht.i32[NamedArray, "seq page"]:  # type: ignore[name-defined]
        """KV page assignments per sequence."""
        return self.sequences.kv_pages
# ---
def terminate(self) -> None:
        if self._ref is not None:
            ray.cancel(self._ref)
            return
        try:
            client = JobSubmissionClient(self._dashboard_address)
            client.stop_job(self._submission_id)
        except Exception as e:
            logger.warning("Failed to stop job %s: %s", self._job_id, e)
# ---
def convert_lingoly_to_dolma(config: ConvertLingolyToDolmaConfig) -> None:
    """Convert Lingoly dataset to Dolma format."""
    pipeline = (
        Dataset.from_list([config.input_path])
        .flat_map(lambda p: load_zip_members(p, pattern="test.jsonl"))
        .flat_map(lambda m: process_lingoly_member(m, max_doc_length=config.max_doc_length))
        .write_jsonl(f"{config.output_path}/{{shard:05d}}.jsonl")
    )
    list(Backend.execute(pipeline))
# ---
def serialize_keras_class_and_config(cls_name, cls_config):
  """Returns the serialization of the class with the given config."""
  return {'class_name': cls_name, 'config': cls_config}
# ---
def test_lambda_multiple_inputs(self):
    ld = keras.layers.Lambda(lambda x: x[0], output_shape=lambda x: x[0])
    x1 = np.ones([3, 2], np.float32)
    x2 = np.ones([3, 5], np.float32)
    out = ld([x1, x2])
    self.assertAllEqual(out.shape, [3, 2])
# ---
def test_filter_passing_partial_fail_excluded():
    """A candidate that passes only some tests should be excluded."""
    candidate = _make_candidate("def f(x):\n    return 1\n")
    tests = ["assert f(1) == 1", "assert f(2) == 2"]
    passing = filter_passing([candidate], tests)
    assert passing == []
# ---
def print_ls_desc(desc, **kwargs):
    print(get_ls_desc(desc, **kwargs))
# ---
def get_memory_mb(self) -> int | None:
        if not self.resources or not self.resources.memory_bytes:
            return None
        return self.resources.memory_bytes // (1024 * 1024)
# ---
def _compiledpats(self):
        pat, matchfunc = _buildregexmatch(self._kindpats, "")
        return matchfunc
# ---
def terminate(self) -> None: ...
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        pass
# ---
def axis_spec_to_tuple(axis_spec: AxisSpec) -> tuple[Axis, ...]: ...
# ---
def get_now(timezone: Optional[datetime.tzinfo]) -> datetime.datetime:
    return datetime.datetime.now(timezone)
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        """Stop cluster and cleanup."""
        del exc_type, exc_val, exc_tb  # unused
        self._stop_jupyter()
        self._rpc_client = None
        if self._manager:
            self._manager.stop()
# ---
def em(node: RenderTreeNode, context: RenderContext) -> str:
    text = make_render_children(separator="")(node, context)
    indicator = node.markup
    return indicator + text + indicator
# ---
def c_identifier(text):
    """Convert input text into an legal identifier in C.

    Example
    -------
    >>> c_identifier("Hello World")
    'HelloWorld'
    >>> c_identifier("Anti-Shake")
    'Antishake'
    """
    if ' ' in text:
        text = camel_case(text)
    text = re.sub(r'\+\d+', lambda x: x.group().replace('+', 'P'), text)
    text = re.sub(r'\-\d+', lambda x: x.group().replace('-', 'N'), text)
    text = replace_punctuations(text)
    return remain_alnum(text)
# ---
def checkDiagonals(self, player, inARow):
        """
        Calls two diagonal functional.
        :return: an int, representing the column where to play or 0 and False if there is no pattern search.
        """
        check = self.checkDiagonalLeftToRight(player, inARow)
        if check[0]:
            return check
        else:
            return self.checkDiagonalRightToLeft(player, inARow)
# ---
def lookup_endpoint(self, request: cluster__pb2.Controller.LookupEndpointRequest, ctx: RequestContext) -> cluster__pb2.Controller.LookupEndpointResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def subject(self, record):
        # in future we can vary the subject depending on the record details
        return 'Updating your Software Carpentry information'
# ---
def output_path_of(step: ExecutorStep, name: str | None = None) -> InputName:
    return InputName(step=step, name=name)
# ---
def width(self, new_width):
        self.set_size(new_width, self.height)
# ---
def _event_wrapper(f):
        f._platform_event = True
        if not hasattr(f, '_platform_event_data'):
            f._platform_event_data = []
        f._platform_event_data.append(data)
        return f
# ---
def fsspec_expand_glob(url):
            if "*" in url:
                fs = fsspec.core.url_to_fs(url)[0]
                return fs.glob(url)
            else:
                return [url]
# ---
def test_nested_dict(self):
        """Store and retrieve a nested dict"""
        self.make_table()
        data = {
            "s": "abc",
            "d": {
                "i": 42,
            },
        }
        self.dynamo.put_item("foobar", {"id": "abc", "d": data})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["d"], data)
# ---
def create_qwen_config():
    return Qwen3Config(
        seq_len=4096,
        hidden_dim=1024,
        intermediate_dim=3072,
        num_heads=16,
        num_kv_heads=8,
        num_layers=28,
        rope=Llama3RotaryEmbeddingsConfig(),
        tie_word_embeddings=True,
    )
# ---
def named_jit(
    *,
    axis_resources: ResourceMapping | None = None,
    in_axis_resources: ResourceMapping | None = None,
    out_axis_resources: ResourceMapping | None = None,
    donate_args: PyTree | None = None,
    donate_kwargs: PyTree | None = None,
    # args from jit
    keep_unused: bool = False,
    backend: str | None = None,
    inline: bool | None = None,
) -> typing.Callable[[Callable[Args, R]], WrappedCallable[Args, R]]: ...
# ---
def __len__(self):
        return len(self._queue)
# ---
def addReplica( self, lfn, force = False, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndicts = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.addReplica( lfndicts, force )
# ---
def register(self, request: cluster__pb2.Controller.RegisterRequest, ctx: RequestContext) -> cluster__pb2.Controller.RegisterResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def fake_eject_subordinate(id, compute_uuid, host_uuid):
            fake_eject_subordinate.called = True
# ---
def saveable_state(self) -> FilterTree:
        return eqx.filter(self, saveable_training_mask(self, self.is_trainable))
# ---
def contains(self, date):
        if self.start is not None and date < self.start:
            return False
        if self.end is not None and date >= self.end:
            return False
        return True
# ---
def start(self):
        attributes = json.loads(self.api_job['attributes'].decode('utf-8'))
        vol_name = attributes['volname']
        option = attributes['option_name']
        option_value = attributes['option_value']
        self.atom().start(vol_name, option, option_value)
        self.api_job['status'] = "finished"
        etcd.Client().write(self.api_job['request_id'],
                            json.dumps(self.api_job))
# ---
def test_count_addition(self):
        """Count addition"""
        count = Count(4, 2)
        self.assertEqual(count + 5, 9)
# ---
def volume(self):
        raise NotImplemented
# ---
def path(self, path):
        """
        Sets the path of this ContributorOrcid.

        :param path: The path of this ContributorOrcid.
        :type: str
        """

        self._path = path
# ---
def test_pack_and_unpack_simple():
    tree = {"a": np.arange(3, dtype=np.float32), "b": np.arange(4, dtype=np.float32).reshape(2, 2)}
    offsets, packed = eqx.filter_jit(pack_pytree)(tree, dtype=jnp.float32)
    rebuilt = eqx.filter_jit(unpack_pytree)(offsets, packed)
    for orig, new in zip(jax.tree_util.tree_leaves(tree), jax.tree_util.tree_leaves(rebuilt)):
        np.testing.assert_array_equal(np.asarray(orig, dtype=np.float32), np.array(new))
# ---
def lang_del(self):
        del self._lang
        if self.req is not None and self.req.environ.get('etalage') is not None \
                and '_lang' in self.req.environ['etalage']:
            del self.req.environ['etalage']['_lang']
# ---
def _set_panel_match(self, match, team_idx, panel_idx):
        self.team_panels[panel_idx].number = match.team_numbers[team_idx]
        self.team_panels[panel_idx].name = match.team_names[team_idx]
# ---
def _format_timestamp(ms: int) -> str:
    if ms == 0:
        return "-"
    return Timestamp.from_ms(ms).as_formatted_date()
# ---
def normalize_dtype(dtype: str) -> str:
    """Normalize dtype string to base form.

    Strips 'amp_' prefix if present, converts to lowercase.
    """
    dtype = dtype.lower()
    if dtype.startswith("amp_"):
        return dtype[4:]
    return dtype
# ---
def on_fuzzing_start_changed(self, value: int):
        self.ui.spinBoxFuzzingEnd.setMinimum(self.ui.spinBoxFuzzingStart.value())
        new_start = self.message.convert_index(value - 1, self.proto_view, 0, False)[0]
        self.current_label.start = new_start
        self.current_label.fuzz_values[:] = []
        self.update_message_data_string()
        self.fuzz_table_model.update()
        self.ui.tblFuzzingValues.resize_me()
# ---
def _to_pallas_dslice(value):
    if is_pallas_dslice(value):
        return value
    if isinstance(value, HaliaxDSlice):
        return dslice(value.start, value.size)
    raise TypeError("Expected a haliax.dslice or pallas.dslice")
# ---
def should_skip(name):
        if name.startswith('.') and name not in ('.github', '.claude'):
            return True
        return name in SKIP_DIRS or name.endswith('.egg-info')
# ---
def port_job():
            info = get_job_info()
            if info is None:
                raise ValueError("JobInfo not available")
            # Verify ports are set
            if "http" not in info.ports or "grpc" not in info.ports:
                raise ValueError(f"Ports not set: {info.ports}")
            # Verify they're valid port numbers
            assert info.ports["http"] > 0
            assert info.ports["grpc"] > 0
# ---
def test_pass_different_length_seq_to_gpt2():
    config = Gpt2Config(
        max_seq_len=64,
        hidden_dim=16,
        num_layers=4,
        num_heads=2,
        gradient_checkpointing=False,
        use_flash_attention=True,
    )
    check_model_works_with_seqlen(Gpt2LMHeadModel, config, 16)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """Peak FLOP/s for a single TPU chip."""
        from fray.cluster.device_flops import device_flops

        flops = device_flops(self.variant, dtype)
        if flops is None:
            raise ValueError(f"Unknown device/dtype: {self.variant}/{dtype}")
        return flops
# ---
def __str__(self):
        return '[' + ', '.join('{}'.format(el) for el in self._queue) + ']'
# ---
def array_equiv(a: NamedArray, b: NamedArray) -> bool:
    """Returns True if two arrays are shape-consistent and equal."""
    try:
        a, b = broadcast_arrays(a, b)
    except ValueError:
        return False
    return bool(jnp.array_equal(a.array, b.array))
# ---
def asCustomKW(self):
        """ @rtype: CustomKW """
        impl_type = EnkfNode.cNamespace().get_impl_type(self)
        assert impl_type == ErtImplType.CUSTOM_KW

        return CustomKW.createCReference(self.valuePointer(), self)
# ---
def __init__(self):
        self.result: str | None = None
        self._done = threading.Event()
# ---
def output_exemplar(self) -> dict:
        return self._exemplar
# ---
def test_deeply_nested(self):
        expr = col("data")["level1"]["level2"]
        assert expr.evaluate({"data": {"level1": {"level2": "value"}}}) == "value"
# ---
def test_start(self):
        p = profiler.init("secret", base_id="1", parent_id="2")
        p.start = mock.MagicMock()
        profiler.start("name", info="info")
        p.start.assert_called_once_with("name", info="info")
# ---
def test_count_statements_function():
    tree = ast.parse("def f():\n    return 1\n")
    # FunctionDef + Return = 2.
    assert count_statements(tree) == 2
# ---
def output_autolink(self, m):
        link = m.group(1)
        if m.group(2) == '@':
            is_email = True
        else:
            is_email = False
        return self.renderer.autolink(link, is_email)
# ---
def estimate_memory_bytes(self, candidate: CandidateConfig) -> int:
        """Estimate memory usage in bytes for training a candidate configuration."""
        ...
# ---
def test_explicit_client_overrides_auto_detection():
    """Explicitly set client should override auto-detection."""
    mock_ctx = MagicMock()
    mock_iris_client_lib = MagicMock()
    mock_ctx.client = mock_iris_client_lib

    explicit = LocalClient(max_threads=1)
    with patch("iris.client.client.get_iris_ctx", return_value=mock_ctx):
        with set_current_client(explicit):
            # Should return explicit client, not auto-detected Iris client
            assert current_client() is explicit
# ---
def test_total_consumed_capacity(self):
        """ConsumedCapacity can parse results with only Total"""
        response = {
            "TableName": "foobar",
            "ReadCapacityUnits": 4,
            "WriteCapacityUnits": 5,
        }
        cap = ConsumedCapacity.from_response(response)
        self.assertEqual(cap.total, (4, 5))
        self.assertIsNone(cap.table_capacity)
# ---
def worker_func_der(args):
    self = args[0]
    m = args[1]
    k = args[2]
    r = args[3]
    i = args[4]

    return ((self.eval_func(m, k, r) -
             self.eval_func(m, k, self.rt) -
             self.temporal_diff_sum(m, k)) * 2 *
            self.eval_func_der(m, k, r, i))
# ---
def loss_fn(lin):
            y = lin(x)
            loss = y * dy.astype(y.dtype)
            return hax.sum(loss).scalar()
# ---
def test_connection_host(self):
        """Connection can access host of endpoint"""
        urlparse(self.dynamo.host)
# ---
def testAssignSubscript(self):
    self.assertEqual((0, "{'bar': None}\n"), _GrumpRun(textwrap.dedent("""\
        foo = {}
        foo['bar'] = None
        print foo""")))
# ---
def convert_constraints(resources: ResourceConfig) -> list[Constraint]:
    """Build Iris scheduling constraints from fray v2 ResourceConfig."""
    from iris.cluster.types import preemptible_constraint

    constraints: list[Constraint] = []
    if not resources.preemptible:
        constraints.append(preemptible_constraint(False))
    return constraints
# ---
def testHighFrequency(self):
        hpcp = HPCP(minFrequency=100, maxFrequency=1000)([1001], [1])
        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
# ---
def scan_step(block: ScanModule, carry: hax.NamedArray) -> tuple[hax.NamedArray, hax.NamedArray]:
        return block(carry)
# ---
def _batch_sizes() -> dict[str, int]:
    return {"130m": 128, "300m": 128, "520m": 128, "1_2b": 128}
# ---
def cookies(self):
        if self._cookies is None:
            cookie = self.headers.get("Cookie")
            if cookie is not None:
                cookies = SimpleCookie()
                cookies.load(cookie)
                self._cookies = {
                    name: cookie.value for name, cookie in cookies.items()
                }
            else:
                self._cookies = {}
        return self._cookies
# ---
def submit_task(self, request: cluster_pb2.Worker.RunTaskRequest) -> str: ...
# ---
def format_pwms(pwms):
    return format_line(prefix='pwms'.rjust(RJUST), values=pwms)
# ---
def eval_metrics(self):
    eval_metrics = [
        metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5,
        metrics.Metrics.ACC_PER_SEQ, metrics.Metrics.NEG_LOG_PERPLEXITY
    ]
    if self._was_reversed:
      eval_metrics += [metrics.Metrics.IMAGE_SUMMARY]
    return eval_metrics
# ---
def inherit_docs(cls):
    for name, func in vars(cls).items():
        if not func.__doc__:
            for parent in cls.__bases__:
                try:
                    parfunc = getattr(parent, name)
                except AttributeError: # parent doesn't have function
                    break
                if parfunc and getattr(parfunc, '__doc__', None):
                    func.__doc__ = parfunc.__doc__
                    break
    return cls
# ---
def _is_string_feature(feature) -> bool:
    """Return True if the feature (possibly nested) stores strings."""
    dtype = getattr(feature, "dtype", None)
    if dtype == "string":
        return True
    nested = getattr(feature, "feature", None)
    if nested is not None:
        return _is_string_feature(nested)
    return False
# ---
def test_permutation_handles_edge_case_length_one(PermutationClass):
    length = 1
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    result = permutation(0)
    assert result == 0
# ---
def finished_task_count(self) -> int:
        """Count of tasks in terminal states."""
        return sum(self.task_state_counts[s] for s in TERMINAL_TASK_STATES)
# ---
def unfrackgitpath(path):
    if path is None:
        return None

    # copied from ansible.utils.path
    return os.path.normpath(os.path.realpath(os.path.expanduser(os.path.expandvars(path))))
# ---
def run(self):
        try:
            with fray_default_job_ctx(SyncContext()):
                super().run()
        except BaseException as e:
            self._exception = e
            raise
# ---
def get_attname(self):
		return "%s_json" % self.name
# ---
def create_configuration():
    configuration = mox.MockObject(conf.Configuration)
    configuration.append_config_values(mox.IgnoreArg())
    configuration.nfs_mount_point_base = '/mnt/test'
    configuration.nfs_mount_options = None
    return configuration
# ---
def requires(self):
        return RTask()
# ---
def start_transfer_server() -> jax_transfer.TransferServer:
    """Start JAX transfer server."""
    ip = get_local_ip_from_hostname()
    backend_client = jax.devices()[0].client

    # Use random port binding for proper network resource management
    server = jax_transfer.start_transfer_server(
        backend_client,
        f"{ip}:0",  # Random port binding
        [f"{ip}:0"] * jax.device_count(),
    )
    return server
# ---
def accumulate_logits():
        xw_scratch_ref[...] += jax.lax.dot_general(
            x_ref[...],
            w_ref[...],
            (((1,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
# ---
def test_scatter_add():
    B, S, V = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 5)
    x = hax.zeros((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    ones = hax.ones((B, S))
    y = x.at[{V: idx}].add(ones)
    ref = jnp.zeros((2, 3, 5)).at[jnp.arange(2)[:, None], jnp.arange(3)[None, :], idx.array].add(1.0)
    assert jnp.array_equal(y.array, ref)
# ---
def selected_target_account_index(self):
        return self._selected_target_index
# ---
def invalidate_deadline_caches(sender, **kwargs):  # pylint: disable=unused-argument
    """Invalidate the cached verification deadline information. """
    cache.delete(VerificationDeadline.ALL_DEADLINES_CACHE_KEY)
# ---
def format_names(names):
    return format_line(prefix='names'.rjust(RJUST), values=names)
# ---
def test_migrate_disk_and_power_off(self):
        instance = db.instance_create(self.context, self.instance_values)
        xenapi_fake.create_vm(instance.name, 'Running')
        instance_type = db.instance_type_get_by_name(self.context, 'm1.large')
        conn = xenapi_conn.get_connection(False)
        conn.migrate_disk_and_power_off(self.context, instance,
                                        '127.0.0.1', instance_type, None)
# ---
def assert_messages_in_order(rendered: str, roles: Iterable[str]) -> None:
    search_pos = 0
    for role in roles:
        marker = f"<|start_header_id|>{role}<|end_header_id|>"
        pos = rendered.find(marker, search_pos)
        assert pos != -1, f"Did not find role {role!r} after position {search_pos}"
        search_pos = pos + 1
# ---
def model_type(self) -> Type["Olmo2LMHeadModel"]:
        return Olmo2LMHeadModel
# ---
def pbar_logger(iterable=None, desc="train", **tqdm_mkwargs):
    kwargs = copy.copy(tqdm_mkwargs)
    if "desc" not in kwargs:
        kwargs["desc"] = desc
    if "iterable" not in kwargs:
        kwargs["iterable"] = iterable

    _tqdm_logging_one_time_setup()
    pbar = tqdm(**kwargs)

    def update_pbar(step: StepInfo):
        pbar.update(step.next_step - pbar.n)
        pbar.set_postfix(loss=jnp_to_python(step.loss))

    return update_pbar
# ---
def __init__(self, content):
        '''secret constructor'''
        super(Secret, self).__init__(content=content)
        self._secrets = None
# ---
def test_compare_columns(self):
        expr = col("a") > col("b")
        assert expr.evaluate({"a": 10, "b": 5}) is True
        assert expr.evaluate({"a": 5, "b": 10}) is False
# ---
def prod(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.prod(self, axis=axis, dtype=dtype, where=where)
# ---
def _path_to_key(self, gcs_path: str) -> str:
        return hashlib.sha256(gcs_path.encode()).hexdigest()[:16]
# ---
def __init__(self):
        self.queue = []
        self.leases = {}
# ---
def test_to_numeric(self):
        def test_impl(df):
            B = pd.to_numeric(df.A, errors='coerce')
            return B

        df = pd.DataFrame({'A': ['123.1', '331.2']})
        hpat_func = self.jit(locals={'B': types.float64[:]})(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def apply_chat_template(self, messages, tokenize=True, add_generation_prompt=True):
        """Simple chat template support."""
        prompt = "\n".join([m["content"] for m in messages])

        if tokenize:
            return self.encode(prompt)
        return prompt
# ---
def insertIndent():
            if self.useTabs:
                cursor.insertText('\t')
            else:  # indent to integer count of indents from line start
                charsToInsert = self.width - (len(self._qpart.textBeforeCursor()) % self.width)
                cursor.insertText(' ' * charsToInsert)
# ---
def fold(
    fn: Callable,
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable: ...
# ---
def test_before_start_date(self):
        """Test when create_time is before the start date"""
        self.assertFalse(check_create_time("2022-12-31 23:59:59 PST", "2023-01-01", "2023-01-31"))
# ---
def callback(ctx: click.Context, json_str: str | None, **kwargs):
            field_values = {k: v for k, v in kwargs.items() if v is not None}
            request = build_request(method, json_str, field_values)
            response = call_rpc(self.service_name, method.name, controller_url, request)
            click.echo(format_response(response))
# ---
def bad_loss(model, batch):
        return hax.sum(batch), [1, 2, 3]
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        """Generate examples."""
        ...
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step, commit=None):
        for tracker in self.loggers:
            tracker.log(metrics, step=step, commit=commit)
# ---
def get_vocab(self):
        return self._vocab
# ---
def _get_data_dir(self, db_version):
        # Try to get from svc first
        output = run('svcprop -p config/data postgresql')
        if output.stdout and exists(output.stdout, use_sudo=True):
            return output.stdout
        return base_postgres.PostgresInstall._get_data_dir(self, db_version)
# ---
def test_new_websocket_client_token_invalid(self, check_token):
        check_token.return_value = False

        self.wh.path = "ws://127.0.0.1/?token=XXX"

        self.assertRaises(exception.InvalidToken,
                          self.wh.new_websocket_client)
        check_token.assert_called_with(mock.ANY, token="XXX")
# ---
def sinc(a: A) -> A:
    return wrap_elemwise_unary(jnp.sinc, a)
# ---
def bmarks():
    return_data = do_register()
    return return_data
# ---
def test_can_scale_up_when_below_max(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """can_scale_up() returns True when below max_slices."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        assert group.can_scale_up()
# ---
def register(self, type_):
        obj = type_(self)
        self.types[obj._type] = obj
# ---
def and_(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def test_gumbel():
    check_gen_is_equal(lambda k, s: jax.random.gumbel(k, s), lambda k, s: hax.random.gumbel(k, s))
# ---
def calculate_volatility(self, daily_returns):
        return np.std(daily_returns, ddof=1) * math.sqrt(self.num_trading_days)
# ---
def test_find_span_end_no_match():
    source = "x = 1\n"
    # Offset 99 doesn't correspond to any node.
    end = _find_span_end(source, 99)
    assert end is None
# ---
def ptp(self, axis: AxisSelection | None = None) -> "NamedArray":  # pragma: no cover
        return haliax.ptp(self, axis=axis)
# ---
def __lt__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.less(self, other)
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return df.four.nunique()
# ---
def eval_func(self, m, k, r):
        """
        The evaluation function value for the set of weights (vector) r
        at the mth game and kth board state """
        return np.dot(r, self.data[m][k])
# ---
def cd(self, name: str) -> "InputName":
        """Refer to the `name` under `self`'s output_path."""
        return InputName(self, name=name)
# ---
def list_tasks(self, job_id: JobName) -> list[cluster_pb2.TaskStatus]:
        """List all tasks for a job.

        Args:
            job_id: Job identifier

        Returns:
            List of TaskStatus protos, one per task
        """
        return self._cluster_client.list_tasks(job_id)
# ---
def __init__(self, base_uri=None):
        super(BiophysicalPerisomaticApi, self).__init__(base_uri)
        self.cache_stimulus = True
        self.ids = {}
        self.sweeps = []
        self.manifest = {}
# ---
def create_transaction(node, txid, to_address, *, amount):
    """ Return signed transaction spending the first output of the
        input txid. Note that the node must have a wallet that can
        sign for the output that is being spent.
    """
    raw_tx = create_raw_transaction(node, txid, to_address, amount=amount)
    tx = tx_from_hex(raw_tx)
    return tx
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"backbone": None, "embeddings": None}
# ---
def allclose(a: NamedArray, b: NamedArray, rtol=1e-05, atol=1e-08, equal_nan=False) -> bool:
    """Returns True if two arrays are element-wise equal within a tolerance."""
    a, b = broadcast_arrays(a, b)
    return bool(jnp.allclose(a.array, b.array, rtol=rtol, atol=atol, equal_nan=equal_nan))
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        matched = self._matcher.match_relative(dir, True)
        if matched:
            # Everything in the directory is selected (ignored)
            return "all"
        else:
            # Not sure
            return True
# ---
def pop_all(self) -> list[RolloutBatch]:
        """Pop all available batches without blocking."""
        with self._lock:
            batches = self._queue[:]
            self._queue.clear()
            self._not_full.notify_all()
            return batches
# ---
def getIcon(self):
        return  QtGui.QIcon(os.path.dirname(__file__) + "/../images/saga.png")
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[False] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[True] = ...,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def _trans_rollback_fn(self, is_transaction=False):
        def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
            raise Exception("breakage")
        return go
# ---
def __getattr__(self, attr):
        if attr not in ('_context', '_sock', '_connection', '_makefile_refs'):
            return getattr(self._connection, attr)
# ---
def matches_constraints(self, constraints: Sequence[cluster_pb2.Constraint]) -> bool:
        """Check if this worker matches all given constraints."""
        for constraint in constraints:
            attr = self.attributes.get(constraint.key)
            if not _evaluate_constraint(attr, constraint):
                return False
        return True
# ---
def _choose_port(id):
    port = int(id) % 2**12 + (65535 - 2**12 + 1)
    return port
# ---
def i_delete_the_selected_objects():
    bpy.ops.object.delete()
    blenderbim.bim.handler.active_object_callback()
# ---
def nanargmin(array: NamedArray, axis: AxisSelector | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.nanargmin, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def get_power(self):
        return self._power
# ---
def path_to_step_name(path):
    # we want llama-8b-tootsie-phase2-730000
    components = path.split("/")
    step = components[-2].split("-")[-1]
    name = components[-4].split("/")[-1]
    return f"analysis/viz/{name}-{step}"
# ---
def _sympy_parse(expr: str):
    """Parses an expression with sympy."""
    py_expr = expr.replace("^", "**")
    return sympy_parser.parse_expr(
        py_expr,
        transformations=(
            sympy_parser.standard_transformations
            + (sympy_parser.implicit_multiplication_application,)
        ),
    )
# ---
def matchfn(self, f):
        match = self._matcher
        return match(f) or any(map(match, util.dirs((f,))))
# ---
def logprobs_from_choice(self, choice: Choice) -> np.ndarray:
        """Extract logprobs array."""
        if not choice.logprobs or not choice.logprobs.content:
            raise ValueError("Choice missing logprobs. Use logprobs=True in API call.")

        logprobs = np.array([t.logprob for t in choice.logprobs.content], dtype=np.float32)

        if np.all(logprobs == 0):
            logger.warning("All logprobs are zero - may cause NaN loss")

        return logprobs
# ---
def update_events(self):
        '''
        We may attach events to this track before setting their `track` parameter.
        This method will move through all events and set their track to this track.
        '''
        for event in self.events:
            event.track = self
# ---
def _get_clean_children(self, element):
        return [c for c in element.children if not (isinstance(c, NavigableString) and not c.strip())]
# ---
def test_sliding_window_mask():
    Pos = hax.Axis("pos", 16)
    KeyPos = Pos.alias("key_pos")
    window = 4
    mask = AttentionMask.causal(sliding_window=window)
    mat = mask.materialize(Pos, KeyPos)
    q_pos = hax.arange(Pos)
    k_pos = hax.arange(KeyPos)
    diff = q_pos.broadcast_axis(KeyPos) - k_pos.broadcast_axis(Pos)
    expected = (diff >= 0) & (diff < window)
    assert hax.all(mat == expected)
# ---
def get_item_attribute(parent, attribute_value=''):
	if not frappe.has_permission("Item"):
		frappe.msgprint(_("No Permission"), raise_exception=1)

	return frappe.get_all("Item Attribute Value", fields = ["attribute_value"],
		filters = {'parent': parent, 'attribute_value': ("like", "%%%s%%" % attribute_value)})
# ---
def _run_viz():
        with remove_tpu_lockfile_on_exit():
            do_viz_lm(levanter_config)
# ---
def __init__(self, nr_eval, input_names, output_names, get_player_fn):
        self.eval_episode = nr_eval
        self.input_names = input_names
        self.output_names = output_names
        self.get_player_fn = get_player_fn
# ---
def _serialize_config(self) -> str:
        """Serialize cluster config to YAML for the controller VM."""
        import yaml

        return yaml.dump(config_to_dict(self.config), default_flow_style=False)
# ---
def dataloader_iterator(
    loader,
    *,
    seq_len: int,
) -> Iterator[dict[str, jax.Array]]:
    while True:
        batch = next(loader)
        tokens = batch[:, :seq_len]
        yield {"tokens": tokens[:, :-1], "labels": tokens[:, 1:]}
# ---
def use_flash_attention(self) -> bool:
        """Whether to use flash attention based on the backend."""
        if self.attn_backend is None:
            return default_attention_type() != AttentionBackend.VANILLA
        return self.attn_backend != AttentionBackend.VANILLA
# ---
def update_gg(idx, gg):
        if gg is None:
            return None
        outer_product = jnp.tensordot(
            grad,
            grad,
            axes=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]] * 2,
            precision=precision,
        )
        return lerp(gg, outer_product, 1 - beta)
# ---
def _make_or_expr(self, check1, _or, check2):
        """Create an 'or_expr'.

        Join two checks by the 'or' operator.
        """

        return [('or_expr', OrCheck([check1, check2]))]
# ---
def _root_key(self):
        return "LicenseInfo"
# ---
def delete(self):
            pass
# ---
def register_preference_adapter(adapter: PreferenceTransformAdapter):
    preference_transform_templates[adapter.source] = adapter
# ---
def __init__(self, crash_point: int):
            self.crash_point = crash_point
# ---
def _parse_line(self, line):
        m = self.COPY_DISK_RE.match(line)
        if m is None:
            raise OutputParserError('unexpected format in "Copying disk"'
                                    ', line: %r' % line)
        return m.group(1), m.group(2), m.group(3)
# ---
def __init__(self, xPos, yPos):
        super(Actor, self).__init__(xPos, yPos)
        self.speed = 5
        self.dy = 0
        self.d = 3
        self.dir = "right"
        # self.newdir = "right"
        self.state = "standing"
        self.walkR = []
        self.walkL = []
# ---
def hardbreak(node: RenderTreeNode, context: RenderContext) -> str:
    if _in_block("heading", node):
        return "<br /> "
    return "\\" + "\n"
# ---
def unflatten_axis(self, axis: AxisSelector, new_axes: AxisSpec) -> "NamedArray":  # pragma: no cover
        return haliax.unflatten_axis(self, axis=axis, new_axes=new_axes)
# ---
def test_kill_nonexistent_task(worker):
    """Test killing a nonexistent task returns False."""
    result = worker.kill_task(JobName.root("nonexistent-task").task(0).to_wire())
    assert result is False
# ---
def test_write_vortex_single_record(self, tmp_path):
        """Test writing single record."""
        records = [{"key": "value", "number": 42}]
        output_path = tmp_path / "single.vortex"

        result = write_vortex_file(records, str(output_path))
        assert result["count"] == 1

        loaded = list(load_vortex(str(output_path)))
        assert loaded == records
# ---
def _make_test_entrypoint() -> cluster_pb2.Entrypoint:
    """Create a minimal Entrypoint proto for testing."""
    entrypoint = cluster_pb2.Entrypoint()
    entrypoint.command.argv[:] = ["python", "-c", "pass"]
    return entrypoint
# ---
def create_zstd_compressed_jsonl(records: list[dict]) -> bytes:
    """Create zstd compressed JSONL content."""
    jsonl_content = "\n".join(json.dumps(record) for record in records) + "\n"
    jsonl_bytes = jsonl_content.encode("utf-8")
    cctx = zstd.ZstdCompressor()
    return cctx.compress(jsonl_bytes)
# ---
def uniform(
    key, shape: AxisSpec, dtype=float, minval: NamedOrNumeric = 0.0, maxval: NamedOrNumeric = 1.0
) -> NamedArray:
    shape = axis_spec_to_shape_dict(shape)
    minval = broadcast_to(minval, shape).array
    maxval = broadcast_to(maxval, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.uniform(key=key, shape=jax_shape, dtype=dtype, minval=minval, maxval=maxval)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def _convert_to_hf_url(model_id: str, revision: Optional[str] = None) -> str:
    """Convert a HuggingFace model ID to an hf:// URL for fsspec streaming.

    Args:
        model_id: HuggingFace model ID like "meta-llama/Llama-2-7b"
        revision: Optional git revision (branch, tag, or commit hash)

    Returns:
        An hf:// URL like "hf://meta-llama/Llama-2-7b" or "hf://meta-llama/Llama-2-7b@main"
    """
    if revision:
        return f"hf://{model_id}@{revision}"
    return f"hf://{model_id}"
# ---
def _recreate(self, changes):
        """Recreate the window with current attributes.

        :Parameters:
            `changes` : list of str
                List of attribute names that were changed since the last
                `_create` or `_recreate`.  For example, ``['fullscreen']``
                is given if the window is to be toggled to or from fullscreen.
        """
        raise NotImplementedError('abstract')
# ---
def RegisterWorker(self, request, context):
        """Worker management (workers send heartbeats to controller)"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def test_dense_regularization(self):
    layer = keras.layers.Dense(
        3,
        kernel_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l1',
        activity_regularizer='l2',
        name='dense_reg')
    layer(keras.backend.variable(np.ones((2, 4))))
    self.assertEqual(3, len(layer.losses))
# ---
def test_simple_limit_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).limit(2).offset(1),
            [(2, 2, 3), (3, 3, 4)],
        )
# ---
def get_task(self, task_id: str) -> TaskInfo | None: ...
# ---
def h_fs_put(_,path,data):
        f=open(path,'w')
        for x in data: f.write(unescape(x))
        f.close()
        pass
# ---
def product(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.product, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def main(cfg: DownloadConfig) -> None:
    """CLI entrypoint for downloading and processing Ar5iv dataset."""
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
    download(cfg)
# ---
def stop(self) -> None:
        """Signal the thread to stop (but don't wait for it to exit).

        To wait for the thread to exit, call join() separately. This allows
        ThreadContainer to manage multiple thread timeouts globally.
        """
        self._stop_event.set()
        logger.debug("Signaled thread %s to stop", self._thread.name)
# ---
def decode(self, token_ids):
            return "".join(chr(tid) for tid in token_ids)
# ---
def getComboBoxList(self, combobox):
        """Get the list of values from the active combox
        """
        active = combobox.get_active()
        model = combobox.get_model()
        activeIter = model.get_iter(active)
        activeLabel = model.get_value(activeIter, 0)
        activeName = model.get_value(activeIter, 1)
        return [activeLabel, activeName]
# ---
def close(self):
        self.close_request.set()
# ---
def do_block(carry, block, *args, **kwargs):
            carry = block(carry, *args, **kwargs)
            return carry
# ---
def managed(self):
        return None
# ---
def name_get(self, cr, uid, ids, context=None):
        if not ids:
            return []
        reads = self.read(cr, uid, ids, ['name', 'prefix', 'ref'], context)
        res = []
        for record in reads:
            name = record['name']
            prefix = record['prefix']
            if prefix:
                name = prefix + '/' + name
            if record['ref']:
                name = '%s [%s]' % (name, record['ref'])
            res.append((record['id'], name))
        return res
# ---
def test_arithmetic(self):
        expr = col("a") + col("b")
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def vms(self) -> list[ManagedVm]:
        return list(self._vms)
# ---
def close(self):
        return
# ---
def __get__(self, instance, owner):
		if instance is None:
			raise AttributeError # ?

		if self.field.name not in instance.__dict__:
			json_string = getattr(instance, self.field.attname)
			instance.__dict__[self.field.name] = json.loads(json_string)

		return instance.__dict__[self.field.name]
# ---
def stop(self):
        self.running = False
# ---
def test_classify_indices_to_db_no_connection(mock_db, mock_jw):
    mock_db.connected_to_db.return_value = False

    with s.app.test_client() as c:
        resp = c.get('/api/v1/classify_documents/to_database?directory=test')
        assert not mock_jw.called
# ---
def current_attempt_id(self) -> int:
        """ID of current attempt (0-indexed), or -1 if no attempts."""
        return len(self.attempts) - 1 if self.attempts else -1
# ---
def int_step(self) -> int:
        """
        Returns the step as an int. On multinode, doing
        """
        return int(self.step)
# ---
def _eq_loaded(self):
        if self.lc:
            return True
        else:
            print("Load eQulibrator local cache.")
            return False
# ---
def main(config: Config):
        assert "wikitext" in config.data.components
        comp = config.data.components["wikitext"]
        assert isinstance(comp.source, UrlDatasetSourceConfig)
        assert comp.cache_dir == "gs://levanter-data/tokenized/wikitext"
# ---
def test_default_tokenize_with_dataset_name():
    step = default_tokenize(
        name="dummy",
        dataset=HfDatasetSpec(id="cnn_dailymail", name="3.0.0"),
        tokenizer="gpt2",
    )
    assert isinstance(step.config, HfTokenizeConfig)
    assert step.config.id == "cnn_dailymail"
    assert step.config.name == "3.0.0"
# ---
def __init__(self, root, cwd, badfn=None, rules=[]):
        super(treematcher, self).__init__(root, cwd, badfn)
        rules = list(rules)
        self._matcher = pathmatcher.treematcher(rules)
        self._rules = rules
# ---
def _multiply_property(self, name, factor):
        self.command('multiply_property', name, factor)
# ---
def get_process_logs(self, request: cluster__pb2.Controller.GetProcessLogsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetProcessLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def __init__(self, qpart):
        self._qpart = qpart

        self.width = self._DEFAULT_INDENT_WIDTH
        self.useTabs = self._DEFAULT_INDENT_USE_TABS

        self._smartIndenter = _getSmartIndenter('normal', self._qpart, self)
# ---
def GetTaskStatus(self, request, context):
        """Task operations"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __iter__(self):
        """Flat map over all chunks."""
        for chunk_data in self.iter_chunks():
            yield from chunk_data
# ---
def init_hotkeys(self):
        self.main_model.hotkey_model.add_hotkey(["Lcontrol", "Lmenu", "J"], self.main_view.focus_job_num_edit)
        self.main_model.hotkey_model.add_hotkey(["Lcontrol", "Lmenu", "O"], self.main_view.open_current_job_folder)
        self.main_model.hotkey_model.add_hotkey(["Lcontrol", "Lmenu", "B"], self.main_view.open_current_job_basecamp)
        self.main_model.hotkey_model.start_detection()
# ---
def __init__(_,ws):_.ws=ws
# ---
def load_rule_file(rules_dir, filename):
    """Load a rule file and return its content, or empty string if not found."""
    if not rules_dir:
        return ""
    path = os.path.join(rules_dir, filename)
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except (OSError, IOError):
        return ""
# ---
def vref(self):
        """Reference voltage used by the chip. You need to set this. It defaults to 3.3V"""
        return self._Vref
# ---
def dummy_weight_callback(model):
        time.sleep(0.1)  # Simulate some work
        return model
# ---
def send_serialdata(self, node):
        if isinstance(node, JsonItem):
            if self.serial.isOpen():
                s = node.to_json()
                self.serial.write(utf8_to_bytearray(s + '\n'))
                self.loggingWidget.log_output(s.strip())
# ---
def set_iter(self, max_iter):
        self.max_iter = max_iter
# ---
def newphoto():
   global image1
   image1 =  takephoto()

   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def test_api_suite() -> None:
    assert repr(Bloom(27_000, 0.0317)) == "<Bloom size_in_bits=193960 approx_items=0.0>"

    _test_bloom(Bloom(13242, 0.0000001))
    _test_bloom(Bloom(9874124, 0.01))
    _test_bloom(Bloom(2837, 0.5))

    operations_with_self()
    test_chunked_write()

    print("All API tests passed")
# ---
def test_deadline_expires():
    """Deadline with short timeout expires after sleeping."""
    deadline = Deadline.from_seconds(0.1)
    time.sleep(0.15)
    assert deadline.expired()
# ---
def main(argv=sys.argv[1:]):
    return KanboardShell().run(argv)
# ---
def round_to_power_of_two(x: float) -> int:
    """Round ``x`` to the nearest power of two."""
    if x <= 1:
        return 1
    return 2 ** math.ceil(math.log2(x))
# ---
def get_output_shape_for(self, input_shape):
        if self.dim_ordering == 'channels_last':
            return (input_shape[0], input_shape[3])
        else:
            return (input_shape[0], input_shape[1])
# ---
def setUp(self):
        super(JsonToYamlTest, self).setUp()
        self.expected_test_count = 2
        self.longMessage = True
        self.maxDiff = None
# ---
def _pick_free_port(host: str) -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind((host, 0))
        return int(sock.getsockname()[1])
# ---
def log_hyperparameters(hparams: dict[str, Any]):
    """
     Log hyperparameters to the global tracker.

    Args:
         hparams: Hyperparameters to log
    """
    global _global_tracker
    if _global_tracker is None:
        warnings.warn("No global tracker set")
        return

    _global_tracker.log_hyperparameters(hparams)
# ---
def test_window(backend):
    """Test window operation (same as batch)."""
    ds = Dataset.from_list([[1, 2, 3, 4, 5]]).flat_map(lambda x: x).window(2)
    windows = list(Backend.execute(ds, context=backend))
    assert windows == [[1, 2], [3, 4], [5]]
# ---
def uipath(self, f):
        """Convert repo path to a display path.  If patterns or -I/-X were used
        to create this matcher, the display path will be relative to cwd.
        Otherwise it is relative to the root of the repo."""
        return (self._relativeuipath and self.rel(f)) or self.abs(f)
# ---
def post():
    return make_post()
# ---
def obj_id(self):
                area_id = Regexp(CleanText('(./preceding-sibling::tr[@class="LnMnTiers"][1])//span[@class="CelMnTiersT1"]'),
                            r'\((\d+)\)', default='')(self)
                acc_id = Regexp(CleanText('./td[1]'), r'(\d+)\s*(\d+)', r'\1\2')(self)
                if area_id:
                    return '%s.%s' % (area_id, acc_id)
                return acc_id
# ---
def testGradientInput1(self):
    with self.test_session(use_gpu=False):
      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2],
                   dtype=tf.float64, name="x")
      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],
                   shape=[2, 4], dtype=tf.float64, name="y")
      m = tf.matmul(x, y, name="matmul")
      err = gc.ComputeGradientError(y, [2, 4], m, [3, 4])
    print("matmul input1 gradient err = ", err)
    self.assertLess(err, 1e-10)
# ---
def evaluate(self, record: dict) -> Any:
        """Evaluate expression against a record."""
        pass
# ---
def test_stmt_exception_pickleable_plus_dbapi(self):
        raw = testing.db.raw_connection()
        the_orig = None
        try:
            try:
                cursor = raw.cursor()
                cursor.execute("SELECTINCORRECT")
            except testing.db.dialect.dbapi.DatabaseError as orig:
                # py3k has "orig" in local scope...
                the_orig = orig
        finally:
            raw.close()
        self._test_stmt_exception_pickleable(the_orig)
# ---
def _get_field_mapping(self, schema):
        """Get mapping for given field schema."""
        if 'mapping' in schema:
            return schema['mapping']
        elif schema['type'] == 'datetime':
            return {'type': 'date'}
        elif schema['type'] == 'string' and schema.get('unique'):
            return {'type': 'string', 'index': 'not_analyzed'}
# ---
def _generate_digest_challenge(ts, secret, realm, opaque, stale=False):
	nonce = _generate_nonce(ts, secret)
	return 'Digest %s' % (_format_kvpairs(
		realm=realm,
		qop='auth',
		nonce=nonce,
		opaque=opaque,
		algorithm='MD5',
		stale='true' if stale else 'false'
	),)
# ---
def on_hide():
            """The window was hidden.

            This event is triggered when a window is minimised or (on Mac OS X)
            hidden by the user.

            :event:
            """
# ---
def init_fn(key, use_b):
        k_a, k_b = jax.random.split(key)
        return MyModule(a=hax.random.normal(k_a, (In, Out)), b=hax.random.normal(k_b, (In, Out)) if use_b else None)
# ---
def createDirectory( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def __init__(self, array: NamedArray):
        self.array = array
# ---
def angle(a: A) -> A:
    return wrap_elemwise_unary(jnp.angle, a)
# ---
def model_type(self) -> type["HackableLMHeadModel"]:
        return HackableLMHeadModel
# ---
def test_pspec_for_named_axes():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        mod = MyModule(named=hax.ones((Dim1, Dim2, Dim3)), unnamed1=jnp.ones(Dim2.size), static_field=1)

        specs: MyModule = pspec_for(mod)

        spec = PartitionSpec(None, ResourceAxis.DATA, ResourceAxis.MODEL)

        assert specs.named == spec
        assert specs.unnamed1 == PartitionSpec(None)
# ---
def setup_only(context):
        """A fixture with no `teardown()`."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        return setup
# ---
def _navp_xml(self, entry, indent_lvl):
        'Write xml for an entry and all its subentries.'
        xml = self._navp.format('  '*indent_lvl, str(entry.no), entry.text,
          entry.target)
        for sub in entry.entries:
            xml += self._navp_xml(sub, indent_lvl+1)
        xml += '  '*indent_lvl + '</navPoint>\n'
        return xml
# ---
def _to_epoch_time(date):
    """Convert a `datetime` object to an integer number of seconds since
    the (local) Unix epoch.
    """
    if hasattr(date, 'timestamp'):
        # The `timestamp` method exists on Python 3.3+.
        return int(date.timestamp())
    else:
        epoch = datetime.fromtimestamp(0)
        delta = date - epoch
        return int(delta.total_seconds())
# ---
def get_col_str(col_desc):
    return col_desc['name'] + DELIMITER(" (") + col_desc['type'] + DELIMITER(")")
# ---
def child(self, name: str) -> "JobName":
        """Create a child job name."""
        return JobName((*self._parts, name))
# ---
def metrics_loss_fn(model, batch, key=None):
    """Loss function returning (loss, metrics) tuple."""
    loss = hax.sum(batch * model.weight)
    metrics = {"accuracy": jnp.array(0.95), "perplexity": jnp.array(2.5)}
    return loss, metrics
# ---
def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "kapow", "boing": "thunk"}
# ---
def finish(self):
        pass
# ---
def get_block(self, header_hash: bytes) -> Optional[Block]:
        with self.lock:
            return self._state.get_block(header_hash)
# ---
def custom_fn(pred):
        pred_embeddings, pred_lm_head = pred
        loss = fused_cross_entropy_loss_and_logsumexp_penalty(
            pred_embeddings,
            pred_lm_head,
            Contract=Embed,
            Label=Vocab,
            target_y=true_ids,
            reduction=None,
            logsumexp_weight=0.0,
            block_size=2,
            dtype=pred_embeddings.dtype,
        )

        return loss.mean().scalar()
# ---
def testSComplexBasic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.complex64)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.complex64)
    self._testCpuMatmul(x, y)
# ---
def explain(self, f):
        return self._matcher.explain(f, True)
# ---
def meets_length_threshold(record: dict) -> bool:
        """Check if document meets minimum length requirement."""
        if config.min_length is None:
            return True
        length = record.get("metadata", {}).get("length", 0)
        return length >= config.min_length
# ---
def output_text(self, m):
        text = m.group(0)
        return self.renderer.text(text)
# ---
def test_url_base(self):
        self.assertEqual(self.xe.url_base, 'https://{0}:{1}/api/v1'.format(node, port))
# ---
def __init__(self, field, pattern, fast=True):
        self.field = field
        self.pattern = pattern
        self.fast = fast
# ---
def test_named_ref_jit_plumbing():
    X = hax.Axis("x", 5)
    ref = hax.new_ref(hax.zeros(X))

    @jax.jit
    def write_and_read(ref):
        ref[{"x": 1}] = 4.2
        return ref[{"x": 1}]

    out = write_and_read(ref)
    assert isinstance(out, hax.NamedArray)
    assert out.axes == ()
    assert pytest.approx(out.array.item()) == 4.2
    assert jnp.allclose(ref.value()[{"x": 1}].array, jnp.asarray(4.2))
# ---
def get_references(self):
        return self.references
# ---
def axes(self) -> tuple[Axis, ...]:
        shape = jnp.shape(self.array)
        if len(shape) != len(self.axis_names):
            raise ValueError(
                f"Shape of underlying array {shape} does not match number of axes {self.axis_names}. {self.array}"
            )

        return tuple(Axis(name, size) for name, size in zip(self.axis_names, shape))
# ---
def add_default_uom_in_conversion_factor_table(self):
		uom_conv_list = [d.uom for d in self.get("uoms")]
		if self.stock_uom not in uom_conv_list:
			ch = self.append('uoms', {})
			ch.uom = self.stock_uom
			ch.conversion_factor = 1

		to_remove = []
		for d in self.get("uoms"):
			if d.conversion_factor == 1 and d.uom != self.stock_uom:
				to_remove.append(d)

		[self.remove(d) for d in to_remove]
# ---
def find_checkpoint_root(path):
        """Find the checkpoint root by looking for metadata.json"""
        current = path
        while current and current != os.path.dirname(current):
            metadata_path = os.path.join(current, "metadata.json")
            if fsspec_utils.exists(metadata_path):
                return current
            current = os.path.dirname(current)
        return path
# ---
def finished(status: Self) -> bool:
        return status in (JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED)
# ---
def __init__(self):
        self.file = None
        self.format = 1
        self.tracks = []
        self.ticks_per_quarter_note = 1024
        self.ticks_per_second = None
# ---
def test_environment_with_base():
    with TemporaryVenv() as venv:
        base_env = {"CUSTOM_VAR": "value", "PATH": "/custom/path"}
        env = venv.get_env(base_env=base_env)
        assert env["VIRTUAL_ENV"] == venv.venv_path
        assert env["CUSTOM_VAR"] == "value"
        assert venv.bin_path in env["PATH"]
        assert "/custom/path" in env["PATH"]
# ---
def _connect_networks_to_routers(self, lnetworks, lrouters, networks_per_router):
        for lrouter in lrouters:
            LOG.info("Connect %s networks to router %s" % (networks_per_router, lrouter["name"]))
            for lnetwork in lnetworks[:networks_per_router]:
                LOG.info("connect networks %s cidr %s" % (lnetwork["name"], lnetwork["cidr"]))
                self._connect_network_to_router(lrouter, lnetwork)

            lnetworks = lnetworks[networks_per_router:]
# ---
def protect(self, tag: str) -> None:
        """Increment refcount for an image (job using it)."""
        self._image_refcounts[tag] = self._image_refcounts.get(tag, 0) + 1
# ---
def __init__(self, x=0.0, y=0.0, z=0.0):
        super(Vector3D, self).__init__(x, y, z)
# ---
def click_and_hold(self, on_element):
        """Holds down the left mouse button on an element.
        Args:
            on_element: The element to mouse down.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.MOUSE_DOWN, {}))
        return self
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: AttentionMask | NamedArray | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        """Compute the activations for the next token in a sequence."""
        x = self.embeddings.embed(input_ids)
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def setUp(self):
        testdata.run()
# ---
def _get_query_pos_renames(Pos):
    new_Pos: list[Axis] = []
    renames: dict[str, str] = {}
    for i, axis in enumerate(Pos):
        ax_name = axis_name(axis)
        axis = axis.alias(f"q_{ax_name}")
        renames[ax_name] = axis.name
        new_Pos.append(axis)

    return tuple(new_Pos), renames
# ---
def _munge_address_port(address: str):
        # the coordinator address typically includes a port that jax wants to use. we want to use our own port
        # we add a deterministic number to the chosen port and then cycle through the ephemeral range
        # this is a hack, but it works
        host, port_str = address.split(":")
        port = int(port_str)
        return host, port
# ---
def start(
        self,
        *,
        model_name_or_path: str,
        host: str,
        port: int | None,
        timeout_seconds: int,
        extra_cli_args: list[str] | None,
    ) -> VllmServerHandle:
        raise NotImplementedError
# ---
def run_vllm_inference(model_path, **model_init_kwargs):
    llm = LLM(model=model_path, **model_init_kwargs)

    sampling_params = SamplingParams(
        max_tokens=100,
        temperature=0.7,
    )

    generated_texts = llm.generate(
        "Hello, how are you?",
        sampling_params=sampling_params,
    )

    return generated_texts
# ---
def __init__(self, method: Any, executor: ThreadPoolExecutor):
        self._method = method
        self._executor = executor
# ---
def replace(self, resource, id_, document):
        args = self._es_args(resource, refresh=True)
        return self.es.index(body=document, id=id_, **args)
# ---
def get_profile():
    return addon.getAddonInfo('profile').decode('utf-8')
# ---
def nanvar(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanvar, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def done(queue_name: str, lease_id: str = Body(...), timestamp: float = Body(...)):
            if queue_name not in self.queues:
                return Response(status_code=404)
            lease = Lease(item=None, lease_id=lease_id, timestamp=timestamp)
            self.queues[queue_name].done(lease)
            return {"status": "ok"}
# ---
def require_permissions(user, *permissions):
    for perm in permissions:
        if not user.has_perm(perm):
            raise PermissionRequired(perm)
# ---
def test_visualize_shardings_model_axis(capsys):
    devices = jax.devices()
    mesh = jax.sharding.Mesh(np.array(devices).reshape(-1, 2), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping({"dim1": ResourceAxis.DATA, "dim2": ResourceAxis.MODEL}), mesh:
        arr = hax.ones((Dim1, Dim2))
        visualize_shardings(arr)

    out = capsys.readouterr().out
    assert "dim2" in out
# ---
def __exit__(self, *args, **kwargs):
    _GLOBAL_CUSTOM_OBJECTS.clear()
    _GLOBAL_CUSTOM_OBJECTS.update(self.backup)
# ---
def status(self) -> VmGroupStatus:
        """Compute status from current VM states."""
        snapshots = [
            VmSnapshot(
                vm_id=vm.info.vm_id,
                state=vm.info.state,
                address=vm.info.address,
                init_phase=vm.info.init_phase,
                init_error=vm.info.init_error,
            )
            for vm in self._vms
        ]
        return VmGroupStatus(vms=snapshots)
# ---
def __send_memoryview(self, data, flags=0):
        if hasattr(data, 'tobytes'):
            data = data.tobytes()
        return self.__send(data, flags)
# ---
def validate_item_type(self):
		if self.has_serial_no == 1 and self.is_stock_item == 0 and not self.is_fixed_asset:
			msgprint(_("'Has Serial No' can not be 'Yes' for non-stock item"), raise_exception=1)

		if self.has_serial_no == 0 and self.serial_no_series:
			self.serial_no_series = None
# ---
def _is_expert(self):
        profile = self._get_profile()
        return profile.user_type == EXPERT_USER_TYPE.lower()
# ---
def _setup_regular(self, env):
        super(Selection, self)._setup_regular(env)
        assert self.selection is not None, "Field %s without selection" % self
# ---
def _shardmap_histogram(a: NamedArray, bins):
    spec = hax.partitioning.pspec_for_axis(a.axes)
    flattened_spec = _flattened_spec(spec)

    def _wrapped_hist(arr):
        return _single_shard_histogram(arr, bin_edges=bins, reduce_mesh=flattened_spec)

    shard_h = shard_map(_wrapped_hist)
    res = shard_h(a)

    # the filter misses the last bin, so we need to add it
    if res.size >= 1:
        res = res.at[-1].add(1)
    return res
# ---
def server(service):
    """Create WorkerDashboard."""
    return WorkerDashboard(service=service, host="127.0.0.1", port=0)
# ---
def __create_navigator(self, main_root):
		navigator_dir = os.path.join(main_root, 'navigator')
		safe_mkdir(navigator_dir)

		navigators = self.__config_mgr.navigator()
		self.__navigator_mgr.create_navigators(navigators, navigator_dir)
# ---
def parse_news(item):
    '''Parse news item
    return is a tuple(id, title, url)
    '''
    url = 'http://www.spa.gov.sa' + item['href']
    url_parsed = urlparse(url)
    qs = parse_qs(url_parsed[4])
    id = qs['newsid'][0]
    title = item.h2.contents[0]
    title = " ".join(title.split())
    item_parsed = (id, title, url)
    return item_parsed
# ---
def reset():
            model.solver.objective = reverse_value
            model.solver.objective.direction = reverse_value.direction
# ---
def delete_vm(zone: str, vm_name: str):
        """Delete a single TPU VM."""
        logging.info(f"Deleting {vm_name} in {zone}")
        run_sh(
            f"gcloud compute tpus tpu-vm delete {vm_name} --zone {zone} --project {config.GCP_PROJECT_ID} --quiet",
            check=False,
        )
# ---
def interaction_alts():
    return pd.DataFrame({
        'prop': [10, 20, 30, 40]},
        index=[1, 2, 3, 4])
# ---
def killall():
    global player
    status = 0
    if player == 'omxplayer':
        control = "/usr/local/bin/omxcontrol"
        status = subprocess.call([control,  "stop"])
    status = subprocess.call(["pkill", player])

    return status
# ---
def __init__(
        self,
        job_id: JobId,
        request: JobRequest,
        processes: list[subprocess.Popen],
        process_env: TemporaryVenv | None,
        start_time: float,
    ):
        self.job_id = job_id
        self.request = request
        self.processes = processes
        self.process_env = process_env
        self.replica_count = len(processes)
        self.start_time = start_time
        self.log_queue: Queue[str] = Queue()
        self._log_threads: list[Thread] = []
# ---
def test_filter_expression_repr():
    """Test FilterOp repr with expression."""
    from zephyr import col
    from zephyr.dataset import FilterOp

    expr = col("score") > 50
    op = FilterOp(predicate=expr.evaluate, expr=expr)
    assert "FilterOp(expr=" in repr(op)
    assert "col('score')" in repr(op)
# ---
def __init__(self, tokenizer: Tokenizer):
        self.tokenizer = tokenizer
# ---
def __call__(self, module: M_contra, *args: P.args, **kwargs: P.kwargs) -> OutputT_co: ...
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        return subprocess.run(self._build_cmd(command), capture_output=True, text=True, timeout=timeout.to_seconds() + 5)
# ---
def _compileddirpats(self):
        pat, matchfunc = _buildregexmatch(
            [("glob", p, "") for p in self._globdirpats], "$"
        )
        return matchfunc
# ---
def tokenizer():
    """Create a real tokenizer for testing."""
    return AutoTokenizer.from_pretrained("gpt2")
# ---
def getMisMatches(data, weights):
    #print data
    list1 = np.empty(len(data))
    list1.fill(weights[0])
    results = list1+ weights[1]*data[:,0]+weights[2]*data[:,1]
    results = -1 * results
    return float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data)
# ---
def activation_genetic_modification(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'CRISPRa',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def nancumsum(a: NamedArray, axis: AxisSelector, *, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.nancumsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nancumsum.html)
    """
    return wrap_axiswise_call(jnp.nancumsum, a, axis, dtype=dtype, single_axis_only=True)
# ---
def root():
    """ Web interface landing page. """
    return render_template('index.html')
# ---
def host(self):
        '''the hostname, but I like host better'''
        return self.hostname
# ---
def end_stage(self) -> None:
        """Flush pending ops and close current stage."""
        self.flush_pending()
        if self.current_ops:
            self.stages.append(
                PhysicalStage(
                    operations=self.current_ops[:],
                    stage_type=self.stage_type,
                    output_shards=self.output_shards,
                )
            )
            self.current_ops = []
            self.output_shards = None
            self.stage_type = StageType.WORKER
# ---
def use_device_mesh(self) -> ContextManager[None]:
        """
        Context manager that sets the device mesh for jax, using Haliax's wrapper.

        In recent jax, this is the same as `jax.set_mesh(self.device_mesh)`, but we use Haliax's wrapper for
        compatibility with older jax versions.
        """
        return haliax.partitioning.set_mesh(self.device_mesh)
# ---
def size(self) -> int:
        return sum(1 for w in self._workers.values() if w.status in (WorkerStatus.IDLE, WorkerStatus.BUSY))
# ---
def ray_cluster():
    """Start Ray cluster for tests."""
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
    yield
# ---
def __invert__(self) -> NotExpr:
        return NotExpr(self)
# ---
def __repr__(self):
        return ', '.join('{}'.format(el) for el in self._queue)
# ---
def _test_do_execute(self, retval):
        with self._run_test(retval) as (conn, m1):
            result = conn.execute("insert into table foo", {"foo": "bar"})
        self._assert(
            retval,
            m1.do_execute, m1.real_do_execute,
            [call(
                    result.context.cursor,
                    "insert into table foo",
                    {"foo": "bar"}, result.context)]
        )
# ---
def _keyify(key):
    return _key_pattern.sub(' ', key.lower())
# ---
def _is_port_free(self, port: int) -> bool:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(("", port))
                return True
            except OSError:
                return False
# ---
def is_started(self):
        return self.last_run is not None
# ---
def test_endswith_autoescape(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e%fg", autoescape=True), {6})
# ---
def mini_transform(mean_axis):
                components = []
                components.append(scale_with_mini(self.beta1, self.beta2, self.epsilon, mean_axis=mean_axis))
                if self.weight_decay > 0:
                    components.append(optax.add_decayed_weights(self.weight_decay, self.build_weight_decay_mask()))
                components.append(optax.scale(-learning_rate))
                optimizer = optax.chain(*components)
                return optimizer
# ---
def test_impl(df):
            B = df.A.str.split(',')
            df2 = pd.DataFrame({'B': B})
            return df2[df2.B.str.len() > 1]
# ---
def close(self):
        if self._makefile_refs < 1:
            self._connection = None
            if self._sock:
                socket.socket.close(self._sock)
        else:
            self._makefile_refs -= 1
# ---
def convert_to_export(self, value, env):
        return value.name_get()[0][1] if value else ''
# ---
def parse_heading(self, m):
        self.tokens.append({
            'type': 'heading',
            'level': len(m.group(1)),
            'text': m.group(2),
        })
# ---
def load_cache(cache_dir: str, *, seq_len: int) -> TreeCache[dict]:
    exemplar = {"input_ids": np.zeros(seq_len, dtype=np.int32)}
    return TreeCache.load(cache_dir, exemplar)
# ---
def test_encrypt_message_with_newlines_at_end(self):
        self._test_encryption('This message has a newline at the end.\n')
# ---
def separator(self):
        ''' getter method for separator '''
        return self._separator
# ---
def _rms_norm(x: NamedArray, bias: NamedArray | None = None) -> NamedArray:
    var = hax.mean(hax.square(x), axis=Embed)
    inv = hax.rsqrt(var + 1e-5)
    out = x * inv
    if bias is not None:
        with pytest.raises(ShardingTypeError):
            # bias needs to be resharded explicitly
            out = out + bias

        # capture the compute axis mapping for bias
        out = out + hax.auto_sharded(bias)

    return out
# ---
def test_sample_edit_with_validation_invalid(tokenizer):
    source = "x = 1 + 2\n"
    replacement_tokens = tokenizer.encode_source("if :")

    mutation = sample_edit_with_validation(
        source=source,
        edit_position=4,
        original_span_end=9,
        replacement_tokens=replacement_tokens,
        tokenizer=tokenizer,
    )
    assert mutation is None
# ---
def choose_match(self, event):
        self.match_control.set_match(event.GetClientData())
# ---
def delete(self,solverId):
    path = "{base}/{solverId}".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.delete(path,
                          params=self.params,
                          headers=self.headers)
    self.validateReply(req)
    return True
# ---
def _get_ip_address() -> str:
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            return s.getsockname()[0]
    except Exception:
        return "127.0.0.1"
# ---
def test_from_files():
    with tempfile.TemporaryDirectory() as tmpdir:
        for i, program in enumerate(SAMPLE_PROGRAMS[:2]):
            (Path(tmpdir) / f"prog_{i}.py").write_text(program)

        # Also write a non-Python file that should be skipped.
        (Path(tmpdir) / "readme.txt").write_text("not python")

        paths = list(Path(tmpdir).glob("*"))
        bank = SubtreeBank.from_files(paths)
        assert bank.total_entries > 0
# ---
def read_batch(self, timeout: float | None = None) -> RolloutBatch | None:
        """Read a single batch with optional timeout."""
        return self._queue.pop(timeout)
# ---
def _get_policy_path(self):
        """Locate the policy json data file.

        :param policy_file: Custom policy file to locate.

        :returns: The policy path

        :raises: ConfigFilesNotFoundError if the file couldn't
                 be located.
        """
        policy_file = CONF.find_file(self.policy_file)

        if policy_file:
            return policy_file

        raise cfg.ConfigFilesNotFoundError(path=CONF.policy_file)
# ---
def check(self):
        pass
# ---
def __ne__(self, other):
        if not isinstance(other, MultiCoverage):
            return NotImplemented
        return not self.__eq__(other)
# ---
def _get_discovery_preamble(self) -> str:
        """Generate static controller discovery script for worker bootstrap."""
        addr = self._bootstrap_config.controller_address
        return f"""
# Use static controller address
CONTROLLER_ADDRESS="{addr}"
if [ -z "$CONTROLLER_ADDRESS" ]; then
    echo "[iris-init] ERROR: No controller address configured"
    exit 1
fi
echo "[iris-init] Using static controller at $CONTROLLER_ADDRESS"
"""
# ---
def ret_self():
            self.objects.update(other.objects)
            return [self]
# ---
def set_current_label_name(self):
        self.current_label.name = self.ui.comboBoxFuzzingLabel.currentText()
        self.ui.comboBoxFuzzingLabel.setItemText(self.ui.comboBoxFuzzingLabel.currentIndex(), self.current_label.name)
# ---
def test_change_permission(self):
        """
        Tests that only staff can change entries
        """
        self.assertTrue(self.creator_admin.has_change_permission(self.request))

        self.request.user = self.user
        self.assertFalse(self.creator_admin.has_change_permission(self.request))
# ---
def swapped_date(date, first, second):
    attrs = {DAY: date.day, MONTH: date.month, YEAR: last_two_digits(date.year)}
    newattrs = {first: attrs[second], second: attrs[first]}
    if YEAR in newattrs:
        newattrs[YEAR] += 2000
    return date.replace(**newattrs)
# ---
def test_mixed_int_and_selector():
    B, C, V = Axis("batch", 3), Axis("channel", 2), Axis("vocab", 6)
    x = hax.arange((B, C, V))
    idx = hax.arange((B,), dtype=jnp.int32) % V.size
    out = x["channel", 1, "vocab", idx]
    assert out.axes == (B,)
    ref = x.array[:, 1, :][jnp.arange(3), idx.array]
    assert jnp.array_equal(out.array, ref)
# ---
def test_technical_contacts(self):
        eq_(self.record.technical_contacts.__class__.__name__, 'list')
        eq_(self.record.technical_contacts, [])
# ---
def _slice_length(start: int, stop: int, step: int) -> int:
    if step == 0:
        raise ValueError("slice step cannot be zero")
    if step > 0:
        if start >= stop:
            return 0
        return (stop - start + step - 1) // step
    if start <= stop:
        return 0
    step_abs = -step
    return (start - stop - step - 1) // step_abs
# ---
def get_branches(git_path, module, dest):
    branches = []
    cmd = '%s branch --no-color -a' % (git_path,)
    (rc, out, err) = module.run_command(cmd, cwd=dest)
    if rc != 0:
        module.fail_json(msg="Could not determine branch data - received %s" % out, stdout=out, stderr=err)
    for line in out.split('\n'):
        if line.strip():
            branches.append(line.strip())
    return branches
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        client = self._handle._resolve()
        return getattr(client, self._method)(*args, **kwargs)
# ---
def test_mem_write_byte_updates_video_ram(self):
        self.mda.mem_write_byte(0x0000, 0x41)
        self.assertEqual(self.mda.video_ram[0x0000], 0x41)
# ---
def bitwise_left_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_left_shift](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_left_shift.html)
    """
    return jnp.bitwise_left_shift(x1, x2)
# ---
def data_saver(self):
        while self.run:
            self.save_data()

            time.sleep(60)
# ---
def in_qdq_fwd(compute_dtype, inp, scale, amax_history):
    qin, new_scale, new_history = qdq_and_return(inp, jnp.float8_e4m3fn, scale, amax_history, compute_dtype)
    return qin, (new_scale, new_history)
# ---
def read(fd):
            data = os.read(fd, 1024)
            output.append(data)
            return data
# ---
def sleep(self, seconds):
        """Blocks thread until chronos.tick() advances past wake time."""
        if seconds <= 0:
            return

        event = threading.Event()
        with self._lock:
            wake_time = self._current_time + seconds
            heapq.heappush(self._sleepers, SleepEvent(wake_time, event))

        # Block WITHOUT holding lock - will be woken by tick()
        event.wait()
# ---
def test_str_split_unbox_df(self):
        def test_impl(df):
            return df.A.iloc[0]

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        df2 = pd.DataFrame({'A': df.A.str.split(',')})
        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(df2), test_impl(df2))
# ---
def _init_small_lev_layer(hidden_size=128, nk=4, nv=8, dk=8, dv=8, ksz=4, key=None):
    if key is None:
        key = jax.random.PRNGKey(0)
    Embed = Axis("embed", hidden_size)
    cfg = GatedDeltaNetConfig(
        Embed=Embed, num_k_heads=nk, num_v_heads=nv, head_k_dim=dk, head_v_dim=dv, conv_kernel_size=ksz
    )
    layer = GatedDeltaNet.init(cfg, key=key)
    return cfg, layer
# ---
def __call__(self, x):
            y = tree_checkpoint_name(hax.sin(x + self.named), "sin")
            y = tree_checkpoint_name(hax.cos(y + x), "cos")
            return y + x
# ---
def init(*, key):
            k1, k2 = jax.random.split(key)
            first = hnn.Linear.init(In, Mid, key=k1, init_scale=0.02)
            second = hnn.Linear.init(Mid, In, key=k2, init_scale=0.02)
            return Module(first, second)
# ---
def load_or_initialize(cache_dir: str, source: ShardedDataSource, processor: BatchProcessor):
        metadata = CacheMetadata(preprocessor_metadata=processor.metadata)
        try:
            return CacheLedger.load(cache_dir, metadata)
        except FileNotFoundError:
            return CacheLedger(
                total_num_rows=0,
                shard_rows={shard: 0 for shard in source.shard_names},
                is_finished=False,
                metadata=metadata,
            )
# ---
def underscore(txt):
        """Return an under_scores text from a CamelCase text.

        This function will leave a CamelCase text unchanged.
        """
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', txt)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
# ---
def wait(ctx, job_id):
    """Wait for job completion."""
    cluster = ctx.obj["cluster"]
    click.echo(f"Waiting for {job_id}...")
    info = cluster.wait(job_id)
    click.echo(f"Job finished: {info.status}")
    if info.error_message:
        click.echo(f"Error: {info.error_message}", err=True)
# ---
def next(self):
        """Get the next row in the page."""
        self._parse_rows()
        if self._remaining > 0:
            self._remaining -= 1
        return next(self._iter_rows)
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["QwenConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfQwenConfig,
        )
# ---
def wrapped_scan(*args, **kwargs):
        scan_calls.append(kwargs.get("unroll"))
        return original_scan(*args, **kwargs)
# ---
def _add_default_env_variables(env: dict, default_env: dict | None):
    if default_env is not None:
        default_env = deepcopy(default_env)
        env = mergedeep.merge(default_env, env)

    # Ray gets mad if the values aren't all strings, but e.g. ints
    env = {str(k): str(v) for k, v in env.items()}
    return env
# ---
def callable_bytes(self) -> bytes | None:
        return self._callable_bytes
# ---
def always(root, cwd):
    return alwaysmatcher(root, cwd)
# ---
def abs(a: A) -> A:
    return wrap_elemwise_unary(jnp.abs, a)
# ---
def cumsum(a: NamedArray, axis: AxisSelector, *, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.cumsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html)
    """
    return wrap_axiswise_call(jnp.cumsum, a, axis, dtype=dtype, single_axis_only=True)
# ---
def convert_figure(self, el, text, convert_as_inline):
        if convert_as_inline:
            return text

        # the super doesn't handle this specifically. we basically want to be sure there's a newline
        if not text.endswith("\n\n"):
            if not text.endswith("\n"):
                text += "\n\n"
            else:
                text += "\n"
        return text
# ---
def create_nano_optimizer_config() -> AdamConfig:
    """Create a minimal AdamConfig for testing."""
    return AdamConfig(
        learning_rate=1e-3,
        weight_decay=0.00,
        warmup=0.0,
        lr_schedule="constant",
    )
# ---
def named(a, axis: AxisSelection) -> NamedArray:
    """Creates a NamedArray from a numpy array and a list of axes."""
    a = jnp.asarray(a)
    axes = check_shape(a.shape, axis)
    return NamedArray(a, axes)
# ---
def _pure_pattern(regex):
    pattern = regex.pattern
    if pattern.startswith('^'):
        pattern = pattern[1:]
    return pattern
# ---
def starting_job(method, notebook, data):
    job = notebook.current_job
    job.fetch_logs()
    if job.is_started:
        return views.render('started_job', notebook, {'job': job})
    return {'job': job, 'username': SAAGIE_USERNAME}
# ---
def empty(self):
        """Check if the queue is empty.

        Returns
        -------
        bool :
            Whether the queue is empty.

        """
        return len(self._queue) <= 0
# ---
def norm_config(self) -> LayerNormConfigBase:
        return RmsNormConfig(use_weight=self.use_layer_norm_weight, use_bias=self.use_bias, eps=self.layer_norm_epsilon)
# ---
def _stop_profiler_if_needed(self):
        """Ensure profiler is stopped if it was started."""
        if self._profiler_started:
            logger.info("Stopping profiler (end of evaluation).")
            jax.profiler.stop_trace()
            self._profiler_started = False
            self._log_profiler_artifact()
# ---
def total_attempts(self) -> int:
        """Total number of attempts."""
        return len(self.attempts)
# ---
def testGradientInput0WithTranspose(self):
    self._VerifyInput0(transpose_a=True, transpose_b=False)
    self._VerifyInput0(transpose_a=False, transpose_b=True)
    self._VerifyInput0(transpose_a=True, transpose_b=True)
# ---
def ba_axis(self) -> Axis:
        # [b | a]; per value head: Î² = Ïƒ(b), g uses a via Mamba2-style discretization
        return Axis("ba", self.num_v_heads * 2)
# ---
def logs_tail(self, handle: VllmServerHandle, *, max_lines: int = 200) -> str:
        return _native_logs_tail(handle.log_dir, max_lines=max_lines)
# ---
def _save_metadata(checkpoint_path, fs, step, is_temporary):
    metadata = {"step": step, "timestamp": datetime.datetime.now().isoformat(), "is_temporary": is_temporary}
    if jax.process_index() == 0:
        with fs.open(os.path.join(checkpoint_path, "metadata.json"), "w") as json_out:
            json.dump(metadata, json_out)
# ---
def peak_demand(self) -> int:
        """Peak demand seen."""
        return self._peak_demand
# ---
def wrapped_fold(*args, **kwargs):
        fold_calls.append(kwargs.get("unroll"))
        return original_fold(*args, **kwargs)
# ---
def __init__(self, urls, columns=None):
        super().__init__(urls)
        self.columns = columns
# ---
def dec(rc):
                    page = pages_row["page", i].scalar()
                    return rc.at["page", page].add(-1)
# ---
def register_endpoint(self, request: cluster__pb2.Controller.RegisterEndpointRequest, ctx: RequestContext) -> cluster__pb2.Controller.RegisterEndpointResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def test_client_with_resolver():
    server = ActorServer(host="127.0.0.1")
    server.register("echo", Echo())
    port = server.serve_background()

    try:
        resolver = FixedResolver({"echo": f"http://127.0.0.1:{port}"})
        client = ActorClient(resolver, "echo")

        assert client.echo("hello") == "echo: hello"
    finally:
        server.stop()
# ---
def create_completeness_minus_contamination_column(pd_tool_bins):
    pd_tool_bins['newcolumn'] = pd_tool_bins['recall_bp'] + pd_tool_bins['precision_bp'] - 1
# ---
def fftfreq(axis: Axis, d: float = 1.0) -> NamedArray:
    """Named version of :func:`jax.numpy.fft.fftfreq`."""

    return NamedArray(jfft.fftfreq(axis.size, d), (axis,))
# ---
def __exit__(self, *args):
        rc, out, err = execCmd([_SSH_ADD.cmd, '-d'], env=self._auth)
        if rc != 0:
            logging.error('Error deleting ssh-add, exit code: %r'
                          ', out: %r, err: %r' %
                          (rc, out, err))

        self._kill_agent()
# ---
def __radd__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.add(other, self)
# ---
def sources_del(self):
        node = bottle.request.body.getvalue().decode('utf-8')
        self.config['sources'].remove(node)
        self.ndb.disconnect_source(node)
# ---
def clear_words(self):
        """Remove all registered words; effectively turns off auto-completion."""
        self._word_freq = []
        self._word_list = defaultdict(lambda: 0)
# ---
def fake_finish_revert_migration(*args, **kwargs):
            self.fake_finish_revert_migration_called = True
# ---
def set_cookie(
        self,
        domain,
        name,
        value,
        path="/",
        exp=time.time() + timedelta(hours=744).total_seconds(),  #: 31 days retention
    ):
        self.cookies[
            name
        ] = f".{domain}\tTRUE\t{path}\tFALSE\t{exp}\t{name}\t{value}"
# ---
def _hf_auth_headers() -> dict[str, str]:
    for env_var in HF_TOKEN_ENV_VARS:
        token = os.environ.get(env_var)
        if token:
            return {"Authorization": f"Bearer {token}"}
    return {}
# ---
def col_clause(self):
        return self.field + " IS NULL", ()
# ---
def vocab_problem(self):
    raise NotImplementedError()
# ---
def cluster_status_html(cluster: str):
            if cluster not in self.clusters:
                return '<div class="error">Unknown cluster</div>'

            ports = self.port_mappings[cluster]
            return self._build_status_html(cluster, ports)
# ---
def make_step_fn():
        return lambda config: RLJob(config).run(config.run_id)
# ---
def slice_id(self) -> str:
        """Alias for group_id - the primary ID used in SliceInfo protos."""
        ...
# ---
def _update_ha_redundancy_level(self, context, logical_global_router,
                                    delta):
        with context.session.begin(subtransactions=True):
            log_g_router_db = self._l3_plugin._get_router(
                context, logical_global_router['id'])
            log_g_router_db.ha_settings.redundancy_level += delta
            context.session.add(log_g_router_db.ha_settings)
# ---
def test_static_method_skip(self, mock_start, mock_stop):
        self.assertEqual(25, FakeTraceStaticMethodSkip.static_method(25))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def _make_toy_inputs():
    key = jax.random.PRNGKey(0)
    key_x, key_w, key_y = jax.random.split(key, 3)

    x = jax.random.normal(key_x, (2, 3, 4), dtype=jnp.float32)
    w = jax.random.normal(key_w, (4, 5), dtype=jnp.float32)
    y = jax.random.randint(key_y, (2, 3), 0, 5, dtype=jnp.int32)
    return x, w, y
# ---
def test_is_stop_signal_invalid_tokens_in_stop_sequences():
    # stop_sequence contains only INVALID tokens (should not match)
    tail_tokens = hax.named(jnp.array([1, 2, 3], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[INVALID, INVALID, INVALID]], dtype=jnp.int32), axis=("seq", "position"))
    assert not is_stop_signal(tail_tokens, stop_sequences)
# ---
def test_build_bootstrap_script_no_key_error(self, minimal_bootstrap_config: config_pb2.BootstrapConfig):
        """Template formatting should not raise KeyError."""
        try:
            script = _build_bootstrap_script(minimal_bootstrap_config, vm_address="10.0.0.1")
            assert script
        except KeyError as e:
            pytest.fail(f"Template has unescaped braces: {{{e.args[0]}}}")
# ---
def __init__(self,
                 namespace,
                 kubeconfig='/etc/origin/master/admin.kubeconfig',
                 verbose=False,
                 all_namespaces=False):
        ''' Constructor for OpenshiftCLI '''
        self.namespace = namespace
        self.verbose = verbose
        self.kubeconfig = Utils.create_tmpfile_copy(kubeconfig)
        self.all_namespaces = all_namespaces
        self.oc_binary = locate_oc_binary()
# ---
def _is_task_timed_out(self, task: ControllerTask, job: ControllerJob) -> bool:
        """Check if a task has exceeded its scheduling timeout."""
        return job.scheduling_deadline is not None and job.scheduling_deadline.expired()
# ---
def __init__(self, prev, as_single_chars=(), as_pattern_chars=()):
        self.prev = prev  # ContentOfGroup or CharClass
        self.single_chars = as_single_chars
        self.pattern_chars = as_pattern_chars
# ---
def genetic_modification_9(lab, award, human_donor_1):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'donor': human_donor_1['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'method': 'transient transfection'
    }
# ---
def to_proto(self) -> "time_pb2.Duration":
        """Convert to proto Duration message."""
        return time_pb2.Duration(milliseconds=self._ms)
# ---
def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        for process in self._processes:
            if process.poll() is not None:
                continue
            _terminate_process(process)
# ---
def CanComplete():
  """Returns whether it's appropriate to provide any completion at the current
     line and column."""
  try:
    line, column = LineAndColumnAfterLastNonWhitespace()
  except TypeError:
    return False
  if ( line, column ) == CurrentLineAndColumn():
    return True
  return ( ToBytes( vim.current.buffer[ line ][ column - 1 ] )
           in potential_hint_triggers )
# ---
def get_stop_sequences(self) -> list[str] | list[int]:
        raise NotImplementedError
# ---
def group_id(self) -> str:
        return self._slice_id
# ---
def __init__(self, *args, bytes_strategy="base64", **kwargs):
        # bytes_strategy: "base64" | "repr" | "hex"
        super().__init__(*args, **kwargs)
        self.bytes_strategy = bytes_strategy
# ---
def __exit__(self, except_type, except_val, traceback):
        'Call finalize() and close the file.'
        try:
            self.finalize()
        finally:
            # Close again in case an exception happened in finalize()
            self.epub_f.close()
        return False
# ---
def triggerCharacters(self):
        """Trigger characters for smart indentation"""
        return self._smartIndenter.TRIGGER_CHARACTERS
# ---
def ClearYcmSyntaxMatches():
  matches = VimExpressionToPythonType( 'getmatches()' )
  for match in matches:
    if match[ 'group' ].startswith( 'Ycm' ):
      vim.eval( 'matchdelete({0})'.format( match[ 'id' ] ) )
# ---
def test_download_indices_for_url(mock_rcw, mock_rw, mock_jw):
    with s.app.test_client() as c:
        resp = c.get('/api/v1/classify_documents/log_only?directory=test')

        assert mock_rcw.called
        assert mock_rw.called
        assert mock_jw.called
# ---
def fix(a: A) -> A:
    return wrap_elemwise_unary(jnp.fix, a)
# ---
def main():

    app = wx.App()
    remote = Remote()
    MainWindow(remote, None)
    remote.start()
    remote.do_load(False)
    app.MainLoop()
# ---
def _resource_preset(*, use_tpu: bool) -> ResourceConfig:
    if use_tpu:
        return ResourceConfig.with_tpu("v5p-8")
    return ResourceConfig.with_gpu("A100-80G", count=1)
# ---
def resolve(self, name: str) -> ResolveResult: ...
# ---
def __init__(self):
        self.group = ast.Group()
        self.is_alt = False
# ---
def test_rescue(self):
        instance = self._create_instance()
        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass')
        vm = vm_utils.VMHelper.lookup(session, instance.name)
        vbd = xenapi_fake.create_vbd(vm, None)
        conn = xenapi_conn.get_connection(False)
        image_meta = {'id': glance_stubs.FakeGlance.IMAGE_VHD,
                      'disk_format': 'vhd'}
        conn.rescue(self.context, instance, [], image_meta)
# ---
def has_axis(self, axis: AxisSelection) -> bool:
        """Returns true if the given axis is present in this NamedArray."""
        return self.axis_indices(axis) is not None
# ---
def _insert_vm_to_db(self, uuid, name, state):
        vm = models.VM(uuid=uuid, name=name, state=state)
        vm.save()
# ---
def test_task_detail_page_loads(client):
    """Test /task/{task_id} page loads successfully."""
    response = client.get("/task/test-task-123")
    assert response.status_code == 200
    assert response.headers["content-type"] == "text/html; charset=utf-8"
# ---
def test_submit_rejects_name_with_slash(local_client):
    """Verify submit raises ValueError for names containing '/'."""
    entrypoint = Entrypoint.from_callable(dummy_entrypoint)
    resources = ResourceSpec(cpu=1, memory="1g")

    with pytest.raises(ValueError) as exc_info:
        local_client.submit(entrypoint, "invalid/name", resources)

    assert "/" in str(exc_info.value)
# ---
def test_normal():
    check_gen_is_equal(jax.random.normal, hax.random.normal)
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        object_ref = self._ray_method.remote(*args, **kwargs)
        return ray.get(object_ref)
# ---
def assertCalled(self, instance):
        disk_image_type = vm_utils.ImageType.DISK_VHD
        vm_ref = "blah"
        first_vdi_ref = "blah"
        vdis = ["blah"]

        self.called = False
        self.conn._vmops._attach_disks(instance, disk_image_type,
                                       vm_ref, first_vdi_ref, vdis)
        self.assertTrue(self.called)
# ---
def vocab_size(self) -> int:
        return self.core.vocab_size
# ---
def raw_array_or_scalar(x: NamedOrNumeric):
    if isinstance(x, NamedArray):
        return x.array
    return x
# ---
def adam_transform():
                components = []
                if self.max_grad_norm:
                    components.append(optax.clip_by_global_norm(self.max_grad_norm))
                components.append(optax.scale_by_adam(self.beta1, self.beta2, self.epsilon))
                components.append(optax.scale(-adam_lr))
                optimizer = optax.chain(*components)
                return optimizer
# ---
def type(self):
        return self._get_profile().user_type
# ---
def list_instances(project: str, zone: str, filter_expr: str | None = None) -> list[dict[str, Any]]:
    """List GCP compute instances."""
    cmd = [
        "gcloud",
        "compute",
        "instances",
        "list",
        f"--project={project}",
        f"--zones={zone}",
        "--format=json",
    ]

    if filter_expr:
        cmd.append(f"--filter={filter_expr}")

    result = run_gcloud_command(cmd)
    return json.loads(result.stdout)
# ---
def __neg__(self) -> "NamedArray":  # pragma: no cover
        return haliax.negative(self)
# ---
def model_type(cls) -> Type["MistralLMHeadModel"]:
        return MistralLMHeadModel
# ---
def ListWorkers(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def arccosh(a: A) -> A:
    return wrap_elemwise_unary(jnp.arccosh, a)
# ---
def raise_for_ec(kls, ec, func, *args):
        ec = 0 if ec > 0 else ec
        ex = kls.EXCEPTION_DICT.get(ec , kls.default_error_handler)
        if ex:
            raise ex(ec, *args)
# ---
def get_platforms(self):
        return [SaagiePlatform(self, platform_data)
                for platform_data in requests.get(PLATFORMS_URL, auth=SAAGIE_BASIC_AUTH_TOKEN).json()]
# ---
def union_axes(a1: AxisSpec, a2: AxisSpec) -> AxisSpec: ...
# ---
def extra(self, response):
        """Add extra info to response."""
        if 'facets' in self.hits:
            response['_facets'] = self.hits['facets']
        if 'aggregations' in self.hits:
            response['_aggregations'] = self.hits['aggregations']
# ---
def the_tokenizer(self) -> PreTrainedTokenizerBase:
        if self.tokenizer is None:
            return self.the_processor.tokenizer
        else:
            return load_tokenizer(self.tokenizer)
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        log_path = self.writer.logdir
        # sync the artifact to the logdir via fsspec
        try:
            fs, fs_path = fsspec.core.url_to_fs(log_path)
            fs.put(artifact_path, os.path.join(fs_path, name or os.path.basename(artifact_path)), recursive=True)
        except Exception:
            pylogger.exception(f"Error logging artifact {artifact_path} to {log_path}")
            return
# ---
def _psum(xs: Any) -> Any:
    return jax.tree.map(lambda x: jnp.sum(x, dtype=x.dtype, axis=0), xs)
# ---
def setColorSetter(self, colorSetter):
        self._colorSetter = colorSetter
        self._program.setColorSetter(colorSetter)
# ---
def create_actor(self) -> ActorHandle:
        raise NotImplementedError()
# ---
def unstacked(self) -> Sequence[M]:
        return self.blocks
# ---
def flatten_for_export(self) -> "XIELUActivation":
        """Expand scalar parameters to [1] shape for HF checkpoint compatibility."""
        Param = Axis("xielu_param", 1)
        alpha_p = hax.named(self.alpha_p.array.reshape(1), Param)
        alpha_n = hax.named(self.alpha_n.array.reshape(1), Param)
        return XIELUActivation(alpha_p, alpha_n, self.beta, self.eps)
# ---
def teardown(self) -> None:
        if self._awaitable:
            _cancel_tasks_and_wait([self._awaitable])
        self._awaitable = None
        self._host_info = None
# ---
def _load_tokenizer(tokenizer_name: str) -> transformers.PreTrainedTokenizer:
    """Load and cache a tokenizer by name"""
    return load_tokenizer_with_backoff(tokenizer_name)
# ---
def get_eval_examples(self, n_examples: int) -> list[dict[str, Any]]:
        """Get evaluation examples."""
        return self.eval_examples[:n_examples]
# ---
def from_state_dict(self: Mod, state_dict: StateDict, prefix: str | None = None) -> Mod:
        return default_eqx_module_from_state_dict(self, state_dict, prefix)
# ---
def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take(Width, 2)
# ---
def _rematcher(regex):
    """compile the regexp with the best available regexp engine and return a
    matcher function"""
    m = util.re.compile(regex)
    try:
        # slightly faster, provided by facebook's re2 bindings
        return m.test_match
    except AttributeError:
        return m.match
# ---
def __init__(self):
        self.parser = yacc.yacc(module=self, debug=0, write_tables=0)
# ---
def __repr__(self):
        return f"LoadFileOp(format={self.format}, columns={self.columns})"
# ---
def normalize_final_weighted_score(self,minimum,maximum):
        value = self.final_weighted
        value -= minimum
        if maximum - minimum > 0:
            value /= ( maximum - minimum )
        else:
            value = 0
        self.weight_rank = "{0:.2f}".format(value * 10)
# ---
def on_connect(self):
        sizes = []
        workshops = getAvailableWorkshops()
        for w in workshops:
            tmp = [w.workshopName, w.q.qsize()]
            sizes.append(tmp)
            self.emit('sizes', tmp)
# ---
def faq():
    """FAQ page for SciNet"""
    return render_template("faq.html")
# ---
def expiration_datetime(self):
        """Datetime that the verification will expire. """
        days_good_for = settings.VERIFY_STUDENT["DAYS_GOOD_FOR"]
        return self.created_at + timedelta(days=days_good_for)
# ---
def test_delete_vm_resource_successful(self):
        RESOURCE_ID = pyuuid.UUID("00000000-0000-0000-0000-000000000001")
        self._insert_vm_to_db(uuid=RESOURCE_ID, name="test", state="off")

        VM_RES_ENDPOINT = self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT,
                                            RESOURCE_ID)

        response = requests.delete(VM_RES_ENDPOINT)

        self.assertEqual(response.status_code, 204)
        self.assertFalse(self._vm_exists_in_db(RESOURCE_ID))
# ---
def text(self, text):
        """Rendering unformatted text.

        :param text: text content.
        """
        if self.options.get('parse_block_html'):
            return text
        return escape(text)
# ---
def test_delete_missing(self):
        """Deleting a missing table returns False"""
        ret = self.dynamo.delete_table("foobar")
        self.assertTrue(not ret)
# ---
def _GrumpRun(cmd):
  p = subprocess.Popen(['grumpy', 'run'], stdin=subprocess.PIPE,
                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
  out, _ = p.communicate(cmd)
  return p.returncode, out
# ---
def with_prefix(prefix: str | None, leaf: str | None) -> str | None: ...
# ---
def html_to_md(cfg: SimpleHtmlToMdConfig):
    """Transform HTML content to markdown using the specified extraction method."""
    pipeline = (
        Dataset.from_files(f"{cfg.input_path}/**/*.jsonl.gz")
        .flat_map(load_jsonl)
        .map(lambda data: _html_to_md(data, cfg.extract_method, cfg.config))
        .write_jsonl(f"{cfg.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )
    list(Backend.execute(pipeline))
# ---
def volume_omxplayer(vol) :
    import math
    control = "/usr/local/bin/omxcontrol"
    if vol == 'up' :
        db = subprocess.check_output([control, "volumeup"])
    else :
        db = subprocess.check_output([control, "volumedown"])

    v = subprocess.check_output([control, "volume"])
    i = v.rfind(':')
    db = 10.0 * math.log(float(v[i+1:]), 10)
    volstring = "%-2.2f dB" % db
    return volstring
# ---
def metadata(self) -> dict[str, Any]:
        return {
            "tokenizer": self.tokenizer.name_or_path,
            "vocab_size": len(self.tokenizer),
            "chat_template": self.chat_template,
            "messages_field": self.messages_field,
            "system_prompt_field": self.system_prompt_field,
            "chat_template_kwargs_field": self.chat_template_kwargs_field,
        }
# ---
def argsort(self, axis: AxisSelector | None, *, stable: bool = False) -> "NamedArray":  # pragma: no cover
        return haliax.argsort(self, axis=axis, stable=stable)
# ---
def protect(self, tag: str) -> None:
        """Mark an image as protected from eviction (used by a running job)."""
        ...
# ---
def read_parquet_file(filepath: str) -> list[dict]:
    """Helper function to read a Parquet file"""
    import pandas as pd

    df = pd.read_parquet(filepath)
    return df.to_dict("records")
# ---
def sample_data(self):
        super(AggregateChecks, self).sample_data()

        self.serial1 = "__TEST__1"
        self.serial2 = "__TEST__2"

        self.dawn = 8*3600
        self.dusk = 20*3600

        sampledata = check.generate_linear(0, self.dawn, self.dusk, 24*3600,
                                           0, 1)

        for ts, y in sampledata:
            self.db.add_historic(self.serial1, ts, y)
            self.db.add_historic(self.serial2, ts, 2*y)
# ---
def parse(expr, **kw):
    sm = PSM()
    sm.source = Source(expr)
    sm.starts_with(OpeningOfGroup(parent=None, initial=True))
    sm.pre_action = kw.get("pre_action", None)
    sm.post_action = kw.get("post_action", None)
    sm.parse()
    return sm.state.g.group
# ---
def __init__(self, addr, dispatcher, send_delay):
		super(Radio, self).__init__()

		self.addr = addr
		self.neighbor = None
		self.dispatcher = dispatcher
		self.q = Queue.Queue()

		self.frequency = 0
		self.bandwidth = 0

		self.send_delay = send_delay
# ---
def get_evaluator(config: EvaluationConfig) -> Evaluator:
    if config.evaluator not in EVALUATORS:
        raise ValueError(f"Unknown evaluator: {config.evaluator}. Available: {list(EVALUATORS.keys())}")
    return EVALUATORS[config.evaluator]()
# ---
def test_to_obj_list():
    msg = '[{"hoge": 1}, {"hogi": 2}]'
    bb = jps.utils.to_obj(msg)
    assert len(bb) == 2
    assert bb[0].hoge == 1
    assert bb[1].hogi == 2
# ---
def validate_kwargs(kwargs, allowed_kwargs,
                    error_message='Keyword argument not understood:'):
  """Checks that all keyword arguments are in the set of allowed keys."""
  for kwarg in kwargs:
    if kwarg not in allowed_kwargs:
      raise TypeError(error_message, kwarg)
# ---
def coverage(geom, srs):
    if isinstance(geom, (list, tuple)):
        return BBOXCoverage(geom, srs)
    else:
        return GeomCoverage(geom, srs)
# ---
def get_runtime_env(self) -> dict:
        """
        Returns the runtime environment to run the evaluator on the Ray cluster.
        """
        return build_runtime_env_for_packages(
            extra=["eval"],
            env_vars={"HF_ALLOW_CODE_EVAL": "1"},
            # Human eval tests code from the model which requires permission to run.
        )
# ---
def create_vm_group(self, tags: dict[str, str] | None = None) -> VmGroupProtocol:
        """Create a new VM group. Returns a ready-to-use VmGroup object.

        Args:
            tags: Optional labels/tags for the VM group and its VMs

        Returns:
            A VmGroup object with lifecycle management
        """
        ...
# ---
def generate_pip_freeze():
    from importlib.metadata import distributions

    dists = distributions()
    return "\n".join(f"{dist.name}=={dist.version}" for dist in dists)
# ---
def determine_draft_value(self, record):
        """ Determine the value of ``self`` for the given draft ``record``. """
        if self.compute:
            self._compute_value(record)
        else:
            record._cache[self] = SpecialValue(self.null(record.env))
# ---
def logit_function(tokens: TokenArray) -> LogitArray:
        Batch, Position = tokens.axes
        token_array = np.array(tokens.array, dtype=np.int64)
        input_ids = torch.from_numpy(token_array).to(config.device)

        with torch.inference_mode():
            outputs = model(input_ids)
            logits = outputs.logits.float().cpu().numpy()
            return hax.named(logits, (Batch, Position, "vocab"))
# ---
def emptyLayout(layout):
    for i in reversed(range(layout.count())):
        layout.itemAt(i).widget().setParent(None)
# ---
def concat_axes(a1: AxisSpec, a2: AxisSpec) -> AxisSpec:
    pass
# ---
def shard_with_axis_mapping(x: T, mapping: ResourceMapping, mesh: Mesh | None = None) -> T:
    # warnings.warn("`shard_with_axis_mapping` is deprecated. Use `shard` instead", DeprecationWarning)
    return shard(x, mapping, mesh)
# ---
def __str__(self):
        'Returns the text.'
        return self.text
# ---
def maybe_cast_to_bf16(arr):
            if jnp.issubdtype(arr.dtype, jnp.floating):
                return arr.astype(jnp.bfloat16)
            return arr
# ---
def totext(obj):
        if isinstance(obj, bytes):
            obj = str(obj, 'UTF-8')
        assert isinstance(obj, str)
        return obj
# ---
def map(self, fn: MapFunction[U], *extra_args, **extra_kwargs) -> "MappedAsyncDataset[T_co, U]":
        return MappedAsyncDataset(self, fn, *extra_args, **extra_kwargs)
# ---
def generate_subsequences(n):
    subsequences = []
    combinations_list = []
    index = 4
#Generate all combinations
    while index > 0:
        combinations_list.append(list(combinations(str(n), index)))
        index -= 1
#Formatting combinations
    for index in combinations_list:
        for combination in index:
            subsequences.append(''.join(combination))
    return subsequences
# ---
def silence_transformer_nag():
    # this is a hack to silence the transformers' "None of PyTorch, TensorFlow 2.0 or Flax have been found..." thing
    # which is annoying and not useful
    # Often we won't call this early enough, but it helps with multiprocessing stuff
    if os.getenv("TRANSFORMERS_VERBOSITY") is None:
        os.environ["TRANSFORMERS_VERBOSITY"] = "error"

    import transformers
# ---
def test_format_shard_path_multiple_consecutive_slashes():
    """Test normalization of multiple consecutive slashes."""
    pattern = "gs://bucket///path////data-{shard:05d}.jsonl"
    result = format_shard_path(pattern, 0, 1)
    assert result == "gs://bucket/path/data-00000.jsonl"
# ---
def __str__(self):
        return self.as_written
# ---
def sync_backend(request):
    with fray_default_job_ctx(create_job_ctx("sync")):
        yield
# ---


def remove_vowels(text):
    """
    remove_vowels is a function that takes string and returns string without vowels.
    >>> remove_vowels('')
    ''
    >>> remove_vowels("abcdef\nghijklm")
    'bcdf\nghjklm'
    >>> remove_vowels('abcdef')
    'bcdf'
    >>> remove_vowels('aaaaa')
    ''
    >>> remove_vowels('aaBAA')
    'B'
    >>> remove_vowels('zbcd')
    'zbcd'
    """
    return "".join([s for s in text if s.lower() not in ["a", "e", "i", "o", "u"]])
# ---
def update_strs(slist):
      update_num(len(slist))
      for s in slist:
        update_str(s)
# ---
def calculate_rank_name(self, rank_points):
        index = 0

        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:
            if(self.rank_to_pos.index(k) > index):
                index = self.rank_to_pos.index(k)

        return self.rank_to_pos[index]
# ---
def translate_inverse(unity):
    for key, value in UNITIES.items():
        if unity == value:
            return key
    else:
        return u"NONE"
# ---
def map_row(row: dict):
    """Transform evaluation record to Dolma format.

    Args:
        row: Record with "prompt" and "response" fields

    Returns:
        Record with "text" field containing prompt + response
    """
    row["text"] = row["prompt"] + "\n" + row["response"]
    return row
# ---
def _add_text_box(self, textbox):
        """Add the given L{TextBox} to the list of widgets to do auto-
            correction on."""
        if not hasattr(self, '_textbox_insert_ids'):
            self._textbox_insert_ids = {}
        handler_id = textbox.connect('text-inserted', self._on_insert_text)
        self._textbox_insert_ids[textbox] = handler_id
        self.widgets.add(textbox)
# ---
def as_axis(ax_name: str) -> Ax:
        if spec[ax_name] is None:
            return ax_name  # type: ignore
        else:
            return Axis(ax_name, spec[ax_name])
# ---
def __init__(
        self,
        cpu: int = 1000,
        memory_gb: int = 1000,
        attributes: dict[str, str | int | float] | None = None,
        device: cluster_pb2.DeviceConfig | None = None,
    ):
        self._cpu = cpu
        self._memory_gb = memory_gb
        self._attributes = attributes or {}
        self._device = device
# ---
def stop(self):
        """Stop the inference worker loop and server."""
        with self._shutdown_condition:
            self._running = False
            self._transfer_client.cleanup()
            self._shutdown_condition.notify()

        # Wait for the main loop to finish
        self._shutdown_complete.wait()

        # Now shutdown the inference server
        self._policy_ctx.shutdown()

        if self.weight_transfer_thread:
            self.weight_transfer_thread.join()
# ---
def test_reduce_with_pipeline(backend):
    """Test reduce integrated with other operations."""
    ds = Dataset.from_list(range(1, 21)).filter(lambda x: x % 2 == 0).map(lambda x: x * 2).reduce(sum)

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    expected = sum(x * 2 for x in range(1, 21) if x % 2 == 0)
    assert results[0] == expected
# ---
def assert_eq(x, y):
                assert x == y
# ---
def DATA_STATES(state):
    if state == 'open':
        return YELLOW() + state + ENDC()
    elif state == 'closing':
        return YELLOW() + state + ENDC()
    elif state == 'closed':
        return GREEN() + state + ENDC()
    else:
        return state
# ---
def test_startswith_sqlexpr(self):
        col = self.tables.some_table.c.data
        self._test(
            col.startswith(literal_column("'ab%c'")),
            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10},
        )
# ---
def get_iqawg(self):
        self._iqawg.set_parameters(
            {'calibration': self._current_cal})  # ensure
        return self._iqawg
# ---
def format_doc(hit, schema, dates):
    """Format given doc to match given schema."""
    doc = hit.get('_source', {})
    doc.setdefault(config.ID_FIELD, hit.get('_id'))
    doc.setdefault('_type', hit.get('_type'))

    for key in dates:
        if key in doc:
            doc[key] = parse_date(doc[key])

    return doc
# ---
def relationships(request):
    accounts = ripple.get_user_accounts(request.profile)
    return locals()
# ---
def test_scan_str_arg():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take("Width", 2)

    total, selected = hax.scan(scan_fun, "Height")(0.0, named1)

    assert jnp.all(jnp.isclose(total, jnp.sum(named1.array, axis=(0, 1, 2))))
    assert jnp.all(jnp.equal(selected.array, named1.take(Width, 2).array))
# ---
def remove_directory_contents(path):
    for f in os.listdir(path):
        os.remove(os.path.join(path, f))
# ---
def flush_stdout():
        sys.stdout.flush()
        sys.stderr.flush()
        time.sleep(5)
        while not event.is_set():
            print("Waiting...", flush=True)
            print("\n", file=sys.stderr, flush=True)
            time.sleep(5)
# ---
def test_startswith_autoescape(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab%c", autoescape=True), {3})
# ---
def _with_mesh(mesh):
        set_mesh = getattr(jax, "set_mesh", None)
        if set_mesh is None:
            return mesh
        set_mesh(mesh)
        return None
# ---
def peek():
        return i < n and pat[i : i + 1]
# ---
def LogPABotMessage(message):
    _pabotlog.info(message)
# ---
def test_plain_desc(self):
        table = self.tables.some_table
        lx = table.c.x.label("lx")
        self._assert_result(
            select([lx]).order_by(lx.desc()), [(3,), (2,), (1,)]
        )
# ---
def __str__(self) -> str:
        return str(self._job_id)
# ---
def parse_format(data):
    """Returns root input type from data."""
    input_types = {}
    data = data['ist_nodes']
    root_id = data[0]['id']      # set root type

    for item in data:
        input_type = _get_input_type(item)
        if input_type is not None:
            input_types[input_type['id']] = input_type  # register by id

    _substitute_ids_with_references(input_types)
    return input_types[root_id]
# ---
def sort(self, objs):
        # TODO: Conversion and null-detection here. In Python 3,
        # comparisons with None fail. We should also support flexible
        # attributes with different types without falling over.

        def key(item):
            field_val = item.get(self.field, '')
            if self.case_insensitive and isinstance(field_val, str):
                field_val = field_val.lower()
            return field_val

        return sorted(objs, key=key, reverse=not self.ascending)
# ---
def get_num_train_steps(param_count, batch_size, seq_len):
    """Compute the number of steps for Chinchilla optimal training (20x params tokens)."""
    total_tokens = param_count * 20
    tokens_per_step = batch_size * seq_len
    return total_tokens // tokens_per_step
# ---
def author(self):
        self._authors = []
# ---
def retrfile(self, filename, filetype):
                self.filename, self.filetype = filename, filetype
                return io.StringIO(self.data), len(self.data)
# ---
def create_child(self, name: str) -> "ThreadContainer":
        """Create a child container whose lifecycle is bound to this parent."""
        child = ThreadContainer(name=name)
        with self._lock:
            self._children.append(child)
        return child
# ---
def test_nested_list(self):
        """Store and retrieve a nested list"""
        self.make_table()
        data = [
            1,
            [
                True,
                None,
                "abc",
            ],
        ]
        self.dynamo.put_item("foobar", {"id": "abc", "l": data})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["l"], data)
# ---
def _read_ovf_from_ova(ova_path):
    """
       virt-v2v support ova in tar, zip formats as well as
       extracted directory
    """
    if os.path.isdir(ova_path):
        return _read_ovf_from_ova_dir(ova_path)
    elif zipfile.is_zipfile(ova_path):
        return _read_ovf_from_zip_ova(ova_path)
    elif tarfile.is_tarfile(ova_path):
        return _read_ovf_from_tar_ova(ova_path)
    raise ClientError('Unknown ova format, supported formats:'
                      ' tar, zip or a directory')
# ---
def cancel_assign(self, cr, uid, ids, *args):
        """ Cancels picking and moves.
        @return: True
        """
        wf_service = netsvc.LocalService("workflow")
        for pick in self.browse(cr, uid, ids):
            move_ids = [x.id for x in pick.move_lines]
            self.pool.get('stock.move').cancel_assign(cr, uid, move_ids)
            wf_service.trg_write(uid, 'stock.picking', pick.id, cr)
        return True
# ---
def test_buffered_row_result_proxy(self):
        self._test_proxy(_result.BufferedRowResultProxy)
# ---
def cluster_backup_jobs(ctx, backup_dir):
    """Backup Ray jobs to specified directory."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        Path(backup_dir).mkdir(parents=True, exist_ok=True)
        _backup_jobs(ctx.obj.config_file, backup_dir)
        print(f"Jobs backed up successfully to {backup_dir}")
# ---
def mod_ndwi(domain, b, threshold=None):
    if threshold == None:
        threshold = float(domain.algorithm_params['mod_ndwi_threshold'])
    return get_mod_ndwi(b).lte(threshold)
# ---
def terminate(self) -> None:
        self._terminated.set()
        self._future.cancel()
# ---
def _reshape(x):
        if is_jax_array_like(x) and x.shape != ():
            return x.reshape([outer_size, inner_size, *x.shape[1:]])
        else:
            return x
# ---
def format_accelerator_display(accel_type: int, variant: str = "") -> str:
    """Format accelerator type and variant for display.

    Examples:
        format_accelerator_display(3, "v5litepod-16") -> "tpu (v5litepod-16)"
        format_accelerator_display(2, "A100") -> "gpu (A100)"
        format_accelerator_display(1, "") -> "cpu"
    """
    friendly = accelerator_type_friendly(accel_type)
    if variant:
        return f"{friendly} ({variant})"
    return friendly
# ---
def remove_references(html: BeautifulSoup):
    # Remove the reference section
    references = html.findAll("section", {"id": "ltx_bibliography"})
    for ref in references:
        ref.decompose()

    # Remove the references section
    references = html.findAll("ul", {"class": "ltx_biblist"})
    for ref in references:
        ref.decompose()
# ---
def __init__(self, target_time, *args, **kwargs):
        super(TimeSensor, self).__init__(*args, **kwargs)
        self.target_time = target_time
# ---
def available_cpu(self) -> int:
        """Available CPU cores after subtracting committed resources."""
        return self.metadata.cpu_count - self.committed_cpu
# ---
def init(cls, Vocab: Axis, config: Qwen3Config, *, key):  # type: ignore[override]
        k_t, k_emb = jrandom.split(key, 2)
        transformer = LlamaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return Qwen3LMHeadModel(transformer, embeddings, lm_head)
# ---
def test_ndim_distance():
    """Test to see if changing val by 1 does what it ought to do
    convert to float to integer because floating arithmetic makes testing
    analytic functions a mess"""
    rand = random.random
    point1 = [rand(), rand(), rand(), rand(), rand(), rand()]
    point2 = [point1[0]+1] + point1[1:] # just shift x to the right by 1
    assert int(round(kmeans.ndim_euclidean_distance(point1, point2))) == 1
# ---
def test_basic_check(self):
        result = spell_checker.check(u'ì•ˆë…• í•˜ì„¸ìš”. ì €ëŠ” í•œêµ­ì¸ ìž…ë‹ˆë‹¤. ì´ë¬¸ìž¥ì€ í•œê¸€ë¡œ ìž‘ì„±ë¬ìŠµë‹ˆë‹¤.')

        assert result.errors == 4
        assert result.checked == u'ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” í•œêµ­ì¸ìž…ë‹ˆë‹¤. ì´ ë¬¸ìž¥ì€ í•œê¸€ë¡œ ìž‘ì„±ëìŠµë‹ˆë‹¤.'
# ---
def do_save(self, match):
        self.lc.publish('Match/Save', match.encode())
# ---
def hard_silu(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_silu, a)
# ---
def test_deduplication(bank):
    """Identical subtrees from different programs should be deduplicated."""
    for node_type, entries in bank.entries.items():
        sources = [e.source for e in entries]
        assert len(sources) == len(set(sources)), (
            f"Duplicates found in {node_type}: " f"{len(sources)} entries but {len(set(sources))} unique"
        )
# ---
def get_replicas(self):
        ''' return replicas setting '''
        return self.get(DeploymentConfig.replicas_path)
# ---
def _setup_graph(self):
        NR_PROC = min(multiprocessing.cpu_count() // 2, 20)
        self.pred_funcs = [self.trainer.get_predictor(
            self.input_names, self.output_names)] * NR_PROC
# ---
def get_slopes_power_of_2(n: int):
        start = 2 ** (-(2 ** -(math.log2(n) - log_bias_max)))
        ratio = start
        return [start * ratio**i for i in range(n)]
# ---
def matches(self, b: int, h: int, v: int) -> bool:
        return self.b_min <= b <= self.b_max and self.h_min <= h <= self.h_max and self.v_min <= v <= self.v_max
# ---
def read_all_remote(self):
        '''
        Return a dict of all remote key data
        '''
        data = {}
        for status, mids in six.iteritems(self.list_keys()):
            for mid in mids:
                keydata = self.read_remote(mid, status)
                if keydata:
                    keydata['acceptance'] = status
                    data[mid] = keydata

        return data
# ---
def _get_ref(self, ref) -> Tuple[str, Optional[str]]:
        if ref is None:
            if self.reference_checkpoint is None:
                raise ValueError("Must provide a reference checkpoint to load HFConfig from")
            ref = self.reference_checkpoint
        ref = _coerce_to_rr(ref)
        return ref.model_name_or_path, ref.revision
# ---
def test_describe_missing(self):
        """Describing a missing table returns None"""
        ret = self.dynamo.describe_table("foobar")
        self.assertIsNone(ret)
# ---
def llama_small_optimizer_config() -> AdamConfig:
    return AdamConfig(
        learning_rate=1e-6,
        # don't overwhelm the learning signal
        weight_decay=1e-5,
        warmup=10,
        lr_schedule="cosine",
    )
# ---
def set_output_state(self, state):
        self._lo.set_output_state(state)
# ---
def __init__(self, fn):
        self.fn = fn
# ---
def update_image_metadata(self, image_id, meta):
        """Updates the metadata for an image."""
        post_body = json.dumps({'metadata': meta})
        resp, body = self.post('images/%s/metadata' % str(image_id), post_body)
        body = json.loads(body)
        self.validate_response(schema.image_metadata, resp, body)
        return service_client.ResponseBody(resp, body['metadata'])
# ---
def bin_path(self) -> str:
        """Path to the bin directory (e.g., venv/bin)."""
        return os.path.join(self.venv_path, "bin")
# ---
def newline(self):
        """Rendering newline element."""
        return ''
# ---
def shutdown(self, wait: bool = True) -> None:
        logger.info("RayClient shutdown (namespace=%s)", self._namespace)
# ---
def resolve_vllm_mode(mode: Literal["native", "docker"] | None) -> Literal["native", "docker"]:
    mode_str = (mode if mode is not None else os.environ.get("MARIN_VLLM_MODE", "docker")).lower()
    if mode_str not in ("native", "docker"):
        raise ValueError(f"Unknown MARIN_VLLM_MODE={mode_str!r}; expected 'native' or 'docker'.")
    return mode_str
# ---
def iter_profile_artifacts(run: "wandb.apis.public.Run") -> Iterable["wandb.sdk.Artifact"]:
    for artifact in run.logged_artifacts():
        if artifact.type == ARTIFACT_TYPE:
            yield artifact
# ---
def test_validate_edit_invalid():
    source = "x = 1 + 2\n"
    mutation = Mutation(start=4, end=9, replacement="if :", node_type="BinOp", original="1 + 2")
    assert not validate_edit(source, mutation)
# ---
def model_type(self) -> type["GemmaLMHeadModel"]:
        return GemmaLMHeadModel
# ---
def max_output_tokens(self) -> int:
        """Maximum output tokens across all lessons in the curriculum."""
        return max(lesson.sampling_params.max_output_tokens for lesson in self.lessons.values())
# ---
def transfer_mode(request):
    """Parametrized weight transfer mode."""
    return request.param
# ---
def __del__(self):
        # delete the work directories in the background, use the ls to make sure we don't
        # accidentally run this on our local machine
        subprocess.Popen(
            [
                "bash",
                "-c",
                "ls /dev/accel* && (rm -rf $HOME/marin/; rm -rf $HOME/.cache/; sudo rm -f /tmp/libtpu_lockfile)",
            ],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        _hacky_remove_tpu_lockfile()
# ---
def is_alive(self) -> bool:
        return self._thread.is_alive()
# ---
def _try_load(path, metadata):
    try:
        ledger = CacheLedger.load(path, metadata)
        if ledger.is_finished:
            return ledger
        logger.debug(f"Cache exists but is not finished at {path}.")
        return None
    except FileNotFoundError:
        return None
# ---
def err1(context):
            stmt = context.statement

            if "ERROR ONE" in str(stmt) or "ERROR TWO" in str(stmt) \
                    or "ERROR THREE" in str(stmt):
                return MyException1("my exception")
            elif "ERROR FOUR" in str(stmt):
                raise MyException3("my exception short circuit")
# ---
def set_failure_mode(self, mode: FailureMode) -> None:
        """Set the failure mode for subsequent operations."""
        self._config.failure_mode = mode
# ---
def print_all(self):
        self._call_all('print_all')
# ---
def test_random_mutation_produces_valid_python(bank):
    source = CORPUS[0]  # fibonacci
    rng = random.Random(42)

    mutation = random_mutation(source, bank, rng=rng)
    assert mutation is not None

    mutated = mutation.apply(source)
    # The result must be valid Python.
    try:
        ast.parse(mutated)
    except SyntaxError:
        pytest.fail(f"Mutation produced invalid Python:\n{mutated}")
# ---
def parse_newline(self, m):
        length = len(m.group(0))
        if length > 1:
            self.tokens.append({'type': 'newline'})
# ---
def get_slice(self, group_id: str) -> VmGroupProtocol | None:
        """Get a specific VM group by ID."""
        with self._vm_groups_lock:
            return self._vm_groups.get(group_id)
# ---
def _tool_call_payload(tool_call: ToolCall) -> dict[str, object]:
    """Minimal JSON payload for embedding in <tool_call> blocks."""
    # Convert from nested structure to flat format for compatibility
    return {
        "name": tool_call.function.name,
        "args": json.loads(tool_call.function.arguments),
    }
# ---
def get_random_samples(self, num_samples):
        num_samples = min(num_samples, len(self.examplers))
        return random.sample(tuple(self.examplers), num_samples)
# ---
def dblock(idx: int, size: int) -> dslice:
    """
    Returns a dslice that selects a single block of size `size` starting at `idx`
    """
    return dslice(idx * size, size)
# ---
def setUp(self):
        super(XenAPIHostTestCase, self).setUp()
        self.flags(xenapi_connection_url='test_url',
                   xenapi_connection_password='test_pass')
        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)
        xenapi_fake.reset()
        xenapi_fake.create_local_srs()
        self.conn = xenapi_conn.get_connection(False)
# ---

def count_upper(s):
    """
    Given a string s, count the number of uppercase vowels in even indices.

    For example:
    count_upper('aBCdEf') returns 1
    count_upper('abcdefg') returns 0
    count_upper('dBBE') returns 0
    """
    count = 0
    for i in range(0,len(s),2):
        if s[i] in "AEIOU":
            count += 1
    return count
# ---
def test_and_one_false(self):
        expr = (col("a") > 0) & (col("b") > 0)
        assert expr.evaluate({"a": 1, "b": -1}) is False
        assert expr.evaluate({"a": -1, "b": 1}) is False
# ---
def init(ui):
    global _usetreematcher
    _usetreematcher = ui.configbool("experimental", "treematcher")
# ---
def _load_audio_file(file_name, sampling_rate):
            with fsspec.open(audio_pointer, "rb", compression="infer") as f:
                array, sr = librosa.load(f, sr=sampling_rate)
            return {"array": array, "sampling_rate": sr}
# ---
def test_repr(self):
        expr = ~(col("flag") == True)  # noqa: E712
        assert repr(expr) == "~((col('flag') == lit(True)))"
# ---
def test_annotate_image(self, annotator_client_mock):
        # Given
        annotate_image_method = annotator_client_mock.annotate_image

        # When
        self.hook.annotate_image(request=ANNOTATE_IMAGE_REQUEST)
        # Then
        # Product ID was provided explicitly in the method call above, should be returned from the method
        annotate_image_method.assert_called_once_with(
            request=ANNOTATE_IMAGE_REQUEST, retry=None, timeout=None
        )
# ---
def transform_linear(x):
        if not isinstance(x, hax.nn.Linear):
            return x

        # do something that distinguishes doing weights jointly from independently
        new_weight = x.weight - hax.mean(x.weight)
        return dataclasses.replace(x, weight=new_weight)
# ---
def test_wait_for_connection_respects_stop_event(mock_conn_avail, _mock_sleep):
    """wait_for_connection returns False when stop_event is set."""
    mock_conn_avail.return_value = False
    conn = MagicMock()
    stop_event = threading.Event()
    stop_event.set()
    assert (
        wait_for_connection(
            conn, timeout=Duration.from_seconds(60), poll_interval=Duration.from_seconds(5), stop_event=stop_event
        )
        is False
    )
# ---
def init_fn(params):
        del params
        buffer_size = rolling_interval_length
        update_norms = jnp.zeros((buffer_size,), dtype=jnp.float32)
        valid_mask = jnp.full((buffer_size,), False, dtype=jnp.bool_)
        current_idx = jnp.zeros((), dtype=jnp.int32)
        count = jnp.zeros((), dtype=jnp.int32)

        return ClipUpdateNormState(
            update_norms=update_norms,
            current_idx=current_idx,
            count=count,
            valid_mask=valid_mask,
        )
# ---
def close(self):
        '''
        Close the file.
        '''
        self.file.close()
# ---
def test_pack_empty_tree():
    tree = {}
    offsets, packed = pack_pytree(tree, dtype=jnp.float32)
    assert packed.size == 0
    rebuilt = unpack_pytree(offsets, packed)
    assert rebuilt == tree
# ---
def _read_ovf_from_tar_ova(ova_path):
    with tarfile.open(ova_path) as tar:
        for member in tar:
            if member.name.endswith('.ovf'):
                with closing(tar.extractfile(member)) as ovf:
                    return ovf.read()
        raise ClientError('OVA does not contains file with .ovf suffix')
# ---
def __exit__(self, *exc) -> None:
        self.shutdown()
# ---
def test_augment_bank_no_duplicates(bank):
    augmented, _ = augment_bank_with_egraph(bank)
    for node_type, entries in augmented.entries.items():
        sources = [e.source for e in entries]
        assert len(sources) == len(set(sources)), f"Duplicates in {node_type}"
# ---
def __init__(self, message, error):
        super(ImejiError, self).__init__(message)
        self.error = error.get('error') if isinstance(error, dict) else error
# ---
def __init__(
            self,
            filepath,
            webhdfs_conn_id='webhdfs_default',
            *args, **kwargs):
        super(WebHdfsSensor, self).__init__(*args, **kwargs)
        self.filepath = filepath
        self.webhdfs_conn_id = webhdfs_conn_id
# ---
def body_init(self):
        self.body = []
# ---
def test_augment_bank_increases_size(bank):
    augmented, added = augment_bank_with_egraph(bank)
    assert added > 0
    assert augmented.total_entries > bank.total_entries
# ---
def test_str_split_default(self):
        def test_impl(df):
            return df.A.str.split()

        df = pd.DataFrame({'A': ['AB CC', 'C ABB D', 'G ', ' ', 'g\t f']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def put(self, obj: Any) -> Any:
        """Store an object and return a reference to it.

        Args:
            obj: Object to store

        Returns:
            Reference to the stored object (type depends on context)
        """
        ...
# ---
def handler():
            nonlocal called
            called += 1
            raise ValueError("yeah sync")
# ---
def _forward(x: jax.Array, labels: jax.Array, w: jax.Array):
        return linear_softmax_cross_entropy_loss_fwd_pallas_mosaic_tpu(
            x,
            labels,
            w,
            block_sizes=block_sizes,
            dtype=dtype,
            logit_soft_cap=logit_soft_cap,
            precision=precision,
        )
# ---
def volume_alsa(vol):
    # With ALSA on CHIP
    if vol == 'up':
        db = subprocess.check_output(["amixer set 'Power Amplifier' 5%+"], shell=True)
        #db = os.system("amixer set 'Power Amplifier' 5%+")
    if vol == 'down':
        db = subprocess.check_output(["amixer set 'Power Amplifier' 5%-"], shell=True)
        #db = os.system("amixer set 'Power Amplifier' 5%-")
    i = db.rfind(':')
    return db[i+1:]
# ---


def median(l: list):
    """Return median of elements in the list l.
    >>> median([3, 1, 2, 4, 5])
    3
    >>> median([-10, 4, 6, 1000, 10, 20])
    15.0
    """
    l = sorted(l)
    if len(l) % 2 == 1:
        return l[len(l) // 2]
    else:
        return (l[len(l) // 2 - 1] + l[len(l) // 2]) / 2.0
# ---
def __repr__(self):
        """
        For `print` and `pprint`
        """
        return self.to_str()
# ---
def flow_backend_ctx():
    """Set up sync backend for all download tests."""
    with fray_default_job_ctx(create_job_ctx("sync")):
        yield
# ---
def platform_type(s):
    if s not in _VALID_PLATFORMS:
      raise argparse.ArgumentTypeError(f'Invalid Platform specified: "{s}".')
    return s
# ---
def test_endswith_sqlexpr(self):
        col = self.tables.some_table.c.data
        self._test(
            col.endswith(literal_column("'e%fg'")), {1, 2, 3, 4, 5, 6, 7, 8, 9}
        )
# ---
def test_stack_state_dict(input_dict, prefix, expected_output):
    result = _stack_state_dict(input_dict, prefix)
    for key in expected_output:
        assert jnp.all(jnp.array_equal(result[key], expected_output[key])), f"Failed on key: {key}"

    # now unstack it
    unstacked = _unstack_state_dict(result, prefix)
    for key in input_dict:
        assert jnp.all(jnp.array_equal(unstacked[key], input_dict[key])), f"Failed on key: {key}"
# ---
def __call__(self, indices: int) -> int:
        """Apply the permutation to a single integer.

        Args:
            indices: An integer to be permuted.

        Returns:
            The permuted value.
        """
        ...
# ---
def getAtomFeed(url, login, pwd):
	# var
	MAX_TRY = 10
	essai = 0

	# get atom document
	while essai < MAX_TRY:
		try:
			r = requests.get('http://' + url, auth=(login,pwd), timeout=10)
		except:
			essai += 1
			continue
		break
	else:
		raise ('Erreur lors de la requÃªte')

	# parse atom document
	try:
		dom = xml.dom.minidom.parseString(r.text)
	except:
		raise ('Erreur lors du parsing du document Atom')

	return dom
# ---
def the_tokenizer(self) -> HfTokenizer:
        if self.tokenizer == "passthrough":
            return PassthroughTokenizer(self.vocab_size)
        else:
            return load_tokenizer(self.tokenizer)
# ---
def the_object_name_is_not_in_the_collection_collection(name, collection):
    assert collection not in [c.name for c in the_object_name_exists(name).users_collection]
# ---
def relationship(request, partner_username):
    partner = get_object_or_404(Profile, user__username=partner_username)
    if partner == request.profile:
        raise Http404  # Can't have relationship with yourself.
    account = request.profile.account(partner)
    if account:
        entries = account.entries
        balance = account.balance
    else:
        entries = []
        balance = 0
    profile = partner  # For profile_base.html.
    return locals()
# ---
def from_periods(cls, start, end):
        """Create an interval with two Periods as the endpoints.
        """
        end_date = end.open_right_endpoint() if end is not None else None
        start_date = start.date if start is not None else None
        return cls(start_date, end_date)
# ---
def setUp(self):
        self.config = ConfigReader("""
            <root>
                <person>
                    <name>å±±ç”°</name>
                    <age>15</age>
                </person>
                <person>
                    <name>ä½è—¤</name>
                    <age>43</age>
                </person>
            </root>
            """)
# ---
def __getitem__(self, slices: SliceSpec) -> "_NamedIndexUpdateRef":
        return _NamedIndexUpdateRef(self.array, _convert_index_expr_to_dict(slices))
# ---
def p_translation_task(self, p):
        """
        translate_task : ID DONE TASK
                       | ID TASK
        """
        if len(p) == 4:
            done = True
            content = p[3]
        elif len(p) == 3:
            done = False
            content = p[2]
        task = Task(p[1], content, done)
        self.todo.append(task)
# ---
def _get_field(node: ast.AST, field_name: str):
    """Get a field value from an AST node, or None if missing."""
    try:
        return getattr(node, field_name)
    except AttributeError:
        return None
# ---
def _get_acl_template_ids(conn, policy_uuid):
    query = sa.sql.select([policy_template.c.template_id]).where(
        policy_template.c.policy_uuid == policy_uuid
    )
    return [acl_template_id for (acl_template_id,) in conn.execute(query).fetchall()]
# ---
def should_update(self):
        result = self._update_changed_options()
        if result:
            return True
        return False
# ---
def do_load(self, clear_first):
        if clear_first:
            self.match_list_box.Clear()
        msg = Forseti.ScheduleLoadCommand()
        msg.clear_first = clear_first
        print('Requesting load')
        self.lc.publish('Schedule/Load', msg.encode())
# ---
def test_random_mutation_returns_none_for_empty_bank():
    bank = SubtreeBank()
    mutation = random_mutation("x = 1\n", bank)
    assert mutation is None
# ---
def testImportMember(self):
    self.assertEqual((0, "<type 'dict'>\n"), _GrumpRun(textwrap.dedent("""\
        from sys import modules
        print type(modules)""")))
# ---
def testFun():
    print('Starting')
    while True:
        time.sleep(3)
        print('looping')
        time.sleep(3)
        print('3 Seconds Later')
# ---
def __repr__(self):
        return '<{}: {}{}>'.format(
            type(self).__name__,
            self.field,
            '+' if self.ascending else '-',
        )
# ---
def body(i, ref_counts):
            def dec(rc):
                page = seq_pages["page", i].scalar()
                return rc.at["page", page].add(-1)

            return jax.lax.cond(is_valid_page["page", i].scalar(), dec, lambda x: x, ref_counts)
# ---
def stop(self):
        """Stop polling."""
        self._stop_event.set()
        if self._thread:
            self._thread.join(timeout=5.0)
        self._client.close()
# ---
def __repr__(self):
        return ("{0.__class__.__name__}({0.field!r}, {0.pattern!r}, "
                "{0.fast})".format(self))
# ---
def test_add_already_added(self):
        prio_set_list = event._PrioritizedSetList()
        obj = object()
        prio_set_list.add(0, obj)

        with pytest.raises(ValueError) as excinfo:
            prio_set_list.add(0, obj)
        excinfo.match(r"has already been added")

        with pytest.raises(ValueError) as excinfo:
            prio_set_list.add(1, obj)
        excinfo.match(r"has already been added")
# ---
def render(self, mystery):
        return json.dumps(mystery.encode(), indent=4)
# ---
def __repr__(self):
        return f"ReshardOp(num_shards={self.num_shards})"
# ---
def backwards(self, orm):
        # Deleting field 'Idea.color'
        db.delete_column(u'brainstorming_idea', 'color')
# ---
def __init__(self, *args, **kwargs):
		self.stateName = kwargs["stateName"]
		self.root = args[0]
		self.id = kwargs["id"]
		Frame.__init__(self, self.root.mainWindow)
		self.config(
			background="gold"
		)
		self.place(relwidth=1, relheight=1)
# ---
def _now_ms() -> int:
    """Return current Unix timestamp in milliseconds."""
    return int(time.time() * 1000)
# ---
def _call_fn(self, index, item):
        if "key" in self._extra_kwargs:
            key = self._maybe_fold_in_key(self._extra_kwargs["key"], index)
            kwargs = {**self._extra_kwargs, "key": key}
        else:
            kwargs = self._extra_kwargs
        return self.fn(item, *self._extra_args, **kwargs)
# ---
def loss_fn(x_in, w_in, y_in):
        return fused_cross_entropy_loss_and_logsumexp_penalty(
            x_in,
            y_in,
            w_in,
            reduction="mean",
            logsumexp_weight=0.0,
            block_sizes=block_sizes,
            dtype=jnp.float32,
            logit_soft_cap=None,
            implementation="xla",
        )
# ---
def get_output_shape_for(self, input_shape):
        if self.dim_ordering == 'channels_last':
            return (input_shape[0], input_shape[4])
        else:
            return (input_shape[0], input_shape[1])
# ---
def convert_sub(self, el, text, convert_as_inline):
        if not text:
            return ""
        return f"<sub>{text}</sub>"
# ---
def test_ragged_paged_attention_multi_seq(seq_lens):
    rng = jr.PRNGKey(hash(tuple(seq_lens)))
    q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs = _build_random_case(rng, seq_lens)

    ragged = jit_rpa(q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs, sm_scale=SM_SCALE)
    ref = _reference_attention(q, kv_pages, kv_lens, page_indices, cu_q_lens, seq_lens)

    assert ragged.axes == ref.axes
    tol = _rpa_tol()
    assert_trees_all_close(ragged.array, ref.array, atol=tol, rtol=tol)
# ---
def compute(mdl, inp):
            return mdl(inp, attn_mask=attn_mask).array
# ---
def _return_hosts(self, hosts: list[str]) -> None:
        """Return hosts to the available pool (callback for ManualVmGroup.terminate)."""
        for host in hosts:
            if host in self._hosts:
                self._available_hosts.add(host)
                logger.debug("Host %s returned to pool", host)
# ---
def __init__(self, api_job):
        super(SetVolumeOption, self).__init__()
        self.api_job = api_job
        self.atom = SetVolumeOption
# ---
def test_arrow_io_pipeline(benchmark: Any, small_parquet_path: str, batch_size: int) -> None:
    """
    Python End-to-End: Python reads file -> Stream of RecordBatches -> Rust (called per batch).
    Includes Parquet parsing overhead and Python loop overhead.
    """

    def _pipeline() -> int:
        batches = pq.ParquetFile(small_parquet_path).iter_batches(batch_size=batch_size)
        return sum(len(dupekit.process_arrow_batch(b)) for b in batches)

    assert benchmark(_pipeline) > 0
# ---


def max_element(l: list):
    """Return maximum element in the list.
    >>> max_element([1, 2, 3])
    3
    >>> max_element([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])
    123
    """
    m = l[0]
    for e in l:
        if e > m:
            m = e
    return m
# ---
def __init__(self):
            super().__init__()
            self.field = named1
            self.flag = True
# ---
def get_size_bytes(blob: str) -> int:
    return len(blob.encode("utf-8"))
# ---
def real(a: A) -> A:
    return wrap_elemwise_unary(jnp.real, a)
# ---
def _visible_endpoints(self, predicate: Callable[[ControllerEndpoint], bool]) -> list[ControllerEndpoint]:
        """Return endpoints matching predicate whose jobs are in non-terminal states."""
        results = []
        for ep in self._endpoints.values():
            if not predicate(ep):
                continue
            job = self._jobs.get(ep.job_id)
            if job and not job.is_finished():
                results.append(ep)
        return results
# ---
def test_mem_write_word_at_bottom_right_just_past(self):
        self.mda.mem_write_word(3999, 0xFF08) # 'Z' with intensity.
        self.assertEqual(self.mda.video_ram[3998], 0x00) # Should be unmodified.
        self.assertEqual(self.mda.video_ram[3999], 0x08)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x00, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def test_ball():
    check_gen_is_equal(lambda k, s: jax.random.ball(k, Digit.size, shape=s), lambda k, s: hax.random.ball(k, s, Digit))
# ---
def mode(self) -> Literal["native", "docker"]:
        if self.docker_container_name:
            return "docker"
        if self.process is not None or self.log_dir is not None:
            return "native"
        raise RuntimeError("Unable to infer vLLM server mode from handle state.")
# ---
def insert(self, resource, doc_or_docs, **kwargs):
        ids = []
        kwargs.update(self._es_args(resource))
        for doc in doc_or_docs:
            doc.update(self.es.index(body=doc, id=doc.get('_id'), **kwargs))
            ids.append(doc['_id'])
        get_indices(self.es).refresh(self.index)
        return ids
# ---
def publish(c):
    """Publish to production via rsync"""
    c.run('pelican -s publishconf.py')
    c.run(
        'rsync --delete --exclude ".DS_Store" -pthrvz -c '
        '{} {production}:{dest_path}'.format(
            CONFIG['deploy_path'].rstrip('/') + '/',
            **CONFIG))
# ---
def unsize_axes(axis_spec: AxisSelection, to_unsize: AxisSelection) -> AxisSelection: ...
# ---
def __post_init__(self):
        if self.temperature < 1e-4:
            logger.warning(
                "SamplingParams.temperature is very low (%f). Greedy decoding is generally "
                "not useful for RL training as it limits exploration.",
                self.temperature,
            )
        if self.top_k == 1:
            logger.warning("SamplingParams.top_k is 1. Greedy decoding is generally not useful for RL training.")
# ---
def num_chips(self) -> int:
        """Get the number of accelerator chips."""

        return _num_accelerator_chips(self.train_config.resources)
# ---
def cell():  # i, j):
            difficulty = 8
            """6 + (
                (9 if i == j else
                 8)
                if i in [0, size - 1] and j in [0, size - 1] else
                (7 if j in [0, size - 1] else
                 (6 if j % 2 == 1 and (i in [0, size - 1] or j in [0, size - 1]) else
                  (5 if 0 < i < size - 1 else 8))))"""

            for li in fpik(e(r(), difficulty)):
                yield li
# ---
def output_close_html(self):
        text = self.token['text']
        return self.renderer.block_html(text)
# ---
def run_id(self) -> str:
        """Returns the run id"""
        assert self.config.id is not None
        return self.config.id
# ---
def set_fps(self, fps):
        """Set the label text for the given FPS estimation.

        Called by `update` every `update_period` seconds.

        :Parameters:
            `fps` : float
                Estimated framerate of the window.

        """
        self.label.text = '%.2f' % fps
# ---
def deposit(amount):
    global balance
    balance += amount
    return balance
# ---
def test_raw_metric_host_disk(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'disk_usage_rate_average',
        host_name)

    for record in query:
        if record.disk_usage_rate_average is not None:
            assert record.disk_usage_rate_average > 0, 'Zero Host Disk IO'
            break
# ---
def find_user_password(self, realm, authuri):
        self.target_realm = realm
        self.target_url = authuri
        return self.user, self.password
# ---
def sources_restart(self):
        node = bottle.request.body.getvalue().decode('utf-8')
        self.ndb.sources[node].start()
# ---
def __init__(
        self,
        pdrop: float = 0.5,
        broadcast_axes: AxisSpec | None = None,
        inference: bool = False,
    ):
        self.pdrop = pdrop
        self.broadcast_axes = broadcast_axes
        self.inference = inference
# ---
def __init__(self, rules):
        """Initialize the 'and' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules
# ---
def setUp(self):
        crypto._fernet = None
# ---
def should_display_status_to_user(self):
        """
        Whether or not the status should be displayed to the user.
        """
        return False
# ---
def translate_unity(unity):
    return UNITIES.get(unity, UNITIES["NONE"])
# ---
def compute_loss(model: LmHeadModel, input_ids):
            example = LmExample.causal(input_ids, eos_id=converter.tokenizer.eos_token_id)
            return model.compute_next_token_loss(example, key=None).scalar()
# ---
def prefix(self):
        return True
# ---
def __getattr__(self, method_name: str) -> LocalActorMethod:
        if method_name.startswith("_"):
            raise AttributeError(method_name)
        method = getattr(self._instance, method_name)
        if not callable(method):
            raise AttributeError(f"{method_name} is not callable on {type(self._instance).__name__}")
        return LocalActorMethod(method, self._executor)
# ---
def stop(self) -> None:
        """Stop background thread."""
        if self._thread is not None:
            self._stop_event.set()
            self._thread.join(timeout=5.0)
            self._thread = None
            logger.info("Stopped ReplayDataLoader background thread")
# ---
def decode_hooks(self):
    return [convert_predictions_to_image_summaries]
# ---
def test_wait_for_connection_returns_false_on_timeout(mock_conn_avail, _mock_sleep):
    """wait_for_connection returns False when timeout expires."""
    mock_conn_avail.return_value = False
    conn = MagicMock()
    # Use a very short timeout so the real monotonic deadline expires quickly
    assert wait_for_connection(conn, timeout=Duration.from_ms(50), poll_interval=Duration.from_ms(10)) is False
# ---
def free(self):
        EnkfNode.cNamespace().free(self)
# ---
def convert_to_cache(self, value, record, validate=True):
        if not value:
            return False
        if isinstance(value, basestring):
            if validate:
                # force parsing for validation
                self.from_string(value)
            return value[:DATE_LENGTH]
        return self.to_string(value)
# ---
def update_pbar(step: StepInfo):
        pbar.update(step.next_step - pbar.n)
        pbar.set_postfix(loss=jnp_to_python(step.loss))
# ---
def _as_array(x):
            # for dumb reasons, pa.array doesn't support ndarrays with ndim > 1
            if isinstance(x, np.ndarray) and x.ndim > 1:
                return [_as_array(y) for y in x]
            elif isinstance(x, np.ndarray):
                return list(x)
            else:
                return pa.array(x)
# ---
def get_listbox_items(drinkers):
    items = []

    for drinker in drinkers:
        items.append(unicode('%s, %d drinks, %s' % (drinker.name, drinker.drinks, drinker.idle)))

    return items
# ---
def task_status(self, task_name: JobName) -> cluster_pb2.TaskStatus:
        """Get status of a specific task.

        Args:
            task_name: Full task name (/job/.../index)

        Returns:
            TaskStatus proto containing state, worker assignment, and metrics
        """
        return self._cluster_client.get_task_status(task_name)
# ---
def _maybe_swap(self, op: ast.AST, group: tuple) -> ast.AST:
        if type(op) in group and self.rng.random() < self.swap_prob:
            alternatives = [cls for cls in group if cls is not type(op)]
            if alternatives:
                self._changed = True
                return self.rng.choice(alternatives)()
        return op
# ---
def test_getitem_out_of_bounds():
    with tempfile.TemporaryDirectory() as tmpdir:
        exemplar = {"a": np.array([0], dtype=np.float64), "b": np.array([0], dtype=np.float64)}
        builder = TreeStore.open(exemplar, tmpdir)

        batch = [
            {"a": np.array([1.0, 2.0]), "b": np.array([3.0, 4.0])},
            {"a": np.array([5.0, 6.0]), "b": np.array([7.0, 8.0])},
        ]
        builder.extend(batch)

        with pytest.raises(IndexError):
            builder[2]
# ---
def main(cfg: DatasetConversionConfig) -> None:
    """CLI entrypoint."""
    hf_dataset_to_jsonl(cfg)
# ---
def rnai_1(testapp, lab, award, source, target):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'RNAi',
        'reagents': [{'source': source['@id'], 'identifier': 'addgene:12345'}],
        'rnai_sequences': ['ATTACG'],
        'modified_site_by_target_id': target['@id']
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def __init__(self,Occurrence):
        self.directory=Occurrence[0]
# ---
def get_block_metadata(self, header_hash: bytes) -> Optional[BlockMetadata]:
        with self.lock:
            return self._state.get_block_metadata(header_hash)
# ---
def model_type(self) -> Type["Gpt2LMHeadModel"]:
        return Gpt2LMHeadModel
# ---
def handle_time(self, channel, data):
        msg = Forseti.Time.decode(data)
        #wx.CallAfter(self.time_text.SetLabel, format_time(msg.game_time_so_far))
        wx.CallAfter(self.match_control.set_time, msg)
# ---
def __repr__json__(self):
        return json.dumps(self.__repr__dict__())
# ---
def _files(self):
        return []
# ---
def get_job_status(
        self,
        job_id: str,
    ) -> cluster_pb2.Controller.GetJobStatusResponse:
        """Get the status of a job."""
        request = cluster_pb2.Controller.GetJobStatusRequest(job_id=job_id)
        return self._service.get_job_status(request, None)
# ---
def display(self, callingWindow, srcContext, mainItem):
        if srcContext not in ("marketItemGroup", "marketItemMisc") or self.mainFrame.getActiveFit() is None:
            return False

        if mainItem is None:
            return False

        for attr in ("emDamage", "thermalDamage", "explosiveDamage", "kineticDamage"):
            if mainItem.getAttribute(attr) is not None:
                return True

        return False
# ---
def generate_hash_from_pair(chosen, rejected) -> str:
    """Generate a hash from chosen and rejected message lists."""
    return hashlib.sha256((str(chosen) + str(rejected)).encode()).hexdigest()
# ---
def dispatch_task(state: ControllerState, task: ControllerTask, worker_id: WorkerId) -> None:
    """Dispatch a task to a worker: assign + mark running."""
    state.handle_event(
        TaskAssignedEvent(
            task_id=task.task_id,
            worker_id=worker_id,
        )
    )
    state.handle_event(
        TaskStateChangedEvent(
            task_id=task.task_id,
            new_state=cluster_pb2.TASK_STATE_RUNNING,
            attempt_id=task.current_attempt_id,
        )
    )
# ---
def test_synthetic_subtrees_valid_python(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=20)
    for entry in entries:
        try:
            ast.parse(entry.source)
        except SyntaxError:
            pytest.fail(f"Synthetic subtree is not valid Python: {entry.source!r}")
# ---
def test_step_slicing():
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=2, dtype=jnp.float32)

        data = jnp.array([[1.0, 2.0], [3.0, 4.0]])
        builder.append(data)
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "Gpt2HyenaModel":
        new_embeddings = self.embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, embeddings=new_embeddings)
# ---
def load(cache_dir: str, exemplar: T, options: Optional["CacheMetadata"] = None) -> "TreeCache":
        logger.info(f"Loading cache from {cache_dir}")
        ledger = CacheLedger.load(cache_dir, options)

        if not ledger.is_finished:
            raise FileNotFoundError(f"Cache at {cache_dir} is not finished. Use build_or_load to build it.")
        return TreeCache(cache_dir, exemplar, ledger)
# ---
def to_active_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            return leaf.as_scanned_result(leading_axis)
        else:
            return leaf
# ---
def _http_headers(cfg: UncheatableEvalDownloadConfig) -> dict[str, str]:
    headers = {"Accept": "application/vnd.github+json"}
    token = cfg.github_token or os.environ.get("GITHUB_TOKEN")
    if token:
        headers["Authorization"] = f"Bearer {token}"
    return headers
# ---
def _create_app(self) -> Starlette:
        rpc_wsgi_app = WorkerServiceWSGIApplication(service=self._service)
        rpc_app = WSGIMiddleware(rpc_wsgi_app)

        routes = [
            Route("/health", self._health),
            Route("/", self._dashboard),
            Route("/task/{task_id:path}", self._task_detail_page),
            Route("/logs", self._logs_page),
            static_files_mount(),
            Mount(rpc_wsgi_app.path, app=rpc_app),
        ]
        return Starlette(routes=routes)
# ---
from typing import List, Any


def filter_integers(values: List[Any]) -> List[int]:
    """ Filter given list of any python values only for integers
    >>> filter_integers(['a', 3.14, 5])
    [5]
    >>> filter_integers([1, 2, 3, 'abc', {}, []])
    [1, 2, 3]
    """
    return [x for x in values if isinstance(x, int)]
# ---
def setup(self, env):
        """ Make sure that ``self`` is set up, except for recomputation triggers. """
        if not self.setup_done:
            if self.related:
                self._setup_related(env)
            else:
                self._setup_regular(env)
            self.setup_done = True
# ---
def copy(a: A) -> A:
    return wrap_elemwise_unary(jnp.copy, a)
# ---
def any_failed(self) -> bool:
        """True if any VM has failed or been preempted."""
        return any(v.state in (vm_pb2.VM_STATE_FAILED, vm_pb2.VM_STATE_PREEMPTED) for v in self.vms)
# ---
def q_heads_per_group(self) -> int:
        return self.num_heads // self.num_kv_heads
# ---
def strikethrough(self, text):
        """Rendering ~~strikethrough~~ text.

        :param text: text content for strikethrough.
        """
        return '<del>%s</del>' % text
# ---
def getvalue(self, datas=None):
        pos_value = 0.0
        for data in datas or self.positions.keys():
            comminfo = self.getcommissioninfo(data)
            position = self.positions[data]
            pos_value += comminfo.getvalue(position, data.close[0])

        return self.cash + pos_value
# ---
def finalize(self):
        self.is_complete = True
        if self.complete_promise is not None:
            self.complete_promise.set_result(None)
            if not asyncio.get_event_loop().is_running():
                _executor.submit(lambda: asyncio.run(self.notify_length_update()))
            else:
                asyncio.create_task(self.notify_length_update())
# ---
def write(self, buf, flags=0):
        return self.sendall(buf, flags)
# ---
def accept(self):
        sock, addr = self._sock.accept()
        client = OpenSSL.SSL.Connection(sock._context, sock)
        return client, addr
# ---
def get_if_frequency(self):
        return self._if_frequency
# ---
def test_reduce_empty(backend):
    """Test reduce on empty dataset."""
    ds = Dataset.from_list([]).reduce(sum)
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 0
# ---
def delete_router_postcommit(self, context, router_context):
        pass
# ---
def find_ports(self, inc_port):
        ''' find a specific port '''
        for port in self.get_ports():
            if port['port'] == inc_port['port']:
                return port

        return None
# ---
def _dest_images(self):
        ret = []
        for vol in self._prepared_volumes:
            ret.append(vol['path'])
        return ret
# ---
def num_shards(self) -> int:
        return len(self.shard_names)
# ---
def argmax(array: NamedArray, axis: AxisSelector | None) -> NamedArray:
    return wrap_reduction_call(jnp.argmax, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def hashable_partition(pytree, filter_spec):
    dynamic, static = eqx.partition(pytree, filter_spec)
    static_leaves, static_treedef = jtu.tree_flatten(static)
    static_leaves = tuple(static_leaves)
    return dynamic, (static_leaves, static_treedef)
# ---
def check_pyrefly(files: list[pathlib.Path], fix: bool) -> int:
    if not files:
        return 0

    click.echo("\nPyrefly type checker:")
    args = ["uvx", "pyrefly@0.40.0", "check", "--baseline", ".pyrefly-baseline.json"]
    return run_cmd(args).returncode
# ---
def withdraw(amount):
    global balance
    balance -= amount
    return balance
# ---
def __init__(self, table=None, n_classes=2, class_names=("1", "0")):
        self.table = table
        self.n_classes = n_classes
        self.class_names = class_names
        if table is None:
            self.table = np.zeros((self.n_classes, self.n_classes), dtype=int)
# ---
def is_jax_array_like(x):
    return hasattr(x, "shape") and hasattr(x, "dtype")
# ---
def data_loader(self):
        self.load_data()

        self.data_saver_th = threading.Thread(target=self.data_saver)
        self.data_saver_th.daemon = True
        self.data_saver_th.start()

        self.data_updater_th = threading.Thread(target=self.data_updater)
        self.data_updater_th.daemon = True
        self.data_updater_th.start()
# ---
def _assign_linear_weight(named_linear: hnn.Linear, np_weight: jnp.ndarray, out_axis: Axis, in_axis: Axis):
            w_named = hax.named(jnp.asarray(np_weight, dtype=jnp.float32), (out_axis.name, in_axis.name))
            return dataclasses.replace(named_linear, weight=w_named)
# ---
def with_group(self, release_time=0.25):
        import supriya.patterns

        return supriya.patterns.Pgroup(self, release_time=release_time)
# ---
def sort(self, items):
        return items
# ---
def model_type(self) -> Type["Olmo3LMHeadModel"]:
        return Olmo3LMHeadModel
# ---
def wait(self, futures: list, num_returns: int = 1) -> tuple[list, list]:
        ready, pending = ray.wait(futures, num_returns=num_returns, fetch_local=False)
        return list(ready), list(pending)
# ---
def device_info(self):
        """Device info."""
        return self.coordinator.device_info
# ---
def tolist(self) -> Any:  # pragma: no cover
        return self.array.tolist()
# ---
def test_registrar(self):
        eq_(self.record.registrar, None)
# ---
def test_evaluate_float(self):
        expr = lit(3.14)
        assert expr.evaluate({}) == 3.14
# ---
def push(self, item):
        """Push a new element on the queue

        Parameters
        ----------
        item :
            The element to push on the queue

        """
        raise NotImplementedError
# ---
def _is_passive_array(arr):
    return isinstance(arr, _PassiveNamedArray)
# ---
def decodeFailed(self, seg):
        if ( seg == None ): return
        mt.log.debug("Segment failed to decode: " + seg.msgid)
        self.segFailed(seg)
# ---
def permutation(key, x: NamedArray, axis: AxisSelector, independent: bool = False):
    axis_index = x.axis_indices(axis)
    jax_array = jrandom.permutation(key, x.array, axis_index, independent=independent)
    return haliax.auto_sharded(NamedArray(jax_array, x.axes))
# ---
def _get_client(self) -> IrisClientLib:
        """Get IrisClient from context."""
        from iris.client.client import get_iris_ctx

        ctx = get_iris_ctx()
        if ctx is None or ctx.client is None:
            raise RuntimeError("IrisActorGroup requires IrisContext with client. " "Set context via iris_ctx_scope().")
        return ctx.client
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        return 1
# ---
def batchify(batch: Iterable, n: int = 1024) -> Iterable:
    iterator = iter(batch)
    while batch := tuple(itertools.islice(iterator, n)):
        yield batch
# ---
def namespace(self) -> str:
        """Get the namespace (root component) for actor isolation."""
        return self._parts[0]
# ---
def plot(files, fac=1.0):
    for f in files:
        if f.split('.')[-1] == 'xy':
            td = np.loadtxt(f)
            plt.plot(td[:, 0], np.log(1. / td[:, 1]) * fac, label=f)
        elif f.split('.')[-1] == 'spc':
            td = SPC(f)
            plt.plot(td.xdata, np.log(1. / np.array(td.ydata)), label=f)
    plt.legend()
    plt.show()
# ---
def Player(self):
        return self.__data['cls_player']
# ---
def embed(self, input_ids, *args):
        input_embeds = self.token_embeddings(input_ids)
        if self.norm is not None:
            input_embeds = self.norm(input_embeds)
        return input_embeds
# ---
def _format_kvpairs(**kwargs):
	return ', '.join('{0!s}="{1}"'.format(k, _filter_token(v)) for (k, v) in kwargs.items())
# ---
def convert_to_read(self, value, use_name_get=True):
        return value.ids
# ---
def delete(name: str, zone: str):
    """Delete a TPU VM by name in the specified zone."""
    delete_tpu_vm(name, zone)
# ---
def test_contains_unescaped(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b%cde"), {1, 2, 3, 4, 5, 6, 7, 8, 9})
# ---
def unique_name(prefix: str) -> str:
    """Generate a unique job name with the given prefix."""
    return f"{prefix}-{uuid.uuid4().hex[:8]}"
# ---
def __init__(
        self,
        problem: str,
        answer: str,
        grader: Literal["sympy", "math_verify"] = "sympy",
        timeout: float = 1.0,
    ):
        self.problem = problem
        self.answer = answer
        self.grader = grader
        self.timeout = timeout
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        for tracker in self.loggers:
            tracker.log_hyperparameters(hparams)
# ---
def valid_key(key, sep='.'):
        '''validate the incoming key'''
        common_separators = list(Yedit.com_sep - set([sep]))
        if not re.match(Yedit.re_valid_key.format(''.join(common_separators)), key):
            return False

        return True
# ---
def clean_primeiro_numero(self):
        cleaned_data = self.cleaned_data

        telefone = Telefone()
        telefone.tipo = self.data['primeiro_tipo']
        telefone.ddd = self.data['primeiro_ddd']
        telefone.numero = self.data['primeiro_numero']
        telefone.principal = self.data['primeiro_principal']

        cleaned_data['primeiro_telefone'] = telefone
        return cleaned_data
# ---
def shutdown(self):
        """Terminate remaining jobs."""
        logger.info(f"Shutting down cluster with {len(self._jobs)} jobs")
        for job_id, job in self._jobs.items():
            try:
                job.cleanup()
            except Exception as e:
                logger.warning(f"Error cleaning up job {job_id}: {e}")
        self._jobs.clear()
# ---
def notify(self, order):
        self.notifs.append(order.clone())
# ---
def __init__(self, name, params):
        base.InstantiationValidationBenchmark.__init__(self, name, params)

        if common.RELEASE == 'liberty':
            temp_name = 'stress_workload_liberty.yaml'
        else:
            temp_name = 'stress_workload.yaml'

        self.template_file = common.get_template_dir() + \
            temp_name
        self.stack_name = 'neighbour'
        self.neighbor_stack_names = list()
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.square.insert(),
            [{"id": 1, "side": 10}, {"id": 10, "side": 42}],
        )
# ---
def active_scale(self):
        """Return the scaling applied to activations."""
        raise NotImplementedError
# ---
def _assert_fn(self, x, value=None):
        eq_(
            testing.db.execute(self.table.select()).fetchall(),
            [(x, value)]
        )
# ---
def test_distinct_selectable_in_unions(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2).distinct()
        s2 = select([table]).where(table.c.id == 3).distinct()

        u1 = union(s1, s2).limit(2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def __call__(self, target, creds, enforcer):
        """Recursively checks credentials based on the defined rules."""

        try:
            return enforcer.rules[self.match](target, creds, enforcer)
        except KeyError:
            # We don't have any matching rule; fail closed
            return False
# ---
def client():
    c = LocalClient(max_threads=4)
    yield c
    c.shutdown(wait=True)
# ---
def _start_process(name: str, cmd: list[str], env: dict[str, str]) -> ManagedProcess:
    printable = " ".join(cmd)
    print(f"Starting {name}: {printable}")
    proc = subprocess.Popen(cmd, cwd=ROOT, env=env)
    return ManagedProcess(name, proc)
# ---
def init(weight):
            return FoldModule(weight=weight)
# ---
def deploymentconfig(self):
        ''' deploymentconfig property '''
        return self.dconfig
# ---
def _clip_fn(u):
            clip_denom = hax.maximum(1.0, hax.sqrt(hax.mean(u * u)) / threshold)
            return u / clip_denom
# ---
def test_no_flexibility():
    partial_order = ("apple", "banana")
    candidates = ("banana", "apple", "cherry")
    with pytest.raises(ValueError):
        rearrange_for_partial_order(partial_order, candidates)
# ---
def __init__(self, config: LocalClusterConfig = LocalClusterConfig()):
        """Initialize local cluster."""
        self._jobs: dict[JobId, _LocalJob] = {}
        self.config = config
# ---
def test_one_step_edit_returns_first_edit():
    source = "a = 1\nb = 2\n"
    target = "a = 10\nb = 20\n"
    mutation = one_step_edit(source, target)
    assert mutation is not None

    # Applying should produce valid Python.
    result = mutation.apply(source)
    ast.parse(result)
# ---
def __init__(self, source: Iterable[T], operations: list[LogicalOp] | None = None):
        """Create a dataset from a source and optional operations.

        Args:
            source: Source data iterable
            operations: List of operations to apply
        """
        self.source = source
        self.operations = operations or []
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            df.A.fillna(5.0, inplace=True)
            DF = df.A.fillna(5.0)
            s = DF.sum()
            m = df.A.mean()
            v = df.A.var()
            t = df.A.std()
            Ac = df.A.cumsum()
            return Ac.sum() + s + m + v + t
# ---
def genetic_modification_1(lab, award):
    return {
        'modification_type': 'deletion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'modifiction_description': 'some description'
    }
# ---
def test_krackhardt_closeness(self):
        c = nx.closeness_centrality(self.K)
        d = {
            0: 0.529,
            1: 0.529,
            2: 0.500,
            3: 0.600,
            4: 0.500,
            5: 0.643,
            6: 0.643,
            7: 0.600,
            8: 0.429,
            9: 0.310,
        }
        for n in sorted(self.K):
            assert almost_equal(c[n], d[n], places=3)
# ---
def __init__(self, *args, **kwargs):
        """Product

        :param int integer
        :param str string
        """
        self.integer = None
        self.string = None

        super(Product, self).__init__(*args, **kwargs)
# ---
def is_jax_or_hax_array_like(x):
    return is_jax_array_like(x) or is_named_array(x)
# ---
def static_files_mount() -> Mount:
    """Mount for serving static JS/CSS assets (vendor libs, shared utils, app components)."""
    return Mount("/static", app=StaticFiles(directory=STATIC_DIR), name="static")
# ---
def __post_init__(self):
        if isinstance(self.name, Axis):
            raise ValueError(f"Axis name cannot be an Axis object: {self.name}")
# ---
def test_literal(self):
        expr = lit(42)
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def genetic_modification(testapp, lab, award):
    item = {
        'award': award['@id'],
        'lab': lab['@id'],
        'modified_site_by_coordinates': {
            'assembly': 'GRCh38',
            'chromosome': '11',
            'start': 20000,
            'end': 21000
        },
        'purpose': 'repression',
        'category': 'deletion',
        'method': 'CRISPR',
        'zygosity': 'homozygous'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> ListCache[KvPageCache]:
        caches = [layer.initial_cache(spec, dtype=dtype) for layer in self.layers.unstacked()]
        return ListCache(caches)
# ---
def test_valid_position_mask_invalid_source(tok):
    mask = tok.valid_position_mask("not valid{{{")
    assert not any(mask)
# ---
def heuristic_is_leaf_batched(x):
    if isinstance(x, list):
        return jnp.isscalar(x[0]) or is_jax_array_like(x[0])
    else:
        return False
# ---
def list_tpu_nodes(project: str, zone: str, filter_expr: str = "") -> list[dict[str, Any]]:
    """List TPU nodes in a zone."""
    cmd = [
        "gcloud",
        "compute",
        "tpus",
        "tpu-vm",
        "list",
        f"--project={project}",
        f"--zone={zone}",
        "--format=json",
    ]
    if filter_expr:
        cmd.append(f"--filter={filter_expr}")

    result = run_gcloud_command(cmd)
    return json.loads(result.stdout)
# ---
def __getitem__(self, idx: SliceSpec | EllipsisType = Ellipsis) -> NamedArray:
        """Read from the reference using named indexing semantics."""
        _, axes_spec, index_tuple = self._prepare(idx)
        result = self._ref[tuple(index_tuple)]
        return named(result, axes_spec)
# ---
def cursor_execute(conn, cursor, statement,
                        parameters, context, executemany):
            canary.append('cursor_execute')
            return statement, parameters
# ---
def update_sort_order(self):
        if self.type_ == 'PITCH_BEND':
            self.sort_order = -10
        if self.type_ == 'NOTE_OFF':
            self.sort_order = -20
# ---
def mk_LayerNorm(self, axis: AxisSpec):
        """Create a layer normalization module using the config."""
        return self.norm_config.build(axis)
# ---
def num_rows(self):
        return len(self.offsets)
# ---
def parse_list_block(self, m):
        bull = m.group(2)
        self.tokens.append({
            'type': 'list_start',
            'ordered': '.' in bull,
        })
        cap = m.group(0)
        self._process_list_item(cap, bull)
        self.tokens.append({'type': 'list_end'})
# ---
def replacer(match):
        old_link = match.group(0)
        path = match.group(1)
        new_link = f"marin.community/data-browser/{path}"
        replacements.append((old_link, new_link))
        return new_link
# ---
def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):
        super(LW_MaxPooling2D, self).__init__(pool_size, strides, padding, data_format)
# ---
def test_set_current_client_context_manager():
    """Explicitly set client should take priority over auto-detection."""
    explicit = LocalClient(max_threads=2)
    with set_current_client(explicit) as c:
        assert c is explicit
        assert current_client() is explicit
    # After exiting, should return a fresh default (or auto-detect)
    assert current_client() is not explicit
# ---
def mp(self) -> jmp.Policy:
        """Returns the mixed precision policy"""
        return self.config.mp
# ---
def delete_all(self):
        '''
        Delete all keys
        '''
        for status, keys in six.iteritems(self.list_keys()):
            for key in keys:
                try:
                    os.remove(os.path.join(self.opts['pki_dir'], status, key))
                except (OSError, IOError):
                    pass
        self.check_minion_cache()
        return self.list_keys()
# ---
def __init__(self, config: RLJobConfig):
        self.config = config
# ---
def hostloc(self):
        '''return host:port'''
        hostloc = self.hostname
        if self.port:
            hostloc = '{}:{}'.format(hostloc, self.port)

        return hostloc
# ---
def materialize_all():
            for shard in shards:
                yield from shard
# ---
def adamh_transform():
                components = []
                if self.max_grad_norm:
                    components.append(optax.clip_by_global_norm(self.max_grad_norm))
                components.append(scale_by_adamh(self.beta1, self.beta2, self.epsilon, learning_rate))
                optimizer = optax.chain(*components)
                return optimizer
# ---
def test_augment_bank_zero_augmentation_preserves_bank(bank, rng):
    augmented = augment_bank(bank, rng, n_renamed=0, n_perturbed=0, synthetic_count=0, use_egraph=False)
    assert augmented.total_entries == bank.total_entries
# ---
def offset(self, offset):
        Util.validate_type(offset, "int")
        return self._offset(offset)
# ---
def _inv_decay_schedule(lr: float, min_lr: float, decay_steps: int):
    def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / ((lr / min_lr - 1) * jnp.maximum(count, 1) / decay_steps + 1))
        return jnp.maximum(lr * decay, min_lr)

    return schedule
# ---
def merge(self):
        weight = self.lora.merge() + self.wrapped.weight
        return dataclasses.replace(self.wrapped, weight=weight)
# ---
def _is_relative_path(url_or_path):
    # if it's a url, it's not a relative path
    parsed_url = urlparse(url_or_path)

    if parsed_url.scheme:
        return False

    # otherwise if it starts with a slash, it's not a relative path
    return not url_or_path.startswith("/")
# ---
def is_trainable_and_floating_point(x):
        if x is True:
            return is_inexact_arrayish
        elif x is False:
            return False
        else:
            return lambda y: is_inexact_arrayish(y) and x(y)
# ---
def valid_date(s):
    try:
      return datetime.strptime(s, _DATE_FORMAT).date()
    except ValueError:
      raise argparse.ArgumentTypeError(f'Invalid date specified: "{s}".')
# ---
def __str__(self):
        return '{}: {}'.format(self.name, self.main_figure)
# ---
def test_bytes_io():
    f = py.io.BytesIO()
    f.write(tobytes("hello"))
    pytest.raises(TypeError, "f.write(totext('hello'))")
    s = f.getvalue()
    assert s == tobytes("hello")
# ---
def __init__(self, instances: list[dict] | None = None):
        self._instances = instances or []
# ---
def create_writer(self) -> "RolloutWriter":
        if self.storage_type == StorageType.FILE:
            if self.path is None:
                raise ValueError("path must be specified for FILE storage type")
            return FileRolloutWriter(self.path, self.max_rollout_files)
        else:
            if self.queue_name is None:
                raise ValueError("queue_name must be specified for IN_MEMORY storage type")
            return _get_or_create_queue(self.queue_name, self.queue_maxlen).writer()
# ---
def datasource_from_json(urls_or_paths: Sequence[str]) -> ShardedDataSource[dict]:
    return JsonDataSource(urls_or_paths)
# ---
def after(self, speaker_id):
        """Pick next person to speak"""
        row = self._transitions[speaker_id]
        sucessor = searchsorted(cumsum(row), rand() * sum(row))
        return sucessor
# ---
def _infer_tpu_type_from_config(config_data: dict | None) -> str | None:
    if not config_data:
        return None

    try:
        return config_data["available_node_types"]["tpu_worker"]["node_config"]["acceleratorType"]
    except KeyError:
        return None
# ---
def redraw(self, ctx):
        ctx.set_source(self.fill.pattern)
        self.indicator.redraw(ctx, self.metric.value)
# ---
def __iter__(self):
        return (x[0] for x in self._data.values())
# ---
def list_cluster_workers(zone: str, project: str) -> list[str]:
    result = subprocess.run(
        [
            "gcloud",
            "compute",
            "tpus",
            "tpu-vm",
            "list",
            f"--zone={zone}",
            f"--project={project}",
            "--format=value(name)",
        ],
        capture_output=True,
        text=True,
        check=True,
    )
    return [name for name in result.stdout.strip().split("\n") if name]
# ---
def run_on_pod_multislice(
    remote_fn: RemoteFunction | Callable, tpu_type: str, num_slices: Sequence[int]
) -> list[ray.ObjectRef]:
    """Run a remote function on multiple TPU slices."""
    return ray.get(
        run_on_pod(
            remote_fn,
            tpu_type,
            num_slices=num_slices,
            max_retries_failure=0,
            max_retries_preemption=0,
        )
    )
# ---
def _environment(self):
        env = super(XenCommand, self)._environment()
        env.update(self._ssh_agent.auth)
        return env
# ---
def jit_vmap_hist(a):
        """
        This function will be JIT compiled and VMapped.
        """
        # Call the sharded histogram function
        hist, bins = hax.vmap(levanter.tracker.histogram.sharded_histogram, Layer)(a, bins=32)
        return hist, bins
# ---
def drag_and_drop_by_offset(self, source, xoffset, yoffset):
        """Holds down the left mouse button on the source element,
           then moves to the target element and releases the mouse button.
        Args:
            source: The element to mouse down.
            xoffset: X offset to move to.
            yoffset: Y offset to move to.
        """
        self.click_and_hold(source)
        self.move_by_offset(xoffset, yoffset)
        self.release(source)
        return self
# ---
def testDeleteClassLocal(self):
    self.assertEqual((0, 'False\n'), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          bar = 'baz'
          del bar
        print hasattr(Foo, 'bar')""")))
# ---
def test_list_tables_page(self):
        """Call to ListTables should page results"""
        hash_key = DynamoKey("id")
        for i in range(120):
            self.dynamo.create_table("table%d" % i, hash_key=hash_key)
        tables = list(self.dynamo.list_tables(110))
        self.assertEqual(len(tables), 110)
# ---
def convert_tokens_to_ids(self, token):
            # In our simple test tokenizer, tokens are single chars
            return ord(token[0]) if token else 0
# ---
def i_press_operator(operator):
    if "(" in operator:
        exec(f"bpy.ops.{operator}")
    else:
        exec(f"bpy.ops.{operator}()")
# ---
def __call__(self, lhs, rhs, dimension_numbers, precision=None, preferred_element_type=None, **kwargs):
            return jax.lax.dot_general(
                lhs,
                rhs,
                dimension_numbers,
                precision=jax.lax.Precision.HIGHEST,
                preferred_element_type=preferred_element_type,
                **kwargs,
            )
# ---
def on_task_transition(self, old_state: int | None, new_state: int) -> int | None:
        """Update counts for a single task transition.

        Args:
            old_state: Previous task state, or None if new task
            new_state: New task state

        Returns:
            New job state if changed, None otherwise
        """
        if old_state is not None:
            self.task_state_counts[old_state] -= 1
        self.task_state_counts[new_state] += 1
        return self._compute_job_state()
# ---
def as_sql(self, compiler, connection):
        lhs, params = compiler.compile(self.lhs)
        return 'TIME({})'.format(lhs), params
# ---
def build_optimizer_config(self) -> AdamConfig:
        return AdamConfig(
            learning_rate=self.lr,
            weight_decay=self.weight_decay,
            lr_schedule=self.lr_schedule,
            decay=self.lr_cooldown_duration,
            min_lr_ratio=self.min_lr_ratio,
        )
# ---
def traced_func(i):
    return i
# ---
def _validate_job_done(job):
    if job.status != STATUS.DONE:
        raise JobNotDone("Job %r is %s" % (job.id, job.status))
# ---
def testMatMul_OutEmpty_A(self):
    n, k, m = 0, 8, 3
    x = self._randMatrix(n, k, np.float32)
    y = self._randMatrix(k, m, np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def do_handshake(self):
        self.__iowait(self._connection.do_handshake)
# ---
def batch_size_at_step(self, step: int) -> int:
        """
        Return the batch size (number of samples) at the given training step.
        """
        for seg in self.segments:
            if seg.start <= step < seg.until:
                return seg.value
        warnings.warn(f"Step {step} is beyond the last defined segment. Using the last segment's batch size.")
        return self.segments[-1].value
# ---
def test_hidden_linear_init_matches_linear_scaling(out_first: bool):
    In = hax.Axis("I", 6)
    Out = hax.Axis("O", 5)
    key = jrandom.PRNGKey(0)

    linear = Linear.init(In, Out, key=key, use_bias=False, out_first=out_first)
    hidden = Linear.init(
        In,
        Out,
        key=key,
        use_bias=False,
        out_first=out_first,
        reparam_cls=HiddenLinearMup,
    )

    assert jnp.allclose(hidden.weight.array, linear.weight.array)
# ---
def done(self, lease: Lease[Any]) -> None:
        with httpx.Client() as client:
            client.post(
                f"http://{self.host}:{self.port}/queues/{self.queue_name}/done",
                json={"lease_id": lease.lease_id, "timestamp": lease.timestamp},
            )
# ---
def evaluate(self, record: dict) -> Any:
        return record.get(self.name)
# ---
def resize_vocab(self, new_size: int, key: PRNGKeyArray | None = None) -> "HackableLMHeadModel":
        raise NotImplementedError("resize_vocab is not implemented for HackableLMHeadModel")
# ---
def test_deadline_timeout_integration():
    """Deadline can be used to enforce timeouts in operations."""
    deadline = Deadline.from_seconds(0.2)  # 200ms timeout
    iterations = 0

    while not deadline.expired():
        iterations += 1
        time.sleep(0.05)  # 50ms per iteration

    # Should have run ~4 iterations (200ms / 50ms)
    assert 3 <= iterations <= 5
# ---
def eliminate_axes(axis_spec: ShapeDict, axes: AxisSelection) -> ShapeDict:  # type: ignore
    ...
# ---
def f(x):
      return x + 1
# ---
def exists(self, tag: str) -> bool: ...
# ---
def test_direct_ssh_connection_accepts_duration_connect_timeout():
    """DirectSshConnection accepts Duration for connect_timeout field."""
    from iris.cluster.vm.ssh import DirectSshConnection

    conn = DirectSshConnection(
        host="10.0.0.1",
        connect_timeout=Duration.from_seconds(45),
    )
    cmd = conn._build_cmd("echo hello")
    # The SSH ConnectTimeout option should be the integer seconds value
    assert "ConnectTimeout=45" in " ".join(cmd)
# ---
def compute_reward(self, correct_answer: str, actual_response: str, tokenizer=None) -> float:
        # how many cats
        num_cats = actual_response.lower().count("cat")
        love_cats = actual_response.lower().count("love cats")

        return (num_cats + (10 * love_cats)) / np.sqrt(1 + len(actual_response))
# ---
def _init(self, maxsize):
        Queue._init(self, maxsize)
        self.all_items = set()
# ---
def test_get_type(self):
        self.assertEqual("http", self.get.get_type())
# ---
def cross_entropy(logits: hax.NamedArray, labels: hax.NamedArray) -> hax.NamedArray:
        return hax.take(logits, Embed, labels)
# ---
def imputed_max_tokens_per_round(self) -> int:
        """Return explicit `max_tokens_per_round` or default to `max_seqs` when unset."""
        return self.max_tokens_per_round if self.max_tokens_per_round is not None else self.max_seqs
# ---
def test_reduce_will_blow_your_mind(self):
        import functools
        # As of Python 3 reduce() has been demoted from a builtin function
        # to the functools module.

        result = functools.reduce(self.add, [2, 3, 4])
        self.assertEqual(int, result.__class__)
        # Reduce() syntax is same as Python 2

        self.assertEqual(9, result)

        result2 = functools.reduce(self.multiply, [2, 3, 4], 1)
        self.assertEqual(24, result2)
# ---
def shard_names(self) -> Sequence[str]:
        return list(self._shard_name_to_url_mapping.keys())
# ---
def __init__(self, reader, read_session=None):
        self._reader = reader
        if read_session is not None:
            self._stream_parser = _StreamParser.from_read_session(read_session)
        else:
            self._stream_parser = None
# ---
def init(cls):
        cfg = aqt_config.config_v3()
        return cls(cfg)
# ---
def test_scalar_eliminates_axis():
    B, S, V = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 4)
    x = hax.arange((B, S, V))
    out = x["seq", 1]
    assert out.axes == (B, V)
    assert jnp.array_equal(out.array, x.array[:, 1, :])
# ---
def make_harness_lm(self):
        if jax.process_index() == 0:
            return LevanterHarnessLM(self)
        else:
            raise ValueError("Only process 0 can create the harness")
# ---
def read_local(self):
        '''
        Read in the local private keys, return an empy dict if the keys do not
        exist
        '''
        path = os.path.join(self.opts['pki_dir'], 'local.key')
        if not os.path.isfile(path):
            return {}
        with salt.utils.fopen(path, 'rb') as fp_:
            return self.serial.loads(fp_.read())
# ---
def __init__(
        self,
        controller_address: str,
        timeout: float = 5.0,
        namespace: Namespace | None = None,
    ):
        self._address = controller_address.rstrip("/")
        self._timeout = timeout
        self._explicit_namespace = namespace
        self._client = ControllerServiceClientSync(
            address=self._address,
            timeout_ms=int(timeout * 1000),
        )
# ---
def _get_cluster_spec(self) -> str:
        return f"local?use_isolated_env={self.config.use_isolated_env}"
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.int32)})
            return df.A.quantile(.25)
# ---
def skip_if_no_torch(f):
    return pytest.mark.torch(pytest.mark.skipif(not has_torch(), reason="torch not installed")(f))
# ---
def endpoints(self) -> list[ResolvedEndpoint]:
        return list(self._resolve().endpoints)
# ---
def wrap_coeffs(coeffs):
        base = x if isinstance(x, NamedArray) else y
        axis = _poly_axis_from_input(base, coeffs.shape[0])
        return NamedArray(coeffs, (axis,))
# ---
def available(self) -> bool:
        """Return if entity is available."""
        return self.coordinator.last_update_success and self.coordinator.data["printer"]
# ---
def gt(cls, left: "PyExpr", right: "PyExpr") -> "PyExpr": ...
# ---
def setdata(self, request, response):
        self._calls.append(Call(request, response))
# ---
def relevant_keys(mapping):
            return [k for k, v in mapping.items()
                    if any(d in indexer_dims for d in v.dims)]
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_counts: typing.Literal[True],
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def on_load(self):
        if 'Authentication' in self.response.headers:
            self.browser.token = self.response.headers['Authentication'].split(' ')[-1]
# ---
def translate_path(path):
    return xbmc.translatePath(path).decode('utf-8')
# ---
def maybe_unstrip_protocol(path: str):
        base_path_protocol = urllib.parse.urlparse(str(checkpoint_path)).scheme
        if base_path_protocol != "" and not urllib.parse.urlparse(path).scheme != "":
            return f"{base_path_protocol}://{path}"
        return path
# ---
def isposinf(a: A) -> A:
    return wrap_elemwise_unary(jnp.isposinf, a)
# ---
def test_string(self):
        """Store and retrieve a string"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "abc"})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["id"], "abc")
        self.assertTrue(isinstance(item["id"], str))
# ---
def convert(messages):
            return [OpenAIChatMessage(role=msg[self.role_key], content=msg[self.content_key]) for msg in messages]
# ---
def spawn(
        self,
        target: Callable[..., Any],
        *,
        name: str | None = None,
        args: tuple = (),
        on_stop: Callable[[], None] | None = None,
    ) -> ManagedThread:
        thread = ManagedThread(target=target, name=name, args=args, on_stop=on_stop, _container=self)
        with self._lock:
            self._threads.append(thread)
        thread.start()
        return thread
# ---
def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
            raise Exception("breakage")
# ---
def _get_random_inputs(config: LlamaConfig, override_Pos=None):
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()
    return x, mask
# ---
def all_inputs_const(op):
    # If all inputs of an op are guaranteed constants, then we can infer that
    # the op produces a constant as well.
    return op.inputs and all(inp.op in constants for inp in op.inputs)
# ---
def __init__(self, data): self.data = data
# ---
def current_len(self) -> Optional[int]:
        return self._run_coroutine(self.dataset.current_len())
# ---
def lang_get(self):
        if self._lang is None:
            # self._lang = self.req.accept_language.best_matches('en-US') if self.req is not None else []
            # Note: Don't forget to add country-less language code when only a "language-COUNTRY" code is given.
            self._lang = ['fr-FR', 'fr']
            if self.req is not None:
                self.req.environ.setdefault('etalage', {})['_lang'] = self._lang
        return self._lang
# ---
def __init__(self, bundle_prefix: str):
        self._prefix = bundle_prefix.rstrip("/")
# ---
def get_process_logs(self, request: cluster__pb2.Worker.GetProcessLogsRequest, ctx: RequestContext) -> cluster__pb2.Worker.GetProcessLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def method5(self, k, l):
        return k + l
# ---
def always(self):
        return self._m1.always() and self._m2.always()
# ---
def _normalize_weights(weights, scale=1.0):
    total = sum(weights.values())
    return {k: v / total * scale for k, v in weights.items()}
# ---
def _run_heartbeat_loop(self, stop_event: threading.Event) -> None:
        """Heartbeat loop running on its own thread so slow RPCs don't block scheduling."""
        while not stop_event.is_set():
            self._heartbeat_event.wait(timeout=self._config.scheduler_interval_seconds)
            self._heartbeat_event.clear()
            if stop_event.is_set():
                break
            self._heartbeat_all_workers()
# ---
def to_state_dict(self, prefix: str | None = None) -> StateDict:
        # this method needs to "devectorize" the blocks, so that we have a list of blocks h.0.FOO, h.1.FOO, etc.
        # first just do the normal thing with our own dict, which we'll post-process
        state_dict: StateDict = super().to_state_dict(prefix)

        return _unstack_state_dict(state_dict, prefix)
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if self._prefix and dir in self._fileset:
            return "all"
        if not self._prefix:
            return True
        return (
            dir in self._fileset
            or dir in self._dirs
            or any(parentdir in self._fileset for parentdir in util.finddirs(dir))
        )
# ---
def cauchy(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.cauchy(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def release(self, lease: Lease[Any]) -> None:
        with httpx.Client() as client:
            client.post(
                f"http://{self.host}:{self.port}/queues/{self.queue_name}/release",
                json={"lease_id": lease.lease_id, "timestamp": lease.timestamp},
            )
# ---
def test_edit_experiment_view_wrong_course(self):
        """ Tests edit_experiment when attempting to access a experiment from a different course """
        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(experiment.id,)))
        self.assertError(response, UNAUTHORIZED_ACCESS)
# ---
def init(weight):
            return ScanModule(weight=weight)
# ---
def _is_object_store_path(path: str) -> bool:
    parsed = urlparse(path)
    return parsed.scheme in {"gs", "s3"}
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        return 1 / hax.axis_size(Out)
# ---
def do(src):
            # match positions where slot_ids == src; take first
            eq = (slot_ids == src).array
            idx = jnp.nonzero(eq, size=1, fill_value=INVALID)[0][0]
            return idx
# ---
def __init__(self):
            super().__init__()
            self.a = jnp.zeros(1)
            self.b = self.a
# ---
def do_fold(init: CarryT, *args, **kwargs) -> CarryT:
            carry = init
            for block in self.blocks:
                carry = fn(block, carry, *args, **kwargs)
                carry = tree_checkpoint_name(carry, self._carry_ckpt_name)
            return carry
# ---
def minute_to_hm(minute):
    if isinstance(minute, int):
        return "%d:%02d" % (divmod(minute, 60))
    return None
# ---
def _from_db_object(cls, obj, db_obj):
        """Converts a physical function to a formal object.

        :param obj: An object of the class.
        :param db_obj: A DB model of the object
        :return: The object of the class with the database entity added
        """
        obj = Deployable._from_db_object(obj, db_obj)
        if cls is PhysicalFunction:
            obj.virtual_function_list = []
        return obj
# ---
def testFunctionDefLocal(self):
    self.assertEqual((0, 'baz\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          def bar():
            print 'baz'
          bar()
        foo()""")))
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)
        start, end = _parse_periods(pattern)
        self.interval = DateInterval.from_periods(start, end)
# ---
def _slice_has_active_workers(self, slice_obj: VmGroupProtocol, vm_status_map: VmWorkerStatusMap) -> bool:
        """Check if any worker in a slice has running tasks (lookup by VM address)."""
        for vm in slice_obj.vms():
            vm_address = vm.info.address
            if not vm_address:
                continue
            status = vm_status_map.get(vm_address)
            if status is not None and not status.is_idle:
                return True
        return False
# ---
def build_env_string(env_dict: dict[str, str]) -> str:
    """Build properly escaped environment variable string for shell."""
    if not env_dict:
        return ""

    env_parts = []
    for key, value in env_dict.items():
        # Escape both key and value for shell safety
        escaped_key = shlex.quote(key)
        escaped_value = shlex.quote(value)
        env_parts.append(f"{escaped_key}={escaped_value}")

    return " ".join(env_parts)
# ---
def __missing__(self, key):
        """Implements the default rule handling."""

        # If the default rule isn't actually defined, do something
        # reasonably intelligent
        if not self.default_rule or self.default_rule not in self:
            raise KeyError(key)

        return self[self.default_rule]
# ---
def add_volume(self, volume):
        ''' add a volume or volume mount to the proper location '''
        exist_volumes = self.get_volumes()
        if not volume:
            return

        if not exist_volumes:
            self.put(DeploymentConfig.volumes_path, [volume])
        else:
            exist_volumes.append(volume)
# ---
def remove_redundant_parens(text: str) -> str:
    """Remove redundant parentheses around simple numbers and expressions."""
    # Handle mixed numbers first: 11(2)/(3) -> 11+2/3
    text = re.sub(r"(\d+)\((\d+)\)/\((\d+)\)", r"\1+\2/\3", text)

    # Remove parens around simple fractions: (5)/(3) -> 5/3
    text = re.sub(r"\((\d+)\)/\((\d+)\)", r"\1/\2", text)

    # Remove parens around standalone numbers
    text = re.sub(r"(?<![a-zA-Z()])\((\d+(?:\.\d+)?)\)(?![a-zA-Z()])", r"\1", text)

    return text
# ---
def get_rank_points(self, nick):
        return self.data[nick.lower()]['rank_points']
# ---
def get_actor_name_from_actor_info(self, actor_info: SliceInfo) -> str:
        return str(actor_info.slice_name)
# ---
def addgen(id, genre) :
    db = cherrypy.session['database']

    sql = "UPDATE Radio SET genre='%s' WHERE id = %s" % (genre, id)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def source_label(self) -> str:
        return f"{self.benchmark}:{self.date_range}"
# ---
def wait_until_or_raise(
        self,
        condition: Callable[[], bool],
        timeout: Duration,
        error_message: str,
    ) -> None:
        if not self.wait_until(condition, timeout):
            raise TimeoutError(error_message)
# ---
def __call__(self, target, cred):
        """Check the policy."""

        return True
# ---
def join(self, timeout: Duration | None = None) -> None:
        self._thread.join(timeout=timeout.to_seconds() if timeout is not None else None)
# ---
def rec_set(tree):
        if has_inference(tree):
            tree = replace_fn(tree)

        if jax.tree_util.tree_leaves(tree) == [tree]:
            return tree

        return jax.tree_util.tree_map(rec_set, tree, is_leaf=lambda x: has_inference(x) and tree is not x)
# ---
def on_line(line: str) -> None:
            logger.info("[%s] %s", host, line)
# ---
def setup_teardown(context):
        """A fixture with both `setup()` and `teardown()`."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "boing"}

        return setup, teardown
# ---
def inspect_side_effect(container_id):
        inspect_call_count[0] += 1
        if inspect_call_count[0] == 1:
            return ContainerStatus(running=True)
        return ContainerStatus(running=False, exit_code=0)
# ---
def _rm_checkpoint(self, checkpoint):
        if jax.process_index() == 0:
            logger.info(f"Removing checkpoint {checkpoint}")
            self._async_checkpoint_remover_queue.put(checkpoint)
# ---
def __unicode__(self):
        return 'ManualIDVerification for {name}, status: {status}'.format(
            name=self.name,
            status=self.status,
        )
# ---
def test_parse_http_list(self):
        tests = [
            ('a,b,c', ['a', 'b', 'c']),
            ('path"o,l"og"i"cal, example', ['path"o,l"og"i"cal', 'example']),
            ('a, b, "c", "d", "e,f", g, h',
             ['a', 'b', '"c"', '"d"', '"e,f"', 'g', 'h']),
            ('a="b\\"c", d="e\\,f", g="h\\\\i"',
             ['a="b"c"', 'd="e,f"', 'g="h\\i"'])]
        for string, list in tests:
            self.assertEqual(urllib.request.parse_http_list(string), list)
# ---
def _find_acl_template(conn, acl_template):
    query = (
        sa.sql.select([acl_template_table.c.id])
        .where(acl_template_table.c.template == acl_template)
        .limit(1)
    )
    return conn.execute(query).scalar()
# ---
def testImportNative(self):
    self.assertEqual((0, '1 1000000000\n'), _GrumpRun(textwrap.dedent("""\
        from "__go__/time" import Nanosecond, Second
        print Nanosecond, Second""")))
# ---
def test_wait_for_connection_returns_true_immediately(mock_conn_avail, _mock_sleep):
    """wait_for_connection returns True if connection available immediately."""
    mock_conn_avail.return_value = True
    conn = MagicMock()
    # Test behavior: function should return True when connection is available
    result = wait_for_connection(conn, timeout=Duration.from_seconds(60), poll_interval=Duration.from_seconds(5))
    assert result is True
# ---
def test_bundle_hash(test_bundle):
    """Compute hash of test bundle."""
    h = hashlib.sha256()
    with open(test_bundle, "rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            h.update(chunk)
    return h.hexdigest()
# ---
def lr_scale(self):
        """Return the learning-rate scaling factor."""
        raise NotImplementedError
# ---
def hello():
            print("Hello from iris!")
            return 42
# ---
def dn2uuid(self, backend, dn, from_db_only=False):
        uuid = ObjectBackendRegistry.backends[backend].dn2uuid(dn)
        if uuid is None and from_db_only is True:
            # fallback to db
            if self.__index is None:
                self.__index = PluginRegistry.getInstance("ObjectIndex")
            res = self.__index.search({'dn': dn}, {'uuid': 1})
            if len(res) == 1:
                uuid = res[0]['_uuid']
        return uuid
# ---
def create_dict(self):
        ''' assign the correct properties for a secret dict '''
        self.data['apiVersion'] = 'v1'
        self.data['kind'] = 'Secret'
        self.data['metadata'] = {}
        self.data['metadata']['name'] = self.name
        self.data['metadata']['namespace'] = self.namespace
        self.data['data'] = {}
        if self.secrets:
            for key, value in self.secrets.items():
                self.data['data'][key] = value
# ---
def _slow():
    import time

    time.sleep(120)
# ---
def output_exemplar(self) -> Any:
        """
        An exemplar of what this processor returns. This is used to determine the output schema of a dataset.
        """
        raise NotImplementedError
# ---
def __truediv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.true_divide(self, other)
# ---
def __enter__(self):
        import levanter.tracker.tracker_fns as tracker_fns

        if hasattr(self, "_tracker_cm"):
            raise RuntimeError("This tracker is already set as the global tracker")
        setattr(self, "_tracker_cm", tracker_fns.current_tracker(self))
        self._tracker_cm.__enter__()
# ---
def scan_checkpoint_policy_encode(policy: ScanCheckpointPolicy):
        return policy
# ---
def test_anonymous_get_requests_redirected_to_index(self):
        self.client.logout()

        response = self.client.get(reverse('results-export'), follow=True)
        self.assertRedirects(response, '/?next=%2Fadmin%2Fresults')
# ---
def fbcode_builder_spec(builder):
    return {
        'depends_on': [fbthrift],
    }
# ---
def items(self):
		return [(k, self[k]) for k in self]
# ---
def log1p(a: A) -> A:
    return wrap_elemwise_unary(jnp.log1p, a)
# ---
def __repr__(self):
        return "<%s>" % self.__class__.__name__
# ---
def init(hidden, mlp):
            return MLP(w1=hax.ones((hidden, mlp)), w2=hax.ones((mlp, hidden)))
# ---
def test_validate_invalid_email_format(self):
        # Ensure invalid email format throws error.
        form = LoginForm(email="unknown", password="example")
        self.assertFalse(form.validate())
# ---
def _compute_beta2(self, batch_size: int) -> float:
        """Compute beta2 from batch size."""
        return max(0.95, self.beta2_base ** (batch_size / self.beta2_batch_divisor))
# ---
def with_bus(self, calculation_rate="audio", channel_count=None, release_time=0.25):
        import supriya.patterns

        return supriya.patterns.Pbus(
            self,
            calculation_rate=calculation_rate,
            channel_count=channel_count,
            release_time=release_time,
        )
# ---
def w_read():
        w_read_future.start()
# ---
def _reset_chaos():
    yield
    reset_chaos()
# ---
def transgene_insertion_2(testapp, lab, award, ctcf):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'transgene insertion',
        'purpose': 'in vivo enhancer characterization',
        'nucleic_acid_delivery_method': ['mouse pronuclear microinjection'],
        'modified_site_by_gene_id': ctcf['@id'],
        'introduced_sequence': 'ATCGTA'
    }
# ---
def __init__(
        self,
        dataset: AsyncDataset[T],
        fn: MapFunction[U],
        *extra_args,
        **extra_kwargs,
    ):
        super().__init__()
        self.dataset = dataset
        self.fn = fn
        self._extra_args = extra_args
        self._extra_kwargs = extra_kwargs
# ---
def output(self):
        output_path = '{}/catalogs/{}.csv'.format(self.root_path,
                                                  self.catalog_name)
        return luigi.s3.S3Target(path=output_path)
# ---
def __str__(self):
        return "NoncentralChiSquare(df={0},lambda={1})#{2}".format(self.df, self.lmbda, self.id())
# ---
def __init__(self, **kwargs):
        super(ApplicationGatewaySslPredefinedPolicy, self).__init__(**kwargs)
        self.name = kwargs.get('name', None)
        self.cipher_suites = kwargs.get('cipher_suites', None)
        self.min_protocol_version = kwargs.get('min_protocol_version', None)
# ---
def run_docker():
        _kill_old_container(name)
        try:
            return _run_command(*docker_cmd)
        except subprocess.CalledProcessError as e:
            logger.exception("Failed to run docker command")
            raise e
# ---
def _get_info_path(output_path: str) -> str:
    """Return the `path` of the info file associated with `output_path`."""
    return os.path.join(output_path, ".executor_info")
# ---
def unprotect(self, tag: str) -> None:
        """Decrement refcount for an image (job done with it)."""
        if tag in self._image_refcounts:
            self._image_refcounts[tag] -= 1
            if self._image_refcounts[tag] <= 0:
                del self._image_refcounts[tag]
# ---
def skip_in_ci(fn_or_msg):
    if isinstance(fn_or_msg, str):

        def decorator(fn):
            return pytest.mark.skipif("CI" in os.environ, reason=fn_or_msg)(fn)

        return decorator

    return pytest.mark.skipif("CI" in os.environ, reason="skipped in CI")(fn_or_msg)
# ---
def _setVolumeForVoiceType(self, voiceType, value):
        """Sets the volume (gain) value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM
        - value: the volume (gain) value to set.
        """

        voiceACSS = self._getACSSForVoiceType(voiceType)
        voiceACSS[acss.ACSS.GAIN] = value
        voiceACSS['established'] = True
# ---
def test_run_streaming_with_retry_raises_after_max_retries(_mock_sleep):
    """run_streaming_with_retry raises RuntimeError after max retries."""
    conn = MagicMock()
    conn.run_streaming.side_effect = OSError("Connection refused")

    with pytest.raises(RuntimeError, match="Command failed after 3 attempts"):
        run_streaming_with_retry(conn, "bootstrap script", max_retries=3)
# ---
def _discover_controller(zone: str, project: str) -> str | None:
    """Find the controller VM in the given zone."""
    vm_names = _list_controller_vms(zone, project)
    if not vm_names:
        return None
    if len(vm_names) > 1:
        click.echo(f"Warning: Multiple controller VMs found: {vm_names}", err=True)
    return vm_names[0]
# ---
def get_daily_weight(self,day_of_week):
        return self.daily_weights[day_of_week]
# ---
def perform_create(self, serializer):
        self.check_hosts(serializer)
        instance = serializer.save()
        instance.user = self.request.user
        instance.save()
        cols = self.request.query_params.get("cols", '80')
        rows = self.request.query_params.get("rows", '24')
        transaction.on_commit(lambda: run_command_execution.apply_async(
            args=(instance.id,), kwargs={"cols": cols, "rows": rows},
            task_id=str(instance.id)
        ))
# ---
def can_retry_failure(self) -> bool:
        return self.failure_count < self.max_retries_failure
# ---
def _ensure_directories(self):
        """Ensure output directory structure exists."""
        self.fs.makedirs(self.path, exist_ok=True)
# ---
def do_blank_image(height, width, filename, color="black"):
  command = "convert -size %dx%d xc:%s %s" % (width, height, color, filename)

  ret = subprocess.call(command, shell=True)

  if ret != 0:
    raise Exception("Command failed: "+ command)
# ---
def __str__(self):
        return f"{self.name}({self.size})"
# ---
def scan(self, *args, **kwargs): ...
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.shift(1)
            return Ac.sum()
# ---
def config(self) -> ToyLmConfig:
        return self._config
# ---
def prepopulate(self):
        conn = sqlite3.connect(self.dbname)
        conn.execute("CREATE TABLE unrelated (random STRING, data INTEGER)")
        conn.commit()
        del conn
# ---
def testAllSemitones(self):
        # Tests whether a spectral peak output of 12 consecutive semitones
        # yields a HPCP of all 1's
        tonic = 440
        freqs = [(tonic * 2**(x/12.)) for x in range(12)]
        mags = [1] * 12
        hpcp = HPCP()(freqs, mags)
        self.assertEqualVector(hpcp, [1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.])
# ---
def log_summary(self, metrics: dict[str, Any]):
        for tracker in self.loggers:
            tracker.log_summary(metrics)
# ---
def finish(self):
        self.writer.close()
# ---
def compute_advantages(self, rollout_group: list[Rollout]) -> np.ndarray:
        """Compute advantages for a group of rollouts."""
        ...
# ---
def test_single_slice_catches_failure():
    """Test that run_on_pod_new correctly handles a failing function after retries."""
    with pytest.raises(RayTaskError) as excinfo:
        run_on_pod(failing_fn, "v5litepod-4", num_slices=1, max_retries_failure=0, max_retries_preemption=0)

    assert "DeliberatelyRaisedException" in str(
        excinfo.value
    ), f"Expected 'Failed too many times' but got: {excinfo.value}"
# ---
def get_role_name(region, account_id, role):
    """Shortcut to insert the `account_id` and `role` into the iam string."""
    prefix = ARN_PREFIXES.get(region, "aws")
    return "arn:{0}:iam::{1}:role/{2}".format(prefix, account_id, role)
# ---
def test_domain_id(self):
        eq_(self.record.domain_id, None)
# ---
def _stop_jupyter(self):
        if self._notebook_proc:
            self._notebook_proc.terminate()
            try:
                self._notebook_proc.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self._notebook_proc.kill()
            self._notebook_proc = None
# ---
def compute(a: int, b: int) -> int:
        result = a + b
        print(f"{a} + {b} = {result}")
        return result
# ---
def service(worker):
    """Create WorkerServiceImpl."""
    return WorkerServiceImpl(provider=worker)
# ---
def test_dispatch_disabled(self, dispatcher, loop, evt):
        called = 0

        @event.event(evt.name, enable=False)
        async def corofunc():
            nonlocal called
            called += 1

        h_inst = event.HandlerInstance.from_handler(corofunc)
        dispatcher.register(h_inst)
        loop.run_until_complete(dispatcher.dispatch(evt))
        assert called == 0
# ---
def test_listdir (self):
    import os
    fs_version = list (fs.listdir (utils.TEST_ROOT))
    os_version = os.listdir (utils.TEST_ROOT)
    self.assertEquals (fs_version, os_version, "%s differs from %s" % (fs_version, os_version))
# ---
def __setattr__(self, attr, value):
		# Do not preserve the value in DBRow! Use the save method to save.
		if self.initialized and attr in self.structure:
			self._set_value(attr, value)
		return super(DBRow, self).__setattr__(attr, value)
# ---
def test_using_capsys_fixture_works_with_sys_stdout_encoding(capsys):
    test_text = 'test text'

    print(test_text.encode(sys.stdout.encoding, 'replace'))
    (out, err) = capsys.readouterr()
    assert out
    assert err == ''
# ---
def delete_element():
    """Deletes a single element with given hid"""
    element = request.get_json()
    home_services.delete_element(element['hid'])
    return 'OK'
# ---
def register_rpc_commands(iris_group: click.Group) -> None:
    """Register RPC service commands on the top-level iris group."""
    iris_group.add_command(ServiceCommands("controller", name="controller-rpc", help="Controller service RPC methods"))
    iris_group.add_command(ServiceCommands("worker", name="worker-rpc", help="Worker service RPC methods"))
    iris_group.add_command(ServiceCommands("actor", name="actor-rpc", help="Actor service RPC methods"))
# ---
def set_job_info(info: JobInfo) -> None:
    _job_info.set(info)
# ---
def init(self, run_id: Optional[str]) -> Tracker:
        raise NotImplementedError
# ---
def sign_tx(self, tx, spend_tx):
        scriptPubKey = bytearray(spend_tx.vout[0].scriptPubKey)
        if (scriptPubKey[0] == OP_TRUE):  # an anyone-can-spend
            tx.vin[0].scriptSig = CScript()
            return
        (sighash, err) = SignatureHash(spend_tx.vout[0].scriptPubKey, tx, 0, SIGHASH_ALL)
        tx.vin[0].scriptSig = CScript([self.coinbase_key.sign(sighash) + bytes(bytearray([SIGHASH_ALL]))])
# ---
def emphasis(self, text):
        """Rendering *emphasis* text.

        :param text: text content for emphasis.
        """
        return '<em>%s</em>' % text
# ---
def remove_before_section(html: BeautifulSoup):
    # We only care about information after the first section
    section = html.find("section")
    if section:
        section = section.previous_sibling
        while section:
            new_section = section.previous_sibling
            section.extract()
            section = new_section
# ---
def list_item(self, text):
        """Rendering list item snippet. Like ``<li>``."""
        return '<li>%s</li>\n' % text
# ---
def __init__(self,
                 filename=None,
                 content=None,
                 content_type='yaml',
                 separator='.',
                 backup=False):
        self.content = content
        self._separator = separator
        self.filename = filename
        self.__yaml_dict = content
        self.content_type = content_type
        self.backup = backup
        self.load(content_type=self.content_type)
        if self.__yaml_dict is None:
            self.__yaml_dict = {}
# ---
def test_is_cloneable_share_goodformat3(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://com.netapp:8080/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def test_llama_params():
    # Check that the computed number of trainable params is close to the actual number of params
    hf_config = transformers.LlamaConfig.from_pretrained("NousResearch/Llama-2-7b-hf")
    llama_config = LlamaConfig.from_hf_config(hf_config)
    actual_params = 6.738415616e9
    params = llama_config.total_trainable_params(hf_config.vocab_size)
    assert np.isclose(actual_params, params, rtol=1e-2)
# ---
def __init__(self, secret, callback, realm='Realm'):
		self.secret = secret
		self.callback = callback
		self.realm = realm
# ---
def batched(iterable: Iterable[T], batch_size: int) -> Iterator[List[T]]:
    """Yields batches of the given size from the given iterable."""
    batch = []
    for item in iterable:
        batch.append(item)
        if len(batch) == batch_size:
            yield batch
            batch = []

    if len(batch) > 0:
        yield batch
# ---
def getName(self):
        return "NoncT({0},{1})".format(self.df, self.mu)
# ---
def resize(self, size) -> "Axis":
        return Axis(self.name, size)
# ---
def heartbeat_loop():
            while not self._stop_event.wait(HEARTBEAT_INTERVAL):
                self._status_file.refresh_lock()
# ---
def spec(test_data):
    return test_data['spec']
# ---
def from_proto(proto: cluster_pb2.AttributeValue) -> "AttributeValue":
        """Convert from protobuf representation."""
        if proto.HasField("string_value"):
            return AttributeValue(proto.string_value)
        elif proto.HasField("int_value"):
            return AttributeValue(proto.int_value)
        elif proto.HasField("float_value"):
            return AttributeValue(proto.float_value)
        # Default to empty string if no value set
        return AttributeValue("")
# ---
def __repr__(self):
        if self.expr is not None:
            return f"FilterOp(expr={self.expr})"
        return f"FilterOp(predicate={_get_fn_name(self.predicate)})"
# ---
def local_config():
    """Create weight transfer config for local filesystem."""
    with tempfile.TemporaryDirectory() as temp_dir:
        config = WeightTransferConfig(
            mode=WeightTransferMode.GCS_CHECKPOINT,
            checkpoint_dir=temp_dir,
        )
        yield config
# ---
def elapsed_ms(self) -> int:
        """Get elapsed time in milliseconds."""
        return int(self.elapsed_seconds() * 1000)
# ---
def convert_batch_dict_to_output_rows(batch_dict: dict, output_column_names: list[str], batch_size: int) -> list[dict]:
    output_rows = []
    for i in range(batch_size):
        output_row = {}
        for col in output_column_names:
            if col in batch_dict:
                output_row[col] = batch_dict[col][i]
        output_rows.append(output_row)

    return output_rows
# ---
def _quick_task():
    """A simple task that returns immediately."""
    return 42
# ---
def __repr__(self) -> str:
        return f"Namespace({super().__repr__()})"
# ---
def var(cls, name: StringLike) -> "PyExpr":
        """A leaf: variable reference or literal."""
        ...
# ---
def load_cache(
        self, split, tokenizer: HfTokenizer, override_cache_dir: str | None = None, enforce_eos=True
    ) -> TreeCache[dict]:
        base_cache = override_cache_dir if override_cache_dir is not None else self.cache_dir
        if base_cache is None:
            raise ValueError("cache_dir must be set or override_cache_dir must be provided")
        return load_lm_dataset_cache(os.path.join(base_cache, split), self.format, tokenizer, enforce_eos=enforce_eos)
# ---
def logs_url(self):
        return self.admin_url + '/logs'
# ---
def to_dict(self):
        return {
            f"dedup/{self.method}/{self.level}/total": self.total,
            f"dedup/{self.method}/{self.level}/dups": self.dups,
            f"dedup/{self.method}/{self.level}/unique": self.unique,
            f"dedup/{self.method}/{self.level}/dup_clusters": self.dup_clusters,
        }
# ---
def wait(self):
        """ Waits for the deployment to complete: first, the network that will contain the cluster is deployed. Once
        the network is deployed, a firewall for the network and an instance template are deployed. Finally,
        once the instance template is deployed, an instance group manager and all its instances are deployed.
        """
        self.deployment.wait_for_completion()
# ---
def set_as_current(self):
        self.notebook.current_job = self
# ---
def test_rerank_empty_candidates():
    ranked = rerank_candidates([], ["assert True"])
    assert ranked == []
# ---

def x_or_y(n, x, y):
    """A simple program which should return the value of x if n is
    a prime number and should return the value of y otherwise.

    Examples:
    for x_or_y(7, 34, 12) == 34
    for x_or_y(15, 8, 5) == 5

    """
    if n == 1:
        return y
    for i in range(2, n):
        if n % i == 0:
            return y
            break
    else:
        return x
# ---
def chaos(key: str) -> ChaosRule | None:
    """Check if chaos should fire for this key.

    Returns the fired rule (with delay_seconds, error, etc.), or None.
    Call sites must explicitly handle delay_seconds and error.
    No side effects - use walrus operator pattern:

        if rule := chaos("worker.building_delay"):
            time.sleep(rule.delay_seconds)
    """
    rule = _rules.get(key)
    if rule is None:
        return None
    if rule.try_fire():
        return rule
    return None
# ---
def build_or_load(
        cache_dir: str,
        shard_source: ShardedDataSource[T],
        processor: BatchProcessor[T, U],
        options: Optional["CacheOptions"] = None,
    ) -> "TreeCache[U]":
        if options is None:
            options = CacheOptions.default()
        return build_or_load_cache(cache_dir, shard_source, processor, options=options)
# ---
def get_stop_tokens(model_type: str) -> list[str]:
    """Get model-specific stop tokens."""
    if model_type == "llama":
        return ["<|eot_id|>"]
    elif model_type == "qwen":
        return ["<|im_end|>"]
    else:
        raise ValueError(f"Unknown model_type: {model_type}")
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        """Block until the job completes or timeout expires."""
        try:
            self._future.result(timeout=timeout)
        except Exception:
            if raise_on_failure:
                raise
        return self.status()
# ---
def imageTag(self, imgName) :
        imgBuilder = self.images.get(imgName, None)
        if imgBuilder :
            return imgBuilder.buildTag()
        return None
# ---
def _setup_regular(self, env):
        super(Char, self)._setup_regular(env)
        assert isinstance(self.size, (NoneType, int)), \
            "Char field %s with non-integer size %r" % (self, self.size)
# ---
def get_path():
    return addon.getAddonInfo('path').decode('utf-8')
# ---
def exitChatty(self):
        self.ignore(self.chattyDoneEvent)
        self.chatty.exit()
# ---
def transfection_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'nucleic_acid_delivery_method': ['stable transfection']
    }
# ---
def swish(a: A) -> A:
    return wrap_elemwise_unary(jnn.swish, a)
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "embed_tokens"}
# ---
def authenticated_userid(self, request):
		params = _parse_authorization(request, self.secret, self.realm)
		if params is None:
			return None
		if not _is_valid_nonce(params['nonce'], self.secret):
			_add_www_authenticate(request, self.secret, self.realm)
			return None
		userid = params['username']
		if self.callback(params, request) is not None:
			return 'u:%s' % userid
		_add_www_authenticate(request, self.secret, self.realm)
# ---
def as_sync_dataset(self) -> "SyncDataset[T_co]":
        raise NotImplementedError("...")
# ---
def values(self):
		"""
		Return a list of the file's values
		"""
		return [self[id] for id in self]
# ---
def delete_router_precommit(self, context, router_context):
        pass
# ---
def observe_property(self, name, handler):
        self._property_handlers[name].append(handler)
        _mpv_observe_property(self._event_handle, hash(name)&0xffffffffffffffff, name.encode('utf-8'), MpvFormat.STRING)
# ---
def init(weight):
            return Module(weight=weight)
# ---
def _fn(x: jax.Array, labels: jax.Array, w: jax.Array):
        return _forward(x, labels, w)
# ---
def getPoints(numberOfPoints):
    pointList = list(zip(np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)))
    return pointList
# ---
def getBitmap(self, callingWindow, context, mainItem):
        return None
# ---
def __next__(self):
        if self._iterator is None:
            if not inspect.isgenerator(self._result):
                raise StopIteration
            self._iterator = iter(self._result)
        return next(self._iterator)
# ---
def _poll_submission(self) -> JobStatus:
        client = JobSubmissionClient(self._dashboard_address)
        info = client.get_job_info(self._submission_id)
        return _convert_ray_status(info.status)
# ---
def draft_force_assign(self, cr, uid, ids, *args):
        """ Confirms picking directly from draft state.
        @return: True
        """
        wf_service = netsvc.LocalService("workflow")
        for pick in self.browse(cr, uid, ids):
            if not pick.move_lines:
                raise osv.except_osv(_('Error!'),_('You cannot process picking without stock moves.'))
            wf_service.trg_validate(uid, 'stock.picking', pick.id,
                'button_confirm', cr)
        return True
# ---
def wait(self) -> None:
        """Block until all threads have exited."""
        with self._lock:
            children = list(self._children)
            threads = list(self._threads)

        for child in children:
            child.wait()
        for thread in threads:
            thread.join()
# ---
def __str__(self):
        return "NoncentralFDistr(df1={0},df2={1},lambda={2})#{3}".format(self.df1, self.df2, self.lmbda, self.id())
# ---
def discover_vm_groups(self) -> list[VmGroupProtocol]:
        """Find and adopt existing VM groups from cloud.

        Called once at startup to recover state from a previous controller.
        Returns ready-to-use VmGroup objects with their VMs already started.

        Returns:
            List of discovered VmGroup objects
        """
        ...
# ---
def discover(self) -> str | None:
        """Find existing controller address, or None."""
        ...
# ---
def generate_type(type_id, is_public):
    return {
        'id': type_id,
        'name': u'test',
        'deleted': False,
        'created_at': datetime.datetime(2012, 1, 1, 1, 1, 1, 1),
        'updated_at': None,
        'deleted_at': None,
        'is_public': bool(is_public)
    }
# ---
def is_remote_path(path: str) -> bool:
    """
    Checks if the given path is a remote path, e.g., Google Cloud Storage (GCS) path.
    """
    fs, _ = fsspec.core.url_to_fs(path)
    return not isinstance(fs, LocalFileSystem)
# ---
def hello_world():
        print("Hello from the cluster!")
        return 42
# ---
def stop(self, handle: VllmServerHandle) -> None:
        raise NotImplementedError
# ---
def in_qdq(compute_dtype, inp, scale, amax_history):
    qin, _, _ = qdq_and_return(inp, jnp.float8_e4m3fn, scale, amax_history, compute_dtype)
    return qin
# ---
def __init__(self, filename="Default.log"):
        self.terminal = sys.stdout
        self.log = open(filename, "a")
# ---
def test_accelerator_descriptor():
    from fray.v2.ray_backend.resources import accelerator_descriptor

    assert accelerator_descriptor(ResourceConfig(device=TpuConfig(variant="v4-8"))) == "v4-8"
    assert accelerator_descriptor(ResourceConfig(device=GpuConfig(variant="H100"))) == "H100"
    assert accelerator_descriptor(ResourceConfig(device=CpuConfig())) is None
# ---
def test_random_mutation_returns_none_for_invalid_source(bank):
    mutation = random_mutation("this is not python{{{", bank)
    assert mutation is None
# ---
def _put(self, item):
        if item not in self.all_items:
            Queue._put(self, item)
            self.all_items.add(item)
# ---
def test_load_vortex_column_projection(self, vortex_file):
        """Test column selection (projection)."""
        spec = InputFileSpec(path=str(vortex_file), columns=["id", "name"])
        records = list(load_vortex(spec))
        assert len(records) == 100
        assert set(records[0].keys()) == {"id", "name"}
# ---
def test_machine(self):
        image_meta = {'id': 'a', 'disk_format': 'ami'}
        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK)
# ---
def _normalize_url(url: str) -> str:
    url = url.strip()
    if url.startswith("<") and url.endswith(">"):
        url = url[1:-1]
    if "#" in url:
        url = url.split("#", 1)[0]
    if "?" in url:
        url = url.split("?", 1)[0]
    return url
# ---
def activate(self, func):
        evaldict = {'ldap3mock': self, 'func': func}
        return get_wrapped(func, _wrapper_template, evaldict)
# ---
def effective_principals(self, request):
		creds = [Everyone]
		params = _parse_authorization(request, self.secret, self.realm)
		if params is None:
			return creds
		if not _is_valid_nonce(params['nonce'], self.secret):
			_add_www_authenticate(request, self.secret, self.realm)
			return creds
		groups = self.callback(params, request)
		if groups is None:
			return creds
		creds.append(Authenticated)
		creds.append('u:%s' % params['username'])
		creds.extend(groups)
		return creds
# ---
def distinct_values(schedule: Sequence[ScheduleStep[T]] | T) -> set[T]:
    if not isinstance(schedule, Sequence) or (schedule and not isinstance(schedule[0], ScheduleStep)):
        return {schedule}  # type: ignore
    return set(step.value for step in schedule)
# ---
def _display_login_form(request, error_message=''):
    request.session.set_test_cookie()
    return render_to_response('admin/login.html', {
        'title': _('Log in'),
        'app_path': request.get_full_path(),
        'error_message': error_message
    }, context_instance=template.RequestContext(request))
# ---
def output_field(self):
        return TimeField()
# ---
def init(config: Gpt2HyenaConfig, *, key):
        # vectorize the blocks
        blocks = Stacked.init(config.Layers, Gpt2HyenaBlock, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = hnn.LayerNorm.init(config.Embed, eps=config.layer_norm_epsilon, use_bias=config.hyena.use_bias)

        return Gpt2HyenaBackbone(config, blocks, ln_f)
# ---
def __len__(self):
        return len(self._dataset._variables) - len(self._dataset._coord_names)
# ---
def key_str(self, match):
        '''
        Return the specified public key or keys based on a glob
        '''
        ret = {}
        for status, keys in six.iteritems(self.name_match(match)):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                path = os.path.join(self.opts['pki_dir'], status, key)
                with salt.utils.fopen(path, 'r') as fp_:
                    ret[status][key] = fp_.read()
        return ret
# ---
def _is_integer_like_scalar_index(value: Any) -> bool:
        if isinstance(value, (int, np.integer)):
            return True
        if not is_jax_array_like(value):
            return False
        shape = getattr(value, "shape", None)
        if shape != ():
            return False
        dtype = getattr(value, "dtype", None)
        if dtype is None:
            return False
        return jnp.issubdtype(dtype, jnp.integer)
# ---
def _get_ovn_controller(self, install_method="sandbox"):
        ovn_nbctl = self.controller_client("ovn-nbctl")
        ovn_nbctl.set_sandbox("controller-sandbox", install_method,
                              self.context['controller']['host_container'])
        ovn_nbctl.set_daemon_socket(self.context.get("daemon_socket", None))
        return ovn_nbctl
# ---
def __exit__(self, *_) -> None:
        self.shutdown()
# ---
def test_setup_only():
    """`setup_only` fixture works as expected"""

    def setup_only(context):
        """A fixture with no `teardown()`."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        return setup

    @with_fixture(setup_only)
    def case(context):
        """Check that the context has been set up."""
        assert context == {"squee": "kapow"}

    case()
# ---
def model_params(self) -> M:
        pass
# ---
def bind(self, existing, imported):
        [match1] = [m for m in self.matches if m[0] is existing]
        [match2] = [m for m in self.matches if m[1] is imported]
        assert match1[1] is None
        assert match2[0] is None
        match1[1] = match2[1]
        self.matches.remove(match2)
# ---
def test_take_overlapping_names():
    Height, Width, Depth = hax.make_axes(Height=20, Width=30, Depth=40)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    Height2 = Axis("Height", 10)
    indices_to_take = hax.arange(Height2, dtype=jnp.int32)
    named2 = hax.take(named1, Height, indices_to_take)

    assert named2.axes == (Height2, Width, Depth)
    assert named2.array.shape == (10, 30, 40)

    assert jnp.all(jnp.equal(named2.array, named1.array[:10]))
# ---
def embed(self, input_ids: NamedArray):
        x = self.token_embeddings(input_ids)
        return self.norm(x) if self.norm is not None else x
# ---
def __init__(self,*args,**kwargs):
            super(ImportFrom,self).__init__(*args,**kwargs)
            HOST_TYPE=((1,"001"),(2,"002"))  #æ›¿æ›çˆ²æ–‡ä»¶

            self.fields['host_type'].widget.choices = models.userInfo.objects.all().values_list("id","name")
            models.userInfo.objects.get()
            models.userInfo.objects.filter()
# ---
def _touch(file_path):
    with open(file_path, "a"):
        os.utime(file_path, None)
# ---
def test_empty_heterogeneous_tuples(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.z).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def flatten(self, new_axis_name: AxisSelector) -> "NamedArray":  # pragma: no cover
        return haliax.flatten(self, new_axis_name=new_axis_name)
# ---
def test_as_remote_kwargs_cpu():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=CpuConfig())
    kwargs = as_remote_kwargs(config)
    assert kwargs == {"num_cpus": 1}
# ---
def __init__(self, tag, text, *args):
        '''The metadata entry is an XML element. *args is used for
        supplying the XML element's attributes as (key, value) pairs.'''
        self.tag = tag
        self.text = text
        self.attr = args
# ---
def test_dot_errors_if_different_sized_axes():
    Height = Axis("Height", 2)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)

    H2 = Axis("Height", 4)

    m1 = hax.ones((Height, Width, Depth))
    m2 = hax.ones((Depth, Width, H2))

    with pytest.raises(ValueError):
        hax.dot(m1, m2, axis="Height")
# ---
def new_translator(domain, localedir, languages, fallback = None):
    new = gettext.translation(domain, localedir, fallback = True, languages = languages)
    if fallback is not None:
        new.add_fallback(fallback)
    return new
# ---
def run(
        self,
        cmd: list[str],
        *,
        check: bool = True,
        env: dict[str, str] | None = None,
        **kwargs,
    ) -> subprocess.CompletedProcess:
        """Run a command within the venv and wait for completion."""
        if env is None:
            env = self.get_env()
        return self._job_group.run(cmd, check=check, env=env, **kwargs)
# ---
def config(self):
        return self.transformer.config
# ---
def test_get_versions_list(self):

        response = requests.get(self.get_endpoint(
            TEMPL_ROOT_COLLECTION_ENDPOINT))

        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.json(), ["v1"])
# ---
def question_suffix(cls) -> str:
        """Use Tinker's question suffix format."""
        return TinkerMathEnvBase.question_suffix()
# ---
def set_directory(self, directory):
        self.directory = directory
# ---
def add_item(self, item):
        self.model.add(item)
        self._items.append(item)
# ---
def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func
# ---
def temporal_diff_sum(self, m, k):
        Nm = self.data[m].shape[0] - 1
        result = 0

        for s in range(k, Nm):
            result += self.lf**(s - k) * self.temporal_diff(m, s)

        return result
# ---
def get_legacy_sigopcount_tx(tx, accurate=True):
    count = 0
    for i in tx.vout:
        count += i.scriptPubKey.GetSigOpCount(accurate)
    for j in tx.vin:
        # scriptSig might be of type bytes, so convert to CScript for the moment
        count += CScript(j.scriptSig).GetSigOpCount(accurate)
    return count
# ---
def get_global_buffer() -> LogRingBuffer:
    return _global_buffer
# ---
def _extend_or_expr(self, or_expr, _or, check):
        """Extend an 'or_expr' by adding one more check."""

        return [('or_expr', or_expr.add_check(check))]
# ---
def open(cls, file, build, structure, environment):
		if isinstance(file, basestring):
			file = open(file, "rb")

		instance = cls(file, build, environment)
		instance._readHeader()
		instance.setStructure(structure)
		instance._rowDynamicFields = 0 # Dynamic fields index, used when parsing a row
		instance._readAddresses()

		return instance
# ---
def __enter__(self):
        if len(self._cmanagers) > 0:
            raise RuntimeError("Trainer is already entered")

        self._cmanagers = [
            levanter.current_tracker(self.tracker),
            haliax.partitioning.set_mesh(self.device_mesh),
            hax.axis_mapping(self.parameter_axis_mapping),
        ]

        for cmanager in self._cmanagers:
            cmanager.__enter__()

        return self
# ---
def accept(self, match, include_rejected=False):
        self._call_all('accept', match, include_rejected)
# ---
def loss_fn(x_in, w_in, y_in):
        return fused_cross_entropy_loss_and_logsumexp_penalty(
            x_in,
            y_in,
            w_in,
            reduction="mean",
            logsumexp_weight=0.0,
            block_sizes=block_sizes,
            dtype=accum_dtype,
            logit_soft_cap=None,
            implementation=implementation,
        )
# ---
def __init__(self, fn_id: str, preempt_until_n_calls: int):
        self.fn_id = fn_id
        self.preempt_until_n_calls = preempt_until_n_calls
        self.call_count = 0
# ---
def service(state, scheduler):
    controller_mock = Mock()
    controller_mock.wake = Mock()
    controller_mock.task_schedule_status = scheduler.task_schedule_status
    controller_mock.autoscaler = None  # No autoscaler by default
    return ControllerServiceImpl(state, controller_mock, bundle_prefix="file:///tmp/iris-test-bundles")
# ---
def duration(self) -> Duration | None:
        """Calculate how long the attempt ran.

        Returns:
            Duration from started_at to finished_at, or None if not finished
        """
        if self.finished_at is None:
            return None
        elapsed_ms = self.finished_at.epoch_ms() - self.started_at.epoch_ms()
        return Duration.from_ms(elapsed_ms)
# ---
def on_btn_add_fuzzing_values_clicked(self):
        if self.ui.comboBoxStrategy.currentIndex() == 0:
            self.__add_fuzzing_range()
        elif self.ui.comboBoxStrategy.currentIndex() == 1:
            self.__add_fuzzing_boundaries()
        elif self.ui.comboBoxStrategy.currentIndex() == 2:
            self.__add_random_fuzzing_values()
# ---
def pick_remove_edge(g):
        u = nx.utils.arbitrary_element(g)
        possible_nodes = list(g.neighbors(u))
        v = nx.utils.arbitrary_element(possible_nodes)
        return (u, v)
# ---
def __iter__(self):
        for i in range(self.begin, self.end, self.stride):
            yield self[i]
# ---
def _np_rng_from_jax_key(prng_key: PRNGKeyArray) -> np.random.Generator:
    # Force CPU usage to avoid jit complaining. These are not critical path in Levanter.
    with local_cpu_mesh():
        key = jax.device_put(jax.device_get(prng_key))
        return np.random.Generator(np.random.PCG64(jrandom.randint(key, (), 0, 2**30).item()))
# ---
def __init__(self, exc, handler):
        self.exc = exc
        self.hndl = handler
        self.cls = type(exc)
        self.tb = None
# ---
def starr_seq(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'episome',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transient transfection']
    }
# ---
def testFunctionDefGenerator(self):
    self.assertEqual((0, "['foo', 'bar']\n"), _GrumpRun(textwrap.dedent("""\
        def gen():
          yield 'foo'
          yield 'bar'
        print list(gen())""")))
# ---
def _find_ovf(entries):
    for entry in entries:
        if '.ovf' == os.path.splitext(entry)[1].lower():
            return entry
    return None
# ---
def testRaiseExitStatus(self):
    self.assertEqual(1, _GrumpRun('raise Exception')[0])
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        h = activations.relu(self.first(x))
        h = activations.relu(self.second(h))
        return self.third(h)
# ---
def unauthenticated_userid(self, request):
		return self.match(request).unauthenticated_userid(request)
# ---
def test_start_not_inited(self):
        profiler.clean()
        profiler.start("name")
# ---
def _inject_implicit_mixed_number(step: str):
    """
    Automatically make a mixed number evalable
    e.g. 7 3/4 => 7+3/4
    """
    p1 = re.compile("([0-9]) +([0-9])")
    step = p1.sub("\\1+\\2", step)  # implicit mults
    return step
# ---
def to_t(x, requires_grad=True):
        t = torch.from_numpy(np.array(x))
        t.requires_grad_(requires_grad)
        return t
# ---
def init(named, array, static):
            return Module(named=named, array=array, static=static)
# ---
def test_tracker_plugin_stuff_works():
    assert TrackerConfig.get_choice_class("wandb") is not None
    with pytest.raises(KeyError):
        TrackerConfig.get_choice_class("foo")
# ---
def always(self):
        return self._always
# ---
def test_edit_experiment_view_started_experiment(self):
        """ Tests edit_experiment template renders when experiment has started """
        experiment = self.create_test_experiment()
        experiment.tracks_finalized = True
        experiment.save()
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(experiment.id,)))
        self.assertTemplateUsed(response, "ab_tool/edit_experiment.html")
# ---
def validate_uom_conversion_factor(self):
		if self.uoms:
			for d in self.uoms:
				value = get_uom_conv_factor(d.uom, self.stock_uom)
				if value:
					d.conversion_factor = value
# ---
def current_len(self) -> Optional[int]:
        """
        Returns the current length of the data store. If the length is infinite or not known, returns None.
        """
        pass
# ---
def init(
        cls: Type[S],
        Block: Axis,
        module: Type[M],
        *,
        gradient_checkpointing: ScanCheckpointSpec = False,
        prevent_cse: bool = False,
    ) -> ModuleInit[S]: ...
# ---
def _loadapp(self, proxy_config_path):
        """
        Load a proxy from an app.conf to get the memcache_ring

        :returns: the memcache_ring of the memcache middleware filter
        """
        with mock.patch('swift.proxy.server.Ring'):
            app = loadapp(proxy_config_path)
        memcache_ring = None
        while True:
            memcache_ring = getattr(app, 'memcache', None)
            if memcache_ring:
                break
            app = app.app
        return memcache_ring
# ---
def shard_names(self) -> Sequence[str]:
        return self.base_ds.shard_names
# ---
def from_proto(cls, proto: "time_pb2.Timestamp") -> "Timestamp":
        """Create from proto Timestamp message."""
        return cls(proto.epoch_ms)
# ---
def __init__(self):
        self.named = hax.ones((Dim2, Dim3))
        self.unnamed1 = jnp.ones(())
        self.named2 = hax.ones(Dim3)
        self.static_field = 1
# ---
def binding_genetic_modification_2(testapp, lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'binding',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def get_layer(self, index: int) -> M:
        """Return the ``index``th layer of the folded module."""

        ...
# ---
def inference_ctx(llama3_tokenizer, dummy_server):
    return LevanterInferenceContext(
        LevanterInferenceContextConfig(
            inference_server_config=None,
            tokenizer=llama3_tokenizer,
            stop_tokens=None,
            max_tokens=100,
            mesh=None,
            axis_mapping={},
        )
    )
# ---
def _bloom_hash(x: str) -> int:
    if isinstance(x, bytes):
        return int.from_bytes(hashlib.blake2b(x, digest_size=8).digest(), "big")
    return int.from_bytes(hashlib.blake2b(x.encode(), digest_size=8).digest(), "big")
# ---
def test_task_lifecycle_phases(worker):
    """Test task transitions through PENDING -> BUILDING -> RUNNING -> SUCCEEDED."""
    request = create_run_task_request()
    task_id = worker.submit_task(request)

    task = worker.get_task(task_id)
    task.thread.join(timeout=15.0)

    final_task = worker.get_task(task_id)
    assert final_task.status == cluster_pb2.TASK_STATE_SUCCEEDED
    assert final_task.exit_code == 0
# ---
def test_write_vortex_empty(self, tmp_path):
        """Test writing empty dataset."""
        output_path = tmp_path / "empty.vortex"
        result = write_vortex_file([], str(output_path))

        assert result["count"] == 0
        assert output_path.exists()
# ---
def __init__(self, length, prng_key):
        self.length = length
        rng = _np_rng_from_jax_key(prng_key)
        self.a, self.b = self._generate_permutation_params(rng)
# ---
def resize_by_area(img, size):
  """image resize function used by quite a few image problems."""
  return tf.to_int64(
      tf.image.resize_images(img, [size, size], tf.image.ResizeMethod.AREA))
# ---
def test_roll_scalar_named_shift():
    H = Axis("H", 4)
    W = Axis("W", 3)

    arr = hax.arange((H, W))
    shift = hax.named(jnp.array(1), ())

    rolled = hax.roll(arr, shift, H)
    expected = jnp.roll(arr.array, shift.array, axis=0)

    assert rolled.axes == arr.axes
    assert jnp.all(rolled.array == expected)
# ---
def device_address(self):
        if self.device_is_address:
            return self._values['device']
# ---
def to_bool(val):
  if val is None:
    return false
  return val == 1
# ---
def xla_impl_batched(x_batched: jax.Array) -> jax.Array:
    """Default implementation (XLA / plain JAX).

    For most kernels, this should be the same as the reference implementation.
    """

    return reference_impl_batched(x_batched)
# ---
def transform_records():
        """Generator that yields transformed records."""
        for raw_row in shard_dataset:
            transformed_row = transform_row(raw_row, task.cfg, adapter)
            if transformed_row is not None:
                yield transformed_row.model_dump()
# ---
def onchange_date(self, cr, uid, ids, date, date_expected, context=None):
        """ On change of Scheduled Date gives a Move date.
        @param date_expected: Scheduled Date
        @param date: Move Date
        @return: Move Date
        """
        if not date_expected:
            date_expected = time.strftime('%Y-%m-%d %H:%M:%S')
        return {'value':{'date': date_expected}}
# ---
def reset(self) -> "ListCache[PageCacheT]":
        return ListCache(tuple(cache.reset() for cache in self.caches))
# ---
def __repr__(self):
        return formatting.vars_repr(self)
# ---
def from_command(cls, *argv: str) -> "Entrypoint":
        """Create a command-line entrypoint.

        Args:
            *argv: Command and arguments (e.g., "python", "train.py", "--epochs", "10")

        Returns:
            Entrypoint configured for command execution
        """
        if not argv:
            raise ValueError("Command must have at least one argument")
        return cls(command=list(argv))
# ---
def test_visualize_shardings_plain_array(capsys):
    x = jnp.ones((4, 4))
    visualize_shardings(x)
    out = capsys.readouterr().out
    assert out.strip() != ""
# ---
def loss_fn(model: LmHeadModel, example: LmExample, *, key=None):
        return model.compute_next_token_loss(example, key=key)
# ---
def test_logging_stream_ownership(self, testdir):
        p = testdir.makepyfile("""
            def test_logging():
                import logging
                import pytest
                stream = capture.CaptureIO()
                logging.basicConfig(stream=stream)
                stream.close() # to free memory/release resources
        """)
        result = testdir.runpytest_subprocess(p)
        assert result.stderr.str().find("atexit") == -1
# ---
def test_binary_converts_unicode(self):
        """Binary will convert unicode to bytes"""
        b = Binary("a")
        self.assertTrue(isinstance(b.value, bytes))
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"wrapped": None, "lora": None}
# ---
def test_monotonic_fails_posix(apply_failing_clock_call,
                               errno_value,
                               strerror):
    """
    A failure in C{clock_gettime} results in an L{OSError} that
    presents the failure's errno.
    """
    calls = apply_failing_clock_call('_clock_gettime', errno_value)

    with pytest.raises(OSError) as exc:
        monotonic()

    assert len(calls) == 1
    assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC

    assert str(exc.value) == strerror
# ---
def setHeight(self, height):
        self.__height = height
# ---
def axis_name(ax: AxisSelection) -> str | tuple[str, ...]:
    """
    Returns the name of the axis. If ax is a string, returns ax. If ax is an Axis, returns ax.name
    """

    def _ax_name(ax: AxisSelector) -> str:
        if isinstance(ax, Axis):
            return ax.name
        else:
            return ax

    if isinstance(ax, (Axis, str)):
        return _ax_name(ax)
    else:
        return tuple(_ax_name(x) for x in ax)
# ---
def __init__(self, want, have=None):
        self.want = want
        self.have = have
# ---
def GetTaskStatus(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def visit_msub(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            sub = self._visit(children[1])
            return BracedNode(f"{{{base}}}_{{{sub}}}")
        return TextNode("")
# ---
def discover(self) -> str | None:
        return self._controller.url if self._controller else None
# ---
def greater_equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.greater_equal](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.greater_equal.html)
    """
    return jnp.greater_equal(x1, x2)
# ---
def __call__(self, batch: dict[str, Any]):
        raise NotImplementedError
# ---
def test_job_name_rejects_invalid_inputs(value: str):
    with pytest.raises(ValueError):
        JobName.from_string(value)
# ---
def is_job_finished(state: int) -> bool:
    return state in (
        cluster_pb2.JOB_STATE_SUCCEEDED,
        cluster_pb2.JOB_STATE_FAILED,
        cluster_pb2.JOB_STATE_KILLED,
        cluster_pb2.JOB_STATE_WORKER_FAILED,
        cluster_pb2.JOB_STATE_UNSCHEDULABLE,
    )
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self._layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)
        return x
# ---
def _version(self):
        ''' return the openshift version'''
        return self.openshift_cmd(['version'], output=True, output_type='raw')
# ---
def _get_profile(self):
        if self.profile:
            return self.profile
        self.profile = self.get_profile()
        return self.profile
# ---
def sello(self):
        return self.__sello
# ---
def __init__(self, opts):
        Key.__init__(self, opts)
        self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)
        self.serial = salt.payload.Serial(self.opts)
# ---
def broadcast_axis(self, axis: AxisSpec) -> "NamedArray":  # pragma: no cover
        return haliax.broadcast_axis(self, axis=axis)
# ---
def testGradientInput1WithTranspose(self):
    self._VerifyInput1(transpose_a=True, transpose_b=False)
    self._VerifyInput1(transpose_a=False, transpose_b=True)
    self._VerifyInput1(transpose_a=True, transpose_b=True)
# ---
def status(self) -> ControllerStatus:
        """Get controller status."""
        ...
# ---
def open_link(self):
        "Open the selected item with the webbrowser"

        data = self.get_selected_item()
        url = data.get('permalink')
        if url:
            self.term.open_browser(url)
        else:
            self.term.flash()
# ---
def from_iris_client(iris_client: IrisClientLib) -> FrayIrisClient:
        """Create a FrayIrisClient by wrapping an existing IrisClient.

        This avoids creating a new connection when we already have an IrisClient
        from the context (e.g., when running inside an Iris task).
        """
        instance = cast(FrayIrisClient, object.__new__(FrayIrisClient))
        instance._iris = iris_client
        return instance
# ---
def testZeros(self):
        hpcp = HPCP()([0]*10, [0]*10)
        self.assertEqualVector(hpcp, [0.]*12)
# ---
def __repr__(self):
        return "<xormatcher matchers=%r>" % self._matchers
# ---
def sentinel(tmp_path) -> SentinelFile:
    """Per-test sentinel file for blocking/unblocking job threads."""
    return SentinelFile(str(tmp_path / "sentinel"))
# ---
def loss_fn(m, x):
        y = m.fold(x)
        return hax.sum(y).scalar()
# ---
def reset(self):
        self.base_controller.reset()
        self.sim.reset()
        self.I = 0.0
# ---
def ArticleEndpoint():
    """Eventual landing page for searching/retrieving articles"""
    if request.method == 'GET':
        return render_template("articles.html")
# ---
def render_children(
        node: RenderTreeNode,
        context: RenderContext,
    ) -> str:
        return separator.join(child.render(context) for child in node.children)
# ---
def get_method_signature(method: MethodInfo) -> str:
    """Get a human-readable signature for a method."""
    input_name = method.input_type.DESCRIPTOR.name
    output_name = method.output_type.DESCRIPTOR.name
    return f"{input_name} -> {output_name}"
# ---
def device_reference(self):
        if not self._values['managed']:
            return None
        return self._values['device_reference']
# ---
def __init__(self, registry: str):
        self._registry = registry
# ---
def _invalidate_cache(self) -> None:
        self._cached_result = None
        self._client = None
        self._client_url = None
# ---


def same_chars(s0: str, s1: str):
    """
    Check if two words have the same characters.
    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddeddabc')
    True
    >>> same_chars('abcd', 'dddddddabc')
    True
    >>> same_chars('dddddddabc', 'abcd')
    True
    >>> same_chars('eabcd', 'dddddddabc')
    False
    >>> same_chars('abcd', 'dddddddabce')
    False
    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddddabc')
    False
    """
    return set(s0) == set(s1)
# ---
def has_len(self):
        return self.data_store.is_finite()
# ---
def _receive_message(self):
        stop_message = jnp.array(_Message.STOP)
        message = broadcast_shard(stop_message, PartitionSpec())
        return message.item()
# ---
def axis_name(ax: AxisSelector) -> str:  # type: ignore
    ...
# ---
def write_xml(self):
        'Write the XML element.'
        return _make_xml_elem(self.tag, self.text, self.attr)
# ---
def __len__(self):
		return len(self._addresses)
# ---
def run_hooks(self, info: StepInfo, force: bool = False):
        self.hooks.run_hooks(info, force=force)
# ---
def is_position_token(self, token_id: int) -> bool:
        """Check if a token ID is a position token."""
        return self.position_token_offset <= token_id < self.position_token_offset + self.num_position_tokens
# ---
def resolve_axis(axis_spec: AxisSpec, axis_selection: AxisSelection) -> AxisSpec: ...
# ---
def name(self):
        return os.path.splitext(os.path.basename(self.path))[0]
# ---
def test_write_string_table(datadir):
    from openpyxl.writer.strings import write_string_table

    datadir.join("reader").chdir()
    table = ['This is cell A1 in Sheet 1', 'This is cell G5']
    content = write_string_table(table)
    with open('sharedStrings.xml') as expected:
        diff = compare_xml(content, expected.read())
        assert diff is None, diff
# ---
def urls_for_split(self, split):
        if split == "train":
            urls = self.train_urls
        elif split == "validation":
            urls = self.validation_urls
        else:
            raise ValueError(f"Unknown split {split}")

        # it's ok for there to be no urls for a split, but if there are, they need to be findable
        if len(urls) == 0:
            return []
        return urls
# ---
def __init__(self, sorts=None):
        self.sorts = sorts or []
# ---
def scenario_debug(function):
    def subfunction(self):
        run_debug(function(self))

    return subfunction
# ---
def __call__(self, x: NamedArray, *, key):
        k1, k2, k3, k4 = haliax.jax_utils.maybe_rng_split(key, 4)

        x_for_hyena = self.ln_1(x)
        hyena_output = self.hyena_operator(x_for_hyena, key=k1)
        hyena_output = self.resid_dropout(hyena_output, key=k2)
        x = x + hyena_output

        ff_output = self.mlp(self.ln_2(x), key=k3)
        ff_output = self.resid_dropout(ff_output, key=k4)
        x = x + ff_output

        return x
# ---
def test_get_profiler_not_inited(self):
        profiler.clean()
        self.assertIsNone(profiler.get())
# ---
def test_sign_flip_on_binop():
    variants = generate_expression_variants("a + b")
    assert "-(a + b)" in variants
# ---
def fake_generate_ephemeral(cls, *args):
            self.called = True
# ---
def test_selector(self):
        self.assertEqual("/~jeremy/", self.get.get_selector())
        req = Request("http://www.python.org/")
        self.assertEqual("/", req.get_selector())
# ---
def visit_msqrt(self, element):
        content = self._visit_children(element)
        return BracedNode(f"\\sqrt{{{content}}}")
# ---
def __call__(self, batch: Sequence[T]) -> Sequence[T]:
        return batch
# ---
def run(self) -> None:
        import uvicorn

        uvicorn.run(self._app, host=self._host, port=self._port)
# ---
def mock_open_for_remote(path, mode="r", **kwargs):
        if "data.commoncrawl.org" in path and "data-jsonl.paths.gz" in path:
            return paths_file.open("rb")
        return original_open(path, mode, **kwargs)
# ---
def test_float_array_padding():
    """Test padding behavior with float arrays."""
    ary = np.array([1.0, 2.0], dtype=np.float32)
    result = train_batch.trim_and_pad(ary, max_seq_len=4, pad_to=4, padding_value=999)

    expected = np.array([1.0, 2.0, 999.0, 999.0], dtype=np.float32)
    np.testing.assert_array_equal(result, expected)
# ---
def observe(self, sample):
        self.steps += 1
        self.memory.add(sample)

        # Reduces exploration rate linearly
        self.explore_rate = self.MIN_EXPLORATION_RATE + (self.MAX_EXPLORATION_RATE - self.MIN_EXPLORATION_RATE) * math.exp(-self.DECAY_RATE * self.steps)
# ---
def h_fs_rmdir (_,path): os.rmdir(path)
# ---
def check_health(self) -> bool:
        return True
# ---
def i_am_on_frame_number(number):
    bpy.context.scene.frame_set(int(number))
# ---
def first(self) -> ResolvedEndpoint:
        """Get the first endpoint.

        Returns:
            The first resolved endpoint

        Raises:
            ValueError: If no endpoints are available
        """
        if not self.endpoints:
            raise ValueError(f"No endpoints for '{self.name}'")
        return self.endpoints[0]
# ---
def to_t(arr: jnp.ndarray):
        return torch.from_numpy(np.array(arr))
# ---
def append(self, ex: T):
        return self.extend([ex])
# ---
def get_shard_dir(dir_name: str, subset_name: str | None, split: str) -> str:
    if (subset_name == "default") or (subset_name is None):
        return os.path.join(dir_name, split)

    logger.info(f"Getting shard dir for {dir_name} {subset_name} {split}")
    logger.info(f"shard dir (os.path.join(dir_name, subset_name, split)): {os.path.join(dir_name, subset_name, split)}")
    return os.path.join(dir_name, subset_name, split)
# ---
def map(fn: Callable[..., T], tree: Any, *rest: Any, is_leaf: Callable[[Any], bool] | None = None) -> Any:
    """Alias for :func:`haliax.tree_util.tree_map` matching :func:`jax.tree.map`."""

    return tree_util.tree_map(fn, tree, *rest, is_leaf=is_leaf)
# ---
def test_count_statements_nested():
    tree = ast.parse("if True:\n    x = 1\n    y = 2\n")
    # If + Assign + Assign = 3 statements.
    assert count_statements(tree) == 3
# ---
def test_routing_binary_job():
    """Binary entrypoint routes to _launch_binary_job."""
    request = JobRequest(
        name="binary-job",
        entrypoint=Entrypoint.from_binary("echo", ["hello"]),
        resources=ResourceConfig(device=CpuConfig()),
    )
    assert request.entrypoint.binary_entrypoint is not None
    assert not isinstance(request.resources.device, TpuConfig)
