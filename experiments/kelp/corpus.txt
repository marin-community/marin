def test_csv(self):
        """
        Check CSV output
        """
        f_result = open(self.dir.get_data_dir() + 'fixtures/escala.csv')

        self.assertEqual(self.escala.csv(), f_result.read())
        f_result.close()
# ---
def now(cls) -> "Timestamp":
        """Create timestamp for current time."""
        return cls(_now_ms())
# ---
def vms(self) -> list[FakeVm]:
        """Get VMs in this VM group.

        Returns FakeVm instances (not ManagedVm). The autoscaler uses this
        to find workers by worker_id, so FakeVm must have a compatible interface.
        """
        return list(self._vms)
# ---
def callInit(self):
        """
        Handle calls to `__init__` methods of extensions of the MainWindow.
        """
        for init in self.__inits:
            init(self)
# ---
def _interleave_kv(new_k, new_v):
    # [T, H, D] x2 -> [T, 2H, D] with (k0,v0,k1,v1,...) along heads
    T, H, D = new_k.shape
    return jnp.stack([new_k, new_v], axis=2).reshape(T, 2 * H, D)
# ---
def iter_repr_nested(seq):
    # convert nested iterators to lists
    if isinstance(seq, list):
        return [iter_repr_nested(item) for item in seq]
    elif isinstance(seq, Iterator):
        return IteratorWithRepr([iter_repr_nested(item) for item in seq])
    else:
        return seq
# ---
def test_to_obj_simple():
    msg = '{"aa": 1, "cc": 3, "bb": 2}'
    converted = jps.utils.to_obj(msg)
    assert converted.aa == 1
    assert converted.bb == 2
    assert converted.cc == 3
    # works only super simple case
    json1 = converted.to_json()
    assert json1 == msg
# ---
def rpc_post(client: TestClient, method: str, body: dict | None = None):
    """Helper to call RPC endpoint and return JSON response."""
    resp = client.post(
        f"/iris.cluster.ControllerService/{method}",
        json=body or {},
        headers={"Content-Type": "application/json"},
    )
    assert resp.status_code == 200, f"RPC {method} failed: {resp.text}"
    return resp.json()
# ---
def flush(self):
        pass
# ---
def EscapedFilepath( filepath ):
  return filepath.replace( ' ' , r'\ ' )
# ---
def max_queued_tokens(self) -> int:
        return self.tokens.axis_size("position")
# ---
def testPrintStatement(self):
    self.assertEqual((0, 'abc 123\nfoo bar\n'), _GrumpRun(textwrap.dedent("""\
        print 'abc',
        print '123'
        print 'foo', 'bar'""")))
# ---
def __iter__(self):
		return self._addresses.__iter__()
# ---
def __unicode__(self):
        return self.name
# ---
def _raw(self, name):
		"""
		Returns the raw value from field 'name'
		"""
		index = self.structure.index(name)
		return self[index]
# ---
def load_apartment(apartment_id):
        # type: (int) -> Optional[ApartmentDTO]
        apartment_orm = Apartment.select().where(Apartment.id == apartment_id).first()
        if apartment_orm is None:
            return None
        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
        return apartment_dto
# ---
def forward(self, x, num_future_frames, target_frames, criterion):  # Hack
        predictions = []
        loss = 0.0
        for t in range(num_future_frames):
            x = self.forward_once(x)  # Predict next frame
            predictions.append(x.unsqueeze(1))
            target = target_frames[:, t, :, :, :]
            loss += criterion(x, target)
        return loss, predictions
# ---
def __getitem__(self, idx):
        return self._calls[idx]
# ---
def do_go(self, e):
        self.remote.do_go()
# ---
def contains(self, date):
        if self.start is not None and date < self.start:
            return False
        if self.end is not None and date >= self.end:
            return False
        return True
# ---
def _require_docker_available() -> None:
    if not os.path.exists("/var/run/docker.sock"):
        raise RuntimeError(
            "Docker socket not available at /var/run/docker.sock. "
            "This job requires docker-alongside-docker (mount the socket into the Ray container)."
        )
    if shutil.which("docker") is None:
        raise RuntimeError(
            "`docker` CLI not found on PATH. Install the Docker client in the Ray image to run vLLM as a sidecar."
        )
# ---
def _docker_logs_tail(container_name: str, *, max_lines: int = 200) -> str:
    result = subprocess.run(
        ["docker", "logs", "--tail", str(max_lines), container_name],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        stderr = (result.stderr or "").strip()
        return f"<failed to read docker logs for {container_name}: {stderr}>"
    return result.stdout
# ---
import math as mt 
def get_Position(a,n,m): 
    for i in range(n): 
        a[i] = (a[i] // m + (a[i] % m != 0))  
    result,maxx = -1,-1
    for i in range(n - 1,-1,-1): 
        if (maxx < a[i]): 
            maxx = a[i] 
            result = i 
    return result + 1
# ---
def empty():
        return CacheMetadata()
# ---
def test_dothefiltering(self):
        self.assertTrue("foo.doc" not in
                        tecautils.filterImages(self.files_list,
                                               self.conf))
        self.assertTrue("yukinon.jpg" in
                        tecautils.filterImages(self.files_list,
                                               self.conf))
# ---
def build_resources(
    tpu: str | None,
    gpu: int | None,
    cpu: int | None,
    memory: str | None,
) -> ResourceSpec:
    """Build ResourceSpec from CLI arguments."""
    spec = ResourceSpec(
        cpu=cpu or 1,
        memory=memory or "2GB",
    )

    if tpu:
        spec.device = tpu_device(tpu)
    elif gpu:
        raise ValueError("GPU support not yet implemented in Iris")

    return spec
# ---
def test_no_hang_if_empty_shard_source():
    class EmptyShardSource(ShardedDataSource[list[int]]):
        @property
        def shard_names(self) -> Sequence[str]:
            return []

        def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
            raise RuntimeError("This should not be called")

    with tempfile.TemporaryDirectory() as tmpdir:
        reader = build_or_load_cache(tmpdir, EmptyShardSource(), TestProcessor())
        assert list(reader) == []
# ---
def get_ion_actor_id(process):
    """Given an ION process, return the ion-actor-id from the context, if set and present"""
    ion_actor_id = None
    if process:
        ctx = process.get_context()
        ion_actor_id = ctx.get(MSG_HEADER_ACTOR, None) if ctx else None
    return ion_actor_id
# ---
def config(self) -> QwenConfig:
        return self.transformer.config
# ---
def max_tokens(self) -> int:
        """Maximum number of tokens that can be generated for each sequence, including any prefix tokens."""
        return self.tokens.axis_size("position")
# ---
from math import tan, pi
def perimeter_polygon(s,l):
  perimeter = s*l
  return perimeter
# ---
import re
def remove_parenthesis(items):
 for item in items:
    return (re.sub(r" ?\([^)]+\)", "", item))
# ---
def causal_loss_mask(Pos: Axis, prompt_length: Optional[int] = None) -> NamedArray:
        loss_weight = hax.logical_not(hax.nn.one_hot(-1, Pos, dtype=jnp.bool_))

        if prompt_length is not None:
            # don't predict the prompt tokens
            prompt_mask = hax.arange(Pos) >= prompt_length - 1
            loss_weight = hax.logical_and(loss_weight, prompt_mask)

        return loss_weight
# ---
def train_batch(
        model: torch.nn.Module, batch: TrainData, loss_fn: Callable
    ) -> TrainBatchOutput:
        loss_per_channel = model(batch, loss_fn=partial(loss_fn, ctx=batch.ctx))
        loss = torch.mean(loss_per_channel)
        return TrainBatchOutput(loss, loss_per_channel)
# ---
def use_cpu_device():
    """Temporarily sets the default device to CPU"""
    cpu = jax.local_devices(backend="cpu")[0]
    with jax.default_device(cpu):
        yield cpu
# ---
def go_account(self, form, idx, idroot):
        form = self.get_form(name=form)
        form['indiceCompte'] = idx
        form['idRacine'] = idroot
        form.submit()
# ---
def output_loose_item(self):
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'list_item_end':
            body += self.tok()
        return self.renderer.list_item(body)
# ---
def add_target_handle(self, layer):
        input_handle = layer.register_forward_hook(get_target_input)
        self.target_handle = input_handle
# ---
def fetch_job(self):
        job_num = self.main_view.job_num
        if self.main_model.job_exists(job_num):
            self.main_view.show_job_already_exists_dialog()
            return

        self.main_model.fetch_job(job_num)
# ---
def redraw(self, _, ctx):
        ctx.set_antialias(cairo.ANTIALIAS_SUBPIXEL)
        for wid in self.widgets:
            wid.redraw(ctx)
# ---
def _init_small_lev_layer(hidden_size=128, nk=4, nv=8, dk=8, dv=8, ksz=4, key=None):
    if key is None:
        key = jax.random.PRNGKey(0)
    Embed = Axis("embed", hidden_size)
    cfg = GatedDeltaNetConfig(
        Embed=Embed, num_k_heads=nk, num_v_heads=nv, head_k_dim=dk, head_v_dim=dv, conv_kernel_size=ksz
    )
    layer = GatedDeltaNet.init(cfg, key=key)
    return cfg, layer
# ---
def constant_time_compare(val1, val2):
    if len(val1) != len(val2):
        return False
    result = 0
    for x, y in zip(val1, val2):
        result |= ord(x) ^ ord(y)
    return result == 0
# ---
def check_Odd_Parity(x): 
    parity = 0
    while (x != 0): 
        x = x & (x - 1) 
        parity += 1
    if (parity % 2 == 1): 
        return True
    else: 
        return False
# ---
def _is_scalar(v) -> bool:
    return isinstance(v, numbers.Number) or (isinstance(v, np.ndarray | jax.Array) and v.ndim == 0)
# ---
def terminate_job(self, job_id: JobName) -> None:
        self._remote_client.terminate_job(job_id)
# ---
def test_evaluate_missing_column(self):
        expr = col("name")
        assert expr.evaluate({"other": "bob"}) is None
# ---
def compute_loss(
        m,
        example: AudioTextExample,
        *,
        key=None,
        reduction: Optional[hax.ReductionFunction] = cast(Optional[hax.ReductionFunction], hax.mean),
        reduction_axis: Optional[hax.AxisSelection] = None,
    ) -> jax.numpy.ndarray | hax.NamedArray:
        return m.compute_loss(example, key=key, reduction=reduction, reduction_axis=reduction_axis)
# ---
def __enter__(self):
        self.start()
# ---
def lang(self):
        '''Language of the ebook. (mandatory)

        The language must be given as a lower-case two-letter code, optionally
        followed by a "-" and an upper-case two-letter country code.
        (e.g., "en", "en-US", "en-UK", "de", "de-DE", "de-AT")

        If this property is left unset, it defaults to "en".'''
        try:
            return self._lang
        except AttributeError:
            self.lang = 'en'
            return self._lang
# ---
def _fn(x: jax.Array, labels: jax.Array, w: jax.Array):
        return _forward(x, labels, w)
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        object_ref = self._ray_method.remote(*args, **kwargs)
        return ray.get(object_ref)
# ---
def test_dontreadfrominput_buffer_python3():
    from _pytest.capture import DontReadFromInput
    f = DontReadFromInput()
    fb = f.buffer
    assert not fb.isatty()
    pytest.raises(IOError, fb.read)
    pytest.raises(IOError, fb.readlines)
    pytest.raises(IOError, iter, fb)
    pytest.raises(ValueError, fb.fileno)
    f.close()
# ---
def signbit(x, /):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in signbit")
    return elemwise(nxp.signbit, x, dtype=nxp.bool)
# ---
def test_multiple_statement_differences():
    source = "a = 1\nb = 2\n"
    target = "a = 10\nb = 20\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 2
# ---
def __init__(self, fn):
        self.fn = fn
# ---
def __init__(self, sorts=None):
        self.sorts = sorts or []
# ---
def ha_interface_ip_address_needed(self, context, router, port,
                                       ha_settings_db, ha_group_uuid):
        if port['device_owner'] == bc.constants.DEVICE_OWNER_ROUTER_GW:
            return False
        else:
            return True
# ---
def append(self, row):
		"""
		Append a row at the end of the file.
		If the row does not have an id, one is automatically assigned.
		"""
		i = len(self) + 1 # FIXME this wont work properly in incomplete files
		if "_id" not in row:
			row["_id"] = i
		self[i] = row
# ---
def is_majority(arr, n, x):
	i = binary_search(arr, 0, n-1, x)
	if i == -1:
		return False
	if ((i + n//2) <= (n -1)) and arr[i + n//2] == x:
		return True
	else:
		return False
def binary_search(arr, low, high, x):
	if high >= low:
		mid = (low + high)//2 
		if (mid == 0 or x > arr[mid-1]) and (arr[mid] == x):
			return mid
		elif x > arr[mid]:
			return binary_search(arr, (mid + 1), high, x)
		else:
			return binary_search(arr, low, (mid -1), x)
	return -1
# ---
def test_rename_variables_deterministic_with_same_seed():
    rng1 = random.Random(123)
    rng2 = random.Random(123)
    source = "def compute(x, y):\n    result = x * y\n    return result"
    r1 = rename_variables(source, rng1)
    r2 = rename_variables(source, rng2)
    assert r1 == r2
# ---
def test_build_data_tree(self):

        dataset_code = "nipa-section1-10101-a"
        self.assertDataTree(dataset_code)
# ---
def set_setting(id, value):
    if not isinstance(value, basestring): value = str(value)
    addon.setSetting(id, value)
# ---
def test_variable_with_encryption(self):
        """
        Test variables with encryption
        """
        Variable.set('key', 'value')
        session = settings.Session()
        test_var = session.query(Variable).filter(Variable.key == 'key').one()
        self.assertTrue(test_var.is_encrypted)
        self.assertEqual(test_var.val, 'value')
# ---
def is_overdue(self):
        """
        Returns True when the due date is in the past and the task has not
        yet been completed.
        """
        return not self.is_completed() and self.days_till_due() < 0
# ---
def is_jax_or_hax_array_like(x):
    return is_jax_array_like(x) or is_named_array(x)
# ---
def test_map(backend):
    """Test map operation with all backends."""
    ds = Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x * 2)
    result = list(Backend.execute(ds, context=backend))
    assert sorted(result) == [2, 4, 6, 8, 10]
# ---
def body_finish(self):
        self.body = b"".join(self.body)
# ---
def copy_dir(self, src, dest):
        # self.mkdir(dest)
        if not src.startswith("/"):
            src = os.path.join(self.sharedir, src)
        try:
            dest = os.path.join(self.workflowdir, dest)
            shutil.copytree(src, dest)
        except OSError as exc:  # python >2.5
            if exc.errno == errno.ENOTDIR:
                shutil.copy(src, dest)
            else:
                raise
# ---
def __init__(self, values):
        self.values = values
        self.it = iter(values)
# ---
def is_compact(self) -> bool:
        """Check if the data source is compact."""
        return _is_compact(self.data, self.means, self.stds)
# ---
def test_scan_reports_eqx_module_field_path():
    Height = Axis("Height", 2)

    class Foo(eqx.Module):
        my_array: jnp.ndarray

    foo = Foo(jnp.zeros((Height.size - 1, 3)))

    def f(c, foo):
        return c, foo.my_array

    with pytest.raises(ValueError) as e:
        hax.scan(f, Height)(0, foo)

    assert "foo.my_array" in str(e.value)
# ---
def serialize(self, with_witness=False):
        r = b""
        r += super(CBlock, self).serialize()
        r += struct.pack("<BQ", 255, len(self.vtx))
        for tx in self.vtx:
            if with_witness:
                r += tx.serialize_with_witness()
            else:
                r += tx.serialize_without_witness()
        return r
# ---
def index(self):
        html = header
        (_1, _2, id) = getradio('0')
        (radio, genre, url) = getradio(id)

        if id != 0:
            html += '''<h3><a href="#" onClick="fplayradio('%s')"> ''' % id
            html += '''Play Last Radio %s <span class="glyphicon glyphicon-play"></span></a></h3>''' % radio
        html += getfooter()
        return html
# ---
def mlp(block: GrugBlockParams, x: Float[Array, "B S D"]) -> Float[Array, "B S D"]:
    gate = jnp.einsum("bsh,hm->bsm", x, block.mlp_gate)
    up = jnp.einsum("bsh,hm->bsm", x, block.mlp_up)
    activated = jax.nn.silu(gate) * up
    return jnp.einsum("bsm,mh->bsh", activated, block.mlp_down, out_sharding=Pbatch)
# ---
def __init__(self, addr, dispatcher, send_delay):
		super(Radio, self).__init__()

		self.addr = addr
		self.neighbor = None
		self.dispatcher = dispatcher
		self.q = Queue.Queue()

		self.frequency = 0
		self.bandwidth = 0

		self.send_delay = send_delay
# ---
def test_tril_triu(spec, k):
    a = xp.ones((4, 5), chunks=(2, 2), spec=spec)
    assert_array_equal(xp.tril(a, k=k), np.tril(np.ones((4, 5)), k))
    assert_array_equal(xp.triu(a, k=k), np.triu(np.ones((4, 5)), k))
# ---
def exists_env_value(self, key, value):
        ''' return whether a key, value  pair exists '''
        results = self.get_env_vars()
        if not results:
            return False

        for result in results:
            if result['name'] == key and result['value'] == value:
                return True

        return False
# ---
def output_path(self) -> Path:
        return Path(self.base_output_dir) / self.name
# ---
def extract_index_list(l1, l2, l3):
    result = []
    for m, n, o in zip(l1, l2, l3):
        if (m == n == o):
            result.append(m)
    return result
# ---
def trim_zeros(f: NamedArray | ArrayLike, trim: str = "fb") -> NamedArray:
    """Named version of [jax.numpy.trim_zeros][].

    If ``f`` is not a [haliax.NamedArray][], the trimmed coefficient axis is named
    ``degree``.
    """

    (arr,) = unwrap_namedarrays(f)
    result = jnp.trim_zeros(arr, trim=trim)
    axis = _poly_axis_from_input(f, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def secrets(self):
        '''secret property setter'''
        if self._secrets is None:
            self._secrets = self.get_secrets()
        return self._secrets
# ---
def _flat_idx_to_idx(
    flat_idx: int,
    dims: Tuple[int],
) -> Tuple[int]:
    idx = []
    for d in reversed(dims):
        idx.append(flat_idx % d)
        flat_idx = flat_idx // d

    return tuple(reversed(idx))
# ---
def stop(self):
        """Stop polling."""
        self._stop_event.set()
        if self._thread:
            self._thread.join(timeout=5.0)
        self._client.close()
# ---
def post_jit(x):
        return jax.device_get(x.addressable_data(0))
# ---
def test_count_statements_simple():
    tree = ast.parse("x = 1")
    # Module body has one Assign statement.
    assert count_statements(tree) == 1
# ---
def count_char(string,char):
 count = 0
 for i in range(len(string)):
    if(string[i] == char):
        count = count + 1
 return count
# ---
def _wrap(txt: str, width: int = 60):
            return "\n".join(fill(line, width) for line in txt.splitlines())
# ---
def log_softmax(data, axis):
    r"""Computes log softmax.

    .. math::

        \text{log_softmax}(x)_i = \log \frac{exp(x_i)}{\sum_j exp(x_j)}

    .. note::
        This operator can be optimized away for inference.

    Parameters
    ----------
    data: relay.Expr
        The input data to the operator.

    axis: int
        The axis to sum over when computing softmax
    """

    return _make.log_softmax(data, axis)
# ---
def exec_stage(self, last: int, arg: Any) -> int:
        """Execute stage function."""
        assert last == self.step, (
            f"stages are executing out of order! On step {self.step!r}."
        )

        self.stage.function(arg, config=self.config)  # type: ignore

        if self.name is not None:
            beam.metrics.metric.Metrics.counter(self.name, "completed_tasks").inc()

        return self.step
# ---
def to_column(self):
        self.column = fields.integer(self.string)
        return self.column
# ---
def test_partial_setup_failure(self, testdir):
        p = testdir.makepyfile("""
            def test_hello(capsys, missingarg):
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*test_partial_setup_failure*",
            "*1 error*",
        ])
# ---
def merge_chunks(xs):
        return nxp.concat(xs, axis=0)
# ---
def test_filter_selects_certain_items_from_a_list(self):
        def is_even(item):
            return (item % 2) == 0

        seq = [1, 2, 3, 4, 5, 6]
        even_numbers = list()

        for item in filter(is_even, seq):
            even_numbers.append(item)

        self.assertEqual([2,4,6], even_numbers)
# ---
def test_aggregate_one(self):
        val = self.db.get_aggregate_one_historic(self.dusk,
                                                 (self.serial1, self.serial2))
        assert_equals(val, 3*((self.dusk - self.dawn - 2) / 300))
# ---
def auth_fields(self):
        return self.__data['list_authkeys']
# ---
def total_noise(self, t):
    return -torch.log1p(-(1 - self.eps) * t)
# ---
def from_state_dict(self: Mod, state_dict: StateDict, prefix: str | None = None) -> Mod:
        return default_eqx_module_from_state_dict(self, state_dict, prefix)
# ---
def extract_sample_arrays(td: TrainData) -> tuple[np.ndarray, np.ndarray]:
    """Extract underlying X, y pairs from TrainData object."""
    steps = len(td)
    x_arrays = [td.get_input(s).numpy(force=True) for s in range(steps)]
    y_arrays = [td.get_label(s).numpy(force=True) for s in range(steps)]

    return np.stack(x_arrays, axis=0), np.stack(y_arrays, axis=0)
# ---
def __getattr__(self, attr):
        if attr not in ('_context', '_sock', '_connection', '_makefile_refs'):
            return getattr(self._connection, attr)
# ---
def test_shift1(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.shift(1)
            return Ac.sum()

        hpat_func = self.jit(test_impl)
        n = 11
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def p_start(self, p):
        "start : translation_unit"
        p[0] = self.todo
# ---
def __init__(self, dist: float = 1.7, freq: float = 0.3) -> None:
        """Initialize the filter.

        Parameters
        ----------
        dist : float, optional
            The maximum distance for a clash.
        freq : float, optional
            The maximum allowed frequency of clashes.

        """
        self._dist = dist
        self._freq = freq
# ---
def shutdown(self, wait: bool = True) -> None:
        self._iris.shutdown(wait=wait)
# ---
def from_read_session(read_session):
        schema_type = read_session._pb.WhichOneof("schema")
        if schema_type == "avro_schema":
            return _AvroStreamParser(read_session)
        elif schema_type == "arrow_schema":
            return _ArrowStreamParser(read_session)
        else:
            raise TypeError(
                "Unsupported schema type in read_session: {0}".format(schema_type)
            )
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"transformer": None}
# ---

def can_arrange(arr):
    """Create a function which returns the largest index of an element which
    is not greater than or equal to the element immediately preceding it. If
    no such element exists then return -1. The given array will not contain
    duplicate values.

    Examples:
    can_arrange([1,2,4,3,5]) = 3
    can_arrange([1,2,3]) = -1
    """
    ind=-1
    i=1
    while i<len(arr):
      if arr[i]<arr[i-1]:
        ind=i
      i+=1
    return ind
# ---
def time_series_granularity_type(s):
    if s not in _VALID_TIME_SERIES_GRANULARITIES:
      raise argparse.ArgumentTypeError('Invalid TimeSeriesGranularity '
                                       f'specified: "{s}".')
    return s
# ---
def update_element():
    """Updates single element with all new values received from the user application"""
    received_element = request.get_json()
    home_services.update_element(received_element)
    return 'OK'
# ---
def client():
    c = LocalClient(max_threads=4)
    yield c
    c.shutdown(wait=True)
# ---
def output_exemplar(self) -> dict:
        return dict(**self.tokenizer("hi there", return_attention_mask=self.return_attention_mask, verbose=False))
# ---
def convert_ova(ova_path, vminfo, job_id, irs):
    command = OvaCommand(ova_path, vminfo, job_id, irs)
    job = ImportVm(job_id, command)
    job.start()
    _add_job(job_id, job)
    return response.success()
# ---
def test_apply_gufunc_elemwise_01_non_cubed_input(spec):
    def add(x, y):
        return x + y

    a = cubed.from_array(np.array([1, 2, 3]), chunks=3, spec=spec)
    b = np.array([1, 2, 3])
    z = apply_gufunc(add, "(),()->()", a, b, output_dtypes=a.dtype)
    assert_equal(z, np.array([2, 4, 6]))
# ---
def validate(self, value, model_instance):
		invalid_values = []
		for val in value:
			try:
				validate_slug(val)
			except ValidationError:
				invalid_values.append(val)

		if invalid_values:
			# should really make a custom message.
			raise ValidationError(self.error_messages['invalid_choice'] % invalid_values)
# ---
def _stop_profiler_if_needed(self):
        """Ensure profiler is stopped if it was started."""
        if self._profiler_started:
            logger.info("Stopping profiler (end of evaluation).")
            jax.profiler.stop_trace()
            self._profiler_started = False
            self._log_profiler_artifact()
# ---
def num_gpus(self) -> int:
        return self.bt.num_gpus
# ---
def wrap(out_key):
                out_coords = out_key[1:]
                offset_in_key = ((offsets.name,) + out_coords,)
                return key_function(out_key) + offset_in_key
# ---
def remove_worker(self, worker_id: WorkerId) -> ControllerWorker | None:
        with self._lock:
            return self._workers.pop(worker_id, None)
# ---
def test_beam_search_sorted_by_score(params, model_cfg, tokenizer):
    """Results should be sorted by score (highest first)."""
    results = beam_search(
        params=params,
        initial_programs=["x = 1 + 2\n", "y = 3\n"],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(1),
        beam_size=4,
        expansions_per_beam=2,
        max_depth=2,
    )
    scores = [c.score for c in results]
    assert scores == sorted(scores, reverse=True)
# ---
def test_binary(self):
        """Store and retrieve a binary"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "data": Binary("abc")})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["data"].value, b"abc")
# ---
def job_id(self) -> JobName:
        """Unique job identifier."""
        return self._job_id
# ---
def __call__(self, lhs, rhs, dimension_numbers, precision=None, preferred_element_type=None, **kwargs):
            return jax.lax.dot_general(
                lhs,
                rhs,
                dimension_numbers,
                precision=jax.lax.Precision.HIGHEST,
                preferred_element_type=preferred_element_type,
                **kwargs,
            )
# ---
def hasattr(self, attr):
        if attr in self.lookup:
            return True
        return False
# ---
def dev():
    import knowledge.names as names

    mcW = mc.MarkovChain()
    nm = names.NameMaker()
    speakers = [nm.random_person() for i in range(1, 4)]
    dm = dialogue_maker([n['name'] for n in speakers], [n['pronoun'] for n in speakers], mcW)
    dlg = dm.make_dialogue(["dog", "run", "spot"])
    print(dlg)
# ---
def __init__(self):
        self.result: str | None = None
        self._done = threading.Event()
# ---
def _run_simple_tpu_job(self, client: IrisClient) -> TestResult:
        """Run a simple TPU job that just prints and returns."""
        return self._run_job_test(
            client=client,
            test_name=f"Simple TPU job ({self.config.tpu_type})",
            entrypoint=Entrypoint.from_callable(_hello_tpu_job),
            job_name=f"smoke-simple-{self._run_id}",
            resources=ResourceSpec(device=tpu_device(self.config.tpu_type)),
        )
# ---
def from_wire(cls, s: str) -> "JobName":
        """Parse from wire format. Alias for from_string."""
        return cls.from_string(s)
# ---
def to_wire(self) -> str:
        """Serialize to wire format for RPC/env vars."""
        return str(self)
# ---
def decodeSuccess(self, seg):
        self.status.current_bytes += seg.size
        self.segments_finished.append(seg.msgid)
        if ( (len(self.segments_finished)+len(self.segments_aborted)) >= len(self.segment_list) ):
            self.all_decoded = True
# ---
def device_is_name(self):
        if not self.device_is_address and not self.device_is_id:
            return True
        return False
# ---
def register(
        self,
        name: str,
        address: str,
        metadata: dict[str, str] | None = None,
    ) -> str:
        """Register an endpoint for actor discovery.

        Args:
            name: Actor name for discovery
            address: Address where actor is listening (host:port)
            metadata: Optional metadata for the endpoint

        Returns:
            Unique endpoint ID for later unregistration
        """
        ...
# ---
def _make_not_expr(self, _not, check):
        """Invert the result of another check."""

        return [('check', NotCheck(check))]
# ---
def test_cluster_list_jobs(cluster):
    request = JobRequest(
        name="list-test-job",
        entrypoint=Entrypoint.from_callable(lambda: None),
        environment=EnvironmentConfig.create(),
    )

    job_id = cluster.launch(request)
    jobs = cluster.list_jobs()
    assert isinstance(jobs, list)
    assert len(jobs) >= 1
    assert any(job.job_id == job_id for job in jobs)
# ---
def hello():
            print("Hello from iris!")
            return 42
# ---
def _str_to_int(x: str) -> int:
    x = x.replace(",", "")
    x_float = float(x)
    return int(x_float)
# ---
def _run_test_case(self, test_case):
        correct_output = self._solved_output.format(
            test_case.expected1,
            test_case.expected2
        )
        test_output = self.get_puzzle_solution(test_case.input)
        if correct_output == test_output:
            print("Test passed for input '{0}'".format(test_case.input))
        else:
            print("Test failed for input '{0}'".format(test_case.input))
            print(test_output)
# ---
def matchfn(self, f):
        match = self._matcher
        return match(f) or any(map(match, util.dirs((f,))))
# ---
def __init__(self, cluster: LocalClusterClient | RemoteClusterClient, namespace: Namespace = Namespace("")):
        """Initialize IrisClient with a cluster client.

        Prefer using factory methods (local(), remote()) over direct construction.

        Args:
            cluster: Low-level cluster client (LocalClusterClient or RemoteClusterClient)
        """
        self._cluster_client = cluster
        self._namespace = namespace
# ---
def __init__(self, subtype, msg=None):
        super(PhylotyperError, self).__init__(
            subtype, msg="Unrecognized subtype {}".format(subtype))
# ---
def deduplicate_shard(items):
        seen = set()
        for item in items:
            key = item["id"]
            if key not in seen:
                seen.add(key)
                yield item
# ---
def __eq__(self, other):
        return self is other or self.value == other or self.value == int(other)
# ---
def axis_name(ax: AxisSelector) -> str:  # type: ignore
    ...
# ---
def context_click(self, on_element):
        """Performs a context-click (right click) on an element.
        Args:
            on_element: The element to context-click.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.CLICK, {'button': 2}))
        return self
# ---
def egcd(a, b):
    if a == 0:
        return (0, 1)
    else:
        y, x = egcd(b % a, a)
        return (x - (b // a) * y, y)
# ---
def combine(*args, **kwargs):
    import warnings

    warnings.warn("combine is deprecated, use eqx.combine instead", DeprecationWarning)
    return eqx.combine(*args, **kwargs)
# ---
def add_service(
    name: str,
    user_profile: UserProfile,
    base_url: Optional[str] = None,
    interface: Optional[int] = None,
    token: Optional[str] = None,
) -> None:
    Service.objects.create(
        name=name, user_profile=user_profile, base_url=base_url, interface=interface, token=token
    )
# ---
def submit_job(ctx, entrypoint, working_dir, runtime_env):
    """Submit a Ray job."""
    runtime_env_dict = json.loads(runtime_env) if runtime_env else None

    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        job_id = _submit_job(entrypoint, working_dir, runtime_env_dict)
        print(f"Job submitted with ID: {job_id}")
# ---
def buy(self, owner, data,
            size, price=None, plimit=None,
            exectype=None, valid=None):

        order = BuyOrder(owner=owner, data=data,
                         size=size, price=price, pricelimit=plimit,
                         exectype=exectype, valid=valid)

        return self.submit(order)
# ---
def _process_loop(self):
        """Worker thread processes tokens from queue."""
        while True:
            token = self._queue.get()
            try:
                self._handle_token(token)
            except Exception as e:
                print(f"[{self.actor_id}] Error processing token: {e}")
# ---
def diagnostics(self, handle: VllmServerHandle, *, max_lines: int = 200) -> dict[str, str]:
        raise NotImplementedError
# ---
def CurrentLineContents():
  return ToUnicode( vim.current.line )
# ---
def __init__(self, cluster: LocalClusterClient | RemoteClusterClient, namespace: Namespace | None = None):
        self._cluster = cluster
        self._namespace = namespace
# ---
def reconnect_p2p(self):
        """Tear down and bootstrap the P2P connection to the node.

        The node gets disconnected several times in this test. This helper
        method reconnects the p2p and restarts the network thread."""
        self.nodes[0].disconnect_p2ps()
        self.bootstrap_p2p()
# ---
def BufferModified( buffer_object ):
  return bool( int( GetBufferOption( buffer_object, 'mod' ) ) )
# ---
def delete_single_tpu(node: dict[str, Any]) -> str | None:
        node_name = node.get("name", "").split("/")[-1]
        try:
            delete_tpu_node(node_name, project, zone, quiet=True)
            logger.info(f"Terminated TPU node: {node_name}")
            return node_name
        except Exception as e:
            logger.error(f"Failed to terminate TPU node {node_name}: {e}")
            return None
# ---
def __init__(self, cap_value=10.0, **kwargs):
        """
        :param cap_value: float: value at which to clip activation
        :param kwargs: passed to torch.nn.LeadyReLU
        """
        super().__init__()
        self.gelu = torch.nn.GELU(**kwargs)
        self.cap = torch.nn.Buffer(torch.tensor(cap_value, dtype=torch.float32))
# ---
def _validate(record: dict) -> None:
        """Validate Dolma record has required fields."""
        assert "id" in record, "Dolma record missing 'id' field"
        assert "text" in record, "Dolma record missing 'text' field"
        assert "source" in record, "Dolma record missing 'source' field"
        # metadata and created are optional but commonly used
        if "metadata" in record:
            assert isinstance(record["metadata"], dict), "metadata must be a dict"
# ---
def crispr_deletion_1(testapp, lab, award, target):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'deletion',
        'purpose': 'repression',
        'method': 'CRISPR',
        'modified_site_by_target_id': target['@id'],
        'guide_rna_sequences': ['ACCGGAGA']
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def train_cfg(model_cfg):
    return EditTrainingConfig(
        model=model_cfg,
        max_seq_len=MAX_SEQ_LEN,
        total_steps=3,
        batch_size=2,
        warmup_steps=1,
        log_interval=1,
    )
# ---
def __call__(self, target, cred):
        """Triggers if instance of the class is called.

        Performs the check. Returns False to reject the access or a
        true value (not necessary True) to accept the access.
        """

        pass
# ---
def _make_unique_job_id(client, run_id):
    job_id = run_id
    try:
        while client.get_job_status(job_id) is not None:
            job_id = f"{run_id}-{time.time_ns()}"
    except Exception as e:  # noqa
        if "does not exist" in str(e):
            pass
        else:
            raise
    return job_id
# ---
def add(self, sample):
        self.examplers.append(sample)
# ---
def logs(ctx, job_id):
    """Stream logs from a job."""
    cluster = ctx.obj["cluster"]
    try:
        job_info = cluster.monitor(job_id)
        click.echo(f"Job status: {job_info.status}")
    except KeyboardInterrupt:
        click.echo("\nInterrupted.")
# ---
def reverse_Array_Upto_K(input, k): 
  return (input[k-1::-1] + input[k:])
# ---
def number(self):
        try:
            return int(self.num_ctrl.GetValue())
        except ValueError:
            return 0
# ---
def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('AB*', regex=True)
            return B.sum()
# ---
def even_bit_toggle_number(n) : 
    res = 0; count = 0; temp = n 
    while (temp > 0) :     
        if (count % 2 == 1) : 
            res = res | (1 << count)      
        count = count + 1
        temp >>= 1 
    return n ^ res
# ---
def allow_more(self, current_count: int) -> bool:
        """Return True if we should store another sample for the benchmark."""
        if not self.should_log():
            return False
        if self.log_all:
            return True
        assert self.max_samples_per_benchmark is not None
        return current_count < self.max_samples_per_benchmark
# ---
def test_is_informational(self):
        self.assertFalse(status.is_informational(99))
        self.assertFalse(status.is_informational(200))

        for i in range(100, 199):
            self.assertTrue(status.is_informational(i))
# ---
def forward(self, dist):
        shape = dist.shape
        dist = dist.view(-1, 1) - self.offset.view(1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2)).reshape(
            *shape, self.num_gaussians
        )
# ---
def fake_clear_pool(id):
            fake_clear_pool.called = True
# ---
def __init__(self, fn: Callable[[StepInfo[S]], Any]):
        self.fn = fn
# ---
def helpButtonClicked(self, widget):
        """Signal handler for the "clicked" signal for the helpButton
           GtkButton widget. The user has clicked the Help button.

        Arguments:
        - widget: the component that generated the signal.
        """

        orca.helpForOrca(page="preferences")
# ---
def revert_dispatch(self) -> None:
        """Revert dispatch if no tasks actually started."""
        self.state = cluster_pb2.JOB_STATE_PENDING
        self.started_at = None
# ---
def clear_widgets(self):
        """Release all registered widgets from the spell of auto-completion."""
        for w in set(self.widgets):
            self.remove_widget(w)
# ---
def tipodecambio(self):
        return self.__tcambio
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        examples = []
        for _ in range(n_examples):
            prompt = "i like cats, i love cats, give me moar cats."
            num_cats = int(rng.integers(1, 5))
            answer = "cats" * num_cats + " love cats" * int(num_cats > 1)
            examples.append({"prompt": prompt, "answer": answer})
        return examples
# ---
def fsspec_cpdir(dir_path: str, target_path: str) -> None:
    """
    Recursively copies all contents of dir_path to target_path.

    Args:
        dir_path (str): The path of the directory to copy.
        target_path (str): The target path.
    """

    fs = fsspec.core.get_fs_token_paths(target_path, mode="wb")[0]
    fs.put(os.path.join(dir_path, "*"), target_path, recursive=True)
# ---
def find_exponentio(test_tup1, test_tup2):
  res = tuple(ele1 ** ele2 for ele1, ele2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        """Total peak FLOP/s across all TPU chips."""
        return self.device_flops(dtype) * self.chip_count()
# ---
def _add_video(root, params):
    e = root.find('./devices/video/model/[@type]')
    if e is not None:
        params['video'] = e.get('type')
# ---
def forward(self, inputs):
        x = self.gelu(inputs)
        x = torch.clamp(x, max=self.cap)
        return x
# ---
def _eye(x, k=None, chunksize=None, block_id=None):
    i, j = block_id
    bk = (j - i) * chunksize
    if bk - chunksize <= k <= bk + chunksize:
        return nxp.eye(x.shape[0], x.shape[1], k=k - bk, dtype=x.dtype)
    else:
        return nxp.zeros_like(x)
# ---
def test_reentrancy(max_capacity):
    test_data = list(range(1, 101))
    background_iterable = BackgroundIterable(lambda: iter(test_data), max_capacity=max_capacity)

    iter1 = iter(background_iterable)
    iter2 = iter(background_iterable)

    data1 = list(iter1)
    data2 = list(iter2)

    assert data1 == data2
    assert data1 == test_data
# ---
def test_index_multiple_axes(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = a[1:, 1:]
    run_operation(tmp_path, executor, "index_multiple_axes", b)
# ---
def addReplica( self, lfn, force = False, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndicts = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.addReplica( lfndicts, force )
# ---
def _job_client(self) -> JobSubmissionClient:
        return JobSubmissionClient(self._dashboard_address)
# ---
def tokenizer_path() -> Path:
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        return Path(tokenizer.name_or_path)
    except Exception as e:  # noqa
        pytest.skip(f"Could not load tokenizer {MODEL_NAME}: {e}", allow_module_level=True)
        raise NotImplementedError("unreachable")
# ---
def __pow__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.power(self, other)
# ---
def join_protocol(file):
        if protocol:
            return f"{protocol}://{file}"
        return file
# ---
def __init__(self, target_time, *args, **kwargs):
        super(TimeSensor, self).__init__(*args, **kwargs)
        self.target_time = target_time
# ---
def test_validate_invalid_email_format(self):
        # Ensure invalid email format throws error.
        form = LoginForm(email="unknown", password="example")
        self.assertFalse(form.validate())
# ---
def __init__(self, Pos: hax.Axis, max_pack_size: int, pad_token: int):
        self.Pos = Pos
        self._ids: list[int] = []
        self._segment_ids: list[int] = []
        self._loss_weight: list[float] = []
        self.num_segments = 0
        self.pad_token = pad_token
        self.max_pack_size = max_pack_size
        assert pad_token is not None, "pad_token must be set"
# ---
import math
def get_First_Set_Bit_Pos(n):
     return math.log2(n&-n)+1
# ---
def __rmul__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.multiply(other, self)
# ---
def bad_loss(model, batch):
        return hax.sum(batch), [1, 2, 3]
# ---
def init(
        cls: Type[S],
        Block: Axis,
        module: Type[M],
        *,
        gradient_checkpointing: ScanCheckpointSpec = False,
        prevent_cse: bool = False,
    ) -> ModuleInit[S]: ...
# ---
def blacklisted(root, path, _cache={}):
    orig_path = path
    if path not in _cache:
        _cache[orig_path] = False
        while (path + os.path.sep).startswith(root):
            if os.path.exists(os.path.join(path, '.no-headers')):
                _cache[orig_path] = True
                break
            path = os.path.normpath(os.path.join(path, '..'))
    return _cache[orig_path]
# ---
def process_mask(data, mask):
    mask = mask.where(mask != 0, np.nan)
    mask = mask.transpose("lat", "lon")
    mask = mask.assign_coords(lat=data.y.values, lon=data.x.values)
    mask = mask.rename({"lat": "y", "lon": "x"})
    return mask
# ---
def test_vmap_error_for_incorrectly_specified_args():
    class Module(eqx.Module):
        # this should usually be declared static, but we're simulating a user error
        field: Axis

        def __call__(self, x):
            return x.sum(self.field)

    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)

    hax.vmap(lambda a: Module(a), Batch)(Width)
# ---
def reset(self):
        self._elapsed = 0.0
# ---
def resolve_vllm_mode(mode: Literal["native", "docker"] | None) -> Literal["native", "docker"]:
    mode_str = (mode if mode is not None else os.environ.get("MARIN_VLLM_MODE", "docker")).lower()
    if mode_str not in ("native", "docker"):
        raise ValueError(f"Unknown MARIN_VLLM_MODE={mode_str!r}; expected 'native' or 'docker'.")
    return mode_str
# ---
def test_write_desc(self):
        s = option.SqlStore()
        s.cursor = self.conn.cursor()
        s.write_desc(self.desc)
        print('READING')
        r = s.read_tree()
        print(r)
        print('print(tree\n', print_tree(r))
        print('WRITING AGAIN')
        s.write_tree(r)
        print("READING AGAIN")
        r = s.read_tree()
        print(r)
        print('print(tree2\n', print_tree(r))
# ---
def convert_lingoly_to_dolma(config: ConvertLingolyToDolmaConfig) -> None:
    """Convert Lingoly dataset to Dolma format."""
    pipeline = (
        Dataset.from_list([config.input_path])
        .flat_map(lambda p: load_zip_members(p, pattern="test.jsonl"))
        .flat_map(lambda m: process_lingoly_member(m, max_doc_length=config.max_doc_length))
        .write_jsonl(f"{config.output_path}/{{shard:05d}}.jsonl")
    )
    list(Backend.execute(pipeline))
# ---
def test_rolling2(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            df['moving average'] = df.A.rolling(window=5, center=True).mean()
            return df['moving average'].sum()

        hpat_func = self.jit(test_impl)
        n = 121
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def test_vmap_mapped_args():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Batch, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, Batch)(named1)

    expected_jax = jnp.array([named1.sum(Width).array for _ in range(Batch.size)])
    expected_names = (Batch, Depth)

    assert jnp.all(jnp.equal(selected.array, expected_jax))
    assert selected.axes == expected_names
# ---
def startup_status_names(self):
        if self._get_startup():
            return [startup_status.program_startup_status.startup_status
                    for startup_status in self.startup.startupstatus_set.all()]
# ---
def store(self, parameters):
    """
    Save the current parameters for restoring later.

    Args:
        parameters: Iterable of `torch.nn.Parameter`; the parameters to be
            temporarily stored.
    """
    self.collected_params = [param.clone() for param in parameters]
# ---
def release(self, ports: list[int]) -> None:
        with self._lock:
            for port in ports:
                self._allocated.discard(port)
# ---
def check_Even_Parity(x): 
    parity = 0
    while (x != 0): 
        x = x & (x - 1) 
        parity += 1
    if (parity % 2 == 0): 
        return True
    else: 
        return False
# ---
def test_parse_invalid_yaml_and_json_template(self):
        tmpl_str = '{test'
        msg = 'line 1, column 1'
        self._parse_template(tmpl_str, msg)
# ---
def resolved_data_root(self) -> ResolvedLocation:
        if self.data_root is None:
            raise ValueError(
                "data_root must be set, try --experiment.data_root=path/to/data"
            )
        default_root = LocalLocation(path=Path.cwd())
        return default_root.resolve(self.data_root)
# ---
def test_beam_candidate_creation():
    c = BeamCandidate(source="x = 1\n", score=-2.5, depth=1, edits=())
    assert c.source == "x = 1\n"
    assert c.score == -2.5
    assert c.depth == 1
    assert c.edits == ()
# ---
def get_user(self, email):
        try:
            return User.objects.get(email=email)
        except User.DoesNotExist:
            return None
# ---
def set_exception(self, exc=True):
        self.exception = exc
# ---
def host_count(appliance, metrics_tbl, mgmt_system_id):
    return bool(appliance.db.client.session.query(metrics_tbl).filter(
        metrics_tbl.parent_ems_id == mgmt_system_id).filter(
        metrics_tbl.resource_type == "Host").count()
    )
# ---
M = 100
def maxAverageOfPath(cost, N): 
	dp = [[0 for i in range(N + 1)] for j in range(N + 1)] 
	dp[0][0] = cost[0][0] 
	for i in range(1, N): 
		dp[i][0] = dp[i - 1][0] + cost[i][0] 
	for j in range(1, N): 
		dp[0][j] = dp[0][j - 1] + cost[0][j] 
	for i in range(1, N): 
		for j in range(1, N): 
			dp[i][j] = max(dp[i - 1][j], 
						dp[i][j - 1]) + cost[i][j] 
	return dp[N - 1][N - 1] / (2 * N - 1)
# ---
def finish_revert_migration(self, instance):
                self.finish_revert_migration_called = True
# ---
def _announce_deprecations(self, result):
        warnings = result.pop('__warnings', [])
        for warning in warnings:
            self.module.deprecate(
                msg=warning['msg'],
                version=warning['version']
            )
# ---
def test_load_env_vars_invalid_key():
    """Test error on key with = sign."""
    with pytest.raises(ValueError, match="cannot contain '='"):
        load_env_vars([["KEY=VALUE"]])
# ---
def make_tpu_worker_config(generation: str, count: int, min_workers: int = 4) -> dict:
    """Create TPU worker configuration."""
    _, config = next(iter(make_tpu_slice_config(generation, count, min_workers).items()))
    return {"tpu_worker": config}
# ---
def test_just_return_first_item_found(self):
        def is_big_name(item):
            return len(item) > 4

        names = ["Jim", "Bill", "Clarence", "Doug", "Eli"]
        name = None

        iterator = filter(is_big_name, names)
        try:
            name = next(iterator)
        except StopIteration:
            msg = 'Ran out of big names'

        self.assertEqual("Clarence", name)
# ---
def _failed_job():
    raise RuntimeError("Intentional failure for screenshot demo")
# ---
def roman_to_int(s):
        rom_val = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}
        int_val = 0
        for i in range(len(s)):
            if i > 0 and rom_val[s[i]] > rom_val[s[i - 1]]:
                int_val += rom_val[s[i]] - 2 * rom_val[s[i - 1]]
            else:
                int_val += rom_val[s[i]]
        return int_val
# ---
def start_container(self, container_id: str) -> None: ...
# ---
def get_writer(self, name: str, description: str) -> Path:
        """Register and return path for a log artifact. Creates parent dirs."""
        path = self._root / name
        path.parent.mkdir(parents=True, exist_ok=True)
        self._artifacts.append(LogArtifact(path=path, description=description))
        return path
# ---
def local_client():
    """Create a IrisClient for testing."""
    config = LocalClientConfig(max_workers=2)
    with IrisClient.local(config) as client:
        yield client
# ---
def make_depends(deps):
            return tuple(deps(recs) if callable(deps) else deps)
# ---
def _make_test_entrypoint() -> cluster_pb2.Entrypoint:
    """Create a minimal Entrypoint proto for testing."""
    entrypoint = cluster_pb2.Entrypoint()
    entrypoint.command.argv[:] = ["python", "-c", "pass"]
    return entrypoint
# ---
def test_submit_callable_failure(client: LocalClient):
    handle = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    status = handle.wait(raise_on_failure=False)
    assert status == JobStatus.FAILED
# ---
def test_is_cloneable_share_goodformat4(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://netapp.com/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def test_random_mutation_returns_none_for_empty_bank():
    bank = SubtreeBank()
    mutation = random_mutation("x = 1\n", bank)
    assert mutation is None
# ---
def named_jit(
    *,
    axis_resources: ResourceMapping | None = None,
    in_axis_resources: ResourceMapping | None = None,
    out_axis_resources: ResourceMapping | None = None,
    donate_args: PyTree | None = None,
    donate_kwargs: PyTree | None = None,
    # args from jit
    keep_unused: bool = False,
    backend: str | None = None,
    inline: bool | None = None,
) -> typing.Callable[[Callable[Args, R]], WrappedCallable[Args, R]]: ...
# ---
def __init__(self, request, policy):
		self.request = request
		self.policy = policy
# ---
def test_wait_all_mixed_failure(client: LocalClient):
    h_ok = client.submit(JobRequest(name="ok", entrypoint=Entrypoint.from_callable(_noop)))
    h_fail = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    with pytest.raises(JobFailed):
        wait_all([h_ok, h_fail], raise_on_failure=True)
# ---
def on_btn_add_fuzzing_values_clicked(self):
        if self.ui.comboBoxStrategy.currentIndex() == 0:
            self.__add_fuzzing_range()
        elif self.ui.comboBoxStrategy.currentIndex() == 1:
            self.__add_fuzzing_boundaries()
        elif self.ui.comboBoxStrategy.currentIndex() == 2:
            self.__add_random_fuzzing_values()
# ---
def test_intermingling(self):
        with self.getcapture() as cap:
            oswritebytes(1, "1")
            sys.stdout.write(str(2))
            sys.stdout.flush()
            oswritebytes(1, "3")
            oswritebytes(2, "a")
            sys.stderr.write("b")
            sys.stderr.flush()
            oswritebytes(2, "c")
            out, err = cap.readouterr()
        assert out == "123"
        assert err == "abc"
# ---
def _get_normal_vector(self):
        """
        :return: the normal vector of the surface. It determined the front side
        of the surface and it's not necessarily a unit vector
        """
        p0 = self.edge_points3d[0]
        p1 = self.edge_points3d[1]
        p3 = self.edge_points3d[3]
        v1 = p3 - p0
        v2 = p1 - p0
        normal = np.cross(v1, v2)
        norm = np.linalg.norm(normal)
        return normal / norm
# ---
def get_num_train_steps(param_count, batch_size, max_seq_len):
    """Compute the number of steps for Chinchilla optimal training (20x params tokens)."""
    total_tokens = param_count * 20
    tokens_per_step = batch_size * max_seq_len
    return total_tokens // tokens_per_step
# ---
def test_namedarray_runtime_check_with_category():
    B = Axis("batch", 1)
    arr = NamedArray(jnp.zeros((B.size,), dtype=jnp.float32), (B,))
    assert arr.matches_axes(Float[NamedArray, "batch"])  # type: ignore
    assert not arr.matches_axes(Int[NamedArray, "batch"])
# ---
def prefix(self):
        """Matcher will match the paths in .files() recursively --
        optimization might be possible."""
        return False
# ---
def compute_rbf_kernel_matrix(X):
    """Compute the RBF kernel matrix with sigma2 as the median pairwise
    distance.
    """
    sigma2 = np.median(pairwise_distances(X, metric='euclidean'))**2
    K = pairwise_kernels(X, X, metric='rbf', gamma=1.0/sigma2, n_jobs=-1)
    return K
# ---
def _batchify_ctor(ctor):
        # this is gross but it basically just vmaps the ctor over each batch dimension
        return functools.reduce(lambda ctor, batch_axis: hax.vmap(ctor, batch_axis), reversed(batch_dims), ctor)
# ---
def test_scales_up_when_demand_exceeds_capacity(self, empty_autoscaler: Autoscaler):
        """Evaluates scale-up when demand > capacity."""
        demand = [DemandEntry(device_type=DeviceType.TPU, device_variant="v5p-8", count=2)]
        decisions = empty_autoscaler.evaluate(demand)

        assert len(decisions) == 1
        assert decisions[0].action == ScalingAction.SCALE_UP
        assert decisions[0].scale_group == "test-group"
        assert "demand=2 > capacity=0" in decisions[0].reason
# ---
def make_flip(ch): 
	return '1' if (ch == '0') else '0'
def get_flip_with_starting_charcter(str, expected): 
	flip_count = 0
	for i in range(len( str)): 
		if (str[i] != expected): 
			flip_count += 1
		expected = make_flip(expected) 
	return flip_count 
def min_flip_to_make_string_alternate(str): 
	return min(get_flip_with_starting_charcter(str, '0'),get_flip_with_starting_charcter(str, '1'))
# ---
def this_output_path(name: str | None = None):
    return OutputName(name=name)
# ---
def test_is_redirect(self):
        self.assertFalse(status.is_redirect(299))
        self.assertFalse(status.is_redirect(400))

        for i in range(300, 399):
            self.assertTrue(status.is_redirect(i))
# ---
def init(config: HackableTransformerConfig, *, key):
        S = Stacked  # use BlockSeq for non-homogeneous layers
        layers = S.init(config.Layers, HackableDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config, key=shaped_rng_split(key, config.num_layers)
        )
        return HackableTransformer(config, layers, config.mk_LayerNorm(config.Embed))
# ---
def find_open_port() -> int:
    """Find an open port on localhost."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("", 0))
        return s.getsockname()[1]
# ---
def retrying_upload_folder(*args, **kwargs):
    return upload_folder(*args, **kwargs)
# ---
def get_session_list(self):
        return url_xpath('https://www.revisor.mn.gov/revisor/pages/'
                         'search_status/status_search.php?body=House',
                         '//select[@name="session"]/option/text()')
# ---
def __bool__(self) -> bool:
        return self.healthy
# ---
def do_scan(init, *extra_args, **extra_kwargs):
            carry, out = haliax.scan(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *extra_args, **extra_kwargs)
            return carry, out
# ---
def can_swap_date_fields(self, first, second): # 'day', 'month', 'year'
        return (first, second) in self._swap_possibilities or (second, first) in self._swap_possibilities
# ---
def poke(self, context):
        import airflow.hooks.hdfs_hook
        sb = airflow.hooks.hdfs_hook.HDFSHook(self.hdfs_conn_id).get_conn()
        logging.getLogger("snakebite").setLevel(logging.WARNING)
        logging.info(
            'Poking for file {self.filepath} '.format(**locals()))
        try:
            files = [f for f in sb.ls([self.filepath])]
        except:
            return False
        return True
# ---
def get_db():
    """Connects to Mongo database"""
    if not hasattr(g, 'mongo_client'):
        g.mongo_client = connect_client()
        g.mongo_db = getattr(g.mongo_client, app.config['DB_NAME'])
        g.groups_collection = g.mongo_db[os.environ.get('DB_GROUPS_COLLECTION')]
    return g.mongo_db
# ---
def insert_element(list,element):
 list = [v for elt in list for v in (element, elt)]
 return list
# ---
def python_path(self) -> str:
        """Path to the Python binary in the venv (e.g., venv/bin/python)."""
        return os.path.join(self.venv_path, "bin", "python")
# ---
def find_first_duplicate(nums):
    num_set = set()
    no_duplicate = -1

    for i in range(len(nums)):

        if nums[i] in num_set:
            return nums[i]
        else:
            num_set.add(nums[i])

    return no_duplicate
# ---
def _find_ovf(entries):
    for entry in entries:
        if '.ovf' == os.path.splitext(entry)[1].lower():
            return entry
    return None
# ---
def value_match(cls, pattern, value):
        """Determine whether the value matches the pattern. Both
        arguments are strings.
        """
        raise NotImplementedError()
# ---
def Embed(self) -> Axis:
        return Axis(name="embed_dim", size=self.d_model)
# ---
def __repr__(self):
        return "%s.%s" % (self.model_name, self.name)
# ---
def VHeadDim(self) -> Axis:
        return Axis("v_head_dim", self.head_v_dim)
# ---
def _astype(a, astype_dtype):
    return nxp.astype(a, astype_dtype)
# ---
def __create_bootstrap(self, main_root):
		bootstrap_dir = os.path.join(main_root, 'bootstrap')
		safe_mkdir(bootstrap_dir)

		bootstrap = self.__config_mgr.bootstrap()
		self.__bootstrap_mgr.create_bootstrap(bootstrap, bootstrap_dir)
# ---
def max_of_nth(test_list, N):
  res = max([sub[N] for sub in test_list])
  return (res)
# ---
def test_sniff_format_for_parquet():

    import pyarrow as pa
    import pyarrow.parquet as pq

    with tempfile.NamedTemporaryFile(suffix=".parquet") as f:
        table = pa.table({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})
        pq.write_table(table, f.name)
        f.flush()

        assert _sniff_format_for_dataset(f.name) == ".parquet"
# ---
def test_activity_regularization(self):
    layer = keras.layers.ActivityRegularization(l1=0.1)
    layer(keras.backend.variable(np.ones((2, 4))))
    self.assertEqual(1, len(layer.losses))
    config = layer.get_config()
    self.assertEqual(config.pop('l1'), 0.1)
# ---
def block_quote(self, text):
        """Rendering <blockquote> with the given text.

        :param text: text content of the blockquote.
        """
        return '<blockquote>%s\n</blockquote>\n' % text.rstrip('\n')
# ---
def test_startswith_autoescape_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab%c", autoescape=True, escape="#"), {3})
        self._test(col.startswith("ab#c", autoescape=True, escape="#"), {7})
# ---
def close(self):
        '''
        Close the file.
        '''
        self.file.close()
# ---
def _stop_on_signal(info: levanter.callbacks.StepInfo):
            if self._should_stop:
                raise StopTrainerException()
# ---
def on_connect(self):
        sizes = []
        workshops = getAvailableWorkshops()
        for w in workshops:
            tmp = [w.workshopName, w.q.qsize()]
            sizes.append(tmp)
            self.emit('sizes', tmp)
# ---
def test_startswith_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab##c", escape="#"), {7})
# ---
def test_nunique_str(self):
        def test_impl(n):
            df = pd.DataFrame({'A': ['aa', 'bb', 'aa', 'cc', 'cc']})
            return df.A.nunique()

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        # test compile again for overload related issues
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# ---
def obj_id(self):
                area_id = Regexp(CleanText('(./preceding-sibling::tr[@class="LnMnTiers"][1])//span[@class="CelMnTiersT1"]'),
                            r'\((\d+)\)', default='')(self)
                acc_id = Regexp(CleanText('./td[1]'), r'(\d+)\s*(\d+)', r'\1\2')(self)
                if area_id:
                    return '%s.%s' % (area_id, acc_id)
                return acc_id
# ---
def main() -> int:
    errors = _check_docs()
    if not errors:
        print("Docs source links: OK")
        return 0

    print("Docs source links: broken")
    for entry in errors:
        print(entry)
    return 1
# ---
def vocab_problem(self):
    raise NotImplementedError()
# ---
def output_dir(self) -> Path:
        return Path(self.base_output_dir) / f"{self.name}"
# ---
def __bool__(self, /):
        if self.ndim != 0:
            raise TypeError("bool is only allowed on arrays with 0 dimensions")
        return bool(self.compute())
# ---
def test_shard_map_decorator_no_kwargs():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    @hax.shard_map
    def fn(x):
        return x - 1

    x = hax.ones(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), hax.partitioning.set_mesh(mesh):
        out = fn(x)

    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array - 1)
# ---
def searchradio(radio, genre) :
    db = cherrypy.session['database']
    #o = 'order by radio'
    o = ''
    sql = "select id, radio, genre, url from Radio where exist > 0 and radio like '%%%s%%' and genre like '%%%s%%' and id > 0 %s" % (radio, genre, o)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
    except:
        return [(0, sql, o, genre)]

    rows = cur.fetchall()
    con.close()
    return rows
# ---
def test_and_short_circuit(self):
        expr = (col("a") > 0) & (col("b") > 0)
        # If a <= 0, b is not evaluated (short-circuit)
        assert expr.evaluate({"a": -1, "b": 1}) is False
# ---
def from_job(cls, job):
        """Create runner from RLJob."""

        _, rollout_config = job.to_worker_configs()
        return cls(rollout_config)
# ---
def test_encoder_missing(self):
        """If no encoder is found, raise ValueError"""
        from datetime import datetime

        dynamizer = Dynamizer()
        with self.assertRaises(ValueError):
            dynamizer.encode(datetime.utcnow())
# ---
def __and__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_and(self, other)
# ---
def broadcast_arrays(
    *arrays: NamedArray, require_subset: bool = True, ensure_order: bool = True
) -> tuple[NamedArray, ...]: ...
# ---
def _ensure_directories(self):
        """Ensure output directory structure exists."""
        self.fs.makedirs(self.path, exist_ok=True)
# ---
def test_endswith_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e##fg", escape="#"), {9})
# ---
def embed(self, input_ids, *args):
        input_embeds = self.token_embeddings(input_ids)
        return input_embeds
# ---
def _delete(self, resource, name=None, selector=None):
        '''call oc delete on a resource'''
        cmd = ['delete', resource]
        if selector is not None:
            cmd.append('--selector={}'.format(selector))
        elif name is not None:
            cmd.append(name)
        else:
            raise OpenShiftCLIError('Either name or selector is required when calling delete.')

        return self.openshift_cmd(cmd)
# ---
def mask_fn(param, path):
            path_str = ".".join(path) if isinstance(path, (list, tuple)) else str(path)
            if "Embedding" in path_str:
                return "adam"
            elif isinstance(param, Linear):
                return dataclasses.replace(param, weight="adamh", bias="adam" if param.bias is not None else None)
            else:
                return "adam"
# ---
def __init__(
        self,
        *,
        callable_bytes: bytes | None = None,
        command: list[str] | None = None,
    ):
        has_callable = callable_bytes is not None
        has_command = command is not None
        if has_callable == has_command:
            raise ValueError("Exactly one of 'callable_bytes' or 'command' must be set")
        self._callable_bytes = callable_bytes
        self.command = command
# ---
def prepared_registry(self, data):
        ''' setter method for prepared_registry attribute '''
        self.__prepared_registry = data
# ---
def status(self) -> JobStatus:
        if self._ref is not None:
            return self._poll_ref()
        return self._poll_submission()
# ---
def convert_to_cache(self, value, record, validate=True):
        if not validate:
            return value or False
        if value in self.get_values(record.env):
            return value
        elif not value:
            return False
        raise ValueError("Wrong value for %s: %r" % (self, value))
# ---
def has_value(self, attr):
        if attr in self.reverse_lookup:
            return True
        return False
# ---
def visit_arg(self, node: ast.arg) -> ast.arg:
        if node.arg in self.mapping:
            return ast.arg(arg=self.mapping[node.arg], annotation=node.annotation)
        return node
# ---
def delete_previous_logs_browser():
    cmd = 'rm -rf browserlogs/*'
    cr.run_command(cmd)
# ---
def upper():
        sdi = start[divergence_idx]
        return [
            path + (slice(sdi, sdi + 1),) + s
            for s in _get_minimal_slice_set(
                start[divergence_idx + 1 :],
                [d - 1 for d in dims[divergence_idx + 1 :]],
                dims[divergence_idx + 1 :],
                start_edges=start_edges[divergence_idx + 1 :],
                end_edges=[1 for _ in end_edges[divergence_idx + 1 :]],
            )
        ]
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> Optional[float]:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.hidden_dim * self.mlp_scale,
            num_layers=self.num_layers,
            num_kv_heads=self.num_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def _to_epoch_time(date):
    """Convert a `datetime` object to an integer number of seconds since
    the (local) Unix epoch.
    """
    if hasattr(date, 'timestamp'):
        # The `timestamp` method exists on Python 3.3+.
        return int(date.timestamp())
    else:
        epoch = datetime.fromtimestamp(0)
        delta = date - epoch
        return int(delta.total_seconds())
# ---
def unembed(self, x: NamedArray):
        return hax.dot(x, self.token_embeddings.weight, axis="embed_dim")
# ---
def test_len_matches_iteration(self, sampler_from_datasets):
        """len() should match actual number of batches yielded."""
        datasets = [MockDataset(10), MockDataset(10)]
        sampler = sampler_from_datasets(
            datasets=datasets,
            group_key=lambda ds: ds.grid_size,
            batch_size=3,
            shuffle=False,
            drop_last=False,
        )

        batches = list(sampler)
        assert len(sampler) == len(batches)
# ---
def __call__(self, module: M_contra, *args: P.args, **kwargs: P.kwargs) -> OutputT_co: ...
# ---
def sources(self) -> Mapping[str, AudioDatasetSourceConfig]:
        return self.configs
# ---
def divisible_by_digits(startnum, endnum):
    return [n for n in range(startnum, endnum+1) \
                if not any(map(lambda x: int(x) == 0 or n%int(x) != 0, str(n)))]
# ---
def num_gpus(self) -> int:
        if self.override_resources is not None:
            return self.override_resources.get("num_gpus", 0)
        return 0
# ---
def cal_sum(n): 
	a = 3
	b = 0
	c = 2
	if (n == 0): 
		return 3
	if (n == 1): 
		return 3
	if (n == 2): 
		return 5
	sum = 5
	while (n > 2): 
		d = a + b 
		sum = sum + d 
		a = b 
		b = c 
		c = d 
		n = n-1
	return sum
# ---
def test_put_block_bytes_large(self, storage_account_name, storage_account_key):
        self._setup(storage_account_name, storage_account_key)
        blob = self._create_blob()

        # Act
        for i in range(5):
            resp = blob.stage_block(
                'block {0}'.format(i).encode('utf-8'), urandom(LARGE_BLOCK_SIZE))
            self.assertIsNotNone(resp)
            assert 'content_md5' in resp
            assert 'content_crc64' in resp
            assert 'request_id' in resp
# ---
def output_path_analyzed(input_path):
                output_dir = (
                    design_dir / const.metrics_dirname
                    if self.output_dir is None
                    else self.output_dir
                )
                return [
                    output_dir / f"data_{input_path.stem}.npz",
                    output_dir / f"metrics_{input_path.stem}.npz",
                ]
# ---
def test_register_plugin(self, dispatcher):
        name = "some_name"

        class AClass:
            @event.event(name)
            def handler(self):
                pass

            @event.event(name)
            async def hander(self):
                pass

        obj = AClass()
        h_insts = dispatcher.register_plugin(obj)
        assert len(dispatcher.event_map) == 1
        assert len(h_insts) == 2
        for h_inst in h_insts:
            assert h_inst in dispatcher.event_map[name]
# ---
def sample_html_simple():
    """Fixture providing simple HTML without LaTeX."""
    return """<!DOCTYPE html>
<html>
<head><title>Simple Page</title></head>
<body>
<h1>Simple Title</h1>
<p>This is a simple paragraph.</p>
<ul>
<li>Item 1</li>
<li>Item 2</li>
</ul>
</body>
</html>"""
# ---
def where(condition, x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "where")
    dtype = result_type(x1, x2)
    return elemwise(nxp.where, condition, x1, x2, dtype=dtype)
# ---
def bytes_to_string( btext ):
    #btext = int('0b110100001100101011011000110110001101111', 2)
    return btext.to_bytes((btext.bit_length() + 7) // 8, 'big').decode()
# ---
def _mode_to_open_mode(mode: str):
    if mode == "r":
        return {"open_mode": ts.OpenMode(open=True)}
    elif mode == "w":
        return {"open_mode": ts.OpenMode(create=True, delete_existing=True)}
    elif mode == "a":
        return {"open_mode": ts.OpenMode(create=True, open=True, delete_existing=False)}
    else:
        raise ValueError(f"Invalid mode: {mode}")
# ---
def unique_batch_sizes(self):
        return set(seg.value for seg in self.segments)
# ---
def text(self, text):
        """Rendering unformatted text.

        :param text: text content.
        """
        if self.options.get('parse_block_html'):
            return text
        return escape(text)
# ---
def submit_job(
    state: ControllerState,
    job_id: str,
    request: cluster_pb2.Controller.LaunchJobRequest,
) -> list[ControllerTask]:
    """Submit a job via event and return tasks."""
    jid = JobName.from_string(job_id) if job_id.startswith("/") else JobName.root(job_id)
    request.name = jid.to_wire()
    state.handle_event(
        JobSubmittedEvent(
            job_id=jid,
            request=request,
            timestamp=Timestamp.now(),
        )
    )
    return state.get_job_tasks(jid)
# ---
def get_describe_filters(self):
        return {
            "Filters": [
                {"Name": "tag:Name", "Values": [self.resource.name]},
                {
                    "Name": "instance-state-name",
                    "Values": [
                        "pending",
                        "running",
                        "shutting-down",
                        " stopping",
                        "stopped",
                    ],
                },
            ]
        }
# ---
def floor(x, /):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in floor")
    if x.dtype in _integer_dtypes:
        # Note: The return dtype of floor is the same as the input
        return x
    return elemwise(nxp.floor, x, dtype=x.dtype)
# ---
def __getitem__(self, key):
        return self.docs[key]
# ---
def open(cls, file, build, structure, environment):
		if isinstance(file, basestring):
			file = open(file, "rb")

		instance = cls(file, build, environment)
		instance._readHeader()
		instance.setStructure(structure)
		instance._rowDynamicFields = 0 # Dynamic fields index, used when parsing a row
		instance._readAddresses()

		return instance
# ---
def hparams_to_dict(hparams, **extra_hparams):
    if hparams is None:
        hparams_to_save = {}
    elif dataclasses.is_dataclass(hparams):
        hparams_to_save = dataclasses.asdict(hparams)
    else:
        hparams_to_save = dict(hparams)
    if extra_hparams:
        hparams_to_save.update(extra_hparams)
    return hparams_to_save
# ---
def as_dict(self):
        dict = dataclasses.asdict(self)
        # remove Nones
        return {k: v for k, v in dict.items() if v is not None}
# ---
def test_wrapped_url(self):
        req = Request("<URL:http://www.python.org>")
        self.assertEqual("www.python.org", req.get_host())
# ---
def test_concat_single_array(spec, axis):
    a = xp.full((4, 5), 1, chunks=(3, 2), spec=spec)
    d = xp.concat([a], axis=axis)
    assert_array_equal(
        d.compute(),
        np.concatenate([np.full((4, 5), 1)], axis=axis),
    )
# ---
def __init__(self, hist: int, tensor_map: TensorMap, normalize: Normalize):
        super().__init__()
        self.hist = hist
        self.tensor_map = tensor_map
        self.normalize = normalize
        self.num_prognostic_channels = len(self.tensor_map.prognostic_var_names)
# ---
def can_perform_swap(self):
        index = self.swap_type_list.selected_index
        if index == SwapType.DayMonth:
            return self._can_swap_date_fields(DAY, MONTH)
        elif index == SwapType.MonthYear:
            return self._can_swap_date_fields(MONTH, YEAR)
        elif index == SwapType.DayYear:
            return self._can_swap_date_fields(DAY, YEAR)
        else:
            return True
# ---
def test_named_jit_with_donation():
    with axis_mapping(resource_map):

        class MyModule(eqx.Module):
            array: jnp.ndarray
            array2: jnp.ndarray

        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModule, donate_args=(True, False))(jnp.zeros((8, 8)), jnp.zeros((8, 16)))
            assert mod.array.sharding.is_fully_replicated
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        if self.server_thread:
            self.server_thread.shutdown()
        return False
# ---
def diagnostics(self, *, max_lines: int = 200) -> dict[str, str]:
        if self.vllm_server is None:
            return {}
        return self._backend.diagnostics(self.vllm_server, max_lines=max_lines)
# ---
import re 
def remove_char(S):
  result = re.sub('[\W_]+', '', S) 
  return result
# ---
def test_number_comparison_task_format_bonus():
    task = NumberComparisonTask()

    digit_reward = task.compute_reward("42", "42")
    non_digit_reward = task.compute_reward("42", "forty-two")

    assert digit_reward > non_digit_reward
    assert digit_reward == pytest.approx(1.0)
    assert non_digit_reward == pytest.approx(0.0)
# ---
def get_channel_dict(
    prefix: str, measure: str, per_channel: torch.Tensor
) -> dict[str, torch.Tensor]:
    tensor_map = TensorMap.get_instance()
    metrics = {}
    for i, channel in enumerate(tensor_map.prognostic_var_names):
        metrics[f"{prefix}/{measure}/channel/{channel}_{measure}"] = per_channel[i]
    return metrics
# ---
def without_axes(axis_spec: Sequence[AxisSelector], to_remove: AxisSelection, allow_mismatched_sizes=False) -> tuple[AxisSpec, ...]:  # type: ignore
    ...
# ---
def init(cls, Vocab: Axis, config: Gpt2Config, *, key) -> "Gpt2LMHeadModel":
        k_t, k_embeddings = jrandom.split(key, 2)
        transformer = Gpt2Transformer.init(config, key=k_t)
        embeddings = Gpt2Embeddings.init(Vocab, config, key=k_embeddings)

        return Gpt2LMHeadModel(transformer, embeddings)
# ---
def test_augment_bank_zero_augmentation_preserves_bank(bank, rng):
    augmented = augment_bank(bank, rng, n_renamed=0, n_perturbed=0, synthetic_count=0)
    assert augmented.total_entries == bank.total_entries
# ---
def _gen_image(self, index):
        image = (
            np.arange(self.Height.size * self.Width.size, dtype=np.int32).reshape(self.Height.size, self.Width.size)
            + index * 1000
        )

        return haliax.named(image, (self.Height, self.Width))
# ---
def test_resolve_resolved_location(self):
        """Test resolving a ResolvedLocation returns it unchanged."""
        base = LocalLocation(path=Path("/base/path"))
        other = LocalLocation(path=Path("/other/path"))

        resolved = base.resolve(other)

        assert resolved == other
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> RayActorHandle:
        """Create a single Ray actor and return a handle immediately."""
        group = self.create_actor_group(actor_class, *args, name=name, count=1, resources=resources, **kwargs)
        return group.wait_ready()[0]
# ---
def frame_step(self):
        self.command('frame_step')
# ---
def make_step_fn():
        return lambda config: RLJob(config).run(config.run_id)
# ---
def forward(
        self,
        z_trunk,  # Float['b n n tz'],
        token_rel_pos_feats,  # Float['b n n 3'],
    ):  # -> Float['b n n tz']:
        z = torch.cat((z_trunk, token_rel_pos_feats), dim=-1)
        z = self.dim_pairwise_init_proj(z)

        for transition in self.transitions:
            z = transition(z) + z

        return z
# ---
def __eq__(self, other: object) -> bool:
        if not isinstance(other, Timestamp):
            return NotImplemented
        return self._epoch_ms == other._epoch_ms
# ---
def model_type(self) -> type["HackableLMHeadModel"]:
        return HackableLMHeadModel
# ---
def __ror__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer or boolean", "__ror__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.bitwise_or, other, self, dtype=result_type(self, other))
# ---
def main():
    load_data('enron_mail_clean.tar.gz')
    import pdb
    pdb.set_trace()
# ---
def test_summary(bank):
    summary = bank.summary()
    assert "SubtreeBank" in summary
    assert "entries" in summary
# ---
def from_spec(spec: dict[str, list[str]]) -> Cluster:
        logger.info(f"Creating local cluster with spec: {spec}")
        use_isolated_env = spec.get("use_isolated_env", ["false"])[0].lower() == "true"
        config = LocalClusterConfig(use_isolated_env=use_isolated_env)
        logger.info(f"Local cluster config: {config}")
        return LocalCluster(config=config)
# ---
def _cumsum(seq, initial_zero=False):
    if initial_zero:
        return tuple(toolz.accumulate(add, seq, 0))
    else:
        return tuple(toolz.accumulate(add, seq))
# ---
def test_tensordot(axes):
    x = np.arange(400).reshape((20, 20))
    a = xp.asarray(x, chunks=(5, 4))
    y = np.arange(200).reshape((20, 10))
    b = xp.asarray(y, chunks=(4, 5))
    assert_array_equal(
        xp.tensordot(a, b, axes=axes).compute(), np.tensordot(x, y, axes=axes)
    )
# ---
def __init__(self, field, pattern, fast=True):
        self.field = field
        self.pattern = pattern
        self.fast = fast
# ---
def _signal_handler(signum, frame):
        raise KeyboardInterrupt
# ---
def max_sub_array_sum(a, size):
  max_so_far = 0
  max_ending_here = 0
  for i in range(0, size):
    max_ending_here = max_ending_here + a[i]
    if max_ending_here < 0:
      max_ending_here = 0
    elif (max_so_far < max_ending_here):
      max_so_far = max_ending_here
  return max_so_far
# ---
def max_similar_indices(test_list1, test_list2):
  res = [(max(x[0], y[0]), max(x[1], y[1]))
   for x, y in zip(test_list1, test_list2)]
  return (res)
# ---
def get_channel_max(self):
        """Return the maximum number of channels"""
        return self._ChannelMax
# ---
def round_and_sum(list1):
  lenght=len(list1)
  round_and_sum=sum(list(map(round,list1))* lenght)
  return round_and_sum
# ---
def tokenizer_name(self) -> str:
        """Return a string identifier for the tokenizer/chat template."""
        if hasattr(self.tokenizer, "name_or_path"):
            return self.tokenizer.name_or_path
        elif hasattr(self.tokenizer, "model_name"):
            return self.tokenizer.model_name
        else:
            return "unknown_tokenizer"
# ---
def get_bandwidth_range(self):
		return self.bandwidth_range
# ---
def _run_command(*args, **kwargs):
    return subprocess.check_call(args, **kwargs)
# ---
def test_combine_blocks_list_key_function():
    key_function = make_combine_blocks_list_key_function(
        "a", numblocks=5, split_every=2
    )

    check_key_function(key_function, (0,), "([('a', 0), ('a', 1)],)")
    check_key_function(key_function, (1,), "([('a', 2), ('a', 3)],)")
    check_key_function(key_function, (2,), "([('a', 4)],)")
# ---
def init_ui(self, main_view):
        self.main_view = main_view
        self.init_hotkeys()
# ---
def predict(self, documents: list[str]):
        raise NotImplementedError
# ---
def _log_edit_metrics(step: int, metrics: dict) -> None:
    """Log training metrics."""
    loss = float(metrics["loss"])
    acc = float(metrics["accuracy"])
    ppl = float(metrics["perplexity"])
    grad_norm = float(metrics["grad_norm"])
    num_tokens = float(metrics["num_loss_tokens"])

    logger.info(
        f"step={step:06d} loss={loss:.4f} acc={acc:.4f} "
        f"ppl={ppl:.2f} grad_norm={grad_norm:.4f} loss_tokens={num_tokens:.0f}"
    )
# ---
def on_job_fetch_update(self, progress):
        self.main_view.update_job_fetch_progress_dialog(progress)
# ---
def test_inv_sqrt_decay_schedule():
    optimizer = AdamConfig(
        learning_rate=1e-3,
        weight_decay=0.0,
        warmup=0.1,
        min_lr_ratio=0.1,
        lr_schedule="inv_sqrt",
        cycles=None,
    )

    sched_fn = optimizer.lr_scheduler(100_000)

    # Warmup phase
    assert np.isclose(sched_fn(0), 0.0)
    assert np.isclose(sched_fn(5000), 0.5e-3)

    # Decay phase: our invsqrt has a non configurable, very long period
    assert sched_fn(50000) < sched_fn(30000)
# ---
def cluster_restore_jobs(ctx, backup_dir):
    """Restore Ray jobs from specified directory."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        _restore_jobs(backup_dir)
        print(f"Jobs restored successfully from {backup_dir}")
# ---
def do_fold(init: CarryT, *args, **kwargs) -> CarryT:
            carry = init
            for block in self.blocks:
                carry = fn(block, carry, *args, **kwargs)
                carry = tree_checkpoint_name(carry, self._carry_ckpt_name)
            return carry
# ---
def status(self):
        return self._status
# ---
def __init__(self, config: HtmlToMarkdownConfig, **kwargs):
        self.include_links = config.include_links
        self.include_images = config.include_images

        kwargs = config.markdownify_kwargs
        super().__init__(**kwargs)
# ---
def test_rowproxy_is_sequence(self):
        import collections
        from sqlalchemy.engine import RowProxy

        row = RowProxy(object(), ['value'], [None], {'key'
                         : (None, None, 0), 0: (None, None, 0)})
        assert isinstance(row, collections.Sequence)
# ---
def _labels_one_hot_emulated(
    labels_adjusted: jax.Array,
    num_classes: int,
    dtype: jnp.dtype,
) -> jax.Array:
    labels_adjusted = labels_adjusted.astype(jnp.int32)
    in_block = (labels_adjusted >= 0) & (labels_adjusted < num_classes)
    safe_labels = jnp.where(in_block, labels_adjusted, -1)
    cols = jnp.arange(num_classes, dtype=labels_adjusted.dtype)[None, :]
    return (cols == safe_labels[:, None]).astype(dtype)
# ---
def find_Sum(arr,n): 
    return sum([x for x in arr if arr.count(x) > 1])
# ---
def nbytes(self) -> int:
        """Number of bytes in array"""
        return self.size * self.dtype.itemsize
# ---
def _chunk_encode(orig_iter):
        for chunk in orig_iter:
            if not len(chunk):
                continue
            chunk_len = b'%X\r\n' % len(chunk)
            yield chunk_len
            yield chunk
            yield b'\r\n'

        yield b'0\r\n\r\n'
# ---
def sentinel(tmp_path) -> SentinelFile:
    """Per-test sentinel file for blocking/unblocking job threads."""
    return SentinelFile(str(tmp_path / "sentinel"))
# ---
def can_retry_failure(self) -> bool:
        return self.failure_count < self.max_retries_failure
# ---
def _get_pitch(self):
        if self.type_ in ['NOTE_ON', 'NOTE_OFF']:
            return self._parameter1
        else:
            return None
# ---
def get_stub(self, address: str) -> WorkerClient:
        return WorkerServiceClientSync(
            address=f"http://{address}",
            timeout_ms=10000,
        )
# ---
def sum(a,b): 
    sum = 0
    for i in range (1,min(a,b)): 
        if (a % i == 0 and b % i == 0): 
            sum += i 
    return sum
# ---
def run(self):

        logger.info('starting webapp')
        logger.info('hosted at %s' % settings.WEBAPP_IP)
        logger.info('running on port %d' % settings.WEBAPP_PORT)

        app.run(settings.WEBAPP_IP, settings.WEBAPP_PORT)
# ---
def abort(self):
        self._status = STATUS.ABORTED
        logging.info('Job %r aborting...', self._id)
        self._abort()
# ---
def trunc(x, /):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in trunc")
    if x.dtype in _integer_dtypes:
        # Note: The return dtype of trunc is the same as the input
        return x
    return elemwise(nxp.trunc, x, dtype=x.dtype)
# ---
def coin_change(S, m, n): 
    table = [[0 for x in range(m)] for x in range(n+1)] 
    for i in range(m): 
        table[0][i] = 1
    for i in range(1, n+1): 
        for j in range(m): 
            x = table[i - S[j]][j] if i-S[j] >= 0 else 0
            y = table[i][j-1] if j >= 1 else 0 
            table[i][j] = x + y   
    return table[n][m-1]
# ---
def test_cannot_scale_up_at_max_slices(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """can_scale_up() returns False when at max_slices."""
        discovered = [make_mock_vm_group(f"slice-{i}") for i in range(5)]
        manager = make_mock_vm_manager(vm_groups_to_discover=discovered)
        group = ScalingGroup(scale_group_config, manager)
        group.reconcile()

        assert group.slice_count() == 5  # max_slices
        assert not group.can_scale_up()
# ---
def _bias_dropout_add(x, bias, scale, residual, prob):
    return bias_dropout_add_scale(
      x, bias, scale, residual, prob, training
    )
# ---
import re
def text_match_word(text):
        patterns = '\w+\S*$'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return 'Not matched!'
# ---
def create_futures_func(input, **kwargs):
        return [
            (
                i,
                asyncio.wrap_future(concurrent_executor.submit(function, i, **kwargs)),
            )
            for i in input
        ]
# ---
def update_form(self, Dialog):
        # empty variables
        self.edits = None
        self.combobox = None
        self.buttons = None
        self.radios = None
        self.labs = None
        self.labels = None

        # empty layout
        for i in reversed(range(self.gridLayout.count())):
            self.gridLayout.itemAt(i).widget().setParent(None)


        self.prepare_form(Dialog)
# ---
def sort(a: NamedArray, axis: AxisSelector) -> NamedArray:
    """
    Named version of [jax.numpy.sort](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.sort.html)
    """
    return wrap_axiswise_call(jnp.sort, a, axis, single_axis_only=True)
# ---
def __unicode__(self):
        return 'SSOIDVerification for {name}, status: {status}'.format(
            name=self.name,
            status=self.status,
        )
# ---
def run_remotely(self, input, func=None, config=None, name=None, compute_id=None):
        print(f"running remotely on {input} in {os.getenv('MODAL_REGION')}")
        # note we can't use the execution_stat decorator since it doesn't work with modal decorators
        result, stats = execute_with_stats(func, input, config=config)
        return result, stats
# ---
def p_translation_unit(self, p):
        """
        translation_unit : translate_task
                         | translation_unit translate_task
                         |
        """
        pass
# ---
def convert(list): 
    s = [str(i) for i in list] 
    res = int("".join(s))  
    return (res)
# ---
def Embed(self) -> Axis:
        return Axis("embed", self.hidden_dim)
# ---
def decomposed_mse_mae(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """Combined MSE and MAE loss."""
    mse = F.mse_loss(pred, target, reduction="none")
    mae = F.l1_loss(pred, target, reduction="none")
    combined = (mse + mae) / 2
    return combined.mean(dim=(0, 2, 3))
# ---
def _test_host_action(self, method, action, expected=None):
        result = method('host', action)
        if not expected:
            expected = action
        self.assertEqual(result, expected)
# ---
def test_format_shard_path_normalizes_double_slashes():
    """Test that double slashes are normalized in paths."""
    # Simulate what happens when output_path has trailing slash
    pattern = "gs://bucket/path//data-{shard:05d}-of-{total:05d}.jsonl"
    result = format_shard_path(pattern, 0, 10)
    assert result == "gs://bucket/path/data-00000-of-00010.jsonl"
    assert "//" not in result.replace("://", "")
# ---
def __init__ (self, canvas, objects):
    self.canvas = canvas
    self.objects = objects
# ---
def test_blob_to_dict(self):
        """
        Test convertion of git blobs to dictionary
        """
        valuesmap = { "foo" : "1", "bar" : "2" }
        self.commit_vars(to_add = valuesmap)

        blob = self.repo.head.commit.tree.blobs[0]
        self.assertEqual(valuesmap, blob_to_dict(blob),
            "commit was not translated correctly to dictionary")
# ---
def signal_shutdown(self) -> None:
        logger.debug("Signalling shutdown")
        self._stop_event.set()
# ---
def test_reshape_chunks_with_smaller_end_chunk(spec, executor):
    a = xp.arange(10, chunks=4, spec=spec)
    b = reshape_chunks(a, (2, 5), (2, 2))

    assert b.shape == (2, 5)
    assert b.chunks == ((2,), (2, 2, 1))

    assert_array_equal(
        b.compute(executor=executor),
        np.array([[0, 1, 4, 5, 8], [2, 3, 6, 7, 9]]),
    )
# ---
def test_pspec_for_namedarray_with_missing_array():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        named = NamedArray(None, ("dim2", "dim3"))

        spec = pspec_for(named)

        assert spec == PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)
# ---
def test_blake2_compliance():
    inputs = [
        b"",
        b"hello",
        b"The quick brown fox jumps over the lazy dog",
        b"\x00\xff" * 100,
    ]

    for data in inputs:
        rust_result_list = hash_blake2(data)
        rust_result_bytes = bytes(rust_result_list)
        assert len(rust_result_bytes) == 64

        # Parity with python
        py_result_bytes = hashlib.blake2b(data).digest()
        assert rust_result_bytes == py_result_bytes
# ---
def style(self):
        """The window style; one of the ``WINDOW_STYLE_*`` constants.
        Read-only.

        :type: int
        """
        return self._style
# ---
def _on_store_loaded(self, storecontroller):
        self.autocomp.add_words_from_units(storecontroller.get_store().get_units())

        if hasattr(self, '_cursor_changed_id'):
            self.store_cursor.disconnect(self._cursor_changed_id)
        self.store_cursor = storecontroller.cursor
        self._cursor_changed_id = self.store_cursor.connect('cursor-changed', self._on_cursor_change)
        self._on_cursor_change(self.store_cursor)
# ---
def __init__(self, docs: List[T]):
        self.docs = docs
# ---
def predict(self, df):
        """
        Returns predictions based on the model/pipeline
        """
        try:
            return self.pipeline.predict(df)
        except (ValueError, TypeError):
            print(colored('Got ValueError while using scikit model.. ', 'red'))
            return None
# ---
def __init__(self, renderer, rules=None, **kwargs):
        self.renderer = renderer
        self.links = {}
        self.footnotes = {}
        self.footnote_index = 0

        if not rules:
            rules = self.grammar_class()

        kwargs.update(self.renderer.options)
        if kwargs.get('hard_wrap'):
            rules.hard_wrap()

        self.rules = rules

        self._in_link = False
        self._in_footnote = False
        self._parse_inline_html = kwargs.get('parse_inline_html')
# ---
def list_jobs(self) -> list[cluster_pb2.JobStatus]:
        return self._remote_client.list_jobs()
# ---
def test_olmo3_sliding_window_config():
    """Test sliding window configuration."""
    config = _get_olmo3_config(sliding_window=64)
    assert config.sliding_window == 64

    hf_config = config.to_hf_config(vocab_size=50304)
    assert hf_config.sliding_window == 64
# ---
def test_page_table_max_len_per_seq():
    pt = _make_table(page_size=2, pages_per_seq=3)
    assert pt.max_len_per_seq == 6
# ---
def __init__(self, leader: _LmEvalHarnessWorker):
        super().__init__()
        self.leader = leader
        self.axis_resources = leader.axis_resources
        # Storage for prompts and generations to include in outputs
        self.sample_outputs: dict[str, list[dict]] = {}
        self.sample_logging_config = leader.sample_logging_config
        self.profiler_config = leader.profiler_config
        self._current_step = 0
        self._profiler_started = False
# ---
def dblock(idx: int, size: int) -> dslice:
    """
    Returns a dslice that selects a single block of size `size` starting at `idx`
    """
    return dslice(idx * size, size)
# ---
import re 
def check_substring(string, sample) : 
  if (sample in string): 
      y = "\A" + sample 
      x = re.search(y, string) 
      if x : 
          return ("string starts with the given substring") 
      else : 
          return ("string doesnt start with the given substring") 
  else : 
      return ("entered string isnt a substring")
# ---
def toggle_string(string):
 string1 = string.swapcase()
 return string1
# ---
def Exists(self):
    """Returns True if the log group exists."""
    describe_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'describe-log-groups',
        '--log-group-name-prefix', self.name,
        '--no-paginate'
    ]
    stdout, _, _ = vm_util.IssueCommand(describe_cmd)
    log_groups = json.loads(stdout)['logGroups']
    group = next((group for group in log_groups
                  if group['logGroupName'] == self.name), None)
    return bool(group)
# ---
def alias(self, new_name: str):
        return Axis(new_name, self.size)
# ---
def test_get_ages(self):
        self.assertEqual(self.config.get_ages(), ['15', '43'])
# ---
def VHeads(self) -> Axis:
        return Axis("v_heads", self.num_v_heads)
# ---
def handle_selection():
    selected_drinker = drinkers[lb.current()]
    urllib2.urlopen("http://192.168.11.5:8080/drinkcounter/add_drink/%d/" % (selected_drinker.id))
    appuifw.note(u"A drink has been added to " + drinkers[lb.current()].name, 'info')

    new_drinkers = get_drinker_list()
    items = get_listbox_items(new_drinkers)

    lb.set_list(items, lb.current())
# ---
def test_domain(self):
        eq_(self.record.domain, None)
# ---
def non_tracking(self, context, **kwargs):
        router = kwargs.get('router', None)
        process_id = router['id']
        self.destroy_process(process_id)
        if process_id in self.routers:
            del self.routers[process_id]
# ---
def is_sublist(l, s):
	sub_set = False
	if s == []:
		sub_set = True
	elif s == l:
		sub_set = True
	elif len(s) > len(l):
		sub_set = False
	else:
		for i in range(len(l)):
			if l[i] == s[0]:
				n = 1
				while (n < len(s)) and (l[i+n] == s[n]):
					n += 1				
				if n == len(s):
					sub_set = True
	return sub_set
# ---
def graphviz_setup(gviz_path):
    os.environ['PATH'] = gviz_path + ";" + os.environ['PATH']
# ---
def finish(self):
        """
        Finish the tracker. This is called when the tracker is no longer needed. This can, e.g.,
        force a commit of all metrics.
        """
        pass
# ---
def setup(self, stage: Optional[str] = None) -> None:  # noqa: ARG002 (unused)
        """Run the setup for the DataModule.

        Parameters
        ----------
        stage : str, optional
            The stage, one of 'fit', 'validate', 'test'.

        """
        return
# ---
def test_gpu_device(self):
        resources = ResourceConfig(device=GpuConfig(variant="H100", count=8))
        spec = convert_resources(resources)
        assert spec.device is not None
        assert spec.device.HasField("gpu")
        assert spec.device.gpu.variant == "H100"
        assert spec.device.gpu.count == 8
# ---
def getExcellon(self):
        return (self._preamble+
                self._content+
                self._postamble)
# ---
def can_pack(self, ids: list[int]) -> bool:
        return len(ids) + len(self._ids) <= self.Pos.size and self.num_segments < self.max_pack_size
# ---
def test_sum_partial_reduce(tmp_path, spec, executor):
    a = cubed.random.random(
        (40000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = partial_reduce(a, nxp.sum, split_every={0: 8})
    run_operation(tmp_path, executor, "sum_partial_reduce", b)
# ---
def tan(a: A) -> A:
    return wrap_elemwise_unary(jnp.tan, a)
# ---
def odd_Equivalent(s,n): 
    count=0
    for i in range(0,n): 
        if (s[i] == '1'): 
            count = count + 1
    return count
# ---
def LatentSize(self) -> Axis:
        return Axis("latent", self.kv_lora_rank)
# ---
def __initialize_manager(self):
		self.__config_mgr	= self.__manager.load_config_manager()

		self.__backend_mgr   = self.__manager.load_backend_manager()
		self.__bootstrap_mgr = self.__manager.load_bootstrap_manager()
		self.__navigator_mgr = self.__manager.load_navigator_manager()
		self.__context_mgr  = self.__manager.load_context_manager()
		self.__action_mgr	= self.__manager.load_action_manager()
		self.__launcher_mgr  = self.__manager.load_launcher_manager()
# ---
def _make_info():
        return {
            'driver_volume_type': 'iscsi',
            'data': {
                'volume_id': 1,
                'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                'target_portal': '127.0.0.1:3260,fake',
                'target_lun': None,
                'auth_method': 'CHAP',
                'auth_method': 'fake',
                'auth_method': 'fake',
            }
        }
# ---
def __init__(self, raw_data):
        self._raw = raw_data
# ---
def _julian_date_validator(value: str | JulianDate) -> JulianDate:
    """Pydantic validator which must handle strings or JulianDate objects."""
    if isinstance(value, str):
        return JulianDate(value)
    else:
        return value
# ---
def register(self, vm: ManagedVm) -> None:
        """Register a VM for tracking.

        Called when a VM is created. If a VM with the same ID already exists,
        it will be replaced.
        """
        with self._lock:
            self._vms[vm.info.vm_id] = vm
# ---
def teardown(self):
        testing.db.execute(users.delete())
# ---
def __init__(self, num_shards: int = 4):
        self._num_shards = num_shards
# ---
def test_causal_mask():
    mask = _make_causal_mask(4)
    expected = jnp.array(
        [
            [True, False, False, False],
            [True, True, False, False],
            [True, True, True, False],
            [True, True, True, True],
        ]
    )
    assert jnp.array_equal(mask, expected)
# ---
from collections import Counter
from itertools import chain
def freq_element(nums):
  result = Counter(chain.from_iterable(nums))
  return result
# ---
from collections import Counter 
def count_Occurrence(tup, lst): 
    count = 0
    for item in tup: 
        if item in lst: 
            count+= 1 
    return count
# ---
def eval_model_multithread(cfg, nr_eval, get_player_fn):
    func = OfflinePredictor(cfg)
    NR_PROC = min(multiprocessing.cpu_count() // 2, 8)
    mean, max = eval_with_funcs([func] * NR_PROC, nr_eval, get_player_fn)
    logger.info("Average Score: {}; Max Score: {}".format(mean, max))
# ---
def cron():
    reminders = load_json('data/reminders.json', True)
    for id, reminder in reminders.items():
        if now() > reminder['alarm']:
            send_message(reminder['chat_id'], reminder['text'])
            del reminders[id]
            save_json('data/reminders.json', reminders)
# ---
def record_batch(
        self,
        data: dict[str, torch.Tensor],
        i_time_start: int = 0,
    ):
        variable_metrics = self._get_variable_metrics(data)
        for name in data.keys():
            for metric in variable_metrics:
                variable_metrics[metric][name].record(
                    tensor=data[name],
                    i_time_start=i_time_start,
                )
# ---
def playlist_prev(self, mode='weak'):
        self.command('playlist_prev', mode)
# ---
def fuse_one_level(arr, *, always_fuse=None):
    # use fuse_predecessors to test one level of fusion
    return partial(
        fuse_predecessors,
        name=next(arr.plan.dag.predecessors(arr.name)),
        always_fuse=always_fuse,
    )
# ---
def slice_id(self) -> str:
        return self._slice_id
# ---
def __init__(self, token_z: int, cutoff_min: float, cutoff_max: float):
        super().__init__()

        self.fourier_embedding = FourierEmbedding(token_z)
        self.encoder = nn.Linear(
            token_z + len(const.contact_conditioning_info) - 1, token_z
        )
        self.encoding_unspecified = nn.Parameter(torch.zeros(token_z))
        self.encoding_unselected = nn.Parameter(torch.zeros(token_z))
        self.cutoff_min = cutoff_min
        self.cutoff_max = cutoff_max
# ---
def bench():
            indices = np.random.randint(0, len(loader), size=len(loader))
            for idx in indices:
                _ = loader.dataset[int(idx)]
# ---
def as_input_name(self) -> "InputName":
        return InputName(step=self, name=None)
# ---
def delete_pki_dir(self):
        '''
        Delete the private key directory
        '''
        path = self.opts['pki_dir']
        if os.path.exists(path):
            shutil.rmtree(path)
# ---
def unbind(self):
        return True
# ---
def test_equal_2(self):
        self.assertEqual(string_color('Joshua'), '6A10D6')
# ---
def test_max_subtree_stmts():
    """Large subtrees should be excluded."""
    programs = [SAMPLE_PROGRAMS[2]]  # merge_sort has many statements.
    bank = SubtreeBank.from_corpus(programs, max_subtree_stmts=2)
    for entries in bank.entries.values():
        for entry in entries:
            assert entry.stmt_count <= 2
# ---
def argmin(self, axis: AxisSelector | None) -> "NamedArray":  # pragma: no cover
        return haliax.argmin(self, axis=axis)
# ---
def test_bitwise_count_invert():
    A = Axis("A", 4)
    x = hax.named(jnp.array([0, 1, 2, 3], dtype=jnp.uint8), (A,))

    inv = hax.bitwise_invert(x)
    assert jnp.all(inv.array == jnp.bitwise_invert(x.array))

    cnt = hax.bitwise_count(x)
    assert jnp.all(cnt.array == jnp.bitwise_count(x.array))
# ---
def print_key(self, match):
        '''
        Print out a single key

        :param str match: A string to match against. i.e. 'web*'
        '''
        matches = self.key.key_str(match)
        salt.output.display_output(
                matches,
                'key',
                self.opts)
# ---
def tolist(self) -> Any:  # pragma: no cover
        return self.array.tolist()
# ---
def test_helper():
    return "test helper text"
# ---
def _restart_daemon(self):
        self._stop_daemon()
        return self._start_daemon()
# ---
def order_clause(self):
        """Generates a SQL fragment to be used in a ORDER BY clause, or
        None if no fragment is used (i.e., this is a slow sort).
        """
        return None
# ---
def find_first_occurrence(A, x):
    (left, right) = (0, len(A) - 1)
    result = -1
    while left <= right:
        mid = (left + right) // 2
        if x == A[mid]:
            result = mid
            right = mid - 1
        elif x < A[mid]:
            right = mid - 1
        else:
            left = mid + 1
    return result
# ---
def hard_silu(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_silu, a)
# ---
def test_parse_yaml_template(self):
        tmpl_str = 'heat_template_version: 2013-05-23'
        expected = {'heat_template_version': '2013-05-23'}
        self.assertEqual(expected, template_format.parse(tmpl_str))
# ---
def test_get_memory_size(self):
        self.assertEqual(self.mda.get_memory_size(), 4096)
# ---
def as_async_dataset(self) -> "AsyncDataset[T_co]":
        return self
# ---
def set_size(self, width, height):
        """Resize the window.

        The behaviour is undefined if the window is not resizable, or if
        it is currently fullscreen.

        The window size does not include the border or title bar.

        :Parameters:
            `width` : int
                New width of the window, in pixels.
            `height` : int
                New height of the window, in pixels.

        """
        raise NotImplementedError('abstract')
# ---
def __init__(self, docs: List[List[T]]):
        self.docs = docs
# ---
def visit_nodes(dag):
    """Return a generator that visits the nodes in the DAG in topological order."""
    nodes = {n: d for (n, d) in dag.nodes(data=True)}
    for name in list(nx.topological_sort(dag)):
        if skip_node(name, dag, nodes):
            continue
        yield name, nodes[name]
# ---
def notfirst():
            nonlocal first
            if first:
                first = False
                return True
            return False
# ---
def get_selector(self):
        ''' get the service selector'''
        return self.get(Service.selector_path) or {}
# ---
def extract_singly(test_list):
  res = []
  temp = set()
  for inner in test_list:
    for ele in inner:
      if not ele in temp:
        temp.add(ele)
        res.append(ele)
  return (res)
# ---
def ceil(a: A) -> A:
    return wrap_elemwise_unary(jnp.ceil, a)
# ---
def add_password(self, realm, uri, user, password):
        self.realm = realm
        self.url = uri
        self.user = user
        self.password = password
# ---
import re
def text_match_wordz_middle(text):
        patterns = '\Bz\B'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def test_ckpt_path_with_trailing_slash():
    path = "checkpoints/llama-8b-tootsie-phase2/checkpoints/step-730000/"
    assert ckpt_path_to_step_name(path) == "llama-8b-tootsie-phase2-730000"
# ---
def to_snake_case(name):
  intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\1_\2', name)
  insecure = re.sub('([a-z])([A-Z])', r'\1_\2', intermediate).lower()
  # If the class is private the name starts with "_" which is not secure
  # for creating scopes. We prefix the name with "private" in this case.
  if insecure[0] != '_':
    return insecure
  return 'private' + insecure
# ---
def _extract_image_params(image_tag: str, image_type: Literal["worker", "controller"]) -> _ImageBuildParams | None:
    parsed = _parse_artifact_registry_tag(image_tag)
    if not parsed:
        return None
    region, project, image_name, version = parsed
    return _ImageBuildParams(
        image_type=image_type, region=region, project=project, image_name=image_name, version=version
    )
# ---
def check_runtime_memory(spec, max_workers):
    allowed_mem = spec.allowed_mem if spec is not None else None
    total_mem = psutil.virtual_memory().total
    if allowed_mem is not None:
        if total_mem < allowed_mem * max_workers:
            raise ValueError(
                f"Total memory on machine ({total_mem}) is less than allowed_mem * max_workers ({allowed_mem} * {max_workers} = {allowed_mem * max_workers})"
            )
# ---
def speaker_sequence(self, speaker_id, n):
        """Random walk through transitions matrix to produce a sequence of speaker ids"""
        seq = []
        for i in range(n):
            seq.append(speaker_id)
            speaker_id = self.after(speaker_id)
        return seq
# ---
def __repr__(self):
        return "Average SAS model with %i atoms"%len(self.atoms)
# ---
def nbytes(self):
        return sum(v.nbytes for v in self.variables.values())
# ---
def parse_fences(self, m):
        self.tokens.append({
            'type': 'code',
            'lang': m.group(2),
            'text': m.group(3),
        })
# ---
def extract_document(self, item):
        raise NotImplementedError
# ---
def root():
    """ Web interface landing page. """
    return render_template('index.html')
# ---
def write_and_read(ref):
        ref[{"x": 1}] = 4.2
        return ref[{"x": 1}]
# ---
def _http_headers(cfg: UncheatableEvalDownloadConfig) -> dict[str, str]:
    headers = {"Accept": "application/vnd.github+json"}
    token = cfg.github_token or os.environ.get("GITHUB_TOKEN")
    if token:
        headers["Authorization"] = f"Bearer {token}"
    return headers
# ---
def sendShutdownSignal(self):
		requests.post('http://{}:{}/shutdown'.format(self.ip, self.port))
# ---
def __post_init__(self):
        if self.ports is None:
            self.ports = {}
# ---
def count_char_position(str1): 
    count_chars = 0
    for i in range(len(str1)):
        if ((i == ord(str1[i]) - ord('A')) or 
            (i == ord(str1[i]) - ord('a'))): 
            count_chars += 1
    return count_chars
# ---
def test_add(tmp_path, spec, executor, optimize_graph):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    c = xp.add(a, b)
    run_operation(tmp_path, executor, "add", c, optimize_graph=optimize_graph)
# ---
def with_prefix(prefix: str, leaf: None) -> str: ...
# ---
def exists_env_key(self, key):
        ''' return whether a key, value  pair exists '''
        results = self.get_env_vars()
        if not results:
            return False

        for result in results:
            if result['name'] == key:
                return True

        return False
# ---
def test_client(baby_llama_config, loaded_model, inference_server):
    """Create a test client for the inference server."""
    with TestClient(inference_server.app) as client:
        yield client, inference_server
# ---
def smallest_Divisor(n): 
    if (n % 2 == 0): 
        return 2; 
    i = 3;  
    while (i*i <= n): 
        if (n % i == 0): 
            return i; 
        i += 2; 
    return n;
# ---
def GetLogs(region, stream_name, group_name, token=None):
  """Fetches the JSON formatted log stream starting at the token."""
  get_cmd = util.AWS_PREFIX + [
      '--region', region,
      'logs', 'get-log-events',
      '--start-from-head',
      '--log-group-name', group_name,
      '--log-stream-name', stream_name,
  ]
  if token:
    get_cmd.extend(['--next-token', token])
  stdout, _, _ = vm_util.IssueCommand(get_cmd)
  return json.loads(stdout)
# ---
def sources(self) -> Mapping[str, LmDatasetSourceConfigBase]:
        sources: dict[str, LmDatasetSourceConfigBase] = {}
        for name, comp in self.components.items():
            if isinstance(comp, DatasetComponent) and comp.source is not None:
                sources[name] = comp.source
        return sources
# ---
def testBareAssert(self):
    # Assertion errors at the top level of a block should raise:
    # https://github.com/google/grumpy/issues/18
    want = (0, 'ok\n')
    self.assertEqual(want, _GrumpRun(textwrap.dedent("""\
        def foo():
         assert False
        try:
         foo()
        except AssertionError:
         print 'ok'
        else:
         print 'bad'""")))
# ---
def remove_odd(l):
    for i in l:
        if i % 2 != 0:
            l.remove(i)
    return l
# ---
def format_pwms(pwms):
    return format_line(prefix='pwms'.rjust(RJUST), values=pwms)
# ---
def nonblocking(self) -> "InputName":
        """
        the step will not block on (or attempt to execute) the parent step.

         (Note that if another step depends on the parent step, it will still block on it.)
        """
        return dataclasses.replace(self, block_on_step=False)
# ---
def batch_preparer(self):
        return TreeBatchPreparer(jtu.tree_map(lambda writer: 9, self.tree, is_leaf=heuristic_is_leaf))
# ---
def test_qr():
    A = np.reshape(np.arange(32, dtype=np.float64), (16, 2))
    Q, R = xp.linalg.qr(xp.asarray(A, chunks=(4, 2)))

    plan_unopt = arrays_to_plan(Q, R)._finalize()
    assert plan_unopt.num_primitive_ops() == 4

    Q, R = cubed.compute(Q, R)

    assert_allclose(Q @ R, A, atol=1e-08)
    assert_allclose(Q.T @ Q, np.eye(2, 2), atol=1e-08)  # Q must be orthonormal
    assert_allclose(R, np.triu(R), atol=1e-08)
# ---
def last_block(self) -> Block:
        with self.lock:
            return self._last_block
# ---
def test_add_different_tables(self):
        """Cannot add ConsumedCapacity of two different tables"""
        c1 = ConsumedCapacity("foobar", Capacity(1, 28))
        c2 = ConsumedCapacity("boofar", Capacity(3, 0))
        with self.assertRaises(TypeError):
            c1 += c2
# ---
def getTimeFromTZ(self, tz):
		# Assume sanitized zones - as they're pulled from pytz
		# Let's get the timezone list
		tz_list = FuzzySearch.search(tz, pytz.all_timezones, None, 3)
		if not tz_list[0]['Ratio'] == 1:
			# We didn't find a complete match
			return None
		zone = pytz.timezone(tz_list[0]['Item'])
		zone_now = datetime.datetime.now(zone)
		return { "zone" : tz_list[0]['Item'], "time" : zone_now.strftime("%I:%M %p") }
# ---
def prepare_plan(self, specs: Iterable[CopySpec]) -> T:
        """Convert copy specifications into a plan."""
        raise NotImplementedError
# ---
def logical_cpu_core_count() -> int:
    """Returns the number of logical CPU cores available to the process."""
    num_cpus = os.getenv("SLURM_CPUS_ON_NODE", None)
    if num_cpus is not None:
        return int(num_cpus)

    try:
        return os.cpu_count() or 1
    except NotImplementedError:
        return 1
# ---
def build_model_config(*, model: str, seq_len: int) -> MixtralConfig:
    if model == MODEL_OLMOE_1B7B:
        return _build_olmoe_1b7b_config(seq_len)
    if model == MODEL_MIXTRAL_8X7B:
        return _build_mixtral_8x7b_config(seq_len)
    raise ValueError(f"Unknown model preset {model!r}. Options: {MODEL_OPTIONS}.")
# ---
def _roots(kindpats):
    """Returns root directories to match recursively from the given patterns."""
    roots, dirs = _patternrootsanddirs(kindpats)
    return roots
# ---
def test_p3_closeness(self):
        c = nx.closeness_centrality(self.P3)
        d = {0: 0.667, 1: 1.000, 2: 0.667}
        for n in sorted(self.P3):
            assert almost_equal(c[n], d[n], places=3)
# ---
def _make_prefix_absolute_path(prefix, override_path):
    if _is_relative_path(override_path):
        override_path = os.path.join(prefix, override_path)
    return override_path
# ---
def replace_blank(str1,char):
 str2 = str1.replace(' ', char)
 return str2
# ---
def _handle_par_h(self):
        'Check if there is a waiting paragraph heading and handle it.'
        try:
            self._heading(self.par_h)
        except AttributeError:
            pass
# ---
def select_standard_prot(self, tokens, random):
        prot_mask = tokens["mol_type"] == const.chain_type_ids["PROTEIN"]
        standard_mask = tokens["is_standard"].astype(bool)
        tokens["design_mask"][prot_mask & standard_mask] = True
        return tokens
# ---
def _canonicalize_batch(batch: Union[dict, list[dict]]) -> list[dict]:
    if isinstance(batch, pa.RecordBatch):
        batch = dict_from_record_batch(batch)

    if isinstance(batch, dict):
        return _to_list_of_dicts(batch)
    else:
        return batch
# ---
def test_load_vortex_basic(self, vortex_file):
        """Test basic vortex file reading."""
        records = list(load_vortex(str(vortex_file)))
        assert len(records) == 100
        assert records[0]["id"] == 0
        assert records[0]["name"] == "item_0"
        assert records[0]["score"] == 0
# ---
def test_collate_order_by(self):
        collation = testing.requires.get_order_by_collation(testing.config)

        self._assert_result(
            select([self.tables.some_table]).order_by(
                self.tables.some_table.c.data.collate(collation).asc()
            ),
            [(1, "collate data1"), (2, "collate data2")],
        )
# ---
def setUp(self):
        super(BaseSystemTest, self).setUp()
        # Clear out any pre-existing tables
        for tablename in self.dynamo.list_tables():
            self.dynamo.delete_table(tablename)
# ---
def copy_file(filename: str) -> None:
        """Copy a single file if it doesn't already exist at destination."""
        output_filename = os.path.join(config.output_path, os.path.basename(filename))
        if not fsspec_exists(output_filename):
            # Ensure output directory exists
            fs.makedirs(config.output_path, exist_ok=True)
            fs.copy(filename, output_filename)
# ---
def get_output_embeddings(self):
        return self.lm_head
# ---
def max_Abs_Diff(arr,n): 
    minEle = arr[0] 
    maxEle = arr[0] 
    for i in range(1, n): 
        minEle = min(minEle,arr[i]) 
        maxEle = max(maxEle,arr[i]) 
    return (maxEle - minEle)
# ---
def setLastColor(self, lastColor):
        self._program.setLastColor(lastColor)
# ---
def after(self, other: "Timestamp") -> bool:
        """Check if this timestamp is after another."""
        return self._epoch_ms > other._epoch_ms
# ---
def fake_generate_ephemeral(cls, *args):
            self.called = True
# ---
def isnan(x: xr.DataArray) -> xr.DataArray:
    """Wrapped around np.isnan which correctly reflects the type we use it on."""
    return np.isnan(x)
# ---
def create_zstd_compressed_jsonl(records: list[dict]) -> bytes:
    jsonl_content = "\n".join(json.dumps(record) for record in records) + "\n"
    jsonl_bytes = jsonl_content.encode("utf-8")
    cctx = zstd.ZstdCompressor()
    return cctx.compress(jsonl_bytes)
# ---
def lr_scale(self):
        return 1
# ---
def get_noOfways(n):
    if (n == 0):
        return 0;
    if (n == 1):
        return 1; 
    return get_noOfways(n - 1) + get_noOfways(n - 2);
# ---
def run_gcloud_command(cmd: list[str], **kwargs) -> subprocess.CompletedProcess:
    """Run a gcloud command with error handling."""
    try:
        logger.info(f"Running {' '.join(cmd)}")
        return subprocess.run(cmd, check=True, capture_output=True, text=True, **kwargs)
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"gcloud command failed: {' '.join(cmd)}\nError: {e.stderr}") from e
# ---
def _tokenize(self, text: str, **kwargs) -> typing.List[str]:
    return list(text.lower())
# ---
def test_init_options(self, mocker):
        constructor = mocker.spy(pymemcache.client, "PooledClient")
        assert storage_from_string(self.storage_url, connect_timeout=1).check()
        assert constructor.call_args[1]["connect_timeout"] == 1
# ---
def show_opencfg_dlg(self):
        # show file dialog
        filename, _ = QFileDialog.getOpenFileName(
            self, self.tr("Open configuration file..."),
            directory=os.path.expanduser("~"),
            filter=self.tr("Json file (*.json);;All files (*.*)")
        )

        # load config file
        if filename:
            self.load_file(filename)
# ---
def logical_xor(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "logical_xor")
    if x1.dtype not in _boolean_dtypes or x2.dtype not in _boolean_dtypes:
        raise TypeError("Only boolean dtypes are allowed in logical_xor")
    return elemwise(nxp.logical_xor, x1, x2, dtype=nxp.bool)
# ---
def add(x, y):
        return x + y
# ---
def format_string_list(objs, field):
    objs[field] = ", ".join(objs[field])
# ---
def any_failed(self) -> bool:
        """True if any VM has failed or been preempted."""
        return any(v.state in (vm_pb2.VM_STATE_FAILED, vm_pb2.VM_STATE_PREEMPTED) for v in self.vms)
# ---
def test_proxy(self):
        self.assertFalse(self.get.has_proxy())
        self.get.set_proxy("www.perl.org", "http")
        self.assertTrue(self.get.has_proxy())
        self.assertEqual("www.python.org", self.get.get_origin_req_host())
        self.assertEqual("www.perl.org", self.get.get_host())
# ---
def fbcode_builder_spec(builder):
    return {
        'depends_on': [fbthrift],
    }
# ---
def overall_penalty(H: float) -> float:
    if H <= 20:
        return H
    if H <= 30:
        return H - 0.27 * (H - 18.0)
    if H <= 40:
        return H - 0.33 * (H - 18.0)
    if H <= 50:
        return H - 0.38 * (H - 18.0)
    return H - 0.447 * (H - 18.0)
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> ActorHandle:
        """Create a named actor instance. Returns a handle immediately."""
        ...
# ---
def __hash__(self) -> int:
        return hash(self._epoch_ms)
# ---
def mark_dup_documents(docs: Iterator[dict]) -> Iterator[dict]:
        fuzzy_dup_map = _load_fuzzy_dupe_map_shard(fuzzy_dup_shards)

        for doc in docs:
            is_fuzzy_dup = fuzzy_dup_map.get(doc["id"], False)
            doc["attributes"] = doc.get("attributes", {})
            assert DedupMode.FUZZY_DOCUMENT not in doc["attributes"]
            doc["attributes"][str(DedupMode.FUZZY_DOCUMENT)] = is_fuzzy_dup
            yield doc
# ---
def do_load(self, clear_first):
        if clear_first:
            self.match_list_box.Clear()
        msg = Forseti.ScheduleLoadCommand()
        msg.clear_first = clear_first
        print('Requesting load')
        self.lc.publish('Schedule/Load', msg.encode())
# ---
def output_text(self, m):
        text = m.group(0)
        return self.renderer.text(text)
# ---
def delete(user_fn):
        user_fn = _strip_wrapped_partial(user_fn)
        if type(user_fn) is types.FunctionType:  # noqa: E721
            try:
                del cache[user_fn]
            except KeyError:
                warnings.warn(f"Could not delete cache for function {user_fn}. Has it already been deleted?")
        else:
            warnings.warn("Could not delete non-function from cache.")
# ---
def run_sh(cmd: str, **kwargs) -> subprocess.CompletedProcess:
    """Run command from string with logging."""
    return run(shlex.split(cmd), **kwargs)
# ---
def rfftfreq(axis: Axis, d: float = 1.0) -> NamedArray:
    """Named version of :func:`jax.numpy.fft.rfftfreq`."""

    new_axis = axis.resize(axis.size // 2 + 1)
    return NamedArray(jfft.rfftfreq(axis.size, d), (new_axis,))
# ---
def __sub__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "sub")
# ---
def scan_step(block: ScanModule, carry: hax.NamedArray) -> tuple[hax.NamedArray, hax.NamedArray]:
        return block(carry)
# ---
def false_heading(self, elem):
        '''Handle a "false heading", i.e., text that appears in heading
        tags in the source even though it is not a chapter heading.'''
        elem.attrs['class'] = 'getebook-false-h'
        elem.tag = 'p'
        self.handle_elem(elem)
# ---
def _get_local_data_browser_port(default: int = 5000) -> int:
    # looks for the port in the local data browser config file
    config_path = _find_data_browser_local_conf()
    if config_path is None:
        return default

    try:
        with config_path.open() as fp:
            for line in fp:
                match = _LOCAL_DATA_BROWSER_PORT_RE.match(line)
                if match:
                    return int(match.group(1))
    except OSError:
        return default

    return default
# ---
def sum_Of_Subarray_Prod(arr,n):
    ans = 0
    res = 0
    i = n - 1
    while (i >= 0):
        incr = arr[i]*(1 + res)
        ans += incr
        res = incr
        i -= 1
    return (ans)
# ---
def withdraw(self, amount):
        self.balance -= amount
        return self.balance
# ---
def mock_image_cache():
    """Create mock ImageCache."""
    cache = Mock(spec=ImageCache)
    cache.build = Mock(
        return_value=BuildResult(
            image_tag="test-image:latest",
            build_time_ms=1000,
            from_cache=False,
        )
    )
    return cache
# ---
def _full_next_token_loss(logits: jax.Array, token_ids: jax.Array, loss_weight: jax.Array) -> jax.Array:
    labels = jnp.concatenate([token_ids[:, 1:], token_ids[:, :1] * 0], axis=1).astype(jnp.int32)
    log_probs = jax.nn.log_softmax(logits, axis=-1)
    nll = -jnp.take_along_axis(log_probs, labels[..., None], axis=-1)[..., 0]
    nll = nll * loss_weight
    denom = jnp.sum(loss_weight)
    return jnp.sum(nll) / jnp.maximum(denom, jnp.array(1.0, dtype=nll.dtype))
# ---
def my_callback():
        _save_metadata(checkpoint_path, fs, step, is_temporary)
        logger.info(f"Saved checkpoint to {checkpoint_path} for step {step}")

        if commit_callback is not None:
            commit_callback()
# ---
def sum_Natural(n): 
    sum = (n * (n + 1)) 
    return int(sum) 
def sum_Even(l,r): 
    return (sum_Natural(int(r / 2)) - sum_Natural(int((l - 1) / 2)))
# ---
def test_equal(self):
        """Test that cmp_version compares a as equal to b"""
        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.4') == 0)
# ---
def _sum(arr):  
    sum=0
    for i in arr: 
        sum = sum + i      
    return(sum)
# ---
def loglikelihood_rolling(self, requests) -> List[Tuple[float]]:
        raise NotImplementedError()
# ---
def get_summary_logs(self):
        # These aggregators require a full timeseries of
        # data and thus wandb logged at the end
        logs: MetricsDict = {}
        for name, aggregator in self._summary_aggregators.items():
            logs.update(aggregator.get_logs(label=name))
        return logs
# ---
def set_configuration(self, frequency, bandwidth, power):
		self.frequency = frequency
		self.bandwidth = bandwidth
# ---
def stdout(self):
        return self._stdout
# ---
def from_submission_id(ray_job_id: str, name: str) -> "RayJobInfo":
        return RayJobInfo(ref=None, submission_id=ray_job_id, name=name)
# ---
def normalize_dtype(dtype: str) -> str:
    """Normalize dtype string to base form.

    Strips 'amp_' prefix if present, converts to lowercase.
    """
    dtype = dtype.lower()
    if dtype.startswith("amp_"):
        return dtype[4:]
    return dtype
# ---
def log_time(label: str, level: int = logging.INFO) -> Iterator[None]:
    t_start = time.perf_counter()
    yield
    t_end = time.perf_counter()
    logger.log(level, f"{label} took {timedelta(seconds=t_end - t_start)}")
# ---
def test_to_obj_list():
    msg = '[{"hoge": 1}, {"hogi": 2}]'
    bb = jps.utils.to_obj(msg)
    assert len(bb) == 2
    assert bb[0].hoge == 1
    assert bb[1].hogi == 2
# ---
def fsdp(parameter_mapping: ResourceMapping, compute_mapping: ResourceMapping) -> typing.Callable[[F], F]: ...
# ---
def test_map_blocks_multiple_inputs_key_function():
    key_function = make_map_blocks_key_function("a", "b")

    check_key_function(key_function, (0,), "(('a', 0), ('b', 0))")
    check_key_function(key_function, (1,), "(('a', 1), ('b', 1))")
# ---
def solution (a, b, n): 
	i = 0
	while i * a <= n: 
		if (n - (i * a)) % b == 0: 
			return ("x = ",i ,", y = ", 
			int((n - (i * a)) / b)) 
			return 0
		i = i + 1
	return ("No solution")
# ---
def count_binary_seq(n): 
	nCr = 1
	res = 1
	for r in range(1, n + 1): 
		nCr = (nCr * (n + 1 - r)) / r 
		res += nCr * nCr 
	return res
# ---
def bmarks():
    return_data = get_edit_tags()
    return return_data
# ---
def canonicalMachineName(machine=''):
    aliases = {'nstxu': ['nstx', 'nstxu', 'nstx-u'],
               'diiid': ['diiid', 'diii-d', 'd3d'],
               'cmod': ['cmod', 'c-mod']}
    for key, value in aliases.items():
        if machine.lower() in value:
            return key
    # invalid machine name
    raise FdpError('"{}" is not a valid machine name\n'.format(machine))
# ---
def run_buf_generate(root_dir: Path) -> None:
    """Run buf generate using npx."""
    print("Running npx buf generate...")
    result = subprocess.run(
        ["npx", "--yes", "@bufbuild/buf", "generate"],
        cwd=root_dir,
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        print(f"Error running buf generate:\n{result.stderr}", file=sys.stderr)
        sys.exit(1)

    print(" buf generate completed successfully")
# ---
def call(*_, **__):
            """Dummy service call"""
            response = {
                "ResponseMetadata": {
                    "HTTPStatusCode": 400,
                },
                "Error": {
                    "Code": "ProvisionedThroughputExceededException",
                    "Message": "Does not matter",
                },
            }
            raise ClientError(response, "list_tables")
# ---
def linebreak(self):
        """Rendering line break like ``<br>``."""
        if self.options.get('use_xhtml'):
            return '<br />\n'
        return '<br>\n'
# ---
def get_indexing_matrix(K, W, H, device):
    assert W % 2 == 0
    assert H % (W // 2) == 0

    h = H // (W // 2)
    assert h % 2 == 0

    arange = torch.arange(2 * K, device=device)
    index = ((arange.unsqueeze(0) - arange.unsqueeze(1)) + h // 2).clamp(
        min=0, max=h + 1
    )
    index = index.view(K, 2, 2 * K)[:, 0, :]
    onehot = one_hot(index, num_classes=h + 2)[..., 1:-1].transpose(1, 0)
    return onehot.reshape(2 * K, h * K).float()
# ---
def unbind(self, existing, imported):
        [match] = [m for m in self.matches if m[0] is existing and m[1] is imported]
        match[1] = None
        self.matches.append([None, imported])
        self._sort_matches()
# ---
def __init__(self,apiKey,licenseId):    
    self.headers = {"content-type": "application/json",
                    "Authorization": apiKey}
    self.params = {"licenseId" : licenseId }
    retries = Retry(total=5,
                    backoff_factor=0.75)
    self.session = requests.Session()
    self.session.mount(KANGROUTER_WEBSERVICE_APPLICATION_ROOT, 
                       HTTPAdapter(max_retries=retries))
# ---
def temporal_diff(self, m, s):
        """
        The temporal diffence value for state s to state (s+1) in the mth game
        """
        return (self.get_reward(m, s) + self.eval_func(m, s + 1, self.rt) -
                self.eval_func(m, s, self.rt))
# ---
def get_milestone_id():
    """Return the milestone ID for 'Release'"""
    url = f"https://api.github.com/repos/{GITHUB_REPO}/milestones"
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    for m in response.json():
        if m["title"] == MILESTONE_NAME:
            return m["number"]
    raise ValueError(f"Milestone '{MILESTONE_NAME}' not found")
# ---
def on_resize(width, height):
            """The window was resized.

            The window will have the GL context when this event is dispatched;
            there is no need to call `switch_to` in this handler.

            :Parameters:
                `width` : int
                    The new width of the window, in pixels.
                `height` : int
                    The new height of the window, in pixels.

            :event:
            """
# ---
def validate_path(self) -> Self:
        if not self.path.is_absolute():
            raise ValueError(
                "Locations with type: 'local' must be absolute. "
                "For relative paths, use a string instead of a structured location. "
                "i.e. 'my/relative/path' instead of "
                "{type: 'local', path: 'my/relative/path'}"
            )
        return self
# ---
def test_mistral_gpt2_roundtrip():
    _roundtrip_compare_gpt2_checkpoint("stanford-crfm/expanse-gpt2-small-x777", "checkpoint-60000")
# ---
def __repr__(self):
        return "{0.__class__.__name__}({0.subqueries!r})".format(self)
# ---
def python_compute_document_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> list[dict[str, str]]:
    results = []
    for record in batch.to_pylist():
        text, record_id = record[text_col], record[id_col]
        results.append({"hash": _str_hash_legacy(text), "id": record_id})
    return results
# ---
def generate_data(config: GenerateDataConfig):
    """Generate numbers from 0 to `n` - 1 and write them to `output_path`."""
    numbers = list(range(config.n))

    # Write to file
    numbers_path = os.path.join(config.output_path, "numbers.json")
    with fsspec.open(numbers_path, "w") as f:
        json.dump(numbers, f)
# ---
def __init__(self, root, cwd, badfn=None):
        super(nevermatcher, self).__init__(root, cwd, badfn)
# ---
def apply_chat_template(self, messages, tokenize, add_generation_prompt):
            # Simple: just return tokens for the user message content
            return [ord(c) for c in messages[0]["content"]]
# ---
def main():
    """ Parse arguments and get things going for the web interface """
    parser = argparse.ArgumentParser(description=HELP_TEXT)

    parser.add_argument(
        '-p', '--port',
        help="Port to serve the interface on.",
        type=int,
        default=5050
    )

    parser.add_argument(
        '-a', '--host',
        help="Host to server the interface on.",
    )

    args = parser.parse_args()

    app.run(port=args.port, host=args.host, debug=False)
# ---
def _succeeded_job():
    print("Snapshot succeeded job completed.")
    return "ok"
# ---
def do_vmap(init: CarryT, *args, **kwargs) -> OutputT_co:
            # Apply fn to each block independently
            outputs = []
            for block in self.blocks:
                output = fn(block, init, *args, **kwargs)
                outputs.append(output)

            # Stack the outputs
            stacked_out = haliax.tree_util.tree_map(lambda *x: haliax.stack(self.Block, x), *outputs)
            return stacked_out
# ---
def __set__(self, instance, value):
		instance.__dict__[self.field.name] = value
		setattr(instance, self.field.attname, json.dumps(value))
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "Gpt2HyenaModel":
        new_embeddings = self.embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, embeddings=new_embeddings)
# ---
def _state_dict_key_map(self):
        # This tells the serialization logic how to map attribute names to state dict keys
        # We want the wrapper's parameters to be at the top level.
        # The Gpt2LMHeadModel's parameters will be nested under "model" if we follow its own mapping.
        return {
            "model": "model",
            "an_int_param": "an_int_param",
            "a_bool_buffer": "a_bool_buffer",
            "a_float_param": "a_float_param",
        }
# ---
def _find_free_port(self) -> int:
        for port in range(self._range[0], self._range[1]):
            if port in self._allocated:
                continue
            if self._is_port_free(port):
                return port
        logger.warning("Port allocation exhausted: no free ports in range %d-%d", self._range[0], self._range[1])
        raise RuntimeError("No free ports available")
# ---
def find_step_containing_offset(self, offset: int) -> int:
        """
        Find the step that contains the given global data offset.
        """
        for seg in self.segments:
            if seg.offset <= offset < seg.offset + (seg.until - seg.start) * seg.value:
                return seg.start + (offset - seg.offset) // seg.value
        raise ValueError(f"Offset {offset} is beyond the last defined segment.")
# ---


def is_prime(n):
    """Return true if a given number is prime, and false otherwise.
    >>> is_prime(6)
    False
    >>> is_prime(101)
    True
    >>> is_prime(11)
    True
    >>> is_prime(13441)
    True
    >>> is_prime(61)
    True
    >>> is_prime(4)
    False
    >>> is_prime(1)
    False
    """
    if n < 2:
        return False
    for k in range(2, n - 1):
        if n % k == 0:
            return False
    return True
# ---
def test_str_split_default(self):
        def test_impl(df):
            return df.A.str.split()

        df = pd.DataFrame({'A': ['AB CC', 'C ABB D', 'G ', ' ', 'g\t f']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def even_ele(test_tuple, even_fnc): 
	res = tuple() 
	for ele in test_tuple: 
		if isinstance(ele, tuple): 
			res += (even_ele(ele, even_fnc), ) 
		elif even_fnc(ele): 
			res += (ele, ) 
	return res 
def extract_even(test_tuple):
  res = even_ele(test_tuple, lambda x: x % 2 == 0)
  return (res)
# ---
def flops_per_token(self, vocab_size: int, context_length: int):
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def init(*, key):
            k1, k2 = jax.random.split(key)
            first = hnn.Linear.init(In, Mid, key=k1, init_scale=0.02)
            second = hnn.Linear.init(Mid, In, key=k2, init_scale=0.02)
            return Module(first, second)
# ---
def test_add_different_chunks(spec, executor):
    a = xp.ones((10, 10), chunks=(10, 2), spec=spec)
    b = xp.ones((10, 10), chunks=(2, 10), spec=spec)
    c = xp.add(a, b)
    assert_array_equal(
        c.compute(executor=executor), np.ones((10, 10)) + np.ones((10, 10))
    )
# ---
def __init__(self):
        self._calls = CallList()
        self._server_mock = None
        self.directory = []
        self.exception = None
        self.reset()
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        """Return the scaling factor for initializing weights
        given input and output axes."""
        raise NotImplementedError
# ---
def prefetch_rule(context: models.Context):
  # Make sure that we have the IAM policy in cache.
  project_ids = {c.project_id for c in gke.get_clusters(context).values()}
  for pid in project_ids:
    iam.get_project_policy(pid)
# ---
def main(cfg: TrainFasttextClassifierConfig):
    train(cfg)
# ---
def __init__(
        self,
        cluster: LocalClusterClient | RemoteClusterClient,
        namespace: Namespace,
        job_id: JobName,
    ):
        self._cluster = cluster
        self._namespace = namespace
        self._job_id = job_id
# ---
def is_numeric(str):
    try:
        _offset = int(eval(str))
    except:
        return False
    return True
# ---
def test_lambda_output_shape_function_multiple_outputs(self):

    def lambda_fn(x):
      return x

    def output_shape_fn(input_shape):
      return input_shape

    l = keras.layers.Lambda(lambda_fn, output_shape=output_shape_fn)
    output_shape = l.compute_output_shape([(10, 10), (10, 20)])
    self.assertAllEqual([(10, 10), (10, 20)], output_shape)
# ---
def testIfElse(self):
    self.assertEqual((0, 'foo\nbar\n'), _GrumpRun(textwrap.dedent("""\
        if True:
          print 'foo'
        else:
          print 'bar'
        if False:
          print 'foo'
        else:
          print 'bar'""")))
# ---
def is_callable(self) -> bool:
        return self._callable_bytes is not None
# ---
def text(self):
        """Get indent text as \t or string of spaces
        """
        if self.useTabs:
            return '\t'
        else:
            return ' ' * self.width
# ---
import heapq
def larg_nnum(list1,n):
 largest=heapq.nlargest(n,list1)
 return largest
# ---
def job_logs(ctx, job_id, follow, tail, grep):
    """View logs for a Ray job."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        if follow:
            subprocess.run(["ray", "job", "logs", "--follow", job_id], check=False)
            return
        _print_job_logs(job_id, tail=tail, grep=grep)
# ---
def test_impl(df):
            return df.A.str.replace('AB', 'EE', regex=False)
# ---
def setUp(self):
        super(NetappDirectCmodeNfsDriverTestCase, self).setUp()
        self._custom_setup()
# ---
def test_scalar_eliminates_axis():
    B, S, V = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 4)
    x = hax.arange((B, S, V))
    out = x["seq", 1]
    assert out.axes == (B, V)
    assert jnp.array_equal(out.array, x.array[:, 1, :])
# ---
def _reconnect(self):
        """Reconnect to the ReadRows stream using the most recent offset."""
        while True:
            try:
                self._wrapped = self._client.read_rows(
                    read_stream=self._name,
                    offset=self._offset,
                    **self._read_rows_kwargs
                )
                break
            except Exception as exc:
                if not self._resource_exhausted_exception_is_retryable(exc):
                    raise
# ---
def _generate(cls, create, attrs):
        note = super()._generate(create, attrs)
        note.pubdate = datetime.now()
        note.save()
        note.related_content.last_note = note
        note.related_content.save()
        return note
# ---
def assert_span(span, operation, parent=None):
    assert span.operation_name == 'MySQLdb:' + operation
    assert span.tags.get(tags.SPAN_KIND) == tags.SPAN_KIND_RPC_CLIENT
    if parent:
        assert span.parent_id == parent.context.span_id
        assert span.context.trace_id == parent.context.trace_id
    else:
        assert span.parent_id is None
# ---
def print(*args, **kwargs):
    force = kwargs.pop("force", False)
    force = force or (get_world_size() > 8)
    if force:
        now = datetime.datetime.now().time()
        builtin_print(f"[{now}] ", end="")  # print with time stamp
        builtin_print(*args, **kwargs)
# ---
def test_no_trivially_short_entries(bank):
    """Entries shorter than 5 chars should be filtered out."""
    for entries in bank.entries.values():
        for entry in entries:
            assert len(entry.source) >= 5, f"Too short: {entry.source!r}"
# ---
def __bool__(self) -> bool:  # pragma: no cover
        return bool(self.array)
# ---
def _updateOrcaModifier(self):
        combobox = self.get_widget("orcaModifierComboBox")
        keystring = ", ".join(self.prefsDict["orcaModifierKeys"])
        combobox.set_active(self.getComboBoxIndex(combobox, keystring))
# ---
def empty_queue_space(self) -> jnp.ndarray:
        """How many tokens can be enqueued in the queue."""
        return self.queued_tokens.axis_size("position") - self.num_queued_tokens
# ---
def fail_once_jax_fn() -> None:
        # Check if we should fail BEFORE initializing JAX/TPU
        # This avoids claiming the TPU on the first attempt
        count = ray.get(counter_actor.count.remote())
        ray.get(counter_actor.increment.remote())
        if count == 0:
            raise DeliberatelyRaisedException(f"Failing deliberately because count is {count}")

        # Only do JAX work after we know we won't fail
        result = simple_jax_fn()
        return result
# ---
def intersect_axes(ax1: tuple[AxisSelector, ...], ax2: AxisSpec) -> tuple[Axis, ...]: ...
# ---
def peek(self) -> Any | None:
        with httpx.Client() as client:
            response = client.get(f"http://{self.host}:{self.port}/queues/{self.queue_name}/peek")
            if response.status_code == 200:
                return pickle.loads(response.json()["payload"])
            return None
# ---
def test_partial_reduce(spec):
    a = xp.asarray(np.arange(242).reshape((11, 22)), chunks=(3, 4), spec=spec)
    b = partial_reduce(a, np.sum, split_every={0: 2})
    c = partial_reduce(b, np.sum, split_every={0: 2})
    assert_array_equal(
        c.compute(), np.arange(242).reshape((11, 22)).sum(axis=0, keepdims=True)
    )
# ---
def i_deselect_all_objects():
    bpy.context.view_layer.objects.active = None
    bpy.ops.object.select_all(action="DESELECT")
# ---
def testInt32Basic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.int32)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.int32)
    self._testCpuMatmul(x, y)
# ---
def add_hook(self, fn: Callable[[StepInfo], Any], *, every: int = 1): ...
# ---
def as_sql(self, compiler, connection):
        lhs, params = compiler.compile(self.lhs)
        return 'TIME({})'.format(lhs), params
# ---
def test_decimal(self):
        """Store and retrieve a Decimal"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "num": Decimal("1.1")})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["num"], Decimal("1.1"))
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "ToyLmHeadModel":
        del key
        if new_size != self.Vocab.size:
            raise NotImplementedError("ToyLmHeadModel.resize_vocab only supports a no-op resize.")
        return self
# ---
def assert_disk_type(self, image_meta, expected_disk_type):
        actual = vm_utils.VMHelper.determine_disk_image_type(image_meta)
        self.assertEqual(expected_disk_type, actual)
# ---
def _get_storage_domain_path(self, path):
        '''
        prepareImage returns /prefix/sdUUID/images/imgUUID/volUUID
        we need storage domain absolute path so we go up 3 levels
        '''
        return path.rsplit(os.sep, 3)[0]
# ---
def __post_init__(self):
        assert isinstance(self.num_seqs, jnp.ndarray), "num_seqs must be a JAX ndarray"
# ---
def __init__(self, config: ServerConfig):
        self.config = config

        # Lazily instantiate remote filesystems to avoid triggering cloud
        # auth during local-only development.
        self.fs_cache = {
            None: fsspec.filesystem("local"),
        }
# ---
def get_actor_name_from_actor_info(self, actor_info: TPUHostInfo) -> str:
        return str(actor_info.worker_index)
# ---
def test_device_flops(device_type, dtype, flops):
    assert device_flops(device_type, dtype) == flops
# ---
def check_solver_status(status, raise_error=False):
    """Perform standard checks on a solver's status."""
    if status == optlang.interface.OPTIMAL:
        return
    elif status == optlang.interface.INFEASIBLE and not raise_error:
        warn("solver status is '{}'".format(status), UserWarning)
    elif status is None:
        raise RuntimeError(
            "model was not optimized yet or solver context switched")
    else:
        raise OptimizationError("solver status is '{}'".format(status))
# ---
def inner(x):
        # x is a BatchTracer with batch_dim=0, aval shape (hidden,)
        na = hax.NamedArray(x, (Hidden,))
        return hax.auto_sharded(na)
# ---
def test_complex_arithmetic(self):
        # (a + b) * c
        expr = (col("a") + col("b")) * col("c")
        assert expr.evaluate({"a": 2, "b": 3, "c": 4}) == 20
# ---
def registry(self) -> VmRegistry:
        """Access the underlying registry for cleanup operations."""
        ...
# ---
def make_snapshot(
    vm_id: str = "vm-0",
    state: vm_pb2.VmState = vm_pb2.VM_STATE_READY,
    address: str = "10.0.0.1",
    init_phase: str = "",
    init_error: str = "",
) -> VmSnapshot:
    """Create a VmSnapshot for testing."""
    return VmSnapshot(
        vm_id=vm_id,
        state=state,
        address=address,
        init_phase=init_phase,
        init_error=init_error,
    )
# ---
def shutdown(self, wait: bool = True) -> None:
        logger.info("RayClient shutdown (namespace=%s)", self._namespace)
# ---
def resize_vocab(self, new_size: int, key: PRNGKeyArray | None = None) -> "GrugWrapper":
        raise NotImplementedError("GrugWrapper does not yet support resizing the vocabulary.")
# ---
def float_to_tuple(test_str):
  res = tuple(map(float, test_str.split(', ')))
  return (res)
# ---
def _related_domain(self):
        if callable(self.domain):
            # will be called with another model than self's
            return lambda recs: self.domain(recs.env[self.model_name])
        else:
            # maybe not correct if domain is a string...
            return self.domain
# ---
def testHighFrequency(self):
        hpcp = HPCP(minFrequency=100, maxFrequency=1000)([1001], [1])
        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
# ---
def decodes():
    pass
# ---
def test_registrar(self):
        eq_(self.record.registrar, None)
# ---
def long_words(n, str):
    word_len = []
    txt = str.split(" ")
    for x in txt:
        if len(x) > n:
            word_len.append(x)
    return word_len
# ---
def failing_job():
            raise ValueError("intentional failure")
# ---
def concat_axes(a1: ShapeDict, a2: AxisSpec) -> ShapeDict:
    pass
# ---
def __init__(self, view_or_error=None):
        self.wrapped = callable(view_or_error)
        error_view = None

        if self.wrapped:
            self.view = view_or_error
        else:
            error_view = view_or_error

        if not error_view:
            from django.conf import settings
            error_view = settings.PERMISSIONS_VIEW

        from django.core.urlresolvers import get_callable
        self.error_view = get_callable(error_view)
# ---
def rotate_right(list1,m,n):
  result =  list1[-(m):]+list1[:-(n)]
  return result
# ---
def test_capfdbinary(self, testdir):
        reprec = testdir.inline_runsource("""
            def test_hello(capfdbinary):
                import os
                # some likely un-decodable bytes
                os.write(1, b'\\xfe\\x98\\x20')
                out, err = capfdbinary.readouterr()
                assert out == b'\\xfe\\x98\\x20'
                assert err == b''
        """)
        reprec.assertoutcome(passed=1)
# ---
def output_footnote(self, m):
        key = _keyify(m.group(1))
        if key not in self.footnotes:
            return None
        if self.footnotes[key]:
            return None
        self.footnote_index += 1
        self.footnotes[key] = self.footnote_index
        return self.renderer.footnote_ref(key, self.footnote_index)
# ---
def access_bot_by_id(user_profile: UserProfile, user_id: int) -> UserProfile:
    try:
        target = get_user_profile_by_id_in_realm(user_id, user_profile.realm)
    except UserProfile.DoesNotExist:
        raise JsonableError(_("No such bot"))
    if not target.is_bot:
        raise JsonableError(_("No such bot"))
    if not user_profile.can_admin_user(target):
        raise JsonableError(_("Insufficient permission"))
    return target
# ---
def __post_init__(self) -> None:
        """Validate configuration."""
        if self.hidden_dim % self.num_heads != 0 and self.head_dim is None:
            raise ValueError(
                f"hidden_dim={self.hidden_dim} must be divisible by "
                f"num_heads={self.num_heads}, or set head_dim explicitly"
            )
        if self.num_heads % self.num_kv_heads != 0:
            raise ValueError(f"num_heads={self.num_heads} must be divisible by num_kv_heads={self.num_kv_heads}")
# ---
def total_attempts(self) -> int:
        """Total number of retries (failure + preemption retries)."""
        return self.failure_count + self.preemption_count
# ---
def test_registrant_contacts(self):
        eq_(self.record.registrant_contacts.__class__.__name__, 'list')
        eq_(self.record.registrant_contacts, [])
# ---
def scenario(function):
    def subfunction(self):
        run(function(self))

    return subfunction
# ---
def test_find_path_converges_to_target():
    source = "return a + b\n"
    target = "return a * b\n"
    path = find_path(source, target)
    assert len(path) >= 1

    current = source
    for mutation in path:
        current = mutation.apply(current)

    # The result should structurally match the target.
    assert ast.dump(ast.parse(current)) == ast.dump(ast.parse(target))
# ---
def default_get(self, cr, uid, fields_list, context=None):
        # merge defaults from stock.picking with possible defaults defined on stock.picking.in
        defaults = self.pool['stock.picking'].default_get(cr, uid, fields_list, context=context)
        in_defaults = super(stock_picking_in, self).default_get(cr, uid, fields_list, context=context)
        defaults.update(in_defaults)
        return defaults
# ---
def detok(text):
      for i, t in enumerate(text, 0):
        text[i] = detokenizer(t)
      return text
# ---
def draw_sizes(shape, size=200):
    """Get size in pixels for all dimensions"""
    mx = max(shape)
    ratios = [mx / max(0.1, d) for d in shape]
    ratios = [ratio_response(r) for r in ratios]
    return tuple(size / r for r in ratios)
# ---
def __contains__(self, key):
        return id(key) in self._data
# ---
def set_trigger_recording_on_release(self, trigger_recording):
        self._should_trigger_recording = trigger_recording
# ---
def set_data(self, data):
        self.data = []
        self.rewards = []

        for game in data:
            game = np.vstack((game, np.zeros(self.num_features + 1)))
            self.data.append(game[:, :-1])
            self.rewards.append(game[:, -1:])
# ---
def test_run_streaming_with_retry_success_first_attempt():
    """run_streaming_with_retry succeeds on first attempt."""
    conn = MagicMock()
    conn.run_streaming.return_value = make_fake_popen()
    lines_received: list[str] = []
    result = run_streaming_with_retry(conn, "bootstrap script", max_retries=3, on_line=lines_received.append)
    assert result.returncode == 0
    assert len(lines_received) > 0
# ---
def forward(self, x):
        return self.avgpool(x)
# ---
def raw_prediction_data(input_data):
    coords = {co: input_data.coords[co] for co in ["time", "y", "x"]}
    return xr.DataArray(
        np.random.random([3, 180, 360, 77]),
        dims=["time", "y", "x", "var"],
        coords=coords,
    ).to_dataset(name="__xarray_dataarray_variable__")
# ---
def __init__(self, dataset: SyncDataset[T_co]):
        super().__init__()
        self.dataset = dataset
# ---
def __init__(self, balance=0):
        self.balance = balance
# ---
def active_scale(self):
        return 1 / hax.axis_size(self.In)
# ---
def config(self):
        return self.model.config
# ---
def _setup_regular(self, env):
        super(Reference, self)._setup_regular(env)
        assert isinstance(self.size, (NoneType, int)), \
            "Reference field %s with non-integer size %r" % (self, self.size)
# ---
def __repr__(self) -> str:
        return f"{self.parent}[{self.field!r}]"
# ---
def _create_definition_if_needed(self):
    """Creates the function definition if it's not created yet."""
    with context.graph_mode():
      self._create_definition_if_needed_impl()
# ---
def get_num_input_blocks(dag, arr_name):
    op_name = next(dag.predecessors(arr_name))
    return dag.nodes(data=True)[op_name]["pipeline"].config.num_input_blocks
# ---
def set_device(self, device):
        self.device = device
# ---
def yield_1(it):
        yield from [1]
# ---
def forwards(self, orm):
        # Adding field 'Idea.color'
        db.add_column(u'brainstorming_idea', 'color',
                      self.gf('django.db.models.fields.CharField')(default='', max_length=100, blank=True),
                      keep_default=False)
# ---
def unregister_key_binding(self, keydef):
        binding_name = MPV._binding_name(keydef)
        self.command('disable-section', binding_name)
        self.command('define-section', binding_name, '')
        if callable(callback):
            del self._key_binding_handlers[binding_name]
            if not self._key_binding_handlers:
                self.unregister_message_handler('key-binding')
# ---
def test_getitem_out_of_bounds():
    with tempfile.TemporaryDirectory() as tmpdir:
        exemplar = {"a": np.array([0], dtype=np.float64), "b": np.array([0], dtype=np.float64)}
        builder = TreeStore.open(exemplar, tmpdir)

        batch = [
            {"a": np.array([1.0, 2.0]), "b": np.array([3.0, 4.0])},
            {"a": np.array([5.0, 6.0]), "b": np.array([7.0, 8.0])},
        ]
        builder.extend(batch)

        with pytest.raises(IndexError):
            builder[2]
# ---
def on_task_end(self, event):
        self.events.append(asdict(event))
# ---
def limit_fastq(fastq_gen, num_sequences=1000):
    for i in range(num_sequences):
        try:
            yield next(fastq_gen)
        except StopIteration:
            return
# ---
def test_brackets_balanced_nested():
    assert brackets_balanced("f(g[h({x})])")
# ---
def timetest():
        return str(time.time())
# ---
def test_nanmean_allnan(spec):
    a = xp.asarray([xp.nan], spec=spec)
    b = cubed.nanmean(a)
    assert_array_equal(b.compute(), np.nanmean(np.array([np.nan])))
# ---
def _import_image(self, url=None, name=None, tag=None):
        ''' perform image import '''
        cmd = ['import-image']

        image = '{0}'.format(name)
        if tag:
            image += ':{0}'.format(tag)

        cmd.append(image)

        if url:
            cmd.append('--from={0}/{1}'.format(url, image))

        cmd.append('-n{0}'.format(self.namespace))

        cmd.append('--confirm')
        return self.openshift_cmd(cmd)
# ---
def _sleep_time(iter):
    """Return the time-to-sleep for the n'th iteration of a retry loop.

    This implementation increases exponentially.

    :param iter: iteration number
    :returns: number of seconds to sleep

    """
    if iter <= 1:
        return 1
    return iter ** 2
# ---
def prepare_stage(self, last: int) -> Iterable[Tuple[int, Any]]:
        """Propagate current stage to Mappables for parallel execution."""
        assert (last + 1) == self.step, (
            f"stages are executing out of order! On step {self.step!r}."
        )
        return zip(itertools.repeat(self.step), cast(Iterable, self.stage.mappable))
# ---
def __init__(self):
                self.name = "instance-0001"
                self.uuid = "1-2-3-4-5"
# ---
def __getattr__(self, method_name: str) -> _IrisActorMethod:
        if method_name.startswith("_"):
            raise AttributeError(method_name)
        return _IrisActorMethod(self, method_name)
# ---


def sum_to_n(n: int):
    """sum_to_n is a function that sums numbers from 1 to n.
    >>> sum_to_n(30)
    465
    >>> sum_to_n(100)
    5050
    >>> sum_to_n(5)
    15
    >>> sum_to_n(10)
    55
    >>> sum_to_n(1)
    1
    """
    return sum(range(n + 1))
# ---
def mlp(block: TreeDiffusionBlockParams, x: Float[Array, "B S D"]) -> Float[Array, "B S D"]:
    """SwiGLU MLP."""
    gate = jnp.einsum("bsh,hm->bsm", x, block.mlp_gate)
    up = jnp.einsum("bsh,hm->bsm", x, block.mlp_up)
    activated = jax.nn.silu(gate) * up
    return jnp.einsum("bsm,mh->bsh", activated, block.mlp_down)
# ---
def capsule_type_chooser(method, notebook, data):
    return {'username': SAAGIE_USERNAME}
# ---
def global_avg(self):
        return self.total / self.count
# ---
def get_binary_path(build_dir_path):
    return os.path.join(build_dir_path, 'local', 'bin', 'jerry')
# ---

def count_upper(s):
    """
    Given a string s, count the number of uppercase vowels in even indices.
    
    For example:
    count_upper('aBCdEf') returns 1
    count_upper('abcdefg') returns 0
    count_upper('dBBE') returns 0
    """
    count = 0
    for i in range(0,len(s),2):
        if s[i] in "AEIOU":
            count += 1
    return count
# ---
def __eq__(self, other): return True
# ---
def check_permutation(str1, str2):
  n1=len(str1)
  n2=len(str2)
  if(n1!=n2):
    return False
  a=sorted(str1)
  str1=" ".join(a)
  b=sorted(str2)
  str2=" ".join(b)
  for i in range(0, n1, 1):
    if(str1[i] != str2[i]):
      return False
  return True
# ---
def upload_to_gcs(local_path: str, gcs_path: str) -> None:
    """
    Uploads a folder `local_path` to Google Cloud Storage (GCS).
    """
    print(f"Uploading {local_path}.")
    fs = fsspec.filesystem("gcs")
    # The slash is needed to upload the contents of the folder to `gcs_path`
    fs.put(local_path + "/", gcs_path, recursive=True)
    logger.info(f"Uploaded {local_path} to {gcs_path}.")
# ---
def check_for_unexpected_keys(name, input_dict, expected_values):
  unknown = set(input_dict.keys()).difference(expected_values)
  if unknown:
    raise ValueError('Unknown entries in {} dictionary: {}. Only expected '
                     'following keys: {}'.format(name, list(unknown),
                                                 expected_values))
# ---
def __add__(self, other: "Duration") -> "Duration":
        return Duration(self._ms + other._ms)
# ---
def test_teardown_capturing_final(self, testdir):
        p = testdir.makepyfile("""
            def teardown_module(mod):
                print ("teardown module")
                assert 0
            def test_func():
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*def teardown_module(mod):*",
            "*Captured stdout*",
            "*teardown module*",
            "*1 error*",
        ])
# ---
def __getattr__(self, method_name: str) -> "_RpcMethod":
        return _RpcMethod(self, method_name)
# ---
def __init__(self, perm, error_view=None):
        self.perm = perm
        self.error_view = error_view
# ---
def _feature_dtype(feature):
    """Recursively extract a dtype string from a datasets Feature."""
    dtype = getattr(feature, "dtype", None)
    if dtype is not None:
        return str(dtype)
    nested = getattr(feature, "feature", None)
    if nested is not None:
        return f"list[{_feature_dtype(nested)}]"
    return str(feature)
# ---
def ensure_versioned(value: VersionedValue[T_co] | T_co) -> VersionedValue[T_co]:
    """
    Ensure that the value is wrapped in a VersionedValue. If it is already wrapped, return it as is.
    """
    return value if isinstance(value, VersionedValue) else VersionedValue(value)
# ---
def output_path_of(step: ExecutorStep, name: str | None = None) -> InputName:
    return InputName(step=step, name=name)
# ---
def test_invalid_absolute_url(self):
        """Test that absolute URLs are rejected."""
        with pytest.raises(ValidationError, match="Absolute urls are not supported"):
            UnresolvedLocation(path="s3://bucket/path")
# ---
def broadcast_msg(server, ns_name, event, *args):
    pkt = dict(type="event",
               name=event,
               args=args,
               endpoint=ns_name)

    for sessid, socket in server.sockets.iteritems():
        socket.send_packet(pkt)
# ---
def protect(self, tag: str) -> None:
        """No-op for local provider (no eviction)."""
        del tag
# ---
def iris_ctx() -> IrisContext:
    """Get the current IrisContext, raising if not in a job.

    Returns:
        Current IrisContext

    Raises:
        RuntimeError: If not running inside an Iris job
    """
    ctx = get_iris_ctx()
    if ctx is None:
        raise RuntimeError("iris_ctx() called outside an Iris job (no job info available)")
    return ctx
# ---
def _write_jsonl(path: Path, rows: list[dict]) -> None:
    with path.open("w", encoding="utf-8") as handle:
        for row in rows:
            handle.write(json.dumps(row) + "\n")
# ---
def _slice_length(start: int, stop: int, step: int) -> int:
    if step == 0:
        raise ValueError("slice step cannot be zero")
    if step > 0:
        if start >= stop:
            return 0
        return (stop - start + step - 1) // step
    if start <= stop:
        return 0
    step_abs = -step
    return (start - stop - step - 1) // step_abs
# ---
def std(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.std, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def total_chunks(shape: T_Shape, chunks: T_RegularChunks) -> int:
    # cf rechunker's chunk_keys
    return prod(ceil(s / c) for s, c in zip(shape, chunks))
# ---
def coerce(xs, arg, fn):
        if not isinstance(arg, list):
            arg = [arg] * len(xs)
        return [fn(x.ndim, a) for x, a in zip(xs, arg)]
# ---


def fib(n: int):
    """Return n-th Fibonacci number.
    >>> fib(10)
    55
    >>> fib(1)
    1
    >>> fib(8)
    21
    """
    if n == 0:
        return 0
    if n == 1:
        return 1
    return fib(n - 1) + fib(n - 2)
# ---
def count_Squares(m,n): 
    if (n < m): 
        temp = m 
        m = n 
        n = temp 
    return n * (n + 1) * (3 * m - n + 1) // 6
# ---
def fast_job(n):
            return n * 2
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> ListCache[KvPageCache]:
        caches = [layer.initial_cache(spec, dtype=dtype) for layer in self.layers.unstacked()]
        return ListCache(caches)
# ---
def getposition(self, data):
        return self.positions[data]
# ---
def __init__(self, xPos, yPos):
        super(Block, self).__init__(xPos, yPos)
        self.state = "visible"
# ---
def embed(self, input_ids, *, key, pos_ids: NamedArray):
        input_embeds = self.token_embeddings(input_ids)
        position_embeds = self.position_embeddings.embed(pos_ids)
        x = input_embeds + position_embeds
        x = self.dropout(x, key=key)

        return x
# ---
def LogPABotMessage(message):
    _pabotlog.info(message)
# ---
def parse_date(date_str):
    """Parse elastic datetime string."""
    try:
        date = arrow.get(date_str)
    except TypeError:
        date = arrow.get(date_str[0])
    return date.datetime
# ---
def wait(self) -> None:
        """Block until all threads have exited."""
        with self._lock:
            children = list(self._children)
            threads = list(self._threads)

        for child in children:
            child.wait()
        for thread in threads:
            thread.join()
# ---
def batch_axis_at_step(self, step: int) -> Axis:
        bs = value_at_step(self.train_batch_size, step)
        return Axis(self.batch_axis_name, bs)
# ---
def sum_column(list1, C):
    result = sum(row[C] for row in list1)
    return result
# ---
def genetic_modification_11(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'disruption',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def delete(id) :
    db = cherrypy.session['database']
    try:
        con = lite.connect( db )
        cur = con.cursor()
        sql =  "DELETE from Radio WHERE id = '%s'" % (id)
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def setGlobalForceParameter(force, key, value):
  for i in range(force.getNumGlobalParameters()):
    if force.getGlobalParameterName(i)==key:
      print('setting force parameter', key, '=', value)
      force.setGlobalParameterDefaultValue(i, value);
# ---
def test_rejects_auto_chunks(self):
    import dask_ee

    with self.assertRaises(NotImplementedError):
      dask_ee.read_ee('WRI/GPPD/power_plants', 'auto')
# ---
def editingCanceledKey(self, editable):
        """Stops user input of a Key for a selected key binding"""

        orca_state.capturingKeys = False
        self._capturedKey = []
        return
# ---
def fix_args():
  global get_next_file

  if PARAMS["PATH"][-1] != "/":
    PARAMS["PATH"] += "/"  

  if PARAMS["FORCE_NO_VFS"]:
    PARAMS["USE_VFS"]
  elif PARAMS["FORCE_NO_VFS"]:
    PARAMS["USE_VFS"]
  else:
    PARAMS["USE_VFS"] = path_is_jnetfs(PARAMS["PATH"]) 

  if not PARAMS["USE_VFS"]:    
    get_next_file = GetFileDir(PARAMS["PICK_RANDOM"]).get_next_file
  else:
    get_next_file = get_next_file_vfs
# ---
def _substitute_key_type():
        """Replaces key type with input_type."""
        # pylint: disable=unused-variable, invalid-name
        for __, value in input_type['keys'].items():
            value['type'] = input_types[value['type']]
# ---
def greater(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "greater")
    return elemwise(nxp.greater, x1, x2, dtype=nxp.bool)
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        client = self._handle._resolve()
        executor = _get_shared_executor()
        future = executor.submit(lambda: getattr(client, self._method)(*args, **kwargs))
        return future
# ---
def show_logs(ctx, tail):
    """View cluster logs."""
    log_command = f"tail -n {tail} -f /tmp/ray/session_latest/logs/monitor*"
    subprocess.run(
        _maybe_add_ray_verbose(ctx.obj, ["ray", "exec", ctx.obj.config_file, log_command]),
        check=True,
    )
# ---
def test_dtype_and_axes_annotation():
    def foo(x: f32["batch embed"]):  # type: ignore  # noqa: F722
        pass

    ann = typing.get_args(typing.get_type_hints(foo, include_extras=True)["x"])
    assert ann[0] is NamedArray
    spec = ann[1]
    assert spec.dtype == jnp.float32
    assert spec.before == ("batch", "embed")
# ---
def __init__(self, base_uri=None):
        super(BiophysicalPerisomaticApi, self).__init__(base_uri)
        self.cache_stimulus = True
        self.ids = {}
        self.sweeps = []
        self.manifest = {}
# ---
def test_write_vortex_empty(self, tmp_path):
        """Test writing empty dataset."""
        output_path = tmp_path / "empty.vortex"
        result = write_vortex_file([], str(output_path))

        assert result["count"] == 0
        assert output_path.exists()
# ---
def as_scanned_result(self, scan_axis: Axis):
        return NamedArray(self.array, (scan_axis,) + self.main_axes)
# ---
def pop(self, lease_timeout: float = 60.0) -> Lease[T_co] | None:
        self._recover_expired_leases()
        if self.queue:
            item = self.queue.pop(0)
            lease_id = str(uuid.uuid4())
            timestamp = time.time()
            lease = Lease(item, lease_id, timestamp)
            self.leases[lease_id] = (item, timestamp, lease_timeout)
            return lease
        return None
# ---
def output_shape_fn(input_shape):
      return input_shape
# ---
def _maybe_force_tokenizer_parallelism(tokenizer: PreTrainedTokenizerBase):
    if tokenizer.is_fast and os.getenv("TOKENIZERS_PARALLELISM") is None:
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
# ---
def model_type(self) -> Type["Olmo2LMHeadModel"]:
        return Olmo2LMHeadModel
# ---
def static_proxy(path):
    if os.environ.get("DEV") == "true":
        return proxy_to_dev_server(path)
    return send_from_directory(app.static_folder, path)
# ---
def get_ip():
  """Get primary IP (the one with a default route) of local machine.

  This works on both Linux and Windows platforms, and doesn't require working
  internet connection.
  """

  s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
  try:
    # doesn't even have to be reachable
    s.connect(('10.255.255.255', 1))
    return s.getsockname()[0]
  except:
    return '127.0.0.1'
  finally:
    s.close()
# ---
def _F(self, right: np.ndarray, key: np.uint64) -> np.ndarray:
        """A simple round function that mixes the right half.

        Operates modulo 2^(right_bits).
        """
        masked_right = right & self.R_mask
        return (masked_right * np.uint64(2654435761) + key) & self.R_mask
# ---
def on_activate():
            """The window was activated.

            This event can be triggered by clicking on the title bar, bringing
            it to the foreground; or by some platform-specific method.

            When a window is "active" it has the keyboard focus.

            :event:
            """
# ---
def _node_list(cls):
        return next((os.environ[o] for o in _NODE_LIST_CHOICES if o in os.environ), None)
# ---
def preprocess_request(req: dict[str, Any]) -> dict[str, Any]:
    """Remove metadata fields from request."""
    return {k: v for k, v in req.items() if not k.startswith("_")}
# ---
def test_spawn_glance(self):
        stubs.stubout_fetch_image_glance_disk(self.stubs)
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_MACHINE,
                         glance_stubs.FakeGlance.IMAGE_KERNEL,
                         glance_stubs.FakeGlance.IMAGE_RAMDISK)
        self.check_vm_params_for_linux_with_external_kernel()
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return df.four.nunique()
# ---
def t(key, shape: AxisSpec, df: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    df = broadcast_to(df, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.t(key, df.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def test_equal_3(self):
        self.assertEqual(string_color('Joshua Smith'), '8F00FB')
# ---
def unembed(self, x: NamedArray):
        return hax.dot(x, self.token_embeddings.weight, axis="embed")
# ---
def modal(method, notebook, data):
    return {}
# ---
def reject_all(self, include_accepted=False):
        self._call_all('reject_all', include_accepted)
# ---
def connect(self, *args, **kwargs):
        return self.__iowait(self._connection.connect, *args, **kwargs)
# ---
def _sample_categorical(categorical_probs):
  gumbel_norm = (
    1e-10
    - (torch.rand_like(categorical_probs) + 1e-10).log())
  return (categorical_probs / gumbel_norm).argmax(dim=-1)
# ---
def __truediv__(self, other, /):
        other = self._check_allowed_dtypes(other, "floating-point", "__truediv__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.divide, self, other, dtype=result_type(self, other))
# ---
def test_rule_300(self):
        oRule = iteration_scheme.rule_300()
        self.assertTrue(oRule)
        self.assertEqual(oRule.name, 'iteration_scheme')
        self.assertEqual(oRule.identifier, '300')

        lExpected = [13, 17]

        oRule.analyze(self.oFile)
        self.assertEqual(lExpected, utils.extract_violation_lines_from_violation_object(oRule.violations))
# ---
def main():
    if os.getenv("CI", None) is not None:
        logger.info("Skipping experiment execution on CI environment, needs HF access.")
        return

    runs = [
        build_config("130m"),
        build_config("300m"),
        build_config("520m"),
        build_config("1_2b"),
    ]

    steps = []
    for name, cfg in runs:
        cfg.print_run_info()
        steps.extend(default_speedrun(name, cfg))

    executor_main(steps=steps, description="AdamH speedruns (Chinchilla optimal)")
# ---
def unload(self):
        """Unload the inference model to free up resources."""
        logger.info("Unloading inference model...")
        with self.model_lock:
            self.model = None
            self.engine = None  # type: ignore[assignment]
        logger.info("Inference model unloaded.")
# ---
def _remove_job(job_id):
    with _lock:
        if job_id not in _jobs:
            raise NoSuchJob("No such job %r" % job_id)
        del _jobs[job_id]
# ---
def update_replicas(self, replicas):
        ''' update replicas value '''
        self.put(DeploymentConfig.replicas_path, replicas)
# ---
def default_error_handler(ec, *args):
        return ValueError(_mpv_error_string(ec).decode('utf-8'), ec, *args)
# ---
def publisher(self):
        'Publisher name. (optional)'
        try:
            return self.opt_meta['publisher']
        except KeyError:
            return None
# ---
def cumprod(x, axis=0):
    return _Ncumprod(x, axis)
# ---
def test_contains_unescaped(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b%cde"), {1, 2, 3, 4, 5, 6, 7, 8, 9})
# ---
def key_function(out_key):
        out_coords = out_key[1:]
        in_coords = tuple(
            bi // repeats if i == axis else bi for i, bi in enumerate(out_coords)
        )
        return ((x.name, *in_coords),)
# ---
def openshift_installed():
        ''' check if openshift is installed '''
        import yum

        yum_base = yum.YumBase()
        if yum_base.rpmdb.searchNevra(name='atomic-openshift'):
            return True

        return False
# ---
def write_corpus(programs: list[str], output_path: Path) -> None:
    """Write programs to a corpus file, separated by '# ---' sentinel lines.

    This separator allows programs to contain internal blank lines,
    unlike the previous blank-line-separated format.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for i, prog in enumerate(programs):
            if i > 0:
                f.write(CORPUS_SEPARATOR + "\n")
            f.write(prog.rstrip("\n"))
            f.write("\n")
# ---
def _get_scheduling_strategy(self, resource_config: ResourceConfig | None):
        if resource_config is None:
            return None
        return get_scheduling_strategy(resource_config)
# ---
def gating_init_(weights):
    with torch.no_grad():
        weights.fill_(0.0)
# ---
def start(self):
        self._thread = concurrent.thread(self._run, name="v2v/" + self._id[:8])
        self._thread.start()
# ---
def on_predict_start(self, trainer, pl_module):
        """Called when prediction starts - update all progress bars."""
        super().on_predict_start(trainer, pl_module)
        self._update_all_progress_bars()
# ---
def opposite_Signs(x,y): 
    return ((x ^ y) < 0);
# ---
def test_open(self):
        self.db = smadata2.db.SQLiteDatabase(self.dbname)
# ---
def bitwise_xor(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_xor](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_xor.html)
    """
    return jnp.bitwise_xor(x1, x2)
# ---
def initial_prognostic(self):
        x_index = self._get_x_index(0)
        data_in = self._get_prognostic(x_index)
        return data_in
# ---
def start(self) -> None:
        app = self._create_app()
        self.server = make_server("localhost", self.proxy_port, app, threaded=True)
        self.thread = threading.Thread(target=self.server.serve_forever, daemon=True)
        self.thread.start()
        logger.info("Started Ray dashboard proxy on http://localhost:%d", self.proxy_port)
# ---
def stop_tokens(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    stop_tokens = tokenizer.eos_token_id
    print("STOP", tokenizer.eos_token, stop_tokens)
    return [stop_tokens]
# ---
def swap_count(s):
	chars = s
	count_left = 0
	count_right = 0
	swap = 0
	imbalance = 0; 
	for i in range(len(chars)):
		if chars[i] == '[':
			count_left += 1
			if imbalance > 0:
				swap += imbalance
				imbalance -= 1
		elif chars[i] == ']':
			count_right += 1
			imbalance = (count_right - count_left) 
	return swap
# ---
def url(self) -> str:
        return f"http://{self._config.host}:{self._config.port}"
# ---
def __eq__(self, other):
        if other is None:
            return False
        return self.platform_id == other.platform_id and self.id == other.id
# ---
def set_dirty(self):
        self.dirty = True
# ---
def test_same_expr_same_hash(self):
        expr1 = col("score") > 0.5
        expr2 = col("score") > 0.5
        assert hash(expr1) == hash(expr2)
# ---
def test_worker_delayed_registration(cluster):
    """Worker registration delayed by 5s on first attempt. Task pends, then
    schedules once registration completes."""
    _url, client = cluster
    enable_chaos("worker.register", delay_seconds=5.0, max_failures=1)
    job = submit(client, _quick, "delayed-reg")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def controller_address(self) -> str:
        """Get controller address from bootstrap config, if set.

        Returns:
            Controller address string, or empty string if not configured
        """
        # TODO: Derive controller address from controller.manual/local when unset.
        bootstrap = self._proto.defaults.bootstrap
        if bootstrap.HasField("controller_address"):
            return bootstrap.controller_address
        return ""
# ---
def kill(self, job_or_id) -> None:
        job_id = self._to_job_id_str(job_or_id)
        request = cluster_pb2.Controller.TerminateJobRequest(job_id=job_id)
        assert self._controller_client is not None
        self._controller_client.terminate_job(request)
# ---
def test_deadline_remaining_time():
    """Deadline correctly reports remaining time."""
    deadline = Deadline.from_seconds(1.0)

    # Initially should have close to 1 second remaining
    remaining = deadline.remaining_seconds()
    assert 0.95 < remaining <= 1.0

    # After sleeping, remaining time should decrease
    time.sleep(0.1)
    remaining_after = deadline.remaining_seconds()
    assert remaining_after < remaining
    assert 0.85 < remaining_after < 0.95
# ---
def current_in_milliamps(self, value):
        self.current = value / 1000
# ---
def get_caller_path() -> str:
    """Return the path of the file that called this function."""
    return inspect.stack()[-1].filename
# ---
def forward_once(self, x):
        return x[:, : self.out_channels] * 10.0 + x[:, -1]
# ---
def __call__(self, indices: int) -> int:
        """Apply the permutation to a single integer.

        Args:
            indices: An integer to be permuted.

        Returns:
            The permuted value.
        """
        ...
# ---
def final_init_(weights):
    with torch.no_grad():
        weights.fill_(0.0)
# ---
def _get_random_inputs(config, override_Pos=None):
    """Generate random inputs for testing."""
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()

    return x, mask
# ---
def setUp(self):
        self._mock_multiplexer = mock.create_autospec(
            plugin_event_multiplexer.EventMultiplexer
        )
        self._mock_tb_context = base_plugin.TBContext(
            multiplexer=self._mock_multiplexer
        )
# ---
def visitdir(self, dir):
        matched = self._matcher.match_recursive(dir)
        if matched is None:
            return True
        elif matched is True:
            return "all"
        else:
            assert matched is False
            return False
# ---
def with_group(self, release_time=0.25):
        import supriya.patterns

        return supriya.patterns.Pgroup(self, release_time=release_time)
# ---
def remove_biblio(html: BeautifulSoup):
    # Remove the biblio since there is a lot of noise
    biblio = html.findAll("section", {"id": "bib"})
    for bib in biblio:
        bib.decompose()
# ---
def test_join_file_workers():
    producers = [Mock()]
    cworker = Mock()
    consumers = [Mock()]

    classify_documents._join_file_workers(cworker, producers, consumers)

    for p in producers:
        assert p.join.called
    assert cworker.set_file_producers_done.called
    for c in consumers:
        assert c.join.called
    assert cworker.clear_file_producers_done.called
# ---
def convert_integer_index_to_slice(ia):
            if isinstance(ia, ndindex.Integer):
                return ndindex.Slice(ia.raw, ia.raw + 1)
            return ia
# ---
def quick_task(task_id: int):
        import time as time_module

        time_module.sleep(1.0)
        print(f"Task {task_id} completed")
        return task_id
# ---
def second_smallest(numbers):
  if (len(numbers)<2):
    return
  if ((len(numbers)==2)  and (numbers[0] == numbers[1]) ):
    return
  dup_items = set()
  uniq_items = []
  for x in numbers:
    if x not in dup_items:
      uniq_items.append(x)
      dup_items.add(x)
  uniq_items.sort()    
  return  uniq_items[1]
# ---
def __post_init__(self):
        if len(self.axes) == 0:
            raise ValueError("Empty axes not allowed")
# ---
def is_pallas_dslice(x: object) -> bool:
    return isinstance(x, Slice)
# ---
def SetLocationList( diagnostics ):
  """Populate the location list with diagnostics. Diagnostics should be in
  qflist format; see ":h setqflist" for details."""
  vim.eval( 'setloclist( 0, {0} )'.format( json.dumps( diagnostics ) ) )
# ---
def _strip_properly_formatted_commas(expr: str):
    # We want to be careful because we don't want to strip tuple commas
    p1 = re.compile("(\\d)(,)(\\d\\d\\d)($|\\D)")
    while True:
        next_expr = p1.sub("\\1\\3\\4", expr)
        if next_expr == expr:
            break
        expr = next_expr
    return next_expr
# ---
def data_size(self):
        # return int(self.offsets[self.num_rows].read().result())
        if self._cached_data_size is not None:
            return self._cached_data_size
        result = int(self.offsets[self.num_rows].read().result())
        if self._cache_metadata:
            self._cached_data_size = result
        return result
# ---
def test_std_functional(self, testdir, opt):
        reprec = testdir.inline_runsource("""
            def test_hello(capsys):
                print (42)
                out, err = capsys.readouterr()
                assert out.startswith("42")
        """, *opt)
        reprec.assertoutcome(passed=1)
# ---
def _make_starttag(tag, attrs):
    'Write a starttag.'
    out = '<' + tag
    for key in attrs:
        out += ' {}="{}"'.format(key, html.escape(attrs[key]))
    out += '>'
    return out
# ---
def rearrange_numbs(array_nums):
  result = sorted(array_nums, key = lambda i: 0 if i == 0 else -1 / i)
  return result
# ---
def backwards(self, orm):
        # Deleting field 'Idea.color'
        db.delete_column(u'brainstorming_idea', 'color')
# ---
def _flush_while_waiting(event):
    def flush_stdout():
        sys.stdout.flush()
        sys.stderr.flush()
        time.sleep(5)
        while not event.is_set():
            print("Waiting...", flush=True)
            print("\n", file=sys.stderr, flush=True)
            time.sleep(5)

    thread = threading.Thread(target=flush_stdout)
    thread.start()
# ---
def get_stub(self, address: str) -> WorkerClient:
        """Get a worker stub for the given address.

        Args:
            address: Worker address in "host:port" format

        Returns:
            A WorkerClient stub for making RPC calls
        """
        ...
# ---


def flip_case(string: str) -> str:
    """ For a given string, flip lowercase characters to uppercase and uppercase to lowercase.
    >>> flip_case('Hello')
    'hELLO'
    """
    return string.swapcase()
# ---
def fmod(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.fmod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fmod.html)
    """
    return jnp.fmod(x1, x2)
# ---
def link(self, link, title, text):
        """Rendering a given link with content and title.

        :param link: href link for ``<a>`` tag.
        :param title: title content for `title` attribute.
        :param text: text content for description.
        """
        link = escape_link(link)
        if not title:
            return '<a href="%s">%s</a>' % (link, text)
        title = escape(title, quote=True)
        return '<a href="%s" title="%s">%s</a>' % (link, title, text)
# ---
def training_step(self, batch, batch_idx):
    loss = self._compute_loss(batch, prefix='train')
    self.log(name='trainer/loss',
             value=loss.item(),
             on_step=True,
             on_epoch=False,
             sync_dist=True)
    return loss
# ---
def log(self, message: str, level: str = "INFO"):
        now = datetime.now()
        elapsed = time.monotonic() - self._start_time
        line = f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] [{elapsed:8.1f}s] [{level}] {message}"
        print(line, flush=True)
        self._file.write(line + "\n")
        self._file.flush()
# ---
def _initialize_global_tracker(config, run_id):
    if isinstance(config, Sequence):
        tracker = levanter.tracker.CompositeTracker([c.init(run_id) for c in config])
    else:
        tracker = config.init(run_id)

    levanter.tracker.set_global_tracker(tracker)
# ---
def _to_list(arr) -> list:
    """Convert array-like object to list, handling both JAX arrays and Python lists."""
    if isinstance(arr, list):
        return arr
    elif hasattr(arr, "tolist"):
        return arr.tolist()
    else:
        # Fallback for other array types
        return list(arr)
# ---
def floor_Max(A,B,N):
    x = min(B - 1,N)
    return (A*x) // B
# ---
def _record_id(record: dict) -> str:
    if "id" in record:
        return record["id"]
    else:
        # compute hash of the msgspec serialization of the record
        s = msgspec.msgpack.encode(record, order="deterministic")
        return str(_bloom_hash(s))
# ---
def last_Digit(n) :
    return (n % 10)
# ---
def is_alive(self) -> bool:
        return self._thread.is_alive()
# ---
def output_field(self):
        return TimeField()
# ---
def dec(rc):
                    page = pages_row["page", i].scalar()
                    return rc.at["page", page].add(-1)
# ---
def send_css(filename):
    return static_file(filename, root='css')
# ---
def wait(self) -> None:
        """Block until the server exits."""
        self._threads.wait()
# ---
def broadcaster():
        while True:
            with lock:
                received = broadcast_one_to_all(status["val"], is_source=is_leader)
                status["val"] = received
                if received != 0:
                    break

            time.sleep(10)

        return status["val"]
# ---
def testRaiseAgain(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        try:
          try:
            raise AssertionError('foo')
          except AssertionError:
            raise
        except Exception as e:
          print e""")))
# ---
def broadcast_axis(self, axis: AxisSpec) -> "NamedArray":  # pragma: no cover
        return haliax.broadcast_axis(self, axis=axis)
# ---
def _ax_name(ax: AxisSelector) -> str:
        if isinstance(ax, Axis):
            return ax.name
        else:
            return ax
# ---
def on_line(line: str) -> None:
            logger.info("[%s] %s", vm_name, line)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        from fray.v2.device_flops import device_flops

        flops = device_flops(self.variant, dtype)
        if flops is None:
            raise ValueError(f"Unknown device/dtype: {self.variant}/{dtype}")
        return flops
# ---
def test_not_comparison(self):
        expr = ~(col("score") > 50)
        assert expr.evaluate({"score": 60}) is False
        assert expr.evaluate({"score": 40}) is True
# ---
def recipient(self, record):
        return record['person'].email
# ---
def to_dict(self):
        return {
            'ident': self.ident,
            'entity_id': self.entity_id,
            'entity_type': self.entity_type, }
# ---
def __new__(cls, path, json):
        if path in cls.CACHE:
            return cls.CACHE[path]
        cls.CACHE[path] = new = super(Notebook, cls).__new__(cls)
        return new
# ---
def hello_world():
        print("Hello from the cluster!")
        return 42
# ---
def test_linecol_to_offset_first_line():
    source = "hello world\nsecond line\n"
    assert _linecol_to_offset(source, 1, 0) == 0
    assert _linecol_to_offset(source, 1, 6) == 6
# ---
def add_item(self, item):
        self.model.add(item)
        self._items.append(item)
# ---
def _try_convert_int(val, default):
    # handles a few cases we see in the wild: the number, "number", 'number', number;
    # punting on percent and fraction
    try:
        return int(val)
    except ValueError:
        val = val.strip().replace('"', "").replace("'", "").replace(";", "").replace(",", "")
        try:
            return int(val)
        except ValueError:
            return default
# ---

def generate_integers(a, b):
    """
    Given two positive integers a and b, return the even digits between a
    and b, in ascending order.

    For example:
    generate_integers(2, 8) => [2, 4, 6, 8]
    generate_integers(8, 2) => [2, 4, 6, 8]
    generate_integers(10, 14) => []
    """
    lower = max(2, min(a, b))
    upper = min(8, max(a, b))

    return [i for i in range(lower, upper+1) if i % 2 == 0]
# ---
def _receive_payload(self):
        payload = broadcast_shard(
            self._dummy_batch,
            hax.partitioning.infer_resource_partitions(self._dummy_batch),
        )
        return payload
# ---
def bmarks():
    return_data = get_bmarklet()
    return return_data
# ---
def _norm(v: Union[str, Sequence[str]]) -> Union[str, Tuple[str, ...]]:
    if isinstance(v, (list, tuple)):
        v = tuple(v)
        return v if len(v) > 1 else v[0]
    return v
# ---
def on_train_epoch_start(self):
    self.backbone.train()
    self.noise.train()
# ---
def get_wandb_key(self) -> str:
        return f"{self.metric_name}/{self.var_name}"
# ---
def remove_similar_row(test_list):
  res = set(sorted([tuple(sorted(set(sub))) for sub in test_list]))
  return (res)
# ---
def _get_caption(self, key: str, name: str, vmin: float, vmax: float) -> str:
        if name in self._metadata:
            caption_name = self._metadata[name]["long_name"]
            units = self._metadata[name]["units"]
        else:
            caption_name, units = name, "unknown_units"
        caption = self._image_captions[key].format(name=caption_name, units=units)
        caption += f" vmin={vmin:.4g}, vmax={vmax:.4g}."
        return caption
# ---
def wait_until_status(client, job_id, status_to_wait_for, timeout_seconds=5):
    start = time.time()
    while time.time() - start <= timeout_seconds:
        status = client.get_job_status(job_id)
        print(f"status: {status}")
        if status in status_to_wait_for:
            break
        time.sleep(1)

    return status
# ---
def with_bus(self, calculation_rate="audio", channel_count=None, release_time=0.25):
        import supriya.patterns

        return supriya.patterns.Pbus(
            self,
            calculation_rate=calculation_rate,
            channel_count=channel_count,
            release_time=release_time,
        )
# ---
def _base_plot(
            ax,
            base_data,
            timestamp,
            timestep_value,
            framedim,
            plotmethod=None,
            **kwargs,
        ):
            data = base_data.isel({framedim: timestamp})
            p = _core_plot(ax, data, plotmethod=plotmethod, **kwargs)
            return p
# ---
def perform(self):
        """Performs all stored actions."""
        for action in self._actions:
            action()
# ---
def _files(self):
        return []
# ---
def __init__(
        self, plotly_name="bordercolor", parent_name="sankey.hoverlabel", **kwargs
    ):
        super(BordercolorValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            array_ok=kwargs.pop("array_ok", True),
            edit_type=kwargs.pop("edit_type", "calc"),
            **kwargs
        )
# ---
def iter_profile_artifacts(run: "wandb.apis.public.Run") -> Iterable["wandb.sdk.Artifact"]:
    for artifact in run.logged_artifacts():
        if artifact.type == ARTIFACT_TYPE:
            yield artifact
# ---
def forward(self, indices, sigma):
    c = None
    if self.temb_strategy is not None:
      c = F.silu(self.sigma_map(sigma))

    with torch.cuda.amp.autocast(dtype=torch.bfloat16):
      x = self.model(indices, time_embeds=c).logits

    return x
# ---
def is_valid_python(source: str) -> bool:
    try:
        ast.parse(source)
        return True
    except SyntaxError:
        return False
# ---
def skip_if_not_enough_devices(count: int, reason: str | None = None):
    msg = f"Not enough devices ({len(jax.devices())})"
    if reason:
        msg += f": {reason}"
    return pytest.mark.skipif(len(jax.devices()) < count, reason=msg)
# ---
def slice(self, *args, **kwargs) -> "NamedArray":  # pragma: no cover
        return haliax.slice(self, *args, **kwargs)
# ---
def compute_ray_retry_count(request: JobRequest) -> int:
    """Map separate failure/preemption retry counts to a single Ray retry count."""
    return request.max_retries_failure + request.max_retries_preemption
# ---
def perfect_squares(a, b):
    lists=[]
    for i in range (a,b+1):
        j = 1;
        while j*j <= i:
            if j*j == i:
                 lists.append(i)  
            j = j+1
        i = i+1
    return lists
# ---
def test_matmul(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    c = xp.astype(a, xp.float32)
    d = xp.astype(b, xp.float32)
    e = xp.matmul(c, d)
    run_operation(tmp_path, executor, "matmul", e)
# ---
def test_arange_step(spec):
    a = xp.arange(20, step=3, chunks=(5,), spec=spec)
    assert_array_equal(a.compute(), np.arange(20, step=3))
# ---
def make_blockwise_spec(
    key_function,
    function,
    function_nargs=1,
    num_input_blocks=(1,),
    num_output_blocks=(1,),
    iterable_input_blocks=(False,),
):
    return BlockwiseSpec(
        key_function=key_function,
        function=function,
        function_nargs=function_nargs,
        num_input_blocks=num_input_blocks,
        num_output_blocks=num_output_blocks,
        iterable_input_blocks=iterable_input_blocks,
        reads_map={},  # unused
        writes_map={},  # unused
    )
# ---
def is_resource_deleted(self, id):
        try:
            self.show_image(id)
        except lib_exc.NotFound:
            return True
        return False
# ---
import math
def otherside_rightangle(w,h):
  s=math.sqrt((w*w)+(h*h))
  return s
# ---
def test_serialization(self):
        """Test that UnresolvedLocation serializes to just the path string."""
        loc = UnresolvedLocation(path="data/test.zarr")
        assert loc.model_dump() == "data/test.zarr"
# ---
def get_time():
    return test_time
# ---
def config(self):
        """A GL config describing the context of this window.  Read-only.

        :type: `pyglet.gl.Config`
        """
        return self._config
# ---
def __init__(self):
        'Initialize.'
        self.meta = []
        self.filelist = []
# ---
def get_citizenship(self, nick):
        return self.data[nick.lower()]['citizenship']
# ---
def _create_resource_impl(self, obj, wrapped=False):
        Util.validate_type(wrapped, "bool")
        return LicenseInfo(self._client, obj, wrapped)
# ---
def dataloader_iterator(
    loader,
    *,
    seq_len: int,
) -> Iterator[dict[str, jax.Array]]:
    while True:
        batch = next(loader)
        tokens = batch[:, :seq_len]
        yield {"tokens": tokens[:, :-1], "labels": tokens[:, 1:]}
# ---
def model_type(self):  # noqa: D401  property returns type, not a str
        """Return the Levanter model class for Gemma-2."""
        return Gemma2LMHeadModel
# ---
def _check_env_closure(output_path: str):
        with open(output_path, "w") as f:
            json.dump(dict(os.environ), f)
# ---
def extract_run_name_from_path(path: str) -> str:
    """Extract run name (last component) from a checkpoint path.

    E.g., 'gs://bucket/checkpoints/my-run-abc123' -> 'my-run-abc123'
    """
    return os.path.basename(path.rstrip("/"))
# ---
def modal_create_futures_func(function: Callable[..., Any]):
    def create_futures_func(input, **kwargs):
        return [
            (i, asyncio.ensure_future(function.remote.aio(i, **kwargs))) for i in input
        ]

    return create_futures_func
# ---
def get_template(name: str):  # type: ignore
        raise ImportError(msg) from exception
# ---
def wait_unit (self, uids, state=[DONE, FAILED, CANCELED], timeout=-1.0) :
        """
        Wait for given unit(s) to enter given state
        """

        raise Exception ("%s.wait_unit() is not implemented" % self.__class__.__name__)
# ---
def hashable_partition(pytree, filter_spec):
    dynamic, static = eqx.partition(pytree, filter_spec)
    static_leaves, static_treedef = jtu.tree_flatten(static)
    static_leaves = tuple(static_leaves)
    return dynamic, (static_leaves, static_treedef)
# ---
def search(arr,n) :
    XOR = 0
    for i in range(n) :
        XOR = XOR ^ arr[i]
    return (XOR)
# ---
def extent(self):
        return reduce(operator.add, [c.extent for c in self.coverages])
# ---
def _run_callable(entry: CallableEntrypoint) -> None:
    entry.callable(*entry.args, **entry.kwargs)
# ---
def __call__(
        self,
        lhs,
        rhs,
        dimension_numbers,
        precision: PrecisionLike = None,
        preferred_element_type: DTypeLike | None = None,
        **kwargs,
    ) -> jnp.ndarray:
        return jax.lax.dot_general(lhs, rhs, dimension_numbers, precision, preferred_element_type, **kwargs)
# ---
def test_manual_controller_start_requires_image(ssh_bootstrap_config: config_pb2.IrisClusterConfig):
    """start() requires image to be configured."""
    config = config_pb2.IrisClusterConfig()
    config.CopyFrom(ssh_bootstrap_config)
    config.controller.image = ""

    controller = ManualController(config)
    with pytest.raises(RuntimeError, match="image required"):
        controller.start()
# ---
def loss_ref(v):
        return jnp.sum(reference_impl_batched(v[None, :])[0])
# ---
def __init__(self):
        super(KanboardShell, self).__init__(
            description='Kanboard Command Line Client',
            version=app_version.VersionInfo('kanboard_cli').version_string(),
            command_manager=commandmanager.CommandManager('kanboard.cli'),
            deferred_help=True)
        self.client = None
        self.is_super_user = True
# ---
def stop(self) -> None:
        if self.server:
            self.server.shutdown()
        if self.thread:
            self.thread.join(timeout=5)
# ---
def __init__(self, inference_config: vLLMInferenceContextConfig):
        inference_config.mode = InferenceMode.ASYNC
        super().__init__(inference_config)
# ---
def modular_inverse(arr, N, P):
	current_element = 0
	for i in range(0, N):
		if ((arr[i] * arr[i]) % P == 1):
			current_element = current_element + 1
	return current_element
# ---
def _stop_task() -> None:
            attempt.should_stop = True
            if attempt.container_id:
                try:
                    self._runtime.kill(attempt.container_id, force=True)
                except RuntimeError:
                    pass
# ---
def path(self, path):
        """
        Sets the path of this ContributorOrcid.

        :param path: The path of this ContributorOrcid.
        :type: str
        """

        self._path = path
# ---
def is_cubed(self):
        """Return True if this stack frame is a Cubed call."""
        return self.module.startswith("cubed.") and not self.module.startswith(
            "cubed.tests."
        )
# ---
def _parse_arrow_schema(self):
        if self._schema:
            return

        self._schema = pyarrow.ipc.read_schema(
            pyarrow.py_buffer(self._first_message.arrow_schema.serialized_schema)
        )
        self._column_names = [field.name for field in self._schema]
        self._first_message = None
# ---
def empty_dit(list1):
 empty_dit=all(not d for d in list1)
 return empty_dit
# ---
def test_basic_rearrange_unordered():
    assert einops_rearrange(z, "{B D H W C} -> B H W D C").axes == (B, H, W, D, C)
    z_t = z.array.transpose((0, 2, 3, 1, 4))
    assert (einops_rearrange(z, "{B D H W C} -> B H W D C").array == z_t).all()

    assert einops_rearrange(z, "{C W H D B} -> B H W D C").axes == (B, H, W, D, C)
    assert (einops_rearrange(z, "{C W H D B} -> B H W D C").array == z_t).all()
# ---
def __ge__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "ge")
# ---
def __init__(self, ray_options: dict | None = None):
        """Initialize Ray context.

        Args:
            ray_options: Options to pass to ray.remote() (e.g., memory, num_cpus, num_gpus)
        """
        self.ray_options = ray_options or {}
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "x": 1, "y": 2, "q": "q1", "p": "p3"},
                {"id": 2, "x": 2, "y": 3, "q": "q2", "p": "p2"},
                {"id": 3, "x": 3, "y": 4, "q": "q3", "p": "p1"},
            ],
        )
# ---
def _inject_implicit_mixed_number(step: str):
    """
    Automatically make a mixed number evalable
    e.g. 7 3/4 => 7+3/4
    """
    p1 = re.compile("([0-9]) +([0-9])")
    step = p1.sub("\\1+\\2", step)  ## implicit mults
    return step
# ---
def greater(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.greater](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.greater.html)
    """
    return jnp.greater(x1, x2)
# ---
def worker_id() -> str:
    import threading

    return f"{os.uname()[1]}-{threading.get_ident()}"
# ---
def test_merge_chunks_fails(spec, target_chunks):
    a = xp.ones((10, 10), dtype=np.uint8, chunks=(2, 3), spec=spec)
    with pytest.raises(
        ValueError, match=r"Chunks .* must be a multiple of array's chunks"
    ):
        merge_chunks(a, target_chunks)
# ---
def evaluate(self, record: dict) -> bool:
        left_val = self.left.evaluate(record)
        right_val = self.right.evaluate(record)
        return _COMPARE_OPS[self.op](left_val, right_val)
# ---
def decode_from(self, container: NDArray[np.uint64] | np.uint64) -> NDArray:
        """Extracts the field from a uint64 container."""
        return (
            ((container >> self.offset) & self.mask())
            .astype(self.uint_type())
            .view(self.dtype)
        )
# ---
def test_hf_gradient():
    _compare_gpt2_checkpoint_gradients("gpt2", None)
# ---
def __bool__(self):
        return bool(self.reused_arrays or self.static_arrays)
# ---
def sort_sublists(input_list):
    result = [sorted(x, key = lambda x:x[0]) for x in input_list] 
    return result
# ---
def authenticated_userid(self, request):
		return self.match(request).authenticated_userid(request)
# ---
def update_num_lines(self):
        """
        Update the number of lines member.
        """
        self.num_lines = len(self.lines)
# ---
def test_outer(spec, executor):
    a = xp.asarray([0, 1, 2], chunks=2, spec=spec)
    b = xp.asarray([10, 50, 100], chunks=2, spec=spec)
    c = xp.linalg.outer(a, b)
    assert_array_equal(c.compute(executor=executor), np.outer([0, 1, 2], [10, 50, 100]))
# ---
def _var_func(a, correction=None, **kwargs):
    dtype = dict(kwargs.pop("dtype"))
    n = _numel(a, dtype=dtype["n"], **kwargs)
    mu = nxp.mean(a, dtype=dtype["mu"], **kwargs)
    M2 = nxp.sum(nxp.square(a - mu), dtype=dtype["M2"], **kwargs)
    return {"n": n, "mu": mu, "M2": M2}
# ---
def transform(self, x, static1, *, static2):
            assert static1 == 1.0
            assert static2 is False
            return x + self.w + static1
# ---
def pop_all(self) -> list[RolloutBatch]:
        """Pop all available batches without blocking."""
        with self._lock:
            batches = self._queue[:]
            self._queue.clear()
            self._not_full.notify_all()
            return batches
# ---
def init(config: LlamaConfig, *, key) -> "LlamaTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, LlamaDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return LlamaTransformer(config, layers, ln_f)
# ---
def server_url(self) -> str:
        if self.vllm_server is None:
            raise RuntimeError("vLLM server is not running in this environment.")
        return self.vllm_server.server_url
# ---
def build(self, output_dir: Path, device: torch.device) -> Profiler:
        if self.cuda_snapshot_frequency is not None and device.type != "cuda":
            raise ValueError(
                "cuda_snapshot_frequency is only supported on CUDA devices, got "
                f"{device.type}"
            )
        return Profiler(output_dir, self.cuda_snapshot_frequency)
# ---
def test_perturb_operators_comparison(rng):
    source = "a > b"
    found = False
    for seed in range(50):
        r = random.Random(seed)
        result = perturb_operators(source, r, swap_prob=1.0)
        if result is not None:
            found = True
            ast.parse(result)
            break
    assert found
# ---
def _signature(self):
    self._create_definition_if_needed()
    return self._op_def
# ---
def remove_climatology(ds):
    # Compute the climatology on the detrended data
    climatology = ds.groupby("time.dayofyear").mean("time").compute()

    # Remove the seasonal cycle (climatology) from the detrended data
    day_of_year = ds["time"].dt.dayofyear
    res = (ds - climatology.sel(dayofyear=day_of_year)).compute()

    return res
# ---
def _ordered_steps() -> list[str]:
    return list(
        name.removeprefix("step_") for name in Viz.__dict__ if name.startswith("step_")
    )
# ---
def check_k_elements(test_list, K):
  res = True
  for tup in test_list:
    for ele in tup:
      if ele != K:
        res = False
  return (res)
# ---
def HeadSize(self) -> Axis:
        return Axis("head_size", self.head_size)
# ---
def test_migrate_disk_and_power_off(self):
        instance = db.instance_create(self.context, self.instance_values)
        xenapi_fake.create_vm(instance.name, 'Running')
        instance_type = db.instance_type_get_by_name(self.context, 'm1.large')
        conn = xenapi_conn.get_connection(False)
        conn.migrate_disk_and_power_off(self.context, instance,
                                        '127.0.0.1', instance_type, None)
# ---
def c_noise(self, sigma):
        return (
            log(sigma / self.sigma_data) * 0.25
        )
# ---
import math
def is_not_prime(n):
    result = False
    for i in range(2,int(math.sqrt(n)) + 1):
        if n % i == 0:
            result = True
    return result
# ---
def test_repeat_vector(self):
    testing_utils.layer_test(
        keras.layers.RepeatVector, kwargs={'n': 3}, input_shape=(3, 2))
# ---
def concurrent_compute(
    *datasets: xr.Dataset,
    executor: ThreadPoolExecutor,
) -> None:
    def load_variable_data(var: xr.Variable) -> None:
        var.load()

    futures = []
    for ds in datasets:
        for var in ds.variables.values():
            futures.append(executor.submit(load_variable_data, var))

    wait(futures)
# ---
def __rtruediv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.true_divide(other, self)
# ---
def test_revert_cold_migration(self):
        """Test cold migrating server and then revert the migration"""
        self._test_cold_migrate_server(revert=True)
# ---
def ravel(array: NamedArray, new_axis_name: AxisSelector) -> NamedArray:
    """
    Returns a flattened view of the array, with all axes merged into one
    """
    flattened = flatten_axes(array, array.axes, new_axis_name)
    return flattened
# ---
def unauthenticated_userid(self, request):
		params = _parse_authorization(request, self.secret, self.realm)
		if params is None:
			return None
		if not _is_valid_nonce(params['nonce'], self.secret):
			_add_www_authenticate(request, self.secret, self.realm)
			return None
		return 'u:%s' % params['username']
# ---
def is_even(item):
            return (item % 2) == 0
# ---
def data_vars(self):
        """Dictionary of xray.DataArray objects corresponding to data variables
        """
        return Variables(self)
# ---
def first_repeated_char(str1):
  for index,c in enumerate(str1):
    if str1[:index+1].count(c) > 1:
      return c 
  return "None"
# ---
def on_compute_start(self, event):
        self.value = 0
# ---
def generate_id(n=16):
    alphabet = string.ascii_letters + string.digits
    return ''.join(random.choice(alphabet) for _ in range(n))
# ---
def bmarks():
    return_data = get_tags()
    return return_data
# ---
def create_container(self, config: ContainerConfig) -> str:
        container_id = f"local-{uuid.uuid4().hex[:8]}"
        self._containers[container_id] = _LocalContainer(
            config=config,
        )
        return container_id
# ---
def open(self, chunks: dict[str, int] | None = None) -> xr.Dataset:
        pass
# ---
def __init__(self, method: Callable, lock: threading.Lock, context):
        self._method = method
        self._lock = lock
        self._context = context
# ---
def _rotary_cache(seq_len: int, head_dim: int, rope: RotaryConfig) -> tuple[Float[Array, "S D"], Float[Array, "S D"]]:
    half_dim = head_dim // 2
    inv_freq = 1.0 / (rope.theta ** (jnp.arange(0, half_dim, dtype=jnp.float32) / half_dim))
    positions = jnp.arange(seq_len, dtype=jnp.float32)
    angles = positions[:, None] * inv_freq[None, :]
    cos = jnp.cos(angles)
    sin = jnp.sin(angles)
    return cos, sin
# ---
def __pow__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__pow__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.pow, self, other, dtype=result_type(self, other))
# ---
def force_assign(self, cr, uid, ids, context=None):
        """ Changes the state to assigned.
        @return: True
        """
        self.write(cr, uid, ids, {'state': 'assigned'})
        wf_service = netsvc.LocalService('workflow')
        for move in self.browse(cr, uid, ids, context):
            if move.picking_id:
                wf_service.trg_write(uid, 'stock.picking', move.picking_id.id, cr)
        return True
# ---
def start(self):
        """Start background streaming thread."""
        self._stop_event.clear()
        if self._mode == "controller":
            self._thread = threading.Thread(target=self._stream_controller, daemon=True)
        else:
            self._thread = threading.Thread(target=self._discover_and_stream_workers, daemon=True)
        self._thread.start()
# ---
def __init__(self, job, run_data):
        self.job = job
        self.id = run_data['id']
        self.status = run_data['status']
        self.stderr = run_data.get('logs_err', '')
        self.stdout = run_data.get('logs_out', '')
# ---
def find_angle(a,b):
 c = 180 - (a + b)
 return c
# ---
import math 
def first_Digit(n) : 
    fact = 1
    for i in range(2,n + 1) : 
        fact = fact * i 
        while (fact % 10 == 0) :  
            fact = int(fact / 10) 
    while (fact >= 10) : 
        fact = int(fact / 10) 
    return math.floor(fact)
# ---
def test_map_blocks_with_non_cubed_array(spec):
    a = xp.arange(10, dtype="int64", chunks=(2,), spec=spec)
    b = np.array([1, 2], dtype="int64")  # numpy array will be coerced to cubed
    c = cubed.map_blocks(nxp.add, a, b, dtype="int64")
    assert_array_equal(c.compute(), np.array([1, 3, 3, 5, 5, 7, 7, 9, 9, 11]))
# ---
def test_nested_field(self):
        expr = col("meta")["score"]
        assert expr.evaluate({"meta": {"score": 0.8}}) == 0.8
# ---
def multiply_int(x, y):
    if y < 0:
        return -multiply_int(x, -y)
    elif y == 0:
        return 0
    elif y == 1:
        return x
    else:
        return x + multiply_int(x, y - 1)
# ---
def set_channel(self, value):
        '''Set the channel of all events in this Track.
        '''
        if value not in range(1, 17):
            raise MidiException('bad channel value: %s' % value)
        for event in self.events:
            event.channel = value
# ---
def to_raw_shape(shape: Union[ShapeSpec, NamedShapeSpec]) -> Optional[Tuple[int, ...]]:
    if isinstance(shape, ShapeDtypeStruct):
        return shape.shape
    else:
        raw = shape.shape
        if raw is None:
            return None
        return tuple(ax.size for ax in raw)
# ---
def vm_manager(
        self,
        group_config: config_pb2.ScaleGroupConfig,
        vm_factory: TrackedVmFactory,
        *,
        dry_run: bool = False,
    ) -> VmManagerProtocol: ...
# ---
def _raw_weight(self, step: int) -> float:
        if step < self.switch_step:
            return 1.0 - self.beta
        t = step - self.switch_step
        frac = jnp.clip(t / self.decay_steps, 0.0, 1.0)
        return float(1.0 - jnp.sqrt(frac))
# ---
def _fromUtf8(s):
        return s
# ---
def main(ctx: click.Context, cluster: str):
    """Fray cluster job management."""
    ctx.ensure_object(dict)
    cluster_obj = create_cluster(cluster)
    ctx.with_resource(cluster_obj.connect())
    ctx.obj["cluster"] = cluster_obj
# ---
def write_xml(self):
        'Write the XML element.'
        return _make_xml_elem(self.tag, self.text, self.attr)
# ---
def testForElse(self):
    self.assertEqual((0, 'foo\nbar\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1,):
          print 'foo'
        else:
          print 'bar'""")))
# ---
def name(self, val):
        self.name_ctrl.SetValue(val)
# ---
def _receive_once():
            try:
                return self._transfer_client.receive_weights(self._policy_model)
            except Exception:
                logger.exception("Weight transfer client failed while receiving weights.")
                return None
# ---
def __setitem__(self, name, value, file_local=False):
        """ Get an option value """
        prefix = 'file-local-options/' if file_local else 'options/'
        return self._set_property(prefix+name, value)
# ---
def build(self, num_train_steps: int):
        raise NotImplementedError
# ---
def inc_shared(ref_counts):
            def body(i, rc):
                page = src_pages["page", i]

                def inc(rc):
                    return rc.at["page", page].add(1)

                return jax.lax.cond(is_valid(page).scalar(), inc, lambda x: x, rc)

            limit = used_pages - jnp.where(is_boundary, 0, 1)
            return jax.lax.fori_loop(0, limit, body, ref_counts)
# ---
def largest_subset(a, n):
	dp = [0 for i in range(n)]
	dp[n - 1] = 1; 
	for i in range(n - 2, -1, -1):
		mxm = 0;
		for j in range(i + 1, n):
			if a[j] % a[i] == 0 or a[i] % a[j] == 0:
				mxm = max(mxm, dp[j])
		dp[i] = 1 + mxm
	return max(dp)
# ---
def test_auto_metric_inference(name, expected_reduction):
    """auto_metric_from_name infers correct reduction type."""
    m = auto_metric_from_name(name, 42.0)
    assert m.reduction == expected_reduction
# ---
def tuple_to_set(t):
  s = set(t)
  return (s)
# ---
def get_children(self, job_id: JobName) -> list[ControllerJob]:
        with self._lock:
            return [job for job in self._jobs.values() if job.parent_job_id == job_id]
# ---
def is_fusable_with_predecessors(node_dict):
    """Return True if a node is a primitive op and can be fused with its predecessors."""
    return (
        is_primitive_op(node_dict)
        and node_dict["primitive_op"].fusable_with_predecessors
    )
# ---
def __call__(self, target, creds, enforcer):
        """Check that there is a matching role in the cred dict."""

        return self.match.lower() in [x.lower() for x in creds['roles']]
# ---
def test_engine_title_set():
    engine = salt.engines.Engine({}, "foobar.start", {}, {}, {}, {}, name="foobar")
    with patch("salt.utils.process.appendproctitle", MagicMock()) as mm:
        with pytest.raises(KeyError):
            # The method does not exist so a KeyError will be raised.
            engine.run()
        mm.assert_called_with("foobar")
# ---
def exp2(a: A) -> A:
    return wrap_elemwise_unary(jnp.exp2, a)
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        """Total peak FLOP/s across all GPUs."""
        return self.device_flops(dtype) * self.count
# ---
def default_device(self):
        return nxp.__array_namespace_info__().default_device()
# ---
def template_choose_axe_adaptors(best_adapt, best_size):
   if best_adapt:
       return "axe_adaptors\t" + best_adapt, ["--adaptor", best_adapt]
   else:
       return "axe_adaptors\tNA", ["--adaptor", "None"]
# ---
def test_repr(self):
        expr = col("meta")["score"]
        assert repr(expr) == "col('meta')['score']"
# ---
def split_image_and_tag(docker_base_image):
    if ":" in docker_base_image:
        base_image, base_tag = docker_base_image.rsplit(":", 1)
    else:
        base_image = docker_base_image
        base_tag = "latest"
    return base_image, base_tag
# ---
def from_text(cls, fname):
        vocab = np.genfromtxt(fname, dtype=str, delimiter=" ", usecols=0)
        clusters = np.genfromtxt(fname, dtype=int, delimiter=" ", usecols=1)
        return cls(vocab=vocab, clusters=clusters)
# ---
def update_fn(updates, state, params=None, **extra_args):
        del params

        def _clip_fn(u):
            clip_denom = hax.maximum(1.0, hax.sqrt(hax.mean(u * u)) / threshold)
            return u / clip_denom

        updates = scan_aware_tree_map(_clip_fn, updates)
        return updates, state
# ---
def _update_block_number_mapping(self, block, batch):
        block_number_mapping = qrl_pb2.BlockNumberMapping(headerhash=block.headerhash,
                                                          prev_headerhash=block.prev_headerhash)
        self._state.put_block_number_mapping(block.block_number, block_number_mapping, batch)
# ---
def tokenizer():
    """Create a tokenizer for testing."""
    return AutoTokenizer.from_pretrained("gpt2")
# ---
def get_legacy_sigopcount_tx(tx, accurate=True):
    count = 0
    for i in tx.vout:
        count += i.scriptPubKey.GetSigOpCount(accurate)
    for j in tx.vin:
        # scriptSig might be of type bytes, so convert to CScript for the moment
        count += CScript(j.scriptSig).GetSigOpCount(accurate)
    return count
# ---
def normal_init_(weights):
    torch.nn.init.kaiming_normal_(weights, nonlinearity="linear")
# ---
def wrap(*a, block_id=None, **kw):
            arrays = kw.pop("arrays")
            args = a + arrays
            return func(*args, block_id=block_id, **kw)
# ---
def test_capturing_readouterr_decode_error_handling(self):
        with self.getcapture() as cap:
            # triggered a internal error in pytest
            print('\xa6')
            out, err = cap.readouterr()
        assert out == py.builtin._totext('\ufffd\n', 'unicode-escape')
# ---


def max_element(l: list):
    """Return maximum element in the list.
    >>> max_element([1, 2, 3])
    3
    >>> max_element([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])
    123
    """
    m = l[0]
    for e in l:
        if e > m:
            m = e
    return m
# ---
def convert_to_export(self, value, env):
        """ convert ``value`` from the cache to a valid value for export. The
            parameter ``env`` is given for managing translations.
        """
        if not value:
            return ''
        return value if env.context.get('export_raw_data') else ustr(value)
# ---
def test_fdcapture_tmpfile_remains_the_same(tmpfile, use):
    if not use:
        tmpfile = True
    cap = StdCaptureFD(out=False, err=tmpfile)
    try:
        cap.start_capturing()
        capfile = cap.err.tmpfile
        cap.readouterr()
    finally:
        cap.stop_capturing()
    capfile2 = cap.err.tmpfile
    assert capfile2 == capfile
# ---
def get_cookie(self, name):
        return self.parse_cookie(name)
# ---
def testIfElif(self):
    self.assertEqual((0, 'foo\nbar\n'), _GrumpRun(textwrap.dedent("""\
        if True:
          print 'foo'
        elif False:
          print 'bar'
        if False:
          print 'foo'
        elif True:
          print 'bar'""")))
# ---
def norm(x, y):
    l = (x*x + y*y)**0.5
    return (x/l, y/l)
# ---
def _get_cluster_spec(self) -> str:
        if self._address == "auto":
            base = "ray"
        elif self._config_path:
            base = f"ray:{self._config_path}"
        else:
            base = "ray"

        # Append namespace as query param if present
        if self._namespace:
            base += f"?namespace={self._namespace}"

        return base
# ---
def profiles():
    """Gets all profiles for all elements for user application to display and manipulate elements"""
    return jsonify(home_services.get_profiles())
# ---
def subtract(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "subtract")
    if x1.dtype not in _numeric_dtypes or x2.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in subtract")
    return elemwise(nxp.subtract, x1, x2, dtype=result_type(x1, x2))
# ---
def tasks(self) -> list[Task]:
        """Get all tasks for this job.

        Returns:
            List of Task handles, one per task in the job
        """
        task_statuses = self._client._cluster_client.list_tasks(self._job_id)
        return [Task(self._client, JobName.from_wire(ts.task_id)) for ts in task_statuses]
# ---
def __hash__(self):
        if self._hash is None:
            self._hash = hash(frozenset(self._dict.items()))
        return self._hash
# ---
def _check_minions_directories(self):
        '''
        Return the minion keys directory paths
        '''
        accepted = os.path.join(self.opts['pki_dir'], self.ACC)
        pre = os.path.join(self.opts['pki_dir'], self.PEND)
        rejected = os.path.join(self.opts['pki_dir'], self.REJ)
        return accepted, pre, rejected
# ---
def find_Min_Sum(a,b,n): 
    a.sort() 
    b.sort() 
    sum = 0  
    for i in range(n): 
        sum = sum + abs(a[i] - b[i]) 
    return sum
# ---


def string_sequence(n: int) -> str:
    """ Return a string containing space-delimited numbers starting from 0 upto n inclusive.
    >>> string_sequence(0)
    '0'
    >>> string_sequence(5)
    '0 1 2 3 4 5'
    """
    return ' '.join([str(x) for x in range(n + 1)])
# ---
def test_optimize_concat(spec):
    # This test fails if concat's general_blockwise call doesn't have fusable_with_predecessors=False
    a = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    b = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    c = xp.concat((a, b), axis=0)
    d = c + 1
    # try to fuse all ops into one (d will fuse with c, but c won't fuse with a and b)
    d.compute(optimize_function=fuse_multiple_levels())
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        self.last_timeout = timeout
        return self._run_result
# ---
def test_add_project_access_with_no_admin_user(self):
        req = fakes.HTTPRequest.blank('/v2/%s/types/%s/action' % (
            fake.PROJECT_ID, fake.VOLUME_TYPE3_ID),
            use_admin_context=False)
        body = {'addProjectAccess': {'project': PROJ2_UUID}}
        self.assertRaises(exception.PolicyNotAuthorized,
                          self.type_action_controller._addProjectAccess,
                          req, fake.VOLUME_TYPE3_ID, body)
# ---
def heuristic_is_leaf_batched(x):
    if isinstance(x, list):
        return jnp.isscalar(x[0]) or is_jax_array_like(x[0])
    else:
        return False
# ---
def current_label_end(self):
        if self.current_label and self.message:
            return self.message.get_label_range(self.current_label, self.proto_view, False)[1]
        else:
            return -1
# ---
def test_get_last_historic_missing(self):
        serial = "__TEST__"

        last = self.db.get_last_historic(serial)
        assert last is None
# ---
def go(*arg, **kw):
                    canary.append(fn.__name__)
                    return fn(*arg, **kw)
# ---
def shard_names(self) -> Sequence[str]:
        return [str(i) for i in range(len(self.docs))]
# ---
def test_nansum_allnan(spec):
    a = xp.asarray([xp.nan], spec=spec)
    b = cubed.nansum(a)
    assert_array_equal(b.compute(), np.nansum(np.array([np.nan])))
# ---
def __rsub__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.subtract(other, self)
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "data": "collate data1"},
                {"id": 2, "data": "collate data2"},
            ],
        )
# ---
def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
            raise Exception("breakage")
# ---
def load_state_dict(self, state_dict):
    self.decay = state_dict['decay']
    self.num_updates = state_dict['num_updates']
    self.shadow_params = state_dict['shadow_params']
# ---
def show(fig):
            _ensure_width(fig)
            plt.tight_layout()
            pdf.savefig(fig)
            if jupyter_nb:
                plt.show(fig)
            plt.close(fig)
# ---
def _resolve_vllm_backend(
    mode: Literal["native", "docker"],
    *,
    docker_image: str | None,
    docker_run_args: list[str] | None,
) -> VllmServerBackend:
    if mode == "docker":
        return DockerVllmServerBackend(docker_image, docker_run_args)
    if mode == "native":
        return NativeVllmServerBackend()
    raise ValueError(f"Unknown vLLM mode {mode!r}; expected 'native' or 'docker'.")
# ---
def skip_node(name, dag, nodes: Dict[str, Any]) -> bool:
    """
    Return True if the array for a node doesn't have a pipeline to compute it,
    or if it is marked as already computed.
    """
    pipeline = nodes[name].get("pipeline", None)
    if pipeline is None:
        return True

    return nodes[name].get("computed", False)
# ---
def ready_count(self) -> int:
        """Number of actors that are available for RPC."""
        return len(self._handles)
# ---
def __init__(self, root, cwd, badfn=None, relativeuipath=True):
        self._root = root
        self._cwd = cwd
        if badfn is not None:
            self.bad = badfn
        self._relativeuipath = relativeuipath
# ---
def matchfn(self, f):
        for match in self._matchers:
            if match(f):
                return True
        return False
# ---
def nanmean(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanmean, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def slope(x1,y1,x2,y2): 
    return (float)(y2-y1)/(x2-x1)
# ---
def __repr__(self):
        """
        For `print` and `pprint`
        """
        return self.to_str()
# ---
def action_print_survey(self):
        return self.survey_id.action_print_survey()
# ---
def __init__(self, df = 2, mu = 0):
        d1 = NormalDistr(mu, 1)
        d2 = distr_sqrt(ChiSquareDistr(df) / df)
        super(NoncentralTDistr, self).__init__(d1, d2)
        self.df = df
        self.mu = mu
# ---
def __init__(self, size=0):
        self.size = size
        self.id = hash(self)
        self.name = None
# ---
def test_compile_2(self):
        compiler = PatternCompiler(pattern_set=dict(
            TEST=r'\w+'
        ))

        try:
            c1 = compiler.compile('$1{TEST}')
        except:
            self.assertTrue(1)

        c1 = compiler.compile('$1{TEST}', ['test'])
        self.assertEqual(c1, r'(?:(?P<test>(\w+)))')
# ---
def test_handles_reasonable_dtypes(PermutationClass, dtype):
    length = 31000
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    index = np.arange(length, dtype=dtype)
    result = permutation(index)
    assert isinstance(result, np.ndarray)
    assert len(result) == length
    # check it's a permutation
    sorted = np.sort(result)
    assert np.all(sorted == np.arange(length, dtype=dtype))
# ---
def round_to_power_of_two(x: float) -> int:
    """Round ``x`` to the nearest power of two."""
    if x <= 1:
        return 1
    return 2 ** math.ceil(math.log2(x))
# ---
def insert(self, resource, doc_or_docs, **kwargs):
        ids = []
        kwargs.update(self._es_args(resource))
        for doc in doc_or_docs:
            doc.update(self.es.index(body=doc, id=doc.get('_id'), **kwargs))
            ids.append(doc['_id'])
        get_indices(self.es).refresh(self.index)
        return ids
# ---
def square(a: A) -> A:
    return wrap_elemwise_unary(jnp.square, a)
# ---
def _index_of_name(names: Sequence[str | Axis], name) -> int:
    for i, x in enumerate(names):
        if isinstance(x, Axis):
            x = axis_name(x)
        if x == name:
            return i
    return -1
# ---
def proto(self) -> config_pb2.IrisClusterConfig:
        """Access underlying proto (read-only)."""
        return self._proto
# ---
def push(self, item):
        """Push an element on the priority queue.

        The element is pushed on the priority queue according
        to its priority.

        Parameters
        ----------
        item :
            The element to push on the queue.

        """
        heapq.heappush(self._queue, (self.func(item), item))
# ---
def __call__(self, cfg: GrugConfigLike, *, key: PRNGKeyArray) -> PyTree: ...
# ---
def metadata(self) -> Dict[str, Any]:
        return {}
# ---
def test_get_lines_with_empty_string():
        assert get_lines("") == [""]
# ---
def table_row(self, content):
        """Rendering a table row. Like ``<tr>``.

        :param content: content of current table row.
        """
        return '<tr>\n%s</tr>\n' % content
# ---
def _run_heartbeat_loop(self, stop_event: threading.Event) -> None:
        """Heartbeat loop running on its own thread so slow RPCs don't block scheduling."""
        while not stop_event.is_set():
            self._heartbeat_event.wait(timeout=self._config.scheduler_interval_seconds)
            self._heartbeat_event.clear()
            if stop_event.is_set():
                break
            self._heartbeat_all_workers()
# ---
def cluster(request, local_cluster, ray_cluster):
    if request.param == "local":
        return local_cluster
    elif request.param == "ray":
        return ray_cluster
# ---
def sources_add(self):
        data = bottle.request.body.getvalue().decode('utf-8')
        node, spec = make_spec(data, self.config)
        self.config['sources'].append(node)
        self.ndb.connect_source(node, spec)
# ---
def _inv_decay_schedule(lr: float, min_lr: float, decay_steps: int):
    def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / ((lr / min_lr - 1) * jnp.maximum(count, 1) / decay_steps + 1))
        return jnp.maximum(lr * decay, min_lr)

    return schedule
# ---
def _keyify(key):
    return _key_pattern.sub(' ', key.lower())
# ---
def get_latest_stack(self, stack_size):
        return self.get_stack(len(self.examplers), stack_size)
# ---
def update(self, rsc, **kw):
        for k, v in kw.items():
            setattr(rsc, k, v)
        return rsc.save()
# ---
def visit_munder(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            under = self._visit(children[1])
            # Check if base is an operator usually taking limits
            if "\\sum" in str(base) or "\\prod" in str(base) or "\\lim" in str(base):
                return BracedNode(f"{base}_{{{under}}}")
            return BracedNode(f"\\underset{{{under}}}{{{base}}}")
        return TextNode("")
# ---
def _to_tensor(self, array: np.ndarray, device: torch.device) -> torch.Tensor:
        """Convert numpy array to tensor on specified device."""
        return torch.from_numpy(array).to(device)
# ---
def __getitem__(self, key):
        return self.__dict__[key]
# ---
def _Create(self):
    """Create the log group."""
    create_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'create-log-group',
        '--log-group-name', self.name
    ]
    vm_util.IssueCommand(create_cmd)
# ---
def __init__(self, message, error):
        super(ImejiError, self).__init__(message)
        self.error = error.get('error') if isinstance(error, dict) else error
# ---
def cos(a: A) -> A:
    return wrap_elemwise_unary(jnp.cos, a)
# ---
def do_block(carry, block, *args, **kwargs):
            carry = block(carry, *args, **kwargs)
            return carry
# ---
def __init__(self, ndb, config):
        self.ndb = ndb
        self.config = config
# ---
def _stop_db_server(self, db_version):
        sudo('svcadm disable postgresql')
# ---
def loss_recur(q_arr):
        qn = hax.named(q_arr, q.axes)
        out, _ = recurrent_gated_delta_rule(qn, k, v, g, beta, output_final_state=False)
        return jnp.sum(out.array)
# ---
def load_cache(
        self, split, tokenizer: HfTokenizer, override_cache_dir: str | None = None, enforce_eos=True
    ) -> TreeCache[dict]:
        base_cache = override_cache_dir if override_cache_dir is not None else self.cache_dir
        if base_cache is None:
            raise ValueError("cache_dir must be set or override_cache_dir must be provided")
        return load_lm_dataset_cache(os.path.join(base_cache, split), self.format, tokenizer, enforce_eos=enforce_eos)
# ---
def test_special_token_ids_are_distinct(tok):
    ids = {tok.pad_token_id, tok.sos_token_id, tok.eos_token_id}
    assert len(ids) == 3
# ---
def _copy_shard(info: dict):
        asyncio.run(
            _extend_cache_with_other_cache(
                output_path, info["path"], exemplar, info["data_offset_tree"], info["row_offset"]
            )
        )
# ---
def icechunk_storage(tmpdir) -> "Storage":
    return Storage.new_local_filesystem(str(tmpdir))
# ---
def evaluate(
        self,
        model: ModelConfig,
        evals: list[EvalTaskConfig],
        output_path: str,
        max_eval_instances: int | None = None,
        wandb_tags: list[str] | None = None,
    ) -> None:
        """What to run to evaluate."""
        pass
# ---
def ap_sum(a,n,d):
  total = (n * (2 * a + (n - 1) * d)) / 2
  return total
# ---
def axis_name(ax: Sequence[AxisSelector]) -> tuple[str, ...]:  # type: ignore
    ...
# ---
def storage_options(self) -> Optional[dict]:
        """Storage options to be passed to fsspec."""
        return self._storage_options
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[T]:
        for doc in self.source.open_shard_at_row(shard_name, row):
            yield self.fn(doc)
# ---
def bias_dropout_add_scale_fused_train(
    x: torch.Tensor,
    bias: typing.Optional[torch.Tensor],
    scale: torch.Tensor,
    residual: typing.Optional[torch.Tensor],
    prob: float) -> torch.Tensor:
  return bias_dropout_add_scale(
    x, bias, scale, residual, prob, True)
# ---
def test_permutation_with_single_index_returns_correct_value(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    index = 5
    result = permutation(index)
    assert isinstance(result, int)
    assert result != index
# ---
def test_repr(self):
        expr = (col("a") > 0) & (col("b") > 0)
        assert repr(expr) == "((col('a') > lit(0)) & (col('b') > lit(0)))"
# ---
def tale_deletion(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'deletion',
        'purpose': 'repression',
        'method': 'TALEN',
        'zygosity': 'heterozygous'
    }
# ---
def reject(self, match, include_accepted=False):
        self._call_all('reject', match, include_accepted)
# ---
def determine_domain(self, records, operator, value):
        """ Return a domain representing a condition on ``self``. """
        if self.search:
            return self.search(records, operator, value)
        else:
            return [(self.name, operator, value)]
# ---
def smallest_num(xs):
  return min(xs)
# ---
def _draw_item(self, win, data, inverted):

        if data['type'] == 'MoreComments':
            return self._draw_more_comments(win, data)
        elif data['type'] == 'HiddenComment':
            return self._draw_more_comments(win, data)
        elif data['type'] == 'Comment':
            return self._draw_comment(win, data, inverted)
        else:
            return self._draw_submission(win, data)
# ---
def test_registered_on_defaults_to_datetime(self):
        # Ensure that registered_on is a datetime.
        with self.client:
            self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="admin_user"),
                follow_redirects=True,
            )
            user = User.query.filter_by(email="ad@min.com").first()
            self.assertIsInstance(user.registered_on, datetime.datetime)
# ---
def _join_list_field(value: Any) -> str | None:
    if isinstance(value, list):
        text_items = [str(item) for item in value if item is not None]
        if text_items:
            return "\n".join(text_items)
    return None
# ---
def delete_vm(zone: str, vm_name: str):
        """Delete a single TPU VM."""
        logging.info(f"Deleting {vm_name} in {zone}")
        run_sh(
            f"gcloud compute tpus tpu-vm delete {vm_name} --zone {zone} --project {config.GCP_PROJECT_ID} --quiet",
            check=False,
        )
# ---
def VariableExists( variable ):
  return GetBoolValue( "exists( '{0}' )".format( EscapeForVim( variable ) ) )
# ---
def eval_func(self, m, k, r):
        """
        The evaluation function value for the set of weights (vector) r
        at the mth game and kth board state """
        return np.dot(r, self.data[m][k])
# ---
def _job_detail_page(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Job Detail", "/static/controller/job-detail.js"))
# ---
def simplify_capture(capture):
        if capture == Ellipsis:
            return Ellipsis
        elif (capture.binding == capture.axes[0] or capture.binding is None) and len(capture.axes) == 1:
            return capture.axes[0]
        elif capture.binding is None:
            return capture.axes
        else:
            return {capture.binding: capture.axes}
# ---
def future_from_value(value):
    future = asyncio.Future()
    future.set_result(value)
    return future
# ---
def testMatMul_OutEmpty_A(self):
    n, k, m = 0, 8, 3
    x = self._randMatrix(n, k, np.float32)
    y = self._randMatrix(k, m, np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def __call__(self, batch: Sequence[BatchEncoding]) -> BatchEncoding:
        stacked = reduce(_stack_batch_encodings, batch)
        return stacked
# ---
def test_type(self):
        assert isinstance(event.Priority.DEFAULT, int)
# ---
def __init_options(self):
        options_file = '/etc/func/modules/'+self.__class__.__name__+'.conf'
        self.options = read_config(options_file, self.Config)
        return
# ---
def mul_even_odd(list1):
    first_even = next((el for el in list1 if el%2==0),-1)
    first_odd = next((el for el in list1 if el%2!=0),-1)
    return (first_even*first_odd)
# ---
def init(cls, Vocab: Axis, config: MistralConfig, *, key) -> "MistralLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = LlamaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return MistralLMHeadModel(transformer, embeddings, lm_head)
# ---
def test_stage_name():
    """PhysicalStage.stage_name() generates descriptive names from operations."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            FilterOp(lambda x: x > 5),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    assert plan.stages[0].stage_name() == "Map"
# ---
def _description_domain(self, env):
        return self.domain(env[self.model_name]) if callable(self.domain) else self.domain
# ---
def word_len(s): 
    s = s.split(' ')   
    for word in s:    
        if len(word)%2!=0: 
            return True  
        else:
          return False
# ---
def init_predict_tqdm(self) -> None:
        """Initialize the prediction progress bar."""
        bar = super().init_predict_tqdm()
        return self._update_bar_description(bar)
# ---
def test_reflected_operation(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = 1 - a

    assert_array_equal(b.compute(), 1 - np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
# ---
def __enter__(self) -> Autoscaler:
        return self
# ---
def min_difference(test_list):
  temp = [abs(b - a) for a, b in test_list]
  res = min(temp)
  return (res)
# ---
def vsync(self):
        """True if buffer flips are synchronised to the screen's vertical
        retrace.  Read-only.

        :type: bool
        """
        return self._vsync
# ---
def test_shift2(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.pct_change(1)
            return Ac.sum()

        hpat_func = self.jit(test_impl)
        n = 11
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def end_of_directory(cache_to_disc=True):
    xbmcplugin.endOfDirectory(int(sys.argv[1]), cacheToDisc=cache_to_disc)
# ---
def get_logs(self, label: str):
        """
        Returns logs as can be reported to WandB.

        Args:
            label: Label to prepend to all log keys.
        """
        return {
            f"{label}/{key}": data for key, data in sorted(self._get_data().items())
        }
# ---
def setUp(self):
        self.client.login(username='admin', password='root')
# ---
def common_in_nested_lists(nestedlist):
    result = list(set.intersection(*map(set, nestedlist)))
    return result
# ---
def pos_nos(list1):
  for num in list1: 
    if num >= 0: 
       return num
# ---
def test_dot_errors_if_different_sized_axes():
    Height = Axis("Height", 2)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)

    H2 = Axis("Height", 4)

    m1 = hax.ones((Height, Width, Depth))
    m2 = hax.ones((Depth, Width, H2))

    with pytest.raises(ValueError):
        hax.dot(m1, m2, axis="Height")
# ---
def _sampling_noise(self):
    return 0
# ---
def test_stdin(self, tmpfile):
        cap = capture.FDCapture(0)
        cap.start()
        x = os.read(0, 100).strip()
        cap.done()
        assert x == tobytes('')
# ---
def test_product_search_client_creation(self, mock_client, mock_get_creds, mock_client_info):
        result = self.hook.get_conn()
        mock_client.assert_called_once_with(
            credentials=mock_get_creds.return_value, client_info=mock_client_info.return_value
        )
        assert mock_client.return_value == result
        assert self.hook._client == result
# ---
def __or__(self, other: object) -> LogicalExpr:
        return LogicalExpr(self, _to_expr(other), "or")
# ---
def ClearYcmSyntaxMatches():
  matches = VimExpressionToPythonType( 'getmatches()' )
  for match in matches:
    if match[ 'group' ].startswith( 'Ycm' ):
      vim.eval( 'matchdelete({0})'.format( match[ 'id' ] ) )
# ---
def tiny_asr_corpus_config(path):
    return AudioIODatasetConfig(
        id="WillHeld/test_librispeech_parquet",
        text_key="text",
        train_split="validation",
        validation_split="validation",
        cache_dir=f"{path}/cache_asr",
    )
# ---
def __iter__(self):
        if self.tree is None:
            return
        else:
            for i in range(len(self)):
                yield self[i]
# ---
def get_job_status(self, request: cluster__pb2.Controller.GetJobStatusRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetJobStatusResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def _find_slice_for_worker(self, vm_address: str) -> tuple[str | None, ScalingGroup | None]:
        """Find the slice and group containing a worker by VM address."""
        for group in self._groups.values():
            for slice_obj in group.vm_groups():
                for vm in slice_obj.vms():
                    if vm.info.address == vm_address:
                        return slice_obj.slice_id, group
        return None, None
# ---
def status(self, job_id: JobName) -> cluster_pb2.JobStatus:
        """Get job status.

        Args:
            job_id: Job ID to query

        Returns:
            JobStatus proto with current state
        """
        return self._cluster_client.get_job_status(job_id)
# ---
def next(self, psm: PSM):
        if psm.char.isdigit():
            self.min.append(psm.char)
            return self
        elif psm.char == ",":
            self._interpret()
            return MaximumOfRepetition(self)
        elif psm.char == "}":
            self._interpret()
            return self.parent
        else:
            psm.error = 'expected digit, "," or "}"'
# ---
def _wandb_mode_disabled() -> bool:
    mode = os.getenv("WANDB_MODE")
    if mode is None:
        return False
    return mode.lower() in {"disabled", "offline", "dryrun"}
# ---
def result_type(*arrays_and_dtypes):
    return nxp.result_type(
        *(a.dtype if isinstance(a, CoreArray) else a for a in arrays_and_dtypes)
    )
# ---
def test_smallest_blockdim():
    assert smallest_blockdim([]) == ()
    assert smallest_blockdim([(5,), (5,)]) == (5,)
    assert smallest_blockdim([(5,), (3, 2)]) == (3, 2)
    assert smallest_blockdim([(5, 5), (3, 3, 3, 1)]) == (3, 3, 3, 1)
    assert smallest_blockdim([(2, 1), (2, 1)]) == (2, 1)
    assert smallest_blockdim([(2, 2, 1), (3, 2), (2, 2, 1)]) == (2, 2, 1)

    with pytest.raises(ValueError, match="Chunks do not add up to same value"):
        smallest_blockdim([(2, 1), (2, 2)])
# ---
def __call__(self, x, *, key):
            return self.down(self.up(x), key=key)
# ---
def load(self):
        pass
# ---
def deduct(self, job: ControllerJob) -> None:
        """Deduct job's resources from available capacity."""
        res = job.request.resources
        self.available_cpu -= res.cpu
        self.available_memory -= res.memory_bytes
        self.available_gpus -= get_gpu_count(res.device)
        self.available_tpus -= get_tpu_chip_count(res.device)
# ---
def defineCharacteristics(self):
        self.name = "Split RGB bands"
        self.group = "Grid - Tools"
        self.addParameter(ParameterRaster(SplitRGBBands.INPUT, "Input layer", False))
        self.addOutput(OutputRaster(SplitRGBBands.R, "Output R band layer"))
        self.addOutput(OutputRaster(SplitRGBBands.G, "Output G band layer"))
        self.addOutput(OutputRaster(SplitRGBBands.B, "Output B band layer"))
# ---
def test_killed_maps_to_stopped(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_KILLED) == JobStatus.STOPPED
# ---
def test_empty_shape():
    key = jax.random.PRNGKey(0)
    hax.random.uniform(key, shape=())
# ---
def Embed(self) -> Axis:
        return Axis("embed", self.grug_config.hidden_dim)
# ---
def test_from_not_handler(self):
        def func():
            pass

        with pytest.raises(ValueError) as excinfo:
            event.HandlerInstance.from_handler(func)
        excinfo.match(r"Event handler must be decorated with `@event`")
# ---
def from_zarr_array(cls, zarray):
        return cls(zarray.shape, zarray.chunks)
# ---
def test_no_param_usage(self):
        @event.event
        def func_name(self):
            pass

        @event.event
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.event_name == "test"
        assert func_name._h_info.event_name == "func_name"
        assert h_info.handler is on_test
        assert h_info.priority is event.Priority.DEFAULT
        assert h_info.should_enable
        assert not h_info.is_async
# ---
def run(self, coro: Coroutine[Any, Any, T]) -> T:
        """
        Run an async function from sync code and wait for result.

        Args:
            coro: Coroutine to run (e.g., async_func(args))

        Returns:
            Result of the coroutine
        """
        if self._loop is None:
            raise RuntimeError("AsyncBridge not started. Call start() first.")

        future = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return future.result()
# ---
def mock_metric(target, gen):
            return (gen - target).abs().mean(dim=(-2, -1))
# ---
def cluster_restart(ctx):
    """Restart cluster by stopping then starting."""
    ctx.invoke(cluster_stop)
    click.echo("")
    ctx.invoke(cluster_start)
# ---
def standardize_config(config: ServerConfig) -> ServerConfig:
    """Replace relative paths with absolute paths, except for cloud storage paths."""
    absolute_root_paths = [resolve_path(path) for path in config.root_paths]
    return replace(config, root_paths=absolute_root_paths)
# ---
def set_last_purchase_rate(self, new_name):
		last_purchase_rate = get_last_purchase_details(new_name).get("base_rate", 0)
		frappe.db.set_value("Item", new_name, "last_purchase_rate", last_purchase_rate)
# ---
def __init__(cls, name, bases, dict):
        cls._platform_event_names = set()
        for base in bases:
            if hasattr(base, '_platform_event_names'):
                cls._platform_event_names.update(base._platform_event_names)
        for name, func in dict.items():
            if hasattr(func, '_platform_event'):
                cls._platform_event_names.add(name)
        super(_WindowMetaclass, cls).__init__(name, bases, dict)
# ---
def datasets():
    ds1 = ListAsyncDataset([1, 2, 3, 4, 5])
    ds2 = ListAsyncDataset([10, 20, 30, 40, 50])
    ds3 = ListAsyncDataset([100, 200, 300, 400, 500])
    ds1.finalize()
    ds2.finalize()
    ds3.finalize()
    return {"ds1": ds1, "ds2": ds2, "ds3": ds3}
# ---
def selected_target_account_index(self, value):
        target = self.target_accounts[value - 1] if value > 0 else None
        self.selected_pane.selected_target = target
        self._selected_target_index = value
        self.import_table.refresh()
# ---
def test_wait_for_connection_returns_true_immediately(mock_conn_avail, _mock_sleep):
    """wait_for_connection returns True if connection available immediately."""
    mock_conn_avail.return_value = True
    conn = MagicMock()
    # Test behavior: function should return True when connection is available
    result = wait_for_connection(conn, timeout=Duration.from_seconds(60), poll_interval=Duration.from_seconds(5))
    assert result is True
# ---
def on_predict_end(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,
    ) -> None:
        """Restore original weights after prediction.

        Parameters
        ----------
        trainer: Trainer
            The Trainer instance.
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._on_eval_end(pl_module)
# ---
def test_variant_preserved_in_display():
    """Variant information is preserved when formatting for display."""
    variant = "v5litepod-16"
    result = format_accelerator_display(config_pb2.ACCELERATOR_TYPE_TPU, variant)
    assert variant in result, f"Variant '{variant}' must be visible in display: {result}"
# ---
def get_sample(self, design_dir: Path, sample_id: Optional[str] = None) -> Dict:
        metadata_path = design_dir / f"{sample_id}.npz"
        generated_path = design_dir / f"{sample_id}.cif"
        native_path = design_dir / f"{sample_id}_native.cif"
        return self.getitem_from_paths(metadata_path, generated_path, native_path)
# ---
def numpy_array_to_backend_array(arr, *, dtype=None):
    if isinstance(arr, dict):
        return {k: namespace.asarray(v, dtype=dtype) for k, v in arr.items()}
    return namespace.asarray(arr, dtype=dtype)
# ---
def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str, ...]:
        return ()
# ---
def get_vocab_size_for_tokenizer(tokenizer_name: str) -> int:
    """Return the vocabulary size for a tokenizer name.

    Args:
        tokenizer_name: HuggingFace tokenizer name or path.

    Returns:
        Vocabulary size for the tokenizer.
    """
    resolved_name = unwrap_versioned_value(tokenizer_name)
    if resolved_name in _KNOWN_VOCAB_SIZES:
        return _KNOWN_VOCAB_SIZES[resolved_name]

    tokenizer = _load_tokenizer(resolved_name)
    return len(tokenizer)
# ---
def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        return parse_response_for_stop_token(response, self.tokenizer, self._end_message_token)
# ---
def test_corrupted_character():
    assert corrupted_character('{([(<{}[<>[]}>{[]{[(<()>')[0] == '}'
    assert corrupted_character('[[<[([]))<([[{}[[()]]]')[0] == ')'
    assert corrupted_character('[{[{({}]{}}([{[{{{}}([]')[0] == ']'
    assert corrupted_character('[<(<(<(<{}))><([]([]()')[0] == ')'
    assert corrupted_character('<{([([[(<>()){}]>(<<{{')[0] == '>'
# ---
def __init__(self, language="en"):
        """
        Constructor for the wordnet manager.
        It takes a main language.
        """
        self.__language = language
# ---
def test_repr(self):
        """
        The instance's repr reflects its C{__dict__}
        """
        namespace = _api._SimpleNamespace()
        namespace.y = 2
        assert repr(namespace) == "namespace(y=2)"
# ---
def test_capturing_readouterr_unicode(self):
        with self.getcapture() as cap:
            print("hx\xc4\x85\xc4\x87")
            out, err = cap.readouterr()
        assert out == py.builtin._totext("hx\xc4\x85\xc4\x87\n", "utf8")
# ---
def test_get_resources_list(self):

        response = requests.get(
            self.get_endpoint(TEMPL_V1_COLLECTION_ENDPOINT))

        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.json(), ["vms"])
# ---
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self._last_epoch = -1
    self.step(epoch=0)
# ---
def rectangle_area(l,b):
  area=l*b
  return area
# ---
def parse_format(data):
    """Returns root input type from data."""
    input_types = {}
    data = data['ist_nodes']
    root_id = data[0]['id']      # set root type

    for item in data:
        input_type = _get_input_type(item)
        if input_type is not None:
            input_types[input_type['id']] = input_type  # register by id

    _substitute_ids_with_references(input_types)
    return input_types[root_id]
# ---
def test_map_blocks_with_different_block_shapes(spec):
    def func(x, y):
        return x

    a = xp.asarray([[[12, 13]]], spec=spec)
    b = xp.asarray([14, 15], spec=spec)
    c = cubed.map_blocks(
        func, a, b, dtype="int64", chunks=(1, 1, 2), drop_axis=2, new_axis=2
    )
    assert_array_equal(c.compute(), np.array([[[12, 13]]]))
# ---
def geturl(self):
        return self.url
# ---
def var(
        self, axis: AxisSelection | None = None, dtype=None, ddof=0, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.var(self, axis=axis, dtype=dtype, ddof=ddof, where=where)
# ---
def job_id(self) -> str:
        return str(self._job.job_id)
# ---
def forward(self, fts: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError()
# ---
def test_makes_patches__high_res():
    x = torch.randn(1, 10, 14, 21)

    patch_embed = PerceiverEncoder(
        in_channels=10,
        out_channels=4,
        patch_extent=(90.0, 120.0),
        perceiver=make_perceiver(10, 4),
    )

    patches = patch_embed(x, make_resolution(x))

    assert patches.shape == (1, 4, 2, 3)
# ---
def with_sliding_window(self, sliding_window: int | None) -> "AttentionMask":
        return AttentionMask(
            is_causal=self.is_causal,
            segment_ids=self.segment_ids,
            sliding_window=sliding_window,
        )
# ---
def put(self):
            pass
# ---
def play_model(cfg, player):
    predfunc = OfflinePredictor(cfg)
    while True:
        score = play_one_episode(player, predfunc)
        print("Total:", score)
# ---
def set_after_delay():
        time.sleep(0.05)
        flag.set()
# ---
def test_single_batch_accumulation(self):
        """Verify that a single batch is recorded correctly."""
        agg = TrainAggregator()

        loss = torch.tensor(0.5)
        loss_per_channel = torch.tensor([0.1, 0.2, 0.3, 0.4])
        batch = TrainBatchOutput(loss=loss, loss_per_channel=loss_per_channel)

        agg.record_batch(batch)

        assert agg._n_batches == 1
        assert torch.allclose(agg._loss, loss)
        assert torch.allclose(agg._loss_per_channel, loss_per_channel)
# ---
def group_keyvalue(l):
    result = {}
    for k, v in l:
         result.setdefault(k, []).append(v)
    return result
# ---
def test_compare_columns(self):
        expr = col("a") > col("b")
        assert expr.evaluate({"a": 10, "b": 5}) is True
        assert expr.evaluate({"a": 5, "b": 10}) is False
# ---
def count_digs(tup):
  return sum([len(str(ele)) for ele in tup ]) 
def sort_list(test_list):
  test_list.sort(key = count_digs)
  return (str(test_list))
# ---
def from_state_dict(self: Mod, state_dict: StateDict, prefix: Optional[str] = None) -> Mod:
        unscaled = default_eqx_module_from_state_dict(self, state_dict, prefix)
        if unscaled.weight is not None:
            unscaled = dataclasses.replace(unscaled, weight=unscaled.weight / self.reparam.active_scale)
        return unscaled
# ---
def _small_dataset(seq_len=128, num_sequences=200) -> AsyncDataset[Sequence[int]]:
    sequences = [np.arange(seq_len) + 1000 * i for i in range(num_sequences)]

    return ListAsyncDataset(sequences, is_complete=True)
# ---
def _unflatten_hist(self, fts: HistBatched) -> HistChanneled:
        return rearrange(fts, "(n hist) c h w -> n (hist c) h w", hist=self.hist + 1)
# ---
def with_time(self, time_range: slice) -> Self:
        """Slice data across the time dimension."""
        return dataclasses.replace(self, data=self.data[:, time_range, :, :, :])
# ---
def _load_cal_db(self):
        self._cal_db = load_IQMX_calibration_database(self._cal_db_name, 0)
# ---
def __init__(self):
        self._n_batches = 0
        self._loss = torch.tensor(torch.nan)
        self._loss_per_channel = torch.tensor(torch.nan)
# ---
def __init__(self, filename):
        self.filename = filename
        tfile, members = self.get_archive_object_tar()
        self.read_files(tfile, members)
# ---
def analyse(self):
        return globals()["Occurrence_"+self.name[1:]](self)
# ---
def _scale_up_wrapper(stop_event):
            self._do_scale_up(group, ts, reason)
# ---
def test_load_vortex_column_projection(self, vortex_file):
        """Test column selection (projection)."""
        spec = InputFileSpec(path=str(vortex_file), columns=["id", "name"])
        records = list(load_vortex(spec))
        assert len(records) == 100
        assert set(records[0].keys()) == {"id", "name"}
# ---
def __init__(self, requested_depth=None):
        self.requested_depth = requested_depth
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        # https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_rope_utils.py#L307
        # Porting that to JAX/Haliax:
        return Llama3RotaryEmbeddings.init(HeadSize, self)
# ---
def __init__(self, dim, vocab_dim):
    super().__init__()
    self.embedding = nn.Parameter(torch.empty((vocab_dim, dim)))
    torch.nn.init.kaiming_uniform_(self.embedding, a=math.sqrt(5))
# ---
def startup_name(self):
        return self._get_title_and_company()['company']
# ---
def compute_expected_heat_content_change(
    surface_heat_flux: Tensor,
    geothermal_heat_flux: Tensor,
    sea_surface_fraction_tensor: Tensor,
    area_weighted_func: Callable,
) -> Tensor:
    # Expected change in heat content from surface flux
    dHC_expected = (
        area_weighted_func(surface_heat_flux * sea_surface_fraction_tensor)
        * SECONDS_PER_TIME_STEP
    )  # [J]

    # Apply geothermal heat flux
    dHC_expected += geothermal_heat_flux

    return dHC_expected
# ---
def _start_db_server(self, db_version):
        sudo('svcadm enable postgresql')
# ---
def make_message(self, record):
        tmplt = get_template('mailing/instructor_activity.txt')
        return tmplt.render(context=record)
# ---
def testFormatUsername(self):
    """Tests the _FormatUsername function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    username_string = test_helper._FormatUsername(
        event, event_data, event_data_stream)
    self.assertEqual(username_string, '-')
# ---
def exists_volume(self, volume):
        ''' return whether a volume exists '''
        exist_volumes = self.get_volumes()

        volume_found = False
        for exist_volume in exist_volumes:
            if exist_volume['name'] == volume['name']:
                volume_found = True
                break

        return volume_found
# ---
def get_mol(ccd: str, mols: Dict, moldir: str) -> Mol:
    """Get mol from CCD code.

    Return mol with ccd from mols if it is in mols. Otherwise load it from moldir,
    add it to mols, and return the mol.
    """
    mol = mols.get(ccd)
    if mol is None:
        # Load molecule
        mol = load_molecules(moldir, [ccd])[ccd]

        # Add to resource/dict
        if isinstance(mols, dict):
            mols[ccd] = mol
        else:
            mols.set(ccd, mol)

    return mol
# ---
def validity_triangle(a,b,c):
 total = a + b + c
 if total == 180:
    return True
 else:
    return False
# ---
def rollout_group_to_dict(group: "RolloutGroup") -> dict[str, Any]:
    """Convert a RolloutGroup to a JSON-serializable dictionary.

    Uses tree mapping to automatically handle all fields including arrays,
    making it robust to schema changes.
    """
    return dataclasses.asdict(
        jax.tree.map(lambda v: _to_list(v) if isinstance(v, list | jax.Array | numpy.ndarray) else v, group)
    )
# ---
def time_elapsed(self) -> float:
        return self.time_end - self.time_start
# ---
def check_model_works_with_seqlen(model_type, config, input_len):
    key = PRNGKey(0)
    Vocab = hax.Axis("vocab", 128)
    model = model_type.init(Vocab, config, key=key)
    input_ids = hax.arange(config.max_Pos.resize(input_len), dtype=jax.numpy.int32)
    causal_mask = AttentionMask.causal()
    a1 = model(input_ids, key=key, attn_mask=causal_mask)
    assert a1.axis_size("position") == input_len
# ---
def create_vm_group_side_effect(tags: dict[str, str] | None = None) -> MagicMock:
        # Generate a unique slice ID based on call count
        slice_id = f"new-slice-{len(manager.create_vm_group.call_args_list)}"
        mock = make_mock_vm_group(slice_id)
        mock.tags = tags  # Store tags so tests can verify if needed
        return mock
# ---
def set_time_range(self, time_range: xr.CFTimeIndex) -> None:
        self.start_day = time_range[0]
        units = f"days since {self.start_day}"
        self.days_since_start = np.array(
            [
                cftime.date2num(date, units, calendar=self.start_day.calendar)
                for date in time_range
            ]
        )
# ---
def test_position_token_id_roundtrip(tok):
    for pos in [0, 1, 100, 511]:
        tid = tok.position_token_id(pos)
        assert tok.is_position_token(tid)
        assert tok.position_from_token(tid) == pos
# ---
def test_iosxe_is_a_IOSXE(self):
        self.assertIsInstance(self.xe, IOSXE)
# ---
def quit_watch_later(self, code=None):
        self.command('quit_watch_later', code)
# ---
def _hard_sample(self, logits):
    assert logits.ndim == 2
    thresholds, _ = torch.sort(logits, dim=-1)
    thresholds = thresholds[:, - self.k][:, None]
    return (logits >= thresholds).type(logits.dtype)
# ---
def stop(self, handle: VllmServerHandle) -> None:
        raise NotImplementedError
# ---
def make_sscc(self, cr, uid, context=None):
        sequence = self.pool.get('ir.sequence').get(cr, uid, 'stock.lot.tracking')
        try:
            return sequence + str(self.checksum(sequence))
        except Exception:
            return sequence
# ---
def _nanmean_func(a, **kwargs):
    n = _nannumel(a, **kwargs)
    total = nxp.nansum(a, **kwargs)
    return {"n": n, "total": total}
# ---
def __init__(self, length, prng_key):
        self.length = length
        rng = _np_rng_from_jax_key(prng_key)
        self.a, self.b = self._generate_permutation_params(rng)
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Invoke the method synchronously (blocking)."""
        ...
# ---
def client(service):
    dashboard = ControllerDashboard(service)
    return TestClient(dashboard._app)
# ---
def __call__(self, fn):
        return self.matchfn(fn)
# ---
def lookup_endpoints(self, name: str) -> list[ControllerEndpoint]:
        """Find endpoints by exact name match for non-terminal jobs."""
        with self._lock:
            return self._visible_endpoints(lambda ep: ep.name == name)
# ---
def test_1D_Var_len(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n), 'B': np.arange(n) + 1.0})
            df1 = df[df.A > 5]
            return len(df1.B)

        hpat_func = self.jit(test_impl)
        n = 11
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def test_gumbel():
    check_gen_is_equal(lambda k, s: jax.random.gumbel(k, s), lambda k, s: hax.random.gumbel(k, s))
# ---
def _fix_a_slash_b(string: str) -> str:
    if len(string.split("/")) != 2:
        return string
    a = string.split("/")[0]
    b = string.split("/")[1]
    try:
        a = int(a)
        b = int(b)
        assert string == f"{a}/{b}"
        new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
        return new_string
    except ValueError:
        return string
# ---
def test_mixtral_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def create_test_rollout_group(key: str, n_rollouts: int, start_idx: int = 0) -> RolloutGroup:
    """Create a test rollout group."""
    rollouts = [create_test_rollout(start_idx + i) for i in range(n_rollouts)]
    return RolloutGroup(rollouts=rollouts)
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str | None = None
    ) -> HFCheckpointConverter["MixtralConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfMixtralConfig,
        )
# ---
def model_type(self) -> type["ParallelLlamaLMHeadModel"]:
        return ParallelLlamaLMHeadModel
# ---
def __call__(self, target, cred):
        """Check the policy."""

        return False
# ---
def test_binary_repr(self):
        """Binary repr should wrap the contained value"""
        self.assertEqual(repr(Binary("a")), "Binary(%r)" % b"a")
# ---
def test_corrupted_character_stack():
    assert corrupted_character('[({(<(())[]>[[{[]{<()<>>')[1] == ['}', '}', ']', ']', ')', '}', ')', ']']
# ---
def python_compute_paragraph_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> list[dict[str, str]]:
    results = []
    for record in batch.to_pylist():
        text, record_id = record[text_col], record[id_col]
        for para in text.split("\n"):
            results.append({"hash": _str_hash_legacy(para), "id": record_id})
    return results
# ---
def main():
    """Simple flower, using global turtle instance"""
    turtle.speed(0)
    turtle.colormode(1.0)
    bloom(5)
    turtle.exitonclick()
# ---
def test_binary_entrypoint(self):
        entry = Entrypoint.from_binary("python", ["-c", "print('hi')"])
        iris_entry = convert_entrypoint(entry)
        assert iris_entry.is_command
        assert iris_entry.command == ["python", "-c", "print('hi')"]
# ---
def unlink(self, cr, uid, ids, context=None):
        raise osv.except_osv(_('Error!'), _('You cannot remove a lot line.'))
# ---
def __init__(self, parent: ContentOfGroup):
        self.parent = parent
        self.between = ast.Between()
        self.min = []
# ---
def test_keras_mask(self):
    x = np.ones((10, 10))
    y = keras.layers.Masking(1.)(x)
    self.assertTrue(hasattr(y, '_keras_mask'))
    self.assertTrue(y._keras_mask is not None)
    self.assertAllClose(self.evaluate(y._keras_mask), np.zeros((10,)))
# ---
def radix_sort(nums):
    RADIX = 10
    placement = 1
    max_digit = max(nums)

    while placement < max_digit:
      buckets = [list() for _ in range( RADIX )]
      for i in nums:
        tmp = int((i / placement) % RADIX)
        buckets[tmp].append(i)
      a = 0
      for b in range( RADIX ):
        buck = buckets[b]
        for i in buck:
          nums[a] = i
          a += 1
      placement *= RADIX
    return nums
# ---
def all_vms(self) -> list[ManagedVm]:
        """Return a snapshot of all tracked VMs."""
        with self._lock:
            return list(self._vms.values())
# ---
def to_seconds(time, unit):
    if unit == 's':
        return float(time)
    elif unit == 'm':
        return float(time) * 60
    elif unit == 'h':
        return float(time) * 60 * 60
    elif unit == 'd':
        return float(time) * 60 * 60 * 24
# ---
def test_dontreadfrominput_has_encoding(testdir):
    testdir.makepyfile("""
        import sys
        def test_capattr():
            # should not raise AttributeError
            assert sys.stdout.encoding
            assert sys.stderr.encoding
    """)
    reprec = testdir.inline_run()
    reprec.assertoutcome(passed=1)
# ---
def __init__(self, subqueries=()):
        self.subqueries = subqueries
# ---
def test_nested_list(self):
        """Store and retrieve a nested list"""
        self.make_table()
        data = [
            1,
            [
                True,
                None,
                "abc",
            ],
        ]
        self.dynamo.put_item("foobar", {"id": "abc", "l": data})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["l"], data)
# ---
def compute_dapo_loss(
    loss_objective: jax.Array,
    loss_masks: jax.Array,
) -> jax.Array:
    """Compute DAPO-like loss (global token normalization).

    Divides by total tokens across all examples in the batch, not per-example.
    """
    return -1 * jnp.mean(jnp.sum(loss_objective * loss_masks, axis=1) / jnp.sum(loss_masks))
# ---
def q_heads_per_group(self) -> int:
        return self.num_heads // self.num_kv_heads
# ---
def wait_all(self, timeout: float | None = None) -> list[CallResult]:
        results = []
        for endpoint, future in self._futures:
            try:
                value = future.result(timeout=timeout)
                results.append(CallResult(endpoint=endpoint, value=value))
            except Exception as e:
                results.append(CallResult(endpoint=endpoint, exception=e))
        return results
# ---
def sort_sublists(list1):
    result = list(map(sorted,list1)) 
    return result
# ---
def shard_map(
    f: Callable[Args, R],
    *,
    in_specs: Any = None,
    out_specs: Any = None,
    mesh: Mesh | None = None,
    axis_mapping: ResourceMapping | None = None,
    check_rep: bool = False,
    **kwargs,
) -> Callable[Args, R]: ...
# ---
def __len__(self) -> int:
        """Get the length of the dataset.

        Returns
        -------
        int
            The length of the dataset.

        """
        return len(self.dataset.target_ids) * self.multiplicity
# ---
def extend(self, items):
        """Append a list of elements at the end of the queue.

        Parameters
        ----------
        items : list
            List of elements.

        """
        self._queue.extend(items)
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> KvPageCache:
        """
        Creates an empty page cache for this layer. Note that in order to create a decoder state, you
        need to couple the KvPageCache to the PageTable's state with a BatchInfo object.
        """
        return self.self_attn.empty_page_cache(spec, dtype=dtype)
# ---
def diameter_circle(r):
  diameter=2*r
  return diameter
# ---
def GetTaskStatus(self, request, context):
        """Task operations"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __call__(self, indices: np.ndarray) -> np.ndarray:
        """Apply the permutation to an array of integers.

        Args:
            indices: An array of integers to be permuted.

        Returns:
            The permuted value(s).
        """
        ...
# ---
def RegisterEndpoint(self, request, context):
        """Endpoint registry (generic service discovery)"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def grilleEmpty(self):
        """
        Implement function to check if the grid is empty.

        """
        for line in self.grille:
            for car in line[:len(line) - 1]:
                if car != '0':
                    return False
        return True
# ---
def allocate_for_seq(
        self,
        token_slot_ids: ht.i32[NamedArray, " position"],  # type: ignore[name-defined]
        token_pos_ids: ht.i32[NamedArray, " position"],  # type: ignore[name-defined]
    ) -> tuple["DecodeState", PageBatchInfo]:
        sequences, page_table, batch_info = self.sequences.allocate_for_seq(
            self.page_table, token_slot_ids, token_pos_ids
        )
        return dataclasses.replace(self, sequences=sequences, page_table=page_table), batch_info
# ---
def rec_set(tree):
        if has_inference(tree):
            tree = replace_fn(tree)

        if jax.tree_util.tree_leaves(tree) == [tree]:
            return tree

        return jax.tree_util.tree_map(rec_set, tree, is_leaf=lambda x: has_inference(x) and tree is not x)
# ---
def __init__(self, bbox, srs):
        self.bbox = bbox
        self.srs = srs
        self.geom = None
# ---
def test_permutation_rejects_invalid_indices(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    with pytest.raises(IndexError):
        permutation(-1)  # Test negative index
    with pytest.raises(IndexError):
        permutation(length)
# ---
def __init__(self, field, record, value):
        self.args = (field, record, value)
# ---
def __len__(self) -> int:
        return self.input_ids.axis_size("batch")
# ---
def load_json(cls, data, default_rule=None):
        """Allow loading of JSON rule data."""

        # Suck in the JSON data and parse the rules
        rules = dict((k, parse_rule(v)) for k, v in
                     jsonutils.loads(data).items())

        return cls(rules, default_rule)
# ---
def check_subset(test_tup1, test_tup2):
  res = set(test_tup2).issubset(test_tup1)
  return (res)
# ---
def rate_noise(self, t):
    return self.sigmas[0] ** (1 - t) * self.sigmas[1] ** t * (
      self.sigmas[1].log() - self.sigmas[0].log())
# ---
def make_account():
    return {'balance': 0}
# ---
def tracker(name):
            def go(conn, *args, **kw):
                canary.append(name)
            return go
# ---
def tale_replacement(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'replacement',
        'purpose': 'characterization',
        'method': 'TALEN',
        'zygosity': 'heterozygous'
    }
# ---
def get_id():
    return addon.getAddonInfo('id')
# ---
def bloom(radius):
    turtle.colormode(255)

    for rad in range(40, 10, -5):
        for looper in range(360//rad):
            turtle.up()
            turtle.circle(radius+rad, rad)
            turtle.begin_fill()
            turtle.fillcolor((200+random.randint(0, rad),
                              200+random.randint(0, rad),
                              200+random.randint(0, rad)))
            turtle.down()
            turtle.circle(-rad)
            turtle.end_fill()
# ---
def parquet_file() -> str:
    print(f"\n[Setup] Ensuring {FILENAME} is available...")
    file_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type="dataset", revision=REVISION)

    # Warm-up OS page cache to prevent disk I/O jitter from affecting the results.
    print(f"[Setup] Warming up OS cache for {file_path}...")
    with open(file_path, "rb") as f:
        while f.read(1024**2):
            pass

    return file_path
# ---
def __contains__(self, obj: T) -> bool:
        return obj in self._obj_to_index
# ---
def __call__(self, x, y, *, key):
            return x + self.array + self.static + hax.random.normal(key, x.axes), x * 2 + y
# ---
def axis_spec_to_tuple(axis_spec: ShapeDict) -> tuple[Axis, ...]: ...
# ---
def test_set_enable_host_disable(self):
        self._test_host_action(self.conn.set_host_enabled, False, 'disabled')
# ---
def build():
    """Image build commands."""
# ---
def body(i, rc):
                def dec(rc):
                    page = pages_row["page", i].scalar()
                    return rc.at["page", page].add(-1)

                return jax.lax.cond(valid["page", i].scalar(), dec, lambda x: x, rc)
# ---
def test_eye(tmp_path, spec, executor):
    a = xp.eye(10000, 10000, chunks=(5000, 5000), spec=spec)
    run_operation(tmp_path, executor, "eye", a)
# ---
def __init__(self, stream_parser, message):
        self._stream_parser = stream_parser
        self._message = message
        self._iter_rows = None
        self._num_items = self._message.row_count
        self._remaining = self._message.row_count
# ---
def test_best_of_n_sorted_by_score(params, model_cfg, tokenizer):
    """Results should be sorted best-first."""
    results = best_of_n(
        params=params,
        source="x = 1 + 2\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(5),
        n=4,
        max_depth=2,
    )
    scores = [c.score for c in results]
    assert scores == sorted(scores, reverse=True)
# ---
def handleNode(currentNodeInAction, referenceNodeNow, referencesToCheck, patchMessageReferenceNode):
    for reference in referencesToCheck[:] :
        if reference in referenceNodeNow.get_children() :
            referencesToCheck.remove(reference)
            return patchMessageReferenceNode[reference]
    if len(referencesToCheck) == 0 :
        referenceNodeNow.get_children()[currentNodeInAction.get_node()] = currentNodeInAction
# ---
def submit(self, coro: Coroutine[Any, Any, T]) -> concurrent.futures.Future[T]:
        """
        Submit an async function without waiting for result.

        Args:
            coro: Coroutine to run

        Returns:
            Future that can be awaited or checked later
        """
        if self._loop is None:
            raise RuntimeError("AsyncBridge not started. Call start() first.")

        return asyncio.run_coroutine_threadsafe(coro, self._loop)
# ---
def possible_mock_calls(name, info):
    # NOTE(boris-42): py33 I hate you.
    return [mock.call(name, info=info), mock.call(name, info=py3_info(info))]
# ---
def free(self, num_bytes):
        self.current_mem -= num_bytes
        self.peak_mem = max(self.peak_mem, self.current_mem)
# ---
def test_augment_bank_increases_size(bank, rng):
    original_size = bank.total_entries
    augmented = augment_bank(bank, rng, n_renamed=1, n_perturbed=1, synthetic_count=10)
    assert augmented.total_entries > original_size
# ---
def canPlayLine(self, line, col):
        """
        Function to check if we can fill the line with a token.
        :param line: which line
        :param col: which column
        :return: true or false
        """
        if line == 5:
            return self.grille[line][col] == '0'
        else:
            return self.grille[line][col] == '0' and self.grille[line + 1][col] != '0'
# ---
def test_spatially_filter():
    # TODO:
    # - test that attributes are preserved
    # - Test that coordinates/shapes etc are the same
    # - Test that this works with a 3d wetmask(which is maybe a bit trickier)
    pass
# ---
def test_compute_bracket_mask_open_paren(tokenizer):
    mask = compute_bracket_mask("f(", tokenizer)
    # ) should be allowed (matches open paren).
    assert float(mask[tokenizer.encode_char(")")]) == 1.0
    # ] and } should be blocked (wrong bracket type).
    assert float(mask[tokenizer.encode_char("]")]) == 0.0
    assert float(mask[tokenizer.encode_char("}")]) == 0.0
# ---
def get_output_shape_for(self, input_shape):
        length = conv_output_length(input_shape[1], self.pool_length, self.border_mode, self.stride)
        return (input_shape[0], length, input_shape[2])
# ---
def __repr__(self):
        return f"JoinOp(type={self.join_type})"
# ---
def llama3_tokenizer():
    """Llama 3 tokenizer with chat template (uses tiktoken, not sentencepiece)."""
    return AutoTokenizer.from_pretrained("NousResearch/Meta-Llama-3-8B-Instruct")
# ---
def vmap_via(self, fn: Callable[[M], OutputT_co]) -> Callable[[], OutputT_co]: ...
# ---
def check_pyrefly(files: list[pathlib.Path], fix: bool) -> int:
    if not files:
        return 0

    click.echo("\nPyrefly type checker:")
    args = ["uvx", "pyrefly@0.40.0", "check", "--baseline", ".pyrefly-baseline.json"]
    return run_cmd(args).returncode
# ---
def test_terminate():
    """Terminate marks a job as STOPPED even if the underlying thread is still running."""
    stop = threading.Event()

    def hang():
        stop.wait(10)

    c = LocalClient(max_threads=4)
    handle = c.submit(JobRequest(name="hang", entrypoint=Entrypoint.from_callable(hang)))
    time.sleep(0.05)
    handle.terminate()
    assert handle.status() == JobStatus.STOPPED
    stop.set()  # unblock the thread so the executor can shut down
    c.shutdown(wait=True)
# ---
def test_add_event_after_connect(self):
        # new feature as of #2978
        canary = Mock()
        e1 = create_engine(config.db_url)
        assert not e1._has_events

        conn = e1.connect()

        event.listen(e1, "before_execute", canary.be1)
        conn.execute(select([1]))

        eq_(canary.be1.call_count, 1)

        conn._branch().execute(select([1]))
        eq_(canary.be1.call_count, 2)
# ---
def test_on_start_date(self):
        """Test when create_time is exactly on the start date"""
        self.assertTrue(check_create_time("2023-01-01 00:00:00 PST", "2023-01-01", "2023-01-31"))
# ---
def intermediate(self, x, y, z, *, static1, static2):
            assert static1 is True
            assert static2 is False
            return x + 2 * self.w + y + z
# ---
def __repr__(self):
        return f"MapOp(fn={_get_fn_name(self.fn)})"
# ---
def demo_client() -> IrisClient:
    manager = ClusterManager(_make_demo_config())
    manager.start()
    try:
        controller_url = manager.controller.discover()
        assert controller_url is not None

        client = IrisClient.remote(
            controller_url,
            workspace=Path(__file__).resolve().parents[3],
        )
        yield client
    finally:
        manager.stop()
# ---
def LoadWF(self, waveform, fs):
        self.s = waveform
        self.fs = fs
        self.sLength, self.nChans = self.s.shape
# ---
def two_unique_nums(nums):
  return [i for i in nums if nums.count(i)==1]
# ---
def total_trainable_params(self, vocab_size: int) -> int:
        """Return total trainable parameter count for this model configuration."""
        ...
# ---
def asinh(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in asinh")
    return elemwise(nxp.asinh, x, dtype=x.dtype)
# ---
def setUp(self):
        super(NovaProxyRequestHandlerBaseTestCase, self).setUp()

        self.wh = websocketproxy.NovaProxyRequestHandlerBase()
        self.wh.socket = mock.MagicMock()
        self.wh.msg = mock.MagicMock()
        self.wh.do_proxy = mock.MagicMock()
        self.wh.headers = mock.MagicMock()
# ---
def test_get_versions_list(self):

        response = requests.get(self.get_endpoint(
            TEMPL_ROOT_COLLECTION_ENDPOINT))

        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.json(), ["v1"])
# ---
def is_empty(self, resource):
        args = self._es_args(resource)
        res = self.es.count(body={'query': {'match_all': {}}}, **args)
        return res.get('count', 0) == 0
# ---
def is_wandb_available():
    try:
        import wandb
    except ImportError:
        return False
    return wandb is not None and wandb.run is not None
# ---
def test_log():
    with tempfile.TemporaryDirectory() as tmpdir:
        with SummaryWriter(logdir=tmpdir) as writer:
            tracker = TensorboardTracker(writer)
            tracker.log({"float": 2.0}, step=0)
            tracker.log({"str": "test"}, step=0)
            tracker.log({"scalar_jax_array": jnp.array(3.0)}, step=0)
            tracker.log({"scalar_np_array": np.array(3.0)}, step=0)
            tracker.log({"histogram": Histogram.from_array(jnp.array([1.0, 2.0, 3.0]))}, step=0)
# ---
def add_portal_ip(self, pip):
        '''add cluster ip'''
        self.put(Service.portal_ip, pip)
# ---
def get_gpu_count(device: cluster_pb2.DeviceConfig) -> int:
    """Extract GPU count from config."""
    if device.HasField("gpu"):
        return device.gpu.count or 1
    return 0
# ---
def test_metric_value(reduction, value, count, expected):
    """Metric.value() applies correct reduction."""
    m = Metric(_value=value, _count=count, reduction=reduction)
    assert jnp.allclose(m.value(), expected)
# ---
def test___cmp__eq(self):
        self._test__cmp__(
            lambda left, right: left == right,
            (
                False,
                True,
                True,
                False,
                True,
                False,
                not PY3,
                False,
                False,
                False,
            ),
            '=='
        )
# ---
def _resolve_bindings(array, bindings: Mapping[str, Axis | str | int]) -> AliasTable:
    b: dict[str, AxisSelector] = {}
    for name, selector in bindings.items():
        if isinstance(selector, str):
            try:
                selector = array.resolve_axis(selector)
            except ValueError:
                pass
        elif isinstance(selector, int):
            selector = Axis(name, selector)
        assert not isinstance(selector, int)
        b[name] = selector
    return AliasTable(b)
# ---
def shutdown(self, wait: bool = True) -> None:
        """Shutdown the client.

        Args:
            wait: If True, wait for pending jobs to complete (local mode only)
        """
        self._cluster_client.shutdown(wait=wait)
# ---
def shape(self):
        """Array dimensions."""
        return self._shape
# ---
def __init__(self, rname, namespace, kubeconfig, options):
        self.kubeconfig = kubeconfig
        self.name = rname
        self.namespace = namespace
        self._options = options
# ---
def test_build_bootstrap_script_prepends_discovery_preamble(
        self, minimal_bootstrap_config: config_pb2.BootstrapConfig
    ):
        """Discovery preamble is prepended to script."""
        preamble = "export CONTROLLER_ADDRESS=http://10.0.0.1:10000\n"
        script = _build_bootstrap_script(
            minimal_bootstrap_config,
            vm_address="10.0.0.1",
            discovery_preamble=preamble,
        )

        assert script.startswith(preamble)
# ---
def stop_job(job_id: str) -> None:
    """Stop a running Ray job.

    Note: This requires RAY_ADDRESS to be set, typically via ray_dashboard context manager.

    Args:
        job_id: The job ID or submission ID to stop
    """
    cmd = ["ray", "job", "stop", job_id]
    run_ray_command(cmd, timeout=60, capture_output=False)
# ---
def _check_worker_running(self, host: str) -> bool:
        """Check if a worker is healthy on the given host."""
        conn = self._create_ssh_connection(host)
        port = self._bootstrap_config.worker_port or 10001
        try:
            result = conn.run(f"curl -sf http://localhost:{port}/health", timeout=Duration.from_seconds(10))
            return result.returncode == 0
        except Exception:
            return False
# ---
def preempted_always():
        return ray.get(actor.run.remote())
# ---
def __ge__(self, other: "Duration") -> bool:
        return self._ms >= other._ms
# ---
def scan(self, *args, **kwargs): ...
# ---
def __iter__(self):
        return chunk_keys(self.shape, self.chunks)
# ---
def print_nofill_field(label, value):
    sys.stdout.write(label + DELIMITER(" " * (FIELD_NAME_WIDTH - len(label))) + value + '\n')
# ---
def _is_subsequence(needle, haystack):
    needle_i = 0
    haystack_j = 0
    while needle_i < len(needle) and haystack_j < len(haystack):
        if needle[needle_i] == haystack[haystack_j]:
            needle_i += 1
        haystack_j += 1

    if needle_i < len(needle):
        return False
    return True
# ---
def supportsSeeking(self):
		return hasattr(self.file, "seek")
# ---
def host(self):
        # it appears that httptools doesn't return the host
        # so pull it from the headers
        return self.headers.get("Host", "")
# ---
def convert_hn(self, n, el, text, convert_as_inline):
        if convert_as_inline:
            return text

        style = self.options["heading_style"].lower()
        text = text.strip()
        if style == markdownify.UNDERLINED and n <= 2:
            line = "=" if n == 1 else "-"
            return self.underline(text, line)
        hashes = "#" * n
        if style == markdownify.ATX_CLOSED:
            return f"{hashes} {text} {hashes}\n\n"

        return f"\n\n{hashes} {text}\n\n"
# ---
def _hackable_125m_config() -> HackableTransformerConfig:
    # Match the 130m preset dims from hackable transformer starter, but use 2048 context for parity with grug defaults.
    return HackableTransformerConfig(
        max_seq_len=2048,
        hidden_dim=512,
        intermediate_dim=1792,
        num_layers=6,
        num_heads=8,
        num_kv_heads=8,
        head_dim=None,
        use_attention_sink=False,
    )
# ---
def coverage(geom, srs):
    if isinstance(geom, (list, tuple)):
        return BBOXCoverage(geom, srs)
    else:
        return GeomCoverage(geom, srs)
# ---
def default_view_class(self):
		""""""
		if TRACE: print(__name__), self.default_view_class.__doc__

		return WaspView
# ---
def f(p):
            token_ids = jnp.zeros((batch, seq), dtype=jnp.int32)
            token_ids = jax.sharding.reshard(token_ids, Pbatch)
            loss_weight = jnp.ones((batch, seq), dtype=jnp.float32)
            loss_weight = jax.sharding.reshard(loss_weight, Pbatch)
            return loss_fn(p, token_ids, loss_weight, cfg, mask=AttentionMask.causal(), reduction="mean")
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("x", Integer),
            Column("y", Integer),
        )
# ---
def _setup_related(self, env):
        super(Selection, self)._setup_related(env)
        # selection must be computed on related field
        field = self.related_field
        self.selection = lambda model: field._description_selection(model.env)
# ---
def set_current_label_name(self):
        self.current_label.name = self.ui.comboBoxFuzzingLabel.currentText()
        self.ui.comboBoxFuzzingLabel.setItemText(self.ui.comboBoxFuzzingLabel.currentIndex(), self.current_label.name)
# ---
def free_pages_for_finished(self, finished_mask: jnp.ndarray) -> "DecodeState":
        sequences, page_table = self.sequences.free_pages_for_finished(self.page_table, finished_mask)
        return dataclasses.replace(self, sequences=sequences, page_table=page_table)
# ---
def present(self):
        if self.exists():
            return False
        return self.create()
# ---
def test_map_transforms_elements_of_a_list(self):
        seq = [1, 2, 3]
        mapped_seq = list()

        mapping = map(self.add_ten, seq)

        self.assertNotEqual(list, mapping.__class__)
        self.assertEqual(map, mapping.__class__)
        # In Python 3 built in iterator funcs return iterable view objects
        # instead of lists

        for item in mapping:
            mapped_seq.append(item)

        self.assertEqual([11, 12, 13], mapped_seq)
# ---
def __init__(self, monitor, parent=None):
        """
        Observe the given ``monitor`` (a :class:`~pyudev.Monitor`):

        ``parent`` is the parent :class:`~PyQt4.QtCore.QObject` of this
        object.  It is passed unchanged to the inherited constructor of
        :class:`~PyQt4.QtCore.QObject`.
        """
        QObject.__init__(self, parent)
        self._setup_notifier(monitor, QSocketNotifier)
# ---
import re 
regex = '^[a-z0-9]+[\._]?[a-z0-9]+[@]\w+[.]\w{2,3}$'
def check_email(email): 
	if(re.search(regex,email)): 
		return ("Valid Email") 
	else: 
		return ("Invalid Email")
# ---
def list_tasks(self, job_id: JobName) -> list[cluster_pb2.TaskStatus]:
        """List all tasks for a job.

        Args:
            job_id: Job ID to query tasks for

        Returns:
            List of TaskStatus protos, one per task in the job
        """
        request = cluster_pb2.Controller.ListTasksRequest(job_id=job_id.to_wire())
        response = self._client.list_tasks(request)
        return list(response.tasks)
# ---
def test_build_event(self):
        evt = event.build_event("evt_name", arg1="val1", arg2=None)
        assert evt.name == "evt_name"
        assert evt.args == {'arg1': "val1", 'arg2': None}
# ---
def bin_path(self) -> str:
        """Path to the bin directory (e.g., venv/bin)."""
        return os.path.join(self.venv_path, "bin")
# ---
def unsupported_kernel(method, notebook, data):
    return {'username': SAAGIE_USERNAME}
# ---
def lcs_of_three(X, Y, Z, m, n, o): 
	L = [[[0 for i in range(o+1)] for j in range(n+1)] 
		for k in range(m+1)] 
	for i in range(m+1): 
		for j in range(n+1): 
			for k in range(o+1): 
				if (i == 0 or j == 0 or k == 0): 
					L[i][j][k] = 0
				elif (X[i-1] == Y[j-1] and
					X[i-1] == Z[k-1]): 
					L[i][j][k] = L[i-1][j-1][k-1] + 1
				else: 
					L[i][j][k] = max(max(L[i-1][j][k], 
					L[i][j-1][k]), 
									L[i][j][k-1]) 
	return L[m][n][o]
# ---
def predict_dataloader(self) -> DataLoader:
        return DataLoader(
            self.predict_set,
            batch_size=self.cfg.batch_size,
            num_workers=self.cfg.num_workers,
            pin_memory=self.cfg.pin_memory,
            shuffle=False,
            collate_fn=collate,
        )
# ---
def test_success(tmp_path, timing_map, n_tasks, retries, use_backups):
    outputs = asyncio.run(
        run_test(
            function=partial(deterministic_failure, tmp_path, timing_map),
            input=range(n_tasks),
            retries=retries,
            use_backups=use_backups,
        )
    )

    assert outputs == set(range(n_tasks))
    check_invocation_counts(tmp_path, timing_map, n_tasks)
# ---
def test_corrupt_program_graceful_when_no_mutations_possible():
    """If the bank has no matching types, corruption returns the original."""
    bank = SubtreeBank()  # empty bank
    source = "x = 1\n"
    corrupted, mutations = corrupt_program(source, num_steps=5, bank=bank)
    assert corrupted == source
    assert mutations == []
# ---
def poke(self, context):
        logging.info('Poking for prefix : {self.prefix}\n'
                     'in bucket s3://{self.bucket_name}'.format(**locals()))
        import airflow.hooks.S3_hook
        hook = airflow.hooks.S3_hook.S3Hook(s3_conn_id=self.s3_conn_id)
        return hook.check_for_prefix(
            prefix=self.prefix,
            delimiter=self.delimiter,
            bucket_name=self.bucket_name)
# ---
def reload_model(self, model: LmHeadModel | None, state_dict: dict) -> LmHeadModel | None:
        raise NotImplementedError
# ---
def test_get_job_status_not_found(service):
    """Verify get_job_status raises ConnectError for unknown job."""
    request = cluster_pb2.Controller.GetJobStatusRequest(job_id=JobName.root("nonexistent").to_wire())

    with pytest.raises(ConnectError) as exc_info:
        service.get_job_status(request, None)

    assert exc_info.value.code == Code.NOT_FOUND
    assert "nonexistent" in exc_info.value.message
# ---
def test_convert_page_with_resiliparse(sample_html_simple):
    config = ResiliparseConfig()
    result = convert_page(sample_html_simple, url=None, extract_method="resiliparse", config=config)

    assert "content" in result
    assert "title" in result
    assert "html" in result
    assert result["content"] is not None
    assert len(result["content"]) > 0
# ---
def test_devices():
    assert len(info.devices()) > 0
# ---
def name(self) -> str:
        """Name of this scale group."""
        return self._config.name
# ---
def __init__(self,occurrence):
        self.label = occurrence[0]
# ---
def flatten(x):
    return reshape(x, (-1,))
# ---
def job_id(self) -> JobName | None:
        return self._job.job_id if self._job else None
# ---
def venv_path(self) -> str:
        """Path to the virtual environment directory (.venv/ if workspace, else temp root)."""
        if not self._temp_dir:
            raise RuntimeError("TemporaryVenv must be entered before accessing venv_path")
        if self._workspace:
            return os.path.join(self.workspace_path, ".venv")
        else:
            return self.workspace_path
# ---
def binary_to_decimal(binary): 
    binary1 = binary 
    decimal, i, n = 0, 0, 0
    while(binary != 0): 
        dec = binary % 10
        decimal = decimal + dec * pow(2, i) 
        binary = binary//10
        i += 1
    return (decimal)
# ---
def without_axes(axis_spec: ShapeDict, to_remove: AxisSelection, allow_mismatched_sizes=False) -> ShapeDict:  # type: ignore
    ...
# ---
def test_get_job_status_returns_status(service, job_request):
    """Verify get_job_status returns correct status for launched job."""
    service.launch_job(job_request("test-job"), None)

    request = cluster_pb2.Controller.GetJobStatusRequest(job_id=JobName.root("test-job").to_wire())
    response = service.get_job_status(request, None)

    assert response.job.job_id == JobName.root("test-job").to_wire()
    assert response.job.state == cluster_pb2.JOB_STATE_PENDING
# ---
def load_dt_model(pickle_model):
    """
    Retrieve model using Pickle binary format.

    :param string pickle_model: location of Pickle model
    :return: Pickle model for re-use
    :rtype: object
    """
    return pickle.loads(pickle_model)
# ---
def test_different_expr_different_hash(self):
        expr1 = col("score") > 0.5
        expr2 = col("score") < 0.5
        assert hash(expr1) != hash(expr2)
# ---
def post_delete(request, slug=None):
    if not request.user.is_staff or not request.user.is_superuser:
        raise Http404
    instance = get_object_or_404(Post, slug=slug)
    instance.delete()
    messages.success(request, "Successfully deleted")
    return redirect("posts:list")
# ---
def output_url(self, m):
        link = m.group(1)
        if self._in_link:
            return self.renderer.text(link)
        return self.renderer.autolink(link, False)
# ---
def to_slice(self) -> slice:
        return slice(self.start, self.start + self.size)
# ---
def test_status_transitions(client: LocalClient):
    handle = client.submit(JobRequest(name="slow", entrypoint=Entrypoint.from_callable(_sleep_then_succeed)))
    # Should be running or pending initially
    initial = handle.status()
    assert initial in (JobStatus.PENDING, JobStatus.RUNNING)
    handle.wait()
    assert handle.status() == JobStatus.SUCCEEDED
# ---
def test_optimize_consecutive_maps():
    """Consecutive maps should be fused into a single stage."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            MapOp(lambda x: x + 1),
            MapOp(lambda x: x * 3),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    assert len(plan.stages[0].operations) == 1
    fused_op = plan.stages[0].operations[0]
    assert isinstance(fused_op, Map)
# ---
def __getstate__(self):
        """Always load data in-memory before pickling"""
        self.load()
        # self.__dict__ is the default pickle object, we don't need to
        # implement our own __setstate__ method to make pickle work
        state = self.__dict__.copy()
        # throw away any references to datastores in the pickle
        state['_file_obj'] = None
        return state
# ---
def testWriteExceptDispatcherBareExceptionNotLast(self):
    visitor = stmt.StatementVisitor(_MakeModuleBlock())
    handlers = [ast.ExceptHandler(type=None),
                ast.ExceptHandler(type=ast.Name(id='foo'))]
    self.assertRaisesRegexp(util.ParseError, r"default 'except:' must be last",
                            visitor._write_except_dispatcher,  # pylint: disable=protected-access
                            'exc', 'tb', handlers)
# ---
def col_clause(self):
        return None, ()
# ---
def __getitem__(self, key):
        idx = ndindex[key]
        newshape = idx.newshape(self.shape)
        # use broadcast trick so array chunks only occupy a single value in memory
        return broadcast_trick(nxp.full)(
            newshape, fill_value=self.fill_value, dtype=self.dtype
        )
# ---
def __init__(self, config):
        self.config = config

        # State tracking
        self.worker = None
        self.thread = None
        self.result_queue = queue.Queue(maxsize=1)
# ---
def importance_sampling_transformation(self, t):
    f_T = torch.log1p(- torch.exp(- self.sigma_max))
    f_0 = torch.log1p(- torch.exp(- self.sigma_min))
    sigma_t = - torch.log1p(- torch.exp(t * f_T + (1 - t) * f_0))
    t = - torch.expm1(- sigma_t) / (1 - self.eps)
    return t
# ---
def num_train_steps(self) -> int:
        return self.config.num_train_steps
# ---
def run_hooks(self, info: StepInfo, force: bool = False):
        for hook in self.hooks:
            if force or info.step % hook.every == 0:
                hook.fn.on_step(info, force=force)
# ---
def quick_job():
        return 42
# ---
def maximize_elements(test_tup1, test_tup2):
  res = tuple(tuple(max(a, b) for a, b in zip(tup1, tup2))
   for tup1, tup2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def is_position_token(self, token_id: int) -> bool:
        """Check if a token ID is a position token."""
        return self.position_token_offset <= token_id < self.position_token_offset + self.num_position_tokens
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        """Convert to proto for RPC APIs."""
        ...
# ---
import re
def remove_lowercase(str1):
 remove_lower = lambda text: re.sub('[a-z]', '', text)
 result =  remove_lower(str1)
 return result
# ---
def __ne__(self, other):
        if not isinstance(other, GeomCoverage):
            return NotImplemented
        return not self.__eq__(other)
# ---
def __init__(self, *args, bytes_strategy="base64", **kwargs):
        # bytes_strategy: "base64" | "repr" | "hex"
        super().__init__(*args, **kwargs)
        self.bytes_strategy = bytes_strategy
# ---
def _get_step_size(self, budget: float) -> int:
        """Get hidden_size search step size based on budget."""
        if budget > self.budget_step_threshold:
            return self.large_budget_step_size
        return self.small_budget_step_size
# ---
def fetch_logs(self):
        job_data = requests.get(self.url, auth=SAAGIE_BASIC_AUTH_TOKEN).json()
        run_data = job_data.get('last_instance')
        if run_data is None or run_data['status'] not in ('SUCCESS', 'FAILED'):
            return
        run_data = requests.get(
            get_absolute_saagie_url('/api/v1/jobtask/%s'
                                    % run_data['id']), auth=SAAGIE_BASIC_AUTH_TOKEN).json()
        self.last_run = SaagieJobRun(self, run_data)
# ---
import math  
def even_binomial_Coeff_Sum( n): 
    return (1 << (n - 1))
# ---
def getcommissioninfo(self, data):
        if data._name in self.comminfo:
            return self.comminfo[data._name]

        return self.comminfo[None]
# ---
def __setitem__(self, index, value):
		if not isinstance(index, int):
			raise TypeError("Expected int instance, got %s instead (%r)" % (type(index), index))
		list.__setitem__(self, index, value)
		col = self.structure[index]
		self._values[col.name] = col.to_python(value, row=self)
# ---
def paths(self):
        '''the path attribute split by /'''
        return filter(None, self.path.split('/'))
# ---
def files(self):
        """Explicitly listed files or patterns or roots:
        if no patterns or .always(): empty list,
        if exact: list exact files,
        if not .anypats(): list all files and dirs,
        else: optimal roots"""
        return self._files
# ---
def shutdown(self, graceful: bool = True) -> bool:
        """Shutdown worker container."""
        cmd = "docker stop iris-worker" if graceful else "docker kill iris-worker"
        try:
            self._conn.run(cmd, timeout=Duration.from_seconds(30))
            return True
        except Exception:
            return False
# ---
def __init__(self, in_array: NamedArray):
                self.array = in_array
                self.array2 = hax.zeros(Dim3)
# ---
def user_linked_in_url(self):
        return self._get_profile().linked_in_url
# ---
def iter_from_step(self, start_from_batch: int | None = None):
        # sometimes we pass in an array for the start_from_batch, so we need to check for that
        start_from_batch = int(start_from_batch) if start_from_batch is not None else None
        return DataLoaderIterator(self, start_from_batch=start_from_batch)
# ---
def _infer_attention_output_block_shape(QPos, KPos, Key, q_i, k, v):
    out_shape = filter_eval_shape(hnn.attention.dot_product_attention, KPos, Key, q_i, k, v)
    return out_shape.axes
# ---
def init(max_pages: int, max_seqs: int, page_size: int, max_pages_per_seq: int) -> "PageTable":
        ref_counts = hax.full({"page": max_pages}, 0, dtype=jnp.int32)
        return PageTable(ref_counts, page_size, max_seqs, max_pages_per_seq)
# ---
def suppress_logging(is_master):
    """Suppress logging for non-master processes."""
    if not is_master:
        # Get root logger
        root = logging.getLogger()
        root.setLevel(logging.WARNING)

        # Also suppress any existing handlers on the root logger
        for handler in root.handlers:
            handler.setLevel(logging.WARNING)
# ---
def block_function(out_key):
        out_coords = out_key[1:]
        return ((in_name, *out_coords),)
# ---
def get_lm_head(self) -> hax.NamedArray:
        assert self.lm_head.bias is None
        return self.lm_head.weight
# ---
def test_create_actor_and_call_remote(client: LocalClient):
    actor = client.create_actor(Counter, name="counter")
    result = actor.increment.remote(5).result()
    assert result == 5
    assert actor.get.remote().result() == 5
# ---
def synthetic_target_extra_dependencies(self, target, target_workdir):
    for source in target.sources_relative_to_buildroot():
      if self._declares_service(os.path.join(get_buildroot(), source)):
        return self._service_deps
    return self._deps
# ---
def __init__(self, number: int):
        self.name = '{}'.format(number)
        tactic = randint(0, FIG_LEN-1)
        self.main_figure = FIGURES[tactic]
        self.__figures = [FIGURES[(tactic+i) % FIG_LEN] for i in range(FIG_LEN)]
# ---
def restart(self) -> str:
        """Stop then start controller."""
        self.stop()
        return self.start()
# ---
def hang():
        stop.wait(10)
# ---
def characterization_insertion_transfection(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['stable transfection'],
        'modified_site_nonspecific': 'random',
        'introduced_elements': 'synthesized DNA'
    }
# ---
def _convert_token_to_id(self, token: str) -> int:
        return int(token)
# ---
def model_params(self) -> M:
        pass
# ---
def lambda_fn(x):
      return math_ops.matmul(x[0], x[1])
# ---
def _nanmean_aggregate(a):
    with np.errstate(divide="ignore", invalid="ignore"):
        return nxp.divide(a["total"], a["n"])
# ---
def test_entrypoint_params_cpu():
    from fray.v2.ray_backend.backend import get_entrypoint_params

    request = JobRequest(
        name="cpu-job",
        entrypoint=Entrypoint.from_binary("echo", ["hello"]),
        resources=ResourceConfig(cpu=4, ram="2g"),
    )
    params = get_entrypoint_params(request)
    assert params["entrypoint_num_cpus"] == 4.0
    assert "entrypoint_num_gpus" not in params
    assert params["entrypoint_memory"] > 0
# ---
def build_tensorboard_command(executable: str, logdir: Path, port: Optional[int]) -> list[str]:
    command = [executable, f"--logdir={logdir}"]
    if port is not None:
        command.append(f"--port={port}")
    return command
# ---
def __getitem__(cls, item: NamedArrayAxesSpec) -> typing.Any:
        axes = _parse_namedarray_axes(item)
        return typing.Annotated[NamedArray, axes]
# ---
def _remove_textbox(self, textbox):
        """Remove the given L{TextBox} from the list of widgets to do
            auto-correction on.
            """
        if not hasattr(self, '_textbox_insert_ids'):
            return
        # Disconnect the "insert-text" event handler
        textbox.disconnect(self._textbox_insert_ids[textbox])

        self.widgets.remove(textbox)
# ---
def get_ready_event(self):
        """
        Returns an Event that is set when the control greenlet is up and running.
        """
        return self._ready_control
# ---
def list_jobs(ctx):
    """List Ray jobs."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        print(json.dumps(_list_jobs(), indent=2))
# ---
def __eq__(self, other):
        if not isinstance(other, MultiCoverage):
            return NotImplemented

        if self.bbox != other.bbox:
            return False

        if len(self.coverages) != len(other.coverages):
            return False

        for a, b in zip(self.coverages, other.coverages):
            if a != b:
                return False

        return True
# ---
def test_to_from_state_dict():
    a = jnp.array([1, 2])
    b = jnp.array([3, 4])
    m = M(a, b)

    state_dict = to_state_dict(m)
    assert state_dict == {"a": a, "b": b}

    m2 = M(jnp.array([0, 0]), jnp.array([0, 0]))
    m2 = from_state_dict(m2, state_dict)
    assert jnp.all(m2.a == a)
    assert jnp.all(m2.b == b)
# ---
def __iter__(self):
        for f in self.FIELDS:
            yield getattr(self, f, '')
# ---
def forward(self, x, c):
    shift, scale = self.adaLN_modulation(c)[:, None].chunk(2, dim=2)
    x = modulate_fused(self.norm_final(x), shift, scale)
    x = self.linear(x)
    return x
# ---
def assert_same_version(**kwargs):
            output_path = get_output_path(**(defaults | kwargs))
            assert output_path == default_output_path
# ---
def depth_offsets(d):
        if isinstance(d, int):
            return -d, d
        return d
# ---
def delete(self, rsc):
        return rsc.delete()
# ---
def get_client(self) -> IrisClient:
        if self._rpc_client is None:
            self._rpc_client = IrisClient.remote(
                f"http://127.0.0.1:{self._controller_port}",
                workspace=Path(__file__).parent.parent.parent,  # lib/iris
            )
        return self._rpc_client
# ---
def _mean_groupby_combine(a, axis, dummy_axis, dtype, keepdims):
    # only combine over the dummy axis, to preserve grouping along 'axis'
    dtype = dict(dtype)
    n = nxp.sum(a["n"], dtype=dtype["n"], axis=dummy_axis, keepdims=keepdims)
    total = nxp.sum(
        a["total"], dtype=dtype["total"], axis=dummy_axis, keepdims=keepdims
    )
    return {"n": n, "total": total}
# ---
def multiply(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "multiply")
    if x1.dtype not in _numeric_dtypes or x2.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in multiply")
    return elemwise(nxp.multiply, x1, x2, dtype=result_type(x1, x2))
# ---
def test_permute_errors_on_invalid_set_of_dims_indices(self):
    with self.assertRaisesRegexp(ValueError, r'Invalid permutation .*dims.*'):
      testing_utils.layer_test(
          keras.layers.Permute,
          kwargs={'dims': (1, 4, 2)}, input_shape=(3, 2, 4))
# ---
def deterministic_failure_modal(i, path=None, timing_map=None, *, name=None):
    return deterministic_failure(path, timing_map, i, name=name)
# ---
def _lookup_indices(self, axis: AxisSelector) -> int | None:  # type: ignore
        ...
# ---
def _track_rollout_generation(self):
        """Called when rollout is generated."""
        self.rollouts_generated += 1
# ---
def validate_password(self, login, password):
    if login in users :
        if encrypt(password) == users[login] :
            cherrypy.session['username'] = login
            cherrypy.session['database'] = userdatabase(login)
            return True

    return False
# ---
def list_instances(project: str, zone: str, filter_expr: str | None = None) -> list[dict[str, Any]]:
    """List GCP compute instances."""
    cmd = [
        "gcloud",
        "compute",
        "instances",
        "list",
        f"--project={project}",
        f"--zones={zone}",
        "--format=json",
    ]

    if filter_expr:
        cmd.append(f"--filter={filter_expr}")

    result = run_gcloud_command(cmd)
    return json.loads(result.stdout)
# ---
def total_noise(self, t):
    cos = torch.cos(t * torch.pi / 2)
    return - torch.log(self.eps + (1 - self.eps) * cos)
# ---
def lit(value: Any) -> LiteralExpr:
    """Create a literal value expression.

    Example:
        >>> col("a") + lit(10)
        (col('a') + lit(10))
    """
    return LiteralExpr(value)
# ---
def _replace(self, fname, force=False):
        '''replace the current object with oc replace'''
        # We are removing the 'resourceVersion' to handle
        # a race condition when modifying oc objects
        yed = Yedit(fname)
        results = yed.delete('metadata.resourceVersion')
        if results[0]:
            yed.write()

        cmd = ['replace', '-f', fname]
        if force:
            cmd.append('--force')
        return self.openshift_cmd(cmd)
# ---
def __init__(self, ebox_data, sensor_type, name):
        """Initialize the sensor."""
        self.client_name = name
        self.type = sensor_type
        self._name = SENSOR_TYPES[sensor_type][0]
        self._unit_of_measurement = SENSOR_TYPES[sensor_type][1]
        self._icon = SENSOR_TYPES[sensor_type][2]
        self.ebox_data = ebox_data
        self._state = None
# ---
def get_date(self, p_tag):
        """ Given a date tag, return a date object. """
        string = self.tag_value(p_tag)
        result = None

        try:
            result = date_string_to_date(string) if string else None
        except ValueError:
            pass

        return result
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("data", String(100)),
        )
# ---
def test_vm_detail_page_escapes_vm_id(client):
    """VM detail page escapes the VM ID to prevent XSS."""
    response = client.get('/vm/"onmouseover="alert(1)')
    assert response.status_code == 200
    assert "onmouseover" not in response.text or "&quot;" in response.text
# ---
def test_clear(self):
        storage = MemcachedStorage("memcached://localhost:22122")
        limiter = FixedWindowRateLimiter(storage)
        per_min = RateLimitItemPerMinute(1)
        limiter.hit(per_min)
        assert not limiter.hit(per_min)
        limiter.clear(per_min)
        assert limiter.hit(per_min)
# ---
def __init__(self, name, zarray, spec, plan):
        self.name = name
        self._zarray = zarray
        self._shape = zarray.shape
        self._dtype = zarray.dtype
        self._chunks = normalize_chunks(
            zarray.chunks, shape=self.shape, dtype=self.dtype
        )
        # get spec from config if not supplied
        self.spec = spec or spec_from_config(config)
        self.plan = plan
# ---
def source_label(self) -> str:
        return f"{self.benchmark}:{self.date_range}"
# ---
def get_lm_head(self) -> hax.NamedArray:
        if self.lm_head is None:
            return self.embeddings.token_embeddings.weight
        else:
            return self.lm_head.weight
# ---
def test_basic_auth_with_single_quoted_realm(self):
        self.test_basic_auth(quote_char="'")
# ---
def create_process(self, process_id, vpnservice, namespace):
        return strongswan_ipsec.StrongSwanProcess(
            self.conf,
            process_id,
            vpnservice,
            namespace)
# ---
def find_demlo(s): 
	l = len(s) 
	res = "" 
	for i in range(1,l+1): 
		res = res + str(i) 
	for i in range(l-1,0,-1): 
		res = res + str(i) 
	return res
# ---
def _find_all_recursive(bundle_path: Path, pattern: str) -> list[Path]:
    return list(bundle_path.rglob(pattern))
# ---
def shouldnt_run_fn(cfg: DummyCfg):
    raise RuntimeError("This function should not run.")
# ---
def test_can_import_read_op(self):
    try:
      from dask_ee import read_ee
    except ModuleNotFoundError:
      self.fail('Cannot import `read_ee` function.')
# ---
def validate_description(self):
		'''Clean HTML description if set'''
		if cint(frappe.db.get_single_value('Stock Settings', 'clean_description_html')):
			self.description = clean_html(self.description)
# ---
def test_stragglers(tmp_path, timing_map, n_tasks, retries):
    outputs = asyncio.run(
        run_test(
            function=partial(deterministic_failure, tmp_path, timing_map),
            input=range(n_tasks),
            retries=retries,
            use_backups=True,
        )
    )

    assert outputs == set(range(n_tasks))

    check_invocation_counts(tmp_path, timing_map, n_tasks, retries)
# ---
def get_default_zone() -> str | None:
    """Get the default GCP zone."""
    try:
        result = run_gcloud_command(["gcloud", "config", "get-value", "compute/zone"])
        return result.stdout.strip() or None
    except RuntimeError:
        return None
# ---
def raw_loss_fn(model, batch):
        val = hax.mean(batch["value"])
        if has_metrics:
            return val, {"metric_a": val * 2, "metric_b": val + 10}
        return val
# ---
import math
def radian_degree(degree):
 radian = degree*(math.pi/180)
 return radian
# ---
def state_dict(self):
    return {'random_state': self.generator.get_state(),
            'counter': self.counter}
# ---
def _matmul(a, b):
    chunk = nxp.matmul(a, b)
    return chunk[..., nxp.newaxis, :]
# ---
def test_df_input2(self):
        def test_impl(df):
            C = df.B == 'two'
            return C.sum()

        n = 11
        df = pd.DataFrame({'A': np.random.ranf(3 * n), 'B': ['one', 'two', 'three'] * n})
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))
# ---
def create_nano_optimizer_config() -> AdamConfig:
    """Create a minimal AdamConfig for testing."""
    return AdamConfig(
        learning_rate=1e-3,
        weight_decay=0.00,
        warmup=0.0,
        lr_schedule="constant",
    )
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)
        return ScaleByMuonHState(momentum_buffer=momentum_buffer)
# ---
def checkpoint_path(self) -> str:
        base = f"{BASE_CHECKPOINT_PREFIX}/{self.run_name}/checkpoints"
        if self.checkpoint_step is None:
            return base
        return f"{base}/step-{self.checkpoint_step}"
# ---
def __init__(
        self,
        loss_fn: LossFnWithContext,
        *,
        gradient_weight: float,
        pad_mode: str,
    ):
        self.loss_fn = loss_fn
        self._gradient_weight = gradient_weight
        self._pad_mode = pad_mode
# ---
def pack_next_sequence(self, max_tokens: int) -> tuple["DecodeState", PackedSequence]:  # type: ignore[name-defined]
        """Forward ``pack_next_sequence`` to ``TokenQueue`` and return updated ``DecodeState`` plus the ``PackedSequence``."""
        new_tqueue, packed = self.tqueue.pack_next_sequence(max_tokens)
        return dataclasses.replace(self, tqueue=new_tqueue), packed
# ---
def _parse_arrow_message(self, message):
        self._parse_arrow_schema()

        return pyarrow.ipc.read_record_batch(
            pyarrow.py_buffer(message.arrow_record_batch.serialized_record_batch),
            self._schema,
        )
# ---
def clean_ar5iv_record(html_blob: dict) -> dict:
    """Clean HTML in a single ar5iv record.

    Args:
        html_blob: Record with 'id' and 'text' (HTML content)

    Returns:
        Record with cleaned HTML text
    """
    content = clean_html(html_blob["text"])
    return {
        "id": html_blob["id"],
        "text": content,
        "source": "ar5iv",
        "added": datetime.datetime.now().isoformat(),
    }
# ---
def _receive_message(self):
        stop_message = jnp.array(_Message.STOP)
        message = broadcast_shard(stop_message, PartitionSpec())
        return message.item()
# ---
def get_status_path(output_path: str) -> str:
    """Return the path of the status file associated with `output_path`."""
    return os.path.join(output_path, ".executor_status")
# ---
def serve(self):
        if not self.server:
            console.print("[red]No model loaded. Use 'load' command first[/red]")
            return

        console.print("[blue]Starting inference server...[/blue]")
        self.server.serve()
# ---
def __init__(self, *args):
    self.custom_objects = args
    self.backup = None
# ---
def axis_name(ax: AxisSelection) -> str | tuple[str, ...]:
    """
    Returns the name of the axis. If ax is a string, returns ax. If ax is an Axis, returns ax.name
    """

    def _ax_name(ax: AxisSelector) -> str:
        if isinstance(ax, Axis):
            return ax.name
        else:
            return ax

    if isinstance(ax, (Axis, str)):
        return _ax_name(ax)
    else:
        return tuple(_ax_name(x) for x in ax)
# ---
def set_git_ssh(ssh_wrapper, key_file, ssh_opts):

    if os.environ.get("GIT_SSH"):
        del os.environ["GIT_SSH"]
    os.environ["GIT_SSH"] = ssh_wrapper

    if os.environ.get("GIT_KEY"):
        del os.environ["GIT_KEY"]

    if key_file:
        os.environ["GIT_KEY"] = key_file

    if os.environ.get("GIT_SSH_OPTS"):
        del os.environ["GIT_SSH_OPTS"]

    if ssh_opts:
        os.environ["GIT_SSH_OPTS"] = ssh_opts
# ---
def forward(self, inputs):
        x = self.relu(inputs)
        return x
# ---
def ndim(self) -> int:
        return len(self.shape)
# ---
def kth_element(arr, n, k):
  for i in range(n):
    for j in range(0, n-i-1):
      if arr[j] > arr[j+1]:
        arr[j], arr[j+1] == arr[j+1], arr[j]
  return arr[k-1]
# ---
def virtual_in_memory(
    array: np.ndarray,
    chunks: T_RegularChunks,
) -> VirtualInMemoryArray:
    return VirtualInMemoryArray(array, chunks)
# ---
def find_secret(self, key):
        ''' find secret'''
        rval = None
        try:
            rval = self.secrets[key]
        except KeyError as _:
            return None

        return {'key': key, 'value': rval}
# ---
def pending_count(self) -> int:
        """Number of pending nodes."""
        return len(self.pending_nodes)
# ---
def get_jobs_status():
    ret = {}
    with _lock:
        items = tuple(_jobs.items())
    for job_id, job in items:
        ret[job_id] = {
            'status': job.status,
            'description': job.description,
            'progress': job.progress
        }
    return ret
# ---
def read(fd):
            data = os.read(fd, 1024)
            output.append(data)
            return data
# ---
def kernel_name(self):
        return self.json['metadata']['kernelspec']['name']
# ---
def test_tokenizer():
    return create_test_tokenizer()
# ---
def _unpatch_tqdm(self):
        """Restore original tqdm."""
        if self._original_tqdm is not None:
            import tqdm as tqdm_module
            tqdm_module.tqdm = self._original_tqdm
            self._original_tqdm = None
# ---
def set_match(self, match):
        self._set_panel_match(match, 0, 0)
        self._set_panel_match(match, 1, 2)
        self._set_panel_match(match, 2, 1)
        self._set_panel_match(match, 3, 3)
        self.match_num_ctrl.SetValue(str(match.match_number))
# ---
def even_num(x):
  if x%2==0:
     return True
  else:
    return False
# ---
def test_select_columns(backend):
    """Test column projection with select."""
    ds = Dataset.from_list(
        [
            {"id": 1, "name": "alice", "score": 80, "extra": "x"},
            {"id": 2, "name": "bob", "score": 60, "extra": "y"},
        ]
    ).select("id", "name")

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert results[0] == {"id": 1, "name": "alice"}
    assert results[1] == {"id": 2, "name": "bob"}
# ---
def __len__(self) -> int:
        return len(self.example_by_step)
# ---
def create_actor(self) -> ActorHandle:
        raise NotImplementedError()
# ---
def test_init_params_shapes(params, tiny_cfg):
    assert params.token_embed.shape == (tiny_cfg.vocab_size, tiny_cfg.hidden_dim)
    assert params.output_proj.shape == (tiny_cfg.hidden_dim, tiny_cfg.vocab_size)
    assert params.final_norm.shape == (tiny_cfg.hidden_dim,)
    assert len(params.blocks) == tiny_cfg.num_layers
# ---
def __hash__(self):
        return hash(tuple(self.sorts))
# ---
def iter(self):
        yield self
        parent = object.__getattribute__(self, '_parent')
        if parent is not None:
            for ancestor in parent.iter():
                yield ancestor
# ---
def Embed(self) -> Axis:
        # Not used by GrugWrapper (it returns logits directly), but LmConfig requires it.
        return Axis("embed", self.hidden_dim)
# ---
def __init__(
    self, hidden_size, out_channels, cond_dim, causal=False
  ):
    super().__init__()
    self.causal = causal
    assert causal == True

    self.norm_final = LayerNorm(hidden_size)
    self.linear = nn.Linear(hidden_size, out_channels)
    self.linear.weight.data.zero_()
    self.linear.bias.data.zero_()
# ---
def choosers(test_data):
    filen = os.path.join(
        os.path.dirname(__file__), 'data', test_data['choosers'])
    return pd.read_csv(filen)
# ---
def list_workers(self, request: cluster__pb2.Controller.ListWorkersRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListWorkersResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def the_object_name_does_not_exist(name):
    assert bpy.data.objects.get(name) is None, "Object exists"
# ---
def reraise(self):
        if self.ex is not None:
            raise self.ex.with_traceback(self.tb.as_traceback())
        else:
            raise Exception("Process failed with no exception").with_traceback(self.tb.as_traceback())
# ---
def Sort(sub_li): 
    sub_li.sort(key = lambda x: x[1]) 
    return sub_li
# ---
def uid(self, val):
        self._uid = _EpubMeta('dc:identifier', str(val), ('id', 'uid_id'))
# ---
def newman_prime(n): 
	if n == 0 or n == 1: 
		return 1
	return 2 * newman_prime(n - 1) + newman_prime(n - 2)
# ---
def main(cfg: DownloadConfig) -> None:
    """CLI entrypoint for downloading and processing Ar5iv dataset."""
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
    download(cfg)
# ---
def same_Length(A,B): 
    while (A > 0 and B > 0): 
        A = A / 10; 
        B = B / 10; 
    if (A == 0 and B == 0): 
        return True; 
    return False;
# ---
def submit(self, request: JobRequest) -> RayJobHandle:
        """Submit a job, routing to TPU/binary/callable based on device type."""
        logger.info("Submitting job: %s", request.name)

        if isinstance(request.resources.device, TpuConfig):
            return self._launch_tpu_job(request)

        if request.entrypoint.binary_entrypoint is not None:
            return self._launch_binary_job(request)

        return self._launch_callable_job(request)
# ---
def on_task_transition(self, old_state: int | None, new_state: int) -> int | None:
        """Update counts for a single task transition.

        Args:
            old_state: Previous task state, or None if new task
            new_state: New task state

        Returns:
            New job state if changed, None otherwise
        """
        if old_state is not None:
            self.task_state_counts[old_state] -= 1
        self.task_state_counts[new_state] += 1
        return self._compute_job_state()
# ---
def is_local_branch(git_path, module, dest, branch):
    branches = get_branches(git_path, module, dest)
    lbranch = '%s' % branch
    if lbranch in branches:
        return True
    elif '* %s' % branch in branches:
        return True
    else:
        return False
# ---
def _randMatrix(self, rows, cols, dtype):
    if dtype is np.complex64:
      real = self._randMatrix(rows, cols, np.float32)
      imag = self._randMatrix(rows, cols, np.float32)
      return real + np.complex(0, 1) * imag
    else:
      return np.random.uniform(low=1.0, high=100.0, size=rows * cols).reshape(
          [rows, cols]).astype(dtype)
# ---
def bmarks():
    return_data = get_bmarks()
    return return_data
# ---
def loss_fn(x_arr):
        x = hax.named(x_arr, x0.axes)
        y, _ = layer(x, inference=False, chunk_size=8)
        return jnp.sum(y.array)
# ---
def _update_chainstate(self, block: Block, batch):
        self._last_block = block
        self._update_block_number_mapping(block, batch)
        self.tx_pool.remove_tx_in_block_from_pool(block)
        self._state.update_mainchain_height(block.block_number, batch)
        self._state.update_tx_metadata(block, batch)
# ---
def supports_fork(self) -> bool:
        pass
# ---
def test_just_out_capture(self):
        with self.getcapture(out=True, err=False) as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            out, err = cap.readouterr()
        assert out == "hello"
        assert not err
# ---
def mock_tensor_map_full():
    """Create a TensorMap with multiple variables for testing loss breakdown."""
    from ocean_emulators.constants import TensorMap
    from ocean_emulators.utils.multiton import MultitonScope

    with MultitonScope():
        TensorMap.init_instance("thermo_dynamic_5", "tau_hfds_hfds_anom")
        yield
# ---
def compute_and_viz_log_probs(step: StepInfo):
        model = step.eval_model
        os.makedirs(html_dir, exist_ok=True)
        path = os.path.join(html_dir, f"step_{step.step}.html")

        compute_and_visualize_log_probs(path, model, tokenizer, log_prob_fn, test_data, max_docs=max_docs)
        # TODO: convert to generic logging
        import wandb

        wandb.log({"log_probs": wandb.Html(path)}, step=step.step)
# ---
def test_duplicate_initialization():
    with MultitonScope():
        DummyMultiton.init_instance(10)
        with pytest.raises(ValueError):
            DummyMultiton.init_instance(20)
# ---
def read_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()
# ---
def read(*paths):
    """Build a file path from *paths* and return the contents."""
    with open(os.path.join(*paths), 'r') as f:
        return f.read()
# ---
def test_pd_DataFrame_from_series_par(self):
        def test_impl(n):
            S1 = pd.Series(np.ones(n))
            S2 = pd.Series(np.random.ranf(n))
            df = pd.DataFrame({'A': S1, 'B': S2})
            return df.A.sum()

        hpat_func = self.jit(test_impl)
        n = 11
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
        self.assertEqual(count_parfor_OneDs(), 1)
# ---
def test_fray_job_ctx_invalid():
    with pytest.raises(ValueError, match="Unknown context type"):
        create_job_ctx("invalid")
# ---
def make_axes(**kwargs: int) -> tuple[Axis, ...]:
    """
    Convenience function for creating a tuple of Axis objects.

    Example:
    ```
    X, Y = axes(X=10, Y=20)
    ```

    """
    return tuple(Axis(name, size) for name, size in kwargs.items())
# ---
def _path_to_step_name(path: str) -> str:
    """
    Converts a path pointing to a levanter checkpoint into a name we can use as an id for the viz step
    """
    # we want llama-8b-tootsie-phase2-730000
    components = path.split("/")
    step = components[-2].split("-")[-1]
    name = components[-4].split("/")[-1]
    return f"analysis/viz-compare/{name}-{step}"
# ---
def transition_to(
        self,
        state: TaskState,
        *,
        message: str = "",
        error: str | None = None,
        exit_code: int | None = None,
    ) -> None:
        self.status = state
        self.status_message = message
        if is_task_finished(state):
            self.finished_at = Timestamp.now()
            if error:
                self.error = error
            if exit_code is not None:
                self.exit_code = exit_code
# ---
def reset(self) -> None:
        """Reset rate limiter to allow immediate run."""
        self._last_run = None
# ---
def test_capsyscapfd(self, testdir):
        p = testdir.makepyfile("""
            def test_one(capsys, capfd):
                pass
            def test_two(capfd, capsys):
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*ERROR*setup*test_one*",
            "E*capfd*capsys*same*time*",
            "*ERROR*setup*test_two*",
            "E*capsys*capfd*same*time*",
            "*2 error*"])
# ---
def render(self, name, value, attrs=None):
        context = self.get_context(name, value, attrs or {})
        return render_to_string(self.template, context)
# ---
def main():
    for name, (input_ext, transform_fn) in DATASETS.items():
        regenerate_dataset(name, input_ext, transform_fn)
# ---
def submit(self, request: JobRequest) -> JobHandle:
        """Submit a job for execution. Returns immediately."""
        ...
# ---
def _stop_jupyter(self):
        if self._notebook_proc:
            self._notebook_proc.terminate()
            try:
                self._notebook_proc.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self._notebook_proc.kill()
            self._notebook_proc = None
# ---
def check_access_rule(self, cr, uid, ids, operation, context=None):
        #override in order to redirect the check of acces rules on the stock.picking object
        return self.pool.get('stock.picking').check_access_rule(cr, uid, ids, operation, context=context)
# ---
def remove_figure_captions(html: BeautifulSoup):
    # Remove the figure captions since they are not needed
    captions = html.findAll("figcaption", {"class": "ltx_caption"})
    for caption in captions:
        caption.decompose()
# ---
def pytest_addoption(parser: Any) -> None:
    parser.addoption("--run-benchmark", action="store_true", default=False, help="run benchmark tests")
# ---
def test_brackets_balanced_simple():
    assert brackets_balanced("(a + b)")
    assert brackets_balanced("[1, 2, 3]")
    assert brackets_balanced("{x: y}")
# ---
def simple_jax_workload():
    return simple_jax_fn()
# ---
def multiply(self,accum,item):
        return accum * item
# ---
def tobytes(obj):
        if isinstance(obj, unicode):
            obj = obj.encode('UTF-8')
        assert isinstance(obj, str)
        return obj
# ---
def is_on_python_lib_path(self):
        """Return True if this stack frame is from a library on Python's library path."""
        python_lib_path = sysconfig.get_path("purelib")

        return self.filename.startswith(python_lib_path)
# ---
def setThreadStopEvent(self, threadStopEvent):
        self.threadStopEvent = threadStopEvent
        self._program.setThreadStopEvent(threadStopEvent)
# ---
def patched__getitem__(proxy_self, keys):
            variable = None
            for var in (self.cf_coord_var, self.cf_bounds_var):
                if proxy_self.variable_name == var.cf_name:
                    return var[keys]
            raise RuntimeError()
# ---
def _GrumpRun(cmd):
  p = subprocess.Popen(['grumpy', 'run'], stdin=subprocess.PIPE,
                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
  out, _ = p.communicate(cmd)
  return p.returncode, out
# ---
def test_default_device():
    assert (
        info.default_device() is None or info.default_device() == xp.asarray(0).device
    )
# ---
def isnan(a: A) -> A:
    return wrap_elemwise_unary(jnp.isnan, a)
# ---
def _set_changed_options(self):
        changed = {}
        for key in Parameters.returnables:
            if getattr(self.want, key) is not None:
                changed[key] = getattr(self.want, key)
        if changed:
            self.changes = UsableChanges(params=changed)
# ---
def __init__(self,occurrence):
        Occurrence.__init__(self,occurrence.name,occurrence.arguments,as_written=occurrence.as_written,position=occurrence.position)
        self.occurrence = occurrence
        self.filename = self.occurrence[0]
        self.input_paths=InputPaths()
        self._file_content=None
# ---
def fsspec_expand_glob(url):
            if "*" in url:
                fs = fsspec.core.url_to_fs(url)[0]
                return fs.glob(url)
            else:
                return [url]
# ---
def count_no_of_ways(n, k): 
	dp = [0] * (n + 1) 
	total = k 
	mod = 1000000007
	dp[1] = k 
	dp[2] = k * k	 
	for i in range(3,n+1): 
		dp[i] = ((k - 1) * (dp[i - 1] + dp[i - 2])) % mod 
	return dp[n]
# ---
def test_all_network_labels(self):
        CONF.network_label_regex = '.*'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(3, len(ip))
        self.assertTrue('10.123.123.123' in ip)
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)
# ---
def pin_memory(self):
        for step in self:
            self.example_by_step[step] = (
                self[step][0].pin_memory(),
                self[step][1].pin_memory(),
            )
        return self
# ---
def test_simple_many(self, tmpfile):
        for i in range(10):
            self.test_simple(tmpfile)
# ---
def do_init(self, e):
        self.remote.do_init(self.get_match())
# ---
def real_service(real_worker):
    """Create WorkerServiceImpl with real worker."""
    return WorkerServiceImpl(real_worker)
# ---
def list_iris_containers(self, all_states: bool = True) -> list[str]:
        del all_states
        return list(self._containers.keys())
# ---
def map_png():
    return make_map(request, format='png')
# ---
def _initialize(self, *args, **kwargs):
        """
        Override this method to initialize the instance.
        """
        raise NotImplementedError(
            "Subclasses must override _initialize() to set up the instance."
        )
# ---
def select_motif_binder(
        self,
        tokens: np.ndarray,
        random: np.random.Generator,
        fixed_crop: bool = False,
    ):
        tokens = self.select_motif(tokens, random, fixed_crop)
        return self.resect_and_reindex(tokens, random)
# ---
def __init__(self, urls, text_key="text"):
        self.text_key = text_key
        self.base_ds = UrlDataSource(urls, columns=[text_key])
# ---
def actual_head_size(self) -> int:
        if self.head_dim is not None:
            return self.head_dim
        return self.hidden_dim // self.num_heads
# ---
def using_gpu() -> bool:
    return get_device().type == "cuda"
# ---
def get_output_shape_for(self, input_shape):
        if self.dim_ordering == 'channels_last':
            return (input_shape[0], input_shape[3])
        else:
            return (input_shape[0], input_shape[1])
# ---
def test_score_candidate_partial_pass():
    candidate = _make_candidate("def f(x):\n    return x\n")
    tests = ["assert f(1) == 1", "assert f(2) == 3"]
    result = score_candidate(candidate, tests)
    assert result.tests_passed == 1
    assert result.tests_total == 2
    assert 0.0 < result.test_pass_rate < 1.0
# ---
def stop(self,solverId):
    path = "{base}/{solverId}/stop".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.put(path,
                       params=self.params,
                       headers=self.headers)
    self.validateReply(req)
    return True
# ---
def is_undulating(n): 
	if (len(n) <= 2): 
		return False
	for i in range(2, len(n)): 
		if (n[i - 2] != n[i]): 
			return False
	return True
# ---
def max_product(arr): 
    arr_len = len(arr) 
    if (arr_len < 2): 
        return None     
    x = arr[0]; y = arr[1]    
    for i in range(0, arr_len): 
        for j in range(i + 1, arr_len): 
            if (arr[i] * arr[j] > x * y): 
                x = arr[i]; y = arr[j] 
    return x,y
# ---
def mask(self) -> np.uint64:
        """Returns an all-1s bitmask for the field."""
        return np.uint64((1 << (self.size_in_bytes() * 8)) - 1)
# ---
def add_children(graph, parent_id, d, level=0):

    blue = "#6b6bd1"
    white = "#fdfefd"
    green = "#33a333"
    colours = [blue, white, green] * 3

    for class_, children in d.items():
        colour = colours[level]
        child_label = class_
        child_id = parent_id + "_" + class_
        add_child(graph, child_id, child_label, parent_id, colour)
        add_children(graph, child_id, children, level+1)
# ---
def teardown(self, trainer, pl_module, stage):
        """Called when the callback is being torn down."""
        super().teardown(trainer, pl_module, stage)
        self._unpatch_tqdm()
# ---
def encode_source(self, source: str) -> list[int]:
        """Encode a Python source string to token IDs.

        Returns a list of base vocabulary token IDs (no special tokens).
        """
        return [self.encode_char(c) for c in source]
# ---
def _get_first_model_id(server_url: str) -> str:
    response = requests.get(f"{server_url}/models", timeout=30)
    response.raise_for_status()
    payload = response.json()
    data = payload.get("data", [])
    if not data:
        raise RuntimeError(f"No models returned from {server_url}/models: {str(payload)[:2000]}")
    model_id = data[0].get("id")
    if not model_id:
        raise RuntimeError(f"Missing model id in {server_url}/models response: {str(payload)[:2000]}")
    return str(model_id)
# ---
def _slow():
    import time

    time.sleep(120)
# ---
def delete_object(obj):  # Release
    compss_delete_object(obj)
# ---
def test_vhd(self):
        image_meta = {'id': 'a', 'disk_format': 'vhd'}
        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_VHD)
# ---
def is_num_keith(x): 
	terms = [] 
	temp = x 
	n = 0 
	while (temp > 0): 
		terms.append(temp % 10) 
		temp = int(temp / 10) 
		n+=1 
	terms.reverse() 
	next_term = 0 
	i = n 
	while (next_term < x): 
		next_term = 0 
		for j in range(1,n+1): 
			next_term += terms[i - j] 
		terms.append(next_term) 
		i+=1 
	return (next_term == x)
# ---
def setUp(self):
        self.xe = IOSXE(node=node, username=username, password=password, disable_warnings=True)
# ---
def update_model(old_model, new_state_dict):
    return hsd.from_state_dict(old_model, new_state_dict)
# ---
def model_type(cls) -> Type["MistralLMHeadModel"]:
        return MistralLMHeadModel
# ---
def generate_numbers(number_string):
        if "-" in number_string:  # it's a range
            start, end = map(int, number_string.split("-"))
            return [str(i).zfill(len(number_string.split("-")[0])) for i in range(start, end + 1)]
        else:  # it's a single number
            return [number_string]
# ---
def check_distinct(test_tup):
  res = True
  temp = set()
  for ele in test_tup:
    if ele in temp:
      res = False
      break
    temp.add(ele)
  return (res)
# ---
def _transfer_from_cpu(self, model) -> PyTree:
        """Transfer params from CPU back to target devices."""
        if self.params_sharding_rules is not None:
            return jax.device_put(model, self.params_sharding_rules)
        else:
            # Use default device placement
            return jax.device_put(model, jax.devices()[0])
# ---
def ClosePreviewWindow():
  """ Close the preview window if it is present, otherwise do nothing """
  vim.command( 'silent! pclose!' )
# ---
def test_scheme_not_supported():

    with pytest.raises(NotImplementedError):
        UrlPath('http:///tmp/test').touch()
# ---
def list_methods(self, request: actor__pb2.ListMethodsRequest, ctx: RequestContext) -> actor__pb2.ListMethodsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def _rm(path):
    path = Path(path)
    if path.is_dir():
        shutil.rmtree(path, ignore_errors=True)
    elif path.is_file():
        os.remove(path)
    elif path.exists():
        raise RuntimeError(f"Remove failed. Path ({path}) is neither a directory nor a file.")
# ---
def Total_Hamming_Distance(n):   
    i = 1
    sum = 0
    while (n // i > 0):  
        sum = sum + n // i  
        i = i * 2     
    return sum
# ---
def consecutive_failures(self) -> int:
        """Number of consecutive scale-up failures."""
        return self._consecutive_failures
# ---
def printStatusThread(q0, q1, q2, q3):
	strtime = time.time()
	while True:
		sys.stdout.write('\r\x1b[K')
		sys.stdout.write("urls:" + str(q0.qsize()) + " | ")
		sys.stdout.write("userids:" + str(q1.qsize()) + " | ")
		sys.stdout.write("user infos:" + str(q2.qsize()) + " | ")
		sys.stdout.write("manager infos:" + str(q3.qsize()))
		sys.stdout.flush()
		time.sleep(1)
# ---
def all_Bits_Set_In_The_Given_Range(n,l,r):  
    num = (((1 << r) - 1) ^ ((1 << (l - 1)) - 1)) 
    new_num = n & num
    if (new_num == 0): 
        return True
    return False
# ---
def pytest_addoption(parser):
    parser.addoption(
        "--use-docker",
        action="store_true",
        default=False,
        help="Use real Docker containers instead of in-process mocks",
    )
    parser.addoption(
        "--update-snapshots",
        action="store_true",
        default=False,
        help="Update golden snapshot files instead of comparing",
    )
# ---
def test_store(tmp_path, spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)

    store = tmp_path / "source.zarr"
    target = zarr.empty(a.shape, chunks=a.chunksize, store=store)

    cubed.store(a, target, executor=executor)
    assert_array_equal(target[:], np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
# ---
def main(config: ConvertLingolyToDolmaConfig) -> None:
    """CLI entrypoint."""
    convert_lingoly_to_dolma(config)
# ---
def test_makes_patches__high_res():
    x = torch.randn(1, 10, 14, 21)

    patch_embed = PerceiverEncoder(
        in_channels=10,
        out_channels=4,
        patch_size=7,
        perceiver=make_perceiver(10, 4),
        lat=torch.linspace(start=-90, end=90, steps=x.shape[-2]),
        lon=torch.linspace(start=0, end=360, steps=x.shape[-1]),
    )

    patches = patch_embed(x)

    assert patches.shape == (1, 4, 2, 3)
# ---
def get_size_bytes(blob: str) -> int:
    return len(blob.encode("utf-8"))
# ---
def terminate(self) -> None:
        if self._ref is not None:
            ray.cancel(self._ref)
            return
        try:
            client = JobSubmissionClient(self._dashboard_address)
            client.stop_job(self._submission_id)
        except Exception as e:
            logger.warning("Failed to stop job %s: %s", self._job_id, e)
# ---
def test_str_contains_noregex(self):
        def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('BB', regex=False)
            return B.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), 1)
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "embed_tokens"}
# ---
def test_count_empty(backend):
    """Test count on empty dataset."""
    ds = Dataset.from_list([]).count()
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 0
# ---
def bitwise_left_shift(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "bitwise_left_shift")
    if x1.dtype not in _integer_dtypes or x2.dtype not in _integer_dtypes:
        raise TypeError("Only integer dtypes are allowed in bitwise_left_shift")
    return elemwise(nxp.bitwise_left_shift, x1, x2, dtype=result_type(x1, x2))
# ---
def _transfer_to_cpu(self, model) -> PyTree:
        """Transfer params to CPU devices."""
        try:
            with hax.set_mesh(self.cpu_mesh):
                cpu_devices = jax.devices("cpu")
                return jax.device_put(model, cpu_devices[0])
        except Exception as e:
            logger.warning(f"Failed to transfer to CPU: {e}, using original params")
            return model
# ---
def example_reading_spec(self):
    label_key = "image/class/label"
    data_fields, data_items_to_decoders = (
        super(Image2TextProblem, self).example_reading_spec())
    data_fields[label_key] = tf.VarLenFeature(tf.int64)
    data_items_to_decoders["targets"] = contrib.slim().tfexample_decoder.Tensor(
        label_key)
    return data_fields, data_items_to_decoders
# ---
def _run_test_in_subprocess(code: str, test: str, queue: multiprocessing.Queue) -> None:
    """Execute a single test in a subprocess. Must be module-level for pickling."""
    try:
        exec_globals: dict = {}
        exec(code, exec_globals)
        exec(test, exec_globals)
        queue.put(True)
    except Exception:
        queue.put(False)
# ---
def _product_get(self, cr, uid, id, product_ids=False, context=None, states=None):
        """
        @param product_ids:
        @param states:
        @return:
        """
        if states is None:
            states = ['done']
        ids = id and [id] or []
        return self._product_get_multi_location(cr, uid, ids, product_ids, context=context, states=states)
# ---
def draw_dag(dag, name="dag"):
    dag = dag.copy()
    for _, d in dag.nodes(data=True):
        # remove keys or values with possibly unescaped characters
        for k in ("name", "pipeline", "primitive_op", "stack_summaries"):
            if k in d:
                del d[k]
    gv = nx.drawing.nx_pydot.to_pydot(dag)
    format = "svg"
    full_filename = f"{name}.{format}"
    gv.write(full_filename, format=format)
# ---
def execute_dag(
        self,
        dag: MultiDiGraph,
        callbacks: Optional[Sequence[Callback]] = None,
        spec: Optional[Spec] = None,
        compute_id: Optional[str] = None,
        **kwargs,
    ) -> None:
        merged_kwargs = {**self.kwargs, **kwargs}
        execute_dag(
            dag,
            callbacks=callbacks,
            spec=spec,
            compute_id=compute_id,
            **merged_kwargs,
        )
# ---
def __repr__(self):
        return ("<{0.__class__.__name__}"
                " id={0.entity_id!r}"
                " type={0.entity_type!r}"
                " ident={0.ident!r}>").format(self)
# ---
def __getattr__(self, method_name: str) -> Callable[..., Any]:
        def call(*args, **kwargs):
            endpoint = self._pool._get_next_endpoint()
            return self._pool._call_endpoint(endpoint, method_name, args, kwargs)

        return call
# ---
def remove_module_prefix(state_dict, prefix="module."):
            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k.removeprefix(prefix)
                new_state_dict[name] = v
            return new_state_dict
# ---
def _default_sort(self, resource):
        datasource = self._datasource(resource)
        return datasource[3]
# ---
def test_connection_available_returns_false_on_os_error():
    """connection_available returns False on OSError."""
    conn = MagicMock()
    conn.run.side_effect = OSError("Connection refused")
    assert connection_available(conn) is False
# ---
def test_beam_search_includes_initial_program(params, model_cfg, tokenizer):
    """The initial program should be among the candidates (it starts with score 0)."""
    source = "x = 1\n"
    results = beam_search(
        params=params,
        initial_programs=[source],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(4),
        beam_size=8,
        expansions_per_beam=2,
        max_depth=1,
    )
    sources = {c.source for c in results}
    assert source in sources
# ---
def resizeable(self):
        """True if the window is resizable.  Read-only.

        :type: bool
        """
        return self._resizable
# ---
def create_colors_list():
    colors_list = []
    for color in plt.cm.tab10(np.linspace(0, 1, 10))[:-1]:
        colors_list.append(tuple(color))
    colors_list.append("black")
    for color in plt.cm.Set2(np.linspace(0, 1, 8)):
        colors_list.append(tuple(color))
    for color in plt.cm.Set3(np.linspace(0, 1, 12)):
        colors_list.append(tuple(color))
    return colors_list
# ---
def mk_LayerNorm(self, axis: Axis) -> hnn.RmsNorm:
        return hnn.RmsNorm.init(
            axis, eps=self.layer_norm_epsilon, use_weight=self.use_layer_norm_weight, use_bias=self.use_bias
        )
# ---
def test_format_shard_path_basic():
    """Test basic path formatting with placeholders."""
    pattern = "output/data-{shard:05d}-of-{total:05d}.jsonl"
    result = format_shard_path(pattern, 0, 10)
    assert result == "output/data-00000-of-00010.jsonl"
# ---
def is_leaf(x):
        return x is None or isinstance(x, OverwriteWithGradient)
# ---
def summary(self) -> str:
        """Return a human-readable summary of bank contents."""
        lines = [f"SubtreeBank: {self.total_entries} entries, {len(self.entries)} types"]
        for node_type in sorted(self.entries.keys()):
            entries = self.entries[node_type]
            lines.append(f"  {node_type}: {len(entries)} entries")
        return "\n".join(lines)
# ---
def __repr__(self):
        return formatting.dataset_repr(self)
# ---
def check_Concat(str1,str2):
    N = len(str1)
    M = len(str2)
    if (N % M != 0):
        return False
    for i in range(N):
        if (str1[i] != str2[i % M]):
            return False         
    return True
# ---
def get_default(cls) -> "BlockSizes":
        return cls()
# ---
def elapsed_ms(self) -> int:
        """Get elapsed time in milliseconds."""
        return int(self.elapsed_seconds() * 1000)
# ---
def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()
# ---
def nCr(n, r): 
	if (r > n / 2): 
		r = n - r 
	answer = 1 
	for i in range(1, r + 1): 
		answer *= (n - r + i) 
		answer /= i 
	return answer 
def binomial_probability(n, k, p): 
	return (nCr(n, k) * pow(p, k) *	pow(1 - p, n - k))
# ---
def find_ind(key, i, n, 
			k, arr):
	ind = -1
	start = i + 1
	end = n - 1;
	while (start < end):
		mid = int(start +
				(end - start) / 2)
		if (arr[mid] - key <= k):
			ind = mid
			start = mid + 1
		else:
			end = mid
	return ind
def removals(arr, n, k):
	ans = n - 1
	arr.sort()
	for i in range(0, n):
		j = find_ind(arr[i], i, 
					n, k, arr)
		if (j != -1):
			ans = min(ans, n -
						(j - i + 1))
	return ans
# ---
def test_launch_job_rejects_duplicate_name(service, job_request):
    """Verify launch_job rejects duplicate job names."""
    request = job_request("duplicate-job")

    response = service.launch_job(request, None)
    assert response.job_id == JobName.root("duplicate-job").to_wire()

    with pytest.raises(ConnectError) as exc_info:
        service.launch_job(request, None)

    assert exc_info.value.code == Code.ALREADY_EXISTS
    assert JobName.root("duplicate-job").to_wire() in exc_info.value.message
# ---
def crispr_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'method': 'CRISPR'
    }
# ---
def assign_rest_molecule(rest_molecule, output_atom_group,
                         model_id="model_1", chain_id="Z", res_name="UNK"):
    chain = bridge.AtomGroup()
    res = bridge.AtomGroup()
    res.name = res_name
    atom_id = 1
    for atom in rest_molecule.get_atom_list():
        res.set_atom(atom_id, atom)
        atom_id += 1
    chain.set_group(1, res)

    output_atom_group[model_id].set_group(chain_id, chain)
# ---
def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            self._stop_tokens = None
            self.max_tokens = 1024
# ---
def config(self) -> GrugConfigLike:
        return self.grug_config
# ---
def add_worker(self, worker: ControllerWorker) -> None:
        """Add worker directly to state (test utility).

        For production use, create Event(WORKER_REGISTERED) instead.
        This method bypasses event logging and is intended for test setup only.
        """
        with self._lock:
            self._workers[worker.worker_id] = worker
# ---
def test_get_with_invalid_election_id_non_integer_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': 'hey' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified a non-integer election ID.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def parse_block_code(self, m):
        # clean leading whitespace
        code = _block_code_leading_pattern.sub('', m.group(0))
        self.tokens.append({
            'type': 'code',
            'lang': None,
            'text': code,
        })
# ---
def test_stack_state_dict(input_dict, prefix, expected_output):
    result = _stack_state_dict(input_dict, prefix)
    for key in expected_output:
        assert jnp.all(jnp.array_equal(result[key], expected_output[key])), f"Failed on key: {key}"

    # now unstack it
    unstacked = _unstack_state_dict(result, prefix)
    for key in input_dict:
        assert jnp.all(jnp.array_equal(unstacked[key], input_dict[key])), f"Failed on key: {key}"
# ---
def format_path( str ):
	while( str.find( '//' ) != -1 ):
		str = str.replace( '//', '/' )
	return str
# ---
def get_external_ips(self):
        ''' get a list of external_ips '''
        return self.get(Service.external_ips) or []
# ---
def _match_less_than(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) < str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def partitionLabels(self, S: str) -> List[int]:
        lastPos, seen, currMax = {}, set(), -1
        res = []
        for i in range(0, 26):
            c = chr(97+i)
            lastPos[c] = S.rfind(c)
        for i, c in enumerate(S):
            # Encounter new index higher than currMax
            if i > currMax:
                res.append(currMax+1)
            currMax = max(currMax, lastPos[c])
        res.append(len(S))
        ans = [res[i]-res[i-1] for i in range(1, len(res))]
        return ans
# ---
def config(self):
        return self.encoder.config
# ---
def list_tasks(self, job_id: JobName) -> list[cluster_pb2.TaskStatus]:
        """List all tasks for a job.

        Args:
            job_id: Job identifier

        Returns:
            List of TaskStatus protos, one per task
        """
        return self._cluster_client.list_tasks(job_id)
# ---
def _get_next_endpoint(self) -> ResolvedEndpoint:
        """Get the next endpoint in round-robin order.

        Thread-safe: uses a lock to protect the endpoint index.
        """
        endpoints = self._resolve().endpoints
        with self._lock:
            if not endpoints:
                raise RuntimeError(f"No endpoints for '{self._name}'")
            endpoint = endpoints[self._endpoint_index % len(endpoints)]
            self._endpoint_index += 1
            return endpoint
# ---
def get_extra_args():
  """Returns the corresponding function arguments for the captured inputs.

  Returns:
    If the default graph is being used to define a function, the
    returned list of place holders are those used inside the function
    body corresponding those returned by get_extra_inputs(). Otherwise,
    returns an empty list.
  """
  g = ops.get_default_graph()
  if isinstance(g, _FuncGraph):
    return g.extra_args
  else:
    return []
# ---
def setUp(self):
        super().setUp()
        self.client.force_authenticate(user=self.user)
# ---
def parse_hrule(self, m):
        self.tokens.append({'type': 'hrule'})
# ---
def _get_comment_invoice(self, cr, uid, picking):
        """
        @return: comment string for invoice
        """
        return picking.note or ''
# ---
def SetFittingHeightForCurrentWindow():
  window_width = GetIntValue( 'winwidth( 0 )' )
  fitting_height = 0
  for line in vim.current.buffer:
    fitting_height += len( line ) // window_width + 1
  vim.command( '{0}wincmd _'.format( fitting_height ) )
# ---
import re 
def match(text): 
		pattern = '[A-Z]+[a-z]+$'
		if re.search(pattern, text): 
				return('Yes') 
		else: 
				return('No')
# ---
def get_autoscaler_status(
        self,
        request: cluster_pb2.Controller.GetAutoscalerStatusRequest,
        ctx: Any,
    ) -> cluster_pb2.Controller.GetAutoscalerStatusResponse:
        """Get current autoscaler status."""
        autoscaler = self._scheduler.autoscaler
        if not autoscaler:
            return cluster_pb2.Controller.GetAutoscalerStatusResponse(status=vm_pb2.AutoscalerStatus())

        return cluster_pb2.Controller.GetAutoscalerStatusResponse(status=autoscaler.get_status())
# ---
def _get_client(self) -> IrisClientLib:
        """Get IrisClient from context."""
        from iris.client.client import get_iris_ctx

        ctx = get_iris_ctx()
        if ctx is None or ctx.client is None:
            raise RuntimeError("IrisActorGroup requires IrisContext with client. " "Set context via iris_ctx_scope().")
        return ctx.client
# ---
def __init__(self, base_controller: Controller, sim: LungEnv, pid_K=[0.0, 0.0], decay=0.1, **kwargs):
        self.base_controller = base_controller
        self.sim = sim
        self.I = 0.0
        self.K = pid_K
        self.decay = decay

        self.reset()
# ---
def test_normalize_shape():
    assert normalize_shape(2) == (2,)
    assert normalize_shape((2,)) == (2,)
    assert normalize_shape((2, 0)) == (2, 0)
    assert normalize_shape((2, 3)) == (2, 3)

    with pytest.raises(TypeError):
        normalize_shape(None)
# ---
import heapq
def k_smallest_pairs(nums1, nums2, k):
   queue = []
   def push(i, j):
       if i < len(nums1) and j < len(nums2):
           heapq.heappush(queue, [nums1[i] + nums2[j], i, j])
   push(0, 0)
   pairs = []
   while queue and len(pairs) < k:
       _, i, j = heapq.heappop(queue)
       pairs.append([nums1[i], nums2[j]])
       push(i, j + 1)
       if j == 0:
           push(i + 1, 0)
   return pairs
# ---
def testForElseContinueNotNested(self):
    self.assertRaisesRegexp(
        util.ParseError, "'continue' not in loop",
        _ParseAndVisit, 'for i in (1,):\n  pass\nelse:\n  continue')
# ---
def __call__(self, x):
            return x.sum(self.field)
# ---
def call(*args, **kwargs):
            endpoint = self._pool._get_next_endpoint()
            return self._pool._call_endpoint(endpoint, method_name, args, kwargs)
# ---
def conjugate(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.conjugate(), self.axes)
# ---
def obj_fun(trainable_model):
            model = eqx.combine(trainable_model, state.model)
            with hax.axis_mapping(self.compute_axis_mapping):
                model = self.mp.cast_to_compute(model)
                result = self._raw_loss_function(model, *batch, **batch_kwargs, key=key)
                # result is (loss, metrics) tuple
                loss_for_opt, _metrics = result
                return loss_for_opt.scalar()
# ---
def device_port(self):
        if self._values['device_port'] is None:
            return None
        return int(self._values['device_port'])
# ---
def use_device_mesh(self) -> ContextManager[None]:
        """
        Context manager that sets the device mesh for jax, using Haliax's wrapper.

        In recent jax, this is the same as `jax.set_mesh(self.device_mesh)`, but we use Haliax's wrapper for
        compatibility with older jax versions.
        """
        return haliax.partitioning.set_mesh(self.device_mesh)
# ---
def finalize(self):
        common.replace_in_file(self.lua_file, 'local out_file = "' +
                               self.results_file + '"',
                               'local out_file = ""')
        # destroy neighbor stacks
        for stack_name in self.neighbor_stack_names:
            common.DEPLOYMENT_UNIT.destroy_heat_template(stack_name)
        self.neighbor_stack_names = list()
# ---
def total(self):
        return self.__total
# ---
def log_norm_passthrough(desc: str) -> GradientTransformation:
    """
    Creates a gradient transformation that logs the L2 norm of the updates
    and returns the updates unchanged.
    """

    def init_fn(params):
        return None

    def update_fn(updates, state, params, **extra_args):
        levanter.tracker.jit_log({desc: optax.tree_utils.tree_l2_norm(updates)})
        return updates, None

    return GradientTransformationExtraArgs(init_fn, update_fn)
# ---
def __init__(self, config: InferenceServerConfig, inference_context: InferenceContext, app: FastAPI):
        """Initialize the inference server with pre-built components.

        Use InferenceServer.create() to build a new server instance.
        """
        self.config = config
        self.inference_context = inference_context
        self.app = app
        self._server = None
# ---
def sum_iter(x):
    assert isinstance(x, Iterator)
    return sum(a for a in x)
# ---
def get(self) -> torch.Tensor:
        """Returns the mean metric across recorded batches."""
        if self._total is None:
            return torch.tensor(torch.nan)
        return self._total / self._n_batches
# ---
def llama_small_config() -> HFCompatConfig:
    hf_config = AutoConfig.from_pretrained(MODEL_NAME)
    hf_converter = HFCheckpointConverter.from_hf(MODEL_NAME)
    lev_config = hf_converter.config_from_hf_config(hf_config)
    return dataclasses.replace(lev_config, max_seq_len=MAX_INPUT_TOKENS + MAX_OUTPUT_TOKENS, tokenizer=MODEL_TOKENIZER)
# ---
def test_from_handler(self):
        @event.event
        def handler():
            pass

        h_inst = event.HandlerInstance.from_handler(handler)
        assert h_inst.info is handler._h_info
        assert h_inst.enabled
        assert h_inst.handler is handler._h_info.handler
# ---
def has_active_lock(self) -> bool:
        """Check if any worker has an active (non-stale) lock."""
        _, lock_data = self._read_lock_with_generation()
        return lock_data is not None and not lock_data.is_stale()
# ---
def binomial_coeffi(n, k): 
	if (k == 0 or k == n): 
		return 1
	return (binomial_coeffi(n - 1, k - 1) 
		+ binomial_coeffi(n - 1, k)) 
def rencontres_number(n, m): 
	if (n == 0 and m == 0): 
		return 1
	if (n == 1 and m == 0): 
		return 0
	if (m == 0): 
		return ((n - 1) * (rencontres_number(n - 1, 0)+ rencontres_number(n - 2, 0))) 
	return (binomial_coeffi(n, m) * rencontres_number(n - m, 0))
# ---
def check_gen_is_equal(
    jax_fn: Callable[[PRNGKeyArray, tuple], jnp.ndarray],
    hax_fn: Callable[[PRNGKeyArray, hax.AxisSpec], hax.NamedArray],
):
    key = jax.random.PRNGKey(0)

    hax_out = hax_fn(key, (Height, Width))
    jax_out = jax_fn(key, (Height.size, Width.size))

    assert hax_out.array.shape == jax_out.shape
    assert jnp.allclose(hax_out.array, jax_out)
# ---
def accelerator_descriptor(config: ResourceConfig) -> str | None:
    """Get accelerator type string (e.g., 'v4-8', 'H100') for logging/tracking."""
    if isinstance(config.device, TpuConfig):
        return config.device.variant
    elif isinstance(config.device, GpuConfig):
        return config.device.variant
    return None
# ---
def convert_to_cache(self, value, record, validate=True):
        if not value:
            return False
        if isinstance(value, basestring):
            if validate:
                # force parsing for validation
                self.from_string(value)
            return value[:DATE_LENGTH]
        return self.to_string(value)
# ---
def reload(self) -> "TreeStore":
        """
        Close the builder and return a TreeStore.
        """
        tree = jtu.tree_map(lambda builder: builder.reload(), self.tree, is_leaf=heuristic_is_leaf)
        return TreeStore(tree, self.path, self.mode)
# ---
def area(self):
        return self.side ** 2
# ---
import re
def remove_multiple_spaces(text1):
  return (re.sub(' +',' ',text1))
# ---
def sources_del(self):
        node = bottle.request.body.getvalue().decode('utf-8')
        self.config['sources'].remove(node)
        self.ndb.disconnect_source(node)
# ---
def __init__(self):
                self.scalar = jnp.zeros(
                    (),
                )
# ---
def _setRateForVoiceType(self, voiceType, value):
        """Sets the speaking rate value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM
        - value: the rate value to set.
        """

        voiceACSS = self._getACSSForVoiceType(voiceType)
        voiceACSS[acss.ACSS.RATE] = value
        voiceACSS['established'] = True
# ---
def d_attn(qkv, fn):
        q, k, v = qkv
        x_out = fn(QPos, KPos, Key, q, k, v, mask=mask)
        return (x_out * x_out).mean().scalar()
# ---
def asdict_without_nones(obj: DataclassInstance) -> dict:
    """Convert dataclass to dictionary, omitting None values."""
    if not dataclasses.is_dataclass(obj):
        raise ValueError(f"Expected dataclass, got '{obj}'")
    return dataclasses.asdict(obj, dict_factory=lambda x: {k: v for (k, v) in x if v is not None})
# ---
def datasource_from_json(urls_or_paths: Sequence[str]) -> ShardedDataSource[dict]:
    return JsonDataSource(urls_or_paths)
# ---
def question_suffix(cls) -> str:
        return " Write your answer in \\boxed{} format."
# ---
def test_byte_length_of_token_multi():
    tok = load_tokenizer("NousResearch/Llama-2-7b-hf")
    multi_checks = [
        "",
    ]

    for expr in multi_checks:
        # stupid llama adds a prefix space
        token_ids = tok.encode(expr, add_special_tokens=False)[1:]
        total_length = sum(byte_length_of_token(tok, token_id) for token_id in token_ids)
        assert total_length == len(expr.encode("utf-8"))
# ---
def on_compute_end(self, event):
        if sum(self.counter.values()) > 0:
            exceeded = [
                f"{k} ({v}/{self.ops[k].num_tasks})" for k, v in self.counter.items()
            ]
            warnings.warn(
                f"Peak memory usage exceeded allowed_mem when running tasks: {', '.join(exceeded)}",
                UserWarning,
                stacklevel=2,
            )
# ---
def protect(self, tag: str) -> None:
        """Mark an image as protected from eviction (used by a running job)."""
        ...
# ---
def sharding_for_axis(
    axis: AxisSelection, mapping: ResourceMapping | None = None, mesh: MeshLike | None = None
) -> NamedSharding:
    """Get the sharding for a single axis"""
    resolved_mesh = _resolve_mesh(mesh)
    if resolved_mesh is None:
        raise ValueError("No mesh found")

    return NamedSharding(resolved_mesh, pspec_for_axis(axis, mapping))
# ---
def GetBoolValue( variable ):
  return bool( int( vim.eval( variable ) ) )
# ---
def shard_names(self) -> Sequence[str]:
        return self.source.shard_names
# ---
def __init__(self, xPos, yPos):
        self.x = xPos
        self.y = yPos
        self.th = 32
        self.tw = 32
# ---
def fsspec_mkdirs(dir_path, exist_ok=True):
    """
    Create a directory in a fsspec filesystem.

    Args:
        dir_path (str): The path of the directory
    """

    # Use fsspec to create the directory
    fs = fsspec.core.url_to_fs(dir_path)[0]
    fs.makedirs(dir_path, exist_ok=exist_ok)
# ---
def _make_candidate(source: str, score: float = 0.0) -> BeamCandidate:
    return BeamCandidate(source=source, score=score, depth=0, edits=())
# ---
def __call__(self, model: M, *inputs: X, **input_kwargs) -> Tuple[Scalar, M]: ...
# ---
def output_nolink(self, m):
        key = _keyify(m.group(1))
        if key not in self.links:
            return None
        ret = self.links[key]
        return self._process_link(m, ret['link'], ret['title'])
# ---
def _process_sensor(sensor_data):
    sensor_data_fields = sensor_data.split('\n')
    sensor_data_dict = {}
    for field in sensor_data_fields:
        if not field:
            continue
        kv_value = field.split(':')
        if len(kv_value) != 2:
            continue
        sensor_data_dict[kv_value[0].strip()] = kv_value[1].strip()

    return sensor_data_dict
# ---
def __init__(self, remote, *args, **kwargs):
        super(MatchControl, self).__init__(*args, **kwargs) 
        self.remote = remote
        self.InitUI()
# ---
def i_add_a_cube():
    bpy.ops.mesh.primitive_cube_add()
# ---
def equal(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "equal")
    return elemwise(nxp.equal, x1, x2, dtype=nxp.bool)
# ---
def available_host_count(self) -> int:
        """Number of hosts available for new VM groups."""
        return len(self._available_hosts)
# ---
def on_compute_end(self, event):
        for pbar in self.pbars.values():
            pbar.close()
# ---
def wrap_key(key):
        if prefix:
            return f"{prefix}/{key}"
        return key
# ---
def find_dissimilar(test_tup1, test_tup2):
  res = tuple(set(test_tup1) ^ set(test_tup2))
  return (res)
# ---
def pop(self):
        """Get the element with the highest priority.

        Get the element with the highest priority (i.e., smallest value).

        Returns
        -------
        The element with the highest priority.

        """
        return heapq.heappop(self._queue)[1]
# ---
def screen(self):
        """The screen this window is fullscreen in.  Read-only.

        :type: `Screen`
        """
        return self._screen
# ---
def getStatus(self,solverId):
    path = "{base}/{solverId}/status".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.get(path,
                       params=self.params,
                       headers=self.headers)
    self.validateReply(req)
    return req.json()
# ---
def testAugAssignPow(self):
    self.assertEqual((0, '64\n'), _GrumpRun(textwrap.dedent("""\
        foo = 8
        foo **= 2
        print foo""")))
# ---
def pick_random_token(
    tokens: np.ndarray,
    random: np.random.Generator,
) -> np.ndarray:
    """Pick a random token from the data.

    Parameters
    ----------
    tokens : np.ndarray
        The token data.
    random : np.random.Generator
        The random state for reproducibility.

    Returns
    -------
    np.ndarray
        The selected token.

    """
    return tokens[random.integers(len(tokens))]
# ---
def nth_nums(nums,n):
 nth_nums = list(map(lambda x: x ** n, nums))
 return nth_nums
# ---
def selection_function(out_key):
        out_coords = out_key[1:]
        return get_item(target_chunks, out_coords)
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> LocalActorHandle:
        """Create an in-process actor, returning a handle immediately."""
        group = self.create_actor_group(actor_class, *args, name=name, count=1, resources=resources, **kwargs)
        return group.wait_ready()[0]
# ---
def test___doc__(self):
        self.assertEqual(
            ctds.Parameter.__doc__,
            '''\
Parameter(value, output=False)

Explicitly define a parameter for :py:meth:`.callproc`,
:py:meth:`.execute`, or :py:meth:`.executemany`. This is necessary
to indicate whether a parameter is *SQL* `OUTPUT` or `INPUT/OUTPUT`
parameter.

:param object value: The parameter's value.
:param bool output: Is the parameter an output parameter.
'''
        )
# ---
def slice_id(self) -> str:
        return self._group_id
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        self.run.config.update(_convert_value_to_loggable_rec(hparams))
# ---
def main():
    # Contingency Table from Wilks (2011) Table 8.3
    table = np.array([[50, 91, 71],
                      [47, 2364, 170],
                      [54, 205, 3288]])
    mct = MulticlassContingencyTable(table, n_classes=table.shape[0],
                                     class_names=np.arange(table.shape[0]).astype(str))
    print(mct.peirce_skill_score())
    print(mct.gerrity_score())
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "%s:%s" % (self.kind, self.match)
# ---
def check_design_specs(args: argparse.Namespace, moldir: Path, mols: Dict[str, Any]):
    last_banner = ""
    for design_spec in args.design_spec:
        banner = f"************** Checking design spec: {design_spec} **************"
        last_banner = banner
        print(banner)
        check_design_spec(args, moldir, design_spec, mols)
    if last_banner:
        print("*" * len(last_banner))
# ---
def get_puzzle_solution(self, alt_input=None):
        if alt_input is None:
            self._load_puzzle_file()
        else:
            self._puzzle_input = alt_input
        old_nice_count, new_nice_count = self._solve_puzzle_parts()
        return self._solved_output.format(old_nice_count, new_nice_count)
# ---
def _SortChunksByFile( chunks ):
  """Sort the members of the list |chunks| (which must be a list of dictionaries
  conforming to ycmd.responses.FixItChunk) by their filepath. Returns a new
  list in arbitrary order."""

  chunks_by_file = defaultdict( list )

  for chunk in chunks:
    filepath = chunk[ 'range' ][ 'start' ][ 'filepath' ]
    chunks_by_file[ filepath ].append( chunk )

  return chunks_by_file
# ---
def lecun_normal_init_(weights):
    trunc_normal_init_(weights, scale=1.0)
# ---
def body(i, state):
            ref_counts, sequences = state

            def do(state):
                ref_counts, sequences = state
                pages_row = sequences.page_indices["seq", i]
                ref_counts = dec_refcounts_for_seq(pages_row, ref_counts)
                sequences = sequences.release_slot(i)
                return ref_counts, sequences

            return jax.lax.cond(finished_mask[i], do, lambda s: s, (ref_counts, sequences))
# ---
def to(self, device: torch.device) -> "InferenceDataset":
        """Move the dataset's context tensors to the specified device.

        Call this before using the dataset for inference to ensure tensors
        are on the correct device (GPU).
        """
        self.ctx = self.ctx.to(device)
        self.wet_label = self.wet_label.to(device, non_blocking=True)
        return self
# ---
def unregister_endpoint(
        self,
        request: cluster_pb2.Controller.UnregisterEndpointRequest,
        ctx: Any,
    ) -> cluster_pb2.Empty:
        """Unregister a service endpoint. Idempotent."""
        self._state.remove_endpoint(request.endpoint_id)
        return cluster_pb2.Empty()
# ---
def qdq_and_return(x, q_dtype, scale, amax_history, compute_dtype):
    dtype_max = get_fp8_max(q_dtype, jnp.float32)
    amax_from_history = jnp.max(amax_history, axis=0)
    new_scale = compute_scale(amax_from_history, scale, dtype_max)

    qx = quantize_dequantize(x, q_dtype, new_scale, compute_dtype)

    new_history = compute_amax_history(x, amax_history)

    return qx, new_scale, new_history
# ---
def run_jit_hooks_outside_step(self, info: StepInfo, cb_infos: Sequence[PyTree], force: bool = False):
        for s_hook, cb_info in zip(self.jit_hooks, cb_infos):
            if force or (info.step % s_hook.every == 0):
                s_hook.fn.on_step(info, cb_info)
# ---
def test_is_success(self):
        self.assertFalse(status.is_success(199))
        self.assertFalse(status.is_success(300))

        for i in range(200, 299):
            self.assertTrue(status.is_success(i))
# ---
def test_run_streaming_with_retry_raises_after_max_retries(_mock_sleep):
    """run_streaming_with_retry raises RuntimeError after max retries."""
    conn = MagicMock()
    conn.run_streaming.side_effect = OSError("Connection refused")

    with pytest.raises(RuntimeError, match="Command failed after 3 attempts"):
        run_streaming_with_retry(conn, "bootstrap script", max_retries=3)
# ---
def longest_common_subsequence(X, Y, m, n): 
    if m == 0 or n == 0: 
       return 0 
    elif X[m-1] == Y[n-1]: 
       return 1 + longest_common_subsequence(X, Y, m-1, n-1) 
    else: 
       return max(longest_common_subsequence(X, Y, m, n-1), longest_common_subsequence(X, Y, m-1, n))
# ---
def usage(n):
    sys.stderr.write("Usage: " + n + " port...\n")
# ---
def nanstd(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanstd, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def loss_fn(x):
        y = model(x)
        return hax.sum(y.slice(Pos, start=loss_pos, length=1)).array
# ---
def decimal_to_binary(n): 
    return bin(n).replace("0b","")
# ---
def get_address_is_used(self, address: bytes) -> bool:
        with self.lock:
            return self._state.get_address_is_used(address)
# ---
def _read(path: Path) -> list[dict]:
        records = []
        with gzip.open(path, "rt", encoding="utf-8") as handle:
            for line in handle:
                if line.strip():
                    records.append(json.loads(line))
        return records
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)
        if isinstance(pattern, str):
            self.pattern = util.str2bool(pattern)
        self.pattern = int(self.pattern)
# ---
def test_compute_bracket_mask_no_open_brackets(tokenizer):
    mask = compute_bracket_mask("", tokenizer)
    assert mask.shape == (tokenizer.vocab_size,)
    # Close brackets should be blocked.
    for ch in ")]}":
        tid = tokenizer.encode_char(ch)
        assert float(mask[tid]) == 0.0
    # Open brackets and regular chars should be allowed.
    for ch in "([{abc":
        tid = tokenizer.encode_char(ch)
        assert float(mask[tid]) == 1.0
# ---
def test_wrong_nodes_prev_cc_raises(self):
        with pytest.raises(nx.NetworkXError):
            G = self.undirected_G.copy()
            edge = self.pick_add_edge(G)
            insert = True
            prev_cc = self.undirected_G_cc.copy()
            num_nodes = len(prev_cc)
            prev_cc.pop(0)
            prev_cc[num_nodes] = 0.5
            nx.incremental_closeness_centrality(G, edge, prev_cc, insert)
# ---
def _get_choices(self):
		if isinstance(self._choices, RegistryIterator):
			return self._choices.copy()
		elif hasattr(self._choices, 'next'):
			choices, self._choices = itertools.tee(self._choices)
			return choices
		else:
			return self._choices
# ---
def logical_and(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logical_and(x1, x2)
# ---
def valuePointer(self):
        return EnkfNode.cNamespace().value_ptr(self)
# ---
def spherical_area_weights(self) -> Grid:
        return spherical_area_weights(self.data)
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        return subprocess.run(self._build_cmd(command), capture_output=True, text=True, timeout=timeout.to_seconds())
# ---
def deleted_user(id):
    """
    Create a User object for a deleted user.
    """
    deleted_user = {
        "id": id,
        "name": "deleted-" + id,
        "deleted": True,
        "is_bot": False,
        "is_app_user": False,
    }
    return User(deleted_user)
# ---
def shape(self) -> Mapping[str, int]:
        """Mapping from axis name to size for the current view."""
        return {ax.name: ax.size for ax in self.axes}
# ---
def feature_encoders(self, data_dir):
    del data_dir
    return {
        "inputs": text_encoder.ImageEncoder(channels=self.num_channels),
        "targets": text_encoder.ClassLabelEncoder(self.class_labels)
    }
# ---
def total_noise(self, t):
    return self.sigma_min + t * (self.sigma_max - self.sigma_min)
# ---
def __init__(self, names, pronouns, mc):
        self.speakers = [{"name": n, "pronoun": p} for n, p in list(zip(names, pronouns))]
        self._transitions = self.make_transition_probs()
        self._speech_acts = ["said", "whispered", "shouted", "cried"]
        self._acts_transitions = [25, 2, 2, 2]
        self.mc = mc
        # self.seeds = seeds
        self.target_len = np.random.randint(5, 50, size=len(names))
# ---
def find_last_occurrence(A, x):
    (left, right) = (0, len(A) - 1)
    result = -1
    while left <= right:
        mid = (left + right) // 2
        if x == A[mid]:
            result = mid
            left = mid + 1
        elif x < A[mid]:
            right = mid - 1
        else:
            left = mid + 1
    return result
# ---
def build(
        self,
        in_channels: int,
        out_channels: int,
        hist: int,
        wet: Grid,
        area_weights: Grid,
        static_data: xr.Dataset | None,
        lat: Lat,
        lon: Lon,
    ) -> BaseModel:
        pass
# ---
def test_always_escaped():
    """Tests characters that are always escaped."""
    test_cases = [
        ("[brackets]", r"\[brackets\]"),
        (r"Backslash " "\\", r"Backslash " "\\"),
        ("`code block`", r"\`code block\`"),
    ]

    for text, expected in test_cases:
        assert minimal_markdown_escape(text) == expected
# ---
def _is_expert(self):
        profile = self._get_profile()
        return profile.user_type == EXPERT_USER_TYPE.lower()
# ---
def state_dict(self) -> dict[str, torch.Tensor]:
        """Return state dictionary for checkpointing."""
        return {"per_channel_scale": self._per_channel_scale.detach().cpu()}
# ---
def check_array_specs(arrays):
    specs = [a.spec for a in arrays if hasattr(a, "spec")]
    if not all(s == specs[0] for s in specs):
        raise ValueError(
            f"Arrays must have same spec in single computation. Specs: {specs}"
        )
    return arrays[0].spec
# ---
def read(self, count=None): pass
# ---
def __init__(self, area_weights: torch.Tensor, target_time: int):
        self._n_batches = 0
        self._variable_metrics: dict | None = None
        self._target_time = target_time
        self._area_weights = area_weights
# ---
def output_paragraph(self):
        return self.renderer.paragraph(self.inline(self.token['text']))
# ---
def __init__(self, min_value, max_value, step, instance=None,
        can_delete_vote=True, key='', read_only=False,
        template='ratings/star_widget.html', attrs=None):
        super(StarWidget, self).__init__(attrs)
        self.min_value = min_value
        self.max_value = max_value
        self.step = step
        self.instance = instance
        self.can_delete_vote = can_delete_vote
        self.read_only = read_only
        self.template = template
        self.key = key
# ---
def values(self):
        """ADC values presented as a list."""
        return self._values
# ---
def is_stale(self) -> bool:
        logger.debug(f"Is stale? {time.time()} {self.timestamp} {time.time() - self.timestamp}")
        return (time.time() - self.timestamp) > HEARTBEAT_TIMEOUT
# ---
def batch_size_at_step(self, step: int) -> int:
        """
        Return the batch size (number of samples) at the given training step.
        """
        for seg in self.segments:
            if seg.start <= step < seg.until:
                return seg.value
        warnings.warn(f"Step {step} is beyond the last defined segment. Using the last segment's batch size.")
        return self.segments[-1].value
# ---
def new_translator(domain, localedir, languages, fallback = None):
    new = gettext.translation(domain, localedir, fallback = True, languages = languages)
    if fallback is not None:
        new.add_fallback(fallback)
    return new
# ---
def __call__(self, x, key=None):
        if key is None and self.dropout.is_active:
            raise RuntimeError(
                "Cannot call LoraLinear with dropout and without a key if dropout is enabled."
                " The base model needs to be retrofitted to pass keys to the Linear layers."
            )
        x = self.dropout(x, key=key)
        z = self.lora_A(x)
        z = self.lora_B(z)
        return z * self.scale
# ---
def bmarks():
    return_data = do_register()
    return return_data
# ---
def conj(x, /):
    if x.dtype not in _complex_floating_dtypes:
        raise TypeError("Only complex floating-point dtypes are allowed in conj")
    return elemwise(nxp.conj, x, dtype=x.dtype)
# ---
def __mod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.mod(self, other)
# ---


def count_distinct_characters(string: str) -> int:
    """ Given a string, find out how many distinct characters (regardless of case) does it consist of
    >>> count_distinct_characters('xyzXYZ')
    3
    >>> count_distinct_characters('Jerry')
    4
    """
    return len(set(string.lower()))
# ---
def test_stderr(self):
        cap = capture.FDCapture(2)
        cap.start()
        print("hello", file=sys.stderr)
        s = cap.snap()
        cap.done()
        assert s == "hello\n"
# ---
def _init_core(core_cfg: GrugModelConfig, key: PRNGKeyArray):
    from levanter.grug.model import init_parameters as grug_init

    return grug_init(core_cfg, key=key)
# ---
def _router_name(self, router_id):
        return N_ROUTER_PREFIX + router_id
# ---
def test_less_equal(self):
        expr = col("score") <= 100
        assert expr.evaluate({"score": 50}) is True
        assert expr.evaluate({"score": 100}) is True
        assert expr.evaluate({"score": 150}) is False
# ---
def __deepcopy__(self, memo=None):
        # memo does nothing but is required for compatibility with
        # copy.deepcopy
        return self.copy(deep=True)
# ---
def test_environment_with_base():
    with TemporaryVenv() as venv:
        base_env = {"CUSTOM_VAR": "value", "PATH": "/custom/path"}
        env = venv.get_env(base_env=base_env)
        assert env["VIRTUAL_ENV"] == venv.venv_path
        assert env["CUSTOM_VAR"] == "value"
        assert venv.bin_path in env["PATH"]
        assert "/custom/path" in env["PATH"]
# ---
def stop(self) -> None: ...
# ---
def test_resolve_unresolved_location_with_endpoint(self):
        """Test resolving preserves endpoint_url."""
        base = S3Location(
            bucket="test-bucket",
            path="base/path",
            endpoint_url="https://s3.example.com",
        )
        unresolved = UnresolvedLocation(path="subdir/file.zarr")

        resolved = base.resolve(unresolved)
        assert isinstance(resolved, S3Location)

        assert resolved.endpoint_url == "https://s3.example.com"
# ---
def _get_disk_bytes() -> int:
    """Get available disk space in bytes."""
    try:
        stat = os.statvfs("/")
        return stat.f_bavail * stat.f_frsize
    except Exception:
        return 100 * 1024**3
# ---
def __init__(self, opts):
        opts['__multi_key'] = True
        super(MultiKeyCLI, self).__init__(opts)
        # Remove the key attribute set in KeyCLI.__init__
        delattr(self, 'key')
        zopts = copy.copy(opts)
        ropts = copy.copy(opts)
        self.keys = {}
        zopts['transport'] = 'zeromq'
        self.keys['ZMQ Keys'] = KeyCLI(zopts)
        ropts['transport'] = 'raet'
        self.keys['RAET Keys'] = KeyCLI(ropts)
# ---
def valida_igualdade(self, texto1, texto2, msg):
        if texto1 != texto2:
            raise ValidationError(msg)
        return True
# ---
def __rfloordiv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.floor_divide(other, self)
# ---
def __repr__(self):
        """ Helper method for debugging """

        _str = "<S3ImportItem %s {item_id=%s uid=%s id=%s error=%s data=%s}>" % \
               (self.table, self.item_id, self.uid, self.id, self.error, self.data)
        return _str
# ---
def exists(self, tag: str) -> bool:
        result = subprocess.run(
            ["docker", "image", "inspect", tag],
            capture_output=True,
            check=False,
        )
        return result.returncode == 0
# ---
def test_mem_write_word_at_top_left(self):
        self.mda.mem_write_word(0x0000, 0x0841) # 'A' with intensity.
        self.assertEqual(self.mda.video_ram[0x0000], 0x41)
        self.assertEqual(self.mda.video_ram[0x0001], 0x08)
        self.assertEqual(self.cg.last_blit, (None, (0, 0), 0x41, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def _np_rng_from_jax_key(prng_key: PRNGKeyArray) -> np.random.Generator:
    # Force CPU usage to avoid jit complaining. These are not critical path in Levanter.
    with local_cpu_mesh():
        key = jax.device_put(jax.device_get(prng_key))
        return np.random.Generator(np.random.PCG64(jrandom.randint(key, (), 0, 2**30).item()))
# ---
def find_rect_num(n):
  return n*(n + 1)
# ---
def __str__(self) -> str:
        return str(self.path)
# ---
def submit_task(self, request: cluster_pb2.Worker.RunTaskRequest) -> str: ...
# ---
def test_stmt_exception_pickleable_plus_dbapi(self):
        raw = testing.db.raw_connection()
        the_orig = None
        try:
            try:
                cursor = raw.cursor()
                cursor.execute("SELECTINCORRECT")
            except testing.db.dialect.dbapi.DatabaseError as orig:
                # py3k has "orig" in local scope...
                the_orig = orig
        finally:
            raw.close()
        self._test_stmt_exception_pickleable(the_orig)
# ---
def vms(self) -> list[ManagedVm]:
        return list(self._vms)
# ---
def _reconstruction_loss(self, x0):
    t0 = torch.zeros(x0.shape[0], dtype=self.dtype,
                     device=self.device)
    assert self.config.noise.type == 'loglinear'
    # The above assert is for d3pm parameterization
    unet_conditioning = self.noise(t0)[0][:, None]
    model_output_t0 = self.forward(x0, unet_conditioning)
    return - torch.gather(input=model_output_t0,
                          dim=-1,
                          index=x0[:, :, None]).squeeze(-1)
# ---
def the_object_name_is_not_voided_by_void(name, void):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    for rel in element.HasOpenings:
        if rel.RelatedOpeningElement.Name == void:
            assert False, "A void was found"
# ---
def is_cloud_storage_path(path: PathType):
    """Determine if a path string is for cloud storage."""
    return urlsplit(str(path)).scheme in ("gs", "s3")
# ---
def generate_random_id() -> str:
    """Generate a short random ID (4 alphanumeric characters)."""
    import random
    import string

    chars = string.ascii_lowercase + string.digits
    return "".join(random.choices(chars, k=4))
# ---
def __str__(self) -> str:
        return self.url()
# ---
def do_load(self, e):
        self.remote.do_load(self.clear_first.GetValue())
# ---
def controller(self) -> ControllerProtocol:
        """Access the underlying controller (must call start() first)."""
        if self._controller is None:
            raise RuntimeError("ClusterManager.start() not called")
        return self._controller
# ---
def set_fps(self, fps):
        """Set the label text for the given FPS estimation.

        Called by `update` every `update_period` seconds.

        :Parameters:
            `fps` : float
                Estimated framerate of the window.

        """
        self.label.text = '%.2f' % fps
# ---
def fake_vdi_attached_here(*args, **kwargs):
            fake_dev = 'fakedev'
            yield fake_dev
# ---
def _loadapp(self, proxy_config_path):
        """
        Load a proxy from an app.conf to get the memcache_ring

        :returns: the memcache_ring of the memcache middleware filter
        """
        with mock.patch('swift.proxy.server.Ring'):
            app = loadapp(proxy_config_path)
        memcache_ring = None
        while True:
            memcache_ring = getattr(app, 'memcache', None)
            if memcache_ring:
                break
            app = app.app
        return memcache_ring
# ---
def test_number_set(self):
        """Store and retrieve a number set"""
        self.make_table()
        item = {
            "id": "a",
            "datas": set([1, 2, 3]),
        }
        self.dynamo.put_item("foobar", item)
        ret = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(ret, item)
# ---
def shutdown(self):
        """Shutdown engine and event loop."""
        if self.engine:
            self.engine.shutdown()
        self.bridge.stop()
# ---
def compute_Last_Digit(A,B): 
    variable = 1
    if (A == B): 
        return 1
    elif ((B - A) >= 5):  
        return 0
    else:   
        for i in range(A + 1,B + 1): 
            variable = (variable * (i % 10)) % 10
        return variable % 10
# ---
def _format_reused_arrays(self):
        return [f"  Reused array {describe_array(arr)} at paths {paths}" for arr, paths in self.reused_arrays]
# ---
def __init__(self, string=None, digits=None, **kwargs):
        super(Float, self).__init__(string=string, _digits=digits, **kwargs)
# ---
def _rms_norm(x: NamedArray, bias: NamedArray | None = None) -> NamedArray:
    var = hax.mean(hax.square(x), axis=Embed)
    inv = hax.rsqrt(var + 1e-5)
    out = x * inv
    if bias is not None:
        with pytest.raises(ShardingTypeError):
            # bias needs to be resharded explicitly
            out = out + bias

        # capture the compute axis mapping for bias
        out = out + hax.auto_sharded(bias)

    return out
# ---
def get_user() -> str | None:
    return subprocess.check_output("whoami", shell=True).strip().decode("utf-8")
# ---
def test_compute_ray_retry_count(max_failure, max_preemption, expected):
    from fray.v2.ray_backend.backend import compute_ray_retry_count

    request = JobRequest(
        name="test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        max_retries_failure=max_failure,
        max_retries_preemption=max_preemption,
    )
    assert compute_ray_retry_count(request) == expected
# ---
def fn(*args, **kwargs):
            stacked = haliax.vmap(module.init, Block)(*args, **kwargs)
            stacked = haliax.auto_sharded(stacked)
            return Stacked(stacked, Block, gradient_checkpointing)
# ---
def setfunc(index):
            self.view.set_swap_button_enabled(self.can_perform_swap())
# ---
def _make_xml_elem(tag, text, attr = []):
    'Write a flat xml element.'
    out = '    <' + tag
    for (key, val) in attr:
        out += ' {}="{}"'.format(key, val)
    if text:
        out += '>{}</{}>\n'.format(text, tag)
    else:
        out += ' />\n'
    return out
# ---
def fix(a: A) -> A:
    return wrap_elemwise_unary(jnp.fix, a)
# ---
def log(t, eps=1e-20):
    return torch.log(t.clamp(min=eps))
# ---
def to_string(value):
        """ Convert a :class:`datetime` value into the format expected by the ORM. """
        return value.strftime(DATETIME_FORMAT) if value else False
# ---
def __call__(self, x):
            return self.blocks.fold(x)
# ---
def prefix(self):
        return True
# ---
def _method(self, i):
        return i
# ---
def __init__(self):
        self._auth = None
        self._agent_pid = None
        self._ssh_auth_re = re.compile(_SSH_AUTH_RE)
# ---
def resource_setup(cls):
        super(VolumesActionsTest, cls).resource_setup()

        # Create a test shared volume for attach/detach tests
        cls.volume = cls.create_volume()
# ---
def stop(self):
        """Stop streaming and wait for thread to exit."""
        self._stop_event.set()
        if self._thread:
            self._thread.join(timeout=5.0)
# ---
def count_samepair(list1,list2,list3):
    result = sum(m == n == o for m, n, o in zip(list1,list2,list3))
    return result
# ---
def modified(self, records):
        # Invalidate cache for self.inverse_fields, too. Note that recomputation
        # of fields that depend on self.inverse_fields is already covered by the
        # triggers (see above).
        spec = super(_Relational, self).modified(records)
        for invf in self.inverse_fields:
            spec.append((invf, None))
        return spec
# ---
def history(request: pytest.FixtureRequest) -> int:
    return request.param
# ---
def test_compile_1(self):
        compiler = PatternCompiler(pattern_set=dict(
            TEST=r'\w+'
        ))

        try:
            c1 = compiler.compile('$1{TEST}')
        except Exception as exc:
            self.assertTrue(1)

        c1 = compiler.compile('$1{TEST}', ['test'])
        self.assertEqual(c1, r'(?:(?P<test>(\w+)))')
# ---
def test_edit_to_mutation_roundtrip():
    edit = Edit(
        source_start=4,
        source_end=9,
        target_fragment="world",
        node_type="Name",
        stmt_count=0,
    )
    source = "say hello there"
    mutation = edit.to_mutation(source)
    assert mutation.apply(source) == "say world there"
    assert mutation.original == "hello"
# ---
def set_up_credentials(self):
        if os.getenv("MODAL_CLOUD_PROVIDER") == "CLOUD_PROVIDER_GCP":
            json = os.environ["SERVICE_ACCOUNT_JSON"]
            path = os.path.abspath("application_credentials.json")
            with open(path, "w") as f:
                f.write(json)
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = path
# ---
def clear(self):
        self.cookies = {}
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Estimated Finish Time", device_id)
# ---
def test_expires_on(self):
        eq_(self.record.expires_on, None)
# ---
def GetBufferFiletypes( bufnr ):
  command = 'getbufvar({0}, "&ft")'.format( bufnr )
  return VimExpressionToPythonType( command ).split( '.' )
# ---
def test_iterated_centroid():
    """ensure that the average across each dimension is returned"""
    new_centroid = kmeans.iterated_centroid([[1, 1, 1], [2, 2, 2]],\
            [[100, 200, 300]], [(0, 0), (1, 0)])
    np.testing.assert_allclose(new_centroid, np.array([[1.5, 1.5, 1.5]]),\
            rtol=1e-5)
# ---
def test_mup_coordinate_check_is_width_invariant():
    result = coord_check(widths=(32, 128, 512), steps=120, base_lr=3e-3)
    mup_span = result["mup_span"]
    ctrl_span = result["ctrl_span"]

    if ctrl_span < 1e-5:
        assert mup_span <= ctrl_span + 1e-6, f"P not at least as invariant: {result}"
    else:
        assert mup_span <= 0.6 * ctrl_span, f"P did not improve width invariance enough.\n{result}"
# ---
def extend(self, items):
        """Extend the queue by a number of elements.

        Parameters
        ----------
        items : list
            A list of items.

        """
        for item in items:
            self.push(item)
# ---
def _flatten_input(self, fts: Input) -> tuple[HistBatched, HistBatched]:
        fts_input = fts[:, : (self.hist + 1) * self.num_prognostic_channels]
        fts_input = self._flatten_hist(fts_input)

        fts_boundary = fts[:, (self.hist + 1) * self.num_prognostic_channels :]
        fts_boundary = self._flatten_hist(fts_boundary)
        return fts_input, fts_boundary
# ---
def broadcast_arrays_and_return_axes(
    *arrays: NamedOrNumeric, require_subset: bool = True, ensure_order: bool = True
) -> tuple[tuple[NamedOrNumeric, ...], tuple[Axis, ...]]: ...
# ---


def get_positive(l: list):
    """Return only positive numbers in the list.
    >>> get_positive([-1, 2, -4, 5, 6])
    [2, 5, 6]
    >>> get_positive([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])
    [5, 3, 2, 3, 9, 123, 1]
    """
    return [e for e in l if e > 0]
# ---
def _checked_request(url):
    try:
        response = requests.get(url, headers={"Metadata-Flavor": "Google"})
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException:
        logger.exception(f"Could not get {url} from metadata server. Is this a TPU VM?", exc_info=True)
        raise
# ---
def update_weights(self, new_state_dict: dict, model_name: str):
        """Synchronous weight update."""
        return self.bridge.run(self._update_weights_async(new_state_dict, model_name))
# ---
def get_worker(self, worker_id: WorkerId) -> ControllerWorker | None:
        with self._lock:
            return self._workers.get(worker_id)
# ---
def bump_seq_len_to_next_page(self, seq_id: int) -> "DecodeState":
        sequences = self.sequences.bump_seq_len_to_next_page(seq_id)
        return dataclasses.replace(self, sequences=sequences)
# ---
def load_pipeline(pipeline_file):
        """
        Loads scikit model/pipeline
        """
        print(colored('Loading pipeline: ' + pipeline_file, 'green'))
        return joblib.load(pipeline_file)
# ---
def extract_column(list1, n):
   result = [i.pop(n) for i in list1]
   return result
# ---
def _flatten_module(module):
        if isinstance(module, ModuleWithStateDictSerialization):
            module = module.flatten_for_export()
            module = scan_aware_tree_map(
                _flatten_module,
                module,
                is_leaf=lambda x: x is not module and isinstance(x, ModuleWithStateDictSerialization),
            )
        return module
# ---
def total_noise(self, t):
    cos = torch.cos(t * torch.pi / 2) ** 2
    return - torch.log(self.eps + (1 - self.eps) * cos)
# ---
def ray_cluster():
    """Start Ray cluster for tests."""
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
    yield
# ---
def getName( self, DN = '' ):
    """ Get the file catalog type name
    """
    return self.name
# ---
def finalist_user_roles(self):
        if not self.user_finalist_roles:
            finalist_roles = BaseUserRole.FINALIST_USER_ROLES
            self.user_finalist_roles = self.programrolegrant_set.filter(
                program_role__user_role__name__in=finalist_roles
            ).values_list('program_role__name', flat=True).distinct()
        return list(self.user_finalist_roles)
# ---
def glu(x: NamedArray, axis: Axis) -> NamedArray:
    axis_index = x.axes.index(axis)
    return NamedArray(jnn.glu(x.array, axis_index), x.axes)
# ---
def finfo(type, /):
    return nxp.finfo(type)
# ---
def testImport(self):
    self.assertEqual((0, "<type 'dict'>\n"), _GrumpRun(textwrap.dedent("""\
        import sys
        print type(sys.modules)""")))
# ---
def bitwise_left_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_left_shift](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_left_shift.html)
    """
    return jnp.bitwise_left_shift(x1, x2)
# ---
def qInitResources():
    QtCore.qRegisterResourceData(rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data)
# ---
def shard_names(self) -> Sequence[str]:
        return ["0"]
# ---
def _render_line(text, y, is_path=False):
                ax.text(
                    0.12,
                    y,
                    text,
                    fontsize=10,
                    ha="left",
                    va="top",
                    wrap=False,
                    fontfamily="monospace" if is_path else None,
                    color="blue" if is_path else "black",
                )
# ---
def flatten(array: NamedArray, new_axis_name: AxisSelector) -> NamedArray:
    """
    Returns a flattened view of the array, with all axes merged into one. Aliax for [haliax.ravel][]
    """
    return ravel(array, new_axis_name)
# ---
def __init__(self):
        self.parser = yacc.yacc(module=self, debug=0, write_tables=0)
# ---
def _root_key_m(self):
        return "LicenseInfo"
# ---
def _make_allocator(max_pages=8, max_seqs=2, page_size=4, pages_per_seq=3):
    pt = PageTable.init(max_pages, max_seqs, page_size, pages_per_seq)
    sequences = SequenceTable.init(max_seqs, pages_per_seq, page_size)
    return sequences, pt
# ---
def weight_transfer_config(transfer_mode):
    """Create weight transfer config for the specified mode."""
    with tempfile.TemporaryDirectory() as temp_dir:
        config = WeightTransferConfig(
            mode=transfer_mode,
            sync_interval_steps=1,
            checkpoint_dir=temp_dir,
        )
        yield config
# ---
def discover_new(self) -> list[ActorHandle]:
        """Return handles not yet yielded. After wait_ready, returns empty."""
        if self._yielded:
            return []
        self._yielded = True
        return self._handles
# ---
def __contains__(self, id):
		return id in self._addresses
# ---
def test_unicode_options(self):
        fmt = HtmlFormatter(title=u'F',
                            cssclass=u'br',
                            cssstyles=u'div:before { content: \'bz\' }',
                            encoding='utf-8')
        handle, pathname = tempfile.mkstemp('.html')
        tfile = os.fdopen(handle, 'w+b')
        fmt.format(tokensource, tfile)
        tfile.close()
# ---
def _call_splash_attention(q_, k_, v_, seg_ids, sinks_):
            return jax.vmap(
                lambda q_b, k_b, v_b, si: kernel(q_b, k_b, v_b, segment_ids=si, sinks=sinks_),
                in_axes=(0, 0, 0, 0),
            )(q_, k_, v_, seg_ids)
# ---
def _spec_shard_factor(entry, mesh) -> int:
    """Compute product of mesh axis sizes referenced by a PartitionSpec entry."""
    if mesh is None:
        return 1
    if entry is None or entry is PartitionSpec.UNCONSTRAINED:
        return 1
    if isinstance(entry, str):
        return int(mesh.shape.get(entry, 1))
    prod = 1
    for e in entry:
        if e is None or e is PartitionSpec.UNCONSTRAINED:
            continue
        prod *= int(mesh.shape.get(e, 1))
    return prod
# ---
def finish(self):
        logger.info("Finishing wandb run...")
        self._write_replicate_file()
        self.run.finish()
# ---
def run(self) -> None:
        uvicorn.run(self._app, host=self._host, port=self._port)
# ---

def minSubArraySum(nums):
    """
    Given an array of integers nums, find the minimum sum of any non-empty sub-array
    of nums.
    Example
    minSubArraySum([2, 3, 4, 1, 2, 4]) == 1
    minSubArraySum([-1, -2, -3]) == -6
    """
    max_sum = 0
    s = 0
    for num in nums:
        s += -num
        if (s < 0):
            s = 0
        max_sum = max(s, max_sum)
    if max_sum == 0:
        max_sum = max(-i for i in nums)
    min_sum = -max_sum
    return min_sum
# ---
def backend(request):
    """Parametrized fixture providing all backend types."""
    return request.param
# ---
def remove(self):
        self._set_changed_options()
        if self.module.check_mode:
            return True
        self.remove_from_device()
        if self.exists():
            raise F5ModuleError("Failed to delete the resource.")
        # Artificial sleeping to wait for remote licensing (on BIG-IP) to complete
        #
        # This should be something that BIG-IQ can do natively in 6.1-ish time.
        time.sleep(60)
        return True
# ---
def getlist(self, name, default=None):
        """Return the entire list"""
        return super().get(name, default)
# ---
def getradio(id) :
    db = cherrypy.session['database']
    if id.isdigit() :
        sql = "select radio, genre, url from Radio where id=%s" % id
    else:
        sql = "select radio, genre, url from Radio where url=%s" % id
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
    except:
        rows = [('Not Found', '', '')]

    rows = cur.fetchone()
    if rows == None:
        rows = ('Not Found', '', '')

    con.close()

    return rows
# ---
def fold(
    fn: Callable,
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable: ...
# ---
def to_state_dict(self, prefix: str | None = None) -> StateDict:
        # this method needs to "devectorize" the blocks, so that we have a list of blocks h.0.FOO, h.1.FOO, etc.
        # first just do the normal thing with our own dict, which we'll post-process
        state_dict: StateDict = super().to_state_dict(prefix)

        return _unstack_state_dict(state_dict, prefix)
# ---
def test_virtual_empty(shape, chunks, index):
    # array contents can be any uninitialized values, so
    # just check shapes not values
    v_empty = virtual_empty(shape, dtype=np.int32, chunks=chunks)
    empty = np.empty(shape, dtype=np.int32)
    assert v_empty[index].shape == empty[index].shape
    assert v_empty[...].shape == empty[...].shape
# ---
def requires(self):
        return DownloadRITA(year=self.year, month=self.month)
# ---
def __getitem__(self, item):
        return self.store[item]
# ---
def value(self):
        return self.deque[-1]
# ---
def check_monthnum(monthname1):
  if monthname1 == "February":
    return True
  else:
    return False
# ---
def __init__(self, name, fileas = None, role = 'aut'):
        '''Initialize the object. If the argument "fileas" is not given,
        "Last-name, First-name" is used for the file-as attribute. If
        the argument "role" is not given, "aut" is used for the role
        attribute.'''
        if not fileas:
            fileas = _normalize(name)
        self.tag = 'dc:creator'
        self.text = name
        self.attr = (('opf:file-as', fileas), ('opf:role', role))
# ---
def test_ckpt_path_with_hf_path():
    path = "meta-llama/Meta-Llama-3.1-8B"
    assert ckpt_path_to_step_name(path) == "Meta-Llama-3.1-8B"
# ---
def any(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """True if any elements along a given axis or axes are True. If axis is None, any elements are True."""
    return wrap_reduction_call(jnp.any, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def dispatch_events(self):
        """Poll the operating system event queue for new events and call
        attached event handlers.

        This method is provided for legacy applications targeting pyglet 1.0,
        and advanced applications that must integrate their event loop
        into another framework.

        Typical applications should use `pyglet.app.run`.
        """
        raise NotImplementedError('abstract')
# ---
def __str__(self) -> str:
        return str(self._job_id)
# ---
def _init(self, maxsize):
        Queue._init(self, maxsize) 
        self.all_items = set()
# ---
def __init__(self, difficulty: str = "medium"):
        if difficulty not in self.DIFFICULTY_RANGES:
            raise ValueError(f"Unknown difficulty: {difficulty}. Must be one of {list(self.DIFFICULTY_RANGES.keys())}")
        self.difficulty = difficulty
        self.min_val, self.max_val = self.DIFFICULTY_RANGES[difficulty]
# ---
def get_spectrum(self):

		spectrum = []
		now = self.time()

		for time in self.channels:
			if now - time < .5:
				p = random.randint(-40, -20)
			else:
				p = random.randint(-90, -80)

			spectrum.append(p)

		return tuple(spectrum)
# ---
def test_invalid_programs_are_skipped():
    """Programs with syntax errors should be skipped, not crash."""
    programs = [
        "def good():\n    return 1\n",
        "def bad(\n",  # syntax error
        "also bad{{{",  # syntax error
        "def also_good(x):\n    return x + 1\n",
    ]
    bank = SubtreeBank.from_corpus(programs)
    assert bank.total_entries > 0
# ---
def test_non_callable(self):
        with pytest.raises(TypeError) as excinfo:
            event.event(123)
        excinfo.match(r"Expected string, callable or None as first argument")

        with pytest.raises(TypeError) as excinfo:
            event.event("name")([])
        excinfo.match(r"Callable must be a function \(`def`\)"
                      r" or coroutine function \(`async def`\)")
# ---
def func_with_block_id(func):
            def wrap(*a, **kw):
                offset = int(a[-1])  # convert from 0-d array
                block_id = offset_to_block_id(offset, numblocks)
                return func(*a[:-1], block_id=block_id, **kw)

            return wrap
# ---
def gen_md5(content_str):
    m = md5()
    m.update(content_str)
    return m.hexdigest()
# ---
def test_filter_expression_repr():
    """Test FilterOp repr with expression."""
    from zephyr import col
    from zephyr.dataset import FilterOp

    expr = col("score") > 50
    op = FilterOp(predicate=expr.evaluate, expr=expr)
    assert "FilterOp(expr=" in repr(op)
    assert "col('score')" in repr(op)
# ---
def __init__(self, pages=None, *args, **kwargs):
        super(GrantsSpider, self).__init__(*args, **kwargs)

        if pages is not None:
            self.pages = pages
            self.start_urls = [ self.start_url_str % str(page) for page in xrange(1,int(self.pages)+1)]
# ---
def _sympy_parse(expr: str):
    """Parses an expression with sympy."""
    py_expr = expr.replace("^", "**")
    return sympy_parser.parse_expr(
        py_expr,
        transformations=(
            sympy_parser.standard_transformations
            + (sympy_parser.implicit_multiplication_application,)
        ),
    )
# ---
def col_clause(self):
        return self.field + " = ?", [self.pattern]
# ---
def cluster_status_html(cluster: str):
            if cluster not in self.clusters:
                return '<div class="error">Unknown cluster</div>'

            ports = self.port_mappings[cluster]
            return self._build_status_html(cluster, ports)
# ---
def add_consecutive_nums(nums):
    result = [b+a for a, b in zip(nums[:-1], nums[1:])]
    return result
# ---
def git_version(git_path, module):
    """return the installed version of git"""
    cmd = "%s --version" % git_path
    (rc, out, err) = module.run_command(cmd)
    if rc != 0:
        # one could fail_json here, but the version info is not that important,
        # so let's try to fail only on actual git commands
        return None
    rematch = re.search('git version (.*)$', to_native(out))
    if not rematch:
        return None
    return LooseVersion(rematch.groups()[0])
# ---
def test_unrecognized_type(self):
        """Dynamizer throws error on unrecognized type"""
        value = {
            "ASDF": "abc",
        }
        with self.assertRaises(TypeError):
            self.dynamo.dynamizer.decode(value)
# ---
def __init__(self, name: str, count: int, job_id: Any):
        """Args:
        name: Actor name prefix
        count: Number of actors to discover
        job_id: JobId/JobName for the actor job
        """
        self._name = name
        self._count = count
        self._job_id = job_id
        self._handles: list[ActorHandle] = []
        self._discovered_names: set[str] = set()
# ---
def sello(self):
        return self.__sello
# ---
def get_slice(self, group_id: str) -> VmGroupProtocol | None:
        """Get a specific VM group by ID."""
        with self._vm_groups_lock:
            return self._vm_groups.get(group_id)
# ---
def go(*args, **kw):
                canary2.append(name)
# ---
def parallelogram_perimeter(b,h):
  perimeter=2*(b*h)
  return perimeter
# ---
def get_job_form(method, notebook, data):
    context = {'platforms': notebook.get_platforms()}
    context['values'] = ({'current': {'options': {}}} if notebook.current_job is None
                         else notebook.current_job.data)
    return context
# ---
def create_contamination_column(pd_tool_bins):
    pd_tool_bins['newcolumn'] = 1 - pd_tool_bins['precision_bp']
# ---
def manipulate(text):
            for key in rules:
                rule = getattr(self.rules, key)
                m = rule.match(text)
                if not m:
                    continue
                getattr(self, 'parse_%s' % key)(m)
                return m
            return False
# ---
def _dirs(self):
        return set(util.dirs(self._fileset))
# ---
def _grug_125m_config() -> GrugformerH2HConfig:
    return GrugformerH2HConfig(
        max_seq_len=2048,
        hidden_dim=512,
        intermediate_dim=1792,
        num_layers=6,
        num_heads=8,
        num_kv_heads=8,
        head_dim=None,
    )
# ---
def extract_cookies(self, response, request):
        self.ec_req, self.ec_r = request, response
# ---
def _maybe_len(a):
    try:
        return len(a)
    except TypeError:
        return 0
# ---
def isfinite(a: A) -> A:
    return wrap_elemwise_unary(jnp.isfinite, a)
# ---
def check_type(test_tuple):
  res = True
  for ele in test_tuple:
    if not isinstance(ele, type(test_tuple[0])):
      res = False
      break
  return (res)
# ---
def loss_fn(x_in, w_in, y_in):
        return fused_cross_entropy_loss_and_logsumexp_penalty(
            x_in,
            y_in,
            w_in,
            reduction="mean",
            logsumexp_weight=0.0,
            block_sizes=block_sizes,
            dtype=accum_dtype,
            logit_soft_cap=None,
            implementation=implementation,
        )
# ---
def Caller_Places_Call (self, Number):
        self.Step (Message = "Caller places call to " + str (Number) + "...")

        self.Log (Message = "Dialling through caller agent...")
        self.Caller.dial (Number)
# ---
def print_ls_l_desc(desc, **kwargs):
    print(get_ls_l_desc(desc, **kwargs))
# ---
def min_length_list(input_list):
    min_length = min(len(x) for x in input_list )  
    min_list = min(input_list, key = lambda i: len(i))
    return(min_length, min_list)
# ---
def _track_training_step(self):
        """Called after each training step."""
        self.steps_completed += 1
# ---
def loss_fn(mlp, x):
        return mlp(x).mean().scalar(), {}
# ---
def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        logger.log(
            level,
            "%s took %.4f seconds",
            func.__qualname__,
            end_time - start_time,
        )
        return result
# ---
def remove_empty(list1):
  remove_empty = [x for x in list1 if x]
  return remove_empty
# ---
def add_plugin(self, route, policy):
		self._routes[route] = policy
# ---
def __str__(self):
        """
        Imprime el cfdi en el siguiente orden
        emisor, fecha de timbrado, tipo de comprobante, rfc emisor, uuid,_
        receptor, rfc receptor, subtotal, ieps, iva, retiva, retisr, tc, total
        """
        respuesta = '\t'.join( map(str, self.lista_valores))
        return respuesta
# ---
def base_token_offset(self) -> int:
        """First base vocabulary token ID."""
        return 3 + self.num_position_tokens
# ---
def load(cls: "NumpySerializable", path: Path) -> "NumpySerializable":
        """Load the object from an NPZ file.

        Parameters
        ----------
        path : Path
            The path to the file.

        Returns
        -------
        Serializable
            The loaded object.

        """
        return cls(**np.load(path))
# ---
def bitwise_or(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "bitwise_or")
    if (
        x1.dtype not in _integer_or_boolean_dtypes
        or x2.dtype not in _integer_or_boolean_dtypes
    ):
        raise TypeError("Only integer or boolean dtypes are allowed in bitwise_or")
    return elemwise(nxp.bitwise_or, x1, x2, dtype=result_type(x1, x2))
# ---
def test_str_split_unbox_df(self):
        def test_impl(df):
            return df.A.iloc[0]

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        df2 = pd.DataFrame({'A': df.A.str.split(',')})
        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(df2), test_impl(df2))
# ---
def __init__(self, conn_id, sql, *args, **kwargs):
        self.sql = sql
        self.conn_id = conn_id
        super(SqlSensor, self).__init__(*args, **kwargs)
# ---
def arctan2(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.arctan2](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.arctan2.html)
    """
    return jnp.arctan2(x1, x2)
# ---
def regenerate(c):
    """Automatically regenerate site upon file modification"""
    c.run('pelican -r -s pelicanconf.py')
# ---
def has_pending_call(self, ar):
        """
        Returns true if the call (keyed by the AsyncResult returned by _routing_call) is still pending.
        """
        for _, qar, _, _, _, _ in self._ctrl_queue.queue:
            if qar == ar:
                return True

        return False
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"transformer": "model", "embeddings": None}
# ---
def cmd(self):
        return '''
               docker run --rm --env AWS_ACCESS_KEY_ID={} --env AWS_SECRET_ACCESS_KEY={} rita/download-rita --year {} --month {} --data_path {}/{} 
        '''.format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, self.year, self.month, self.root_path, self.raw_path)
# ---
def cleanup(self):
        """Clean up venv and all tracked processes."""
        logger.info(f"Cleaning up job {self.job_id}")
        if self.process_env is not None:
            self.process_env.__exit__(None, None, None)
            self.process_env = None
# ---
def tokenize_prompt(self, prompt):
            return np.array([ord(c) for c in prompt], dtype=np.int32)
# ---
def action_assign(self, cr, uid, ids, *args):
        """ Changes state to confirmed or waiting.
        @return: List of values
        """
        todo = []
        for move in self.browse(cr, uid, ids):
            if move.state in ('confirmed', 'waiting'):
                todo.append(move.id)
        res = self.check_assign(cr, uid, todo)
        return res
# ---
def skip_if_hf_model_not_accessible(model_id: str):
    def try_load_hf(model_id):
        try:
            AutoConfig.from_pretrained(model_id)
        except Exception:
            return False
        else:
            return True

    return pytest.mark.skipif(not try_load_hf(model_id), reason="HuggingFace model not accessible")
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "x": 1, "y": 2, "z": "z1"},
                {"id": 2, "x": 2, "y": 3, "z": "z2"},
                {"id": 3, "x": 3, "y": 4, "z": "z3"},
                {"id": 4, "x": 4, "y": 5, "z": "z4"},
            ],
        )
# ---
def test_compute_bracket_mask_nested(tokenizer):
    mask = compute_bracket_mask("f([", tokenizer)
    # ] should be allowed (matches open bracket).
    assert float(mask[tokenizer.encode_char("]")]) == 1.0
    # ) should be blocked (wrong bracket type -- innermost is [).
    assert float(mask[tokenizer.encode_char(")")]) == 0.0
# ---
def load_template(self, file_name):
        filepath = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                'templates', file_name)
        f = open(filepath)
        t = template_format.parse(f.read())
        f.close()
        return t
# ---
def test_bundle_download_intermittent(cluster):
    """Bundle download fails intermittently, task retries handle it."""
    _url, client = cluster
    enable_chaos(
        "worker.bundle_download", failure_rate=0.5, max_failures=2, error=RuntimeError("chaos: download failed")
    )
    job = submit(client, _quick, "bundle-fail", max_retries_failure=3)
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def parse(self, x, y):
        if (x,y) not in self.resolutions:
            resolutions = ', '.join(['%sx%s' % (a, b) for a,b in self.resolutions])
            raise Exception('Resolution %s x %s not supported. Available resolutions: %s' % (x,y, resolutions)  )
        return Resolution(x, y)
# ---
def test_ball():
    check_gen_is_equal(lambda k, s: jax.random.ball(k, Digit.size, shape=s), lambda k, s: hax.random.ball(k, s, Digit))
# ---
def test_with_scan():
    X = hax.Axis("x", 5)
    ref = hax.new_ref(hax.zeros(X))

    @jax.jit
    def foo(ref, xs):
        def scan_fn(_, x):
            ref_slice = ref.slice({"x": x})
            ref_slice[...] = (x * x).astype(ref_slice.dtype)
            return None, x * 2

        return hax.scan(scan_fn, X)(None, xs)[1]

    out = foo(ref, jnp.arange(X.size))

    assert jnp.all(ref.value().array == jnp.arange(X.size) ** 2)

    assert jnp.all(out == jnp.arange(X.size) * 2)
# ---
def case(context):
        """Check that the context has been set up."""
        assert context == {"squee": "kapow"}
# ---
def filter_evennumbers(nums):
 even_nums = list(filter(lambda x: x%2 == 0, nums))
 return even_nums
# ---
def address(self) -> str:
        port = self._actual_port or self._port
        return f"{self._host}:{port}"
# ---
def testAssignName(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        foo = 'bar'
        print foo""")))
# ---
def min_token_distances(
    tokens1: Tokenized,
    tokens2: Tokenized,
    random: np.random.Generator,
    noise_std: float = 1,
    axis: int = 1,
):
    tokens2_centers = tokens2["center_coords"].copy()
    tokens2_centers[~tokens2["resolved_mask"]] = np.nan
    tokens1_centers = tokens1["center_coords"].copy()
    tokens1_centers[~tokens1["resolved_mask"]] = np.nan
    return min_distances(tokens1_centers, tokens2_centers, random, noise_std, axis)
# ---
def _pacify_named_arrays(leaf):
    if isinstance(leaf, NamedArray):
        return _PassiveNamedArray(leaf.array, leaf.axes)
    elif isinstance(leaf, _PassiveNamedArray):
        assert False, "PassiveNamedArray should not be present in the tree"
    else:
        return leaf
# ---
def the_object_name_has_a_type_representation_of_context(name, type, context):
    ifc = an_ifc_file_exists()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    context, subcontext, target_view = context.split("/")
    assert ifcopenshell.util.representation.get_representation(
        element, context, subcontext or None, target_view or None
    )
# ---
def test_allocated_ports_are_usable(allocator):
    """Test that allocated ports can actually be bound."""
    ports = allocator.allocate(count=3)

    for port in ports:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(("", port))
# ---
def selected_target(self, value):
        self._selected_target = value
        self._match_entries()
# ---
def EscapeForVim( text ):
  return ToUnicode( text.replace( "'", "''" ) )
# ---
def flow_backend_ctx():
    """Set up sync backend for all transform tests."""
    with fray_default_job_ctx(create_job_ctx("sync")):
        yield
# ---
def _load_dataset(self):
        # obnoxiously, the dataset loading stuff doesn't work with ray because of multiprocessing
        # so we have to do this hacky thing where we load the dataset in the worker
        return datasets.load_dataset(self.id, split=self.split, streaming=self.streaming, **self.kwargs)
# ---
import re
def find_adverbs(text):
  for m in re.finditer(r"\w+ly", text):
    return ('%d-%d: %s' % (m.start(), m.end(), m.group(0)))
# ---
def _scan_binop(scn, inc, block_id=None, **kwargs):
        bi = block_id[axis] % split_every
        ind = tuple(
            slice(bi, bi + 1) if i == axis else slice(None) for i in range(inc.ndim)
        )
        return binop(scn, inc[ind])
# ---
def check_Type_Of_Triangle(a,b,c): 
    sqa = pow(a,2) 
    sqb = pow(b,2) 
    sqc = pow(c,2) 
    if (sqa == sqa + sqb or sqb == sqa + sqc or sqc == sqa + sqb): 
        return ("Right-angled Triangle") 
    elif (sqa > sqc + sqb or sqb > sqa + sqc or sqc > sqa + sqb): 
        return ("Obtuse-angled Triangle") 
    else: 
        return ("Acute-angled Triangle")
# ---
def __enter__(self):
        """\
        Enter context handler. May raise RuntimeError in case the connection
        could not be created.
        """
        self.start()
        # Wait for protocol to connect.
        event = OrEvent(self.connected, self.closed)
        event.wait(self.default_timeout_s)
        return self
# ---
def destroy(self, context):
        """Delete a the pf from the DB."""
        del self.virtual_function_list[:]
        super(PhysicalFunction, self).destroy(context)
# ---
def killall():
    global player
    status = 0
    if player == 'omxplayer':
        control = "/usr/local/bin/omxcontrol"
        status = subprocess.call([control,  "stop"])
    status = subprocess.call(["pkill", player])

    return status
# ---
def check_vm_params_for_linux_with_external_kernel(self):
        self.assertEquals(self.vm['platform']['nx'], 'false')
        self.assertEquals(self.vm['PV_args'], 'root=/dev/xvda1')
        self.assertNotEquals(self.vm['PV_kernel'], '')
        self.assertNotEquals(self.vm['PV_ramdisk'], '')

        # check that these are not set
        self.assertEquals(self.vm['HVM_boot_params'], {})
        self.assertEquals(self.vm['HVM_boot_policy'], '')
# ---
def __init__(
        self,
        shape: T_Shape,
        dtype: T_DType,
        chunks: T_RegularChunks,
    ):
        super().__init__(shape, dtype, chunks)
# ---
def _send_message(self, message):
        assert jax.process_index() == 0
        out = broadcast_shard(jnp.array(message), PartitionSpec())
        return out
# ---
def do_block(carry: CarryT, block: M, *args, **kwargs) -> CarryT:
            return fn(block, carry, *args, **kwargs)
# ---
def run_hooks(self, info: StepInfo, force: bool = False):
        self.hooks.run_hooks(info, force=force)
# ---
def initialize(self):
        if self.auto_start_cluster:
            auto_ray_cluster(address=self.address, start_workers=self.start_workers)
# ---
def size(self) -> int:
        """Number of elements in the array."""
        return reduce(mul, self.shape, 1)
# ---
def test_hf_gpt2_roundtrip_fa():
    hf_config = HfGpt2Config.from_pretrained("gpt2")
    config = Gpt2Config.from_hf_config(hf_config)
    config = dataclasses.replace(config, use_flash_attention=True, flash_attention_block_size=128)
    _roundtrip_compare_gpt2_checkpoint("gpt2", None, config=config)
# ---
def _flatten_hist(self, fts: HistChanneled) -> HistBatched:
        return rearrange(fts, "n (hist c) h w -> (n hist) c h w", hist=self.hist + 1)
# ---
def sleep_seconds(n: int):
        time.sleep(n)
# ---
def test_scoring_autocomplete():
    assert score_autocomplete('}}]])})]') == 288957
    assert score_autocomplete(')}>]})') == 5566
    assert score_autocomplete('}}>}>))))') == 1480781
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        pass
# ---
def test_cumulative_sum_unsupported_include_initial(spec):
    a = xp.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], chunks=(4,))
    with pytest.raises(NotImplementedError):
        xp.cumulative_sum(a, axis=0, include_initial=True)
# ---
def find_length(string, n): 
	current_sum = 0
	max_sum = 0
	for i in range(n): 
		current_sum += (1 if string[i] == '0' else -1) 
		if current_sum < 0: 
			current_sum = 0
		max_sum = max(current_sum, max_sum) 
	return max_sum if max_sum else 0
# ---
def test_get_style_defs(self):
        fmt = HtmlFormatter()
        sd = fmt.get_style_defs()
        self.assert_(sd.startswith('.'))

        fmt = HtmlFormatter(cssclass='foo')
        sd = fmt.get_style_defs()
        self.assert_(sd.startswith('.foo'))
        sd = fmt.get_style_defs('.bar')
        self.assert_(sd.startswith('.bar'))
        sd = fmt.get_style_defs(['.bar', '.baz'])
        fl = sd.splitlines()[0]
        self.assert_('.bar' in fl and '.baz' in fl)
# ---
def test_works_with_defined_features(self):
    # Make a list of Features.
    features = [
        ee.Feature(
            ee.Geometry.Rectangle(30.01, 59.80, 30.59, 60.15),
            {'name': 'Voronoi'},
        ),
        ee.Feature(ee.Geometry.Point(-73.96, 40.781), {'name': 'Thiessen'}),
        ee.Feature(ee.Geometry.Point(6.4806, 50.8012), {'name': 'Dirichlet'}),
    ]

    fc = ee.FeatureCollection(features)

    df = dask_ee.read_ee(fc)

    self.assertEqual(list(df.columns), ['geo', 'name'])
# ---
def _flatten_nested_dict(d):
    def items():
        for key, value in d.items():
            if isinstance(value, dict):
                for subkey, subvalue in _flatten_nested_dict(value).items():
                    yield key + "/" + subkey, subvalue
            else:
                yield key, value

    return dict(items())
# ---
def utf8_to_bytearray(x):
    return bytearray(x, 'utf-8')
# ---
def compute_svm_subjects(K, y, n_folds=5):
    """
    """
    cv = KFold(len(K)/2, n_folds)
    scores = np.zeros(n_folds)
    for i, (train, test) in enumerate(cv):
        train_ids = np.concatenate((train, len(K)/2+train))
        test_ids = np.concatenate((test, len(K)/2+test))
        clf = SVC(kernel='precomputed')
        clf.fit(K[train_ids, :][:, train_ids], y[train_ids])
        scores[i] = clf.score(K[test_ids, :][:, train_ids], y[test_ids])

    return scores.mean()
# ---
def test_stdin_nulled_by_default(self):
        print("XXX this test may well hang instead of crashing")
        print("XXX which indicates an error in the underlying capturing")
        print("XXX mechanisms")
        with self.getcapture():
            pytest.raises(IOError, "sys.stdin.read()")
# ---
def _teardown_volumes(self):
        for drive in self._prepared_volumes:
            try:
                self._irs.teardownImage(drive['domainID'],
                                        drive['poolID'],
                                        drive['imageID'])
            except Exception as e:
                logging.error('Job %r error tearing down drive: %s',
                              self._vmid, e)
# ---
def should_display_status_to_user(self):
        """Whether or not the status from this attempt should be displayed to the user."""
        return True
# ---
def validate_variant_based_on_change(self):
		if not self.is_new() and (self.variant_of or (self.has_variants and frappe.get_all("Item", {"variant_of": self.name}))):
			if self.variant_based_on != frappe.db.get_value("Item", self.name, "variant_based_on"):
				frappe.throw(_("Variant Based On cannot be changed"))
# ---
def test_node_source_span():
    source = "x = 1 + 2\n"
    tree = ast.parse(source)
    # The BinOp node should span "1 + 2".
    assign = tree.body[0]
    binop = assign.value
    span = _node_source_span(source, binop)
    assert span is not None
    start, end = span
    assert source[start:end] == "1 + 2"
# ---
def test_two_network_labels(self):
        CONF.network_label_regex = '^(private|public)$'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(2, len(ip))
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)
# ---
def test_success(tmp_path, timing_map, n_tasks, retries, use_backups):
    outputs = asyncio.run(
        run_test(
            function=partial(deterministic_failure, tmp_path, timing_map),
            input=range(n_tasks),
            retries=retries,
            use_backups=use_backups,
        )
    )

    assert outputs == set(range(n_tasks))

    check_invocation_counts(tmp_path, timing_map, n_tasks, retries)
# ---
def __init__(
        self, plotly_name="minexponent", parent_name="choropleth.colorbar", **kwargs
    ):
        super(MinexponentValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            edit_type=kwargs.pop("edit_type", "colorbars"),
            min=kwargs.pop("min", 0),
            **kwargs
        )
# ---
def yaml_dict(self, value):
        ''' setter method for yaml_dict '''
        self.__yaml_dict = value
# ---
def open_shard_at_row(self, shard_name: str, row: int):
        return self.docs[int(shard_name)][row:]
# ---
def fai(domain, b, threshold=None):
    ''' Floating Algae Index. Method from paper: Feng, Hu, Chen, Cai, Tian, Gan,
    Assessment of inundation changes of Poyang Lake using MODIS observations
    between 2000 and 2010. Remote Sensing of Environment, 2012.
    '''
    if threshold == None:
        threshold = float(domain.algorithm_params['fai_threshold'])
    return get_fai(b).lte(threshold)
# ---
def _multislice_info_from_head(head: SliceInfo, slice_id: int, num_slices: int) -> MultisliceInfo:
    return MultisliceInfo(
        coordinator_ip=head.ip_address,
        slice_id=slice_id,
        num_slices=num_slices,
        port=8081,
    )
# ---
def saveActiveProfile(newProfile = True):
            if newProfile:
                activeProfileIter = self.profilesComboModel.append(profile)
                self.profilesCombo.set_active_iter(activeProfileIter)

            self.prefsDict['profile'] = profile
            self.prefsDict['activeProfile'] = profile
            self.saveBasicSettings()
            self.writeUserPreferences()
# ---
def __init__(self, driver):
        super(AddMoviePage, self).__init__(driver)
        self.nav = NavBlock(driver)
# ---
def temp_cache_dir(tmp_path):
    """Create a temporary cache directory."""
    cache_dir = tmp_path / "cache"
    cache_dir.mkdir()
    return cache_dir
# ---
def _prod(nums):
    out = 1
    for n in nums:
        out = out * n
    return out
# ---
def _dtype_name(dtype: Optional[jnp.dtype]) -> Optional[str]:
    if dtype is None:
        return None
    return jnp.dtype(dtype).name
# ---
def remove_references_from_html(html: str) -> str:
    """
    Removes the references list and heading from the article.
    """
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html, "html.parser")

    reflist = soup.find("div", {"class": "reflist"})
    if reflist:
        reflist.extract()

    ref_heading = soup.find("h2", {"id": "References"})
    if ref_heading:
        ref_heading.extract()

    return str(soup)
# ---
def fn(a, b):
        return a + b
# ---
def sum(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.sum, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype)
# ---
def peek(self) -> T | None:
        files = sorted(self.fs.ls(str(self.pending_dir), detail=False))
        files = [f for f in files if f.rstrip("/") != str(self.pending_dir).rstrip("/")]

        if not files:
            return None

        with self.fs.open(files[0], "rb") as f:
            return pickle.load(f)
# ---
def get_cmap_limits(data: np.ndarray, diverging=False) -> tuple[float, float]:
    vmin = np.nanmin(data)
    vmax = np.nanmax(data)
    if diverging:
        vmax = max(abs(vmin), abs(vmax))
        vmin = -vmax
    return vmin, vmax
# ---
def tokendata_to_tuple(token):
    return tuple(getattr(token, f.name) for f in fields(token))
# ---
def index_where(pred: Callable[[T], bool], xs: Sequence[T], start: int = 0) -> int:
    for i in range(start, len(xs)):
        if pred(xs[i]):
            return i
    raise ValueError("No element satisfies predicate")
# ---
def format_sci(self, val):
        return f"{val:.0e}".replace("e-0", "e-").replace("e+0", "e+")
# ---
def set_current_task(self, task_name: str):
        self._current_task = task_name
        if self.sample_logging_config.should_log() and task_name not in self.sample_outputs:
            self.sample_outputs[task_name] = []
# ---
def test_token_uri(self):
        self.assertEqual(self.xe.token_uri, '/auth/token-services')
# ---
def __neg__(self, /):
        if self.dtype not in _numeric_dtypes:
            raise TypeError("Only numeric dtypes are allowed in __neg__")
        return elemwise(nxp.negative, self, dtype=self.dtype)
# ---
import re
def extract_quotation(text1):
  return (re.findall(r'"(.*?)"', text1))
# ---
def spec(tmp_path):
    return cubed.Spec(tmp_path, allowed_mem=100000)
# ---
def seq_lens(self) -> ht.i32[NamedArray, "seq"]:  # type: ignore[name-defined]
        """Current logical length for each active sequence."""
        return self.sequences.seq_lens
# ---
def items(self, section_name):
            if section_name != section:
                raise NoSectionError(section_name)
            return {
                'memcache_servers': memcache_servers,
                'memcache_serialization_support':
                memcache_serialization_support,
                'memcache_max_connections': memcache_max_connections,
            }
# ---
def get_key(dict): 
    list = [] 
    for key in dict.keys(): 
        list.append(key)           
    return list
# ---
def __len__(self) -> int:
        return len(self.caches)
# ---
def generate_fun_name(zone: str) -> str:
    """
    Generate a fun VM name in wandb style.

    Format: {verb}-{noun}-{zone}-{random_id}
    Example: running-tiger-us-west4-a-7x3k
    """
    import random

    verb = random.choice(VERBS)
    noun = random.choice(NOUNS)
    random_id = generate_random_id()
    return f"{verb}-{noun}-{zone}-{random_id}"
# ---
def format_pwms_new(pwms_new):
    return format_line(prefix='new pwms'.rjust(RJUST), values=pwms_new)
# ---
def randint(key, shape: AxisSpec, minval: NamedOrNumeric, maxval: NamedOrNumeric, dtype=int):
    shape = axis_spec_to_shape_dict(shape)
    minval = broadcast_to(minval, shape).array
    maxval = broadcast_to(maxval, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.randint(key=key, shape=jax_shape, minval=minval, maxval=maxval, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def run_streaming(self, command: str) -> Any:
        """Run command with streaming stdout.

        Returns subprocess.Popen in production, FakePopen in tests.
        """
        ...
# ---
def run_chainlink(args):
    """Run a chainlink command and return output."""
    try:
        result = subprocess.run(
            ["chainlink"] + args,
            capture_output=True,
            text=True,
            timeout=5
        )
        return result.stdout.strip() if result.returncode == 0 else None
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        return None
# ---
def __init__(self, **kwargs):
        self.kwargs = kwargs
# ---
def output_filename(self, suffix: str = ".jsonl.gz") -> str:
        return f"{self.benchmark}_{self.date_range}{suffix}"
# ---
def visitdir(self, dir):
        m1dir = self.m1.visitdir(dir)
        m2dir = self.m2.visitdir(dir)

        # if both matchers return "all" then we know for sure we don't need
        # to visit this directory. Same if all matchers return False. In all
        # other case we have to visit a directory.
        if m1dir == "all" and m2dir == "all":
            return False
        if not m1dir and not m2dir:
            return False
        return True
# ---
def move_to_element(self, to_element):
        """Moving the mouse to the middle of an element.
        Args:
            to_element: The element to move to.
        """
        self._actions.append(lambda:
            self._driver.execute(Command.MOVE_TO, {'element': to_element.id}))
        return self
# ---
import re
def capital_words_spaces(str1):
  return re.sub(r"(\w)([A-Z])", r"\1 \2", str1)
# ---
def c_skip(self, sigma):
        return (self.sigma_data**2) / (sigma**2 + self.sigma_data**2)
# ---
def discover(self) -> str | None:
        """Find existing controller address, or None."""
        ...
# ---
def cli():
    """Iris Controller - Cluster control plane."""
    pass
# ---
import math   
def min_Operations(A,B):  
    if (A > B): 
        swap(A,B)  
    B = B // math.gcd(A,B);  
    return B - 1
# ---
def test_synthetic_subtrees_valid_python(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=20)
    for entry in entries:
        try:
            ast.parse(entry.source)
        except SyntaxError:
            pytest.fail(f"Synthetic subtree is not valid Python: {entry.source!r}")
# ---
def keys(self):
		return self._addresses.keys()
# ---
def TrainBatch(self):
        return self.config.TrainBatch
# ---
def merge_lora_modules(module: M) -> M:
    """
    Merges LoRA modules into their wrapped modules. That is, it adds the LoRA parameters to the wrapped weights,
    producing a modified base model with no LoRA parameters.
    """

    def _merge_lora_modules(module):
        if isinstance(module, LoraLinear):
            return module.merge()
        else:
            return module

    return jax.tree_util.tree_map(_merge_lora_modules, module, is_leaf=lambda node: isinstance(node, LoraLinear))
# ---
def _visible_endpoints(self, predicate: Callable[[ControllerEndpoint], bool]) -> list[ControllerEndpoint]:
        """Return endpoints matching predicate whose jobs are in non-terminal states."""
        results = []
        for ep in self._endpoints.values():
            if not predicate(ep):
                continue
            job = self._jobs.get(ep.job_id)
            if job and not job.is_finished():
                results.append(ep)
        return results
# ---
def test_hash(self):
        @event.event
        def handler():
            pass

        h_inst = event.HandlerInstance.from_handler(handler)
        h_inst2 = event.HandlerInstance.from_handler(handler)
        assert h_inst is not h_inst2
        assert hash(h_inst) == hash(h_inst2)
        assert h_inst != h_inst2
# ---
def reconcile(self) -> None:
        """Discover and adopt existing VM groups from the cloud.

        Called once at startup to recover state from a previous controller.
        """
        with self._vm_groups_lock:
            for vm_group in self._vm_manager.discover_vm_groups():
                self._vm_groups[vm_group.group_id] = vm_group
# ---
def _building_job():
    """Simple job for testing BUILDING state (used with chaos delay injection)."""
    return "ok"
# ---
def init(cls, Vocab: Axis, config: MConfig, *, key: PRNGKeyArray) -> "LmWithHfSerializationMixin":
        pass
# ---
def round_axis_for_partitioning(axis: Axis, mapping: ResourceMapping | None = None) -> Axis:
    """Round an axis so that it's divisible by the size of the partition it's on"""
    size = physical_axis_size(axis, mapping)
    if size is None:
        return axis
    else:
        new_size = (axis.size + size - 1) // size * size
        return Axis(axis.name, new_size)
# ---
def find_ports(self, inc_port):
        ''' find a specific port '''
        for port in self.get_ports():
            if port['port'] == inc_port['port']:
                return port

        return None
# ---
def accuracy(self):
        """The fractional voltage of the least significant bit. """
        return self._Vref / float(self.get_value_max())
# ---
def terminate(self) -> None:
        """Terminate this VM group and all its VMs.

        This stops VM lifecycle threads, unregisters VMs from the registry,
        and deletes the underlying cloud resource.
        """
        ...
# ---
def blockquote(node: RenderTreeNode, context: RenderContext) -> str:
    marker = "> "
    with context.indented(len(marker)):
        text = make_render_children(separator="\n\n")(node, context)
        lines = text.splitlines()
        if not lines:
            return ">"
        quoted_lines = (f"{marker}{line}" if line else ">" for line in lines)
        quoted_str = "\n".join(quoted_lines)
        return quoted_str
# ---
def init(a1, a2):
            return MyModule2(MyModule(a1, a2), MyModule(a1, a2))
# ---
def _unary_op(f):
        @functools.wraps(f)
        def func(self, *args, **kwargs):
            ds = self.coords.to_dataset()
            for k in self.data_vars:
                ds._variables[k] = f(self._variables[k], *args, **kwargs)
            return ds
        return func
# ---
def encode(self, value) -> NDArray[np.uint64]:
        """Encodes & places the field; caller should + the field into the container."""
        # Note that just `view(self.uint_type())` is not sufficient -- we need to
        # extend the value to fill the full uint64 size so when we shift it doesn't
        # just shift off the end of the (smaller) uint_type.
        return (
            value.astype(self.dtype).view(self.uint_type()).astype(np.uint64)
            << self.offset
        )
# ---
def _write_jsonl_gz(path: Path, records: list[dict]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with gzip.open(path, "wt", encoding="utf-8") as handle:
        for record in records:
            handle.write(json.dumps(record))
            handle.write("\n")
# ---
def find_external_ips(self, inc_external_ip):
        ''' find a specific external IP '''
        val = None
        try:
            idx = self.get_external_ips().index(inc_external_ip)
            val = self.get_external_ips()[idx]
        except ValueError:
            pass

        return val
# ---
def __call__(self):
        with self._lock:
            if self._process and self._process.poll() is None:
                self.kill()

            print(f"Syncing changes to {self._host_alias}...")
            sync_to_remote(self._host_alias, self._sync_path)

            ssh_cmd = build_ssh_command(self._host_alias, self._command_str, self._env_dict)
            print(f"Running: {self._command_str}")
            self._process = subprocess.Popen(ssh_cmd, stdin=subprocess.DEVNULL)
# ---
def get_Char(strr):  
    summ = 0
    for i in range(len(strr)): 
        summ += (ord(strr[i]) - ord('a') + 1)  
    if (summ % 26 == 0): 
        return ord('z') 
    else: 
        summ = summ % 26
        return chr(ord('a') + summ - 1)
# ---
def _rpa_tol() -> float:
    devices = jax.devices()
    # 2%?!?!
    return 2e-2 if any(device.platform == "tpu" for device in devices) else 1e-4
# ---
def average(self):
        if self._n == 0:
            return 0.0
        return self._elapsed / self._n
# ---
def get_validation_aggregator(
        metadata: dict[str, dict[str, str]],
        hist: int,
        area_weights: torch.Tensor,
        wet: torch.Tensor,
        num_prognostic_channels: int,
    ) -> ValidateAggregator:
        return ValidateAggregator(
            metadata=metadata,
            hist=hist,
            area_weights=area_weights,
            wet=wet,
            num_prognostic_channels=num_prognostic_channels,
        )
# ---
def init_optimizer_for_trainables(optimizer, trainable_model):
    """
    Initializes the optimizer state for the trainable parameters of the model.
    """
    _, trainable = partition_for_grad_overwrite(trainable_model)  # doesn't make a huge difference, but saves some ram
    opt_state = optimizer.init(trainable)
    return opt_state
# ---
def _json_default(value):
    """
    Provide a best-effort JSON serialization for objects returned by the eval harness.
    """
    if dataclasses.is_dataclass(value):
        return dataclasses.asdict(value)

    if isinstance(value, set):
        return list(value)

    if hasattr(value, "to_dict") and callable(value.to_dict):
        try:
            return value.to_dict()
        except Exception:
            pass

    return repr(value)
# ---
def test_std(spec, axis, correction, keepdims):
    a = xp.asarray(
        [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], chunks=(2, 2), spec=spec
    )
    b = xp.std(a, axis=axis, correction=correction, keepdims=keepdims)
    assert_array_equal(
        b.compute(),
        np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]).std(
            axis=axis, ddof=correction, keepdims=keepdims
        ),
    )
# ---
def to_int(val):
  return int(val)
# ---
def logical_and(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "logical_and")
    if x1.dtype not in _boolean_dtypes or x2.dtype not in _boolean_dtypes:
        raise TypeError("Only boolean dtypes are allowed in logical_and")
    return elemwise(nxp.logical_and, x1, x2, dtype=nxp.bool)
# ---
def build(self, reference_model: eqx.Module) -> eqx.Module:
        """Initialize any learned components (e.g., value heads)."""
        return self
# ---
def _process_blockwise_chunk(
    a,
    axis=None,
    by=None,
    blockwise_func=None,
    by_read_chunks=None,
    target_chunks=None,
    groups_per_chunk=None,
    block_id=None,
    **kwargs,
):
    idx = block_id
    bi = idx[axis]

    by = by[get_item(by_read_chunks, (bi,))]

    start_group = bi * groups_per_chunk

    return blockwise_func(
        a,
        by,
        axis=axis,
        start_group=start_group,
        num_groups=target_chunks[axis][bi],
        **kwargs,
    )
# ---
def mock_bundle_cache():
    """Create mock BundleCache."""
    cache = Mock(spec=BundleCache)
    cache.get_bundle = Mock(return_value=Path("/tmp/bundle"))
    return cache
# ---
def modulate(x, shift, scale):
  return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)
# ---
def start_server(self, model: LmHeadModel) -> None:
        pass
# ---
def _maybe_fold_in_key(self, key, index):
        if key is not None:
            key = jax.random.fold_in(key, index)
        return key
# ---
def _apply_block(self, block: Block, batch) -> bool:
        address_set = self._state.prepare_address_list(block)  # Prepare list for current block
        addresses_state = self._state.get_state_mainchain(address_set)
        if not block.apply_state_changes(addresses_state):
            return False
        self._state.put_addresses_state(addresses_state, batch)
        return True
# ---
def test_transaction_engine_fn_rollback(self):
        fn = self._trans_rollback_fn()
        assert_raises_message(
            Exception,
            "breakage",
            testing.db.transaction, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def big_sum(nums):
      sum= max(nums)+min(nums)
      return sum
# ---
def train_set(
        self,
        options: CacheOptions = CacheOptions.default(),
        *,
        key: Optional[PRNGKeyArray] = None,
    ) -> AsyncDataset[AudioTextDict]:
        pass
# ---
def flow_backend_ctx():
    """Set up sync backend for all download tests."""
    with fray_default_job_ctx(create_job_ctx("sync")):
        yield
# ---
def totext(obj):
        if isinstance(obj, bytes):
            obj = str(obj, 'UTF-8')
        assert isinstance(obj, str)
        return obj
# ---


def remove_vowels(text):
    """
    remove_vowels is a function that takes string and returns string without vowels.
    >>> remove_vowels('')
    ''
    >>> remove_vowels("abcdef\nghijklm")
    'bcdf\nghjklm'
    >>> remove_vowels('abcdef')
    'bcdf'
    >>> remove_vowels('aaaaa')
    ''
    >>> remove_vowels('aaBAA')
    'B'
    >>> remove_vowels('zbcd')
    'zbcd'
    """
    return "".join([s for s in text if s.lower() not in ["a", "e", "i", "o", "u"]])
# ---
def push(self, item):
        """Push a new element on the queue

        Parameters
        ----------
        item :
            The element to push on the queue

        """
        raise NotImplementedError
# ---
def put(self, obj: Any):
        """Store object in Ray's object store."""
        return ray.put(obj)
# ---
def copy(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.copy(), self.axes)
# ---
def build_request(method_info: MethodInfo, json_str: str | None, kwargs: dict[str, Any]) -> Message:
    """Build a protobuf request message from JSON or keyword arguments."""
    if json_str:
        data = json.loads(json_str)
    else:
        data = kwargs

    return json_format.ParseDict(data, method_info.input_type())
# ---
def _get_key_finger(self, path):
        '''
        Return a sha256 kingerprint for the key
        '''
        with salt.utils.fopen(path, 'r') as fp_:
            keydata = self.serial.loads(fp_.read())
            key = 'pub: {0}\nverify: {1}'.format(
                    keydata['pub'],
                    keydata['verify'])
        return hashlib.sha256(key).hexdigest()
# ---
def name(self) -> str:
        return "coiled"
# ---
def true_divide(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.true_divide(x1, x2)
# ---
def script_message_to(self, target, *args):
        self.command('script_message_to', target, *args)
# ---
def stop(self):
        self._elapsed += time.time() - self._start_time
# ---
def shutdown(self, wait: bool = True) -> None:
        del wait
        self._remote_client.shutdown()
        self._manager.stop()
# ---
def error():
    """ Display errors. """
    return render_template('error.html')
# ---
def __delitem__(self, field):
        delattr(self, field)
# ---
def run(self):
        try:
            super().run()
        except Exception as e:
            self._exception = e
# ---
def track_job(self, job_id: str):
        """Add a job to monitor."""
        self._tracked_jobs.add(job_id)
# ---
def empty_queue_space(self) -> jnp.ndarray:
        """Expose remaining queue capacity from ``TokenQueue``."""
        return self.tqueue.empty_queue_space
# ---
def cosh(a: A) -> A:
    return wrap_elemwise_unary(jnp.cosh, a)
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if dir == "":
            dir = self._path
        else:
            dir = self._path + "/" + dir
        return self._matcher.visitdir(dir)
# ---
def _load_audio_file(file_name, sampling_rate):
            with fsspec.open(audio_pointer, "rb", compression="infer") as f:
                array, sr = librosa.load(f, sr=sampling_rate)
            return {"array": array, "sampling_rate": sr}
# ---
def testDoubleRandom(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.float64)
      y = self._randMatrix(k, m, np.float64)
      self._testCpuMatmul(x, y)
# ---
def __init__(
            self,
            poke_interval=60,
            timeout=60*60*24*7,
            soft_fail=False,
            *args, **kwargs):
        super(BaseSensorOperator, self).__init__(*args, **kwargs)
        self.poke_interval = poke_interval
        self.soft_fail = soft_fail
        self.timeout = timeout
# ---
def partition(self, tensor):
        """Partition tensor into blocks."""
        assert tensor.shape == self._shape
        tensors = [tensor]
        for i, indices in self._splits:
            tensors_local = []
            for t in tensors:
                tensors_local.extend(jnp.split(t, indices_or_sections=indices, axis=i))
            tensors = tensors_local
        return tuple(tensors)
# ---
def _scale_weights(weights: dict[str, float], fraction: float) -> dict[str, float]:
    """Normalize `weights` and rescale them so they sum to `fraction`."""
    total_weight = sum(weights.values())
    if total_weight <= 0:
        raise ValueError("Mixture weights must sum to a positive value.")
    return {name: (value * fraction) / total_weight for name, value in weights.items()}
# ---
def get_choice_tokens(self, choice):
            return np.array([ord(c) for c in choice.message.content], dtype=np.int32)
# ---
def get_last_transactions(self):
        with self.lock:
            return self._state.get_last_txs()
# ---
def remove_length(test_str, K):
  temp = test_str.split()
  res = [ele for ele in temp if len(ele) != K]
  res = ' '.join(res)
  return (res)
# ---
def test_decode_token_position(tok):
    tid = tok.position_token_id(42)
    assert tok.decode_token(tid) == "<POS 42>"
# ---
def test_comparison(self):
        expr = col("score") > 0.5
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def _build_adjacency(node_id: RecordId, links: Iterator[tuple[RecordId, RecordId]]) -> CCNode:
    all_links = list(links)
    return CCNode(
        node_id=all_links[0][0],
        adjacency_list=list(set([link[1]["record_id_norm"] for link in all_links])),
        # init with own id as component
        component_id=node_id,
        changed=True,
    )
# ---
def is_in_jit():
    return isinstance(jnp.zeros((), dtype=jnp.float32), jax.core.Tracer)
# ---
def clone_pages_from(self, src, dest) -> "DecodeState":
        """
        Clone kv_pages from src slot to dest slot.
        """
        sequences, page_table = self.sequences.clone_pages_from(self.page_table, src, dest)
        return dataclasses.replace(self, sequences=sequences, page_table=page_table)
# ---
def _setup_regular(self, env):
        super(Selection, self)._setup_regular(env)
        assert self.selection is not None, "Field %s without selection" % self
# ---
def testGetFormatStringAttributeNames(self):
    """Tests the GetFormatStringAttributeNames function."""
    event_formatter = chrome_cache.ChromeCacheEntryEventFormatter()

    expected_attribute_names = [u'original_url']

    self._TestGetFormatStringAttributeNames(
        event_formatter, expected_attribute_names)
# ---
def test_hf_gpt2_roundtrip():
    _roundtrip_compare_gpt2_checkpoint("gpt2", None)
# ---
def drawSquare(turtle, x, y, length):
    turtle.up()
    turtle.move(x, y)
    turtle.setDirection(270)
    turtle.down()
    for count in xrange(4):
        turtle.move(length)
        turtle.turn(90)
# ---
def EvalBatch(self):
        """Get the evaluation batch axis from the trainer configuration."""
        return self.trainer.EvalBatch
# ---
def resolution(self) -> tuple[Lat, Lon]:
        return (
            torch.from_numpy(self.data.lat.values),
            torch.from_numpy(self.data.lon.values),
        )
# ---
def __create_action(self, main_root):
		action_dir = os.path.join(main_root, 'action')
		safe_mkdir(action_dir)

		actions = set()
		for unit, param in self.__task.data['workflow'].items():
			actions.add(param['type'])
		self.__action_mgr.create_actions(actions, action_dir)
# ---
def __call__(self, x: NamedArray, *, key=None):
        k1, k2 = haliax.jax_utils.maybe_rng_split(key, 2)
        x = self.fc1(x, key=k1)
        x = self.act(x)
        x = self.fc2(x, key=k2)
        return x
# ---
def __init__(self, token_z, r_max=32, s_max=2):
        super().__init__()
        self.r_max = r_max
        self.s_max = s_max
        self.linear_layer = LinearNoBias(4 * (r_max + 1) + 2 * (s_max + 1) + 1, token_z)
# ---
def create_test_rollout_stats(episode_reward: float, lesson_id: str = "test") -> RolloutStats:
    """Helper to create rollout stats for testing."""
    return RolloutStats(
        lesson_id=lesson_id, episode_reward=episode_reward, env_example_id="test_example", temperature=1.0, top_k=8
    )
# ---
def test_raw_metric_host_cpu(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'cpu_usagemhz_rate_average',
        host_name)

    for record in query:
        if record.cpu_usagemhz_rate_average is not None:
            assert record.cpu_usagemhz_rate_average > 0, 'Zero Host CPU Usage'
            break
# ---
def __init__(
        self,
        service: ControllerServiceImpl,
        host: str = "0.0.0.0",
        port: int = 8080,
    ):
        self._service = service
        self._host = host
        self._port = port
        self._app = self._create_app()
        self._server: uvicorn.Server | None = None
# ---
def to_mutation(self, source: str) -> Mutation:
        """Convert to a Mutation that can be applied to the source."""
        return Mutation(
            start=self.source_start,
            end=self.source_end,
            replacement=self.target_fragment,
            node_type=self.node_type,
            original=source[self.source_start : self.source_end],
        )
# ---
def key_function(out_key):
            out_coords = out_key[1:]
            in_coords = tuple(
                get_dim_index(ia, bi) for ia, bi in zip(idx.args, out_coords)
            )
            return ((self.array.name, *in_coords),)
# ---
def _root_key(self):
        return "LicenseInfo"
# ---
def create_futures_func(input, **kwargs):
            return [
                (i, asyncio.wrap_future(run_remotely.remote(i, **kwargs).future()))
                for i in input
            ]
# ---
def init_instance(cls, *args, **kwargs) -> Self:
        if Multiton._current_scope.get(cls) is not None:
            raise ValueError(f"{cls} already initialized")

        instance = super().__new__(cls)
        instance._initialize(*args, **kwargs)
        Multiton._current_scope[cls] = instance

        return instance
# ---
def test_nameservers(self):
        eq_(self.record.nameservers.__class__.__name__, 'list')
        eq_(self.record.nameservers, [])
# ---
def __repr__(self) -> str:
        return f"lit({self.value!r})"
# ---
def _parse_level(x) -> float:
        return float(x.split("_lev_")[1].replace("_", "."))
# ---
import re
def text_match_two_three(text):
        patterns = 'ab{2,3}'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def mark_dispatched(self) -> None:
        """Mark job as running. Called when first task starts."""
        self.state = cluster_pb2.JOB_STATE_RUNNING
        self.started_at = Timestamp.now()
# ---
def activations(
    params: GrugModelParameters,
    token_ids: Int[Array, "B S"],
    cfg: GrugModelConfig,
    *,
    mask: AttentionMask | jax.Array | None = None,
) -> Float[Array, "B S D"]:
    """Return final hidden states with shape (batch, seq, hidden_dim)."""
    return _transformer_hidden(params, token_ids, cfg, mask=mask)
# ---
def forwards(self, orm):
        # Adding field 'UserProject.drive_auth'
        db.add_column(u'user_project', 'drive_auth',
                      self.gf('django.db.models.fields.BooleanField')(default=False),
                      keep_default=False)
# ---
def get_message(self):
        xpath = '//div[@class="bienvenueMdp"]/following-sibling::div'
        return '%s%s' % (CleanText(xpath + '/strong')(self.doc), CleanText(xpath, children=False)(self.doc))
# ---
def __init__(
        self,
        config: WeightTransferConfig,
        axis_mapping: ResourceMapping | None = None,
        mesh: Mesh | None = None,
    ):
        self.config = config
        self.checkpoint_queue = deque()
        self.axis_mapping = axis_mapping
        self.mesh = mesh
        self.metrics = WeightTransferServerMetrics()
# ---
def _change_state(self, state):
        """ Helper method for changing state """
        self.table_entry.state = state
        self.creator_admin.save_model(self.request, self.table_entry, None, True)
# ---
def get_layer(self, index: int) -> M:
        """Return the ``index``th layer of the folded module."""

        ...
# ---
def setup_only(context):
        """A fixture with no `teardown()`."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        return setup
# ---
def update_item_price(self):
		frappe.db.sql("""update `tabItem Price` set item_name=%s,
			item_description=%s, brand=%s where item_code=%s""",
					(self.item_name, self.description, self.brand, self.name))
# ---
def template_cv(minstitch, maxstitch):
    return "minstitch: %d, maxstitch: %d" % (minstitch, maxstitch), ["--min_overlap", str(minstitch), "--max_overlap", str(maxstitch)]
# ---
def test_hashing_throughput(benchmark: Any, sample_batch: pa.RecordBatch, func: Callable, mode: str) -> None:
    # Use the sample_batch fixture (10k rows) and convert to bytes
    text_samples = [t.as_py().encode("utf-8") for t in sample_batch["text"]]

    def _run() -> list[Any]:
        if mode == "batch":
            return func(text_samples)
        return [func(x) for x in text_samples]

    benchmark(_run)
# ---
def testAssertNoMsg(self):
    self.assertEqual((0, 'AssertionError()\n'), _GrumpRun(textwrap.dedent("""\
        try:
          assert False
        except AssertionError as e:
          print repr(e)""")))
# ---
def add_default_uom_in_conversion_factor_table(self):
		uom_conv_list = [d.uom for d in self.get("uoms")]
		if self.stock_uom not in uom_conv_list:
			ch = self.append('uoms', {})
			ch.uom = self.stock_uom
			ch.conversion_factor = 1

		to_remove = []
		for d in self.get("uoms"):
			if d.conversion_factor == 1 and d.uom != self.stock_uom:
				to_remove.append(d)

		[self.remove(d) for d in to_remove]
# ---
def delete_all(self):
        '''
        Delete all keys
        '''
        for status, keys in six.iteritems(self.list_keys()):
            for key in keys:
                try:
                    os.remove(os.path.join(self.opts['pki_dir'], status, key))
                except (OSError, IOError):
                    pass
        self.check_minion_cache()
        return self.list_keys()
# ---
def compute_log_probs(model, example):
            model = trainer.mp.cast_to_compute(model)
            logprobs = model.compute_loss(example, key=None, reduction=None)
            # roll forward to get the loss for each predicted token
            logprobs = hax.roll(logprobs, 1, Pos)
            return logprobs.rearrange((EvalBatch, Pos)).array
# ---
def _can_swap_date_fields(self, first, second): # 'day', 'month', 'year'
        pane = self.selected_pane
        if pane is None:
            return False
        return pane.can_swap_date_fields(first, second)
# ---
import re 
def replace(string, char): 
    pattern = char + '{2,}'
    string = re.sub(pattern, char, string) 
    return string
# ---
def getText(self, callingWindow, itmContext, mainItem):
        return "Set {} as Damage Pattern".format(itmContext if itmContext is not None else "Item")
# ---
def poke(self, context):
        dag = context['dag']
        target_dttm = dag.following_schedule(context['execution_date'])
        target_dttm += self.delta
        logging.info('Checking if the time ({0}) has come'.format(target_dttm))
        return datetime.now() > target_dttm
# ---
def test_local_llm_inference():
    config = ModelConfig(
        name="test-llama-200m",
        path="gs://marin-us-east5/gcsfuse_mount/perplexity-models/llama-200m",
        engine_kwargs={"enforce_eager": True, "max_model_len": 1024},
        generation_params={"max_tokens": 16},
    )
    model_name_or_path, config = resolve_model_name_or_path(config)
    run_vllm_inference(model_name_or_path, **config.engine_kwargs)
# ---
def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    if args.command == "run":
        run_command(args)
    elif args.command == "configure":
        configure_command(args)
    elif args.command == "execute":
        execute_command(args)
    elif args.command == "download":
        download_command(args)
    elif args.command == "check":
        check_command(args)
    else:
        parser.print_help()
# ---
def parse(self, data):
        # reset list
        self.todo = Todo()
        return self.parser.parse(data)
# ---
def format_num_tokens(self) -> str:
        """Format total number of tokens in B/M/K notation."""
        tokens = self.total_tokens
        if tokens >= 1_000_000_000:
            return f"{tokens / 1_000_000_000:.1f}B"
        elif tokens >= 10_000_000:
            return f"{int(tokens / 1_000_000)}M"
        elif tokens >= 1_000_000:
            return f"{tokens / 1_000_000:.1f}M"
        elif tokens >= 1_000:
            return f"{tokens / 1_000:.1f}K"
        return str(tokens)
# ---
def sample_batch(parquet_file: str) -> pa.RecordBatch:
    """
    Loads a single batch (10k rows) for algorithm benchmarks (hashing, dedupe logic).
    Columns are restricted to ensure we have 'text' and 'id'.
    """
    pf = pq.ParquetFile(parquet_file)
    # Ensure we get necessary columns if they exist, though 'iter_batches' defaults to all.
    return next(pf.iter_batches(batch_size=10_000))
# ---
def resolve_all_mro(cls, name, reverse=False):
    """ Return the (successively overridden) values of attribute ``name`` in ``cls``
        in mro order, or inverse mro order if ``reverse`` is true.
    """
    klasses = reversed(cls.__mro__) if reverse else cls.__mro__
    for klass in klasses:
        if name in klass.__dict__:
            yield klass.__dict__[name]
# ---
def forward(self, dist):
        dist = (dist.unsqueeze(-1) - self.offset.view(1, 1, -1)).flatten(start_dim=1)
        return torch.exp(self.coeff * torch.pow(dist, 2))
# ---
def v(self, vol=""):
        html = ""
        if vol == "" or vol == None :
           html += "Error"
        v = volume(vol)

        html += "<h6>%s (%s) </h6>" % (v, vol)
        return html
# ---
def insert(
        self,
        input_: torch.Tensor,
        boundary: torch.Tensor,
        label: torch.Tensor,
    ):
        """Add a prognostic input, boundary, and prognostic label as the last step."""
        self.raw_data.append((input_, boundary, label))
# ---
def read_volts(self, channel):
        """Read the ADC value from channel and convert to volts, assuming that Vref is set correctly. """
        return self._Vref * self.read_adc(channel) / self.get_value_max()
# ---
def vmap_fun2(x):
        return x.sum(Width).array
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.stuff.insert(),
            [
                {"id": 1, "data": "some data"},
                {"id": 2, "data": "some data"},
                {"id": 3, "data": "some data"},
                {"id": 4, "data": "some other data"},
            ],
        )
# ---
def local_cluster():
    yield LocalCluster(LocalClusterConfig(use_isolated_env=False))
# ---
def __init__(self, token_z: int, num_bins: int) -> None:
        """Initialize the distogram module.

        Parameters
        ----------
        token_z : int
            The token pairwise embedding size.

        """
        super().__init__()
        self.distogram = nn.Linear(token_z, num_bins)
        self.num_bins = num_bins
# ---
def default_choice_name(cls) -> Optional[str]:
        return "rms"
# ---
def parse_redesign_yaml(
    path: Path,
    tokenized: Tokenized,
) -> Target:
    """parse a design mask override yaml file"""
    with path.open("r") as file:
        if path.suffix == ".yaml":
            data = yaml.safe_load(file)
        else:
            raise ValueError(f"Unsupported file type: {str(path)}")
    target = parse_redesign_schema(data, tokenized)

    return target
# ---
def load_rollout_batch(pickle_file: str) -> RolloutBatch:
    """Load rollout batch from pickle file."""
    with open(pickle_file, "rb") as f:
        batch = pickle.load(f)
    return batch
# ---
def get_pell(n): 
	if (n <= 2): 
		return n 
	a = 1
	b = 2
	for i in range(3, n+1): 
		c = 2 * b + a 
		a = b 
		b = c 
	return b
# ---
def register_options_page(page_class):
    _pages.register(page_class.__module__, page_class)
# ---
def __init__(self, region, name, retention_in_days=7):
    super(LogGroup, self).__init__()
    self.region = region
    self.name = name
    self.retention_in_days = retention_in_days
# ---
def show(self):
        super().show()
        self.inputLine.setFocus()
# ---


def triangle_area(a, h):
    """Given length of a side and high return area for a triangle.
    >>> triangle_area(5, 3)
    7.5
    """
    return a * h / 2.0
# ---
def define_tables(cls, metadata):
        Table(
            "square",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("side", Integer),
            Column("area", Integer, Computed("side * side")),
            Column("perimeter", Integer, Computed("4 * side")),
        )
# ---
def _from_db_object(cls, obj, db_obj):
        """Converts a physical function to a formal object.

        :param obj: An object of the class.
        :param db_obj: A DB model of the object
        :return: The object of the class with the database entity added
        """
        obj = Deployable._from_db_object(obj, db_obj)
        if cls is PhysicalFunction:
            obj.virtual_function_list = []
        return obj
# ---
def optimize(
        self,
        optimize_function: Optional[Callable[..., nx.MultiDiGraph]] = None,
        array_names: Optional[Tuple[str]] = None,
    ):
        if optimize_function is None:
            optimize_function = multiple_inputs_optimize_dag
        dag = optimize_function(self.dag, array_names=array_names)
        return Plan(dag)
# ---
def test_array_memory():
    assert array_memory(np.int64, (3,)) == 24
    assert array_memory(np.int32, (3,)) == 12
    assert array_memory(np.int32, (3, 5)) == 60
    assert array_memory(np.int32, (0,)) == 0
# ---
def write_batch(self, batch: RolloutBatch) -> None:
        """Write a batch.

        Args:
            batch: RolloutBatch to write.
        """
        pass
# ---
def register(self, request: cluster__pb2.Controller.RegisterRequest, ctx: RequestContext) -> cluster__pb2.Controller.RegisterResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def calculate_single_stage_io_ops(
    shape: Sequence[int], in_chunks: Sequence[int], out_chunks: Sequence[int]
) -> int:
    """Count the number of read/write operations required for rechunking."""
    return prod(map(_count_intermediate_chunks, in_chunks, out_chunks, shape))
# ---
def process_bind_param(self, value, dialect):
                raise Exception("nope")
# ---
def address(self) -> str:
        """Target address (IP, hostname, or VM identifier)."""
        ...
# ---
def on_predict_start(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,
    ) -> None:
        """Use EMA weights for prediction.

        Parameters
        ----------
        trainer: Trainer
            The Trainer instance.
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._on_eval_start(pl_module)
# ---
def action_from_output(cls, output_array):
        return np.argmax(output_array)
# ---
def __init__(
        self,
        config: config_pb2.IrisClusterConfig,
        threads: ThreadContainer | None = None,
    ):
        self._config = config
        self._threads = threads if threads is not None else get_thread_container()
        self._controller: ControllerProtocol | None = None
# ---
def __add__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__add__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.add, self, other, dtype=result_type(self, other))
# ---
def generate_type(type_id, is_public):
    return {
        'id': type_id,
        'name': u'test',
        'deleted': False,
        'created_at': datetime.datetime(2012, 1, 1, 1, 1, 1, 1),
        'updated_at': None,
        'deleted_at': None,
        'is_public': bool(is_public)
    }
# ---
def test_pspec_for_plain_array_uses_typeof_sharding():
    # In explicit sharding mode, jax.typeof carries sharding info for plain arrays.
    devices = jax.devices()
    mesh = Mesh(np.array(devices), (ResourceAxis.DATA,), axis_types=(AxisType.Explicit,))
    sharding = NamedSharding(mesh, PartitionSpec(ResourceAxis.DATA, None))
    array = jax.device_put(jnp.ones((len(devices), 2)), sharding)

    spec = pspec_for(array, resource_mapping={})
    assert spec == PartitionSpec(ResourceAxis.DATA, None)
# ---
def _get_job(job_id):
    with _lock:
        if job_id not in _jobs:
            raise NoSuchJob("No such job %r" % job_id)
        return _jobs[job_id]
# ---
def is_jax_array_like(x):
    return hasattr(x, "shape") and hasattr(x, "dtype")
# ---
def init(self, run_id: Optional[str]) -> Tracker:
        return NoopTracker()
# ---
def test_capture_binary_output(testdir):
    testdir.makepyfile(r"""
        import pytest

        def test_a():
            import sys
            import subprocess
            subprocess.call([sys.executable, __file__])

        def test_foo():
            import os;os.write(1, b'\xc3')

        if __name__ == '__main__':
            test_foo()
        """)
    result = testdir.runpytest('--assert=plain')
    result.assert_outcomes(passed=2)
# ---
def to(self, device: torch.device) -> Self:
        """Move the label mask to the specified device."""
        return dataclasses.replace(
            self, label_mask=self.label_mask.to(device, non_blocking=True)
        )
# ---
def sos_token_id(self) -> int:
        return 1
# ---
def convert_to_read(self, value, use_name_get=True):
        """ convert ``value`` from the cache to a value as returned by method
            :meth:`BaseModel.read`

            :param bool use_name_get: when True, value's diplay name will
                be computed using :meth:`BaseModel.name_get`, if relevant
                for the field
        """
        return False if value is None else value
# ---
def remove_all_iris_containers(self) -> int:
        count = len(self._containers)
        self._containers.clear()
        return count
# ---
def lr_scale(self):
        """Return the learning-rate scaling factor."""
        raise NotImplementedError
# ---
def _pipeline() -> int:
        batches = pq.ParquetFile(small_parquet_path).iter_batches(batch_size=batch_size)
        return sum(len(dupekit.process_arrow_batch(b)) for b in batches)
# ---
def dec(*args, **kwargs):
            try:
                return self.view(*args, **kwargs)
            except PermissionRequired as e:
                kwargs['_perm'] = e.perm
                kwargs['_view'] = self.view
                return self.error_view(*args, **kwargs)
# ---
def replace(self, idx: int, value: PageCacheT) -> "ListCache[PageCacheT]":
        caches = list(self.caches)
        caches[idx] = value
        return ListCache(tuple(caches))
# ---
def find_one_raw(self, resource, _id):
        args = self._es_args(resource)
        hit = self.es.get(id=_id, **args)
        return self._parse_hits({'hits': {'hits': [hit]}}, resource).first()
# ---
def certificado(self):
        return self.__certificado
# ---
def batchify(batch: Iterable, n: int = 1024) -> Iterable:
    iterator = iter(batch)
    while batch := tuple(itertools.islice(iterator, n)):
        yield batch
# ---
def use_docker(request):
    return request.config.getoption("--use-docker")
# ---
def factorial(start,end): 
    res = 1 
    for i in range(start,end + 1): 
        res *= i      
    return res 
def sum_of_square(n): 
   return int(factorial(n + 1, 2 * n)  /factorial(1, n))
# ---
def _compiledpats(self):
        pat, matchfunc = _buildregexmatch(self._kindpats, "")
        return matchfunc
# ---
def create_reader(self) -> "RolloutReader":
        if self.storage_type == StorageType.FILE:
            if self.path is None:
                raise ValueError("path must be specified for FILE storage type")
            return FileRolloutReader(self.path)
        else:
            if self.queue_name is None:
                raise ValueError("queue_name must be specified for IN_MEMORY storage type")
            return _get_or_create_queue(self.queue_name, self.queue_maxlen).reader()
# ---
def __new__(cls):
        # Prevent direct instantiation
        raise TypeError(
            f"{cls} cannot be instantiated directly. Use init_instance() instead."
        )
# ---
def close(self):
        return
# ---
def average_Odd(n) : 
    if (n%2==0) : 
        return ("Invalid Input") 
        return -1 
    sm =0
    count =0
    while (n>=1) : 
        count=count+1
        sm = sm + n 
        n = n-2
    return sm//count
# ---
def test_less_than(self):
        """Test that cmp_version compares a as less than b"""
        self.assertTrue(vmops.cmp_version('1.2.3.4', '1.2.3.5') < 0)
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()
# ---
def forward(
    params: GrugModelParameters,
    token_ids: Int[Array, "B S"],
    cfg: GrugModelConfig,
    *,
    mask: AttentionMask | jax.Array | None = None,
) -> Float[Array, "B S V"]:
    hidden = _transformer_hidden(params, token_ids, cfg, mask=mask)
    logits = jnp.einsum("bsh,hd->bsd", hidden, params.output_proj, out_sharding=Pbatch)
    return logits
# ---
def maximum_value(test_list):
  res = [(key, max(lst)) for key, lst in test_list]
  return (res)
# ---
def test_apply_gufunc_axes_two_kept_coredims(spec):
    ra = np.random.normal(size=(20, 30))
    rb = np.random.normal(size=(10, 1, 40))

    a = cubed.from_array(ra, chunks=(10, 30), spec=spec)
    b = cubed.from_array(rb, chunks=(5, 1, 40), spec=spec)

    def outer_product(x, y):
        return np.einsum("i,j->ij", x, y)

    c = apply_gufunc(outer_product, "(i),(j)->(i,j)", a, b, vectorize=True)
    assert c.compute().shape == (10, 20, 30, 40)
# ---
def ascii_value_string(str1):
  for i in range(len(str1)):
   return ord(str1[i])
# ---
def visit_Name(self, node: ast.Name) -> ast.Name:
        if node.id in self.mapping:
            return ast.Name(id=self.mapping[node.id], ctx=node.ctx)
        return node
# ---
def _mean_groupby_aggregate(a, **kwargs):
    return nxp.divide(a["total"], a["n"])
# ---
def test_map_nested_lists():
    inc = lambda x: x + 1

    assert map_nested(inc, [1, 2]) == [2, 3]
    assert map_nested(inc, [[1, 2]]) == [[2, 3]]
    assert map_nested(inc, [[1, 2], [3, 4]]) == [[2, 3], [4, 5]]
# ---
def apartment_id_exists(apartment_id):
        # type: (int) -> bool
        apartments = ApartmentController.load_apartments()
        ids = (x.id for x in apartments)
        return apartment_id in ids
# ---
def __init__(self, client, config, serializer, deserializer):
        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer
        self._config = config
# ---
def create_parquet_file(data: list[dict], filepath: str):
    """Helper function to create a Parquet file"""
    import pandas as pd

    df = pd.DataFrame(data)
    df.to_parquet(filepath, index=False)
# ---
def test_slice_nd_shorthand_syntax():
    # syntax like arr["X", 0:10, "Y", 0:10] is supported

    H, W, D = hax.make_axes(H=10, W=20, D=30)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    assert jnp.all(jnp.equal(named1["H", 0:10, "D", 0:10].array, named1.array[0:10, :, 0:10]))
# ---
def transform_linear_layer(layer: haliax.nn.Linear):
            assert layer.weight.ndim == 2
            # steps is now a concrete int
            array = layer.weight.array
            updated_weight_array = zeropower_via_newtonschulz5(
                array, steps=steps, eps=muon_eps, coefficient_type=coefficient_type
            )

            updated_weight = dataclasses.replace(layer.weight, array=updated_weight_array)

            return dataclasses.replace(layer, weight=updated_weight)
# ---
def _assert_fn(self, x, value=None):
        eq_(
            testing.db.execute(self.table.select()).fetchall(),
            [(x, value)]
        )
# ---
def __len__(self):
        return len(self._index_to_obj)
# ---
def replacer(match):
        old_link = match.group(0)
        path = match.group(1)
        new_link = f"marin.community/data-browser/{path}"
        replacements.append((old_link, new_link))
        return new_link
# ---
def find_list_of_ids(self, resource, ids, client_projection=None):
        args = self._es_args(resource)
        return self._parse_hits(self.es.multi_get(ids, **args), resource)
# ---
def get_project_id() -> str | None:
    """Get the current GCP project ID."""
    try:
        result = run_gcloud_command(["gcloud", "config", "get-value", "project"])
        return result.stdout.strip() or None
    except RuntimeError:
        return None
# ---
def capture(self, tensor, name=None):
    """Adds the given tensor to this graph and returns the captured tensor."""
    if tensor in self._captured:
      # Captured already.
      return self._captured[tensor]
    elif self._capture_by_value:
      return self._add_tensor_and_parents(tensor)
    else:
      return self._capture_tensor_as_extra_input(tensor, name)
# ---
def test_list_instances_0(self):
        instances = self.conn.list_instances()
        self.assertEquals(instances, [])
# ---
def test_non_lexical(self):
        """Test that cmp_version compares non-lexically"""
        self.assertTrue(vmops.cmp_version('1.2.3.10', '1.2.3.4') > 0)
# ---
def Split(list): 
    ev_li = [] 
    for i in list: 
        if (i % 2 == 0): 
            ev_li.append(i)  
    return ev_li
# ---
def _draw_more_comments(self, win, data):

        n_rows, n_cols = win.getmaxyx()
        n_cols -= 1

        self.term.add_line(win, '{body}'.format(**data), 0, 1)
        self.term.add_line(
            win, ' [{count}]'.format(**data), attr=curses.A_BOLD)

        attr = Color.get_level(data['level'])
        self.term.addch(win, 0, 0, self.term.vline, attr)

        return attr | self.term.vline
# ---
def merge(self):
        weight = self.lora.merge() + self.wrapped.weight
        return dataclasses.replace(self.wrapped, weight=weight)
# ---
def _delete_tpu(name: str, zone: str, project: str) -> bool:
    result = _run_gcloud(
        ["compute", "tpus", "tpu-vm", "delete", name, f"--project={project}", f"--zone={zone}", "--quiet"]
    )
    if result.returncode != 0:
        error = result.stderr.strip()
        if "not found" in error.lower():
            click.echo(f"  TPU {name} already deleted")
            return True
        click.echo(f"  Failed to delete TPU {name}: {error}", err=True)
        return False
    return True
# ---
def are_shape_checks_enabled():
    return _ENABLE_SHAPE_CHECKS
# ---
def test__data_is_not_zeros(train_config):
    with make_loader(train_config) as loader:
        for sample in loader:
            X, y = extract_sample_arrays(sample)
            assert np.count_nonzero(np.zeros(X.shape)) == 0, (
                "Sanity check: Zero is zero."
            )
            assert np.count_nonzero(X) != 0, "Input data should not be a zeros matrix!"
            assert np.count_nonzero(y) != 0, "Label data should not be a zeros matrix!"
# ---
def getName(self):
        return "NoncBeta({0},{1},{2})".format(self.alpha, self.beta, self.lmbda)
# ---
def test_get_host_unquote(self):
        req = Request("http://www.%70ython.org/")
        self.assertEqual("www.python.org", req.get_host())
# ---
def on_move(x, y):
            """The window was moved.

            :Parameters:
                `x` : int
                    Distance from the left edge of the screen to the left edge
                    of the window.
                `y` : int
                    Distance from the top edge of the screen to the top edge of
                    the window.  Note that this is one of few methods in pyglet
                    which use a Y-down coordinate system.

            :event:
            """
# ---
def _recover_preserve_chars(text: str, replacements: str) -> str:
    replacement_iterator = iter(replacements)
    return "".join(
        next(replacement_iterator) if c == PRESERVE_CHAR else c for c in text
    )
# ---
def _apply_failing_clock_call(name, errno_value):
        calls = []

        def _failing_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            monkeypatch.setattr(_api.ffi, "errno", errno.EINVAL)
            return -1

        monkeypatch.setattr(_api, name, _failing_clock_call)

        return calls
# ---
def __len__(self):
        return len(iter(self))
# ---
def _check_auth(self):

        if not self.username or not self.password or not self.pin:
            logger.warning("Invalid username or password or pin. Check your settings")

        return True
# ---
def get_logs(self, label: str) -> Metrics: ...
# ---
def __init__(self,domain='rds.aliyuncs.com',port=80):
		RestApi.__init__(self,domain, port)
		self.AccountName = None
		self.DBInstanceId = None
		self.resourceOwnerAccount = None
# ---
def max_gen_toks(self) -> int:
        """Backward compatibility property for max_gen_toks."""
        return self.generation_kwargs.get("max_gen_toks", 256)
# ---
def _validate_job_finished(job):
    if job.status not in (STATUS.DONE, STATUS.FAILED, STATUS.ABORTED):
        raise JobNotDone("Job %r is %s" % (job.id, job.status))
# ---
def get_task_logs(self, request: cluster__pb2.Controller.GetTaskLogsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetTaskLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def test_empty_set_against_integer_negation(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.notin_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [(1,), (2,), (3,), (4,)], params={"q": []})
# ---
def test_index_slice_unsupported_step(spec):
    a = xp.arange(12, chunks=(4,), spec=spec)
    with pytest.raises(NotImplementedError):
        a[::-1]
# ---
def resolve_model_name_or_path(model: ModelConfig) -> tuple[str, ModelConfig]:
    """Resolve the `model` argument to pass to vLLM."""
    model = _maybe_enable_streaming(model)
    model_name_or_path = model.path if model.path is not None else model.name
    return model_name_or_path, model
# ---
def checkDiagonals(self, player, inARow):
        """
        Calls two diagonal functional.
        :return: an int, representing the column where to play or 0 and False if there is no pattern search.
        """
        check = self.checkDiagonalLeftToRight(player, inARow)
        if check[0]:
            return check
        else:
            return self.checkDiagonalRightToLeft(player, inARow)
# ---
def _discover_controller(zone: str, project: str) -> str | None:
    """Find the controller VM in the given zone."""
    vm_names = _list_controller_vms(zone, project)
    if not vm_names:
        return None
    if len(vm_names) > 1:
        click.echo(f"Warning: Multiple controller VMs found: {vm_names}", err=True)
    return vm_names[0]
# ---
def test_digraph(self):
        G = nx.path_graph(3, create_using=nx.DiGraph())
        c = nx.closeness_centrality(G)
        cr = nx.closeness_centrality(G.reverse())
        d = {0: 0.0, 1: 0.500, 2: 0.667}
        dr = {0: 0.667, 1: 0.500, 2: 0.0}
        for n in sorted(self.P3):
            assert almost_equal(c[n], d[n], places=3)
            assert almost_equal(cr[n], dr[n], places=3)
# ---
def Sum_of_Inverse_Divisors(N,Sum): 
    ans = float(Sum)*1.0 /float(N);  
    return round(ans,2);
# ---
def __init__(
        self,
        shape: T_Shape,
        dtype: T_DType,
        chunks: T_RegularChunks,
        fill_value: Any = None,
    ):
        super().__init__(shape, dtype, chunks)
        self.fill_value = fill_value
# ---
def __init__(self, difficulty: str = "medium"):
        pass
# ---
def post_init(self):
        """
        A method executed at the end of each Transformer model initialization, to execute code that needs the model's
        modules properly initialized (such as weight initialization).
        """
        self.init_weights()
        self._backward_compatibility_gradient_checkpointing()
# ---
def test_transaction_tlocal_engine_ctx_commit(self):
        fn = self._trans_fn()
        engine = engines.testing_engine(options=dict(
                                strategy='threadlocal',
                                pool=testing.db.pool))
        ctx = engine.begin()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def unembed_active_scale(self):
        return 1
# ---
def mask_fn(param, path):
            path_str = ".".join(path) if isinstance(path, (list, tuple)) else str(path)
            if "Embedding" in path_str:
                return "adam"
            elif "lm_head" in path_str:
                return "adamh"
            elif isinstance(param, Linear):
                # muonh for linear layers
                return dataclasses.replace(param, weight="muonh", bias="adam" if param.bias is not None else None)
            else:
                return "adam"
# ---
def __init__(self, datasets: list[InferenceDataset], lengths: list[int]):
        self.datasets = datasets
        self.lengths = lengths
# ---
def year(self):
        return self.__year
# ---
def result(self) -> "TreeCache":
        if not self._is_closed:
            raise RuntimeError("Cannot get result until TreeCacheWriter is closed")
        return TreeCache.load(self.cache_dir, self._exemplar, self.metadata)
# ---
def __eq__(self, other):
        return self._dict == other._dict
# ---
def test_from_array(x, chunks, asarray):
    a = cubed.from_array(WrappedArray(x), chunks=chunks, asarray=asarray)
    assert isinstance(a, cubed.Array)
    assert_array_equal(a, x)
# ---
def _cached_filter_eval_shape(fun, *args, **kwargs):
    """
    eval_shape is surprisingly expensive, so we cache it. We use this for named_pjit for evaluating resource partitions
    of the output.
    """
    dynamic, static = hashable_partition((fun, args, kwargs), is_array)
    if static not in _eval_shape_cache:
        _eval_shape_cache[static] = eqx.filter_eval_shape(fun, *args, **kwargs)

    return _eval_shape_cache[static]
# ---
def answer(L,R): 
    if (2 * L <= R): 
        return (L ,2*L)
    else: 
        return (-1)
# ---
def Check_Vow(string, vowels): 
    final = [each for each in string if each in vowels] 
    return(len(final))
# ---
def increasing_trend(nums):
    if (sorted(nums)== nums):
        return True
    else:
        return False
# ---
def test_causal_mask_slicing():
    pos = hax.Axis("pos", 128)
    key_pos = pos.alias("key_pos")

    mask = AttentionMask.causal()

    mat_mask = mask.materialize(pos, key_pos)
    mat_sliced = mask.materialize(pos, key_pos, q_slice=hax.dslice(7, 16), k_slice=hax.dslice(24, 16))

    for i in range(16):
        for j in range(16):
            assert mat_sliced.array[i, j] == mat_mask.array[7 + i, 24 + j]
# ---
def __init__(self, array: NamedArray, slices: SliceSpec):
        self._array = array
        self._slices = slices
# ---
def should_update(self):
        result = self._update_changed_options()
        if result:
            return True
        return False
# ---
def resolve_axis(self, axis: AxisSelector) -> Axis: ...
# ---
def add_nested_tuples(test_tup1, test_tup2):
  res = tuple(tuple(a + b for a, b in zip(tup1, tup2))
   for tup1, tup2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def fold_via(
        self, fn: Callable[..., CarryT], *, unroll: int | bool | None = None
    ) -> Callable[Concatenate[CarryT, P], CarryT]: ...
# ---
def key_dim(self) -> int:
        return self.num_k_heads * self.head_k_dim
# ---
def post_with_null_comment():
    return make_post(with_comments=False)
# ---
import re
def is_decimal(num):
  num_fetch = re.compile(r"""^[0-9]+(\.[0-9]{1,2})?$""")
  result = num_fetch.search(num)
  return bool(result)
# ---
def copy_process_status(self, process):
        return {
            'id': process.vpnservice['id'],
            'status': process.status,
            'updated_pending_status': process.updated_pending_status,
            'ipsec_site_connections': copy.deepcopy(process.connection_status)
        }
# ---
def __init__(self, address: str = "auto", namespace: str | None = None):
        self._address = os.environ.get("RAY_ADDRESS", "auto") if address == "auto" else address
        if namespace is None:
            self._namespace = f"fray_{uuid.uuid4().hex[:8]}"
        else:
            self._namespace = namespace
        self._dashboard_address = self._get_dashboard_address()
        logger.info("RayClient connected to %s (namespace=%s)", self._address, self._namespace)
# ---
def square_perimeter(a):
  perimeter=4*a
  return perimeter
# ---
def to_hf_config(self) -> tuple[float, dict]:
        return self.theta, {
            "rope_type": "yarn",
            "factor": self.factor,
            "beta_fast": self.beta_fast,
            "beta_slow": self.beta_slow,
            "original_max_position_embeddings": self.original_max_position_embeddings,
            "mscale": self.mscale,
        }
# ---
def clip(self, a_min=None, a_max=None) -> Any:  # pragma: no cover
        return haliax.clip(self, a_min=a_min, a_max=a_max)
# ---
def logsumexp(a: A, axis: AxisSelection | None = None) -> A:
    # TODO: logsumexp indirectly supports where via `b`. we should support it directly
    return wrap_reduction_call(jnn.logsumexp, a, axis=axis, single_axis_only=False, supports_where=False)
# ---
def test_endswith_unescaped(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e%fg"), {1, 2, 3, 4, 5, 6, 7, 8, 9})
# ---
def get_runtime_env(self) -> dict:
        """
        Returns the runtime environment to run the evaluator on the Ray cluster.
        """
        return build_runtime_env_for_packages(
            extra=["eval", "tpu"],
            pip_packages=["statsmodels==0.14.4"],
            env_vars={
                "TOKENIZERS_PARALLELISM": "false",
                "HF_DATASETS_TRUST_REMOTE_CODE": "1",
                "HF_ALLOW_CODE_EVAL": "1",
            },
        )
# ---
def show_recordsettings(self):
        dlg = CSVSettingsDialog(self)
        dlg.exec_()
# ---
def test_score_candidate_syntax_error():
    candidate = _make_candidate("def f(:\n")
    tests = ["assert f(1) == 1"]
    result = score_candidate(candidate, tests)
    assert result.tests_passed == 0
# ---
def silu(a: A) -> A:
    return wrap_elemwise_unary(jnn.silu, a)
# ---
def custom_validate_repo_id(repo_id):
            if _is_url_like(repo_id):
                return
            return original_validate_repo_id(repo_id)
# ---
def get_environment():  # type: ignore
        raise ImportError(msg) from exception
# ---
def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
      return self.mixer.allocate_inference_cache(
        batch_size, max_seqlen, dtype=dtype, **kwargs)
# ---
def get_dates(schema):
    """Return list of datetime fields for given schema."""
    dates = [config.LAST_UPDATED, config.DATE_CREATED]
    for field, field_schema in schema.items():
        if field_schema['type'] == 'datetime':
            dates.append(field)
    return dates
# ---
def arccos(a: A) -> A:
    return wrap_elemwise_unary(jnp.arccos, a)
# ---
def _normalize_fts_prognostic(self, fts: Prognostic) -> Prognostic:
        fts = self.normalize.normalize_tensor_prognostic(fts)
        return fts.to(torch.float32)
# ---
def test_nanmean(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, xp.nan]], chunks=(2, 2), spec=spec)
    b = cubed.nanmean(a)
    assert_array_equal(
        b.compute(), np.nanmean(np.array([[1, 2, 3], [4, 5, 6], [7, 8, np.nan]]))
    )
# ---
def free_pages(self, seq_id: int) -> "DecodeState":
        sequences, page_table = self.sequences.free_pages(self.page_table, seq_id)
        return dataclasses.replace(self, sequences=sequences, page_table=page_table)
# ---
def cabinet_decision(last_id=-1):
    '''Retrive cabinet decisions
    if last_id not defiend it will return the max
    return list of cabinet decisions tuples up to MAX_PAGES_TO_SEARCH (page=10)
    [(id, title, url, text)...]
    '''
    decisions = []
    _news = retrieve_news(cabinet=1, last_id=last_id)
    for item in _news:
        _detail = retrieve_detail(item)
        decisions.append(_detail)
    return decisions
# ---
def model_type(self) -> type["GemmaLMHeadModel"]:
        return GemmaLMHeadModel
# ---
def max_occurrences(nums):
    max_val = 0
    result = nums[0] 
    for i in nums:
        occu = nums.count(i)
        if occu > max_val:
            max_val = occu
            result = i 
    return result
# ---
def wait(ctx, job_id):
    """Wait for job completion."""
    cluster = ctx.obj["cluster"]
    click.echo(f"Waiting for {job_id}...")
    info = cluster.wait(job_id)
    click.echo(f"Job finished: {info.status}")
    if info.error_message:
        click.echo(f"Error: {info.error_message}", err=True)
# ---
def get_verb(cls, verb):
        return cls.verbs.get(verb)
# ---
def _slow_job():
    import time as _time

    _time.sleep(120)
    return "done"
# ---
def ntimes_list(nums,n):
    result = map(lambda x:n*x, nums) 
    return list(result)
# ---
def name(self) -> str | None:
        return self._thread.name
# ---
def imputed_max_tokens_per_round(self) -> int:
        """Return explicit `max_tokens_per_round` or default to `max_seqs` when unset."""
        return self.max_tokens_per_round if self.max_tokens_per_round is not None else self.max_seqs
# ---
def copy(self, cr, uid, id, default=None, context=None):
        if default is None:
            default = {}
        default = default.copy()
        default.update({'move_ids': [], 'date_done': False})
        return super(stock_inventory, self).copy(cr, uid, id, default, context=context)
# ---
def _make_config(self, cfg: "AnyTopLevelConfig", data_container: "DataContainer"):
        config = {
            f"data_{i}/attrs": src.data.attrs
            for i, src in enumerate(data_container.sources)
        }
        config.update(config=cfg.model_dump())
        return config
# ---
def _mem_to_mib(size, unit):
    lunit = unit.lower()
    if lunit in ('bytes', 'b'):
        return size / 1024 / 1024
    elif lunit in ('kib', 'k'):
        return size / 1024
    elif lunit in ('mib', 'm'):
        return size
    elif lunit in ('gib', 'g'):
        return size * 1024
    elif lunit in ('tib', 't'):
        return size * 1024 * 1024
    else:
        raise InvalidVMConfiguration("Invalid currentMemory unit attribute:"
                                     " %r" % unit)
# ---
def axis_mapping(mapping: ResourceMapping, *, merge: bool = False, **kwargs):
    """Context manager for setting the global resource mapping"""
    mapping = dict(mapping)

    old_mapping = current_thread_local_mapping()
    if merge:
        mapping.update(old_mapping or {})

    if len(kwargs):
        mapping.update(kwargs)

    _mapping_holder.thread_data.resource_mapping = mapping
    try:
        yield
    finally:
        _mapping_holder.thread_data.resource_mapping = old_mapping
# ---
def No_of_cubes(N,K):
    No = 0
    No = (N - K + 1)
    No = pow(No, 3)
    return No
# ---
def log_image(self, name, path, model):
        if model.logger is not None:
            try:
                model.logger.log_image(name, images=[str(path)])
            except:
                import traceback

                traceback.print_exc()  # noqa: T201
                print(f"Image logging failed for {name} {str(path)}.")
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "!"
# ---
def min_slices(self) -> int:
        """Minimum number of VM groups to maintain."""
        return self._config.min_slices
# ---
def GetBufferChangedTick( bufnr ):
  return GetIntValue( 'getbufvar({0}, "changedtick")'.format( bufnr ) )
# ---
def test_must_be_absolute(self):
        """Test LocalLocation must be absolute."""
        with pytest.raises(ValidationError):
            LocalLocation(path=Path("relative/path"))
# ---
def test_shard_map_basic():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    def fn(x):
        return x + 1

    sm = hax.shard_map(fn, mesh=mesh, check_rep=False)
    x = hax.ones(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), mesh:
        out = sm(x)
    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array + 1)
# ---
def walkSpeed(self):
        return ToontownGlobals.GoofySpeed
# ---
def ensure_tuple(x: Sequence[T] | T) -> tuple[T, ...]:
    if isinstance(x, str):
        return (x,)  # type: ignore
    elif isinstance(x, Sequence):
        return tuple(x)
    return (x,)
# ---
def unflatten_axis(self, axis: AxisSelector, new_axes: AxisSpec) -> "NamedArray":  # pragma: no cover
        return haliax.unflatten_axis(self, axis=axis, new_axes=new_axes)
# ---
def _get_mistral_config(use_flash=False, num_kv_heads=4) -> MistralConfig:
    return MistralConfig(
        num_layers=2,
        max_seq_len=128,
        hidden_dim=16,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def test_permute_dims(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.permute_dims(a, (1, 0))
    assert_array_equal(
        b.compute(executor=executor),
        np.transpose(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])),
    )
# ---
def checkCollision(self, otherSprite):
        if (self.x < otherSprite.x + otherSprite.tw and otherSprite.x < self.x + self.tw
            and self.y < otherSprite.y + otherSprite.th and otherSprite.y < self.y + self.th):
            return True
        else:
            return False
# ---
def unstacked(self) -> Sequence[M]:
        return self.blocks
# ---
def log_once(self, msg: str):
        """Print *msg* exactly once per *training job* (only on rank-0 if DDP)."""
        try:
            import torch.distributed as _dist

            if _dist.is_available() and _dist.is_initialized():
                is_rank0 = _dist.get_rank() == 0
            else:
                is_rank0 = True
        except Exception:
            is_rank0 = True
        if is_rank0 and msg not in self._once_keys:
            print(msg)
            self._once_keys.add(msg)
# ---
def get_version(module, git_path, dest, ref="HEAD"):
    ''' samples the version of the git repo '''

    cmd = "%s rev-parse %s" % (git_path, ref)
    rc, stdout, stderr = module.run_command(cmd, cwd=dest)
    sha = to_native(stdout).rstrip('\n')
    return sha
# ---
def test_filter_expression_logical_or(backend):
    """Test filter with logical OR expression."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"a": 1, "b": 2},
            {"a": -1, "b": 3},
            {"a": 2, "b": -1},
            {"a": -1, "b": -1},
        ]
    ).filter((col("a") > 0) | (col("b") > 0))

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 3
# ---
def as_formatted_date(self) -> str:
        """Format as ISO 8601 string in UTC."""
        dt = datetime.fromtimestamp(self.epoch_seconds(), tz=timezone.utc)
        return dt.isoformat()
# ---
def test_build_resources_gpu_not_supported():
    """Test that GPU raises error."""
    with pytest.raises(ValueError, match="GPU support not yet implemented"):
        build_resources(tpu=None, gpu=2, cpu=None, memory=None)
# ---
def is_named_array(leaf):
    from .core import NamedArray

    "Typically used as the is_leaf predicate in tree_map"
    return isinstance(leaf, NamedArray)
# ---
def div_list(nums1,nums2):
  result = map(lambda x, y: x / y, nums1, nums2)
  return list(result)
# ---
def __init__(self, config: config_pb2.IrisClusterConfig):
        self.config = config
        self._manual_config = config.controller.manual
        self._bootstrapped = False

        if not self._manual_config.host:
            raise RuntimeError("controller.manual.host is required for ManualController")

        port = self._manual_config.port or DEFAULT_CONTROLLER_PORT
        self.address = f"http://{self._manual_config.host}:{port}"
# ---
def _is_gcs(self) -> bool:
        return self.path.startswith("gs://")
# ---
def get_dartmouth(b):
    A = 500
    B = 2500
    return b['b2'].add(A).divide(b['b1'].add(B)).select(['sur_refl_b02'], ['b1'])
# ---
def findPlugin(plugin_name): 
    """
    @type plugin_name: str
    """
    pass
# ---
def get(self, ref: Any) -> Any:
        """Get result, handling GeneratorFuture, Future objects and plain values."""
        if isinstance(ref, GeneratorFuture):
            return ref.result()
        if isinstance(ref, Future):
            return ref.result()
        return ref
# ---
def __getitem__(self, idx: int) -> PageCacheT:
        return self.caches[idx]
# ---
def __init__(self, cap_value=10.0, **kwargs):
        """
        :param cap_value: float: value at which to clip activation
        :param kwargs: passed to torch.nn.LeadyReLU
        """
        super().__init__()
        self.relu = torch.nn.LeakyReLU(**kwargs)
        self.cap = torch.nn.Buffer(torch.tensor(cap_value, dtype=torch.float32))
# ---
import math
def area_pentagon(a):
  area=(math.sqrt(5*(5+2*math.sqrt(5)))*pow(a,2))/4.0
  return area
# ---
def processing_func(input_file_path):
        attr_file_paths = (
            [
                rebase_file_path(config.input_doc_path, input_file_path, input_attr_path)
                for input_attr_path in config.input_attr_paths
            ]
            if config.input_attr_paths is not None
            else []
        )
        return create_dataset_shard(input_file_path, config, attr_file_paths)
# ---
def _wait_for_server(self, timeout: float = 5.0):
        """Wait for server to be ready to accept connections."""
        start = time.time()
        while time.time() - start < timeout:
            try:
                with httpx.Client() as client:
                    client.get(f"http://{self.host}:{self.port}/docs", timeout=1.0)
                return
            except (httpx.ConnectError, httpx.TimeoutException):
                time.sleep(1.0)
# ---
def sort(self, items):
        slow_sorts = []
        switch_slow = False
        for sort in reversed(self.sorts):
            if switch_slow:
                slow_sorts.append(sort)
            elif sort.order_clause() is None:
                switch_slow = True
                slow_sorts.append(sort)
            else:
                pass

        for sort in slow_sorts:
            items = sort.sort(items)
        return items
# ---
def disruption_genetic_modification(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'CRISPR cutting',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def even_Power_Sum(n): 
    sum = 0; 
    for i in range(1,n + 1): 
        j = 2*i; 
        sum = sum + (j*j*j*j); 
    return sum;
# ---
def check_smaller(test_tup1, test_tup2):
  res = all(x > y for x, y in zip(test_tup1, test_tup2))
  return (res)
# ---
def __hash__(self):
        return hash((self.field, hash(self.pattern)))
# ---
def to(self, device: torch.device, non_blocking: bool = True) -> Self:
        return dataclasses.replace(
            self,
            data=self.data.to(device, non_blocking=non_blocking),
            means=self.means.to(device, non_blocking=non_blocking),
            stds=self.stds.to(device, non_blocking=non_blocking),
            mask=self.mask.to(device, non_blocking=non_blocking),
        )
# ---
def wrap(q_bhsd, k_bhsd, v_bhsd, seg_ids, kernel):
        return jax.vmap(kernel, in_axes=(0, 0, 0, segment_batch_axis))(q_bhsd, k_bhsd, v_bhsd, seg_ids)
# ---
def centered_hexagonal_number(n):
  return 3 * n * (n - 1) + 1
# ---
def __init__(self, transforms, num_cpus, num_gpus, resources):
        self.transforms = transforms
        self._num_cpus = num_cpus
        self._num_gpus = num_gpus
        self._resources = resources
# ---
def foo(x):
        assert x.shape in ((2,), (1,))
        return 2 * x
# ---
def test_label_detection_with_error_response(self, annotator_client_mock):
        # Given
        detect_text_method = annotator_client_mock.label_detection
        detect_text_method.return_value = AnnotateImageResponse(
            error={"code": 3, "message": "test error message"}
        )

        # When
        with pytest.raises(AirflowException) as ctx:
            self.hook.label_detection(image=DETECT_TEST_IMAGE)

        err = ctx.value
        assert "test error message" in str(err)
# ---
def __call__(self, target, creds, enforcer):
        """Recursively checks credentials based on the defined rules."""

        try:
            return enforcer.rules[self.match](target, creds, enforcer)
        except KeyError:
            # We don't have any matching rule; fail closed
            return False
# ---
def test_repr(self):
        expr = col("score") > 50
        assert repr(expr) == "(col('score') > lit(50))"
# ---
def forward_once(self, x, ctx: GridContext):
        return x[:, : self.out_channels] * 10.0 + x[:, -1]
# ---
def _lookup_indices(self, axis: AxisSelection) -> tuple[int | None, ...]: ...
# ---
def to_state_dict(self, prefix: str | None = None) -> StateDict:
        w = [self.w1.weight, self.w2.weight, self.w3.weight]
        out = {}

        num_experts = self.w1.Experts.size
        for i in range(num_experts):
            for j in range(3):
                key = f"{prefix}.{i}.w{j + 1}.weight"
                val = w[j]["experts", i].array
                out[key] = jnp.swapaxes(val, -1, -2)

        return out
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df.A[2] = 0
            return df.A.nunique()
# ---
def invert():
   global image1
   image1= ImageChops.invert(image1)

   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def setUp(self):
        self.config = ConfigReader("""
            <root>
                <person>
                    <name></name>
                    <age>15</age>
                </person>
                <person>
                    <name></name>
                    <age>43</age>
                </person>
            </root>
            """)
# ---
def __create_navigator(self, main_root):
		navigator_dir = os.path.join(main_root, 'navigator')
		safe_mkdir(navigator_dir)

		navigators = self.__config_mgr.navigator()
		self.__navigator_mgr.create_navigators(navigators, navigator_dir)
# ---
def test_build_runtime_env_gpu_clears_jax_platforms():
    from fray.v2.ray_backend.backend import build_runtime_env

    request = JobRequest(
        name="gpu-test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=GpuConfig(variant="H100")),
    )
    env = build_runtime_env(request)
    assert env["env_vars"]["JAX_PLATFORMS"] == ""
# ---
def activate(self, callingWindow, fullContext, mainItem, i):
        fitID = self.mainFrame.getActiveFit()
        Fit.getInstance().setAsPattern(fitID, mainItem)
        wx.PostEvent(self.mainFrame, GE.FitChanged(fitIDs=(fitID,)))
# ---
def create_loss_fn(self, reference_model: eqx.Module, train_model: eqx.Module) -> Callable:
        """Create the loss function for training."""
        ...
# ---
def current_demand(self) -> int:
        """Current demand level."""
        return self._current_demand
# ---
def prepare_output_dirs(self) -> None:
        self.experiment.nets_dir.mkdir(parents=True, exist_ok=True)
        self.experiment.output_dir.mkdir(parents=True, exist_ok=True)
# ---
def debug_strings(self):
        print(self.strings)
# ---
def test_mem_write_word_at_bottom_right(self):
        self.mda.mem_write_word(3998, 0x085A) # 'Z' with intensity.
        self.assertEqual(self.mda.video_ram[3998], 0x5A)
        self.assertEqual(self.mda.video_ram[3999], 0x08)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x5A, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def testForContinue(self):
    self.assertEqual((0, '1\n2\n3\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1, 2, 3):
          print i
          continue
          raise AssertionError""")))
# ---
def tokenize_dolmino_subset(name: str, tokenizer: str | None = None) -> ExecutorStep[TokenizeConfig]:
    """Get a specific dolmino split tokenization step."""
    assert name in DOLMINO_DATASETS, f"Split {name} not found in DOLMINO_DATASETS"
    return tokenize_dolmino(tokenizer=tokenizer)[f"dolmino/{name}"]
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "RotaryEmbeddingsConfig":
        return Llama3RotaryEmbeddingsConfig(
            theta=rope_theta,
            factor=config.get("factor", 8.0),
            low_freq_factor=config.get("low_freq_factor", 1.0),
            high_freq_factor=config.get("high_freq_factor", 4.0),
            original_max_position_embeddings=config.get("original_max_position_embeddings", 8192),
        )
# ---
def create_container(self, config: ContainerConfig) -> str: ...
# ---
def hrule(self):
        """Rendering method for ``<hr>`` tag."""
        if self.options.get('use_xhtml'):
            return '<hr />\n'
        return '<hr>\n'
# ---
def zone(self) -> str:
        return self._zone
# ---
def deposit(self, amount):
        self._balance += amount
        return self._balance
# ---
def announce(self):
        print("closed session. gets: %r, regets: %r, puts: %r, dels: %r, renames: %r get_dirs: %r, put_dirs: %r, get_bytes: %r put_bytes: %r window_seconds: %d" % \
              (self.get_requests, self.reget_requests, self.put_requests, self.del_requests, self.rename_requests, self.get_dirs, self.put_dirs, self.get_bytes, self.put_bytes, self.window_seconds))
# ---
def test_finalize_tracks_no_tracks(self):
        """ Tests that finalize fails if there are no tracks for an experiment """
        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)
        response = self.client.post(reverse("ab_testing_tool_finalize_tracks", args=(experiment.id,)),
                                    follow=True)
        self.assertError(response, NO_TRACKS_FOR_EXPERIMENT)
# ---
def url(self):
        return (JOBS_URL_PATTERN + '/%s') % (self.platform_id, self.id)
# ---
def _calculate_bytes_per_token_type(self, tokenizer: HfTokenizer) -> Optional[hax.NamedArray]:
        if tokenizer is None:
            return None
        else:
            # calculate the number of bytes in each token
            Vocab = hax.Axis("vocab", len(tokenizer.get_vocab()))
            bytes = np.ndarray((Vocab.size,), dtype=np.int32)

            for i in range(Vocab.size):
                bytes[i] = byte_length_of_token(tokenizer, i)

            return hax.named(jnp.array(bytes), Vocab)
# ---
def duration(self) -> Duration | None:
        """Calculate how long the attempt ran.

        Returns:
            Duration from started_at to finished_at, or None if not finished
        """
        if self.finished_at is None:
            return None
        elapsed_ms = self.finished_at.epoch_ms() - self.started_at.epoch_ms()
        return Duration.from_ms(elapsed_ms)
# ---
def _all_steps() -> set[str]:
    return set(_ordered_steps())
# ---
def check_Validity(a,b,c):  
    if (a + b <= c) or (a + c <= b) or (b + c <= a) : 
        return False
    else: 
        return True
# ---
def multiple(context):
        """A fixture to be invoked multiple times."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "kapow", "boing": "thunk"}

        return setup, teardown
# ---
def new_queue(self, name: str) -> "HttpQueue":
        """Create or get a named queue, returns client."""
        if name not in self.queues:
            self.queues[name] = MemoryQueue()
        # Use client-accessible host instead of bind host
        client_host = self.get_client_host()
        return HttpQueue(host=client_host, port=self.port, queue_name=name)
# ---
def test_non_monotonic(self):
        """
        L{get_clock_info} only knows about the monotonic clock.
        """
        with pytest.raises(ValueError):
            get_clock_info("not monotonic")
# ---
def accumulate_logits():
        xw_scratch_ref[...] += jax.lax.dot_general(
            x_ref[...],
            w_ref[...],
            (((1,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
# ---
def needs_space_before(self, other: "LatexNode") -> bool:
        # Macro needs space before text, another macro, or braced content
        return isinstance(other, (MacroNode, TextNode, BracedNode))
# ---
def get_now(timezone: Optional[datetime.tzinfo]) -> datetime.datetime:
    return datetime.datetime.now(timezone)
# ---
def crispr_gm(lab, award, source):
    return {
        'lab': lab['uuid'],
        'award': award['uuid'],
        'source': source['uuid'],
        'guide_rna_sequences': [
            "ACA",
            "GCG"
        ],
        'insert_sequence': 'TCGA',
        'aliases': ['encode:crispr_technique1'],
        '@type': ['Crispr', 'ModificationTechnique', 'Item'],
        '@id': '/crisprs/79c1ec08-c878-4419-8dba-66aa4eca156b/',
        'uuid': '79c1ec08-c878-4419-8dba-66aa4eca156b'
    }
# ---
def _flavor_clean_up(self, flavor_id):
        try:
            self.admin_flavors_client.delete_flavor(flavor_id)
            self.admin_flavors_client.wait_for_resource_deletion(flavor_id)
        except exceptions.NotFound:
            pass
# ---
def best_validation_checkpoint_path(self) -> Path:
        return self.checkpoint_dir / "best_validation_ckpt.pt"
# ---
def Histogram(self, *args, **kwargs):
        return wandb.Histogram(*args, **kwargs)
# ---
def __repr__(self):
        return "{0.__class__.__name__}({0.field!r}, {0.fast})".format(self)
# ---
def resolver(self) -> Resolver:
        """Get a resolver for actor discovery.

        The resolver uses the namespace derived from this context's job ID.

        Raises:
            RuntimeError: If no client is available
        """
        return NamespacedResolver(self.client._cluster_client, self.namespace)
# ---
def test_mem_read_byte(self):
        self.mda.video_ram[77] = 0xA5
        self.assertEqual(self.mda.mem_read_byte(77), 0xA5)
# ---
def test_connection_as_ctx(self):
        fn = self._trans_fn()
        ctx = testing.db.connect()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        # autocommit is on
        self._assert_fn(5, value=8)
# ---
def join(self, *args, **kwargs):
        super().join(*args, **kwargs)
        if self._exception:
            raise self._exception
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._exception: BaseException | None = None
# ---
def output_exemplar(self):
        return AudioTextDict_exemplar
# ---
def __getitem__(self, idx: SliceSpec | EllipsisType = Ellipsis) -> NamedArray:
        """Read from the reference using named indexing semantics."""
        _, axes_spec, index_tuple = self._prepare(idx)
        result = self._ref[tuple(index_tuple)]
        return named(result, axes_spec)
# ---
import heapq
def merge_sorted_list(num1,num2,num3):
  num1=sorted(num1)
  num2=sorted(num2)
  num3=sorted(num3)
  result = heapq.merge(num1,num2,num3)
  return list(result)
# ---
def test_register_encoder(self):
        """Can register a custom encoder"""
        from datetime import datetime

        dynamizer = Dynamizer()
        dynamizer.register_encoder(datetime, lambda d, v: (STRING, v.isoformat()))
        now = datetime.utcnow()
        self.assertEqual(dynamizer.raw_encode(now), (STRING, now.isoformat()))
# ---
def params(self):
        param = {
            'x': 0,
            'y': 0,
            'w': 500,
            'h': 500,
            'title': '% Type Prog Title Here %',
        }
        return param
# ---
def test_axis_shapes_multiple_dcn_absorbers_error():
    cfg = MeshConfig(dcn_axes={"replica_dcn": -1, "other": -1})
    with pytest.raises(ValueError):
        cfg.axis_shapes(num_devices=8, num_slices=2)
# ---
def test_noncontig_selectors():
    B, X, Z, Y = Axis("batch", 2), Axis("x", 4), Axis("z", 6), Axis("y", 5)
    a = hax.arange((B, X, Z, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a["x", ix, "y", iy]
    assert out.axes == (B, Z)
    ref = a.array[jnp.arange(2), ix.array, :, iy.array]
    assert jnp.array_equal(out.array, ref)
# ---
def test_collect_workdir_size_mb_nonexistent_directory():
    """Test workdir size returns 0 for non-existent directory."""
    nonexistent = Path("/nonexistent/path/does/not/exist")

    size_mb = collect_workdir_size_mb(nonexistent)

    assert size_mb == 0
# ---
def tree_flatten(self) -> Any:
        return ((self.array,), self.axis_names)
# ---
def transform(self, x, y, z, *, static1, static2):
            assert static1 is True
            assert static2 is False
            return x + self.w + y + z
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["QwenConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfQwenConfig,
        )
# ---
def poisson(key, shape: AxisSpec, lam: NamedOrNumeric, dtype=int):
    shape = axis_spec_to_shape_dict(shape)
    lam = broadcast_to(lam, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.poisson(key=key, lam=lam, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def backwards(self, orm):
        # Deleting model 'ArticleComment'
        db.delete_table('cms_articlecomment')
# ---
def define_class_with_no_name():
            @six.add_metaclass(profiler.TracedMeta)
            class FakeTraceWithMetaclassNoName(FakeTracedCls):
                pass
# ---
def test_merge_chunks(spec, target_chunks, expected_chunksize):
    a = xp.ones((10, 10), dtype=np.uint8, chunks=(2, 3), spec=spec)
    b = merge_chunks(a, target_chunks)
    assert b.chunksize == (expected_chunksize or target_chunks)
    assert_array_equal(b.compute(), np.ones((10, 10)))
# ---
def service(self, config):
        ''' setter for service property '''
        self.svc = config
# ---
def __rand__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_and(other, self)
# ---
def to_arrow(self, message):
        return self._parse_arrow_message(message)
# ---
def overlay_add(self, overlay_id, x, y, file_or_fd, offset, fmt, w, h, stride):
        self.command('overlay_add', overlay_id, x, y, file_or_fd, offset, fmt, w, h, stride)
# ---
def accept(self, match, include_rejected=False):
        self._call_all('accept', match, include_rejected)
# ---
def corrcoef(x, y=None):
    c = cov(x, y)
    d = diag(c)
    return c/sqrt(multiply.outer(d,d))
# ---
def check_element(list,element):
  check_element=all(v== element for v in list)
  return check_element
# ---
def raise_parse_error(message: str, expression: str, pos: int | tuple[int, int] | None) -> NoReturn:
    """Raise a ValueError with a message and the position in the expression."""
    fmt = f"Error while parsing:\n    {expression}"
    if pos is not None:
        if isinstance(pos, int):
            fmt += f'\n    {" " * pos}^'
        else:
            fmt += f"\n    {' ' * pos[0]}{'^' * max(1, pos[1] - pos[0])}"

    fmt += f"\n{message}"

    raise ValueError(fmt)
# ---
def get_job_status(
        self,
        job_id: str,
    ) -> cluster_pb2.Controller.GetJobStatusResponse:
        """Get the status of a job."""
        request = cluster_pb2.Controller.GetJobStatusRequest(job_id=job_id)
        return self._service.get_job_status(request, None)
# ---
def test_can_supply_filename_None(self):
        out = StringIO()
        c_log, f_log, formatter = logging_support.configure_logging(out, None)
        self.assertEqual(None, f_log)
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetDepartmentsRow(), 3)
# ---
def setUpTestData(cls):
        cls.url = reverse('search-list')
        Feature.objects.create(name='archival descriptions', enabled=True)

        org_group_type = GroupType.objects.create(codename='organization')
        cls.group = Group.objects.create(group_type=org_group_type)
        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)
        cls.archive_type = TagVersionType.objects.create(name='archive', archive_type=True)
# ---
from collections import Counter 
	
def second_frequent(input): 
	dict = Counter(input) 
	value = sorted(dict.values(), reverse=True)  
	second_large = value[1] 
	for (key, val) in dict.items(): 
		if val == second_large: 
			return (key)
# ---
def position_min(list1):
    min_val = min(list1)
    min_result = [i for i, j in enumerate(list1) if j == min_val]
    return min_result
# ---
def start(self):
        self.init()
# ---
def assertBlobEqual(self, container_name, blob_name, expected_data):
        blob = self.bsc.get_blob_client(container_name, blob_name)
        actual_data = blob.download_blob()
        self.assertEqual(b"".join(list(actual_data.chunks())), expected_data)
# ---
def kernel_display_name(self):
        return self.json['metadata']['kernelspec']['display_name']
# ---
import re
def text_match_three(text):
        patterns = 'ab{3}?'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def test_clock_increases():
    """
    A monotonic moment is never greater than a succeeding monotonic
    moment.
    """
    assert monotonic() <= monotonic()
# ---
def wait_for_job(
        self,
        job_id: JobName,
        timeout: float = 300.0,
        poll_interval: float = 2.0,
    ) -> cluster_pb2.JobStatus:
        return self._remote_client.wait_for_job(job_id, timeout=timeout, poll_interval=poll_interval)
# ---
def formfield(self, *args, **kwargs):
		kwargs["form_class"] = JSONFormField
		return super(JSONField, self).formfield(*args, **kwargs)
# ---
def from_feat_batch(
        self, feat: Dict[str, torch.Tensor], res_atoms_only: bool = False
    ) -> "Structure":
        sample = {k: v.squeeze() for k, v in feat.items()}
        self.from_feat(sample, res_atoms_only=res_atoms_only)
# ---
def setUp(self):
        pass
# ---
def max_seqs(self) -> int:
        return self.seq_lens.axis_size("seq")
# ---
def refresh(self, force=False):
        self.refresh_cnt += 1
        if self.refresh_cnt >= 60000:
            self.refresh_cnt = 0
        redraw_required = False
        for wid in self.widgets:
            if (self.refresh_cnt % wid.metric.refresh_rate == 0) or force:
                wid.refresh()
                redraw_required = True
        if redraw_required:
            self.queue_draw()
        return True
# ---
def __init__(self, start=0.0, stop=5.0, num_gaussians=50):
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer("offset", offset)
# ---
def test_asdict_excluding_invalid():
    """Test asdict_excluding with non-dataclass input."""
    with pytest.raises(ValueError, match="Only dataclasses are supported"):
        asdict_excluding({"key": "value"}, exclude=set())
# ---
def stop(self) -> None:
        if self._controller:
            self._controller.stop()
            self._controller = None
        # Clean up autoscaler's temp dir
        if self._autoscaler and hasattr(self._autoscaler, "_temp_dir"):
            self._autoscaler._temp_dir.cleanup()
            self._autoscaler = None
        # Clean up controller's temp dir
        if self._temp_dir:
            self._temp_dir.cleanup()
            self._temp_dir = None
# ---
def getResiduePositions(residue, positions):
  """ Returns array w. atomic positions of residue """
  ndx = atomIndexInResidue(residue)
  return np.array(positions)[ndx]
# ---
def __len__(self):
        """The length of the array.

        Returns
        -------
        int :
            The size of the array

        """
        return self._size
# ---
def pspec_for_axis(axis: AxisSelection, mapping: ResourceMapping | None = None) -> PartitionSpec:
    """Get the PartitionSpec for a single axis"""
    axis = axis_spec_to_shape_dict(axis)
    return PartitionSpec(*(physical_axis_name(a, mapping) for a in axis))
# ---
def __post_init__(self):
        if self.hyena.max_seq_len != self.max_seq_len:
            object.__setattr__(self, "hyena", dataclasses.replace(self.hyena, max_seq_len=self.max_seq_len))
# ---
def setUp(self):
        super(IntegrationTestAnalyzers, self).setUp()

        self.api = SuperSearchWithFields(config=self.config)
        self.now = datetimeutil.utc_now()
# ---
def __list_methods(self):
        return self.__list_handlers().keys() + self.__base_methods.keys()
# ---
def __setstate__(self, state):
        self.host = state["host"]
        self.port = state["port"]
        self.queue_name = state["queue_name"]
# ---
def is_Perfect_Square(n) :
    i = 1
    while (i * i<= n):
        if ((n % i == 0) and (n / i == i)):
            return True     
        i = i + 1
    return False
# ---
def __init__(self, host: str = "0.0.0.0", port: int = 9999):
        self.host = host
        self.port = port
        self.queues: dict[str, MemoryQueue] = {}
        self.app = self._create_app()

        import uvicorn

        config = uvicorn.Config(self.app, host=host, port=port, log_level="error", access_log=False)
        self.server = uvicorn.Server(config)
        self.server_thread: ServerThread | None = None
# ---
def fake_volume_type_get(context, id, inactive=False, expected_fields=None):
    vol = VOLUME_TYPES[id]
    if expected_fields and 'projects' in expected_fields:
        vol['projects'] = [a['project_id']
                           for a in ACCESS_LIST if a['volume_type_id'] == id]
    return vol
# ---
def to(self, device: torch.device):
        self.raw_data = [
            (
                input_.to(device, non_blocking=True),
                boundary.to(device, non_blocking=True),
                label.to(device, non_blocking=True),
            )
            for input_, boundary, label in self.raw_data
        ]
# ---
def test_job_expands_to_correct_number_of_tasks(make_job_request):
    """expand_job_to_tasks creates correct number of tasks based on replicas."""
    request = make_job_request()
    request.replicas = 3
    job = ControllerJob(job_id=JobName.root("test-job"), request=request)

    tasks = expand_job_to_tasks(job)

    assert len(tasks) == 3
    for i, task in enumerate(tasks):
        assert task.task_index == i
        assert task.job_id == job.job_id
# ---
def test_arrow_batch_sizes(benchmark: Any, in_memory_table: pa.Table, batch_size: int) -> None:
    """
    Benchmarks the effect of PyArrow batch size on marshaling throughput.
    """

    def _pipeline() -> int:
        return sum(len(dupekit.process_arrow_batch(b)) for b in in_memory_table.to_batches(max_chunksize=batch_size))

    assert benchmark(_pipeline) > 0
# ---
def the_object_name_is_contained_in_container_name(name, container_name):
    ifc = an_ifc_file_exists()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    container = ifcopenshell.util.element.get_container(element)
    if not container:
        assert False, f'Object "{name}" is not in any container'
    assert container.Name == container_name, f'Object "{name}" is in {container}'
# ---
def test_append_error():
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=1, dtype=jnp.float32)
        with pytest.raises(ValueError):
            builder.append(jnp.array([[1.0, 2.0]]))
# ---
def slice_all_ready(slice_info: vm_pb2.SliceInfo) -> bool:
    """Compute all_ready from vms[] in proto."""
    return all(vm.state == vm_pb2.VM_STATE_READY for vm in slice_info.vms)
# ---
def has_keyword(func, keyword):
    try:
        return keyword in inspect.signature(func).parameters
    except Exception:
        return False
# ---
from collections import Counter 
def max_char(str1):
    temp = Counter(str1) 
    max_char = max(temp, key = temp.get)
    return max_char
# ---
def output_escape(self, m):
        text = m.group(1)
        return self.renderer.escape(text)
# ---
from collections import Counter 
def assign_freq(test_list):
  res = [(*key, val) for key, val in Counter(test_list).items()]
  return (str(res))
# ---
def frequency_Of_Smallest(n,arr): 
    mn = arr[0] 
    freq = 1
    for i in range(1,n): 
        if (arr[i] < mn): 
            mn = arr[i] 
            freq = 1
        elif (arr[i] == mn): 
            freq += 1
    return freq
# ---
from array import array
def positive_count(nums):
    n = len(nums)
    n1 = 0
    for x in nums:
        if x > 0:
            n1 += 1
        else:
          None
    return round(n1/n,2)
# ---
def get_toplevel_xid():
    if app.window.get_window():
        try:
            return app.window.get_window().get_xid()
        except AttributeError:  # non x11
            pass
    return 0
# ---
def convert_task():
        export_lm_to_hf.main(convert_config)
# ---
def __getattr__(self, method_name: str) -> LocalActorMethod:
        if method_name.startswith("_"):
            raise AttributeError(method_name)
        method = getattr(self._instance, method_name)
        if not callable(method):
            raise AttributeError(f"{method_name} is not callable on {type(self._instance).__name__}")
        return LocalActorMethod(method, self._executor)
# ---
def any(x, /, *, axis=None, keepdims=False, split_every=None):
    if x.size == 0:
        return asarray(False, dtype=x.dtype)
    return reduction(
        x,
        nxp.any,
        axis=axis,
        dtype=nxp.bool,
        keepdims=keepdims,
        split_every=split_every,
    )
# ---
def first_repeated_word(str1):
  temp = set()
  for word in str1.split():
    if word in temp:
      return word;
    else:
      temp.add(word)
  return 'None'
# ---
def _fill_queue_with_batches(self):
        try:
            iterator = self._producer_fn()
            if isinstance(iterator, Iterator):
                self._produce_batches_sync(iterator)
            else:
                asyncio.run(self._produce_batches_async(iterator))
        except Exception:
            self.q.put(_ExceptionWrapper(sys.exc_info()))
# ---
def set_as_current(self):
        self.notebook.current_job = self
# ---
def test_scale_down_nonexistent_vm_group_is_noop(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """scale_down() on a nonexistent VM group does nothing."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        # Should not raise
        group.scale_down("nonexistent-slice")

        assert group.slice_count() == 0
# ---
def __ne__(self, other, /):
        other = self._check_allowed_dtypes(other, "all", "__ne__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.not_equal, self, other, dtype=nxp.bool)
# ---
def test_job_name_require_task_errors_on_non_task():
    with pytest.raises(ValueError):
        JobName.from_string("/root/child").require_task()
# ---
def _update(self, records, value):
        """ Update the cached value of ``self`` for ``records`` with ``value``. """
        for record in records:
            if self in record._cache:
                record._cache[self] = record[self.name] | value
            else:
                record._cache[self] = UnionUpdate(self, record, value)
# ---
def test_run_task_with_ports(worker):
    """Test run_task allocates ports correctly."""
    task_id = JobName.root("job-with-ports").task(0).to_wire()
    request = create_run_task_request(task_id=task_id, ports=["http", "grpc"])
    worker.submit_task(request)

    # Verify ports were allocated
    task = worker.get_task(task_id)
    assert len(task.ports) == 2
    assert "http" in task.ports
    assert "grpc" in task.ports
# ---
def load_thermo_from_postgres(
        self, postgres_uri: str = "postgresql:///eq_compounds"
    ) -> None:
        """Load a LocalCompoundCache from a postgres uri for equilibrator.

        Parameters
        ----------
        postgres_uri : str, optional
            uri of the postgres DB to use, by default "postgresql:///eq_compounds"
        """
        self.lc = LocalCompoundCache()
        self.lc.ccache = CompoundCache(create_engine(postgres_uri))

        self._water = self.lc.get_compounds("O")
# ---
def test_entrypoint_params_tpu():
    from fray.v2.ray_backend.backend import get_entrypoint_params

    request = JobRequest(
        name="tpu-job",
        entrypoint=Entrypoint.from_binary("train", []),
        resources=ResourceConfig(device=TpuConfig(variant="v4-8")),
    )
    params = get_entrypoint_params(request)
    assert "entrypoint_resources" in params
    assert params["entrypoint_resources"]["TPU-v4-8-head"] == 1.0
    assert params["entrypoint_resources"]["TPU"] == 4.0
# ---
def remove_empty(tuple1): #L = [(), (), ('',), ('a', 'b'), ('a', 'b', 'c'), ('d')]
   tuple1 = [t for t in tuple1 if t]
   return tuple1
# ---
def test_actor_group_create_and_wait_ready(client: LocalClient):
    group = client.create_actor_group(Counter, name="counters", count=3)
    assert group.ready_count == 3
    handles = group.wait_ready()
    assert len(handles) == 3

    for i, h in enumerate(handles):
        h.increment.remote(i + 1).result()

    values = [h.get.remote().result() for h in handles]
    assert values == [1, 2, 3]
# ---
def unregister():
    global imported_mods
    print(imported_mods)
    for mod in imported_mods.values():
        mod.unregister()
    imported_mods = {}
# ---
def max_difference(test_list):
  temp = [abs(b - a) for a, b in test_list]
  res = max(temp)
  return (res)
# ---
def _get_ema_name(self, name: str) -> str:
        # remove as '.'-character is not allowed in buffers
        # And make ourselves agnostic to module vs not
        return name.removeprefix("module.").replace(".", "")
# ---
def get_block_datapoint(self, headerhash):
        with self.lock:
            return self._state.get_block_datapoint(headerhash)
# ---
def stub_add_volume_type_access(context, type_id, project_id):
            raise exception.VolumeTypeAccessExists(volume_type_id=type_id,
                                                   project_id=project_id)
# ---
def test_addition_task_reward():
    task = AdditionTask()
    examples = task.generate_examples(10, np.random.default_rng(42))
    assert len(examples) == 10
    assert all("+" in ex["prompt"] for ex in examples)

    assert task.compute_reward("42", "42") == pytest.approx(1.0)
    assert task.compute_reward("42", "43") == pytest.approx(0.0)
    assert task.compute_reward("42", "-") == pytest.approx(0.0)
    assert task.compute_reward("42", "-2") == pytest.approx(0.0)
# ---
def __init__(self, models=None):
        self.models = models or []
# ---
def main(args: RayCachedLMDatasetConfig):
    """Caches two different kinds of datasets. It can cache a dataset from a list of urls, or a dataset from a hf dataset"""
    init_logging(".", "cache_dataset.log")
    args.initialize()

    for split in ["train", "validation"]:
        print(f"Caching {split} for all components.")
        # build_caches will build or load as needed
        args.build_caches(split)
        print(f"Finished caching {split}.")
# ---
def launch(self, request: JobRequest) -> JobId:
        """Launch a job on the cluster.

        Args:
            request: Job specification including resources, environment, and entrypoint

        Returns:
            Unique identifier for the launched job

        Raises:
            ValueError: If the request is invalid
            RuntimeError: If job submission fails
        """
        ...
# ---
def testImportWildcardMemberRaises(self):
    regexp = 'wildcard member import is not implemented'
    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,
                            'from foo import *')
    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,
                            'from "__go__/foo" import *')
# ---
def out_qdq_fwd(compute_dtype, out, scale, amax_history):
    return out, (scale, amax_history)
# ---
def are_Rotations(string1,string2): 
    size1 = len(string1) 
    size2 = len(string2) 
    temp = '' 
    if size1 != size2: 
        return False
    temp = string1 + string1 
    if (temp.count(string2)> 0): 
        return True
    else: 
        return False
# ---
def l1(*arg, **kw):
            pass
# ---
def __init__(self, size: int, grid_size: GridSize = (100, 100)):
        self._size = size
        self.input_src = self
        self.label_src = self
        self.grid_size = grid_size
# ---
def sample_params():
    """Generate sample JAX parameters for testing."""
    return create_sample_pytree(seed=42)
# ---
def _deshape(x):
        if is_jax_array_like(x) and x.shape != ():
            return x.reshape([length, *x.shape[2:]])
        else:
            return x
# ---
def __init__(self):
        self._queue = []
# ---
def logged(self):
        if len(self.doc.xpath('//b[text()="Session interrompue"]')) > 0:
            return False
        return True
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)
        start, end = _parse_periods(pattern)
        self.interval = DateInterval.from_periods(start, end)
# ---
def today(*args):
        """ Return the current day in the format expected by the ORM.
            This function may be used to compute default values.
        """
        return date.today().strftime(DATE_FORMAT)
# ---
def __init__(self,domain='gw.api.taobao.com',port=80):
		RestApi.__init__(self,domain, port)
		self.user_nick = None
# ---
def cumsum(a: NamedArray, axis: AxisSelector, *, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.cumsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html)
    """
    return wrap_axiswise_call(jnp.cumsum, a, axis, dtype=dtype, single_axis_only=True)
# ---
def cleanup(self) -> None:
        """Cleanup resources."""
        pass
# ---
def even_or_odd(N): 
    l = len(N) 
    if (N[l-1] =='0'or N[l-1] =='2'or 
        N[l-1] =='4'or N[l-1] =='6'or 
        N[l-1] =='8'or N[l-1] =='A'or 
        N[l-1] =='C'or N[l-1] =='E'): 
        return ("Even") 
    else: 
        return ("Odd")
# ---
def test_add(self):
        expr = col("a") + col("b")
        assert expr.evaluate({"a": 10, "b": 5}) == 15
# ---
def _tree_byte_size(tree) -> int:
    """Return the per-device number of bytes represented by ``tree``."""

    return sharded_tree_size(tree)
# ---
def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig, _encoding)
# ---
def unpickle_and_call(f, inp, **kwargs):
    import cloudpickle

    f = cloudpickle.loads(f)
    inp = cloudpickle.loads(inp)
    kwargs = {k: cloudpickle.loads(v) for k, v in kwargs.items()}
    return f(inp, **kwargs)
# ---
def subsystem_dependencies(cls):
    return (super(GoThriftGen, cls).subsystem_dependencies() +
            (ThriftDefaults, ThriftBinary.Factory.scoped(cls)))
# ---
def an_ifc_file_exists():
    ifc = IfcStore.get_file()
    if not ifc:
        assert False, "No IFC file is available"
    return ifc
# ---
def normalize_and_mask(
            tensor: torch.Tensor,
            means: torch.Tensor,
            stds: torch.Tensor,
            mask: torch.Tensor,
        ) -> torch.Tensor:
            if self.normalize_before_mask:
                tensor = normalize(tensor, means, stds)
            tensor = torch.where(mask, tensor, self.masked_fill_value)
            if not self.normalize_before_mask:
                tensor = normalize(tensor, means, stds)
            return tensor
# ---
def on_btn_add_row_clicked(self):
        self.current_label.add_fuzz_value()
        self.fuzz_table_model.update()
# ---
def _declares_service(self, source):
    with open(source) as thrift:
      return any(line for line in thrift if self.SERVICE_PARSER.search(line))
# ---
def test_stage_name_truncation():
    """PhysicalStage.stage_name() truncates long names."""
    stage = PhysicalStage(operations=[Map(fn=lambda x: x) for _ in range(20)])
    name = stage.stage_name(max_length=20)
    assert len(name) <= 20
    assert name.endswith("...")
# ---
def crispr_knockout(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'knockout',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def _touch(file_path):
    with open(file_path, "a"):
        os.utime(file_path, None)
# ---
def __init__(
        self,
        prediction: torch.Tensor,
        target: torch.Tensor,
        time: xr.DataArray,
    ):
        assert prediction.shape == target.shape
        self.prediction = prediction
        self.target = target
        self.time = time
# ---
def uri(self, uri):
        """
        Sets the uri of this ContributorOrcid.

        :param uri: The uri of this ContributorOrcid.
        :type: str
        """

        self._uri = uri
# ---
def _pspec_parts(spec_part) -> str:
    if spec_part is None:
        return "unsharded"
    elif isinstance(spec_part, (tuple, list)):
        return "+".join(str(p) for p in spec_part)
    else:
        return str(spec_part)
# ---
def handle404(error):
    return '<H1>Ooops, its not here<BR>'
# ---
def cli():
    """Iris Worker - Job execution daemon."""
    pass
# ---
def unescape(s):
    s = s.replace("&lt;", "<")
    s = s.replace("&gt;", ">")
    # this has to be last:
    s = s.replace("&amp;", "&")
    return s
# ---
def get_parent_id(self, name, attrs):
        final_attrs = self.build_attrs(attrs, type=self.input_type, name=name)
        return final_attrs['id']
# ---
def gen_signature(self, privkey, pubkey, sig_path):
        '''
        Generate master public-key-signature
        '''
        return salt.crypt.gen_signature(privkey,
                                        pubkey,
                                        sig_path)
# ---
def to_str(val):
  return val
# ---
def ray_cluster():
    if not ray.is_initialized():
        logging.info("Initializing Ray cluster")
        ray.init(
            address="local",
            num_cpus=8,
            ignore_reinit_error=True,
            logging_level=logging.INFO,
            log_to_driver=True,
            resources={"head_node": 1},
        )
        yield
# ---
def is_well_known_file_type(self, wkf, name):
        '''Check if a structure has the expected name.

        Parameters
        ----------
        wkf : dict
            A well-known-file structure with nested type information.
        name : string
            The expected type name

        See Also
        --------
        read_json: where this helper function is used.
        '''
        try:
            return wkf['well_known_file_type']['name'] == name
        except:
            return False
# ---
def info(self):
        return self.headers
# ---
def test_multiple_ops(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]], chunks=(2, 2), spec=spec)
    c = xp.add(a, b)
    d = xp.negative(c)
    assert_array_equal(
        d.compute(executor=executor),
        np.array([[-2, -3, -4], [-5, -6, -7], [-8, -9, -10]]),
    )
# ---
def __init__(
        self,
        attn_window_queries,
        attn_window_keys,
        **diffusion_transformer_kwargs,
    ):
        super().__init__()
        self.attn_window_queries = attn_window_queries
        self.attn_window_keys = attn_window_keys
        self.diffusion_transformer = DiffusionTransformer(
            **diffusion_transformer_kwargs
        )
# ---
def slice_valid_index(index, ccd_to_valid_id_array, args=None):
    index = ccd_to_valid_id_array[index]
    valid_index_mask = (~np.isnan(index)).all(axis=0)
    index = index[:, valid_index_mask]
    if args is None:
        return index
    args = (arg[valid_index_mask] for arg in args)
    return index, args
# ---
def _add(a, b):
    return a + b
# ---
def __init__(self, comodel_name=None, string=None, **kwargs):
        super(Many2one, self).__init__(comodel_name=comodel_name, string=string, **kwargs)
# ---
def __init__(
        self,
        producer_fn: Callable[[], Union[Iterator[Ex], AsyncIterator[Ex]]],
        max_capacity: Optional[int] = None,
    ):
        self.max_capacity = max_capacity
        self._producer_fn = producer_fn
# ---
def unprotect(self, tag: str) -> None:
        """Decrement refcount for an image (job done with it)."""
        if tag in self._image_refcounts:
            self._image_refcounts[tag] -= 1
            if self._image_refcounts[tag] <= 0:
                del self._image_refcounts[tag]
# ---
def validate_axis(axis, ndim):
    """Validate an input to axis= keywords"""
    if isinstance(axis, (tuple, list)):
        return tuple(validate_axis(ax, ndim) for ax in axis)
    if not isinstance(axis, numbers.Integral):
        raise TypeError("Axis value must be an integer, got %s" % axis)
    if axis < -ndim or axis >= ndim:
        raise np.AxisError(
            "Axis %d is out of bounds for array of dimension %d" % (axis, ndim)
        )
    if axis < 0:
        axis += ndim
    return axis
# ---
def test_is_stop_signal_no_match():
    # tail_tokens does not match stop_sequence
    tail_tokens = hax.named(jnp.array([5, 6, 7], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[1, 2, 3]], dtype=jnp.int32), axis=("seq", "position"))
    assert not is_stop_signal(tail_tokens, stop_sequences)
# ---
import re
def text_match(text):
        patterns = 'ab*?'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def __str__(self) -> str:
        return self.datetime.strftime("%Y-%m-%d")
# ---
def __len__(self):
        if self.tree is None:
            return 0
        else:
            return len(jax.tree.leaves(self.tree)[0])
# ---
def _delete_vm(name: str, zone: str, project: str) -> bool:
    result = _run_gcloud(["compute", "instances", "delete", name, f"--project={project}", f"--zone={zone}", "--quiet"])
    if result.returncode != 0:
        error = result.stderr.strip()
        if "not found" in error.lower():
            click.echo(f"  VM {name} already deleted")
            return True
        click.echo(f"  Failed to delete VM {name}: {error}", err=True)
        return False
    return True
# ---
def test_bundle_creator_uses_fallback_when_git_unavailable(workspace):
    with patch("iris.cluster.client.bundle._get_git_non_ignored_files", return_value=None):
        creator = BundleCreator(workspace)
        bundle_bytes = creator.create_bundle()

    with zipfile.ZipFile(io.BytesIO(bundle_bytes)) as zf:
        names = zf.namelist()
        assert "pyproject.toml" in names
        assert "src/main.py" in names
        assert not any("__pycache__" in n for n in names)
# ---
def test_str_split_filter(self):
        def test_impl(df):
            B = df.A.str.split(',')
            df2 = pd.DataFrame({'B': B})
            return df2[df2.B.str.len() > 1]

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D', 'G', '', 'g,f']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_frame_equal(
            hpat_func(df), test_impl(df).reset_index(drop=True))
# ---
def coscheduling_group_by(self) -> str | None:
        """The attribute key used to group workers for coscheduling, or None."""
        if self.is_coscheduled:
            return self.request.coscheduling.group_by
        return None
# ---
def id(self):
		"Temporary hack to transition between _id and id"
		return self._id
# ---
def on_test(self):
            pass
# ---
def frexp(a: A) -> A:
    return wrap_elemwise_unary(jnp.frexp, a)
# ---
def _l3_plugin(self):
        return bc.get_plugin(bc.constants.L3)
# ---
def find_platform(arr, dep, n): 
    arr.sort() 
    dep.sort() 
    plat_needed = 1
    result = 1
    i = 1
    j = 0
    while (i < n and j < n): 
        if (arr[i] <= dep[j]):           
            plat_needed+= 1
            i+= 1
        elif (arr[i] > dep[j]):           
            plat_needed-= 1
            j+= 1
        if (plat_needed > result):  
            result = plat_needed           
    return result
# ---
def signal(self) -> None:
        dirname = os.path.dirname(self._path)
        if dirname:
            os.makedirs(dirname, exist_ok=True)
        with open(self._path, "w"):
            pass
# ---
def output_path_design_folded(input_path):
                output_dir = (
                    design_dir / const.refold_design_cif_dirname
                    if self.output_dir is None
                    else self.output_dir
                )
                return [
                    output_dir / f"{input_path.stem}.cif",
                ]
# ---
def __init__(self, main_controller, word_list=[], comp_len=DEFAULT_COMPLETION_LENGTH):
        """Constructor.

            @type  word_list: iterable
            @param word_list: A list of words that should be auto-completed."""
        self.main_controller = main_controller
        assert isinstance(word_list, list)
        self.comp_len = comp_len
        self._word_list = []
        self._word_freq = defaultdict(lambda: 0)
        self.add_words(word_list)
        self.widgets = set()
# ---
def _find_packages(path='.', prefix=''):
        yield prefix
        prefix = prefix + "."
        for _, name, ispkg in walk_packages(path, 
                                            prefix,
                                            onerror=lambda x: x):
            if ispkg:
                yield name
# ---
def test_startswith_sqlexpr(self):
        col = self.tables.some_table.c.data
        self._test(
            col.startswith(literal_column("'ab%c'")),
            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10},
        )
# ---
def setUp(self):
        self.ll = generate_ll()
        self.pl = self.ll.pl
# ---
def get_evaluator(config: EvaluationConfig) -> Evaluator:
    if config.evaluator not in EVALUATORS:
        raise ValueError(f"Unknown evaluator: {config.evaluator}. Available: {list(EVALUATORS.keys())}")
    return EVALUATORS[config.evaluator]()
# ---
def __init__(self, dim, base=10_000):
    super().__init__()
    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
    self.register_buffer('inv_freq', inv_freq)
    self.seq_len_cached = None
    self.cos_cached = None
    self.sin_cached = None
# ---
def on_test_start(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,
    ) -> None:
        """Use EMA weights for testing.

        Parameters
        ----------
        trainer: Trainer
            The Trainer instance.
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._on_eval_start(pl_module)
# ---
def local_keys(self):
        '''
        Return a dict of local keys
        '''
        ret = {'local': []}
        fn_ = os.path.join(self.opts['pki_dir'], 'local.key')
        if os.path.isfile(fn_):
            ret['local'].append(fn_)
        return ret
# ---
def read_remote(self, minion_id, status=ACC):
        '''
        Read in a remote key of status
        '''
        path = os.path.join(self.opts['pki_dir'], status, minion_id)
        if not os.path.isfile(path):
            return {}
        with salt.utils.fopen(path, 'rb') as fp_:
            return self.serial.loads(fp_.read())
# ---
def test_conf_from_extra_conf(self):
        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '1.2.3.4:5')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, True)
        self.assertEqual(
            app.memcache._client_cache['1.2.3.4:5'].max_size, 4)
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' or '.join(str(r) for r in self.rules)
# ---
def condition(self):
        return len(self.el) >= 5 and not self.el.get('id', '').startswith('libelleLong')
# ---
def endpoints(self) -> list[ResolvedEndpoint]:
        return list(self._resolve().endpoints)
# ---
def get_fp8_max(fp8_dtype, out_dtype):
    assert fp8_dtype in (jnp.float8_e4m3fn, jnp.float8_e5m2)
    return jnp.finfo(fp8_dtype).max.astype(out_dtype)
# ---
def evaluate(self, record: dict) -> Any:
        """Evaluate expression against a record."""
        pass
# ---
def __rxor__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer or boolean", "__rxor__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.bitwise_xor, other, self, dtype=result_type(self, other))
# ---
def _var_name_encode_level(var_name: str) -> bool:
    """Check if the variable name encodes the level."""
    var_name_encodes_level = re.compile(r"_[0-9]+")
    return bool(var_name_encodes_level.search(var_name))
# ---
def wait(self) -> None:
        self._threads.wait()
# ---
def convert_to_write(self, value, target=None, fnames=None):
        """ convert ``value`` from the cache to a valid value for method
            :meth:`BaseModel.write`.

            :param target: optional, the record to be modified with this value
            :param fnames: for relational fields only, an optional collection of
                field names to convert
        """
        return self.convert_to_read(value)
# ---
def test_ckpt_path_with_valid_string_path():
    path = "checkpoints/llama-8b-tootsie-phase2/checkpoints/step-730000"
    assert ckpt_path_to_step_name(path) == "llama-8b-tootsie-phase2-730000"
# ---
def totext(obj):
        if isinstance(obj, str):
            obj = unicode(obj, 'UTF-8')
        assert isinstance(obj, unicode)
        return obj
# ---
def test_impl(A):
            df = pd.DataFrame({'A': A})
            return df.A.quantile(.25)
# ---
def __call__(self, x: NamedArray, *, key=None):
        k1, k2 = haliax.jax_utils.maybe_rng_split(key, 2)
        x = self.c_fc(x, key=k1)
        x = self.act(x)
        x = self.c_proj(x, key=k2)
        return x
# ---
def shutdown(self):
        """Shutdown the inference context."""
        self.inference_context.shutdown()
# ---
def pytest_configure(config):
    """Register markers related to testimony tokens"""
    for marker in [
        'importance: CaseImportance testimony token, use --importance to filter',
        'component: Component testimony token, use --component to filter',
        'assignee: Assignee testimony token, use --assignee to filter',
    ]:
        config.addinivalue_line("markers", marker)
# ---
def func(block, block_id=None, c=0):
        return nxp.ones_like(block) * int(sum(block_id)) + c
# ---
def test_clip(spec, min, max):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    npa = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    b = xp.clip(a, min, max)
    if min is max is None:
        assert b is a
    else:
        assert_array_equal(b.compute(), np.clip(npa, min, max))
# ---
def test_less_than(self):
        expr = col("score") < 100
        assert expr.evaluate({"score": 50}) is True
        assert expr.evaluate({"score": 100}) is False
        assert expr.evaluate({"score": 150}) is False
# ---
from collections import Counter
def count_common(words):
  word_counts = Counter(words)
  top_four = word_counts.most_common(4)
  return (top_four)
# ---
def _test_syntax(self, cfile_path):
        LOG.info('Testing the new puppet configuration file')
        cmd = "puppet parser validate %s" % cfile_path

        try:
            utils.execute(cmd)
        except exception.ProcessExecutionError as e:
            LOG.warn('Did not pass the configuration syntax test: %s', e)
            raise
# ---
def swish(a: A) -> A:
    return wrap_elemwise_unary(jnn.swish, a)
# ---
def test_clean_text():
    """Test text cleaning (lowercase, punct removal, whitespace norm)."""
    text = "Hello,   World! This is a test."
    expected = "hello world this is a test"
    batch = pa.RecordBatch.from_pydict({"text": [text, None, "   "]})
    pipeline = [Transformation.CleanText(input_col="text", output_col="clean")]
    clean = transform(batch, pipeline)["clean"]
    assert clean[0].as_py() == expected
    assert clean[1].as_py() is None
    assert clean[2].as_py() == ""
# ---
def test_len_without_drop_last(self):
        """Length should count all batches, including partial ones."""
        # Group sizes: 3, 5, 2 -> batches: ceil(3/2)=2, ceil(5/2)=3, ceil(2/2)=1 = 6
        sampler = EquivalenceGroupBatchSampler.from_dataset_sizes(
            dataset_sizes=[3, 5, 2],
            batch_size=2,
            shuffle=False,
            drop_last=False,
        )
        assert len(sampler) == 6
# ---
def Player(self):
        return self.__data['cls_player']
# ---
def visible(self):
        """True if the window is currently visible.  Read-only.

        :type: bool
        """
        return self._visible
# ---
def greater_equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.greater_equal](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.greater_equal.html)
    """
    return jnp.greater_equal(x1, x2)
# ---
def testFunctionDecorator(self):
    self.assertEqual((0, '<b>foo</b>\n'), _GrumpRun(textwrap.dedent("""\
        def bold(fn):
          return lambda: '<b>' + fn() + '</b>'
        @bold
        def foo():
          return 'foo'
        print foo()""")))
# ---
def test_transaction_connection_ctx_rollback(self):
        fn = self._trans_rollback_fn(True)
        conn = testing.db.connect()
        ctx = conn.begin()
        assert_raises_message(
            Exception,
            "breakage",
            testing.run_as_contextmanager, ctx, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def user_personal_website_url(self):
        return self._get_profile().personal_website_url
# ---
def start(self) -> None:
        pass
# ---
def list_workers(limit: int = 10000) -> list[dict[str, Any]]:
    """Get list of Ray workers."""
    result = run_ray_command(
        ["ray", "list", "workers", "--format=json", f"--limit={limit}"],
    )
    return json.loads(result.stdout)
# ---
def test_shard_map_decorator_usage():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    @hax.shard_map(mesh=mesh, check_rep=False)
    def fn(x):
        return x + 5

    x = hax.ones(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), mesh:
        out = fn(x)

    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array + 5)
# ---
def get_internal_type(self):
		return "TextField"
# ---
def test_axis_shapes_multiple_absorbers_error():
    cfg = MeshConfig(axes={"data": -1, "model": -1})
    with pytest.raises(ValueError):
        cfg.axis_shapes(num_devices=8, num_slices=1)
# ---
def check_Equality(s): 
    return (ord(s[0]) == ord(s[len(s) - 1])); 
def count_Substring_With_Equal_Ends(s): 
    result = 0; 
    n = len(s); 
    for i in range(n):
        for j in range(1,n-i+1): 
            if (check_Equality(s[i:i+j])): 
                result+=1; 
    return result;
# ---
def clear_queue(self) -> None:
        """Clear all batches from the queue (for testing/debugging)."""
        pattern = f"{self.path}/*"
        files = self.fs.glob(pattern)

        for file_path in files:
            self.fs.delete(file_path)

        self._batch_counter = 0

        logger.info(f"Cleared queue at {self.path}")
# ---
def __delete__(self, instance):
		del(instance.__dict__[self.field.name])
		setattr(instance, self.field.attname, json.dumps(None))
# ---
def __init__(
        self,
        loss_fn: LossFnWithMask,
        *,
        gradient_weight: float,
        pad_mode: str,
    ):
        self.loss_fn = loss_fn
        self._gradient_weight = gradient_weight
        self._pad_mode = pad_mode
# ---
def get_api_key(user_profile: UserProfile) -> str:
    return user_profile.api_key
# ---
def start(
        self,
        *,
        model_name_or_path: str,
        host: str,
        port: int | None,
        timeout_seconds: int,
        extra_cli_args: list[str] | None,
    ) -> VllmServerHandle:
        raise NotImplementedError
# ---
def compute(input):
            model_output = model(input, attn_mask=attn_mask)
            return hax.nn.softmax(model_output, axis=model.Vocab)
# ---
def binomial_Coeff(n,k): 
    if k > n : 
       return 0
    if k==0 or k ==n : 
        return 1 
    return binomial_Coeff(n-1,k-1) + binomial_Coeff(n-1,k)
# ---
def include_file(path: str):
                return os.path.getmtime(path) > initial_time
# ---
def list_tpu_nodes(project: str, zone: str, filter_expr: str = "") -> list[dict[str, Any]]:
    """List TPU nodes in a zone."""
    cmd = [
        "gcloud",
        "compute",
        "tpus",
        "tpu-vm",
        "list",
        f"--project={project}",
        f"--zone={zone}",
        "--format=json",
    ]
    if filter_expr:
        cmd.append(f"--filter={filter_expr}")

    result = run_gcloud_command(cmd)
    return json.loads(result.stdout)
# ---
def __init__(self, future: Future):
        self._future = future
        self._iterator: Iterator[Any] | None = None
# ---
def setup(bot):
    n = Runescapecompare(bot)
    bot.add_cog(n)
# ---
def test_resolve_audio_pointer():
    AudioTextUrlDataSource.resolve_audio_pointer("https://ccrma.stanford.edu/~jos/mp3/trumpet.mp3", 16_000)
# ---
def __init__(self, amount):
        super().__init__()
        self.amount = amount
# ---
def test_scatter_add():
    B, S, V = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 5)
    x = hax.zeros((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    ones = hax.ones((B, S))
    y = x.at[{V: idx}].add(ones)
    ref = jnp.zeros((2, 3, 5)).at[jnp.arange(2)[:, None], jnp.arange(3)[None, :], idx.array].add(1.0)
    assert jnp.array_equal(y.array, ref)
# ---
def expand_dims(x, /, *, axis):
    if not isinstance(axis, tuple):
        axis = (axis,)
    ndim_new = len(axis) + x.ndim
    axis = validate_axis(axis, ndim_new)

    chunks_it = iter(x.chunks)
    chunks = tuple(1 if i in axis else next(chunks_it) for i in range(ndim_new))

    return map_blocks(
        _expand_dims, x, dtype=x.dtype, chunks=chunks, new_axis=axis, axis=axis
    )
# ---
def test_resolved_param_mapping_inherits_shared():
    cfg = MeshConfig()
    # shared mapping defaults map mlp/heads to model
    mapping = cfg.resolved_param_mapping
    assert mapping["mlp"] == "model"
    assert mapping["heads"] == "model"
    # embed should come from the default param_mapping override
    assert mapping["embed"] == "data"
# ---
def _l2norm(x: NamedArray, axis: hax.AxisSelector, eps: float = 1e-6) -> NamedArray:
    """L2-normalize x along a named axis.

    Args:
        x: NamedArray of any shape.
        axis: the single axis to normalize along (e.g., the head dimension Dk).
    """
    x32 = x.astype(jnp.float32)
    inv = hax.rsqrt(hax.sum(hax.square(x32), axis=axis) + jnp.asarray(eps, dtype=jnp.float32))
    return (x32 * inv).astype(x.dtype)
# ---
def get_carol(n): 
	result = (2**n) - 1
	return result * result - 2
# ---
import re
def text_match(text):
  patterns = 'a.*?b$'
  if re.search(patterns,  text):
    return ('Found a match!')
  else:
    return ('Not matched!')
# ---
def nextafter(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "nextafter")
    if x1.dtype not in _real_floating_dtypes or x2.dtype not in _real_floating_dtypes:
        raise TypeError("Only real floating-point dtypes are allowed in nextafter")
    return elemwise(nxp.nextafter, x1, x2, dtype=x1.dtype)
# ---
def stop(self):
        pass
# ---
def test_tree_broadcast_to_mixed_types():
    from levanter.utils.jax_utils import tree_broadcast_to

    # Test with mixed types
    prefix = {"a": 1, "b": 2}
    target = {"a": [10, 20], "b": {"x": 30, "y": 40}}
    result = tree_broadcast_to(prefix, target)

    assert result == {"a": [1, 1], "b": {"x": 2, "y": 2}}
# ---
def _all_compat(self, other, compat_str):
        """Helper function for equals and identical"""
        # some stores (e.g., scipy) do not seem to preserve order, so don't
        # require matching order for equality
        compat = lambda x, y: getattr(x, compat_str)(y)
        return (self._coord_names == other._coord_names
                and utils.dict_equiv(self._variables, other._variables,
                                     compat=compat))
# ---
def stateful_ops(self):
    """Returns the list of stateful ops in function definition.

    Returns:
      A list of (op.name, op.type) pairs.
    """
    self._create_definition_if_needed()
    return self._stateful_ops
# ---
def get_values(self, env):
        """ return a list of the possible values """
        selection = self.selection
        if isinstance(selection, basestring):
            selection = getattr(env[self.model_name], selection)()
        elif callable(selection):
            selection = selection(env[self.model_name])
        return [value for value, _ in selection]
# ---
def format_names(names):
    return format_line(prefix='names'.rjust(RJUST), values=names)
# ---
def test_permutation_is_deterministic1(PermutationClass):
    length = 4
    prng_key = jrandom.PRNGKey(1)
    permutation = PermutationClass(length, prng_key)
    indices = jnp.arange(length)
    results = permutation(indices)
    prng_key = jrandom.PRNGKey(1)
    permutation = PermutationClass(length, prng_key)
    results2 = permutation(indices)
    assert jnp.all(results == results2)
# ---
def test_impl(df):
            return pd.DataFrame({'B': df.A.str.split(',')})
# ---
def forward(
        self,
        times,  # Float[' b'],
    ):  # -> Float['b d']:
        times = rearrange(times, "b -> b 1")
        rand_proj = torch.addmm(self.proj.bias, times, self.proj.weight.t())
        return torch.cos(2 * pi * rand_proj)
# ---
def run_docker():
        _kill_old_container(name)
        try:
            return _run_command(*docker_cmd)
        except subprocess.CalledProcessError as e:
            logger.exception("Failed to run docker command")
            raise e
# ---
def lang_get(self):
        if self._lang is None:
            # self._lang = self.req.accept_language.best_matches('en-US') if self.req is not None else []
            # Note: Don't forget to add country-less language code when only a "language-COUNTRY" code is given.
            self._lang = ['fr-FR', 'fr']
            if self.req is not None:
                self.req.environ.setdefault('etalage', {})['_lang'] = self._lang
        return self._lang
# ---
def add_router_interface_postcommit(self, context, r_port_context):
        pass
# ---
def _init_weight(key: PRNGKeyArray, shape: tuple[int, ...], std: float) -> Float[Array, "..."]:
    return std * random.truncated_normal(key, -3, 3, shape)
# ---
def maximum(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.maximum(x1, x2)
# ---
def has_local_mods(module, git_path, dest, bare):
    if bare:
        return False

    cmd = "%s status --porcelain" % (git_path)
    rc, stdout, stderr = module.run_command(cmd, cwd=dest)
    lines = stdout.splitlines()
    lines = list(filter(lambda c: not re.search('^\\?\\?.*$', c), lines))

    return len(lines) > 0
# ---
def test_run_inference_raises_for_empty_glob(tmp_path):
    config = InferenceConfig(
        input_path=str(tmp_path),
        output_path=str(tmp_path / "out"),
        model_name="compression",
        model_type="compression",
        attribute_name="test",
    )

    with pytest.raises(FileNotFoundError):
        run_inference(config)
# ---
def test_disclaimer(self):
        eq_(self.record.disclaimer, None)
# ---
def test_aggregate(self):
        yield self.check_aggregate_range, 0, 24*3600
        yield self.check_aggregate_range, 8*3600, 20*3600
        yield self.check_aggregate_range, 13*3600, 14*3600
# ---
def test_prediction_data_test(self, prediction_data, input_data):
        # should always pass on the test data
        prediction_data_test(prediction_data, input_data)
        pass
# ---
def test_get_extra_attributes_parsing(monkeypatch, env_value, expected):
    """Test parsing of IRIS_WORKER_ATTRIBUTES environment variable."""
    monkeypatch.setenv("IRIS_WORKER_ATTRIBUTES", env_value)
    result = _get_extra_attributes()
    assert result == expected
# ---
def create_paths_file(paths: list[str]) -> bytes:
    content = "\n".join(paths) + "\n"
    return gzip.compress(content.encode("utf-8"))
# ---
def check_info(step_info: dict, step: ExecutorStep):
            assert step_info["name"] == step.name
            assert step_info["output_path"] == executor.output_paths[step]
            assert step_info["config"] == asdict_optional(executor.configs[step])
            assert step_info["version"] == executor.versions[step]
# ---
def values(self):
        return self.example_by_step
# ---
def notify_complete(self, result: str) -> str:
        """Called when token passing completes."""
        self.result = result
        self._done.set()
        return "ack"
# ---
def test_broadcast_arrays(executor):
    a = xp.ones(30, chunks=(3,))
    b = xp.ones(30, chunks=(6,))
    a_b, b_b = xp.broadcast_arrays(a, b)

    assert_array_equal(a_b.compute(), np.ones(30))
    assert_array_equal(b_b.compute(), np.ones(30))

    a = xp.ones((1, 30), chunks=(1, 3))
    b = xp.ones(30, chunks=(6,))
    a_b, b_b = xp.broadcast_arrays(a, b)

    assert_array_equal(a_b.compute(executor=executor), np.ones((1, 30)))
    assert_array_equal(b_b.compute(executor=executor), np.ones((1, 30)))
# ---
def _generate_permutation_params(self, rng):
        length = self.length

        if length == 1:
            return 1, 0

        while True:
            a = rng.integers(1, length)
            if np.gcd(a, length) == 1:
                break

        b = rng.integers(0, length)  # b can be in [0, length-1]
        return a, b
# ---
def testWhileElse(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        while False:
          print 'foo'
        else:
          print 'bar'""")))
# ---
def test_engine_connect(self):
        engine = engines.testing_engine()

        tracker = Mock()
        event.listen(engine, "engine_connect", tracker)

        c1 = engine.connect()
        c2 = c1._branch()
        c1.close()
        eq_(
            tracker.mock_calls,
            [call(c1, False), call(c2, True)]
        )
# ---
def on_update(self):
		invalidate_cache_for_item(self)
		self.validate_name_with_item_group()
		self.update_variants()
		self.update_item_price()
		self.update_template_item()
# ---
def _default(self):
		"""
		Change all fields to their default values
		"""
		del self[:]
		self._values = {}
		for col in self.structure:
			char = col.char
			if col.dyn:
				self.append(None)
			elif char == "s":
				self.append("")
			elif char == "f":
				self.append(0.0)
			else:
				self.append(0)
# ---
def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4')]
# ---
def _get_logs(self, limit: int = 50) -> list[LogEntry]:
        with self._logs_lock:
            return list(self._logs)[-limit:]
# ---
def prompt_subreddit(self):
        "Open a prompt to navigate to a different subreddit"

        name = self.term.prompt_input('Enter page: /')
        if name is not None:
            with self.term.loader('Loading page'):
                content = SubredditContent.from_name(
                    self.reddit, name, self.term.loader)
            if not self.term.loader.exception:
                self.selected_subreddit = content
                self.active = False
# ---
def _has_validation_set(self):
        if len(self.validation_urls) > 0:
            return True

        if self.id is not None:
            dataset = datasets.load_dataset(
                self.id, name=self.name, streaming=self.stream, split=self.validation_split
            )
            try:
                next(iter(dataset))
                return True
            except StopIteration:
                return False

        return False
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "LmHeadModel[LmConfigT]":
        """
        Resizes the vocabulary of the model. Key may be provided to use random initialization, otherwise, there
        should be some deterministic initialization of any new parameters.
        """
        pass
# ---
def current_thread_local_mapping():
    """
    Get the current thread-local resource mapping, or None if there is no resource mapping set.
    :return:
    """
    if _mapping_holder.thread_data is None:
        return None
    if not hasattr(_mapping_holder.thread_data, "resource_mapping"):
        return None

    return _mapping_holder.thread_data.resource_mapping
# ---
def train(self, x, y):
        data = dict(zip(self.loss.arguments, [y, x]))
        self.trainer.train_minibatch(data, outputs=[self.loss.output])
# ---
def count_first_elements(test_tup):
  for count, ele in enumerate(test_tup):
    if isinstance(ele, tuple):
      break
  return (count)
# ---
def test_stop(self):
        p = profiler.init("secret", base_id="1", parent_id="2")
        p.stop = mock.MagicMock()
        profiler.stop(info="info")
        p.stop.assert_called_once_with(info="info")
# ---
def __sub__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__sub__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.subtract, self, other, dtype=result_type(self, other))
# ---
def validate(n): 
    for i in range(10): 
        temp = n;  
        count = 0; 
        while (temp): 
            if (temp % 10 == i): 
                count+=1;  
            if (count > i): 
                return False
            temp //= 10; 
    return True
# ---
def get_chain_cluster(chain: ChainInfo, record: Record) -> str:  # noqa: ARG001
    """Get the cluster id for a chain.

    Parameters
    ----------
    chain : ChainInfo
        The chain id to get the cluster id for.
    record : Record
        The record the interface is part of.

    Returns
    -------
    str
        The cluster id of the chain.

    """
    return chain.cluster_id
# ---
def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
# ---
def Embed(self) -> Axis:
        return self.token_embeddings.Embed
# ---
def HiddenEnabled( buffer_object ):
  return bool( int( GetBufferOption( buffer_object, 'hid' ) ) )
# ---
def get(self, key):
        ''' get a specified key'''
        try:
            entry = Yedit.get_entry(self.yaml_dict, key, self.separator)
        except KeyError:
            entry = None

        return entry
# ---
def tearDown(self):
        os.remove(self.path)
# ---
def __init__(self, root, cwd, badfn=None, relativeuipath=False):
        super(alwaysmatcher, self).__init__(
            root, cwd, badfn, relativeuipath=relativeuipath
        )
# ---
def scheduler(state):
    """Create a Scheduler instance."""
    return Scheduler(state)
# ---
def get_dir(self, name: str, description: str) -> Path:
        """Register and return path for a directory artifact."""
        path = self._root / name
        path.mkdir(parents=True, exist_ok=True)
        self._artifacts.append(LogArtifact(path=path, description=description))
        return path
# ---
def list(self, body, ordered=True):
        """Rendering list tags like ``<ul>`` and ``<ol>``.

        :param body: body contents of the list.
        :param ordered: whether this list is ordered or not.
        """
        tag = 'ul'
        if ordered:
            tag = 'ol'
        return '<%s>\n%s</%s>\n' % (tag, body, tag)
# ---
def prepared_geom(self):
        # GEOS internal data structure for prepared geometries grows over time,
        # recreate to limit memory consumption
        if not self._prepared_geom or self._prepared_counter > self._prepared_max:
            self._prepared_geom = shapely.prepared.prep(self.geom)
            self._prepared_counter = 0
        self._prepared_counter += 1
        return self._prepared_geom
# ---
def shard_names(self) -> Sequence[str]:
            return []
# ---
def anypats(self):
        """None of .always(), .isexact(), and .prefix() is true --
        optimizations will be difficult."""
        return not self.always() and not self.isexact() and not self.prefix()
# ---
def done(self, lease: Lease[Any]) -> None:
        with httpx.Client() as client:
            client.post(
                f"http://{self.host}:{self.port}/queues/{self.queue_name}/done",
                json={"lease_id": lease.lease_id, "timestamp": lease.timestamp},
            )
# ---
def test_filter_passing_sorted_by_model_score():
    c1 = _make_candidate("def f(x):\n    return x\n", score=-1.0)
    c2 = _make_candidate("def f(x):\n    return x\n", score=-0.5)
    tests = ["assert f(1) == 1"]
    passing = filter_passing([c1, c2], tests)
    assert len(passing) == 2
    # Higher model score first.
    assert passing[0].score >= passing[1].score
# ---
def create_vllm_inference_config():
    return vLLMInferenceContextConfig(
        model_name="Qwen/Qwen3-0.6B",
        max_model_len=1024,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.90,
        sampling_params=SamplingParams(
            temperature=1.0,
            n=4,
            max_tokens=16,
            logprobs=1,
            stop=None,
            # Workaround for vllm-project/tpu-inference#1386: default top_k forces greedy sampling
            top_k=4096,
        ),
    )
# ---
MAX = 1000000
def breakSum(n): 
	dp = [0]*(n+1) 
	dp[0] = 0
	dp[1] = 1
	for i in range(2, n+1): 
		dp[i] = max(dp[int(i/2)] + dp[int(i/3)] + dp[int(i/4)], i); 
	return dp[n]
# ---
def get_metadata(dtype, chunks, compressor):
    metadata = {}
    if dtype is not None:
        dtype = np.dtype(dtype)
        metadata["dtype"] = encode_dtype(dtype)
    if chunks is not None:
        if isinstance(chunks, int):
            chunks = (chunks,)
        metadata["chunks"] = chunks
    if compressor != "default":
        metadata["compressor"] = compressor
    return metadata
# ---
def unwrap_namedarrays(*a):
    return tuple(x.array if isinstance(x, NamedArray) else x for x in a)
# ---
def read(self, path):
        return False
# ---
def dog_age(h_age):
 if h_age < 0:
 	exit()
 elif h_age <= 2:
	 d_age = h_age * 10.5
 else:
	 d_age = 21 + (h_age - 2)*4
 return d_age
# ---
def __getattr__(self, name):
        """ Access non-slot field attribute. """
        try:
            return self._attrs[name]
        except KeyError:
            raise AttributeError(name)
# ---
def __enter__(self):
        self._prev = jax_config.abstract_mesh_context_manager.swap_local(jax_config.config_ext.unset)
        return self
# ---
import re
def words_ae(text):
 list = re.findall("[ae]\w+", text)
 return list
# ---
def create(self, model: M) -> EmaDecaySqrtModelAveraging[M]:
        return EmaDecaySqrtModelAveraging(
            model=model,
            beta=self.beta,
            switch_step=self.switch_step,
            decay_steps=self.decay_steps,
        )
# ---
import heapq as hq
def heap_queue_smallest(nums,n):
  smallest_nums = hq.nsmallest(n, nums)
  return smallest_nums
# ---
def get_task_status(self, task_name: JobName) -> cluster_pb2.TaskStatus:
        """Get status of a specific task within a job.

        Args:
            task_name: Full task name (/job/.../index)

        Returns:
            TaskStatus proto for the requested task
        """
        task_name.require_task()
        request = cluster_pb2.Controller.GetTaskStatusRequest(task_id=task_name.to_wire())
        response = self._client.get_task_status(request)
        return response.task
# ---
def add(self, other):
        if self.is_alt:
            last_alt = self.alt.parts[-1] + (other,)
            self.alt.parts = self.alt.parts[:-1] + (last_alt,)
        else:
            self.group.seq = self.group.seq + (other,)
# ---
def shard_with_axis_mapping(x: T, mapping: ResourceMapping, mesh: Mesh | None = None) -> T:
    # warnings.warn("`shard_with_axis_mapping` is deprecated. Use `shard` instead", DeprecationWarning)
    return shard(x, mapping, mesh)
# ---
def grayscale():
   global image1
   r, g, b = image1.split()
   image1 = Image.merge("RGB", (g,g,g))

   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def _convert_id_to_token(self, index: int) -> str:
        return str(index)
# ---
def test_repr(self):
        expr = col("a") + col("b")
        assert repr(expr) == "(col('a') + col('b'))"
# ---
def wait_ready(self, count: int | None = None, timeout: float = 300.0) -> list[ActorHandle]:
        """Return ready actor handles. Ray actors are ready immediately."""
        if count is None:
            count = len(self._handles)
        self._yielded = True
        return self._handles[:count]
# ---
def test_get_lines_with_short_string():
        assert len(get_lines("a"*(LINEWIDTH-1))) == 1
# ---
def fake_remove_from_aggregate(context, aggregate, host):
            fake_remove_from_aggregate.called = True
# ---
def avatarEnterNextState(self):
        if len(self.nearbyAvatars) == 1:
            if self.fsm.getCurrentState().getName() != 'Walk':
                self.fsm.request('Chatty')
            else:
                self.notify.debug('avatarEnterNextState: in walk state')
        else:
            self.notify.debug('avatarEnterNextState: num avatars: ' + str(len(self.nearbyAvatars)))
# ---
def sinh(a: A) -> A:
    return wrap_elemwise_unary(jnp.sinh, a)
# ---
def astype(x, dtype, /, *, copy=True, device=None):
    if not copy and dtype == x.dtype:
        return x
    return map_blocks(_astype, x, dtype=dtype, astype_dtype=dtype)
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"blocks": "h"}
# ---
def _get_caption(self, key: str, name: str) -> str:
        if name in self._metadata:
            caption_name = self._metadata[name]["long_name"]
            units = self._metadata[name]["units"]
        else:
            caption_name, units = name, "unknown_units"
        caption = self._captions[key].format(name=caption_name, units=units)
        return caption
# ---
def go(conn, *args, **kw):
                canary.append(name)
# ---
def callback(self, r):
        print("Iteration %d completed at %s" %
              (self.cur_iter, datetime.now().strftime("%d/%m/%Y %H:%M:%S")))
        self.cur_iter += 1
# ---
def set_default_settings(settings: QSettingsManager):
    settings.set_defaults({
        DECIMAL_SETTING: ',',
        SEPARATOR_SETTING: ';'
    })
# ---
def get_witness_script(witness_root, witness_nonce):
    witness_commitment = uint256_from_str(hash256(ser_uint256(witness_root) + ser_uint256(witness_nonce)))
    output_data = WITNESS_COMMITMENT_HEADER + ser_uint256(witness_commitment)
    return CScript([OP_RETURN, output_data])
# ---
def batch_completions(self, prompts, temperature, n, max_tokens=None, stop=None, system_prompt=None, top_k=None):
        """Return mock completions for each prompt."""
        return [create_mock_chat_completion(self.tokenizer) for prompt in prompts]
# ---
def model_type(self) -> Type["ApertusLMHeadModel"]:
        return ApertusLMHeadModel
# ---
def discover_hf_checkpoints(base_path: str):
    """
    Discover the Hugging Face checkpoints in the given path, sorted by the last modified time. (Most recent last)
    Args:
        base_path:  Fsspec Path to the directory containing the checkpoints, possibly in nested directories.
    Returns:
        List of paths to the checkpoints, sorted by the last modified time.
    """

    return discover_checkpoints(base_path, "**/config.json", ["config.json", "tokenizer_config.json"])
# ---
def test_mean_pool3d():
    _x = jnp.arange(64).reshape(4, 4, 4)
    x = hax.named(_x, ("H", "W", "D"))
    output = mean_pool((hax.Axis("H", 1), hax.Axis("W", 3), hax.Axis("D", 1)), x, stride=2)

    answer = jnp.array([[[4, 6]], [[36, 38]]])

    assert jnp.all(output.array == answer)
# ---
def i_duplicate_the_selected_objects():
    bpy.ops.object.duplicate_move()
    blenderbim.bim.handler.active_object_callback()
# ---
def do_eval_lm(config: LevanterEvalLmConfig) -> None:
    """
    Visualizes log probabilities of a language model.

    Args:
        config (EvalLmConfig): The configuration for visualizing log probabilities.
    """
    # Levanter can read `gs://` checkpoints directly via fsspec/tensorstore, and HF
    # checkpoints via fsspec as well. Avoid staging large directories locally.
    eval_lm_main(config)
# ---
def distributed_work():
            from iris.cluster.client import get_job_info

            info = get_job_info()
            if info is None:
                raise RuntimeError("Not running in an Iris job context")
            print(f"Task {info.task_index} of {info.num_tasks} on worker {info.worker_id}")
            return f"Task {info.task_index} done"
# ---
def eval_metrics(self):
    eval_metrics = [
        metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5,
        metrics.Metrics.ACC_PER_SEQ, metrics.Metrics.NEG_LOG_PERPLEXITY
    ]
    if self._was_reversed:
      eval_metrics += [metrics.Metrics.IMAGE_SUMMARY]
    return eval_metrics
# ---
def setup_class(cls):
        from sqlalchemy.engine import base, default
        cls.engine = engine = testing_engine('sqlite://')
        m = MetaData()
        cls.table = t = Table('test', m,
            Column('x', Integer, primary_key=True),
            Column('y', String(50, convert_unicode='force'))
        )
        m.create_all(engine)
        engine.execute(t.insert(), [
            {'x':i, 'y':"t_%d" % i} for i in range(1, 12)
        ])
# ---
def register_transfer_server(self, transfer_server_address: str):
        """
        Register the actual JAX transfer server address with the coordinator.
        Called by the server when it starts up.
        """
        self.transfer_server_address = transfer_server_address
# ---
def on_validation_start(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,
    ) -> None:
        """Use EMA weights for validation.

        Parameters
        ----------
        trainer: Trainer
            The Trainer instance.
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._on_eval_start(pl_module)
# ---
def device(self):
        return next(self.score_model.parameters()).device
# ---
def sort_by_dnf(arr, n):
  low=0
  mid=0
  high=n-1
  while mid <= high:
    if arr[mid] == 0:
      arr[low], arr[mid] = arr[mid], arr[low]
      low = low + 1
      mid = mid + 1
    elif arr[mid] == 1:
      mid = mid + 1
    else:
      arr[mid], arr[high] = arr[high], arr[mid]
      high = high - 1
  return arr
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None:
        return None
# ---
def tobytes(self, order="C") -> Any:  # pragma: no cover
        return self.array.tobytes(order=order)
# ---
def _make_serializable(obj):
            """Recursively convert non-serializable objects (like Enums) to serializable forms."""
            if isinstance(obj, Enum):
                return obj.name
            elif isinstance(obj, dict):
                return {k: _make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [_make_serializable(v) for v in obj]
            return obj
# ---
def lower(self, *args, **kwargs) -> jax.stages.Lowered:
        return self._call(True, *args, **kwargs)
# ---
def log_metrics(
    metrics: typing.Mapping[str, LoggableValue | Any], *, step: Optional[int], commit: Optional[bool] = None
):
    """
    Deprecated. Use log instead.
    """
    warnings.warn("log_metrics is deprecated in favor of log", DeprecationWarning)
    log(metrics, step=step, commit=commit)
# ---
def create_and_sign_transaction(self, spend_tx, value, script=CScript([OP_TRUE])):
        tx = self.create_tx(spend_tx, 0, value, script)
        self.sign_tx(tx, spend_tx)
        tx.rehash()
        return tx
# ---
def _event_wrapper(f):
        f._platform_event = True
        if not hasattr(f, '_platform_event_data'):
            f._platform_event_data = []
        f._platform_event_data.append(data)
        return f
# ---
def test_iadd(self, evt):
        rval = event.ReturnValue(append_events=[evt])
        rval2 = event.ReturnValue(eat=True, append_events=[evt])
        rset = event.ResultSet()

        rset += rval
        rset += rval2
        rset += None
        assert rset.eat
        assert rset.append_events == [evt, evt]
# ---
def decode(self, token_ids, skip_special_tokens=True):
        words = []
        for tid in token_ids:
            token = self.TOKENS[tid]
            if skip_special_tokens and token in (self.bos_token, self.eos_token):
                continue
            words.append(token)
        return "".join(words)
# ---
def l3(*arg, **kw):
            canary.append("l3")
# ---
def teardown(self):
        self.drain_actor_pool()
        self._slice_info = None
# ---
def physical_axis_name(axis: AxisSelector, mapping: ResourceMapping | None = None) -> PhysicalAxisSpec | None:
    """Get the physical axis name for a logical axis from the mapping. Returns none if the axis is not mapped."""
    if mapping is None:
        mapping = current_thread_local_mapping()
    if mapping is None:
        return None
    elif isinstance(axis, str):
        return mapping.get(axis, None)
    else:
        return mapping.get(axis.name, None)
# ---
def init_fn(key):
        return hax.nn.MLP.init(In, Out, 2, 1, key=key, use_bias=False, use_final_bias=False)
# ---
def transform_to(self, srs):
        if srs == self.srs:
            return self

        bbox = self.srs.transform_bbox_to(srs, self.bbox)
        return BBOXCoverage(bbox, srs)
# ---
def __exit__(self, *args):
        """Leave context: close port"""
        self.close()
        self.closed.wait()
# ---
def __call__(
        self,
        model: M_con,
        *inputs: X,
        reduction: Optional[hax.ReductionFunction] = cast(Optional[hax.ReductionFunction], hax.mean),
        reduction_axis: Optional[hax.AxisSelection] = None,
        **kwargs,
    ) -> Scalar | hax.NamedArray: ...
# ---
def axis_indices(self, axis: AxisSelector) -> int | None:
        name = axis_name(axis)
        for i, ax in enumerate(self.axes):
            if ax.name == name:
                return i
        return None
# ---
def slice_is_terminal(slice_info: vm_pb2.SliceInfo) -> bool:
    """Compute is_terminal from vms[] in proto."""
    terminal = {
        vm_pb2.VM_STATE_READY,
        vm_pb2.VM_STATE_FAILED,
        vm_pb2.VM_STATE_TERMINATED,
        vm_pb2.VM_STATE_PREEMPTED,
    }
    return all(vm.state in terminal for vm in slice_info.vms)
# ---
def normalize_answer(answer: str | None) -> str | None:
    if answer is None:
        return None
    answer = answer.strip()
    try:
        # Remove enclosing `\text{}`.
        m = re.search("^\\\\text\\{(?P<text>.+?)\\}$", answer)
        if m is not None:
            answer = m.group("text").strip()
        return _strip_string(str(answer))
    except Exception:
        return answer
# ---
def _reduce_buckets(bucket: str, items: Iterator[CCInput]) -> BucketWithIds:
    # TODO: do we want/need this optimization?
    # if len(all_items) <= 1:
    #    return None  # No duplicates in this bucket
    return {
        "bucket": bucket,
        "ids": [RecordId(record_id=item["id"], record_id_norm=_internal_orderable_id(item["id"])) for item in items],
    }
# ---
def tetrahedral_number(n): 
	return (n * (n + 1) * (n + 2)) / 6
# ---
import itertools
def remove_duplicate(list1):
 list.sort(list1)
 remove_duplicate = list(list1 for list1,_ in itertools.groupby(list1))
 return remove_duplicate
# ---
def match(self, item):
        return all(q.match(item) for q in self.subqueries)
# ---
def minute_to_hm(minute):
    if isinstance(minute, int):
        return "%d:%02d" % (divmod(minute, 60))
    return None
# ---
def __init__(self, matchers):
        m1 = matchers[0]
        super(unionmatcher, self).__init__(m1._root, m1._cwd)
        self.traversedir = m1.traversedir
        self._matchers = matchers
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            return df.A.mean()
# ---
def __init__(
        self,
        dataset: AsyncDataset[T],
        fn: MapFunction[U],
        *extra_args,
        **extra_kwargs,
    ):
        super().__init__()
        self.dataset = dataset
        self.fn = fn
        self._extra_args = extra_args
        self._extra_kwargs = extra_kwargs
# ---
def show_progress(self):
        self.command('show_progress')
# ---
def max_val(listval):
     max_val = max(i for i in listval if isinstance(i, int)) 
     return(max_val)
# ---
def plot_adjusted_rand_index_vs_assigned_bps(colors, summary_per_query, labels, output_dir, rank=None):
    plot_summary(colors,
                 summary_per_query,
                 labels,
                 output_dir,
                 rank,
                 'p',
                 'ari_vs_assigned_bps',
                 'Adjusted Rand index',
                 'Percentage of binned base pairs')
# ---
def to_hf_config(self) -> tuple[float, dict]:
        return self.theta, {
            "factor": self.factor,
            "low_freq_factor": self.low_freq_factor,
            "high_freq_factor": self.high_freq_factor,
            "original_max_position_embeddings": self.original_max_position_embeddings,
            "rope_type": "llama3",
        }
# ---
def unset_updated_pending_status(self, process):
        process.updated_pending_status = False
        for connection_status in process.connection_status.values():
            connection_status['updated_pending_status'] = False
# ---
def __repr__(self):
        return "<" + ", ".join(repr(v) for v in self.values) + ">"
# ---
def _worker_loop(self) -> None:
        """Main worker loop for background data loading."""
        while not self._stop_event.is_set():
            try:
                self._collect_rollouts()
                time.sleep(self.rollout_fetch_interval)
            except Exception as e:
                logger.error(f"Error in ReplayDataLoader worker loop: {e}", exc_info=True)

            self._stop_event.wait(self.rollout_fetch_interval)
# ---
def init(named):
            return Module(w=named)
# ---
def _loss_function(model, batch, key):
            return loss_fn(model, batch, key)
# ---
def __init__(self, coverages):
        self.coverages = coverages
        self.bbox = self.extent.bbox
# ---
def _needs_long_sequence_workaround(self):
        if isinstance(self.tokenizer, PreTrainedTokenizerFast):
            normalizer = self.tokenizer.backend_tokenizer.normalizer
            if normalizer is None:
                return False
            return isinstance(normalizer, (normalizers.Replace, normalizers.Sequence))
        return False
# ---
def from_spec(cls, query_params: dict[str, list[str]]) -> "RayCluster":
        namespace = None
        config_path = None
        address = "auto"

        if "namespace" in query_params:
            namespace = query_params["namespace"][0]

        if "cluster" in query_params:
            config_path = find_config_by_region(query_params["cluster"][0])

        return cls(address=address, config_path=config_path, namespace=namespace)
# ---
def test_rescue(self):
        instance = self._create_instance()
        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass')
        vm = vm_utils.VMHelper.lookup(session, instance.name)
        vbd = xenapi_fake.create_vbd(vm, None)
        conn = xenapi_conn.get_connection(False)
        image_meta = {'id': glance_stubs.FakeGlance.IMAGE_VHD,
                      'disk_format': 'vhd'}
        conn.rescue(self.context, instance, [], image_meta)
# ---
def _normalize(name):
    '''Transform "Firstname [Middlenames] Lastname" into
    "Lastname, Firstname [Middlenames]".'''
    split = name.split()
    if len(split) == 1:
        return name
    return split[-1] + ', ' + ' '.join(name[0:-1])
# ---
def extract_freq(test_list):
  res = len(list(set(tuple(sorted(sub)) for sub in test_list)))
  return (res)
# ---
import re
def split_list(text):
  return (re.findall('[A-Z][^A-Z]*', text))
# ---
def test_set_mesh_defaults_to_explicit_axis_types():
    mesh = _build_explicit_mesh()
    with set_mesh(mesh):
        abstract = get_abstract_mesh()
        assert abstract is not None
        assert abstract.axis_types == (AxisType.Explicit,)
# ---
def _get_dolmino_split_paths(split: str):
    """Helper to get file paths for a dolmino split."""
    patterns = DOLMINO_DATASETS[split]
    dolmino_split_input_base_path = _dolmino_base_dir / split
    return [dolmino_split_input_base_path / pattern for pattern in patterns]
# ---
def test_pspec_for_plain_array_axis_names():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        mod = ArrayModule(jnp.ones((Dim2.size, Dim3.size)))

        specs: ArrayModule = pspec_for(mod)

        assert specs.arr == PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)
# ---
def _compute_shard_names(self):
        dataset = self._load_dataset()
        if isinstance(dataset, datasets.IterableDataset):
            try:
                return [str(i) for i in range(dataset.n_shards)]
            except NotImplementedError:
                return ["data"]
        else:
            return ["data"]
# ---
def test_t():
    param = hax.arange(Width, start=0.1)
    check_gen_is_equal(lambda k, s: jax.random.t(k, param.array, shape=s), lambda k, s: hax.random.t(k, s, param))

    check_gen_is_equal(lambda k, s: jax.random.t(k, 0.5, shape=s), lambda k, s: hax.random.t(k, s, 0.5))
# ---
def _health(self, _request: Request) -> JSONResponse:
        """Simple health check endpoint for bootstrap and load balancers."""
        return JSONResponse({"status": "healthy"})
# ---
import heapq as hq
def heap_sort(iterable):
    h = []
    for value in iterable:
        hq.heappush(h, value)
    return [hq.heappop(h) for i in range(len(h))]
# ---
def encodes(text):

    bext = text.encode(encoding="utf-8")
    enc_bext = codecs.encode(bext, "hex_codec")

    return enc_bext.decode("utf-8")
# ---
def __init__(self, model_name: str, attribute_name: str, *args, **kwargs):
        self.model_name = model_name
        self.attribute_name = attribute_name
# ---
def reducer(key, items):
        items_list = list(items)
        if len(items_list) > 1:
            return key
        return None
# ---
def vms(self) -> list[ManagedVm]:
        """Individual VM instances in this group."""
        ...
# ---
def find_Divisor(x,y):  
    if (x==y): 
        return y 
    return 2
# ---
def batched(iterable, n):
    # batched('ABCDEFG', 3) --> ABC DEF G
    if n < 1:
        raise ValueError("n must be at least one")
    it = iter(iterable)
    while batch := tuple(islice(it, n)):
        yield batch
# ---
def validate_retain_sample(self):
		if self.retain_sample and not frappe.db.get_single_value('Stock Settings', 'sample_retention_warehouse'):
			frappe.throw(_("Please select Sample Retention Warehouse in Stock Settings first"))
		if self.retain_sample and not self.has_batch_no:
			frappe.throw(_(" {0} Retain Sample is based on batch, please check Has Batch No to retain sample of item").format(
				self.item_code))
# ---
def poll(self, job_id: JobId) -> JobInfo:
        return self._get_job(job_id).get_info()
# ---
def rearrange(self, axes: Sequence[AxisSelector | EllipsisType]) -> "NamedArray":
        """See [haliax.rearrange][] for details."""
        pass
# ---
def coerce_depth(ndim, depth):
    default = 0
    if depth is None:
        depth = default
    if isinstance(depth, Integral):
        depth = (depth,) * ndim
    if isinstance(depth, tuple):
        depth = dict(zip(range(ndim), depth))
    if isinstance(depth, dict):
        depth = {ax: depth.get(ax, default) for ax in range(ndim)}
    return coerce_depth_type(ndim, depth)
# ---
def __repr__json__(self):
        return json.dumps(self.__repr__dict__())
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["Gpt2Config"]:  # type: ignore
        # We trust this code because it's in our hub repo
        return HFCheckpointConverter(self.__class__, reference_checkpoint="gpt2", ignore_prefix="transformer")
# ---
def __iter__(self):
        return iter(self._queue)
# ---
def the_object_name_is_not_voided(name):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(element.HasOpenings):
        assert False, "An opening was found"
# ---
def assert_diff_version(**kwargs):
            output_path = get_output_path(**(defaults | kwargs))
            assert output_path != default_output_path
# ---
def test_broadcast_to(
    executor, shape, chunks, new_shape, new_chunks, new_chunks_expected
):
    x = np.random.randint(10, size=shape)
    a = xp.asarray(x, chunks=chunks)
    b = xp.broadcast_to(a, shape=new_shape, chunks=new_chunks)

    assert b.shape == new_shape
    assert b.chunks == new_chunks_expected
    assert_array_equal(b.compute(executor=executor), np.broadcast_to(x, new_shape))
# ---
def generate_id(line: str, line_number: str) -> str:
    """
    Generate a unique ID for a line based on its content and line number.

    Args:
        line (str): The content of the line.
        line_number (int): The line number in the file.

    Returns:
        str: A SHA-256 hash hexdigest representing the unique ID.
    """
    unique_string = f"{line_number}:{line}"
    hash_object = hashlib.sha256(unique_string.encode("utf-8"))

    return hash_object.hexdigest()
# ---
def combine_lora_params(params: M, lora_params: M) -> M:
    """
    Combines the given LoRA parameters with the given parameter tree.
    """
    return eqx.combine(params, lora_params, is_leaf=is_lora_param)
# ---
def __post_init__(self):
        if np.any(self.lat < -90.0) or np.any(self.lat > 90.0):
            raise ValueError("lat values are expected to be between -90 and 90.")

        if np.any(self.lng < 0.0) or np.any(self.lng > 360.0):
            raise ValueError("lng values are expected to be between 0 and 360 degrees.")

        self._days_since_start_field.ensure_value_fits(self.days_since_start)
# ---
def _tool_call_payload(tool_call: ToolCall) -> dict[str, object]:
    """Minimal JSON payload for embedding in <tool_call> blocks."""
    # Convert from nested structure to flat format for compatibility
    return {
        "name": tool_call.function.name,
        "args": json.loads(tool_call.function.arguments),
    }
# ---
def test_synthetic_subtrees_nonempty(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=10)
    assert len(entries) > 0
# ---
def find_repo_root(start: Path | None = None) -> Path:
    """Return the nearest parent containing a .git directory."""
    here = (start or Path(__file__)).resolve()
    for p in [here] + list(here.parents):
        if (p / ".git").exists():
            return p
    return Path.cwd()
# ---
def needs_space_before(self, other: "LatexNode") -> bool:
        """Determine if a space is needed before the other node."""
        return False
# ---
def __call__(self, indices: np.ndarray) -> np.ndarray: ...
# ---
def identity_func(a, **kwargs):
        return a
# ---
def is_integer(i) -> bool:
    """
    >>> is_integer(6)
    True
    >>> is_integer(42.0)
    True
    >>> is_integer('abc')
    False
    """
    return isinstance(i, Integral) or (isinstance(i, float) and i.is_integer())
# ---
def _get_vocab_size(pretraining_data):
    tokenizer = unwrap_versioned_value(pretraining_data.tokenizer)
    vocab_size = _cached_load_tokenizer(tokenizer).vocab_size
    return vocab_size
# ---
def _r1_is_too_big(R1):
    array_mem = array_memory(R1.dtype, R1.shape)
    # conservative values for max_mem (4 copies, doubled to give some slack)
    max_mem = (R1.spec.allowed_mem - R1.spec.reserved_mem) // (4 * 2)
    return array_mem > max_mem
# ---
def init(config: Gpt2HyenaConfig, *, key):
        # vectorize the blocks
        blocks = Stacked.init(config.Layers, Gpt2HyenaBlock, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = hnn.LayerNorm.init(config.Embed, eps=config.layer_norm_epsilon, use_bias=config.hyena.use_bias)

        return Gpt2HyenaBackbone(config, blocks, ln_f)
# ---
def get_rank_points(self, nick):
        return self.data[nick.lower()]['rank_points']
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> KvPageCache:
        return self.self_attn.empty_page_cache(spec, dtype=dtype)
# ---
def _load_environment(self, lesson_id: str) -> MarinEnv:
        """Load environment from lesson ID."""
        if lesson_id in self._environments:
            return self._environments[lesson_id]

        lesson_config = self.config.curriculum_config.lessons[lesson_id]
        env = load_environment_from_spec(lesson_config.env_config)
        self._environments[lesson_id] = env
        return env
# ---
def count_element_in_list(list1, x): 
    ctr = 0
    for i in range(len(list1)): 
        if x in list1[i]: 
            ctr+= 1          
    return ctr
# ---
def min_of_two( x, y ):
    if x < y:
        return x
    return y
# ---
def is_remote_path(path: str) -> bool:
    """
    Checks if the given path is a remote path, e.g., Google Cloud Storage (GCS) path.
    """
    fs, _ = fsspec.core.url_to_fs(path)
    return not isinstance(fs, LocalFileSystem)
# ---
def _ingest_outputs(self, outputs: _DecodeOutputs | None) -> int:
        """Drain device outputs into host results and apply host-side release.

        Returns the number of tokens appended to results. No-op if outputs is None.
        """
        if outputs is None:
            return 0
        appended = self._extract_outputs(outputs)
        return appended
# ---
def on_mouse_enter(x, y):
            """The mouse was moved into the window.

            This event will not be trigged if the mouse is currently being
            dragged.

            :Parameters:
                `x` : int
                    Distance in pixels from the left edge of the window.
                `y` : int
                    Distance in pixels from the bottom edge of the window.

            :event:
            """
# ---
def __repr__(self):
        return "<recursivematcher %r>" % self._matcher
# ---
def test_impl(df):
            B = df.A.str.split(',')
            C = pd.to_numeric(B.str.get(1), errors='coerce')
            return C
# ---
def _split(self, path):
        if "/" in path:
            subdir, rest = path.split("/", 1)
        else:
            subdir, rest = path, ""
        if not subdir:
            raise error.ProgrammingError("path cannot be absolute")
        return subdir, rest
# ---
def assert_trees_not_close(a, b):
    try:
        assert_trees_all_close(jax.tree_util.tree_leaves(arrays_only(a)), jax.tree_util.tree_leaves(arrays_only(b)))
    except AssertionError:
        pass
    else:
        raise AssertionError("Trees are equal")
# ---
def fake_finish_revert_migration(*args, **kwargs):
            self.fake_finish_revert_migration_called = True
# ---
def test_init(self):
        """
        The initializer updates the instance's C{__dict__} with its
        keyword arguments.
        """
        namespace = _api._SimpleNamespace(x=1)
        assert namespace.x == 1
# ---
def left_rotate(s,d):
    tmp = s[d : ] + s[0 : d]
    return tmp
# ---
def default() -> "SeqDecodingParams":
        """
        Returns a default SeqDecodingParams with the given number of stop sequences and maximum stop tokens.
        """
        max_int_jnp = jnp.iinfo(jnp.int32).max
        return SeqDecodingParams(
            max_num_tokens=jnp.array(max_int_jnp - 100000, dtype=jnp.int32),
            stop_tokens=None,
            temperature=jnp.array(0.0, dtype=jnp.float32),
            key=jax.random.PRNGKey(0),
        )
# ---
def peek(self) -> T_co | None:
        self._recover_expired_leases()
        if self.queue:
            return self.queue[0]
        return None
# ---
def _get_one_lb_info(self, line_all, line_index, line_total):
        value = []

        for i in range(line_index, line_total):
            line = line_all[i]

            if line.startswith('\t'):
                value.append(line)
            elif line.startswith('listen'):
                return i, value

        return line_total - 1, value
# ---
def trace_hide_args_func(a, i=10):
    return (a, i)
# ---
def get_Odd_Occurrence(arr,arr_size):   
    for i in range(0,arr_size): 
        count = 0
        for j in range(0,arr_size): 
            if arr[i] == arr[j]: 
                count+=1     
        if (count % 2 != 0): 
            return arr[i]     
    return -1
# ---
def _read_ovf_from_ova_dir(ova_path):
    files = os.listdir(ova_path)
    name = _find_ovf(files)
    if name is not None:
        with open(os.path.join(ova_path, name), 'r') as ovf_file:
            return ovf_file.read()
    raise ClientError('OVA directory %s does not contain ovf file' % ova_path)
# ---
def parse_def_links(self, m):
        key = _keyify(m.group(1))
        self.def_links[key] = {
            'link': m.group(2),
            'title': m.group(3),
        }
# ---
def parallelogram_area(b,h):
  area=b*h
  return area
# ---
def eot_token_id(self) -> int:
        """Return the end-of-text token ID."""
        return self.tokenizer.eos_token_id
# ---
def getComboBoxList(self, combobox):
        """Get the list of values from the active combox
        """
        active = combobox.get_active()
        model = combobox.get_model()
        activeIter = model.get_iter(active)
        activeLabel = model.get_value(activeIter, 0)
        activeName = model.get_value(activeIter, 1)
        return [activeLabel, activeName]
# ---
def unbounded_config() -> config_pb2.ScaleGroupConfig:
    """A scale group with no min/max constraints."""
    return config_pb2.ScaleGroupConfig(
        name="unbounded-group",
        min_slices=0,
        max_slices=100,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def genetic_modification_2(lab, award):
    return {
        'modification_type': 'deletion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'modification_description': 'some description',
        'modification_zygocity': 'homozygous',
        'modification_purpose': 'tagging',
        'modification_treatments': [],
        'modification_genome_coordinates': [{
            'chromosome': '11',
            'start': 5309435,
            'end': 5309451
            }]
    }
# ---


def strlen(string: str) -> int:
    """ Return length of given string
    >>> strlen('')
    0
    >>> strlen('abc')
    3
    """
    return len(string)
# ---
def _wrap_up(self, o: torch.Tensor, q_x: torch.Tensor) -> torch.Tensor:
        if self.linear_g is not None:
            g = self.sigmoid(self.linear_g(q_x))

            # [*, Q, H, C_hidden]
            g = g.view(g.shape[:-1] + (self.no_heads, -1))
            o = o * g

        # [*, Q, H * C_hidden]
        o = flatten_final_dims(o, 2)

        # [*, Q, C_q]
        o = self.linear_o(o)

        return o
# ---
def finish(self):
        summary = {**self._summary_metrics, **self._last_metrics}
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "finish",
                "summary": summary,
            }
        )
        self.logger.info(json.dumps(record))
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            df['moving average'] = df.A.rolling(window=5, center=True).mean()
            return df['moving average'].sum()
# ---
def sort_by_name(self, reverse=False):
        Util.validate_type(reverse, "bool")
        return self._sort_by_name(reverse)
# ---
def __init__(self, selection=None, string=None, **kwargs):
        if callable(selection):
            from openerp import api
            selection = api.expected(api.model, selection)
        super(Selection, self).__init__(selection=selection, string=string, **kwargs)
# ---
def load_checkpoint(
        self,
        state: M,
        path: Optional[PathLike] = None,
        *,
        discover_latest: bool = True,
        axis_mapping: Optional[haliax.partitioning.ResourceMapping] = None,
        mesh: Optional[haliax.partitioning.Mesh] = None,
    ) -> Optional[M]:
        if path is None:
            path = self.base_path
        return load_checkpoint(state, path, discover_latest=discover_latest, axis_mapping=axis_mapping, mesh=mesh)
# ---
def get_item(chunks: T_RectangularChunks, idx: Tuple[int, ...]) -> Tuple[slice, ...]:
    """Convert a chunk index to a tuple of slices."""
    # could use Dask's cached_cumsum here if it improves performance
    starts = tuple(_cumsum(c, initial_zero=True) for c in chunks)
    loc = tuple((start[i], start[i + 1]) for i, start in zip(idx, starts))
    return tuple(slice(*s, None) for s in loc)
# ---
def from_ref(ref: ray.ObjectRef, name: str) -> "RayJobInfo":
        return RayJobInfo(ref=ref, submission_id=None, name=name)
# ---
def vf_env():
    """Create a real verifiers SingleTurnEnv with example dataset."""
    dataset = vf.load_example_dataset("gsm8k", n=2)
    return vf.SingleTurnEnv(dataset=dataset)
# ---
def _apply_replacements(text: str, replacements: dict[str, str]) -> str:
    updated = text
    for old, new in replacements.items():
        updated = updated.replace(old, new)
    return updated
# ---
def scheme(self):
        if (
            self.app.websocket_enabled
            and self.headers.get("upgrade") == "websocket"
        ):
            scheme = "ws"
        else:
            scheme = "http"

        if self.transport.get_extra_info("sslcontext"):
            scheme += "s"

        return scheme
# ---
def remove(self, container_id: str) -> None:
        self._containers.pop(container_id, None)
# ---
def cleanup(self) -> None:
        """Cleanup Flight client resources."""
        try:
            logger.info("Shutting down Arrow Flight client thread pool...")
            self._receive_pool.shutdown(wait=False, cancel_futures=False)
            logger.info("Thread pool shutdown completed")
        except Exception as e:
            logger.warning(f"Error shutting down thread pool: {e}")
# ---
def mock_requests_get(url, **kwargs):
        from unittest.mock import Mock

        response = Mock()
        response.status_code = 200
        response.headers = {"content-length": str(len(compressed_data))}
        response.raw = BytesIO(compressed_data)
        return response
# ---
def read_local(self):
        '''
        Read in the local private keys, return an empy dict if the keys do not
        exist
        '''
        path = os.path.join(self.opts['pki_dir'], 'local.key')
        if not os.path.isfile(path):
            return {}
        with salt.utils.fopen(path, 'rb') as fp_:
            return self.serial.loads(fp_.read())
# ---
import re
def num_position(text):
 for m in re.finditer("\d+", text):
    return m.start()
# ---
def get_url(self):
        '''return the dsn back into url form'''
        return urlparse.urlunparse((
            self.scheme,
            self.netloc,
            self.path,
            self.params,
            self.query_str,
            self.fragment,
        ))
# ---
def check_load_config(config_class, config_file):
    try:
        draccus.parse(config_class, config_file, args=[])
    except Exception as e:
        raise Exception(f"failed to parse {config_file}") from e
# ---
def speedrun_paloma_tokenized(tokenizer: str = llama3_tokenizer):
    return paloma_tokenized(base_path="raw/paloma-speedrun", tokenizer=tokenizer, paloma_raw=paloma_speedrun)
# ---
def init(max_queued_tokens: int) -> "TokenQueue":
        """Create a ``JitScheduler`` with empty buffers."""
        return TokenQueue(
            queued_tokens=hax.full({"position": max_queued_tokens}, INVALID, dtype=jnp.int32),
            queued_slot_ids=hax.full({"position": max_queued_tokens}, INVALID, dtype=jnp.int32),
            queued_pos_ids=hax.full({"position": max_queued_tokens}, INVALID, dtype=jnp.int32),
            num_queued_tokens=jnp.array(0, dtype=jnp.int32),
        )
# ---
def init_app(self, app):
        app.config.setdefault('ELASTICSEARCH_URL', 'http://localhost:9200/')
        app.config.setdefault('ELASTICSEARCH_INDEX', 'eve')

        self.index = app.config['ELASTICSEARCH_INDEX']
        self.es = get_es(app.config['ELASTICSEARCH_URL'])

        self.create_index(self.index)
        self.put_mapping(app)
# ---
def start(self):
        """Start the inference and batch processing threads"""
        logger.info("Starting inference context...")
        self.inference_thread.start()
        self.batch_thread.start()
# ---
def __init__(self):
            super().__init__()
            self.m = M()
            self.c = self.m.a
# ---
def _with_dtype(axes: NamedArrayAxes, dtype):
        """Attach dtype to axes metadata if not already set."""
        return axes if axes.dtype is not None else replace(axes, dtype=dtype)
# ---
def the_object_name_is_not_in_the_collection_collection(name, collection):
    assert collection not in [c.name for c in the_object_name_exists(name).users_collection]
# ---
def from_state_dict(self: M, state_dict: StateDict, prefix: str | None = None) -> M:
        out_blocks = []
        for i, block in enumerate(self.blocks):
            my_prefix = with_prefix(prefix, str(i))
            block = block.from_state_dict(state_dict, my_prefix)
            out_blocks.append(block)

        return eqx.tree_at(lambda m: m.blocks, self, out_blocks)
# ---
def largest_neg(list1): 
    max = list1[0] 
    for x in list1: 
        if x < max : 
             max = x  
    return max
# ---
def is_in_preformatted(el):
            return el.name == "pre" or el.find_parent("pre")
# ---
def test_lambda_output_shape_function(self):
    def get_output_shape(input_shape):
      return 1 * input_shape

    l = keras.layers.Lambda(lambda x: x + 1, output_shape=get_output_shape)
    l(keras.backend.variable(np.ones((1, 1))))
    self.assertEqual('lambda', l.get_config()['output_shape_type'])
# ---
def check_value(dict, n):
    result = all(x == n for x in dict.values()) 
    return result
# ---
def _get_field_mapping(self, schema):
        """Get mapping for given field schema."""
        if 'mapping' in schema:
            return schema['mapping']
        elif schema['type'] == 'datetime':
            return {'type': 'date'}
        elif schema['type'] == 'string' and schema.get('unique'):
            return {'type': 'string', 'index': 'not_analyzed'}
# ---
def __unicode__(self):
		return self.contenido
# ---
def __exit__(self, *args) -> None:
        self.shutdown()
# ---
def lr_scale(self):
        return 1 / hax.axis_size(self.In)
# ---
def save(self, commit=False):
        usuario = super(UsuarioForm, self).save(commit)

        # Cria User
        u = User.objects.create(username=usuario.username, email=usuario.email)
        u.set_password(self.cleaned_data['password'])
        u.is_active = True
        u.groups.add(get_or_create_grupo(self.cleaned_data['tipo'].descricao))

        u.save()
        usuario.user = u
        usuario.save()
        return usuario
# ---
def broadcast(self) -> "_PoolBroadcastProxy[T]":
        return _PoolBroadcastProxy(self)
# ---
def __init__(self, run_result: subprocess.CompletedProcess | None = None):
        self._run_result = run_result or subprocess.CompletedProcess(args=[], returncode=0, stdout="ok", stderr="")
        self.last_timeout: Duration | None = None
# ---
def current_label_index(self):
        return self.ui.comboBoxFuzzingLabel.currentIndex()
# ---
def format_differences(differences):
    return format_line(prefix='differences'.rjust(RJUST), values=differences)
# ---
def default_choice_name(cls) -> str | None:
        return "cached"
# ---
def GetJobStatus(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def test_transaction_connection_ctx_commit(self):
        fn = self._trans_fn(True)
        conn = testing.db.connect()
        ctx = conn.begin()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def post():
    return make_post()
# ---
def jacobsthal_lucas(n): 
	dp=[0] * (n + 1) 
	dp[0] = 2
	dp[1] = 1
	for i in range(2, n+1): 
		dp[i] = dp[i - 1] + 2 * dp[i - 2]; 
	return dp[n]
# ---
def test_plain(self):
        table = self.tables.some_table
        lx = table.c.x.label("lx")
        self._assert_result(select([lx]).order_by(lx), [(1,), (2,), (3,)])
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.two.unique() == 'foo').sum()
# ---
def __repr__(self) -> str:
        return f"Namespace({super().__repr__()})"
# ---
def stack(axis: AxisSelector, arrays: Sequence[NamedArray]) -> NamedArray:
    """Version of [jax.numpy.stack][] that returns a NamedArray"""
    if isinstance(axis, str):
        axis = Axis(axis, len(arrays))
    if len(arrays) == 0:
        return zeros(axis)
    arrays = [a.rearrange(arrays[0].axes) for a in arrays]
    return NamedArray(jnp.stack([a.array for a in arrays], axis=0), (axis,) + arrays[0].axes)
# ---
def testFun():
    print('Starting')
    while True:
        time.sleep(3)
        print('looping')
        time.sleep(3)
        print('3 Seconds Later')
# ---
def test_bytes_io():
    f = py.io.BytesIO()
    f.write(tobytes("hello"))
    pytest.raises(TypeError, "f.write(totext('hello'))")
    s = f.getvalue()
    assert s == tobytes("hello")
# ---
def equilibrium_index(arr):
  total_sum = sum(arr)
  left_sum=0
  for i, num in enumerate(arr):
    total_sum -= num
    if left_sum == total_sum:
      return i
    left_sum += num
  return -1
# ---
def test_bundle_hash(test_bundle):
    """Compute hash of test bundle."""
    h = hashlib.sha256()
    with open(test_bundle, "rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            h.update(chunk)
    return h.hexdigest()
# ---
def main():

    app = wx.App()
    remote = Remote()
    MainWindow(remote, None)
    remote.start()
    remote.do_load(False)
    app.MainLoop()
# ---
def get_name():
    return addon.getAddonInfo('name')
# ---
def after(self, speaker_id):
        """Pick next person to speak"""
        row = self._transitions[speaker_id]
        sucessor = searchsorted(cumsum(row), rand() * sum(row))
        return sucessor
# ---
def get_cumulative_difficulty(self):
        with self.lock:
            last_block_metadata = self._state.get_block_metadata(self._last_block.headerhash)
            return last_block_metadata.cumulative_difficulty
# ---
def __init__(self, default, routes=None):
		self._default = default
		if routes is None:
			routes = {}
		self._routes = routes
# ---
def scheduler():
    """
    A pytest fixture that sets up a BatchScheduler with the following schedule:
      - Use batch size 32 until step 1000
      - Then batch size 64 until step 100000
      - Then batch size 128 forever
    """
    schedule = [
        ScheduleStep(start=0, value=32),
        ScheduleStep(start=1000, value=64),
        ScheduleStep(start=100000, value=128),
    ]
    return BatchSchedule(schedule)
# ---
def __init__(self, job_id: str, status: JobStatus):
        self.job_id = job_id
        self.failed_status = status
        super().__init__(f"Job {job_id} finished with status {status}")
# ---
def hashable_combine(dynamic, static):
    static_leaves, static_treedef = static
    static = jtu.tree_unflatten(static_treedef, static_leaves)
    return eqx.combine(dynamic, static)
# ---
def finish(self):
        pass
# ---
def _round_to_power_of_two(x: float) -> int:
    """Round x UP to the nearest power of 2."""
    if x <= 1:
        return 1
    return 2 ** math.ceil(math.log2(x))
# ---
def test_bundle(tmp_path):
    """Create a test bundle and return file:// path."""
    return create_test_bundle(tmp_path)
# ---
def forward_once(self, fts, ctx: GridContext):
        raise NotImplementedError()
# ---
def fake_vdi_resize(*args, **kwargs):
            raise Exception("This shouldn't be called")
# ---
def test_decay_phase():
    ma = EmaDecaySqrtModelAveraging(model=_dummy(0.0), beta=0.0, switch_step=0, decay_steps=4)
    ma = ma.update(_dummy(1.0), step=0)
    ma_end = ma.update(_dummy(10.0), step=4)
    assert jnp.allclose(ma_end.model["p"], 1.0)
    assert jnp.isclose(ma_end.tot_weight, 1.0)
# ---
def _binding_name(callback_or_cmd):
        return 'py_kb_{:016x}'.format(hash(callback_or_cmd)&0xffffffffffffffff)
# ---
def item(self):  # pragma: no cover
        """Returns the value of this NamedArray as a python scalar."""
        return self.array.item()
# ---
def binsend(self, bindata):
		self.dispatcher(self.neighbor, bindata, self.frequency, self.bandwidth)
		time.sleep(self.send_delay)
# ---
def normalize_final_weighted_score(self,minimum,maximum):
        value = self.final_weighted
        value -= minimum
        if maximum - minimum > 0:
            value /= ( maximum - minimum )
        else:
            value = 0
        self.weight_rank = "{0:.2f}".format(value * 10)
# ---
def get_status(self) -> vm_pb2.AutoscalerStatus:
        """Get autoscaler status."""
        ...
# ---
def with_gpu(gpu_type: str = "auto", count: int = 1, **kwargs: Any) -> ResourceConfig:
        device = GpuConfig(variant=gpu_type, count=count)
        return ResourceConfig(device=device, **kwargs)
# ---
def update(self: S, new_model: M, step: int) -> S:
        del step
        # 1 - beta because increment_update expects the weight of the new model
        return dataclasses.replace(self, model=optax.incremental_update(new_model, self.model, 1 - self.beta))
# ---
def add_cookies(self, clist):
        for c in clist:
            name = c.split("\t")[5]
            self.cookies[name] = c
# ---
def isclose(a: NamedArray, b: NamedArray, rtol=1e-05, atol=1e-08, equal_nan=False) -> NamedArray:
    """Returns a boolean array where two arrays are element-wise equal within a tolerance."""
    a, b = broadcast_arrays(a, b)
    # TODO: numpy supports an array atol and rtol, but we don't yet
    return NamedArray(jnp.isclose(a.array, b.array, rtol=rtol, atol=atol, equal_nan=equal_nan), a.axes)
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)
        pattern = self._normalize(pattern)
        try:
            self.pattern = re.compile(self.pattern)
        except re.error as exc:
            # Invalid regular expression.
            raise InvalidQueryArgumentValueError(pattern,
                                                 "a regular expression",
                                                 format(exc))
# ---
def record_batch(
        self,
        loss: torch.Tensor,
        target_data,
        gen_data,
        input_data,
        target_data_norm,
        gen_data_norm,
        input_data_norm,
    ):
        self._loss = loss
        self._target_data = target_data
        self._gen_data = gen_data
        self._target_data_norm = target_data_norm
        self._gen_data_norm = gen_data_norm
        self._input_data = input_data
        self._input_data_norm = input_data_norm
# ---
def null(self, env):
        return env[self.comodel_name]
# ---
def comma_cooldown_mixture(*, tokenizer: str = llama3_tokenizer, permutation_type="feistel"):
    """LmMixtureDatasetConfig for the cooldown stage."""
    tokenized = common_pile_tokenized(tokenizer=tokenizer)
    components = {f"common_pile/{dataset}": tokenized[f"common_pile/{dataset}"] for dataset in COMMON_PILE_DATASETS}
    return lm_mixture_data_config(
        components=components,
        weights=COMMA_COOLDOWN_MIXTURE_WEIGHTS,
        permutation_type=permutation_type,
    )
# ---
def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            value=self.value,
        )
# ---
def find_Min_Diff(arr,n): 
    arr = sorted(arr) 
    diff = 10**20 
    for i in range(n-1): 
        if arr[i+1] - arr[i] < diff: 
            diff = arr[i+1] - arr[i]  
    return diff
# ---
def dealnnoy_num(n, m): 
	if (m == 0 or n == 0) : 
		return 1
	return dealnnoy_num(m - 1, n) + dealnnoy_num(m - 1, n - 1) + dealnnoy_num(m, n - 1)
# ---
def clearCache(self):
        mt.utils.rmdir(self.cache_path)
# ---
def show_savecfg_dlg(self):
        filename, _ = QFileDialog.getSaveFileName(
            self, self.tr("Save configuration file..."),
            directory=os.path.expanduser("~"),
            filter="Json file (*.json)"
        )

        if filename:
            self.filename = filename
            self.save_file()
# ---
def test_broadcast_to():
    H, W, D = hax.make_axes(H=10, W=20, D=30)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    assert jnp.all(jnp.equal(hax.broadcast_to(named1, (H, W, D)).array, named1.array))

    ZZ = Axis("ZZ", 5)

    assert jnp.all(jnp.equal(hax.broadcast_to(named1, (H, W, D, ZZ)).array, named1.array.reshape(10, 20, 30, 1)))
# ---
def build(
        self,
        in_channels: int,
        out_channels: int,
        hist: int,
        static_data_for_corrector: xr.Dataset | None,
        srcs: list[DataSource],
    ) -> BaseModel:
        pass
# ---
def i_get_the_project(step, resource):
    resource = world.api.get_project(resource)
    world.status = resource['code']
    assert world.status == HTTP_OK
    world.project = resource['object']
# ---
def _remove_spans(text: str, spans: list[list[int]]) -> str:
        """
        Return `text` with `spans` removed.
        Example: text = "hello", spans = [[1, 4]], returns "ho"
        """
        # Sort spans in reverse order to avoid index shifting
        sorted_spans = sorted(spans, key=lambda x: x[1], reverse=True)
        for start, end, _ in sorted_spans:
            text = text[:start] + text[end:]

        return text
# ---
def _bos_tokens(self) -> list[int]:
        return self.tokenizer.encode("<|begin_of_text|>", add_special_tokens=False)
# ---
def list_split(S, step):
    return [S[i::step] for i in range(step)]
# ---
def main():
    configs = {
        "mixtral_300m": build_speedrun_config("mixtral_300m", train_batch_size=256, lr=5e-4),
        "mixtral_1b": build_speedrun_config("mixtral_1b", train_batch_size=128, lr=3e-4),
        "mixtral_1_5b": build_speedrun_config("mixtral_1_5b", train_batch_size=96, lr=2e-4),
    }
    for name in MODEL_ORDER:
        cfg = configs[name]
        LOGGER.info("Launching %s", name)
        executor_main(steps=default_speedrun(f"pranshu_mixtral_moe_gmm_sweep_{name}", cfg))
# ---
def max_product_tuple(list1):
    result_max = max([abs(x * y) for x, y in list1] )
    return result_max
# ---
def addInit(cls, init):
        """
        Adds the `init` method to the list of extensions of the `MainWindow.__init__`.
        """
        cls.__inits.append(init)
# ---
def from_iterable(iterable: Iterable[T]) -> Dataset[T]:
        """Create a dataset from any iterable."""
        return Dataset(iterable)
# ---
def test_health_check_rpc(self, real_service):
        """Test HealthCheck RPC returns healthy status."""
        ctx = Mock(spec=RequestContext)

        response = real_service.health_check(cluster_pb2.Empty(), ctx)

        assert response.healthy
        assert response.uptime.milliseconds >= 0
# ---
def destroy(self, request, *args, **kwargs):
        raise MethodNotAllowed(self.action)
# ---
def _setup_regular(self, env):
        super(_Relational, self)._setup_regular(env)
        if self.comodel_name not in env.registry:
            _logger.warning("Field %s with unknown comodel_name %r"
                            % (self, self.comodel_name))
            self.comodel_name = '_unknown'
# ---
def update_sort_order(self):
        if self.type_ == 'PITCH_BEND':
            self.sort_order = -10
        if self.type_ == 'NOTE_OFF':
            self.sort_order = -20
# ---
def _noop():
    pass
# ---
def do_block_with_args(block: M, *args, **kwargs) -> OutputT_co:
                return fn(block, *args, **kwargs)
# ---
def record_batch(
        self,
        *,
        loss: torch.Tensor,
        target_data,
        gen_data,
        input_data,
        target_data_norm,
        gen_data_norm,
        input_data_norm,
    ): ...
# ---
def test_run_blocking_no_check():
    with TemporaryVenv() as venv:
        result = venv.run([venv.python_path, "-c", "import sys; sys.exit(1)"], check=False)
        assert result.returncode == 1
# ---
def reraise(self):
        raise self.exc.with_traceback(self.tb.as_traceback())
# ---
def build(self, reference_model: eqx.Module) -> eqx.Module:
        """Initialize any learned components (e.g., value heads)."""
        ...
# ---
def h_fs_rmdir (_,path): os.rmdir(path)
# ---
def variables(self):
        """Frozen dictionary of xray.Variable objects constituting this
        dataset's data
        """
        return Frozen(self._variables)
# ---
def test_startswith_autoescape(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab%c", autoescape=True), {3})
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["ApertusConfig"]:
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=False,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfApertusConfig,
        )
# ---
def __init__(self, connection):
                self.connection = connection
# ---
def test_wait_for_connection_respects_stop_event(mock_conn_avail, _mock_sleep):
    """wait_for_connection returns False when stop_event is set."""
    mock_conn_avail.return_value = False
    conn = MagicMock()
    stop_event = threading.Event()
    stop_event.set()
    assert (
        wait_for_connection(
            conn, timeout=Duration.from_seconds(60), poll_interval=Duration.from_seconds(5), stop_event=stop_event
        )
        is False
    )
# ---
def _run_gcloud(args: list[str], check: bool = False) -> subprocess.CompletedProcess[str]:
    """Run a gcloud command and return the result."""
    return subprocess.run(["gcloud", *args], capture_output=True, text=True, check=check)
# ---
def exponential(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.exponential(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def __init__(self, vocab_size: int, eos_id: int = 3):
        self.Vocab = Axis("vocab", vocab_size)
        self.eos = eos_id
# ---
def rust_mark_document_duplicates(
    batch: pa.RecordBatch, text_col: str, id_col: str, dup_map: dict, attribute_name: str
) -> pa.RecordBatch:
    # The mark function only needs the batch, map, and attr name.
    # text_col and id_col are ignored but kept for signature compatibility with the benchmark.
    return dupekit.mark_document_duplicates(batch, dup_map, attribute_name)
# ---
def forward(self, inputs):
        x = self.relu(inputs)
        x = torch.clamp(x, max=self.cap)
        return x
# ---
def get_rest_of_frame_molecule(frame_molecule, selected_molecule):
    # calc the rest
    selector = bridge.Select_AtomGroup(selected_molecule)
    selected = frame_molecule.select(selector)
    rest_molecule = frame_molecule ^ selected

    return rest_molecule
# ---
def get_nick(self, nick):
        return self.data[nick.lower()]['nick']
# ---
def _fileset(self):
        return set(self._files)
# ---
def __rsub__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__rsub__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.subtract, other, self, dtype=result_type(self, other))
# ---
def _make_dims(indices, numblocks, new_axes):
    """Returns a dictionary mapping between each index specified in
    `indices` and the number of output blocks for that indice.
    """
    dims = broadcast_dimensions(indices, numblocks)
    for k, v in new_axes.items():
        dims[k] = len(v) if isinstance(v, tuple) else 1
    return dims
# ---
def __repr__(self):
        return "<%s>" % self.__class__.__name__
# ---
def test_peak_measured_mem():
    assert peak_measured_mem() > 0
# ---
def foo(x):
        assert x.shape == (3,)
        return 2 * x
# ---
def decomposed_mse_scaled(
    pred: torch.Tensor, target: torch.Tensor, scaling: torch.Tensor
) -> torch.Tensor:
    """MSE loss with scaled residuals."""
    scaled_pred = pred * scaling.view(1, -1, 1, 1)
    scaled_target = target * scaling.view(1, -1, 1, 1)
    return F.mse_loss(scaled_pred, scaled_target, reduction="none").mean(dim=(0, 2, 3))
# ---
def test_constant_schedule():
    optimizer = AdamConfig(
        learning_rate=1e-3,
        weight_decay=0.0,
        warmup=0.0,
        min_lr_ratio=1.0,  # No decay
        lr_schedule="constant",
        cycles=None,
    )

    sched_fn = optimizer.lr_scheduler(1000)

    assert sched_fn(0) == 1e-3
    assert sched_fn(500) == 1e-3
    assert sched_fn(999) == 1e-3
# ---
def get_tpu_chip_count(device: cluster_pb2.DeviceConfig) -> int:
    """Extract TPU chip count from config."""
    if device.HasField("tpu"):
        return device.tpu.count or 0
    return 0
# ---
def pre_jit(x):
        if jax.process_index() == source:
            inp = np.array(x)
        else:
            inp = jnp.zeros(x.shape, dtype=x.dtype)

        shape = (len(jax.devices()),) + inp.shape
        inp = jnp.expand_dims(inp, axis=0)
        out = jax.make_array_from_callback(shape, sharding, lambda _: inp)

        return out
# ---
def __getitem__(self, key: str) -> FieldAccessExpr:
        return FieldAccessExpr(self, key)
# ---
def test_mup_embedding_init_matches_embedding(init_scale: float):
    Vocab = hax.Axis("V", 8)
    Embed = (hax.Axis("E", 4),)

    key = jrandom.PRNGKey(0)

    baseline = Embedding.init(Vocab, Embed, key=key, init_scale=init_scale)
    mup = Embedding.init(Vocab, Embed, key=key, init_scale=init_scale, reparam_cls=EmbeddingMup)

    assert mup.weight.axes == baseline.weight.axes
    scale_factor = hax.axis_size(Embed)
    assert jnp.allclose(mup.weight.array, baseline.weight.array * scale_factor)
# ---
from typing import List


def intersperse(numbers: List[int], delimeter: int) -> List[int]:
    """ Insert a number 'delimeter' between every two consecutive elements of input list `numbers'
    >>> intersperse([], 4)
    []
    >>> intersperse([1, 2, 3], 4)
    [1, 4, 2, 4, 3]
    """
    if not numbers:
        return []

    result = []

    for n in numbers[:-1]:
        result.append(n)
        result.append(delimeter)

    result.append(numbers[-1])

    return result
# ---
def _make_explicit_data_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape((len(devices),))
    return Mesh(mesh_devices, axis_names=("data",), axis_types=(AxisType.Explicit,))
# ---
def test_str_split_bool_index(self):
        def test_impl(df):
            C = df.A.str.split(',')
            return C[df.B == 'aa']

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D'], 'B': ['aa', 'bb']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def elem_from_name(atom_name, res_name):
    atom_key = re.sub(r"\d", "", atom_name)
    if atom_key in const.ambiguous_atoms:
        if isinstance(const.ambiguous_atoms[atom_key], str):
            element = const.ambiguous_atoms[atom_key]
        elif res_name in const.ambiguous_atoms[atom_key]:
            element = const.ambiguous_atoms[atom_key][res_name]
        else:
            element = const.ambiguous_atoms[atom_key]["*"]
    else:
        element = atom_key[0]
    return element
# ---
def test_as_remote_kwargs_cpu():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=CpuConfig())
    kwargs = as_remote_kwargs(config)
    assert kwargs == {"num_cpus": 1}
# ---
def do_fold(init, *extra_args, **extra_kwargs):
            carry = haliax.fold(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *extra_args, **extra_kwargs)
            return carry
# ---
def test_py36_windowsconsoleio_workaround_non_standard_streams():
    """
    Ensure _py36_windowsconsoleio_workaround function works with objects that
    do not implement the full ``io``-based stream protocol, for example execnet channels (#2666).
    """
    from _pytest.capture import _py36_windowsconsoleio_workaround

    class DummyStream(object):
        def write(self, s):
            pass

    stream = DummyStream()
    _py36_windowsconsoleio_workaround(stream)
# ---
def add_hook(self, *, every: int = 1): ...
# ---
def _get_fineweb2_split_paths(split):
    patterns = FINEWEB2_DATASETS[split]
    fineweb2_split_paths = [output_path_of(fineweb2_raw, pattern) for pattern in patterns]
    return fineweb2_split_paths
# ---
def __init__(self):
        self.events_count = 0
        self.events_count_by_type = dict()
# ---
import re
def remove_whitespaces(text1):
  return (re.sub(r'\s+', '',text1))
# ---
def cleanTransformation( self, transID, rpc = '', url = '', timeout = None ):
    """ Clean the transformation, and set the status parameter (doing it here, for easier extensibility)
    """
    # Cleaning
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    res = rpcClient.cleanTransformation( transID )
    if not res['OK']:
      return res
    # Setting the status
    return self.setTransformationParameter( transID, 'Status', 'TransformationCleaned' )
# ---
def __iter__(self):
        """Iterate over the dataloader, converting RawTrainData to TrainData."""
        for raw_train_data in self._dataloader:
            dataset = self._datasets[raw_train_data.dataset_id]
            train_data = dataset.to_train_data(raw_train_data, self._device)
            yield train_data
# ---
def find_kth(arr1, arr2, m, n, k):
	sorted1 = [0] * (m + n)
	i = 0
	j = 0
	d = 0
	while (i < m and j < n):
		if (arr1[i] < arr2[j]):
			sorted1[d] = arr1[i]
			i += 1
		else:
			sorted1[d] = arr2[j]
			j += 1
		d += 1
	while (i < m):
		sorted1[d] = arr1[i]
		d += 1
		i += 1
	while (j < n):
		sorted1[d] = arr2[j]
		d += 1
		j += 1
	return sorted1[k - 1]
# ---
def test_empty_string(datadir):
    datadir.join('reader').chdir()
    src = 'sharedStrings-emptystring.xml'
    with open(src) as content:
        assert read_string_table(content.read()) == ['Testing empty cell', '']
# ---
def updLine(self, row, tmpLine):
    #print("--- %d ---" % row)
    pass
# ---
def withdraw(self, amount):
        if self.balance - amount < self.minimum_balance:
            print('Sorry, minimum balance must be maintained.')
        else:
            BankAccount.withdraw(self, amount)
# ---
def transfer_mode(request):
    """Parametrized weight transfer mode."""
    return request.param
# ---
def test_parse_empty_json_template(self):
        tmpl_str = '{}'
        msg = 'Template format version not found'
        self._parse_template(tmpl_str, msg)
# ---
def vocab_size(self) -> int:
    return len(self._vocab_str_to_int)
# ---
def serialize_keras_object(instance):
  _, instance = tf_decorator.unwrap(instance)
  if instance is None:
    return None
  if hasattr(instance, 'get_config'):
    return serialize_keras_class_and_config(instance.__class__.__name__,
                                            instance.get_config())
  if hasattr(instance, '__name__'):
    return instance.__name__
  raise ValueError('Cannot serialize', instance)
# ---
def __mul__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.multiply(self, other)
# ---
def format_dict_list(objs, field):
    objs[field] = "\n".join(
        "- " + ", ".join("%s: %s" % (k, v)
                         for k, v in elem.items())
        for elem in objs[field])
# ---
def get_reference_answer(self) -> str:
        return self.answer
# ---
def register_endpoint(
        self,
        name: str,
        address: str,
        job_id: JobName,
        metadata: dict[str, str] | None = None,
    ) -> str:
        return self._remote_client.register_endpoint(name=name, address=address, job_id=job_id, metadata=metadata)
# ---
def contractionTableComboChanged(self, combobox):
        model = combobox.get_model()
        myIter = combobox.get_active_iter()
        self.prefsDict["brailleContractionTable"] = model[myIter][1]
# ---
def _column_selection(self):
        if isinstance(self.selection, basestring):
            method = self.selection
            return lambda self, *a, **kw: getattr(self, method)(*a, **kw)
        else:
            return self.selection
# ---
def bitwise_invert(a: A) -> A:
    return wrap_elemwise_unary(jnp.bitwise_invert, a)
# ---
def pad_token_id(self) -> int:
        return 0
# ---
def parse_partition_name(self, partition):
        try:
            schema, table_partition = partition.split('.')
            table, partition = table_partition.split('/', 1)
            return schema, table, partition
        except ValueError as e:
            raise ValueError('Could not parse ' + partition)
# ---
def lerp(
    start: Array,
    end: Array,
    weight: Numeric,
):
    return start + weight * (end - start)
# ---
def encode_cmd(text: str, tokenizer: str | None):
    """Encode text to tokens."""
    if tokenizer:
        load_tokenizer(tokenizer)
    encode_text(text)
# ---
def test_created_on(self):
        eq_(self.record.created_on, None)
# ---
def test_host_state(self):
        stats = self.conn.get_host_stats()
        self.assertEquals(stats['disk_total'], 10000)
        self.assertEquals(stats['disk_used'], 20000)
        self.assertEquals(stats['host_memory_total'], 10)
        self.assertEquals(stats['host_memory_overhead'], 20)
        self.assertEquals(stats['host_memory_free'], 30)
        self.assertEquals(stats['host_memory_free_computed'], 40)
# ---
def test_spawn_fail_cleanup_2(self):
        """Simulates an error while creating VM record.

        It verifies that VDIs created are properly cleaned up.

        """
        vdi_recs_start = self._list_vdis()
        stubs.stubout_create_vm(self.stubs)
        self.assertRaises(xenapi_fake.Failure,
                          self._test_spawn, 1, 2, 3)
        # No additional VDI should be found.
        vdi_recs_end = self._list_vdis()
        self._check_vdis(vdi_recs_start, vdi_recs_end)
# ---
def unlink(self, cr, uid, ids, context=None):
        if context is None:
            context = {}
        ctx = context.copy()
        for move in self.browse(cr, uid, ids, context=context):
            if move.state != 'draft' and not ctx.get('call_unlink', False):
                raise osv.except_osv(_('User Error!'), _('You can only delete draft moves.'))
        return super(stock_move, self).unlink(
            cr, uid, ids, context=ctx)
# ---
def get_vm(self, vm_id: str) -> vm_pb2.VmInfo | None:
        """Get info for a specific VM."""
        ...
# ---
def message(self, *args, **kwargs): # real signature unknown
        pass
# ---
def robust_open_dataset(target: str, **kwargs) -> xr.Dataset:
    return xr.open_dataset(target, **kwargs)
# ---
def __init__(
            self, table, partition_name, schema="default",
            mysql_conn_id="metastore_mysql",
            *args, **kwargs):

        self.partition_name = partition_name
        self.table = table
        self.schema = schema
        self.first_poke = True
        self.conn_id = mysql_conn_id
        super(SqlSensor, self).__init__(*args, **kwargs)
# ---
def _interpret(self):
        if not self.min:
            return

        try:
            count = int("".join(self.min))
        except ValueError:
            assert False, "internal error: cannot convert to number minimum of repetition"
        self.between.min = count
# ---
def _tri_mask(N, M, k, chunks, spec):
    from cubed.array_api.elementwise_functions import greater_equal
    from cubed.array_api.manipulation_functions import expand_dims

    # based on dask
    chunks = normalize_chunks(chunks, shape=(N, M))

    # TODO: use min_int for arange dtype
    m = greater_equal(
        expand_dims(arange(N, chunks=chunks[0][0], spec=spec), axis=1),
        arange(-k, M - k, chunks=chunks[1][0], spec=spec),
    )

    return m
# ---
def __init__(self, rule):
        msg = _("Policy doesn't allow %s to be performed.") % rule
        super(PolicyNotAuthorized, self).__init__(msg)
# ---
def test_scan_reports_mismatched_unnamed_array():
    Height = Axis("Height", 2)

    def f(c, x, y):
        return c, x + y

    good = jnp.zeros((Height.size, 3))
    bad = jnp.zeros((Height.size - 1, 3))

    with pytest.raises(ValueError) as e:
        hax.scan(f, Height)(0, good, y=bad)

    assert "y has leading dimension" in str(e.value)
# ---
def __exit__(self, *args):
        if self._rpc_client:
            self._rpc_client = None
        if self._controller_client:
            self._controller_client.close()
        if self._manager:
            self._manager.stop()
        else:
            # Docker path cleanup
            for worker in self._workers:
                worker.stop()
            if self._controller:
                self._controller.stop()
            if self._temp_dir:
                self._temp_dir.cleanup()
# ---
def test_tpu_device(self):
        resources = ResourceConfig(device=TpuConfig(variant="v5litepod-16"))
        spec = convert_resources(resources)
        assert spec.device is not None
        assert spec.device.HasField("tpu")
        assert spec.device.tpu.variant == "v5litepod-16"
# ---
def bias_dropout_add_scale_fused_train(
  x: torch.Tensor,
  bias: typing.Optional[torch.Tensor],
  scale: torch.Tensor,
  residual: typing.Optional[torch.Tensor],
  prob: float,
) -> torch.Tensor:
  return bias_dropout_add_scale(
    x, bias, scale, residual, prob, True
  )
# ---
def isexact(self):
        """Matcher will match exactly the list of files in .files() --
        optimization might be possible."""
        return False
# ---
def _cancel_pending_call(self, ar):
        """
        Cancels a pending call (keyed by the AsyncResult returend by _routing_call).

        @return True if the call was truly pending.
        """
        if self.has_pending_call(ar):
            ar.set(False)
            return True

        return False
# ---
def _find_user(self, dn):
        return next(i for (i, d) in enumerate(self.directory) if d["dn"] == dn)
# ---
def convert_to_read(self, value, use_name_get=True):
        return value.ids
# ---
def test_mem_write_byte_calls_char_generator_top_left(self):
        self.mda.mem_write_byte(0x0000, 0x41)
        self.assertEqual(self.cg.last_blit, (None, (0, 0), 0x41, MDA_GREEN, MDA_BLACK))
# ---
def docker_cleanup_scope():
    """Cleans up all Docker containers and images created during the test."""
    with _docker_cleanup(cleanup_images=True):
        yield
# ---
def name_get(self, cr, uid, ids, context=None):
        if not ids:
            return []
        reads = self.read(cr, uid, ids, ['name', 'prefix', 'ref'], context)
        res = []
        for record in reads:
            name = record['name']
            prefix = record['prefix']
            if prefix:
                name = prefix + '/' + name
            if record['ref']:
                name = '%s [%s]' % (name, record['ref'])
            res.append((record['id'], name))
        return res
# ---
def test_dslice_oob_read_and_write():
    Seq = hax.Axis("seq", 5)
    from haliax import ds

    arr = hax.arange((Seq,), dtype=int)
    out = arr[{"seq": ds(3, 4)}]
    ref = jnp.take(arr.array, jnp.arange(3, 7), mode="fill", fill_value=0)
    assert jnp.array_equal(out.array, ref)

    upd = hax.arange((Seq.resize(4),), dtype=int)
    updated = arr.at[{"seq": ds(3, 4)}].set(upd)
    ref_upd = arr.array.at[jnp.arange(3, 7)].set(upd.array, mode="drop")
    assert jnp.array_equal(updated.array, ref_upd)
# ---
def map(self, fn: MapFunction[U], *extra_args, **extra_kwargs) -> "MappedAsyncDataset[T_co, U]":
        return MappedAsyncDataset(self, fn, *extra_args, **extra_kwargs)
# ---
def shutdown(self) -> None:
        self._executor.shutdown(wait=True)
# ---
def __le__(self, other: "Timestamp") -> bool:
        return self._epoch_ms <= other._epoch_ms
# ---
def phoenixToOpenVGDB(self, phoenixID):
        ret = ""
        try:
            ret = self.phoenixToOpenVGDBMap[phoenixID]
        except KeyError:
            ret = ""
        return ret
# ---
def _set_pdeathsig_preexec():
    """Use prctl(PR_SET_PDEATHSIG, SIGKILL) to kill subprocess if parent dies."""
    PR_SET_PDEATHSIG = 1
    try:
        libc = ctypes.CDLL(ctypes.util.find_library("c"), use_errno=True)
        if libc.prctl(PR_SET_PDEATHSIG, signal.SIGKILL) != 0:
            errno = ctypes.get_errno()
            logger.warning(f"Failed to set parent death signal: errno {errno}")
    except Exception as e:
        logger.info(f"Could not set parent death signal: {e}")
# ---
def imprime_reporte(nf, nr):
        reporte  = "Nmero de archivos procesados:\t {}\n".format(nf)
        reporte += "Nmero de filas en tsv:\t {}\n".format(nr)
        if(nf!=nr):
            reporte += "\n\n**** Atencin ****\n"

        return reporte
# ---
def test_rename_variables_preserves_structure(rng):
    source = "def foo(x):\n    return x + 1"
    result = rename_variables(source, rng)
    assert result is not None
    # Should still have a function with a return and addition.
    tree = ast.parse(result)
    func = tree.body[0]
    assert isinstance(func, ast.FunctionDef)
    ret = func.body[0]
    assert isinstance(ret, ast.Return)
    assert isinstance(ret.value, ast.BinOp)
    assert isinstance(ret.value.op, ast.Add)
# ---
def model_type(self) -> Type["LlamaLMHeadModel"]:
        return LlamaLMHeadModel
# ---
def append(self, item: T):
        if self.is_complete:
            raise ValueError("Cannot append to a finalized dataset")
        self.data.append(item)
        asyncio.create_task(self.notify_length_update())
# ---
def remote(self, *args, **kwargs):
        # Check if context has executor (ThreadContext) or not (SyncContext)
        if hasattr(self._context, "executor"):

            def locked_call():
                with self._lock:
                    return self._method(*args, **kwargs)

            return self._context.executor.submit(locked_call)
        else:
            with self._lock:
                result = self._method(*args, **kwargs)
            return _ImmediateFuture(result)
# ---
import math 
def find_Index(n): 
    x = math.sqrt(2 * math.pow(10,(n - 1))); 
    return round(x);
# ---
def flatten_for_export(self: Mod) -> Mod:
        """
        Flatten articulated named arrays for export to torch. In general this method should, for a linear layer, flatten
        all input axes into a single axis, and all output axes into a single axis. You can do whatever else
        you want to support pytorch-compatible serialization if you want.

        This method is less general than to_state_dict and is only called when using to_torch_compatible_state_dict.
        """
        return self
# ---
def checkpoint(self, carry_name: str, input_name: str, callable):
        if self.disable:
            return callable
        elif self.simple:
            return eqx.filter_checkpoint(callable, prevent_cse=self.prevent_cse)
        else:
            policy = self._to_jax_policy(carry_name, input_name)
            return eqx.filter_checkpoint(callable, policy=policy, prevent_cse=self.prevent_cse)
# ---
def get_cluster(self, word):
        """
        Returns the cluster number for a word in the vocabulary
        """
        idx = self.ix(word)
        return self.clusters[idx]
# ---
def test_create_experiment_view(self):
        """ Tests edit_experiment template renders for url 'create_experiment' """
        response = self.client.get(reverse("ab_testing_tool_create_experiment"))
        self.assertOkay(response)
        self.assertTemplateUsed(response, "ab_tool/edit_experiment.html")
# ---
def __init__(self, x):
        self.x = x
        self.dtype = x.dtype
        self.shape = x.shape
        self.ndim = len(x.shape)
# ---
def get_replicas(self):
        ''' return replicas setting '''
        return self.get(DeploymentConfig.replicas_path)
# ---
def status(self) -> ControllerStatus:
        """Get controller status."""
        ...
# ---
def load(cls, config_path: Path | str) -> "IrisConfig":
        """Load IrisConfig from YAML file.

        Args:
            config_path: Path to YAML configuration file

        Returns:
            IrisConfig with defaults applied and validated
        """
        proto = load_config(config_path)
        return cls(proto)
# ---
def run_command(*args, **kwargs):
    print("Running:", " ".join(list(args)))
    return subprocess.check_call(args, **kwargs)
# ---
def count_ways(n): 
	A = [0] * (n + 1) 
	B = [0] * (n + 1) 
	A[0] = 1
	A[1] = 0
	B[0] = 0
	B[1] = 1
	for i in range(2, n+1): 
		A[i] = A[i - 2] + 2 * B[i - 1] 
		B[i] = A[i - 1] + B[i - 2] 
	return A[n]
# ---
def __ror__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_or(other, self)
# ---
def _calculate_fan(linear_weight_shape, fan="fan_in"):
    fan_out, fan_in = linear_weight_shape

    if fan == "fan_in":
        f = fan_in
    elif fan == "fan_out":
        f = fan_out
    elif fan == "fan_avg":
        f = (fan_in + fan_out) / 2
    else:
        raise ValueError("Invalid fan option")

    return f
# ---
from collections import Counter
import re
def n_common_words(text,n):
  words = re.findall('\w+',text)
  n_common_words= Counter(words).most_common(n)
  return list(n_common_words)
# ---
def genetic_modification_6(lab, award, crispr_gm, source):
    return {
        'purpose': 'validation',
        'category': 'deeltion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'description': 'blah blah description blah',
        "method": "CRISPR",
        "modified_site_by_target_id": "/targets/FLAG-ZBTB43-human/",
        "reagents": [
            {
                "identifier": "placeholder_id",
                "source": source['uuid']
            }
        ]
    }
# ---
def __init__(self, check_timestamps=True) -> None:
        self.check_timestamps = check_timestamps
# ---
def test_repr_handles_lambdas():
    """Ensure anonymous lambdas work correctly."""
    op = FilterOp(lambda x: x > 0)
    assert repr(op) == "FilterOp(predicate=test_repr_handles_lambdas.<locals>.<lambda>)"
# ---
def format_buffers(buffers):
    return format_line(prefix='buffers'.rjust(RJUST), values=buffers)
# ---
def test_heartbeat_temporary_failure(cluster):
    """Test heartbeat fails 3 times (30s gap), but worker timeout is 60s. Worker should
    NOT be marked failed. Task should still succeed.
    """
    _url, client = cluster
    enable_chaos("worker.heartbeat", failure_rate=1.0, max_failures=3)
    job = submit(client, _quick, "temp-hb-fail")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def check_String(str): 
    flag_l = False
    flag_n = False
    for i in str: 
        if i.isalpha(): 
            flag_l = True  
        if i.isdigit(): 
            flag_n = True
    return flag_l and flag_n
# ---
def map_data(
        self, func: Callable[[xr.Dataset], xr.Dataset], *, suffix: str | None = None
    ) -> Self:
        """Map the function over just data in DataSource."""
        if suffix is None:
            suffix = func.__qualname__
        data = func(self.data.copy())
        return dataclasses.replace(self, name=f"{self.name}_{suffix}", data=data)
# ---
def run_func(input, func=None, config=None, name=None, compute_id=None):
    result = func(input, config=config)
    return result
# ---
import math
def surfacearea_cone(r,h):
  l = math.sqrt(r * r + h * h)
  SA = math.pi * r * (r + l)
  return SA
# ---
def start(self) -> None: ...
# ---
def __call__(self, text, rules=None):
        return self.parse(text, rules)
# ---
def get_slice(indices):
        arr = array_or_slice[indices]
        if dtype is not None:
            arr = arr.astype(dtype)

        return arr
# ---
def test_area_weighted_rmse_with_error(self):
        """Verify RMSE correctly measures error."""
        target = torch.zeros(1, 4, 4)
        gen = torch.ones(1, 4, 4)  # Constant error of 1
        weights = torch.ones(4, 4) / 16

        rmse = area_weighted_rmse(target, gen, weights)

        # RMSE should be 1.0 for constant error of 1
        assert torch.allclose(rmse, torch.tensor([1.0]))
# ---
def init(weight):
            return ScanModule(weight=weight)
# ---
def _extend_or_expr(self, or_expr, _or, check):
        """Extend an 'or_expr' by adding one more check."""

        return [('or_expr', or_expr.add_check(check))]
# ---
def config(self):
        return self.transformer.config
# ---
def variance(self) -> Scalar:
        """
        Calculate the variance of the histogram.
        Variance = E[X^2] - (E[X])^2
        where E[X] is the mean and E[X^2] is the mean of squares.
        """
        mean = self.mean
        mean_of_squares = self.sum_squares / self.num
        variance = mean_of_squares - (mean**2)
        return variance
# ---
def _fetch_dims(tree):
    shapes = []
    tree_type = type(tree)
    if tree_type is dict:
        for v in tree.values():
            shapes.extend(_fetch_dims(v))
    elif tree_type is list or tree_type is tuple:
        for t in tree:
            shapes.extend(_fetch_dims(t))
    elif tree_type is torch.Tensor:
        shapes.append(tree.shape)
    else:
        raise ValueError("Not supported")

    return shapes
# ---
def test_permutation_handles_large_length_no_overflow(PermutationClass):
    large_length = 2**34
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(large_length, prng_key)
    index = 2**32  # A large index within the range
    result = permutation(index)
    assert isinstance(result, int)
    assert 0 <= result < large_length
# ---
def main(cfg: DownloadConfig) -> None:
    """Download HuggingFace dataset."""
    download_hf(cfg)
# ---
def test_makes_patches():
    x = torch.randn(3, 10, 4, 8)

    patch_embed = PerceiverEncoder(
        in_channels=10,
        out_channels=4,
        patch_extent=(180, 180),
        perceiver=make_perceiver(10, 4),
    )

    patches = patch_embed(x, make_resolution(x))

    assert patches.shape == (3, 4, 1, 2)
# ---
def _num_accelerator_chips(resources: ResourceConfig) -> int:
    """Return the total number of accelerator chips for the provided resources."""
    return resources.chip_count()
# ---
def __init__(self, qpart):
        self._qpart = qpart

        self.width = self._DEFAULT_INDENT_WIDTH
        self.useTabs = self._DEFAULT_INDENT_USE_TABS

        self._smartIndenter = _getSmartIndenter('normal', self._qpart, self)
# ---
def is_point_on_left(self, point):
        return self.a * point[0] + self.b * point[1] + self.c > 0
# ---
def apply_gradients(model, grads):
        overwrites, grads = partition_for_grad_overwrite(grads)
        updates = jax.tree_util.tree_map(lambda g: -lr * g, grads)
        model = apply_updates(model, updates, overwrites)
        return model
# ---
def registry() -> VmRegistry:
    """Create a fresh VmRegistry for testing."""
    return VmRegistry()
# ---
def play_n_episodes(player, predfunc, nr):
    logger.info("Start evaluation: ")
    for k in range(nr):
        if k != 0:
            player.restart_episode()
        score = play_one_episode(player, predfunc)
        print("{}/{}, score={}".format(k, nr, score))
# ---
def combine_pdb_models(self, pdb_strings):
        combined_pdb = ""
        model_number = 1

        for pdb in pdb_strings:
            # Add a model number at the start of each model
            combined_pdb += f"MODEL     {model_number}\n"
            combined_pdb += pdb.split("\nEND")[0]
            combined_pdb += "\nENDMDL\n"  # End of model marker
            model_number += 1

        return combined_pdb
# ---
def test_dropout_supports_masking(self):
    dropout = keras.layers.Dropout(0.5)
    self.assertEqual(True, dropout.supports_masking)
# ---
def __init__(self, iterable=None):
        self._data = {}
        if iterable is not None:
            self.update(iterable)
# ---
def test_hmac_secret_is_text(self, mocked_render_template):
        config.init('kinto.ini', 'postgresql')
        args, kwargs = list(mocked_render_template.call_args)
        self.assertEquals(type(kwargs['secret']), six.text_type)
# ---
def add_tuple(test_list, test_tup):
  test_list += test_tup
  return (test_list)
# ---
def generateUUID():  # pylint: disable=invalid-name
    """ Utility function; generates UUIDs """
    return str(uuid.uuid4())
# ---
def __call__(self, x):
            return x + self.w
# ---
def __init__(self, notebook, platform_data):
        self.notebook = notebook
        self.id = platform_data['id']
        self.name = platform_data['name']
        self.capsule_types = {c['code'] for c in platform_data['capsules']}
# ---
def from_seconds(cls, seconds: float) -> "Duration":
        """Create duration from seconds."""
        return cls(int(seconds * 1000))
# ---
def show_image(self, image_id):
        """Returns the details of a single image."""
        resp, body = self.get("images/%s" % str(image_id))
        self.expected_success(200, resp.status)
        body = json.loads(body)
        self.validate_response(schema.get_image, resp, body)
        return service_client.ResponseBody(resp, body['image'])
# ---
def default():
        return DefaultDotGeneralOp.init()
# ---
def remove_words(list1, charlist):
    new_list = []
    for line in list1:
        new_words = ' '.join([word for word in line.split() if not any([phrase in word for phrase in charlist])])
        new_list.append(new_words)
    return new_list
# ---
def test_capturing_bytes_in_utf8_encoding(testdir, method):
    testdir.makepyfile("""
        def test_unicode():
            print ('b\\u00f6y')
    """)
    result = testdir.runpytest("--capture=%s" % method)
    result.stdout.fnmatch_lines([
        "*1 passed*"
    ])
# ---
def Vocab(self) -> Axis:
        return Axis("vocab", len(self.tokenizer))
# ---
from collections import Counter 
def check_occurences(test_list):
  res = dict(Counter(tuple(ele) for ele in map(sorted, test_list)))
  return  (res)
# ---
def test_detail(self):
        resp = FakeResponse()
        self.type_action_controller.detail(self.req, resp)
        self.assertEqual(
            [{'id': fake.VOLUME_TYPE_ID,
              'os-volume-type-access:is_public': True},
             {'id': fake.VOLUME_TYPE3_ID,
              'os-volume-type-access:is_public': False}],
            resp.obj['volume_types'])
# ---
def build_asr(self, Vocab: Axis, *, key: PRNGKeyArray) -> "ASRMixin":
        return self.asr_model_type.init(Vocab, self, key=key)
# ---
def cross_entropy(logits: hax.NamedArray, labels: hax.NamedArray) -> hax.NamedArray:
        return hax.take(logits, Embed, labels)
# ---
def __init__(self):
        # Load available backends
        for entry in pkg_resources.iter_entry_points("gosa.object.backend"):
            clazz = entry.load()
            ObjectBackendRegistry.backends[clazz.__name__] = clazz()
# ---
def _parse_line(self, line):
        m = self.COPY_DISK_RE.match(line)
        if m is None:
            raise OutputParserError('unexpected format in "Copying disk"'
                                    ', line: %r' % line)
        return m.group(1), m.group(2), m.group(3)
# ---
def guide_entry(self):
        '''Write the XML element for the guide.
        (Empty string if no guide title and type are given.)'''
        if self.guide_title and self.guide_type:
            return _make_xml_elem('reference', '',
              [
                ('title', self.guide_title),
                ('type', self.guide_type),
                ('href', self.name)
              ])
        else:
            return ''
# ---
def clear(self):
		"""
		Delete every row in the file
		"""
		for k in self.keys(): # Use key, otherwise we get RuntimeError: dictionary changed size during iteration
			del self[k]
# ---
def hvp(f, x, v):
    """Compute the Hessian-vector product of a function."""
    return eqx.filter_jvp(eqx.filter_grad(f), (x,), (v,))[1]
# ---
def client(server):
    """Create test client for HTTP requests."""
    return TestClient(server._app)
# ---
def broadcast(*args, **kwargs) -> BroadcastFuture:
            result = self._pool._resolve()
            futures = []
            for endpoint in result.endpoints:
                future = self._pool._executor.submit(
                    self._pool._call_endpoint,
                    endpoint,
                    method_name,
                    args,
                    kwargs,
                )
                futures.append((endpoint, future))
            return BroadcastFuture(futures)
# ---
def __iter__(self):
        return (key for key in self._dataset._variables
                if key not in self._dataset._coord_names)
# ---
def decorator(f):
        f.http_route = path
        f.http_method = method
        return f
# ---
def all_keys(self):
        '''
        Merge managed keys with local keys
        '''
        keys = self.list_keys()
        keys.update(self.local_keys())
        return keys
# ---
def synthetic_target_type(self, target):
    return GoThriftGenLibrary
# ---
def projected_mem_utilization(row):
        return row["peak_measured_mem_end_mb_max"] / row["projected_mem_mb"]
# ---
def openai_address(self) -> str:
        return f"http://{self._inference_server.address()}/v1"
# ---
def make_floating_point_trainable_filter(is_trainable: FilterTree) -> FilterTree:
    """
    Combines the is_trainable filter with a filter that only allows floating point parameters.
    """

    def is_trainable_and_floating_point(x):
        if x is True:
            return is_inexact_arrayish
        elif x is False:
            return False
        else:
            return lambda y: is_inexact_arrayish(y) and x(y)

    return haliax.tree_util.tree_map(is_trainable_and_floating_point, is_trainable)
# ---
def netloc(self):
        '''return username:password@hostname:port'''
        s = ''
        prefix = ''
        if self.username:
            s += self.username
            prefix = '@'

        if self.password:
            s += ":{}".format(self.password)
            prefix = '@'

        s += "{}{}".format(prefix, self.hostloc)
        return s
# ---

def add(lst):
    """Given a non-empty list of integers lst. add the even elements that are at odd indices..


    Examples:
        add([4, 2, 6, 7]) ==> 2 
    """
    return sum([lst[i] for i in range(1, len(lst), 2) if lst[i]%2 == 0])
# ---
def is_invalid(x, invalid=INVALID):
    return (x < 0) | (x == invalid)
# ---
def __getattribute__(self, key):
                fn = object.__getattribute__(self, key)
                def go(*arg, **kw):
                    canary.append(fn.__name__)
                    return fn(*arg, **kw)
                return go
# ---
def test_unify_chunks_elemwise(chunks_a, chunks_b, expected_chunksize):
    a = xp.ones((10,), chunks=chunks_a)
    b = xp.ones((10,), chunks=chunks_b)

    _, arrays = unify_chunks(a, "i", b, "i")
    for arr in arrays:
        assert arr.chunksize == expected_chunksize

    c = xp.add(a, b)
    assert_array_equal(c.compute(), np.ones((10,)) + np.ones((10,)))
# ---
def attach(self, **kwargs):
        pass
# ---
def id(self):
        return self._id
# ---
def decode(self, input_ids, kv_cache, batch_info, pos_ids):
        # Produce logits that prefer `eos` for every sampled position
        Pos = input_ids.resolve_axis("position")
        Vocab = self.Vocab
        # One-hot on vocab axis for eos token, broadcast over positions
        logits = hax.nn.one_hot(self.eos, Vocab, dtype=jnp.float32)
        logits = logits.broadcast_axis(Pos)
        return logits, kv_cache
# ---
def separator(self, inc_sep):
        ''' setter method for separator '''
        self._separator = inc_sep
# ---
def take(self, axis: AxisSelector, index: int | "NamedArray") -> "NamedArray":  # pragma: no cover
        return haliax.take(self, axis=axis, index=index)
# ---
def test_action_registry(self):
        """Do some sanity checks on the action registry."""
        self.assertEqual(
            len(action_registry.Registry.get_all_actions()), 3)
# ---
def Tournament(self):
        return self.__data['cls_tournament']
# ---
def _convert_ray_status(ray_status: RayJobStatus) -> JobStatus:
    mapping = {
        RayJobStatus.PENDING: JobStatus.PENDING,
        RayJobStatus.RUNNING: JobStatus.RUNNING,
        RayJobStatus.SUCCEEDED: JobStatus.SUCCEEDED,
        RayJobStatus.FAILED: JobStatus.FAILED,
        RayJobStatus.STOPPED: JobStatus.STOPPED,
    }
    return mapping.get(ray_status, JobStatus.FAILED)
# ---
def resolve(self, name: str) -> ResolveResult:
        urls = self._endpoints.get(name, [])
        endpoints = [ResolvedEndpoint(url=url, actor_id=f"fixed-{name}-{i}") for i, url in enumerate(urls)]
        return ResolveResult(name=name, endpoints=endpoints)
# ---
def null(self, env):
        """ return the null value for this field in the given environment """
        return False
# ---
def _sqrts(x):
        yield nxp.sqrt(x)
        yield -nxp.sqrt(x)
# ---
def get_preset(name: str) -> ModelPreset:
    """Get a preset by name.

    Args:
        name: Preset name.

    Returns:
        ModelPreset instance.

    Raises:
        ValueError: If preset name is unknown.
    """
    if name not in PRESETS:
        raise ValueError(f"Unknown preset: {name}. Available: {list(PRESETS.keys())}")
    return PRESETS[name]()
# ---
def key_function(out_key):
        out_coords = out_key[1:]
        all_in_coords = tuple(
            out_coords[:axis] + (i,) + out_coords[axis:]
            for i in range(x.numblocks[axis])
        )
        return tuple((x.name,) + in_coords for in_coords in all_in_coords)
# ---
def cluster_update_configs(ctx):
    """Update all cluster configuration files from templates."""
    print("Updating cluster configuration files...")
    update_cluster_configs("infra/")
    print("Cluster configurations updated successfully!")
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"_layers": "layers"}
# ---
def forward(self, t):
    t_freq = self.timestep_embedding(t, self.frequency_embedding_size)
    t_emb = self.mlp(t_freq)
    return t_emb
# ---
def rnai(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'RNAi'
    }
# ---
def list_all_jobs(self) -> list[ControllerJob]:
        with self._lock:
            return list(self._jobs.values())
# ---
def concatenate_tuple(test_tup):
    delim = "-"
    res = ''.join([str(ele) + delim for ele in test_tup])
    res = res[ : len(res) - len(delim)]
    return (str(res))
# ---
def _simple_flatten(axis: AxisSpec, new_axis: AxisSelector) -> Axis:
    size = haliax.axis_size(axis)
    if isinstance(new_axis, Axis):
        if new_axis.size != size:
            raise ValueError(f"Cannot merge {axis} into {new_axis}: size mismatch")
        return new_axis

    assert isinstance(new_axis, str)
    return Axis(new_axis, size)
# ---
def _q_matmul(a1, a2, q2_chunks=None, block_id=None):
    q1 = a1
    # this array only has a single chunk, but we need to get a slice corresponding to q2_chunks
    q2 = a2[get_item(q2_chunks, block_id)]
    return q1 @ q2
# ---
def validate_path(self) -> Self:
        if urlparse(self.path).scheme:
            raise ValueError(
                "Absolute urls are not supported, please use a "
                "relative path or set type = 's3' or 'local'"
            )
        return self
# ---
def par_heading(self, elem):
        '''Handle a "paragraph heading", i.e., a chaper heading or part
        of a chapter heading inside paragraph tags. If it is immediately
        followed by a heading, they will be merged into one.'''
        self.par_h = elem
# ---
def testGradientInput0WithTranspose(self):
    self._VerifyInput0(transpose_a=True, transpose_b=False)
    self._VerifyInput0(transpose_a=False, transpose_b=True)
    self._VerifyInput0(transpose_a=True, transpose_b=True)
# ---
def output_close_html(self):
        text = self.token['text']
        return self.renderer.block_html(text)
# ---
def _update_state(self, result):
        super()._update_state(result)
        if isinstance(result, TemplateError):
            self._state = None
            return
        self._state = result.lower() in ("true", STATE_ON)
# ---
def _array_if_named(x):
        if isinstance(x, NamedArray):
            return x.array
        return x
# ---
def isdtype(dtype, kind):
    return nxp.isdtype(dtype, kind)
# ---
def write_jsonl_gz():
    """Fixture to write JSONL gzipped files."""

    def _write(path: Path, records: list[dict]) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        with gzip.open(path, "wt", encoding="utf-8") as handle:
            for record in records:
                handle.write(json.dumps(record))
                handle.write("\n")

    return _write
# ---
def beta(key, shape: AxisSpec, a: NamedOrNumeric, b: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    a = broadcast_to(a, shape).array
    b = broadcast_to(b, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.beta(key=key, a=a, b=b, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def comments():
    return [make_comment() for _ in range(3)]
# ---
def testImportMember(self):
    self.assertEqual((0, "<type 'dict'>\n"), _GrumpRun(textwrap.dedent("""\
        from sys import modules
        print type(modules)""")))
# ---
def __get__(self, instance, owner):
		if instance is None:
			raise AttributeError # ?

		if self.field.name not in instance.__dict__:
			json_string = getattr(instance, self.field.attname)
			instance.__dict__[self.field.name] = json.loads(json_string)

		return instance.__dict__[self.field.name]
# ---
def split(word): 
    return [char for char in word]
# ---
def to_markdown(html, config: HtmlToMarkdownConfig = HtmlToMarkdownConfig()):
    if isinstance(html, str):
        html = BeautifulSoup(html, "html.parser")
    text = MyMarkdownConverter(config).convert_soup(html)
    # cleanup: replace nbsp as space
    # this isn't quite right if we preserve html in places, but we currently are not doing that
    text = text.replace("\xa0", " ")
    return text
# ---
def test_random(spec, executor):
    a = cubed.random.random((10, 10), chunks=(4, 5), spec=spec)

    assert a.shape == (10, 10)
    assert a.chunks == ((4, 4, 2), (5, 5))
    assert a.dtype == xp.float64

    x = nxp.unique_values(a.compute(executor=executor))
    assert x.dtype == xp.float64
    assert len(x) > 90
# ---
def _get_available_subsets(cfg: TransformSFTDatasetConfig) -> Sequence[str | None]:
    configured_subsets = unwrap_versioned_value(cfg.subsets)
    if configured_subsets:
        return configured_subsets

    try:
        subsets = datasets.get_dataset_config_names(cfg.source)
    except Exception as exc:
        logging.log(logging.WARNING, f"Unable to fetch dataset configs for {cfg.source}: {exc}")
        subsets = []
    if not subsets:
        return [None]
    return subsets
# ---
def move_num(test_str):
  res = ''
  dig = ''
  for ele in test_str:
    if ele.isdigit():
      dig += ele
    else:
      res += ele
  res += dig
  return (res)
# ---
def playlist_next(self, mode='weak'):
        self.command('playlist_next', mode)
# ---
def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = "{median:.4f} ({global_avg:.4f})"
        self.deque: deque[float] = deque(maxlen=window_size)
        self.total: float = 0.0
        self.count: int = 0
        self.fmt: str = fmt
# ---
def divisor(n):
  for i in range(n):
    x = len([i for i in range(1,n+1) if not n % i])
  return x
# ---
def _update_ha_redundancy_level(self, context, logical_global_router,
                                    delta):
        with context.session.begin(subtransactions=True):
            log_g_router_db = self._l3_plugin._get_router(
                context, logical_global_router['id'])
            log_g_router_db.ha_settings.redundancy_level += delta
            context.session.add(log_g_router_db.ha_settings)
# ---
def to_proto(self) -> "time_pb2.Duration":
        """Convert to proto Duration message."""
        return time_pb2.Duration(milliseconds=self._ms)
# ---
def division_elements(test_tup1, test_tup2):
  res = tuple(ele1 // ele2 for ele1, ele2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def skip_if_no_torch(f):
    return pytest.mark.skipif(not has_torch(), reason="torch not installed")(f)
# ---
def __init__(self, workspace: Path):
        self._workspace = workspace
# ---
def schedule(request: pytest.FixtureRequest) -> TrainSchedule:
    return request.param
# ---
def new(self, **kwargs):
        """ Return a field of the same type as ``self``, with its own parameters. """
        return type(self)(**kwargs)
# ---
def are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str):
    are_equal = False
    try:
        expr = f"({ground_truth_normalized})-({given_normalized})"
        if should_allow_eval(expr):
            sympy_diff = _sympy_parse(expr)
            simplified = sympy.simplify(sympy_diff)
            if simplified == 0:
                are_equal = True
    except Exception:
        pass
    return are_equal
# ---
def on_answer(self, *args):
        pass
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_inverse: typing.Literal[True],
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def run_async(
        self,
        cmd: list[str],
        *,
        env: dict[str, str] | None = None,
        **kwargs,
    ) -> subprocess.Popen:
        """Start a command within the venv without waiting."""
        if env is None:
            env = self.get_env()
        return self._job_group.run_async(cmd, env=env, **kwargs)
# ---
def format_archive_policy(ap):
    format_dict_list(ap, "definition")
    format_string_list(ap, "aggregation_methods")
# ---
def visit_msub(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            sub = self._visit(children[1])
            return BracedNode(f"{{{base}}}_{{{sub}}}")
        return TextNode("")
# ---
def test_parse_chat_completion_logprobs(tokenizer):
    """Test logprob extraction."""
    token_strs = ["Hello", "world"]
    logprobs = [-0.123, -0.456]

    chat_completion = create_mock_chat_completion_with_logprobs("Hello world", token_strs, logprobs)

    parsed_logprobs = parse_chat_completion_logprobs(chat_completion)

    assert len(parsed_logprobs) == 2
    assert parsed_logprobs == logprobs
# ---
def _matches(self, **kwargs):
		for k, v in kwargs.items():
			if not self._query(k, v):
				return False
		return True
# ---
def intermediate(self, x, scalar):
            return x + 2 * self.w + scalar
# ---
def __del__(self):
        self.stop()
# ---
def filter_lidar_data_by_neighbourhood(in_pts,target_xy,radius):
    pts = np.zeros((0,in_pts.shape[1]))
    if in_pts.shape[0]>0:
        x,y,inside =  points_in_radius(in_pts[:,0],in_pts[:,1],target_xy[0],target_xy[1],radius)
        pts = in_pts[inside,:]
    else:
        print( "\t\t\t no points in neighbourhood")
    return pts
# ---
def get_team_name(code):
    return team_mapping[code]
# ---
def print_key(self, match):
        self._call_all('print_key', match)
# ---
def unpermute_sharded(out_repeat_sort_: Array, sort_idx_: Array):
            inv_sort_idx_ = jnp.argsort(sort_idx_)
            out_repeat_ = jnp.take(out_repeat_sort_, inv_sort_idx_, axis=0)
            out_repeat_unflat_ = jnp.reshape(out_repeat_, (-1, self.config.num_experts_per_tok, self.config.hidden_dim))

            return out_repeat_unflat_
# ---
def get_github_token() -> str:
    """Get GitHub token from environment variable."""
    token = os.getenv("MARIN_CI_TOKEN")

    if not token:
        logging.error("MARIN_CI_TOKEN environment variable not set")
        logging.error("Create a fine-grained PAT with repo:actions scope")
        sys.exit(1)

    return token
# ---
def matplotlib_pyplot():
  import matplotlib  # pylint: disable=g-import-not-at-top
  matplotlib.use("agg")
  import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top
  return plt
# ---
def device(self):
        return "cpu"
# ---
def test_binary_bytes(self):
        """Store and retrieve bytes as a binary"""
        self.make_table()
        data = {"a": 1, "b": 2}
        self.dynamo.put_item("foobar", {"id": "a", "data": Binary(dumps(data))})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(loads(item["data"].value), data)
# ---
def _as_array(x):
            # for dumb reasons, pa.array doesn't support ndarrays with ndim > 1
            if isinstance(x, np.ndarray) and x.ndim > 1:
                return [_as_array(y) for y in x]
            elif isinstance(x, np.ndarray):
                return list(x)
            else:
                return pa.array(x)
# ---
def _is_mean_reduction(reduction):
    # Mean reductions need special handling for weighted reductions
    return reduction is hax.mean or reduction is jnp.mean
# ---
def check_answer(self, sample_str: str, ground_truth: str) -> bool:
        """Grade the answer using Tinker's grading."""
        try:
            answer = extract_boxed(sample_str)
        except ValueError:
            return False
        return grade_answer(answer, ground_truth)
# ---
def test_unify_chunks_elemwise_2d(chunks_a, chunks_b, expected_chunksize):
    a = xp.ones((10, 10), chunks=chunks_a)
    b = xp.ones((10, 10), chunks=chunks_b)

    _, arrays = unify_chunks(a, "ij", b, "ij")
    for arr in arrays:
        assert arr.chunksize == expected_chunksize

    c = xp.add(a, b)
    assert_array_equal(c.compute(), np.ones((10, 10)) + np.ones((10, 10)))
# ---
def test_backup(self):
        assert os.path.exists(self.bakname)
        backup = open(self.bakname).read()
        assert_equals(self.original, backup)
# ---
def greater_specificnum(list,num):
 greater_specificnum=all(x >= num for x in list)
 return greater_specificnum
# ---
def getBitmap(self, callingWindow, context, mainItem):
        return None
# ---
def check_status(self):
        if self._process and self._process.poll() is not None:
            print(f"Remote process exited with code {self._process.returncode}")
# ---
def norm_grads(g):
            return g / (jnp.linalg.norm(g) + 1e-16)
# ---
def to_dict(self):
        """
        Convert the TaskConfig to a dictionary, excluding None values.

        Returns:
            Dictionary representation of the task configuration
        """
        base_dict = dataclasses.asdict(self)
        return {k: v for k, v in base_dict.items() if v is not None}
# ---
def _iter_progress(self, stream):
        chunk = ''
        while True:
            c = stream.read(1)
            if not c:
                raise OutputParserError('copy-disk stream closed unexpectedly')
            chunk += c
            if c == '\r':
                yield chunk
                chunk = ''
# ---
def test_nested_field_with_comparison(self):
        expr = col("meta")["score"] > 0.5
        assert expr.evaluate({"meta": {"score": 0.8}}) is True
        assert expr.evaluate({"meta": {"score": 0.2}}) is False
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_index: typing.Literal[True],
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def update_secret(self, key, value):
        ''' update a secret'''
        if key in self.secrets:
            self.secrets[key] = value
        else:
            self.add_secret(key, value)

        return True
# ---
def icon(self):
        """Icon to use in the frontend, if any."""
        return self._icon
# ---
def __ne__(self, other):  # pragma: no cover
        return haliax.not_equal(self, other)
# ---
def KVCombinedSize(self) -> Axis:
        return Axis("kv_combined", self.kv_lora_rank + self.qk_rope_head_dim)
# ---
def test_host_reboot(self):
        self._test_host_action(self.conn.host_power_action, 'reboot')
# ---
def backwards(self, orm):

        # Deleting model 'Journey'
        db.delete_table('places_journey')

        # Deleting model 'ScheduledStop'
        db.delete_table('places_scheduledstop')
# ---
def __getitem__(self, key):
        if key == () and self.shape == ():
            return nxp.asarray(0, dtype=self.dtype)
        return numpy_array_to_backend_array(
            np.ravel_multi_index(_key_to_index_tuple(key), self.shape), dtype=self.dtype
        )
# ---
def __getitem__(self, index: int) -> T_co:
        return self.get_batch([index])[0]
# ---
def tagged_eval_sets(self, Pos: Axis) -> list[tuple[AsyncDataset[LmExample], list[str]]]:
        eval_sets = self.validation_sets(Pos)
        tagged = []
        for name, ds in eval_sets.items():
            tags = (self.components[name].tags or []) + [name]
            tagged.append((ds, tags))
        return tagged
# ---
def fsspec_mkdirs(dirname, exist_ok=True):
  """Mkdirs in manner compatible with fsspec."""
  fs, _ = fsspec.core.url_to_fs(dirname)
  fs.makedirs(dirname, exist_ok=exist_ok)
# ---
def __call__(self, x):
            return x + self.weight
# ---
def test_one_step_edit_returns_first_edit():
    source = "a = 1\nb = 2\n"
    target = "a = 10\nb = 20\n"
    mutation = one_step_edit(source, target)
    assert mutation is not None

    # Applying should produce valid Python.
    result = mutation.apply(source)
    ast.parse(result)
# ---
def _sanitize_shard_name(name: str) -> str:
    safe = "".join(ch if ch.isalnum() or ch in ("-", "_", ".") else "_" for ch in name)
    return safe or "shard"
# ---
def run(self):
        cmd = '''
              docker run --rm -v rita_store:/rita/data  rita/test-python --inputfile {} --outputfile {}
        '''.format(os.path.join("/rita/data", os.path.basename(self.input().path)),
                   os.path.join("/rita/data", os.path.basename(self.output().path)))

        logger.debug(cmd)

        out = subprocess.call(cmd, shell=True)

        logger.debug(out)
# ---
def leaderboard():
    """Leaderboard page for SciNet"""
    get_db()
    groups = get_groups(g.groups_collection)
    return render_template("leaderboard.html", groups=groups)
# ---
def dummy(*args, **kwargs):
            pass
# ---
def temp_bundle_dir():
    """Create a temporary directory for bundle storage testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)
# ---
def group_id(self) -> str:
        return self._slice_id
# ---
def _path_to_key(self, gcs_path: str) -> str:
        return hashlib.sha256(gcs_path.encode()).hexdigest()[:16]
# ---
def __init__(self):
        super(FIFOQueue, self).__init__()
# ---
def in_memory_table(small_parquet_path: str) -> pa.Table:
    """
    Loads 250k rows into memory once. Used for marshaling and batch size tuning benchmarks.
    """
    return pq.read_table(small_parquet_path)
# ---
def test_to_obj_list():
    msg = '["hoge", "hogi", {"atr1": "val2", "atr2": 1.0}]'
    bb = jps.utils.to_obj(msg)
    assert len(bb) == 2
    assert bb[0] == 'hoge'
    assert bb[1] == 'hogi'
    assert bb[2].atr1 == 'val2'
    assert bb[2].atr2 == 1.0
# ---
def rev(num):    
    rev_num = 0
    while (num > 0):  
        rev_num = (rev_num * 10 + num % 10) 
        num = num // 10  
    return rev_num  
def check(n):    
    return (2 * rev(n) == n + 1)
# ---
def bell_Number(n): 
    bell = [[0 for i in range(n+1)] for j in range(n+1)] 
    bell[0][0] = 1
    for i in range(1, n+1):
        bell[i][0] = bell[i-1][i-1]
        for j in range(1, i+1): 
            bell[i][j] = bell[i-1][j-1] + bell[i][j-1] 
    return bell[n][0]
# ---
def InsertRecord(x, y):
	student_phoneNumber_name[x] = y
	return;
# ---
def _refresh_target_selection(self):
        if not self.panes:
            return
        target = self.selected_pane.selected_target
        self._selected_target_index = 0
        if target is not None:
            try:
                self._selected_target_index = self.target_accounts.index(target) + 1
            except ValueError:
                pass
# ---
def transform_to(self, srs):
        if srs == self.srs:
            return self

        geom = transform_geometry(self.srs, srs, self.geom)
        return GeomCoverage(geom, srs)
# ---
def test_linear_has_no_function_leaves_by_default():
    H, C, W, E = hax.make_axes(H=10, C=12, W=14, E=16)

    hax_linear = hax.nn.Linear.init((H, C, W), E, key=jrandom.PRNGKey(0))
    assert all(not isinstance(v, Callable) for v in jax.tree_util.tree_leaves(hax_linear))
# ---
def open_if_lazy_zarr_array(array: T_ZarrArray) -> zarr.Array:
    """If array is a LazyZarrArray then open it, leaving other arrays unchanged."""
    return array.open() if isinstance(array, LazyZarrArray) else array
# ---
def _test_do_execute(self, retval):
        with self._run_test(retval) as (conn, m1):
            result = conn.execute("insert into table foo", {"foo": "bar"})
        self._assert(
            retval,
            m1.do_execute, m1.real_do_execute,
            [call(
                    result.context.cursor,
                    "insert into table foo",
                    {"foo": "bar"}, result.context)]
        )
# ---
def __getitem__(self, index: int) -> T:
        return self._index_to_obj[index]
# ---
def close(self):
        if self._makefile_refs < 1:
            self._connection = None
            if self._sock:
                socket.socket.close(self._sock)
        else:
            self._makefile_refs -= 1
# ---
def redirect(h, req, url=to_url):
            h.http_error_302(req, MockFile(), 302, "Blah",
                             MockHeaders({"location": url}))
# ---
def __unicode__(self):
		return self.titulo
# ---
def output_block_quote(self):
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'block_quote_end':
            body += self.tok()
        return self.renderer.block_quote(body)
# ---
def CurrentLineAndColumn():
  """Returns the 0-based current line and 0-based current column."""
  # See the comment in CurrentColumn about the calculation for the line and
  # column number
  line, column = vim.current.window.cursor
  line -= 1
  return line, column
# ---
def stop(self) -> None:
        """Stop the actor server and wait for threads to exit."""
        self._threads.stop()
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "not %s" % self.rule
# ---
def slice(self, start: Mapping[AxisSelector, int], length: Mapping[AxisSelector, int | Axis]) -> "NamedArray": ...
# ---
def __init__(self, maxlen: int | None = None):
        self._queue: list[RolloutBatch] = []
        self._maxlen = maxlen
        self._lock = threading.Lock()
        self._not_full = threading.Condition(self._lock)
        self._not_empty = threading.Condition(self._lock)
# ---
def assert_messages_in_order(rendered: str, roles: Iterable[str]) -> None:
    search_pos = 0
    for role in roles:
        marker = f"<|start_header_id|>{role}<|end_header_id|>"
        pos = rendered.find(marker, search_pos)
        assert pos != -1, f"Did not find role {role!r} after position {search_pos}"
        search_pos = pos + 1
# ---
def and_tuples(test_tup1, test_tup2):
  res = tuple(ele1 & ele2 for ele1, ele2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def __floordiv__(self, other, /):
        other = self._check_allowed_dtypes(other, "real numeric", "__floordiv__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.floor_divide, self, other, dtype=result_type(self, other))
# ---
def concat_axes(a1: AxisSpec, a2: AxisSpec) -> AxisSpec:
    pass
# ---
def __delete__(self, obj):
        if hasattr(obj, self.slot):
            delattr(obj, self.slot)
# ---
def decode_hooks(self):
    return [convert_predictions_to_image_summaries]
# ---
def test_get_insert_cmd(self):
        print(get_insert_cmd(go('Integer'), base_col_def))
        print(get_insert_cmd(go('String'), base_col_def))
        print(get_insert_cmd(go('Point'), base_col_def))
        print(get_insert_cmd(go('Role'), base_col_def))
        print(get_insert_cmd(go('RoleIO'), base_col_def))
        print(get_insert_cmd(go('Log'), base_col_def))
        print(get_insert_cmd(go('Meta'), base_col_def))
# ---
def write_input(
    output_dir: Path,
    num_docs: int,
    words_per_doc: int,
    num_input_files: int,
) -> None:
    """Generate and write input files for benchmarking."""
    output_dir.mkdir(parents=True, exist_ok=True)
    setup_input_files(num_docs, words_per_doc, num_input_files, str(output_dir))
    print(f"\nInput files written to: {output_dir}")
# ---
def reserved_mem(self) -> int:
        """
        The memory reserved on a worker for non-data use when running a task, in bytes.

        See Also
        --------
        cubed.measure_reserved_mem
        """
        return self._reserved_mem
# ---
def bitwise_and(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "bitwise_and")
    if (
        x1.dtype not in _integer_or_boolean_dtypes
        or x2.dtype not in _integer_or_boolean_dtypes
    ):
        raise TypeError("Only integer or boolean dtypes are allowed in bitwise_and")
    return elemwise(nxp.bitwise_and, x1, x2, dtype=result_type(x1, x2))
# ---
def dtype(self):
        """Data type of the array elements."""
        return self._dtype
# ---
def heartbeat(self, request: cluster__pb2.HeartbeatRequest, ctx: RequestContext) -> cluster__pb2.HeartbeatResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def test_basic_order():
    partial_order = ("apple", ..., "banana")
    candidates = ("banana", "apple", "cherry")
    expected_output = ("apple", "cherry", "banana")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert actual_output == expected_output
    assert_partial_order_respected(partial_order, actual_output)
# ---
def fsspec_exists(filename):
  """Check if a file exists using fsspec."""
  fs, _ = fsspec.core.url_to_fs(filename)
  return fs.exists(filename)
# ---
def parse_news(item):
    '''Parse news item
    return is a tuple(id, title, url)
    '''
    url = 'http://www.spa.gov.sa' + item['href']
    url_parsed = urlparse(url)
    qs = parse_qs(url_parsed[4])
    id = qs['newsid'][0]
    title = item.h2.contents[0]
    title = " ".join(title.split())
    item_parsed = (id, title, url)
    return item_parsed
# ---
def negative(x):
    assert isinstance(x, int)
    return -x
# ---
def get_git_commit() -> str | None:
    """Return the git commit of the current branch (if it can be found)"""
    if os.path.exists(".git"):
        return os.popen("git rev-parse HEAD").read().strip()
    else:
        return None
# ---
def max_volume (s): 
    maxvalue = 0
    i = 1
    for i in range(s - 1): 
        j = 1
        for j in range(s): 
            k = s - i - j 
            maxvalue = max(maxvalue, i * j * k)         
    return maxvalue
# ---
def __init__(self, subquery):
        self.subquery = subquery
# ---
def split_2d_3d(ds: xr.Dataset, depth_dim="lev"):
    ds_2d = xr.Dataset({v: ds[v] for v in ds.data_vars if depth_dim not in ds[v].dims})
    ds_3d = xr.Dataset({v: ds[v] for v in ds.data_vars if depth_dim in ds[v].dims})
    return ds_2d, ds_3d
# ---
def get_actor_pool_name(self) -> str:
        assert self._slice_info
        return f"slice {self._slice_info.slice_name}"
# ---
def x_read():
        x_read_future.start()
# ---
def __iter__(self):
        return iter(self._calls)
# ---
def __repr__(self):
        return f"EraShufflingDataset({repr(self.dataset)}, era_length={self.era_length})"
# ---
def test_venv_creation():
    """Test basic venv creation and cleanup."""
    venv_path = None
    with TemporaryVenv() as venv:
        venv_path = venv.venv_path
        assert os.path.exists(venv_path)
        assert os.path.exists(venv.python_path)
        assert os.path.exists(venv.bin_path)
        assert venv.python_path == os.path.join(venv_path, "bin", "python")
        assert venv.bin_path == os.path.join(venv_path, "bin")
    assert not os.path.exists(venv_path)
# ---
def test_decode_token_specials(tok):
    assert tok.decode_token(tok.pad_token_id) == ""
    assert tok.decode_token(tok.sos_token_id) == "<SOS>"
    assert tok.decode_token(tok.eos_token_id) == "<EOS>"
# ---
def test_cursor_execute_w_replace(self):
        self._test_cursor_execute(True)
# ---
def test_can_accept_demand_true_when_available(self, unbounded_config: config_pb2.ScaleGroupConfig):
        """can_accept_demand() returns True when AVAILABLE."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(unbounded_config, manager)

        assert group.can_accept_demand() is True
# ---
def __init__(self, fp, msg, status, reason):
        self.fp = fp
        self.msg = msg
        self.status = status
        self.reason = reason
        self.code = 200
# ---
def init_logits():
        xw_tiled[...] = jnp.zeros_like(xw_tiled)
# ---
def ravel(self, new_axis_name: AxisSelector) -> "NamedArray":  # pragma: no cover
        return haliax.ravel(self, new_axis_name=new_axis_name)
# ---
def create(self, cr, user, vals, context=None):
        if ('name' not in vals) or (vals.get('name')=='/'):
            seq_obj_name =  self._name
            vals['name'] = self.pool.get('ir.sequence').get(cr, user, seq_obj_name)
        new_id = super(stock_picking, self).create(cr, user, vals, context)
        return new_id
# ---
def topbottom_surfacearea(r):
  toporbottomarea=3.1415*r*r
  return toporbottomarea
# ---
def GetVimGlobalsKeys():
  return vim.eval( 'keys( g: )' )
# ---
def __iter__(self):
        return (x[0] for x in self._data.values())
# ---
def multiple_to_single(L):
  x = int("".join(map(str, L)))
  return x
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        """Block until the job completes or timeout expires."""
        try:
            self._future.result(timeout=timeout)
        except Exception:
            if raise_on_failure:
                raise
        return self.status()
# ---
def get_containing(self, name, depth = 0):
        """Return the n-th (n = ``depth``) context containing attribute named ``name``."""
        ctx_dict = object.__getattribute__(self, '__dict__')
        if name in ctx_dict:
            if depth <= 0:
                return self
            depth -= 1
        parent = ctx_dict.get('_parent')
        if parent is None:
            return None
        return parent.get_containing(name, depth = depth)
# ---
def init_block(i):
                (block_args, block_kwargs) = haliax.tree_util.tree_map(
                    functools.partial(BlockSeq._slice_out, Block, i), (args, kwargs)
                )
                return module.init(*block_args, **block_kwargs)
# ---
def _product_all_get(self, cr, uid, id, product_ids=False, context=None, states=None):
        if states is None:
            states = ['done']
        # build the list of ids of children of the location given by id
        ids = id and [id] or []
        location_ids = self.search(cr, uid, [('location_id', 'child_of', ids)])
        return self._product_get_multi_location(cr, uid, location_ids, product_ids, context, states)
# ---
def num_lines(string):
    """
    Return number of lines.
    """
    line_list = string.split("\n")
    num = len(line_list)
    for l in line_list:
        num += (len(string) // LINEWIDTH + 1)

    return num
# ---
def use_backups_default(spec: Optional[Spec]) -> bool:
    """The default setting for ``use_backups``.

    Backup tasks are only enabled on cloud object stores since they provide atomic writes.
    """
    return (
        spec is not None
        and spec.work_dir is not None
        and is_cloud_storage_path(spec.work_dir)
    )
# ---
def __init__(self, app):
        self._app = app
        self._items = []
        self.model = MyMusicModel(app)
# ---
def _assert(self, retval, m1, m2, mock_calls):
        eq_(m1.mock_calls, mock_calls)
        if retval:
            eq_(m2.mock_calls, [])
        else:
            eq_(m2.mock_calls, mock_calls)
# ---
def sum_list(lst1,lst2):
  res_list = [lst1[i] + lst2[i] for i in range(len(lst1))] 
  return res_list
# ---
def test_subscribe(self):
        """Can subscribe and unsubscribe from hooks"""
        hook = lambda: None
        self.dynamo.subscribe("precall", hook)
        self.assertEqual(len(self.dynamo._hooks["precall"]), 1)
        self.dynamo.unsubscribe("precall", hook)
        self.assertEqual(len(self.dynamo._hooks["precall"]), 0)
# ---
def test_get_all_bw_usage_in_failure_case(self):
        """Test that get_all_bw_usage returns an empty list when metrics
        compilation failed.  c.f. bug #910045.
        """
        class testinstance(object):
            def __init__(self):
                self.name = "instance-0001"
                self.uuid = "1-2-3-4-5"

        result = self.conn.get_all_bw_usage([testinstance()],
                datetime.datetime.utcnow())
        self.assertEqual(result, [])
# ---
def _run_with_lockfile():
        with remove_tpu_lockfile_on_exit():
            convert_task()
# ---
def list_tasks(self) -> list[TaskInfo]:
        """List all tasks.

        Returns TaskInfo views (implemented by TaskAttempt) to decouple callers
        from execution internals.
        """
        return list(self._tasks.values())
# ---
def name(self) -> str:
        return "dask"
# ---
import re
def find_long_word(text):
  return (re.findall(r"\b\w{5}\b", text))
# ---
def get_shard_source(self, split) -> ShardedDataSource[dict] | None:
        split_urls = self.urls_for_split(split)

        if len(split_urls) == 0:
            return None

        return UrlDataSource(split_urls)
# ---
def __init__(self, srcs: list[DataSource], target_time: int):
        self.srcs = srcs
        self._n_batches = 0
        self._variable_metrics: dict | None = None
        self._target_time = target_time
# ---
def probe(self) -> cluster_pb2.WorkerMetadata: ...
# ---
def __str__(self):
        return f"EraShufflingDataset({str(self.dataset)})"
# ---
def test_do_executemany_wo_replace(self):
        self._test_do_executemany(False)
# ---
def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the AndCheck object for convenience.
        """

        self.rules.append(rule)
        return self
# ---
def count(self) -> int:
        return self._count
# ---
def current_tracker(tracker: "Tracker") -> typing.ContextManager:
    """Returns a context manager for setting the global tracker"""
    ...
# ---
def cli(ctx):
    """Token Decoder REPL."""
    if ctx.invoked_subcommand is None:
        ctx.invoke(repl_cmd)
# ---
def intersect_axes(ax1: ShapeDict, ax2: AxisSelection) -> ShapeDict: ...
# ---
def test_actor_options_default_preemptible():
    from fray.v2.ray_backend.backend import _actor_ray_options

    options = _actor_ray_options(ResourceConfig())
    assert options["num_cpus"] == 1
    assert "resources" not in options
# ---
def _parse_template(self, tmpl_str, msg_str):
        parse_ex = self.assertRaises(ValueError,
                                     template_format.parse,
                                     tmpl_str)
        self.assertIn(msg_str, six.text_type(parse_ex))
# ---
def default_combiner(left, right):
                if left is None or right is None:
                    raise ValueError(
                        "Default combiner requires both left and right items (use custom combiner for outer joins)"
                    )
                return {**left, **right}
# ---
def __rshift__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer", "__rshift__")
        if other is NotImplemented:
            return other
        return elemwise(
            nxp.bitwise_right_shift, self, other, dtype=result_type(self, other)
        )
# ---
def url_query_params(url):
    """Return query parameters as a dict from the specified URL.

    :param url: URL.
    :type url: str
    :rtype: dict
    """
    return dict(parse_qsl(urlparse(url).query, True))
# ---
def shutdown(self, wait: bool = True) -> None:
        del wait
# ---
def check_equilateral(x,y,z):
  if x == y == z:
	   return True
  else:
     return False
# ---
def dtype(self):
        """Return the dtype of the underlying reference."""
        return self._ref.dtype
# ---
def get_median(arr1, arr2, n):
  i = 0
  j = 0
  m1 = -1
  m2 = -1
  count = 0
  while count < n + 1:
    count += 1
    if i == n:
      m1 = m2
      m2 = arr2[0]
      break
    elif j == n:
      m1 = m2
      m2 = arr1[0]
      break
    if arr1[i] <= arr2[j]:
      m1 = m2
      m2 = arr1[i]
      i += 1
    else:
      m1 = m2
      m2 = arr2[j]
      j += 1
  return (m1 + m2)/2
# ---
def calcDottedNetmask(mask):
    bits = 0
    for i in xrange(32 - mask, 32):
        bits |= (1 << i)
    packed_value = pack('!I', bits)
    addr = inet_ntoa(packed_value)
    return addr
# ---
def get_dim_index(ia, i):
            if isinstance(ia, ndindex.Slice):
                step = ia.step or 1
                return ia.start + (step * i)
            elif isinstance(ia, ndindex.IntegerArray):
                return ia.raw[i]
            else:
                raise NotImplementedError(
                    "Only integer, slice, or int array indexes are supported."
                )
# ---
def _unique_segment_ids(max_Segments, segment_ids):
    # Extract unique segment IDs with padding
    # TODO: add unique to haliax
    unique_segment_ids = jnp.unique(segment_ids.array, size=max_Segments.size, fill_value=-1)
    unique_segment_ids = hax.named(unique_segment_ids, max_Segments)
    return unique_segment_ids
# ---
def append(self, record: BufferedLogRecord) -> None: ...
# ---
def _repeat(x, repeats, axis=None, chunksize=None, block_id=None):
    out = nxp.repeat(x, repeats, axis=axis)
    bi = block_id[axis] % repeats
    ind = tuple(
        slice(bi * chunksize[i], (bi + 1) * chunksize[i]) if i == axis else slice(None)
        for i in range(x.ndim)
    )
    return out[ind]
# ---
def __add_fuzzing_range(self):
        start = self.ui.sBAddRangeStart.value()
        end = self.ui.sBAddRangeEnd.value()
        step = self.ui.sBAddRangeStep.value()
        self.fuzz_table_model.add_range(start, end + 1, step)
# ---
def _navp_xml(self, entry, indent_lvl):
        'Write xml for an entry and all its subentries.'
        xml = self._navp.format('  '*indent_lvl, str(entry.no), entry.text,
          entry.target)
        for sub in entry.entries:
            xml += self._navp_xml(sub, indent_lvl+1)
        xml += '  '*indent_lvl + '</navPoint>\n'
        return xml
# ---
def root(cls, name: str) -> "JobName":
        """Create a root job name (no parent)."""
        return cls((name,))
# ---
def test_new_websocket_client(self, check_token):
        check_token.return_value = {
            'host': 'node1',
            'port': '10000'
        }
        self.wh.socket.return_value = '<socket>'
        self.wh.path = "ws://127.0.0.1/?token=123-456-789"

        self.wh.new_websocket_client()

        check_token.assert_called_with(mock.ANY, token="123-456-789")
        self.wh.socket.assert_called_with('node1', 10000, connect=True)
        self.wh.do_proxy.assert_called_with('<socket>')
# ---
def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        for process in self._processes:
            if process.poll() is not None:
                continue
            _terminate_process(process)
# ---
def test_routing_callable_job():
    """Callable entrypoint on non-TPU device routes to _launch_callable_job."""
    request = JobRequest(
        name="callable-job",
        entrypoint=Entrypoint.from_callable(lambda: 42),
        resources=ResourceConfig(device=CpuConfig()),
    )
    assert request.entrypoint.callable_entrypoint is not None
    assert request.entrypoint.binary_entrypoint is None
    assert not isinstance(request.resources.device, TpuConfig)
# ---
def recursive_check(data):
            if isinstance(data, dict):
                for key, value in data.items():
                    if key not in yaml_keys:
                        invalid_keys.add(key)
                    recursive_check(value)
            elif isinstance(data, list):
                for item in data:
                    recursive_check(item)
# ---
def test_reshape(spec, executor):
    a = xp.arange(12, chunks=4, spec=spec)
    b = xp.reshape(a, (3, 4))

    assert_array_equal(
        b.compute(executor=executor),
        np.arange(12).reshape((3, 4)),
    )
# ---
def create_router(self, router):
        """Handling create router event."""
        pass
# ---
def num_shards(self) -> int:
        return len(self.shard_names)
# ---
def test_connect_error_with_traceback_populates_timestamp() -> None:
    try:
        raise ValueError("boom")
    except ValueError as exc:
        err = connect_error_with_traceback(Code.INTERNAL, "Error launching job", exc=exc)

    details = extract_error_details(err)
    assert details is not None
    assert details.exception_type.endswith("ValueError")
    assert details.timestamp.epoch_ms > 0
# ---
def reset(self) -> None:
        try:
            os.unlink(self._path)
        except FileNotFoundError:
            pass
# ---
def run(self):
        while not self._quit:
            try:
                if self.serial.isOpen() and self.serial.inWaiting():
                    self.data_received.emit(
                        datetime.datetime.now(),
                        strip(bytearray_to_utf8(self.serial.readline()))
                    )
            except SerialException:
                pass
# ---
def _dummy_step_info(step):
    return StepInfo(
        state=TrainerState(
            # + 1 b/c step here is next step
            step=step + 1,
            model=None,
            optimizer=None,  # type: ignore
            opt_state=None,
            training_key=jax.random.PRNGKey(0),
            is_trainable=True,
            mp=None,
            model_averaging=None,
        ),
        loss=0.0,
        step_duration=0.0,
    )
# ---
def on_task_end(self, event):
        allowed_mem = self.ops[event.name].allowed_mem
        if (
            event.peak_measured_mem_end is not None
            and event.peak_measured_mem_end > allowed_mem
        ):
            self.counter.update({event.name: 1})
# ---
def on_delivery(self, err, msg):
        if err:
            print('Delivery report: Failed sending message {0}'.format(msg.value()))
            print(err)
            # We could retry sending the message
        else:
            print('Message produced, offset: {0}'.format(msg.offset()))
# ---
def split_into(iterable, sizes):
    """Yield a list of sequential items from *iterable* of length 'n' for each
    integer 'n' in *sizes*."""
    it = iter(iterable)
    for size in sizes:
        yield list(islice(it, size))
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "WhisperDecoder":
        new_embeddings = self.embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, embeddings=new_embeddings)
# ---
def highest_Power_of_2(n): 
    res = 0; 
    for i in range(n, 0, -1): 
        if ((i & (i - 1)) == 0): 
            res = i; 
            break; 
    return res;
# ---
def isLeft(a, b, c):
    return 1 if ((b[0] - a[0])*(c[1] - a[1]) - (b[1] - a[1])*(c[0] - a[0])) > 0 else -1;
# ---
def function_def_from_tf_function(c_func):
  """Converts a SWIG-wrapped TF_Function* to a FunctionDef proto."""
  with c_api_util.tf_buffer() as buf:
    c_api.TF_FunctionToFunctionDef(c_func, buf)
    data = c_api.TF_GetBuffer(buf)
  fdef = function_pb2.FunctionDef()
  fdef.ParseFromString(compat.as_bytes(data))
  return fdef
# ---
def do_go(self):
        self.do_time_ctrl('start')
# ---
def get_line(self, n):
        """
        Return the line with index n.
        """
        if n < self.num_lines:
            return self.lines[n]
        else:
            raise IndexError("Line index out of range.")
# ---
def refresh_window_title(self):
        s = "%s %s" % (QCoreApplication.applicationName(),
                       QCoreApplication.applicationVersion())
        if self.filename is not None:
            s += " - " + self.filename
        if self.dirty:
            s += "*"
        self.setWindowTitle(s)
# ---
def test_status(self):
        eq_(self.record.status, [])
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()
        return False
# ---
def count_all(self):
        """Tells how many events have been counted globally

        Returns:
            int

        """
        return self.events_count
# ---
def add_dims_to_spec(qss, sds):
            if partition_grads_into_blocks:
                qss = jax.tree.map(lambda qs: PartitionSpec(*((None,) + qs)), qss)
            if sds is not None:
                qss = jax.tree.map(lambda qs: PartitionSpec(*(sds + qs)), qss)
            return qss
# ---
def Layers(self) -> Axis:
        """Alias for :attr:`Block` used by some downstream code."""

        return self.Block
# ---
def __init__(self):
                self.array = jnp.zeros((8, 8))
# ---
def _type_list_to_str(types):
  if any([_ not in _DTYPE_TO_STR for _ in types]):
    raise ValueError("Unsupported dtypes: %s" % types)
  return "".join([_DTYPE_TO_STR[_] for _ in types])
# ---
def reset(self) -> "PageTable":
        ref_counts = hax.full_like(self.page_ref_counts, 0)
        return PageTable(ref_counts, self.page_size, self._max_seqs, self._pages_per_seq)
# ---
def _poll_submission(self) -> JobStatus:
        client = JobSubmissionClient(self._dashboard_address)
        info = client.get_job_info(self._submission_id)
        return _convert_ray_status(info.status)
# ---
def __init__ (self, url=None, scheduler='default', session=None) :

        Attributes.__init__ (self)
# ---
def __post_init__(self):
        if self.max_samples_per_benchmark is not None and self.max_samples_per_benchmark < 0:
            raise ValueError("max_samples_per_benchmark must be non-negative or None")
# ---
def __gt__(self, other, /):
        other = self._check_allowed_dtypes(other, "all", "__gt__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.greater, self, other, dtype=nxp.bool)
# ---
def _handle_image(self, attrs):
        'Returns the alt text of an image tag.'
        try:
            return attrs['alt']
        except KeyError:
            return ''
# ---
def get_metrics(self) -> dict:
        pass
# ---
def resize_vocab(self, new_size: int, key: PRNGKeyArray | None = None) -> "HackableLMHeadModel":
        raise NotImplementedError("resize_vocab is not implemented for HackableLMHeadModel")
# ---
def get_max_sum (n):
	res = list()
	res.append(0)
	res.append(1)
	i = 2
	while i<n + 1:
		res.append(max(i, (res[int(i / 2)] 
						+ res[int(i / 3)] +
							res[int(i / 4)]
						+ res[int(i / 5)])))
		i = i + 1
	return res[n]
# ---
def _description_string(self, env):
        if self.string and env.lang:
            field = self.base_field
            name = "%s,%s" % (field.model_name, field.name)
            trans = env['ir.translation']._get_source(name, 'field', env.lang)
            return trans or self.string
        return self.string
# ---
def check(self):
        if self.name not in step_names:
            raise ValueError(
                f"Invalid step name: {self.name}. Available steps: {step_names}"
            )
        if not os.path.exists(self.config_path):
            raise FileNotFoundError(f"Config file not found: {self.config_path}")
# ---
def __init__(self, name=None, display_name=None, internal_name=None, to_be_exported_for_shoebox=None):
        super(Dimension, self).__init__()
        self.name = name
        self.display_name = display_name
        self.internal_name = internal_name
        self.to_be_exported_for_shoebox = to_be_exported_for_shoebox
# ---
def _convert_id_to_token(self, index: int) -> str:
    return self._vocab_int_to_str[index]
# ---
def trainer_pair(
    train_config,
    request,
    config_name: str,
) -> Generator[tuple[TrainConfig, Trainer], None, None]:
    """Provides a state-scoped config and trainer for tests."""
    with MultitonScope():
        trainer = Trainer(train_config)

        # cur_step will set the number of pairs in the input/output sample
        trainer.init_data_loaders(cur_step=train_config.steps[0])

        yield train_config, trainer
# ---
def lang_del(self):
        del self._lang
        if self.req is not None and self.req.environ.get('etalage') is not None \
                and '_lang' in self.req.environ['etalage']:
            del self.req.environ['etalage']['_lang']
# ---
def remove_title(html: BeautifulSoup):
    # Title is added by markdown parser
    title = html.find("title")
    if title:
        title.decompose()
# ---
def start_date(self):
        """ Returns a date object of the todo's start date. """
        return self.get_date(config().tag_start())
# ---
def log_hyperparameters(hparams: dict[str, Any]):
    """
     Log hyperparameters to the global tracker.

    Args:
         hparams: Hyperparameters to log
    """
    global _global_tracker
    if _global_tracker is None:
        warnings.warn("No global tracker set")
        return

    _global_tracker.log_hyperparameters(hparams)
# ---
def __call__(self, *args: Args.args, **kwargs: Args.kwargs) -> R:
        raise NotImplementedError
# ---
def T(self):
        if self.ndim != 2:
            raise ValueError("x.T requires x to have 2 dimensions.")
        from cubed.array_api.linear_algebra_functions import matrix_transpose

        return matrix_transpose(self)
# ---
def loss_ref(x_raw, w_raw):
        loss_val, lse_val = linear_softmax_cross_entropy_loss_reference(
            x_raw,
            y,
            w_raw,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
        )
        loss_val = loss_val + logsumexp_weight * (lse_val**2)
        return loss_val.mean()
# ---
def EvalBatch(self):
        return self.config.EvalBatch
# ---
def dirty(self):
        return self._dirty
# ---
def _train_step(model, x, dy):
        def loss_fn(lin):
            y = lin(x)
            loss = y * dy.astype(y.dtype)
            return hax.sum(loss).scalar()

        grad_fn = eqx.filter_grad(loss_fn)
        grads = grad_fn(model)
        return apply_gradients(model, grads)
# ---
def on_compute_start(self, event):
        self.sessions = []
# ---
def spec_from_config(config):
    return _spec_from_serialized_config(config.serialize())
# ---
def list_units (self, utype=ANY) :
        """
        List IDs of data and/or compute units
        """

        raise Exception ("%s.list_units() is not implemented" % self.__class__.__name__)
# ---
def square_Sum(n):  
    return int(2*n*(n+1)*(2*n+1)/3)
# ---
def test_find_span_end_call():
    source = "x = foo(1)\n"
    # The Call "foo(1)" starts at offset 4.
    end = _find_span_end(source, 4)
    assert end is not None
    assert source[4:end] == "foo(1)"
# ---
def poll(self) -> bool:
        """Return True if job is finished."""
        if self._job_id is None:
            return True
        return JobStatus.finished(self.cluster.poll(self._job_id).status)
# ---
def __lshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.left_shift(self, other)
# ---
def set_time(value):
    global test_time
    test_time = value
    log.debug("Time now set to : %d" % test_time)
# ---
def test_fix_rule_300(self):
        oRule = iteration_scheme.rule_300()

        oRule.fix(self.oFile)

        lActual = self.oFile.get_lines()

        self.assertEqual(lExpected, lActual)

        oRule.analyze(self.oFile)
        self.assertEqual(oRule.violations, [])
# ---
def lower():
        edi = end[divergence_idx]
        return [
            path + (slice(edi, edi + 1),) + s
            for s in _get_minimal_slice_set(
                [0 for _ in start[divergence_idx + 1 :]],
                end[divergence_idx + 1 :],
                dims[divergence_idx + 1 :],
                start_edges=[1 for _ in start_edges[divergence_idx + 1 :]],
                end_edges=end_edges[divergence_idx + 1 :],
            )
        ]
# ---
def __init__(self, k, shape, gamma_tau=1.0):
    super().__init__(shape)
    self.k = k
    self.gamma_tau = gamma_tau
    self.num_betas = 10
    self.sampler = torch.distributions.gamma.Gamma(
      1 / k * torch.ones(self.num_betas, * self.shape), 1.0)
# ---
def setup(self):
        IfcStore.purge()
        bpy.ops.wm.read_homefile(app_template="")
        if bpy.data.objects:
            bpy.data.batch_remove(bpy.data.objects)
            bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)
        blenderbim.bim.handler.setDefaultProperties(None)
# ---
def test_impl(df):
            return df.A.iloc[0]
# ---
def get_initial_input(self) -> Input:
        return self.get_input(0)
# ---
def _define_methods(self, methods):
        for spec in methods:
            if len(spec) == 2: name, action = spec
            else: name, action = spec, None
            meth = FakeMethod(name, action, self.handle)
            setattr(self.__class__, name, meth)
# ---
def test_job_failure_propagates(self, test_cluster):
        """Job that raises exception is marked FAILED."""

        def failing_job():
            raise ValueError("intentional failure")

        job_id = test_cluster.submit(failing_job, name=unique_name("fail-job"))
        status = test_cluster.wait(job_id, timeout=30)
        assert status["state"] == "JOB_STATE_FAILED"
# ---
def from_array(array: jax.Array, num_bins: int = 31) -> "Histogram":
        array = array.ravel()
        min = array.min()
        max = array.max()
        num = array.size
        sum = array.sum()
        sum_squares = (array**2).sum()
        counts, edges = jax.numpy.histogram(array, bins=num_bins)
        return Histogram(min, max, num, sum, sum_squares, edges, counts)
# ---
def ptb_detokenizer(x):
  x = x.replace(" 's", "'s")
  x = x.replace("s ' ", "s' ")
  x = x.replace(" n't", "n't")
  x = x.replace(" \n ", "\n")
  x = x.replace("\\/", "/")
  for _ in range(10):
      x = x.replace(" N ", " 1 ")
  x = x.replace("$ 1", "$1")
  x = x.replace("# 1", "#1")
  x = x.replace("<unk>", "?")
  return x
# ---
def _fsdp_impl(fn: F, parameter_mapping, compute_mapping):
    return named_jit(
        fn, in_axis_resources=parameter_mapping, out_axis_resources=parameter_mapping, axis_resources=compute_mapping
    )
# ---
def to_arrow(self):
        """Create an :class:`pyarrow.RecordBatch` of rows in the page.

        Returns:
            pyarrow.RecordBatch:
                Rows from the message, as an Arrow record batch.
        """
        return self._stream_parser.to_arrow(self._message)
# ---
def fn(data):
            mod = MyModule()
            return mod.array
# ---
def __init__(self, k):
    super().__init__(k, shape=(1, 1))
# ---
def get_inv_count(arr, n): 
	inv_count = 0
	for i in range(n): 
		for j in range(i + 1, n): 
			if (arr[i] > arr[j]): 
				inv_count += 1
	return inv_count
# ---
def cleanup(files):
        '''Clean up on exit '''
        for sfile in files:
            if os.path.exists(sfile):
                if os.path.isdir(sfile):
                    shutil.rmtree(sfile)
                elif os.path.isfile(sfile):
                    os.remove(sfile)
# ---
def reset(git_path, module, dest):
    '''
    Resets the index and working tree to HEAD.
    Discards any changes to tracked files in working
    tree since that commit.
    '''
    cmd = "%s reset --hard HEAD" % (git_path,)
    return module.run_command(cmd, check_rc=True, cwd=dest)
# ---
def __exit__(self, *args):
        rc, out, err = execCmd([_SSH_ADD.cmd, '-d'], env=self._auth)
        if rc != 0:
            logging.error('Error deleting ssh-add, exit code: %r'
                          ', out: %r, err: %r' %
                          (rc, out, err))

        self._kill_agent()
# ---
def __call__(self, *args, **kwargs) -> M_co: ...
# ---
def __repr__(self):
        return "<gitignorematcher>"
# ---
def test_spawn_raw_glance(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_RAW, None, None)
        self.check_vm_params_for_linux()
# ---
def is_on_mac_metal():
    return jax.devices()[0].platform.lower() == "metal"
# ---
def count_range_in_list(li, min, max):
	ctr = 0
	for x in li:
		if min <= x <= max:
			ctr += 1
	return ctr
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        return self.device_flops(dtype) * self.count
# ---
def get(self):
            self.response.write('CSRF Token:%s' % self.csrf_token)
# ---
def fold_fun(acc, z, x):
        return acc + z * x
# ---
def rearrange(self, expression: str, **bindings: AxisSelector | int) -> "NamedArray":
        """See [haliax.rearrange][] for details."""
        pass
# ---
def loss_pallas(x_raw, y_raw, w_raw):
        return fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
            x_raw,
            y_raw,
            w_raw,
            reduction="mean",
            logsumexp_weight=logsumexp_weight,
            block_sizes=block_sizes,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
            implementation="pallas_tpu",
        )
# ---
def testFormatHostname(self):
    """Tests the _FormatHostname function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    hostname_string = test_helper._FormatHostname(
        event, event_data, event_data_stream)
    self.assertEqual(hostname_string, 'ubuntu')
# ---
def test_arithmetic(self):
        expr = col("a") + col("b")
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def with_prefix(prefix: str | None, leaf: str) -> str: ...
# ---
def _materialize_sharded_tensor_from_host_array(
    array: np.ndarray,
    sharding: jax.sharding.Sharding,
) -> jax.Array:
    indices_map = sharding.devices_indices_map(array.shape)
    local_devices = sharding.addressable_devices
    per_device_arrays = []
    for device in local_devices:
        indices = tuple(indices_map[device])
        per_device_arrays.append(jax.device_put(array[indices], device))
    return jax.make_array_from_single_device_arrays(array.shape, sharding, per_device_arrays)
# ---
def test_find_path_identical_empty():
    source = "x = 1\n"
    path = find_path(source, source)
    assert path == []
# ---
def executor_name(self) -> Optional[str]:
        return self._executor_name
# ---
def full_like(a: NamedArray, fill_value: T, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to `fill_value`"""
    return NamedArray(jnp.full_like(a.array, fill_value, dtype=dtype), a.axes)
# ---
def upgrade(engine_name):
    globals()["upgrade_%s" % engine_name]()
# ---
def is_palindrome(n) : 
	divisor = 1
	while (n / divisor >= 10) : 
		divisor *= 10
	while (n != 0) : 
		leading = n // divisor 
		trailing = n % 10
		if (leading != trailing) : 
			return False
		n = (n % divisor) // 10
		divisor = divisor // 100
	return True
def largest_palindrome(A, n) : 
	A.sort() 
	for i in range(n - 1, -1, -1) : 
		if (is_palindrome(A[i])) : 
			return A[i] 
	return -1
# ---
def check_vm_params_for_windows(self):
        self.assertEquals(self.vm['platform']['nx'], 'true')
        self.assertEquals(self.vm['HVM_boot_params'], {'order': 'dc'})
        self.assertEquals(self.vm['HVM_boot_policy'], 'BIOS order')

        # check that these are not set
        self.assertEquals(self.vm['PV_args'], '')
        self.assertEquals(self.vm['PV_bootloader'], '')
        self.assertEquals(self.vm['PV_kernel'], '')
        self.assertEquals(self.vm['PV_ramdisk'], '')
# ---
def ndim(self) -> int:
        """Number of axes in the current view."""
        return len(self.axes)
# ---
def run_train_test_overlap(config: DeconConfig) -> str:
    logger.info(f"Starting train-test overlap dedupe with config: {config}")
    decontaminate(config)
    logger.info(f"Train-test overlap completed! Results written to {config.output_path}")
    return config.output_path
# ---
def test_transaction_engine_ctx_commit(self):
        fn = self._trans_fn()
        ctx = testing.db.begin()
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def test_dtype_category_annotation_and_check():
    def baz(x: Float["b"]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(baz, include_extras=True)["x"])[1]
    assert str(spec.dtype) == "float"

    B = Axis("b", 1)
    arr = NamedArray(jnp.ones((B.size,), dtype=jnp.float32), (B,))
    assert arr.matches_axes(Float["b"])  # type: ignore
    assert not arr.matches_axes(Int["b"])
# ---
def test_from_corpus_builds_nonempty_bank(bank):
    assert bank.total_entries > 0
    assert len(bank.entries) > 0
# ---
def _make(cpu: int = 10, memory_bytes: int = 10 * 1024**3) -> cluster_pb2.ResourceSpecProto:
        return cluster_pb2.ResourceSpecProto(cpu=cpu, memory_bytes=memory_bytes, disk_bytes=10 * 1024**3)
# ---
def max_gen_toks(self):
        return self.leader.max_gen_toks
# ---
def create_superuser(self, email, password, **extra_fields):
        return self._create_user(email, password, True, True,
                                 **extra_fields)
# ---
def kebab_to_pascal(name: str) -> str:
    """Convert kebab-case to PascalCase."""
    return "".join(word.capitalize() for word in name.split("-"))
# ---
def job_context():
    """Ensure a shared job context for all tests."""
    from fray.job.context import create_job_ctx, fray_default_job_ctx

    # Use threadpool context for tests to avoid Ray overhead unless needed
    ctx = create_job_ctx("threadpool")
    with fray_default_job_ctx(ctx):
        yield ctx
# ---
def test_close_and_capture_again(testdir):
    testdir.makepyfile("""
        import os
        def test_close():
            os.close(1)
        def test_capture_again():
            os.write(1, b"hello\\n")
            assert 0
    """)
    result = testdir.runpytest_subprocess()
    result.stdout.fnmatch_lines("""
        *test_capture_again*
        *assert 0*
        *stdout*
        *hello*
    """)
# ---
def test_write_parquet_file_empty():
    """Test writing an empty parquet file."""
    with tempfile.TemporaryDirectory() as tmpdir:
        output_path = str(Path(tmpdir) / "empty.parquet")
        records = []

        result = write_parquet_file(records, output_path)

        assert result["path"] == output_path
        assert result["count"] == 0
        assert Path(output_path).exists()

        table = pq.read_table(output_path)
        assert len(table) == 0
# ---
def __init__(self, max_dist: int = 10.0) -> None:
        """Initialize the filter.

        Parameters
        ----------
        max_dist : float, optional
            The maximum allowed distance.

        """
        self._max_dist = max_dist
# ---
def parse_args():
    parser = argparse.ArgumentParser(description='easy 249')
    parser.add_argument('stock_prices', action='store', nargs='+',
                        help='prices of a given stock')
    return parser.parse_args()
# ---
def testLowFrequency(self):
        hpcp = HPCP(minFrequency=100, maxFrequency=1000)([99], [1])
        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
# ---
def __exit__(self, exc_type, exc_value, traceback):
        self.close()
# ---
def read(self, oldstr):
        self.time, newstr = get_variable_length_number(oldstr)
        return self.time, newstr
# ---
def test_length(self):
        """Test that cmp_version compares by length as last resort"""
        self.assertTrue(vmops.cmp_version('1.2.3', '1.2.3.4') < 0)
# ---
def mkdirs(path):
    """Create a directory and any necessary parent directories."""
    fs, path = fsspec.core.url_to_fs(path)
    fs.makedirs(path, exist_ok=True)
# ---
def _setVolumeForVoiceType(self, voiceType, value):
        """Sets the volume (gain) value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM
        - value: the volume (gain) value to set.
        """

        voiceACSS = self._getACSSForVoiceType(voiceType)
        voiceACSS[acss.ACSS.GAIN] = value
        voiceACSS['established'] = True
# ---
def forward(self, x):
    return self.embedding[x]
# ---
def _np(x):
    return np.array(x.detach().cpu().numpy())
# ---
def __init__(self, delimiter="\t"):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter
# ---
def init(named):
            return Module(named=named)
# ---
def latest_checkpoint_path_with_epoch(self, epoch: int) -> Path:
        return self.checkpoint_dir / f"ckpt_{epoch}.pt"
# ---
def _compute_beta2(self, batch_size: int) -> float:
        """Compute beta2 from batch size."""
        return max(0.95, self.beta2_base ** (batch_size / self.beta2_batch_divisor))
# ---
def BeginTransaction(self, request, context):
        """Begins a new transaction. This step can often be skipped:
    [Read][google.spanner.v1.Spanner.Read], [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] and
    [Commit][google.spanner.v1.Spanner.Commit] can begin a new transaction as a
    side-effect.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def on_line(line: str) -> None:
            logger.info("[%s] %s", self._vm_name, line)
# ---
def array_size(shape: T_Shape) -> int:
    """Number of elements in an array."""
    return reduce(mul, shape, 1)
# ---
def safe_update_wrapper(wrapper, wrapped):
    """
    As [safe_wraps][] but not a decorator.
    Args:
        wrapper:
        wrapped:

    Returns:

    """
    if isinstance(wrapper, equinox.Module):
        return equinox.module_update_wrapper(wrapper, wrapped)
    else:
        return functools.update_wrapper(wrapper, wrapped)
# ---
def path(self):
        """
        Gets the path of this ContributorOrcid.

        :return: The path of this ContributorOrcid.
        :rtype: str
        """
        return self._path
# ---
def ensure_tensorboard_available(executable: str) -> None:
    if os.path.sep in executable or executable.startswith("."):
        if not Path(executable).exists():
            raise FileNotFoundError(f"TensorBoard executable '{executable}' was not found.")
        return

    if shutil.which(executable) is None:
        raise FileNotFoundError(
            f"TensorBoard executable '{executable}' not found on PATH. Use --tensorboard to point to it explicitly."
        )
# ---
def get_method_signature(method: MethodInfo) -> str:
    """Get a human-readable signature for a method."""
    input_name = method.input_type.DESCRIPTOR.name
    output_name = method.output_type.DESCRIPTOR.name
    return f"{input_name} -> {output_name}"
# ---
def test_actor_named_get_if_exists(job_context):
    actor1 = job_context.create_actor(SimpleActor, 100, name="test_actor", get_if_exists=True)
    future1 = actor1.increment.remote(10)
    job_context.get(future1)

    actor2 = job_context.create_actor(SimpleActor, 999, name="test_actor", get_if_exists=True)
    future2 = actor2.increment.remote(0)
    assert job_context.get(future2) == 110
# ---
def test_filter_expression_equality(backend):
    """Test filter with equality expression."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"category": "A", "value": 1},
            {"category": "B", "value": 2},
            {"category": "A", "value": 3},
        ]
    ).filter(col("category") == "A")

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert all(r["category"] == "A" for r in results)
# ---
def prod(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.prod, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype)
# ---
def _normalize_record(raw: Any, dataset: UncheatableEvalDataset, index: int) -> dict[str, str]:
    text = _extract_text(raw)
    if text is None or not str(text).strip():
        raise ValueError(f"Record {index} in {dataset.name} does not contain text")
    record_id = _extract_id(raw, dataset, index)
    return {"id": record_id, "text": text, "source": dataset.source_label}
# ---
def __init__(self, do_open):
        self._do_open = do_open
# ---
def table(self, header, body):
        """Rendering table element. Wrap header and body in it.

        :param header: header part of the table.
        :param body: body part of the table.
        """
        return (
            '<table>\n<thead>%s</thead>\n'
            '<tbody>\n%s</tbody>\n</table>\n'
        ) % (header, body)
# ---
def rel(self, f):
        """Convert repo path back to path that is relative to cwd of matcher."""
        return util.pathto(self._root, self._cwd, f)
# ---
def _write(filename, contents):
        ''' Actually write the file contents to disk. This helps with mocking. '''

        tmp_filename = filename + '.yedit'

        with open(tmp_filename, 'w') as yfd:
            yfd.write(contents)

        os.rename(tmp_filename, filename)
# ---
def mul_consecutive_nums(nums):
    result = [b*a for a, b in zip(nums[:-1], nums[1:])]
    return result
# ---
def non_caching_cycle(iterable):
    """Like itertools.cycle, but doesn't cache the iterable."""
    while True:
        yield from iterable
# ---
def transform(self, x):
            return x + self.w
# ---
def __init__(self, num_classes, cond_size):
    super().__init__()
    self.embedding_table = nn.Embedding(num_classes + 1, cond_size)
    self.num_classes = num_classes
# ---
def register_message_handler(self, target, handler):
        self._message_handlers[target] = handler
# ---
def reverse_vowels(str1):
	vowels = ""
	for char in str1:
		if char in "aeiouAEIOU":
			vowels += char
	result_string = ""
	for char in str1:
		if char in "aeiouAEIOU":
			result_string += vowels[-1]
			vowels = vowels[:-1]
		else:
			result_string += char
	return result_string
# ---
def stop(self) -> None:
        """No-op: ManualVmManager has no background threads to stop."""
        pass
# ---
def test_propagate_option_engine_to_connection(self):
        e1 = testing_engine("sqlite://",
                        options=dict(execution_options={"foo": "bar"}))
        e2 = e1.execution_options(bat="hoho")
        c1 = e1.connect()
        c2 = e2.connect()
        eq_(c1._execution_options, {"foo": "bar"})
        eq_(c2._execution_options, {"foo": "bar", "bat": "hoho"})
# ---
def test_parse_string_template(self):
        tmpl_str = 'just string'
        msg = 'The template is not a JSON object or YAML mapping.'
        self._parse_template(tmpl_str, msg)
# ---
def convert_figure(self, el, text, convert_as_inline):
        if convert_as_inline:
            return text

        # the super doesn't handle this specifically. we basically want to be sure there's a newline
        if not text.endswith("\n\n"):
            if not text.endswith("\n"):
                text += "\n\n"
            else:
                text += "\n"
        return text
# ---
def find_star_num(n): 
	return (6 * n * (n - 1) + 1)
# ---
def _prepend_named_batch_axis(leading_axis: Axis):
    def to_active_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            return leaf.as_scanned_result(leading_axis)
        else:
            return leaf

    return to_active_named_array
# ---
def hard_sigmoid(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_sigmoid, a)
# ---

def starts_one_ends(n):
    """
    Given a positive integer n, return the count of the numbers of n-digit
    positive integers that start or end with 1.
    """
    if n == 1: return 1
    return 18 * (10 ** (n - 2))
# ---
def _patsplit(pattern, default):
    """Split a string into the optional pattern kind prefix and the actual
    pattern."""
    if ":" in pattern:
        kind, pat = pattern.split(":", 1)
        if kind in allpatternkinds:
            return kind, pat
    return default, pattern
# ---
def cross_entropy_loss(logits: jax.Array, labels: jax.Array) -> jax.Array:
    log_probs = jax.nn.log_softmax(logits, axis=-1)
    gathered = jnp.take_along_axis(log_probs, labels[..., None], axis=-1)
    return -jnp.mean(gathered)
# ---
def test_sharded_tree_size_tuple_axis_partition_spec():
    mesh = jax.sharding.AbstractMesh((2, 2), ("data", "model"))
    spec = jax.sharding.PartitionSpec(("data", "model"), None)
    sharding = jax.sharding.NamedSharding(mesh, spec)

    struct = jax.ShapeDtypeStruct((8, 4), jnp.float32, sharding=sharding)

    per_device_bytes = sharded_tree_size(struct, mesh=mesh)

    assert per_device_bytes == (8 * 4 * jnp.dtype(jnp.float32).itemsize) // 4
# ---
def size(self) -> int:
        return self.byte_end - self.byte_start
# ---
def get_platform():
    """Get an instance of the Platform most appropriate for this
    system.

    :deprecated: Use `pyglet.canvas.Display`.

    :rtype: `Platform`
    :return: The platform instance.
    """
    return Platform()
# ---
def ping_endpoint():
    """API endpoint determines potential article hash exists in db

    :return: status code 204 -- hash not present, continue submission
    :return: status code 201 -- hash already exists, drop submission
    """
    db = get_db()
    target_hash = request.form.get('hash')
    if db.raw.find({'hash': target_hash}).count():
        return Response(status=201)
    else:
        return Response(status=204)
# ---
def test_get_autoscaler_status_returns_disabled_when_no_autoscaler(client):
    """GetAutoscalerStatus RPC returns empty status when autoscaler is not configured."""
    resp = rpc_post(client, "GetAutoscalerStatus")
    status = resp.get("status", {})

    # When no autoscaler, should return empty status
    assert status.get("groups", []) == []
# ---
def cd(self, name: str) -> "InputName":
        """Refer to the `name` under `self`'s output_path."""
        return InputName(self, name=name)
# ---
def __enter__(self) -> "ActorPool[T]":
        return self
# ---
def test_argmin_axis_0(spec):
    a = xp.asarray([[11, 12, 13], [11, 11, 14], [10, 13, 11]], chunks=(2, 2), spec=spec)
    b = xp.argmin(a, axis=0)
    assert_array_equal(
        b.compute(),
        np.array([[11, 12, 13], [11, 11, 14], [10, 13, 11]]).argmin(axis=0),
    )
# ---
def snake_to_camel(word):
        import re
        return ''.join(x.capitalize() or '_' for x in word.split('_'))
# ---
def test_impl(df):
            B = df.A.str.split(',')
            df2 = pd.DataFrame({'B': B})
            return df2[df2.B.str.len() > 1]
# ---
def coarsen_data(ds: xr.Dataset) -> xr.Dataset:
    return ds.coarsen(lat=2, lon=2).mean()
# ---
def __init__(self, bundle_path: Path):
        self._bundle_path = bundle_path
# ---
def validate(self, task):
        """Validate driver_info for ipmitool driver.

        Check that node['driver_info'] contains IPMI credentials.

        :param task: a TaskManager instance containing the node to act on.
        :raises: InvalidParameterValue if required ipmi parameters are missing.
        :raises: MissingParameterValue if a required parameter is missing.

        """
        _parse_driver_info(task.node)
# ---
def __getattribute__(self, name):
        try:
            return object.__getattribute__(self, name)
        except AttributeError:
            parent = object.__getattribute__(self, '_parent')
            if parent is None:
                default_values = object.__getattribute__(self, 'default_values')
                if name in default_values:
                    return default_values[name]
                raise
            return getattr(parent, name)
# ---
def parameter_axis_mapping(self) -> ResourceMapping:
        return self.config.parameter_axis_mapping
# ---
def add_cookie_header(self, request):
        self.ach_req = request
# ---
def __init__(self, milliseconds: int):
        self._ms = milliseconds
# ---
def writer(self) -> "InMemoryRolloutWriter":
        """Create a writer for this queue."""
        return InMemoryRolloutWriter(self)
# ---
def update(val):
			self.__lam = 10**sSmooth.val
			self.__p = sAsym.val
			self.__baseline = sbcorr.val*self._baseline_als(np.absolute(self.z_data_raw),self.__lam,self.__p,niter=niter)
			l0.set_ydata(np.absolute(self.z_data_raw))
			l0b.set_ydata(np.absolute(self.__baseline))
			l1.set_ydata(np.absolute(self.z_data_raw/self.__baseline))
			fig.canvas.draw_idle()
# ---
def __rmatmul__(self, other):  # pragma: no cover
        raise ValueError("Matrix multiplication is too ambiguous with NamedArrays. Use dot instead.")
# ---
def output_path_folded(input_path):
                output_dir = (
                    design_dir / const.folding_dirname
                    if self.output_dir is None
                    else self.output_dir
                )
                return [
                    output_dir / f"{input_path.stem}.npz",
                    output_dir / f"{input_path.stem}.npz",
                ]
# ---
def UnplaceSignInBuffer( buffer_number, sign_id ):
  if buffer_number < 0:
    return
  vim.command(
    'try | exec "sign unplace {0} buffer={1}" | catch /E158/ | endtry'.format(
        sign_id, buffer_number ) )
# ---
def __init__(self):
        self.id = 0
        self.name = ""
        self.prom = 0.0
        self.idle = ""
        self.drinks = 0
# ---
def test_remote_main_non_empty_pool(self):
        """Ensure AggregateError is raised if removing the main."""
        aggregate = self._aggregate_setup(aggr_state=aggregate_states.ACTIVE,
                                          hosts=['host', 'host2'],
                                          metadata=self.fake_metadata)
        self.assertRaises(exception.InvalidAggregateAction,
                          self.conn._pool.remove_from_aggregate,
                          self.context, aggregate, "host")
# ---
def __init__(self):
        args = self.arg_parser.parse_known_args()[0]
        super(ScikitBase, self).__init__()
        self.pipeline = self.load_pipeline(args.pipeline)
        if args.feature_names:
            self.feature_names = self.load_pipeline(args.feature_names)
# ---
def _get_logs(self) -> Metrics:
        """
        Returns logs as can be reported to WandB.
        """
        logs: MetricsDict = {}
        for name, aggregator in self._aggregators.items():
            logs.update(aggregator.get_logs(label=name))
        for name, time_dependent_aggregator in self._time_dependent_aggregators.items():
            logs.update(time_dependent_aggregator.get_logs(label=name))
        return logs
# ---
def group_id(self) -> str:
        """Unique identifier for this VM group."""
        ...
# ---
def convert_sup(self, el, text, convert_as_inline):
        if not text:
            return ""
        return f"<sup>{text}</sup>"
# ---
def test_find_path_simple():
    source = "x = 1 + 2\n"
    target = "x = 3 + 4\n"
    path = find_path(source, target)
    assert len(path) >= 1

    # Replaying the path should produce valid Python.
    current = source
    for mutation in path:
        current = mutation.apply(current)
    ast.parse(current)
# ---
def finger_all(self):
        '''
        Return fingerprints for all keys
        '''
        ret = {}
        for status, keys in six.iteritems(self.list_keys()):
            ret[status] = {}
            for key in keys:
                if status == 'local':
                    path = os.path.join(self.opts['pki_dir'], key)
                else:
                    path = os.path.join(self.opts['pki_dir'], status, key)
                ret[status][key] = self._get_key_finger(path)
        return ret
# ---
def new_tuple(test_list, test_str):
  res = tuple(test_list + [test_str])
  return (res)
# ---
def rearrange(self, *args, **kwargs) -> "NamedArray":  # pragma: no cover
        """See [haliax.rearrange][] for details."""
        return haliax.rearrange(self, *args, **kwargs)
# ---
def __getattr__(self, name):
        """Names of resource classes are accepted and resolved as dynamic attribute names.

        This allows convenient retrieval of resources as api.<resource-class>(id=<id>),
        or api.<resource-class>s(q='x').
        """
        return GET(self, name)
# ---
def test_sharded_tree_size_shape_dtype_struct_with_named_sharding():
    mesh = jax.sharding.AbstractMesh((4,), ("data",))
    spec = jax.sharding.PartitionSpec("data", None)
    sharding = jax.sharding.NamedSharding(mesh, spec)

    struct = jax.ShapeDtypeStruct((8, 4), jnp.float32, sharding=sharding)

    per_device_bytes = sharded_tree_size(struct, mesh=mesh)

    assert per_device_bytes == (8 * 4 * jnp.dtype(jnp.float32).itemsize) // 4
# ---
def diagnostics(self, handle: VllmServerHandle, *, max_lines: int = 200) -> dict[str, str]:
        diagnostics: dict[str, str] = {}
        if handle.log_dir:
            diagnostics["vLLM native log dir"] = handle.log_dir
        diagnostics["vLLM native logs (tail)"] = self.logs_tail(handle, max_lines=max_lines)
        return diagnostics
# ---
def test_map_blocks_key_function():
    key_function = make_map_blocks_key_function("a")

    check_key_function(key_function, (0,), "(('a', 0),)")
    check_key_function(key_function, (1,), "(('a', 1),)")
# ---
def _core_plugin(self):
        return bc.get_plugin()
# ---
def find_fixed_point(arr, n): 
	for i in range(n): 
		if arr[i] is i: 
			return i 
	return -1
# ---
def worker_func(args):
    self = args[0]
    m = args[1]
    k = args[2]
    r = args[3]

    return (self.eval_func(m, k, r) -
            self.eval_func(m, k, self.rt) -
            self.temporal_diff_sum(m, k)) ** 2
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._exception = None
# ---
def write_batch(self, batch: RolloutBatch) -> None:
        """Write batch to memory queue, blocking if full."""
        self._queue.push(batch)
# ---
def _weight_transfer_hook(info: levanter.callbacks.StepInfo):
            self.weight_transfer_hook(trainer, info)
# ---
def relu6(a: A) -> A:
    return wrap_elemwise_unary(jnn.relu6, a)
# ---
def check_monthnumb_number(monthnum2):
  if(monthnum2==1 or monthnum2==3 or monthnum2==5 or monthnum2==7 or monthnum2==8 or monthnum2==10 or monthnum2==12):
    return True
  else:
    return False
# ---
def named_call(*, name: str | None = None) -> Callable[[F], F]: ...
# ---
def loraize(model: M, config: LoraConfig, key: jax.random.PRNGKey) -> M:
    """
    Applies LoRA transform to the given model by replacing Linear layers that match the given pattern with LoraLinear layers.
    """
    return _loraize(model, config, key, "", batch_dims=())
# ---
def __init__(self, slot):
        """ Creates a new datetime descriptor.

        :param str slot:
            The attribute name where the actual value is stored.
        """
        self.slot = slot
# ---
def setRow(self, key, **values):
		self.__setitem__(key, DBRow(self, columns=values))
# ---
def _read_all(directory: Path, pattern: str = "*.jsonl.gz") -> list[dict]:
        records = []
        for file_path in sorted(directory.glob(pattern)):
            with gzip.open(file_path, "rt", encoding="utf-8") as handle:
                for line in handle:
                    if line.strip():
                        records.append(json.loads(line))
        return records
# ---
def __init__(self, geom, srs, clip=False):
        self.geom = geom
        self.bbox = geom.bounds
        self.srs = srs
        self.clip = clip
        self._prep_lock = threading.Lock()
        self._prepared_geom = None
        self._prepared_counter = 0
        self._prepared_max = 10000
# ---
def to_hf_config(self, vocab_size: int, config_overrides: Optional[dict] = None) -> HfConfig:
        pass
# ---
def add(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "add")
    if x1.dtype not in _numeric_dtypes or x2.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in add")
    return elemwise(nxp.add, x1, x2, dtype=result_type(x1, x2))
# ---
def tokenizer():
    """Create a real tokenizer for testing."""
    return AutoTokenizer.from_pretrained("gpt2")
# ---
def passthrough(module, grad_input, grad_output):
    """No change made to gradients"""
    return None
# ---
def fsspec_isdir(dir_path):
    """
    Check if a path is a directory in fsspec filesystem.
    """
    fs, _ = fsspec.core.url_to_fs(dir_path)
    return fs.isdir(dir_path)
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        return vm_pb2.SliceInfo(
            slice_id=self._group_id,
            scale_group=self._scale_group,
            created_at=self._created_at.to_proto(),
            vms=[vm.info for vm in self._managed_vms],
        )
# ---
def get_client(
    client,
    profile_name,
    aws_access_key_id,
    aws_secret_access_key,
    region=None,
):
    """Shortcut for getting an initialized instance of the boto3 client."""

    boto3.setup_default_session(
        profile_name=profile_name,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=region,
    )
    return boto3.client(client)
# ---
def get_slopes_power_of_2(n: int):
        start = 2 ** (-(2 ** -(math.log2(n) - log_bias_max)))
        ratio = start
        return [start * ratio**i for i in range(n)]
# ---
def load_canonicals(moldir: str) -> dict[str, Mol]:
    """Load the given input data.

    Parameters
    ----------
    moldir : str
        The molecules to load.

    Returns
    -------
    dict[str, Mol]
        The loaded molecules.

    """
    return load_molecules(moldir, const.canonical_tokens)
# ---
def ready_count(self) -> int:
        """All local actors are ready immediately after creation."""
        return len(self._handles)
# ---
def get_output_shape_for(self, input_shape):
        if self.dim_ordering == 'channels_last':
            return (input_shape[0], input_shape[4])
        else:
            return (input_shape[0], input_shape[1])
# ---
def get_evals(self):
        return self.evals
# ---
def entropy_from_logits(logits: hax.NamedArray, axis: hax.AxisSelector) -> hax.NamedArray:
    """
    Computes entropy over the given axis in a numerically stable way using raw logits.
    """
    log_z = hnn.logsumexp(logits, axis=axis)
    probs = hax.exp(logits - log_z)
    entropy = log_z - hax.sum(probs * logits, axis=axis)
    return entropy
# ---
def calculate_sortino(self, mar=None):
        """
        http://en.wikipedia.org/wiki/Sortino_ratio
        """
        if mar is None:
            mar = self.treasury_period_return

        return sortino_ratio(self.algorithm_returns,
                             self.algorithm_period_returns,
                             mar)
# ---
def default(self, o):
        # We can probably get rid of this if we require python 3.11
        # and change ActivationFunctionEnum to a StrEnum
        if isinstance(o, ActivationFunctionEnum):
            return o.name
        return super().default(o)
# ---
def write_watch_later_config(self):
        self.command('write_watch_later_config')
# ---
def training_sets(self) -> Mapping[str, ProcessedAudioCache]:
        doc_caches = self.build_caches("train")
        return doc_caches
# ---
def __init__(self, deadline_monotonic: float):
        self._deadline = deadline_monotonic
# ---
def __add__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "add")
# ---
def __init__(self, client: "IrisClient", task_name: JobName):
        self._client = client
        self._task_name = task_name
# ---
def init_train_tqdm(self) -> None:
        """Initialize the training progress bar."""
        bar = super().init_train_tqdm()
        return self._update_bar_description(bar)
# ---
def _on_worker_heartbeat_failed(self, txn: TransactionLog, event: WorkerHeartbeatFailedEvent) -> None:
        worker = self._workers.get(event.worker_id)
        if not worker:
            return
        worker.consecutive_failures += 1
        txn.log("heartbeat_failed", event.worker_id, consecutive=worker.consecutive_failures)
        if worker.consecutive_failures >= HEARTBEAT_FAILURE_THRESHOLD:
            self._on_worker_failed(txn, WorkerFailedEvent(worker_id=event.worker_id, error=event.error))
# ---
def _fix_sqrt(string: str) -> str:
    if "\\sqrt" not in string:
        return string
    splits = string.split("\\sqrt")
    new_string = splits[0]
    for split in splits[1:]:
        if split[0] != "{":
            a = split[0]
            new_substr = "\\sqrt{" + a + "}" + split[1:]
        else:
            new_substr = "\\sqrt" + split
        new_string += new_substr
    return new_string
# ---
def strip_axis(self, axis: AxisSelector):
        if isinstance(axis, Axis):
            index = self.main_axes.index(axis)
        else:
            index = index_where(lambda a: a.name == axis, self.main_axes)
        return NamedArray(self.array, self.main_axes[:index] + self.main_axes[index + 1 :])
# ---
def schedule_router_postcommit(self, context, router_context):
        # When the hosting device hosts a Neutron router with external
        # connectivity, a "global" router (modeled as a Neutron router) must
        # also run on the hosting device (outside of any VRF) to enable the
        # connectivity.
        current = router_context.current
        if current['gw_port_id'] and current[HOSTING_DEVICE_ATTR] is not None:
            self._conditionally_add_global_router(context.elevated(), current)
# ---
def lcopy(xs):
  return xs[:]
# ---
def __call__(self, x, *, key):
            return x + self.array + self.static + hax.random.normal(key, x.axes), x * 2
# ---
def clean(c):
    """Remove generated files"""
    if os.path.isdir(CONFIG['deploy_path']):
        shutil.rmtree(CONFIG['deploy_path'])
        os.makedirs(CONFIG['deploy_path'])
# ---
def test_default_spec(executor):
    # default spec works for small computations
    a = xp.ones((3, 3), chunks=(2, 2))
    b = xp.negative(a)
    assert_array_equal(
        b.compute(executor=executor),
        -np.ones((3, 3)),
    )
# ---
def __init__(self, dict_):
        super(self.__class__, self).__init__(dict_)
# ---
def get_key(opts):
    if opts['transport'] in ('zeromq', 'tcp'):
        return Key(opts)
    else:
        return RaetKey(opts)
# ---
def test_einsum_ordered_ellipsis():
    Height = Axis("Height", 2)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)

    m1 = hax.ones((Height, Width, Depth))
    m2 = hax.ones((Depth, Width, Height))

    assert jnp.all(
        jnp.equal(
            einsum("h ... d,d ... h-> ...", m1, m2).array,
            jnp.einsum("ijk,kji->j", m1.array, m2.array),
        )
    )
# ---
def zip_logs():
    if platform.system() == "Windows":
        cmd = "Compress-Archive logs logs-$(date +%Y-%m-%d-%H%M).zip"
        subprocess.call(["powershell.exe", cmd])

    elif platform.system() == "Linux" or platform.system() == "Darwin":
        cmd = "zip -vr logs-$(date +%Y-%m-%d-%H%M).zip" + " logs/"
        cr.run_command(cmd)
# ---
def test_list_type_access_public(self):
        """Querying os-volume-type-access on public type should return 404."""
        req = fakes.HTTPRequest.blank('/v2/%s/types/os-volume-type-access' %
                                      fake.PROJECT_ID,
                                      use_admin_context=True)
        self.assertRaises(webob.exc.HTTPNotFound,
                          self.type_access_controller.index,
                          req, fake.VOLUME_TYPE2_ID)
# ---
def find_Nth_Digit(p,q,N) :  
    while (N > 0) : 
        N -= 1;  
        p *= 10;  
        res = p // q;  
        p %= q;  
    return res;
# ---
def _save(self):
		for name in self._values:
			index = self.structure.index(name)
			col = self.structure[index]
			self[index] = col.from_python(self._values[name])
# ---
def chunklen(shapelen):
            return (1,) * shapelen if shapelen > 0 else (0,)
# ---
def positive(a: A) -> A:
    return wrap_elemwise_unary(jnp.positive, a)
# ---
def load_requests(requests_file: str) -> list[dict[str, Any]]:
    """Load requests from JSON file."""
    logger.info(f"Loading requests from {requests_file}")
    with open(requests_file, "r") as f:
        requests = json.load(f)
    logger.info(f"Loaded {len(requests)} chat completion requests")
    return requests
# ---
def to_df(page: ee.FeatureCollection) -> pd.DataFrame:
    return ee.data.computeFeatures(
        {
            'expression': page,
            'fileFormat': 'PANDAS_DATAFRAME',
        }
    )
# ---
def tobytes(obj):
        if isinstance(obj, str):
            obj = obj.encode('UTF-8')
        assert isinstance(obj, bytes)
        return obj
# ---
def _load_file_gen(stream: Iterator) -> Iterator:
    from zephyr.readers import load_file

    for spec in stream:
        try:
            yield from load_file(spec)
        except Exception as e:
            logger.exception(f"Failed to load from {spec}")
            raise RuntimeError(f"Failed to load from {spec}: {e}") from e
# ---
def message_subscribe(self, *args, **kwargs):
        """Send the subscribe action on stock.picking model as it uses _name in request"""
        return self.pool.get('stock.picking').message_subscribe(*args, **kwargs)
# ---
def prime_num(num):
  if num >=1:
   for i in range(2, num//2):
     if (num % i) == 0:
                return False
     else:
                return True
  else:
          return False
# ---
def decode(string, backend=None, keys=False):
    """Convert a JSON string into a Python object.

    The keyword argument 'keys' defaults to False.
    If set to True then jsonpickle will decode non-string dictionary keys
    into python objects via the jsonpickle protocol.

    >>> str(decode('"my string"'))
    'my string'
    >>> decode('36')
    36
    """
    if backend is None:
        backend = json
    return unpickler.decode(string, backend=backend, keys=keys)
# ---
def reg_nick_write(self, nick, id):
        if(nick.lower() in self.data.keys()):
            self.data[nick.lower()]['id'] = int(id)
        else:
            self.data[nick.lower()] = {'id': int(id), 'nick': nick, 'level': 1, 'strength': 0, 'rank_points': 0, 'citizenship': ''}

        self.update_data(nick.lower())
# ---
def heartbeat(
        self,
        request: cluster_pb2.HeartbeatRequest,
    ) -> cluster_pb2.HeartbeatResponse: ...
# ---
def collapse_alt(self):
        if self.is_alt:
            self.alt.parts = self.alt.parts + ((),)
        else:
            self.is_alt = True
            first_alt_elems = self.group.seq
            self.group.seq = (ast.Alternative(),)
            self.alt.parts = (first_alt_elems,())
# ---
def run_all_tests(marin_tokenizer: PreTrainedTokenizer):
    """Run all tests on the modified tokenizer."""
    special_tokens_injection_check(marin_tokenizer)
    chat_template_checks(marin_tokenizer)
# ---
def _set_match_panel(self, match, team_idx, panel_idx):
        match.team_numbers[team_idx] = self.team_panels[panel_idx].number
        match.team_names[team_idx] = self.team_panels[panel_idx].name
# ---
def kv_pages(self) -> ht.i32[NamedArray, "seq page"]:  # type: ignore[name-defined]
        """KV page assignments per sequence."""
        return self.sequences.kv_pages
# ---
def monitor(self, job_id: JobId) -> JobInfo:
        """Stream logs from a running job, blocking until completion.

        Logs are emitted directly via the logger. Blocks until the job
        completes or is terminated.

        Args:
            job_id: Job identifier returned by launch()

        Returns:
            JobInfo with final job status

        Raises:
            KeyError: If job_id is not found
        """
        ...
# ---
def train_step(state: TinyMLP, xb: NamedArray, yb: NamedArray):
        loss, grads = _loss_and_grad(state, xb, yb)
        new_state = _apply_sgd(state, grads, base_lr=base_lr, use_mup=use_mup)
        return new_state, loss
# ---
def _get_discovery_preamble(self) -> str:
        """Generate static controller discovery script for worker bootstrap."""
        addr = self._bootstrap_config.controller_address
        return f"""
# Use static controller address
CONTROLLER_ADDRESS="{addr}"
if [ -z "$CONTROLLER_ADDRESS" ]; then
    echo "[iris-init] ERROR: No controller address configured"
    exit 1
fi
echo "[iris-init] Using static controller at $CONTROLLER_ADDRESS"
"""
# ---
def _is_valid_nonce(nonce, secret):
	comp = nonce.split(':')
	if len(comp) != 3:
		return False
	calc_nonce = _generate_nonce(comp[0], secret, comp[1])
	if nonce == calc_nonce:
		return True
	return False
# ---
def __call__(
        self,
        target: torch.Tensor,
        gen: torch.Tensor,
    ) -> torch.Tensor: ...
# ---
def split_Arr(a,n,k):  
   b = a[:k] 
   return (a[k::]+b[::])
# ---
def T(self):
        return self.transpose()
# ---
def __init__(self, surfaces):
        self.surfaces = surfaces
# ---
def append_for(module, suffix):
        # Modules have to be appended to the existing mysql.pp
        # otherwise pp will fail for some of them saying that
        # Mysql::Config definition is missing.
        template = "mysql_%s_%s.pp" % (module, suffix)
        manifestdata.append(getManifestTemplate(template))
# ---
def _detect_resource_type() -> Literal["tpu", "gpu", "unknown"]:
    if _detect_tpu_environment():
        return "tpu"
    if _detect_nvidia_gpu_environment():
        return "gpu"
    return "unknown"
# ---
def to_fn(self) -> ActivationFunction:
        if self is ActivationFunctionEnum.xielu:
            raise ValueError("xielu is parameterized; use XIELUActivation directly.")
        return TO_FN[self]
# ---
def dump(self, path: Path) -> None:
        """Dump the object to a JSON file.

        Parameters
        ----------
        path : Path
            The path to the file.

        """
        with path.open("w") as f:
            json.dump(self.to_dict(), f)
# ---
def AddResponses(self, responses):
    artifact_id = responses.request_data["artifact_id"]
    # TODO(user): Check whether artifact collection succeeded.
    self.state.host_data[artifact_id] = list(responses)
# ---
def __next__(self):
        if self._exhausted:
            raise StopIteration
        try:
            return self._run_async_task(self.async_iter.__anext__())
        except StopAsyncIteration:
            self._exhausted = True  # Mark the iterator as exhausted
            if self.loop.is_running():
                self.loop.call_soon_threadsafe(self.loop.stop)
            self.thread.join()
            raise StopIteration
# ---
def get_node(self):
        return self.node
# ---
def find_Sum(arr,n): 
    arr.sort() 
    sum = arr[0] 
    for i in range(0,n-1): 
        if (arr[i] != arr[i+1]): 
            sum = sum + arr[i+1]   
    return sum
# ---
def __init__(self, min_residues: int = 1, max_residues: int = 500) -> None:
        """Initialize the filter.

        Parameters
        ----------
        min_chains : int
            The minimum number of chains allowed.
        max_chains : int
            The maximum number of chains allowed.

        """
        self.min_residues = min_residues
        self.max_residues = max_residues
# ---
def get_target_input(module, input, output):
    """A forward hook which saves the tensor - attached to its graph.
    Used if we want to explain the interim outputs of a model
    """
    try:
        del module.target_input
    except AttributeError:
        pass
    setattr(module, 'target_input', input)
# ---
def rec_fn(x, *rest):
        if isinstance(x, haliax.nn.Stacked):
            new_inner = haliax.vmap(mapped_fn, x.Block)(x.stacked, *[r.stacked for r in rest])
            return dataclasses.replace(x, stacked=new_inner)  # type: ignore
        else:
            return fn(x, *rest)
# ---
def __init__(self, buffer_size=100):
        self._queue = asyncio.Queue(buffer_size)
# ---
def is_scalarish(x):
    if isinstance(x, haliax.NamedArray):
        return x.ndim == 0
    else:
        return jnp.isscalar(x) or (getattr(x, "shape", None) == ())
# ---
def __init__(self, logits):
        self._logits = logits
# ---
def test_dense_dtype(self):
    inputs = ops.convert_to_tensor(
        np.random.randint(low=0, high=7, size=(2, 2)))
    layer = keras.layers.Dense(5, dtype='float32')
    outputs = layer(inputs)
    self.assertEqual(outputs.dtype, 'float32')
# ---
def _get_sublocations(self, cr, uid, ids, context=None):
        """ return all sublocations of the given stock locations (included) """
        return self.search(cr, uid, [('id', 'child_of', ids)], context=context)
# ---
def libretroToPhoenix(self, libretroSystem):
        return self.libretroToPhoenixMap[libretroSystem]
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: NamedArray | AttentionMask | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        x = self.embeddings.embed(input_ids)
        normalizer = jnp.sqrt(self.config.hidden_dim).astype(x.dtype)
        x = x * normalizer
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def __eq__(self, other):
        if isinstance(other, Spec):
            return (
                self.work_dir == other.work_dir
                and self.allowed_mem == other.allowed_mem
                and self.reserved_mem == other.reserved_mem
                and self.executor == other.executor
                and self.storage_options == other.storage_options
                and self.zarr_compressor == other.zarr_compressor
            )
        else:
            return False
# ---
def __post_init__(self):
        super().__post_init__()
        assert (
            self.num_experts_per_tok <= self.n_routed_experts
        ), f"num_experts_per_tok={self.num_experts_per_tok} greater than by n_routed_experts={self.n_routed_experts}."
# ---
def get_total_rmse(data):
    # Calculate total RMSE for each seed
    for entry in data:
        try:
            entry["total_rmse"] = (
                entry["thetao"] + entry["so"] + entry["zos"] + entry["uo"] + entry["vo"]
            )
        except:
            entry["total_rmse"] = entry["thetao"] + entry["so"] + entry["zos"]

    # Extract total RMSE values and labels
    total_rmse_vals = [d["total_rmse"] for d in data]
    seeds = [d["name"] for d in data]

    return total_rmse_vals, seeds
# ---
def deadline_for_course(cls, course_key):
        """
        Retrieve the verification deadline for a particular course.

        Arguments:
            course_key (CourseKey): The identifier for the course.

        Returns:
            datetime or None

        """
        try:
            deadline = cls.objects.get(course_key=course_key)
            return deadline.deadline
        except cls.DoesNotExist:
            return None
# ---
def _handle_heartbeat_failure(self, worker: ControllerWorker) -> None:
        """Handle a failed heartbeat RPC via the state layer."""
        self._state.handle_event(
            WorkerHeartbeatFailedEvent(
                worker_id=worker.worker_id,
                error=f"Heartbeat failed for worker {worker.worker_id}",
            )
        )
# ---
def binrecv(self, timeout=None):
		if timeout is None:
			timeout = self.RECEIVE_TIMEOUT

		try:
			bindata = self.q.get(True, timeout)
		except Queue.Empty:
			raise RadioTimeout
		else:
			return bindata
# ---
def handle500(error):
    return '<H1>Oops, its broken:&nbsp;{}<BR>'.format(error)
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "Gpt2LMHeadModel":
        new_embeddings = self.embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, embeddings=new_embeddings)
# ---
def substract_elements(test_tup1, test_tup2):
  res = tuple(map(lambda i, j: i - j, test_tup1, test_tup2))
  return (res)
# ---
def put(self, b):
		self.write(chr(b))
		return
# ---
def std(
        self, axis: AxisSelection | None = None, *, dtype=None, ddof=0, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.std(self, axis=axis, dtype=dtype, ddof=ddof, where=where)
# ---
import re
def remove_splchar(text): 
 pattern = re.compile('[\W_]+')
 return (pattern.sub('', text))
# ---
def resolve(self, name: str) -> ResolveResult: ...
# ---
def apply_rotary_pos_emb(qkv, cos, sin):
  cos = cos[0,:,0,0,:cos.shape[-1]//2]
  sin = sin[0,:,0,0,:sin.shape[-1]//2]
  return flash_attn.layers.rotary.apply_rotary_emb_qkv_(qkv, cos, sin)
# ---
def test_index_2d_no_op(spec, ind):
    a = xp.asarray(
        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]],
        chunks=(2, 2),
        spec=spec,
    )
    assert a is a[ind]
# ---
def get_task_status(self, request: cluster__pb2.Controller.GetTaskStatusRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetTaskStatusResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def generate_submission_id(command: str) -> str:
    """Generate a nice submission ID based on the inferred experiment."""
    parsed = parse_user_command_line(command)
    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    parts = ["ray-run", getpass.getuser(), parsed["entrypoint"], timestamp]
    return "-".join(parts)
# ---
def alive(self):
        return self.thread.is_alive() if self.thread else False
# ---
def make_comment(with_author=True):
    author = make_author() if with_author else None
    return Comment(id=fake.random_int(), body=fake.bs(), author=author)
# ---
def _checkpoint_leaf(x):
        if is_jax_array_like(x):
            return checkpoint_name(x, name)
        else:
            return x
# ---
def _find_data_browser_local_conf(max_parents: int = 6) -> Path | None:
    here = Path.cwd().resolve()
    for _ in range(max_parents + 1):
        candidate = here / _LOCAL_DATA_BROWSER_CONFIG_REL
        if candidate.exists():
            return candidate
        parent = here.parent
        if parent == here:
            break
        here = parent
    return None
# ---
def append(self, obj: T) -> int:
        index = len(self)
        self._index_to_obj.append(obj)
        self._obj_to_index[obj] = index
        return index
# ---
def wait(self, poll_interval: float = 0.1, timeout: Duration | None = None) -> None:
        """Block until the sentinel file exists."""
        deadline = Deadline.from_now(timeout) if timeout is not None else None
        while not os.path.exists(self._path):
            if deadline is not None and deadline.expired():
                raise TimeoutError(f"SentinelFile {self._path} not signalled within {timeout}")
            time.sleep(poll_interval)
# ---
def main(config: Config):
        assert config.project == "test"
        assert config.x == 2
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.subquery == other.subquery
# ---
def display_name(self):
        """
        Find the most appropriate display name for a user: look for a "display_name", then
        a "real_name", and finally fall back to the always-present "name".
        """
        for k in self._NAME_KEYS:
            if self._raw.get(k):
                return self._raw[k]
            if "profile" in self._raw and self._raw["profile"].get(k):
                return self._raw["profile"][k]
        return self._raw["name"]
# ---
def model_type(self) -> Type["Olmo3LMHeadModel"]:
        return Olmo3LMHeadModel
# ---
def sum_elements(test_tup):
  res = sum(list(test_tup))
  return (res)
# ---
def test_drawn(color, do_open, content, content_after):
    door_card = card.door(color)
    door_card.drawn(core.Core(DoorActor(do_open), agent.Observer(), content))
    assert content == content_after
# ---
def test_batch(tmp_path):
    # input is unbounded, so if entire input were consumed and not read
    # in batches then it would never return, since it would never
    # run the first (failing) input
    with pytest.raises(RuntimeError):
        asyncio.run(
            run_test(
                function=partial(deterministic_failure, tmp_path, {0: [-1]}),
                input=itertools.count(),
                retries=0,
                batch_size=10,
            )
        )
# ---
def additionally_the_object_name_is_selected(name):
    obj = bpy.context.scene.objects.get(name)
    if not obj:
        assert False, 'The object "{name}" could not be selected'
    bpy.context.view_layer.objects.active = obj
    obj.select_set(True)
# ---
def _group_reduction_func_wrapper(func):
        def wrapper(a, by, **kwargs):
            return func(a, nxp.squeeze(by), **kwargs)

        return wrapper
# ---
def format_time(seconds):
    return '{}:{:02}'.format(seconds // 60,
                             seconds % 60)
# ---
def genetic_modification_7_valid_reagent(lab, award, crispr_gm):
    return {
        'purpose': 'characterization',
        'category': 'deletion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'description': 'blah blah description blah',
        "method": "CRISPR",
        "modified_site_by_target_id": "/targets/FLAG-ZBTB43-human/",
        "reagents": [
            {
                "identifier": "ABC123",
                "source": "/sources/sigma/"
            }
        ]
    }
# ---
def _restart_db_server(self, db_version):
        sudo('svcadm restart postgresql')
# ---
def start(self):
        self.thread.start()
# ---
def num_seqs(self) -> int:
        return sum(req.n_generations for req in self)
# ---
def test_wait_all_timeout():
    stop = threading.Event()

    def hang():
        stop.wait(10)

    c = LocalClient(max_threads=4)
    handle = c.submit(JobRequest(name="hang", entrypoint=Entrypoint.from_callable(hang)))
    with pytest.raises(TimeoutError):
        wait_all([handle], timeout=0.2)
    handle.terminate()
    stop.set()
    c.shutdown(wait=True)
# ---
def test_get_chunks_for_groups(
    num_chunks, expected_newchunks, expected_groups_per_chunk
):
    # group 3 has no data
    labels = nxp.asarray([0, 0, 0, 1, 1, 2, 2, 4, 4, 4])
    newchunks, groups_per_chunk = _get_chunks_for_groups(
        num_chunks, labels, num_groups=5
    )
    assert_array_equal(newchunks, expected_newchunks)
    assert groups_per_chunk == expected_groups_per_chunk
# ---
def test_train_dataset_no_input_change(
    tiny_dataset_input, normalize_before_mask, masked_fill_value
):
    train_loader, _ = tiny_dataset_input
    td = train_loader[0]
    pred = torch.randn_like(td.get_label(0)) * 0.1

    inp1 = td.get_input(1).clone()
    td.merge_prognostic_and_boundary(pred, 1)

    # Get a fresh copy from the loader
    td_new = train_loader[0]
    assert torch.equal(td_new.get_input(1), inp1)
# ---
def testExprNameGlobal(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        foo = 42
        foo""")))
# ---
def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('BB', regex=False)
            return B.sum()
# ---
def compute(input):
            return hax.nn.softmax(model(input, key=None, attn_mask=attn_mask), axis=model.Vocab)
# ---
def logit_function(tokens: TokenArray) -> LogitArray:
        Batch, Position = tokens.axes
        token_array = np.array(tokens.array, dtype=np.int64)
        input_ids = torch.from_numpy(token_array).to(config.device)

        with torch.inference_mode():
            outputs = model(input_ids)
            logits = outputs.logits.float().cpu().numpy()
            return hax.named(logits, (Batch, Position, "vocab"))
# ---
def test_regular_chunks(spec):
    xp.ones((5, 5), chunks=((2, 2, 1), (5,)), spec=spec)
    with pytest.raises(ValueError, match="Array must have regular chunks"):
        xp.ones((5, 5), chunks=((2, 1, 2), (5,)), spec=spec)
# ---
def action_cancel_draft(self, cr, uid, ids, context=None):
        """ Cancels the stock move and change inventory state to draft.
        @return: True
        """
        for inv in self.browse(cr, uid, ids, context=context):
            self.pool.get('stock.move').action_cancel(cr, uid, [x.id for x in inv.move_ids], context=context)
            self.write(cr, uid, [inv.id], {'state':'draft'}, context=context)
        return True
# ---
import math
def volume_cone(r,h):
  volume = (1.0/3) * math.pi * r * r * h
  return volume
# ---
def test_group_by_empty(backend):
    """Test groupby on empty dataset."""
    ds = Dataset.from_list([]).group_by(
        key=lambda x: x["cat"], reducer=lambda key, items: {"cat": key, "count": sum(1 for _ in items)}
    )

    results = list(Backend.execute(ds, context=backend))
    assert results == []
# ---
def parent_job_id(self) -> JobName | None:
        """Parent job ID, or None if this is a root job.

        For job_id "/root/parent/child", returns "/root/parent".
        For job_id "/root", returns None.
        """
        if self.job_id is None:
            return None
        return self.job_id.parent
# ---
def __init__(
        self,
        inference_config: LevanterInferenceContextConfig,
    ):
        self.inference_server_config = inference_config.inference_server_config
        self.tokenizer = inference_config.tokenizer
        self._stop_tokens = inference_config.stop_tokens
        self.max_tokens = inference_config.max_tokens
        self.mesh = inference_config.mesh
        self.axis_mapping = inference_config.axis_mapping
# ---
def set_to_tuple(s):
  t = tuple(sorted(s))
  return (t)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            Ac = df.A.rolling(3, center=True).apply(lambda a: a[0] + 2 * a[1] + a[2])
            return Ac.sum()
# ---
def test_one_network_label(self):
        CONF.network_label_regex = 'public'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(['15.123.123.123'], ip)
# ---
def __init__(self, string, cind=None):
        CmdText.__init__(self)
        self.insert(string)

        if (cind is not None):
            self.command = cind
# ---
def check_format(self, sample_str: str) -> bool:
        """Check if the response follows the boxed format."""
        try:
            _ = extract_boxed(sample_str)
            return True
        except ValueError:
            return False
# ---
def scale_group(self) -> str:
        """Name of the scale group this VM group belongs to."""
        ...
# ---
def fake_create_vbd(cls, session, vm_ref, vdi_ref, userdevice,
                            vbd_type='disk', read_only=False, bootable=True):
            pass
# ---
def ListSessions(self, request, context):
        """Lists all sessions in a given database.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def remove_even(l):
    for i in l:
        if i % 2 == 0:
            l.remove(i)
    return l
# ---
def _select_0th(axis):
    def select_0th(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.take(axis, 0)
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            # other leaves don't matter
            return leaf

    return select_0th
# ---
def delete_router_precommit(self, context, router_context):
        pass
# ---
def test_contains_autoescape_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b%cd", autoescape=True, escape="#"), {3})
        self._test(col.contains("b#cd", autoescape=True, escape="#"), {7})
# ---
def urls_for_split(self, split):
        if split == "train":
            urls = self.train_urls
        elif split == "validation":
            urls = self.validation_urls
        else:
            raise ValueError(f"Unknown split {split}")

        # it's ok for there to be no urls for a split, but if there are, they need to be findable
        if len(urls) == 0:
            return []
        return urls
# ---
def test_wordpress_resolved(self):
        self.compare_stacks('WordPress_Single_Instance.template',
                            'WordPress_Single_Instance.yaml',
                            {'KeyName': 'test'})
# ---
def test_machine(self):
        image_meta = {'id': 'a', 'disk_format': 'ami'}
        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK)
# ---
def i_set_prop_to_value(prop, value):
    try:
        eval(f"bpy.context.{prop}")
    except:
        assert False, "Property does not exist"
    try:
        exec(f'bpy.context.{prop} = "{value}"')
    except:
        exec(f"bpy.context.{prop} = {value}")
# ---
def get_actor_pool_name(self) -> str:
        return f"{self._tpu_type} slices"
# ---
def remove_router_interface_postcommit(self, context, r_port_context):
        pass
# ---
def __init__(self, module, message):
        """
        SearchException constructor.

        Args:
            module (str): name of module/class that's raising exception
            message (str): exception message to be displayed

        Usage:
            raise SearchException("Test", "this is an error")

        """
        message = "{0} - {1}".format(module, message)
        Exception.__init__(self, message)
# ---
def has_error(self):
        return CleanText('//span[@id="id_error_msg"]')(self.doc)
# ---
def compute(self) -> Tensor:
    """Computes the Perplexity.

    Returns:
     Perplexity
    """
    return torch.exp(self.mean_value / self.weight)
# ---
def the_tokenizer(self):
        """Load and return the tokenizer from the specified path."""
        return load_tokenizer(self.tokenizer)
# ---
def __iter__(self):
        return iter(range(len(self)))
# ---
def cancel(self, order):
        try:
            self.pending.remove(order)
        except ValueError:
            # If the list didn't have the element we didn't cancel anything
            return False

        order.cancel()
        self.notify(order)
        return True
# ---
def convert_to_cache(self, value, record, validate=True):
        return bool(value)
# ---
def findForce(system, forcetype, add=True):
  """ Finds a specific force in the system force list - added if not found."""
  for force in system.getForces():
    if isinstance(force, forcetype):
      return force
  if add==True:
    system.addForce(forcetype())
    return findForce(system, forcetype)
  return None
# ---
def count_occurance(s):
  count=0
  for i in range(len(s)):
    if (s[i]== 's' and s[i+1]=='t' and s[i+2]== 'd'):
      count = count + 1
  return count
# ---
def test_impl(df):
            B = df.A.str.split(',')
            return B
# ---
def update_pbar(step: StepInfo):
        pbar.update(step.next_step - pbar.n)
        pbar.set_postfix(loss=jnp_to_python(step.loss))
# ---
def _add_www_authenticate(request, secret, realm):
	resp = request.response
	if not resp.www_authenticate:
		resp.www_authenticate = _generate_digest_challenge(
			round(time.time()),
			secret, realm, 'NPDIGEST'
		)
# ---
def should_poll(self):
        """Return the polling state."""
        return False
# ---
def gen_keys(self):
        '''
        Use libnacl to generate and safely save a private key
        '''
        import libnacl.public
        d_key = libnacl.dual.DualSecret()
        path = '{0}.key'.format(os.path.join(
            self.opts['gen_keys_dir'],
            self.opts['gen_keys']))
        d_key.save(path, 'msgpack')
# ---
def rate_noise(self, t):
    cos = (1 - self.eps) * torch.cos(t * torch.pi / 2)
    sin = (1 - self.eps) * torch.sin(t * torch.pi / 2)
    scale = torch.pi / 2
    return scale * sin / (cos + self.eps)
# ---
def allocate(self, count: int = 1) -> list[int]:
        with self._lock:
            ports = []
            for _ in range(count):
                port = self._find_free_port()
                self._allocated.add(port)
                ports.append(port)
            return ports
# ---
def test_column_mean(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            return df.A.mean()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
# ---
def _multiply_property(self, name, factor):
        self.command('multiply_property', name, factor)
# ---
def __init__(self):
        self._balance = 0
        self.__id = generate_id()
# ---
def build_train_lm_on_pod_config(self) -> TrainLmOnPodConfig:
        return TrainLmOnPodConfig(
            train_config=self.build_train_lm_config(),
            resources=self.build_pod_config(),
            output_path=this_output_path(),
        )
# ---
def get_depth_loss_dict(
    label: str, loss_per_channel: torch.Tensor
) -> dict[str, torch.Tensor]:
    tensor_map = TensorMap.get_instance()
    metrics = {}
    for depth in tensor_map.DEPTH_SET:
        metrics[f"{label}/loss/depth/depth_{depth}_loss"] = loss_per_channel[
            tensor_map.DP_3D_IDX[depth]
        ].mean()
    return metrics
# ---
def reload(self) -> str:
        """Re-run bootstrap on existing VM without recreating it.

        Unlike restart() which deletes and recreates the VM, reload() SSHs
        into the existing VM and re-runs the bootstrap script to pull the
        latest image and restart the container.

        Raises:
            RuntimeError: If the controller VM doesn't exist or health check fails.
        """
        ...
# ---
def count_Hexadecimal(L,R) :  
    count = 0;  
    for i in range(L,R + 1) : 
        if (i >= 10 and i <= 15) : 
            count += 1;  
        elif (i > 15) : 
            k = i;  
            while (k != 0) :  
                if (k % 16 >= 10) : 
                    count += 1;  
                k = k // 16;  
    return count;
# ---
def test_buffered_column_result_proxy(self):
        self._test_proxy(_result.BufferedColumnResultProxy)
# ---
def make_mock_connection(host, **kwargs):
        mock_conn = MagicMock()
        mock_conn.host = host
        # Simulate healthy for 10.0.0.1 and 10.0.0.3, unhealthy for 10.0.0.2
        if host in ["10.0.0.1", "10.0.0.3"]:
            mock_conn.run.return_value = MagicMock(returncode=0)
        else:
            mock_conn.run.return_value = MagicMock(returncode=1)
        return mock_conn
# ---
def extract_string(str, l):
    result = [e for e in str if len(e) == l] 
    return result
# ---
def output_linebreak(self, m):
        return self.renderer.linebreak()
# ---
def _run() -> None:
        with remove_tpu_lockfile_on_exit():
            launch(model, evals, output_path, max_eval_instances, wandb_tags)
# ---
def test_get_last_historic(self):
        serial = "__TEST__"

        self.db.add_historic(serial, 0, 0)
        assert_equals(self.db.get_last_historic(serial), 0)

        self.db.add_historic(serial, 300, 0)
        assert_equals(self.db.get_last_historic(serial), 300)

        self.db.add_historic(serial, 3600, 0)
        assert_equals(self.db.get_last_historic(serial), 3600)

        self.db.add_historic(serial, 2000, 0)
        assert_equals(self.db.get_last_historic(serial), 3600)
# ---
def _(a: cubed.Array) -> np.ndarray:
    return to_numpy(a.compute())
# ---
def test_virtual_offsets(shape):
    v_offsets = virtual_offsets(shape)
    offsets = np.arange(prod(shape)).reshape(shape, order="C")
    for t in product(*(range(n) for n in shape)):
        assert v_offsets[t] == offsets[t]

    # test some length 1 slices
    if len(shape) == 1:
        assert v_offsets[1:2] == offsets[1:2]
    elif len(shape) == 2:
        assert v_offsets[1:2, 0:1] == offsets[1:2, 0:1]
# ---
def __init__(self, tokenizer: Tokenizer):
        self.tokenizer = tokenizer
# ---
def nested_scan_outer_block(nested_remat, axis_size):
        outer_block_size: int | None
        if nested_remat is True:
            outer_block_size = find_closest_divisible_int_to_sqrt(axis_size)
        elif nested_remat is False:
            outer_block_size = None
        else:
            outer_block_size = nested_remat
        return outer_block_size
# ---
def test_force_conn_events_false(self):
        canary = Mock()
        e1 = create_engine(config.db_url)
        assert not e1._has_events

        event.listen(e1, "before_execute", canary.be1)

        conn = e1._connection_cls(e1, connection=e1.raw_connection(),
                            _has_events=False)

        conn.execute(select([1]))

        eq_(canary.be1.call_count, 0)

        conn._branch().execute(select([1]))
        eq_(canary.be1.call_count, 0)
# ---
def finish(self):
        if self.executor is not None:
            self.executor.shutdown()
        self.wandb_logger.finish()
# ---
def gcloud_config():
    client = storage.Client()
    out: dict[str, str | None] = {
        "project": client.project,
    }
    try:
        out["zone"] = get_default_zone()
    except subprocess.CalledProcessError:
        out["zone"] = None

    return out
# ---
def __init__(self):
            super().__init__()
            self.field = named1
            self.flag = True
# ---
def find(self):
        return self._find()
# ---
def checkpoint_path(self) -> str:
        checkpoint_path = self.config.load_checkpoint_path
        if checkpoint_path is None:
            checkpoint_path = self.config.checkpointer.expanded_path(self.run_id)
        return checkpoint_path
# ---
def updateversiondb(cur) :
    db = cherrypy.session['database']
    username = cherrypy.session['username']

    dt = time.strftime("%Y-%m-%d %H:%M:%S")
    try:
        sql = "UPDATE Radio SET radio='%s', genre='%s' WHERE id = 0" % (hostname, dt)
        cur.execute(sql)
    except:
        return
# ---
def longest_subseq_with_diff_one(arr, n): 
	dp = [1 for i in range(n)] 
	for i in range(n): 
		for j in range(i): 
			if ((arr[i] == arr[j]+1) or (arr[i] == arr[j]-1)): 
				dp[i] = max(dp[i], dp[j]+1) 
	result = 1
	for i in range(n): 
		if (result < dp[i]): 
			result = dp[i] 
	return result
# ---
def heidke_skill_score(self):
        n = float(self.table.sum())
        nf = self.table.sum(axis=1)
        no = self.table.sum(axis=0)
        correct = float(self.table.trace())
        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (nf * no).sum() / n ** 2)
# ---
def max_gen_toks(self) -> int:
        """Backward compatibility property for max_gen_toks."""
        return self._generation_kwargs.get("max_gen_toks", 256)
# ---
def sample(self, x):
    assert x.ndim == 2
    n = x.shape[-1] // 2
    mu = x[:, :n]
    sigma = self.softplus(x[:, n:]).sqrt()
    return mu + sigma * torch.randn_like(mu)
# ---
import re
def is_valid_URL(str):
	regex = ("((http|https)://)(www.)?" +
			"[a-zA-Z0-9@:%._\\+~#?&//=]" +
			"{2,256}\\.[a-z]" +
			"{2,6}\\b([-a-zA-Z0-9@:%" +
			"._\\+~#?&//=]*)")
	p = re.compile(regex)
	if (str == None):
		return False
	if(re.search(p, str)):
		return True
	else:
		return False
# ---
def run_unittests(options):
    ret_build = ret_test = 0
    for job in JERRY_UNITTESTS_OPTIONS:
        ret_build, build_dir_path = create_binary(job, options)
        if ret_build:
            break

        ret_test |= run_check([
            settings.UNITTEST_RUNNER_SCRIPT,
            os.path.join(build_dir_path, 'tests'),
            "-q" if options.quiet else "",
        ])

    return ret_build | ret_test
# ---
def populate_history(wrapped_optimizer, initial_state, params, num_steps, base_loss, base_grads):
    current_state = initial_state
    for i in range(num_steps):
        # Vary loss and grads slightly to have some variance
        loss = base_loss + jnp.sin(i * 0.1)
        grads = jax.tree_util.tree_map(lambda g: g * (1 + 0.1 * jnp.cos(i * 0.1)), base_grads)
        _, current_state = wrapped_optimizer.update(grads, current_state, params, loss=loss)
    return current_state
# ---
def validate_edit(source: str, mutation: Mutation) -> bool:
    """Check whether applying a mutation produces valid Python.

    Args:
        source: Current program source.
        mutation: Proposed edit.

    Returns:
        True if the edited program parses successfully.
    """
    edited = mutation.apply(source)
    try:
        ast.parse(edited)
        return True
    except SyntaxError:
        return False
# ---
def test_prediction_data(prediction_data):
    ds_prediction_validate(prediction_data)
# ---
def done(self, lease: Lease[T_co]) -> None:
        """Mark a leased task as successfully completed."""
        ...
# ---
def get_cookies(self):
        return list(self.cookies.values())
# ---
def __init__(self, dataset: AsyncDataset[T_co]):
        self.dataset = dataset
# ---
def _get_stage_for_block(self, block_id: int) -> int:
        block_start = block_id * self.block_size
        stage_starts = np.array([start for start, _ in self.weight_stages])
        return max(0, np.searchsorted(stage_starts, block_start, side="right") - 1)
# ---
def poke(self, context):
        logging.info(
            'Checking if the time ({0}) has come'.format(self.target_time))
        return datetime.now().time() > self.target_time
# ---
def with_tpu(tpu_type: str, slice_count: int = 1, **kwargs) -> ResourceConfig:
        device = TpuConfig(variant=tpu_type)
        return ResourceConfig(device=device, replicas=slice_count, **kwargs)
# ---
def create_job_base_data(data):
    return {
        'platform_id': data['saagie-platform'],
        'category': 'processing',
        'name': data['job-name'],
        'description': data['description'],
        'current': {
            'cpu': data['cpu'],
            'disk': data['disk'],
            'memory': data['ram'],
            'isInternalSubDomain': False,
            'isInternalPort': False,
            'options': {}
        }
    }
# ---
def init(cls, Vocab: Axis, config: GemmaConfig, *, key):
        k_t, k_emb = jrandom.split(key, 2)
        transformer = Gemma2Transformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return Gemma2LMHeadModel(transformer, embeddings, lm_head)
# ---
def elemwise(func, *args: "Array", dtype=None) -> "Array":
    """Apply a function elementwise to array arguments, respecting broadcasting."""
    shapes = [arg.shape for arg in args]
    out_ndim = len(np.broadcast_shapes(*shapes))
    expr_inds = tuple(range(out_ndim))[::-1]
    if dtype is None:
        raise ValueError("dtype must be specified for elemwise")
    return blockwise(
        func,
        expr_inds,
        *concat((a, tuple(range(a.ndim)[::-1])) for a in args),
        dtype=dtype,
    )
# ---
def unauthenticated_userid(self, request):
		return self.match(request).unauthenticated_userid(request)
# ---
def last_Digit_Factorial(n): 
    if (n == 0): return 1
    elif (n <= 2): return n  
    elif (n == 3): return 6
    elif (n == 4): return 4 
    else: 
      return 0
# ---
def __init__(self, bindings=None):
        if bindings is None:
            self.bindings = {}
        else:
            self.bindings = {**bindings}
# ---
def selected_target(self):
        return self._selected_target
# ---
def _log_profiler_artifact(self):
        """Log profiler artifact to the tracker."""
        levanter.tracker.current_tracker().log_artifact(self.profiler_config.profile_path, type="jax_profile")
# ---
import re
def text_match_string(text):
        patterns = '^\w+'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return 'Not matched!'
# ---
def group_key(ds):
            return tuple(prog.grid_size for prog in ds.prognostic_srcs)
# ---
def do_handshake(self):
        self.__iowait(self._connection.do_handshake)
# ---
def test_metric_float_conversion():
    """Metrics support float() conversion."""
    m = Metric.from_value(42.0, ReductionType.MEAN)
    assert float(m) == 42.0
# ---
def __exit__(self, *args):
        problems = []
        for cmanager in reversed(self._cmanagers):
            try:
                cmanager.__exit__(*args)
            except Exception as e:
                problems.append(e)

        self._cmanagers = []

        if len(problems) > 0:
            raise RuntimeError("Exception(s) occurred while exiting trainer", problems) from problems[0]
# ---
def __array__(self, dtype=None) -> np.ndarray:
        x = self.compute()
        if dtype and x.dtype != dtype:
            x = x.astype(dtype)
        if not isinstance(x, np.ndarray):
            x = np.array(x)
        return x
# ---
def bank(toy_corpus):
    return SubtreeBank.from_corpus(toy_corpus)
# ---
def __str__(self):
        return '[' + ', '.join('({},{})'.format(*el) for el in self._queue) + ']'
# ---
def write(self, s):
            pass
# ---
def html_block(node: RenderTreeNode, context: RenderContext) -> str:
    content = node.content.rstrip("\n")
    # Need to strip leading spaces because we do so for regular Markdown too.
    # Without the stripping the raw HTML and Markdown get unaligned and
    # semantic may change.
    content = content.lstrip()
    return content
# ---
def test_decode_token_base(tok):
    tid = tok.encode_char("Z")
    assert tok.decode_token(tid) == "Z"
# ---
def get_well_known_file_ids(self, neuronal_model_id):
        '''Query the current RMA endpoint with a neuronal_model id
        to get the corresponding well known file ids.

        Returns
        -------
        list
            A list of well known file id strings.
        '''
        rma_builder_fn = self.build_rma
        json_traversal_fn = self.read_json

        return self.do_query(rma_builder_fn, json_traversal_fn, neuronal_model_id)
# ---
def accept_handler(fd, events):
        while True:
            try:
                connection, address = sock.accept()
            except socket.error as e:
                if e.args[0] in (errno.EWOULDBLOCK, errno.EAGAIN):
                    return
                raise
            callback(connection, address)
# ---
def any_executor(request):
    return request.param
# ---
def _overlap_num_input_blocks(depth, numblocks):
    num = 1
    for i in depth.keys():
        num *= min(numblocks[i], 3)
    return num
# ---
def __init__(self, uri, username, password, vminfo, vmid, irs):
        super(LibvirtCommand, self).__init__(vminfo, vmid, irs)
        self._uri = uri
        self._username = username
        self._password = password
# ---
def test_rerank_preserves_all_candidates():
    candidates = [_make_candidate(f"x = {i}\n", score=-float(i)) for i in range(5)]
    ranked = rerank_candidates(candidates, ["assert True"])
    assert len(ranked) == 5
# ---
def absent(self):
        if self.exists():
            return self.remove()
        return False
# ---
def test_mem_write_byte_updates_video_ram(self):
        self.mda.mem_write_byte(0x0000, 0x41)
        self.assertEqual(self.mda.video_ram[0x0000], 0x41)
# ---
def baseline_func_amp(self,z_data,f_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		returns the baseline as a function
		the points in between the datapoints are computed by cubic interpolation
		'''
		return interp1d(f_data, self._baseline_als(np.absolute(z_data),lam,p,niter=niter), kind='cubic')
# ---
def current_attempt_id(self) -> int:
        """ID of current attempt (0-indexed), or -1 if no attempts."""
        return len(self.attempts) - 1 if self.attempts else -1
# ---
def default_dtypes(self, *, device=None):
        return nxp.__array_namespace_info__().default_dtypes(device=device)
# ---
def crumble(self, instance, attrs):
        return {
            'id': instance.id.hex,
            'name': instance.label,
            'dateCreated': instance.date_created,
        }
# ---
def with_output(self, x, static1, *, static2):
            assert static1 == 1.0
            assert static2 is False
            out = x + self.w + static1
            return out, 2 * self.w
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[dict]:
        url = self._shard_name_to_url_mapping[shard_name]
        i = 0
        with fsspec.open(url, "r", compression="infer") as f:
            # TODO: would be nice if we could seek faster than this. Right now, all we do is skip json parsing
            # which is not nothing, but not ideal.
            for line in f:
                if i >= row:
                    yield json.loads(line)
                i += 1
# ---
def test_pickle_roundtrip_preserves_name(self):
        handle = IrisActorHandle("my-actor")
        data = pickle.dumps(handle)
        restored = pickle.loads(data)
        assert restored._endpoint_name == "my-actor"
        assert restored._client is None
# ---
def _delete_vpn_processes(self, sync_router_ids, vpn_router_ids):
        for process_id in sync_router_ids:
            if process_id not in vpn_router_ids:
                self.destroy_process(process_id)
# ---
def test_update(self):
        db = smadata2.db.sqlite.create_or_update(self.dbname)
# ---
def fast_read_adc0(self):
        """This reads the actual ADC value of channel 0, with as little overhead as possible.
        Use with SPIDEV ONLY!!!!
        returns: The ADC value as an n-bit integer value, with n=10 or 12 depending on the chip."""

        dat = self._dev.xfer(self._control0)
        value = (dat[1] << 8) + dat[2]
        return value
# ---
def format_rpms(rpms):
    return format_line(prefix='rpms'.rjust(RJUST), values=rpms)
# ---
def __init__(self):
        self._containers: dict[str, _LocalContainer] = {}
# ---
def to_ms(self) -> int:
        """Convert to milliseconds."""
        return self._ms
# ---
def require_task(self) -> tuple["JobName", int]:
        """Return (parent_job, task_index) for task names.

        Raises:
            ValueError: If this name is not a task or has no parent.
        """
        task_index = self.task_index
        if task_index is None:
            raise ValueError(f"JobName is not a task: {self}")
        if self.parent is None:
            raise ValueError(f"Task has no parent job: {self}")
        return (self.parent, task_index)
# ---
def handle_partial(state):
            ref_counts, indices = state
            has_free = hax.any(ref_counts == 0).scalar()
            ref_counts = eqx.error_if(ref_counts, ~has_free, "Out of free pages during clone_pages_from")
            free_idx = hax.argmax(ref_counts == 0, "page")
            indices = indices.at["seq", dst_seq_id, "page", last_idx].set(free_idx)
            ref_counts = ref_counts.at["page", free_idx].add(1)
            return ref_counts, indices
# ---
def _to_expr(value: Any) -> Expr:
    """Convert a value to an expression if not already one."""
    if isinstance(value, Expr):
        return value
    return LiteralExpr(value)
# ---
def __repr__(self):
        if self.expr is not None:
            return f"FilterOp(expr={self.expr})"
        return f"FilterOp(predicate={_get_fn_name(self.predicate)})"
# ---
def test_missing_elements_errors():
    partial_order = ("qux", ...)
    candidates = ("banana", "apple", "cherry")
    with pytest.raises(ValueError):
        rearrange_for_partial_order(partial_order, candidates)
# ---
def __init__(self, platform: config_pb2.GcpPlatformConfig, label_prefix: str):
        self._platform = platform
        self._label_prefix = label_prefix
# ---
def test_trivial(self):
        assert isinstance(self.db, smadata2.db.base.BaseDatabase)
# ---
import re
def fill_spaces(text):
  return (re.sub("[ ,.]", ":", text))
# ---
def delete_tpu_node(node_name: str, project: str, zone: str, quiet: bool = False) -> None:
    """Delete a TPU node."""
    cmd = [
        "gcloud",
        "compute",
        "tpus",
        "tpu-vm",
        "delete",
        node_name,
        f"--project={project}",
        f"--zone={zone}",
    ]

    if quiet:
        cmd.append("--quiet")

    run_gcloud_command(cmd)
# ---
def make_input_name(name, step_name) -> InputName:
    return InputName(name=name, step=SimpleNamespace(name=step_name))
# ---
def epoch_seconds(self) -> float:
        """Get seconds since epoch."""
        return self._epoch_ms / 1000.0
# ---
def config(self) -> config_pb2.ScaleGroupConfig:
        """Configuration for this scale group."""
        return self._config
# ---
def sum_digits_single(x) : 
    ans = 0
    while x : 
        ans += x % 10
        x //= 10  
    return ans 
def closest(x) : 
    ans = 0
    while (ans * 10 + 9 <= x) : 
        ans = ans * 10 + 9  
    return ans   
def sum_digits_twoparts(N) : 
    A = closest(N)  
    return sum_digits_single(A) + sum_digits_single(N - A)
# ---
def before_execute(conn, clauseelement, multiparams, params):
            assert isinstance(multiparams, (list, tuple))
            assert isinstance(params, dict)
# ---
def width(self):
        return self.__width
# ---
def test_raw(self):
        image_meta = {'id': 'a', 'disk_format': 'raw'}
        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_RAW)
# ---
def random_ascii_string(length):
    random = SystemRandom()
    return ''.join([random.choice(UNICODE_ASCII_CHARACTERS) for x in range(length)])
# ---
def wait(self, futures: list, num_returns: int = 1) -> tuple[list, list]:
        """Wait for Ray futures to complete."""
        # NOTE: fetch_local=False is paramount to avoid copying the data to the Zephyr
        # driver node, especially for the data futures.
        ready, pending = ray.wait(futures, num_returns=num_returns, fetch_local=False)
        return list(ready), list(pending)
# ---
def test_can_scale_up_when_below_max(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """can_scale_up() returns True when below max_slices."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        assert group.can_scale_up()
# ---
def test_mean(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.mean(a, axis=0)
    run_operation(tmp_path, executor, "mean", b)
# ---
def test_profiler_get_id(self, mock_generate_uuid):
        mock_generate_uuid.return_value = "43"
        prof = profiler._Profiler("secret")
        prof.start("test")
        self.assertEqual(prof.get_id(), "43")
# ---
def path_from_key_path(key_path):
        return "/".join(key_path.split("."))
# ---
def __hash__(self):
        # Hash based on the initial configuration values
        return hash(self.build_name())
# ---
def test_prof__read_ee(self):
    fc = ee.FeatureCollection('WRI/GPPD/power_plants')
    with cProfile.Profile() as pr:
      _ = dask_ee.read_ee(fc)

      # Modified version of `pr.print_stats()`.
      pstats.Stats(pr).sort_stats('cumtime').print_stats()
# ---
def _set_value(self, name, value):
		index = self.structure.index(name)
		col = self.structure[index]
		self._values[name] = col.to_python(value, self)
		self[index] = value
# ---
def _no_arg_stage(
    last: int, *, current: int, fun: NoArgumentStageFunction, config: Config
) -> int:
    """Execute a NoArgumentStageFunction, ensuring execution order."""
    assert (last + 1) == current, (
        f"stages are executing out of order! On step {current!r}."
    )

    fun(config=config)

    beam.metrics.metric.Metrics.counter("cubed", "completed_tasks").inc()

    return current
# ---
def controller_address_metadata_key(label_prefix: str) -> str:
    """Metadata key for the controller address for a given prefix."""
    return f"iris-controller-address-{label_prefix}"
# ---
def clear_queue(self) -> None:
        """Clear all batches from the queue."""
        with self._lock:
            self._queue.clear()
# ---
def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "kapow", "boing": "thunk"}
# ---
def test_read(self):
        config = read_config('config')
        self.assertEqual(config['cmus_host'], 'raspberry')
        self.assertEqual(config['cmus_passwd'], 'PaSsWd')
        self.assertEqual(config['app_host'], 'localhost')
        self.assertEqual(config['app_port'], '8080')
# ---
def traslado_isr(self):
        return self.__trisr
# ---
def count(self) -> Dataset[int]:
        """Count the total number of items in the dataset.

        Returns:
            Dataset containing a single integer count

        Example:
            >>> ds = Dataset.from_list(range(100)).filter(lambda x: x % 2 == 0)
            >>> count = list(Backend.execute(ds.count()))[0]
            50
        """
        return self.reduce(
            local_reducer=lambda items: sum(1 for _ in items),
            global_reducer=sum,
        )
# ---
def model_type(self) -> type[GrugWrapper]:
        return GrugWrapper
# ---
def write_batch(self, batch: BatchResult):
        if isinstance(batch, pa.RecordBatch):
            batch = dict_from_record_batch(batch)

        cbatch = _canonicalize_batch(batch)  # type: ignore[arg-type]
        self._tree_store.extend(cbatch)
# ---
def iter_chunks(self) -> Iterator[list]:
        """Iterate over chunks (each chunk is a list of items)."""
        for chunk in self.chunks:
            data = self.context.get(chunk.data)
            yield data
# ---
def test_one_hot_out_of_bound():
    i, c = hax.make_axes(i=2, c=3)
    actual = hax.nn.one_hot(hax.NamedArray(jnp.array([-1, 3]), (i,)), c)
    expected = jnp.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])
    assert jnp.all(jnp.isclose(actual.array, expected))
# ---
def __getitem__(self, slices: SliceSpec) -> "_NamedIndexUpdateRef":
        return _NamedIndexUpdateRef(self.array, _convert_index_expr_to_dict(slices))
# ---
def test_contains_autoescape(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b%cde", autoescape=True), {3})
# ---
def define_tables(cls, metadata):
        cls.table = Table('exec_test', metadata,
            Column('a', Integer),
            Column('b', Integer),
            test_needs_acid=True
        )
# ---
def _vm_has_snapshot(vm):
    try:
        return vm.hasCurrentSnapshot() == 1
    except libvirt.libvirtError:
        logging.exception('Error checking if snapshot exist for vm: %s.',
                          vm.name())
        return False
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: AttentionMask | NamedArray | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        return self.transformer(self.embeddings.embed(input_ids), attn_mask=attn_mask, key=key, pos_ids=pos_ids)
# ---
def get_description(self, env):
        """ Return a dictionary that describes the field ``self``. """
        desc = {'type': self.type}
        for attr, prop in self.description_attrs:
            value = getattr(self, prop)
            if callable(value):
                value = value(env)
            if value is not None:
                desc[attr] = value

        return desc
# ---
def test_copy_experiment(self):
        """ Tests that copy_experiment creates a new experiment """
        experiment = self.create_test_experiment()
        num_experiments = Experiment.objects.count()
        url = reverse("ab_testing_tool_copy_experiment", args=(experiment.id,))
        response = self.client.post(url, follow=True)
        self.assertOkay(response)
        self.assertEqual(Experiment.objects.count(), num_experiments + 1)
# ---
def selkeys(dict_, keys):
            if dict_ is None:
                return None
            return dict((d, dict_[d]) for d in keys if d in dict_)
# ---
def test_from_files_empty_glob_error(tmp_path):
    """Test from_files raises error when no files match and empty_glob_ok=False."""
    input_dir = tmp_path / "input"
    input_dir.mkdir()

    # Should raise FileNotFoundError
    with pytest.raises(FileNotFoundError, match="No files found"):
        Dataset.from_files(f"{input_dir}/*.txt", empty_glob_ok=False)
# ---
def init(named, array, static):
            return Module(named=named, array=array, static=static)
# ---
def _product_virtual_get(self, cr, uid, id, product_ids=False, context=None, states=None):
        if states is None:
            states = ['done']
        return self._product_all_get(cr, uid, id, product_ids, context, ['confirmed', 'waiting', 'assigned', 'done'])
# ---
def bump_seq_len_to_next_page(self, seq_id: int) -> "SequenceTable":
        cur = self.seq_lens["seq", seq_id]
        size = jnp.array(self.page_size, dtype=jnp.int32)
        next_page = ((cur + size - 1) // size) * size
        new_seq_lens = self.seq_lens.at["seq", seq_id].set(next_page)
        return dataclasses.replace(self, seq_lens=new_seq_lens)
# ---
def assert_equal(in_tree, fail_message: str = ""):
    """Verifies that all the hosts have the same tree of values."""
    expected = broadcast_one_to_all(in_tree)
    if not jax.tree_util.tree_all(jax.tree_util.tree_map(lambda *x: np.all(np.equal(*x)), in_tree, expected)):
        raise AssertionError(f"{fail_message} Expected: {expected}; got: {in_tree}.")
# ---
def user_name_represent(id):
            # @todo: use s3_present_user?

            rep_str = "-"
            table = db.auth_user
            query = (table.id == id)
            row = db(query).select(table.first_name,
                                   table.last_name,
                                   limitby=(0, 1)).first()
            if row:
                rep_str = "%s %s" % (row.first_name, row.last_name)
            return rep_str
# ---
def __len__(self):
        return self.vocab_size
# ---
def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()
# ---
def _print_rejected(matches, after_match):
            if self.key.REJ in after_match:
                rejected = sorted(
                    set(after_match[self.key.REJ]).difference(
                        set(matches.get(self.key.REJ, []))
                    )
                )
                for key in rejected:
                    print('Key for minion {0} rejected.'.format(key))
# ---
def tff_model_fn() -> tff.learning.Model:
    return tff.learning.from_keras_model(
        keras_model=model_builder(),
        input_spec=input_spec,
        loss=loss_builder(),
        metrics=metrics_builder())
# ---
def test_best_of_n_returns_candidates(params, model_cfg, tokenizer):
    """Best-of-N should return at least one candidate."""
    results = best_of_n(
        params=params,
        source="x = 1 + 2\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(0),
        n=4,
        max_depth=2,
        temperature=1.0,
    )
    assert len(results) > 0
    assert all(isinstance(c, BeamCandidate) for c in results)
# ---
def is_old(node_info):
    '''
    Check if node or node.bl_idname is among
    the old nodes
    '''
    if isinstance(node_info, str):
        # assumes bl_idname
        return node_info in old_bl_idnames
    elif isinstance(node_info, bpy.types.Node):
        return node_info.bl_idname in old_bl_idnames
    else:
        return False
# ---
def close(self):
        """Close the event loop and thread gracefully."""
        if self.loop.is_running():
            self.loop.call_soon_threadsafe(self.loop.stop)
        self.thread.join()
        self.loop.close()
# ---
def test_count_subtraction(self):
        """Count subtraction"""
        count = Count(4, 2)
        self.assertEqual(count - 2, 2)
# ---
import re
def remove_all_spaces(text):
 return (re.sub(r'\s+', '',text))
# ---
def title(self):
        '''Title of the ebook. (mandatory)

        If this property is left unset, it defaults to "Untitled".'''
        try:
            return self._title
        except AttributeError:
            self.title = 'Untitled'
            return self._title
# ---
def vm_ops(self) -> PlatformOps:
        return _ManualPlatformOps(self._ssh, self._bootstrap)
# ---
def setUp(self):
        self.latitude = 32.074322
        self.longitude = 34.792081
        self.radius_meters = 100
        self.number_of_vertices = 36
        self.polycircle = \
            polycircles.Polycircle(latitude=self.latitude,
                                   longitude=self.longitude,
                                   radius=self.radius_meters,
                                   number_of_vertices=self.number_of_vertices)
# ---
def dig_let(s):
 d=l=0
 for c in s:
    if c.isdigit():
        d=d+1
    elif c.isalpha():
        l=l+1
    else:
        pass
 return (l,d)
# ---
def _show(x):
        if isinstance(x, NamedArray):
            arr = x.array
            axes = x.axes
        else:
            arr = x
            axes = None

        def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass

        jax.debug.inspect_array_sharding(arr, callback=cb)
        return x
# ---
def filter(self, record: Record) -> bool:
        """Filter complexes with error ligands.

        Parameters
        ----------
        record : Record
            The record to filter.

        Returns
        -------
        bool
            Whether the record should be keps.

        """
        return record.id.lower() not in IGNORE
# ---
def cleanup(self) -> None:
        """Cleanup transfer server and thread pool."""
        logger.info("Cleaning up JAX transfer server")
        self.executor.shutdown(wait=True)
        self.transfer_server = None
# ---
def _get_policy_uuid(conn, policy_name):
    policy_query = sa.sql.select([policy_table.c.uuid]).where(
        policy_table.c.name == policy_name
    )

    for policy in conn.execute(policy_query).fetchall():
        return policy[0]
# ---
def isnan(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in isnan")
    return elemwise(nxp.isnan, x, dtype=nxp.bool)
# ---
def resize_embeddings(self, new_size: int, key: Optional[PRNGKeyArray] = None):
        new_token_embeddings = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, Vocab=self.Vocab.resize(new_size), token_embeddings=new_token_embeddings)
# ---
def _simplify_captures(expr):
    def simplify_capture(capture):
        if capture == Ellipsis:
            return Ellipsis
        elif (capture.binding == capture.axes[0] or capture.binding is None) and len(capture.axes) == 1:
            return capture.axes[0]
        elif capture.binding is None:
            return capture.axes
        else:
            return {capture.binding: capture.axes}

    return [simplify_capture(capture) for capture in expr.captures]
# ---
def requirements():
    """Build the requirements list for this project"""
    requirements_list = []

    with open('requirements.txt') as requirements:
        for install in requirements:
            requirements_list.append(install.strip())

    return requirements_list
# ---
def save_utf8_file(fn, lines):
    """Save string lines into an UTF8 text files.
    """
    with open(fn, "w") as out_file:
        out_file.write("\n".join(lines).encode("utf-8"))
# ---
def get_stop_tokens(model_type: str) -> list[str]:
    """Get model-specific stop tokens."""
    if model_type == "llama":
        return ["<|eot_id|>"]
    elif model_type == "qwen":
        return ["<|im_end|>"]
    else:
        raise ValueError(f"Unknown model_type: {model_type}")
# ---
def _check_batch_size_divisibility(self):
        if self._data_axis_size is None:
            return
        for size in self.scheduler.unique_batch_sizes():
            if size % self._data_axis_size != 0:
                raise ValueError(
                    f"Batch size {size} is not divisible by data axis size {self._data_axis_size}. "
                    "This will lead to incorrect sharding. Set allow_non_divisible_batch_size=True to allow this."
                )
# ---
def i_enable_prop(prop):
    exec(f"bpy.context.{prop} = True")
# ---
def double_emphasis(self, text):
        """Rendering **strong** text.

        :param text: text content for emphasis.
        """
        return '<strong>%s</strong>' % text
# ---
def pytest_collection_modifyitems(config: Any, items: list[pytest.Item]) -> None:
    # If the --run-benchmark flag is set, do not skip anything.
    if config.getoption("--run-benchmark"):
        return
    # If the flag is not set, we check every test item
    skip_benchmark = pytest.mark.skip(reason="need --run-benchmark option to run")
    for item in items:
        if "benchmark" in item.fixturenames:
            item.add_marker(skip_benchmark)
# ---
def _unwrap(x):
        if isinstance(x, Metric):
            return x.value()
        return x
# ---
def attrs(self, value):
        self._attrs = OrderedDict(value)
# ---
def choice(question, options, default):
    "Ask the user to choose from a short list of named options"
    while True:
        sys.stdout.write("{} ({}) [{}]: ".format(question, "/".join(options), default))
        answer = sys.stdin.readline().strip()
        if len(answer) == 0:
            return default
        for opt in options:
            if answer == opt:
                return answer
# ---
def repeat(
    a: NamedArray, repeats: int | jnp.ndarray, axis: AxisSelector, total_repeat_length: int | None = None
) -> NamedArray:
    """Version of [jax.numpy.repeat][] that returns a NamedArray"""
    index = a.axis_indices(axis)
    if index is None:
        raise ValueError(f"Axis {axis} not found in array {a}")

    return named(
        jnp.repeat(a.array, repeats, axis=index, total_repeat_length=total_repeat_length),
        a.axes[:index] + (axis_name(axis),) + a.axes[index + 1 :],
    )
# ---
def make_perceiver(in_channels, out_channels):
    return Perceiver(
        num_freq_bands=4,
        max_freq=1.0,
        depth=2,
        input_axis=2,
        input_channels=in_channels,
        latent_dim=3,
        num_latents=2,
        num_classes=out_channels,
        weight_tie_layers=True,
        self_per_cross_attn=2,
    )
# ---
def get_Pairs_Count(arr,n,sum):
    count = 0  
    for i in range(0,n):
        for j in range(i + 1,n):
            if arr[i] + arr[j] == sum:
                count += 1
    return count
# ---
def format_temps(temps):
    return format_line(prefix='temps'.rjust(RJUST), values=temps)
# ---
def calculate_period_returns(self, returns):
        period_returns = (1. + returns).prod() - 1
        return period_returns
# ---
def approve(self, request, *args, **kwargs):
        response = super().update(request, *args, **kwargs)
        instance = self.get_object()
        instance.approve(processor=self.request.user)
        return response
# ---
def test_correct_output(self):
        hfmt = HtmlFormatter(nowrap=True)
        houtfile = StringIO.StringIO()
        hfmt.format(tokensource, houtfile)

        nfmt = NullFormatter()
        noutfile = StringIO.StringIO()
        nfmt.format(tokensource, noutfile)

        stripped_html = re.sub('<.*?>', '', houtfile.getvalue())
        escaped_text = escape_html(noutfile.getvalue())
        self.assertEquals(stripped_html, escaped_text)
# ---
import re
text = 'Python Exercises'
def replace_spaces(text):
  text =text.replace (" ", "_")
  return (text)
  text =text.replace ("_", " ")
  return (text)
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        return ScaleByCautiousState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)
# ---
def image_url(self):
        return self._get_profile().image_url()
# ---
def screenshot_to_file(self, filename, includes='subtitles'):
        self.command('screenshot_to_file', filename.encode(fs_enc), includes)
# ---
def _make(name: str = "test-job") -> cluster_pb2.Controller.LaunchJobRequest:
        job_name = JobName.root(name)
        return cluster_pb2.Controller.LaunchJobRequest(
            name=job_name.to_wire(),
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=1024**3),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )
# ---
def resize_vocab(self, new_size: int, key: PRNGKeyArray | None = None) -> "HackableLMHeadModel":
        pass
# ---
def with_gpu(gpu_type: str = "auto", count: int = 1, **kwargs) -> ResourceConfig:
        device = GpuConfig(variant=gpu_type, count=count)
        return ResourceConfig(device=device, **kwargs)
# ---
def test_area_weighted_rmse_perfect_prediction(self):
        """Verify RMSE is zero for perfect predictions."""
        target = torch.ones(2, 4, 4)
        gen = torch.ones(2, 4, 4)
        weights = torch.ones(4, 4) / 16

        rmse = area_weighted_rmse(target, gen, weights)

        assert torch.allclose(rmse, torch.zeros(2))
# ---
import re
def extract_date(url):
        return re.findall(r'/(\d{4})/(\d{1,2})/(\d{1,2})/', url)
# ---
def params(model_cfg):
    return init_edit_params(model_cfg, key=jax.random.PRNGKey(0))
# ---
def not_equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.not_equal(x1, x2)
# ---
def purge_queue_of_slot(self, slot_id: hax.NamedArray | int) -> "DecodeState":
        """Forward ``purge_queue_of_slot`` to ``TokenQueue`` and return an updated ``DecodeState``."""
        new_tqueue = self.tqueue.purge_queue_of_slot(slot_id)
        return dataclasses.replace(self, tqueue=new_tqueue)
# ---
import math
def sum_gp(a,n,r):
 total = (a * (1 - math.pow(r, n ))) / (1- r)
 return total
# ---
def backend_array_to_numpy_array(arr):
        return arr
# ---
def _testCpuMatmul(self, x, y, transpose_x=False, transpose_y=False):
    x_mat = np.matrix(x).T if transpose_x else np.matrix(x)
    y_mat = np.matrix(y).T if transpose_y else np.matrix(y)
    np_ans = x_mat * y_mat
    with self.test_session(use_gpu=False):
      tf_ans = tf.matmul(x, y, transpose_x, transpose_y).eval()
    self.assertAllClose(np_ans, tf_ans)
    self.assertAllEqual(np_ans.shape, tf_ans.shape)
# ---
def __str__(self):
        return f"WrappedAsyncDataset({str(self.dataset)})"
# ---
def testEmpty(self):
        gen = VectorInput([])
        tcToTotal = sTCToTotal()
        p = Pool()

        gen.data >> tcToTotal.envelope
        tcToTotal.TCToTotal >> (p, 'lowlevel.tctototal')

        run(gen)

        self.assertRaises(KeyError, lambda: p['lowlevel.tctototal'])
# ---
def test_get_instance_without_init():
    with pytest.raises(ValueError):
        DummyMultiton.get_instance()
# ---
def total_entries(self) -> int:
        return sum(len(v) for v in self.entries.values())
# ---
def test_ar_loss_is_positive(params, tiny_cfg):
    batch_size, seq_len = 2, 16
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (batch_size, seq_len), 1, 100)
    loss_mask = jnp.ones((batch_size, seq_len))

    loss, _ = ar_loss(params, token_ids, loss_mask, tiny_cfg)
    assert float(loss) > 0
# ---
def __repr__(self):
        return f"GroupByOp(key={_get_fn_name(self.key_fn)})"
# ---
def list_all(self):
        self._call_all('list_all')
# ---
def count_Num(n): 
    if (n == 1): 
        return 1
    count = pow(2,n - 2) 
    return count
# ---
def __call__(self, indices: int) -> int: ...
# ---
def _is_compact(data: xr.Dataset, means: xr.Dataset, stds: xr.Dataset) -> bool:
    return all(
        not _var_name_encode_level(str(v))
        for d in [data, means, stds]
        for v in d.keys()
    )
# ---
def current(self):
        return self.voltage / self.resistance
# ---
def series_sum(number):
 total = 0
 total = (number * (number + 1) * (2 * number + 1)) / 6
 return total
# ---
def get_transactions(self, request: cluster__pb2.Controller.GetTransactionsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetTransactionsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def get_destroy_serializer(self):
        return serializers.Dict(
            InstanceIds=serializers.ListOfOne(serializers.Property("InstanceId"))
        )
# ---
def get_padding_count_from_batch(batch: LmExample, pad_token_id: int) -> tuple[int, int]:
    """
    Extract padding statistics from a batch (on host).
    """
    tokens = jax.device_get(batch.tokens.array)
    padding_count = int(np.sum(tokens == pad_token_id))
    total_tokens = batch.tokens.size
    return padding_count, total_tokens
# ---
def test_preemptible_false_adds_constraint(self):
        resources = ResourceConfig(preemptible=False)
        constraints = convert_constraints(resources)
        assert len(constraints) == 1
        c = constraints[0]
        assert c.key == "preemptible"
        assert c.value == "false"
# ---
def h_fs_unlink(_,path): os.unlink(path)
# ---
def __contains__(self, item):
        return item in self.subqueries
# ---
def less(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.less](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.less.html)
    """
    return jnp.less(x1, x2)
# ---
def zarr_compressor(self) -> Union[dict, str, None]:
        """The compressor used by Zarr for intermediate data."""
        return self._zarr_compressor
# ---
def _matches_target(entry: dict) -> bool:
        if entry.get("dataset") not in {None, dataset_name}:
            return False
        if config_name and entry.get("config") != config_name:
            return False
        return True
# ---
def axis_spec_to_tuple(axis_spec: AxisSelection) -> tuple[AxisSelector, ...]:
    if isinstance(axis_spec, Mapping):
        return tuple(Axis(name, size) if size is not None else name for name, size in axis_spec.items())

    if isinstance(axis_spec, Axis | str):
        return (axis_spec,)

    if isinstance(axis_spec, Sequence):
        return tuple(axis_spec)

    raise ValueError(f"Invalid axis spec: {axis_spec}")
# ---
def remove_directory_contents(path):
    for f in os.listdir(path):
        os.remove(os.path.join(path, f))
# ---
def basic(
    da,
    fig,
    timestamp,
    timestep_value,
    framedim="time",
    plotmethod=None,
    subplot_kw=None,
    **kwargs,
):
    # create axis
    ax = fig.subplots(subplot_kw=subplot_kw)
    pp = _base_plot(
        ax, da, timestamp, timestep_value, framedim, plotmethod=plotmethod, **kwargs
    )
    return ax, pp
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """Peak FLOP/s for a single device."""
        return self.device.device_flops(dtype)
# ---
def name(self) -> str:
        return "processes"
# ---
def __init__(self, models, config):
        self.models = models
        self.config = config
        self.es = Elasticsearch(
            hosts=self.config.hosts,
            **self.config.get("client_options", {})
        )

        self.types = AttributeDict()
# ---
def test_creation_with_endpoint(self):
        """Test S3Location creation with custom endpoint."""
        loc = S3Location(
            bucket="test-bucket",
            path="data/test.zarr",
            endpoint_url="https://s3.example.com",
        )
        assert loc.endpoint_url == "https://s3.example.com"
# ---
def test_deploy(self, cav, ue):
        s3 = boto.connect_s3()
        s3.create_bucket("laterpay-rubberjack-ebdeploy")  # FIXME Remove hardcoded bucket name

        with tempfile.NamedTemporaryFile() as tmp:
            result = CliRunner().invoke(rubberjack, ['deploy', tmp.name], catch_exceptions=False)

            self.assertEquals(result.exit_code, 0, result.output)
# ---
def _coerce_to_rr(s: Union[str, RepoRef]) -> RepoRef:
    if isinstance(s, RepoRef):
        return s
    else:
        return RepoRef.from_string(s)
# ---
def sigmoid(a: A) -> A:
    return wrap_elemwise_unary(jnn.sigmoid, a)
# ---
def handler():
            nonlocal called
            called += 1
            raise ValueError("yeah sync")
# ---
def __init__(self, prev):
        self.prev = prev  # ContentOfGroup or CharClass
        self.q = WrappedCharClass()

        # forward function
        self.add = self.q.add

        self.next_is_range = False
        self.empty = True
        self.can_mutate = True
# ---
def chunks(lst, n):
      """Yield successive n-sized chunks from lst."""
      for i in range(0, len(lst), n):
        yield lst[i:i + n]
# ---
def sample_input_files(tmp_path):
    """Create standard sample input files for skip_existing tests."""
    input_dir = tmp_path / "input"
    input_dir.mkdir()
    for i in range(3):
        with open(input_dir / f"input-{i}.jsonl", "w") as f:
            f.write(f'{{"id": {i}}}\n')
    return input_dir
# ---
def _leaf_conforms(shape_spec: Union[ShapeSpec, NamedShapeSpec], leaf):
        if isinstance(shape_spec, ShapeSpec):  # type: ignore
            return shape_spec.shape == leaf.shape and shape_spec.dtype == leaf.dtype
        else:
            return (shape_spec.shape is None or shape_spec.shape == leaf.axes) and (
                shape_spec.dtype is None or shape_spec.dtype == leaf.dtype
            )
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "RotaryEmbeddingsConfig":
        return DefaultRotaryEmbeddingsConfig(theta=rope_theta, factor=config.get("factor", 1.0))
# ---
def last(n):
   return n[-1]
def sort_list_last(tuples):
  return sorted(tuples, key=last)
# ---
def main(argv=sys.argv[1:]):
    return KanboardShell().run(argv)
# ---
def testRegression(self):
        envelope = range(22050)
        envelope.reverse()
        envelope = range(22050) + envelope

        gen = VectorInput(envelope)
        tcToTotal = sTCToTotal()
        p = Pool()

        gen.data >> tcToTotal.envelope
        tcToTotal.TCToTotal >> (p, 'lowlevel.tctototal')

        run(gen)

        self.assertAlmostEqual(p['lowlevel.tctototal'],
                               TCToTotal()(envelope))
# ---
def _tri_upper_eq_mask(Ci: Axis, Cj: Axis) -> NamedArray:
    """Mask for i <= j (upper-triangular incl. diagonal) in (Ci, Cj) coordinates.

    Used to zero-out invalid contributions when building strictly lower-triangular
    in-chunk operators for the UT forward substitution.
    """
    ii = hax.arange(Ci)
    jj = hax.arange(Cj)
    I = ii.broadcast_axis(Cj)
    J = jj.broadcast_axis(Ci)
    return I <= J
# ---
def find_Diff(arr,n): 
    arr.sort()  
    count = 0; max_count = 0; min_count = n 
    for i in range(0,(n-1)): 
        if arr[i] == arr[i + 1]: 
            count += 1
            continue
        else: 
            max_count = max(max_count,count) 
            min_count = min(min_count,count) 
            count = 0
    return max_count - min_count
# ---
def get_shard_dir(dir_name: os.PathLike, subset_name: str | None, split: str) -> os.PathLike | str:
    """Creates a new path with the subset and split names.
    e.g., create_subset_name('gs://thisserver/testfolder-a982374', 'subset', 'train') -> 'gs://thisserver/testfolder-a982374/subset/train'
    """
    if (subset_name == "default") or (subset_name is None):
        return os.path.join(dir_name, split)
    return os.path.join(dir_name, subset_name, split)
# ---
def prepopulate(self):
        conn = sqlite3.connect(self.dbname)
        conn.execute("CREATE TABLE unrelated (random STRING, data INTEGER)")
        conn.commit()
        del conn
# ---
def state(self) -> cluster_pb2.TaskState:
        """Get current task state (shortcut for status().state)."""
        return self.status().state
# ---
def test_xxh3_64_vector():
    # Catch un-intentional regressions
    assert hash_xxh3_64(b"hello") == 10760762337991515389
# ---
def _tensordot(a, b, axes):
    x = nxp.tensordot(a, b, axes=axes)
    ind = [slice(None, None)] * x.ndim
    for a in sorted(axes[0]):
        ind.insert(a, None)
    x = x[tuple(ind)]
    return x
# ---
def test_ar_generate_respects_max_tokens(params, model_cfg, tokenizer):
    """Generation should not exceed max_new_tokens."""
    context = tokenizer.encode_source("x = 1\n")
    max_tokens = 5
    generated, _ = _ar_generate_tokens(
        params=params,
        context_token_ids=context,
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(0),
        temperature=1.0,
        max_new_tokens=max_tokens,
    )
    assert len(generated) <= max_tokens
# ---
def cell():  # i, j):
            difficulty = 8
            """6 + (
                (9 if i == j else
                 8)
                if i in [0, size - 1] and j in [0, size - 1] else
                (7 if j in [0, size - 1] else
                 (6 if j % 2 == 1 and (i in [0, size - 1] or j in [0, size - 1]) else
                  (5 if 0 < i < size - 1 else 8))))"""

            for li in fpik(e(r(), difficulty)):
                yield li
# ---
def v5p8_scale_group() -> config_pb2.ScaleGroupConfig:
    """Single-host TPU scale group (v5p-8)."""
    return config_pb2.ScaleGroupConfig(
        name="tpu-v5p-8",
        min_slices=0,
        max_slices=10,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def restore_view(self):
        self.import_table.columns.restore_columns()
# ---
def age_ms(self) -> int:
        """Get age of this timestamp in milliseconds."""
        return _now_ms() - self._epoch_ms
# ---
def _multislice_info_to_env_vars(multislice: MultisliceInfo) -> dict[str, str]:
    if multislice is not None:
        mxla_env = {
            "MEGASCALE_COORDINATOR_ADDRESS": f"{multislice.coordinator_ip}:{multislice.port}",
            "MEGASCALE_NUM_SLICES": str(multislice.num_slices),
            "MEGASCALE_PORT": f"{multislice.port}",
            "MEGASCALE_SLICE_ID": str(multislice.slice_id),
        }
    else:
        mxla_env = {}
    return mxla_env
# ---
def decode(self, cipherText, shift):
        a = ord('A')
        decoder = [a + (c - shift if c >= shift else c - shift + 26) for c in range(26)]
        plain = [chr(decoder[ord(c) - a]) for c in cipherText]
        return ''.join(plain)
# ---
def total_attempts(self) -> int:
        """Total number of attempts."""
        return len(self.attempts)
# ---
def test_preserved(self):
        serial, timestamp, tyield = self.PRESERVE_RECORD

        assert_equals(self.db.get_last_historic(serial), timestamp)
        assert_equals(self.db.get_one_historic(serial, timestamp), tyield)
# ---
def convert_to_export(self, value, env):
        if not isinstance(self.selection, list):
            # FIXME: this reproduces an existing buggy behavior!
            return value if value else ''
        for item in self._description_selection(env):
            if item[0] == value:
                return item[1]
        return False
# ---
def from_ms(cls, epoch_ms: int) -> "Timestamp":
        """Create timestamp from milliseconds since epoch."""
        return cls(epoch_ms)
# ---
def wrapper(a, by, **kwargs):
            return func(a, nxp.squeeze(by), **kwargs)
# ---
def __init__(self):
        self._server_info = None
# ---
def check_Triangle(x1,y1,x2,y2,x3,y3): 
    a = (x1*(y2-y3)+x2*(y3-y1)+x3*(y1-y2))   
    if a == 0: 
        return ('No') 
    else: 
        return ('Yes')
# ---
def get_list_of_block_numbers(item):
    """ Creates a list of block numbers of the given list/single event"""
    if isinstance(item, list):
        return [element["blockNumber"] for element in item]

    if isinstance(item, dict):
        block_number = item["blockNumber"]
        return [block_number]

    return list()
# ---
def the_collection_name1_is_not_in_the_collection_name2(name1, name2):
    assert not bpy.data.collections.get(name2).children.get(name1)
# ---
def _as_spec(source: str | InputFileSpec) -> InputFileSpec:
    """Normalize source to InputFileSpec for consistent downstream handling."""
    if isinstance(source, InputFileSpec):
        return source
    return InputFileSpec(path=source)
# ---
def list_item(self, text):
        """Rendering list item snippet. Like ``<li>``."""
        return '<li>%s</li>\n' % text
# ---
def _update_word_list(self):
        """Update and sort found words according to frequency."""
        wordlist = self._word_freq.items()
        wordlist.sort(key=lambda x:x[1], reverse=True)
        self._word_list = [items[0] for items in wordlist]
# ---
def add_sort(self, sort):
        self.sorts.append(sort)
# ---
def _add_graphics(root, params):
    e = root.find('./devices/graphics/[@type]')
    if e is not None:
        params['graphics'] = e.get('type')
# ---
def num_gpus(self) -> int:
        return 0
# ---
def __len__(self):
        return SummaryKeyMatcher.cNamespace().size(self)
# ---
def store_weights(self, weight_id: int, params_dict: dict[str, tuple[pa.Schema, Sequence[pa.RecordBatch]]]) -> None:
        with self._lock:
            # remove all other weights
            self._weights_store.clear()
            self._weights_store[weight_id] = params_dict
            self._latest_weight_id = weight_id
# ---
def __contains__(self, key):
        return SummaryKeyMatcher.cNamespace().match_key(self, key)
# ---
def JOB_STATES(state):
    if state == 'failed':
        return BOLD() + RED() + state + ENDC()
    elif state == 'done':
        return BOLD() + GREEN() + state + ENDC()
    elif state in ['running', 'in_progress']:
        return GREEN() + state + ENDC()
    elif state == 'partially_failed':
        return RED() + state + ENDC()
    else:
        return YELLOW() + state + ENDC()
# ---
def foo():
            return hax.zeros((Dim1, Dim2, Dim3))
# ---
def is_metadata_ref(thing, reftype=dict):
    return isinstance(thing, reftype) and \
        len(thing) == 1 and \
        isinstance(thing.get('$dnanexus_link'), reftype) and \
        isinstance(thing['$dnanexus_link'].get('metadata'), basestring)
# ---
def loss_fn(x_in, w_in, y_in):
        return linear_softmax_cross_entropy_loss(
            x_in,
            y_in,
            w_in,
            reduction="mean",
            implementation="mosaic_tpu",
        )
# ---
def size(self) -> int:
        return sum(1 for w in self._workers.values() if w.status in (WorkerStatus.IDLE, WorkerStatus.BUSY))
# ---
def name(dtype):
            if hasattr(dtype, "name"):
                return dtype.name
            elif hasattr(dtype, "dtype"):
                return name(dtype.dtype)
# ---
def _recover_expired_leases(self) -> None:
        """Move expired leases back to the front of the queue."""
        current_time = time.time()
        expired = []
        for lease_id, (_item, timestamp, timeout) in self.leases.items():
            if current_time - timestamp >= timeout:
                expired.append(lease_id)

        for lease_id in expired:
            item, _, _ = self.leases[lease_id]
            self.queue.insert(0, item)
            del self.leases[lease_id]
# ---
def Check_Solution(a,b,c):  
    if b == 0:  
        return ("Yes")  
    else: 
        return ("No")
# ---
def map_row(row: dict):
    """Transform evaluation record to Dolma format.

    Args:
        row: Record with "prompt" and "response" fields

    Returns:
        Record with "text" field containing prompt + response
    """
    row["text"] = row["prompt"] + "\n" + row["response"]
    return row
# ---
def build_translations(srcdir, blddir, langs):
    for lang in langs:
        outdir = os.path.join(blddir, lang)
        os.makedirs(outdir, exist_ok=True)
        subprocess.call([
            'msgfmt', os.path.join(srcdir, lang, lang + '.po'),
            '-o', os.path.join(outdir, lang + '.gmo')
        ])
# ---
def __enter__(self):
        self.start()
        return self
# ---
def __post_init__(self):
        assert (
            self.num_heads % self.num_kv_heads == 0
        ), f"num_heads={self.num_heads} not divisible by num_kv_heads={self.num_kv_heads}."
# ---
def scan_fun(acc, z, x):
        return (acc + z * x).scalar(), x * z
# ---
def startup_industry(self):
        return self.startup.primary_industry if self._get_startup() else None
# ---
def __neg__(self) -> "NamedArray":  # pragma: no cover
        return haliax.negative(self)
# ---
def list_slices(self, group_config: config_pb2.ScaleGroupConfig) -> list[str]:
        return []
# ---
def test_scale_up_passes_tags_to_manager(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """scale_up() passes tags to the VmManager."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        group.scale_up(tags={"env": "prod", "team": "ml"})

        manager.create_vm_group.assert_called_once_with({"env": "prod", "team": "ml"})
# ---
def __abs__(self, /):
        if self.dtype not in _numeric_dtypes:
            raise TypeError("Only numeric dtypes are allowed in __abs__")
        if self.dtype == complex64:
            dtype = float32
        elif self.dtype == complex128:
            dtype = float64
        else:
            dtype = self.dtype
        return elemwise(nxp.abs, self, dtype=dtype)
# ---
def odd_position(nums):
	return all(nums[i]%2==i%2 for i in range(len(nums)))
# ---
def _format_static_arrays(self):
        return [f"  Static array at field {field}" for field in self.static_arrays]
# ---
def tn_ap(a,n,d):
  tn = a + (n - 1) * d
  return tn
# ---
def launch(self, job_request: JobRequest) -> None:
        """Launch job and start heartbeat thread."""
        self._status_file.write_status(STATUS_RUNNING)
        self._job_id = self.cluster.launch(job_request)
        self._start_heartbeat()
# ---
def reset(self):
        self._calls = []
# ---
def callback(self):
            print(f"{prefix}JitScheduler State:")
            print(f"{prefix}Queued Tokens: {self.queued_tokens}")
            print(f"{prefix}Queued Slot IDs: {self.queued_slot_ids}")
            print(f"{prefix}Num Queued Tokens: {self.num_queued_tokens}")
# ---

def digits(n):
    """Given a positive integer n, return the product of the odd digits.
    Return 0 if all digits are even.
    For example:
    digits(1)  == 1
    digits(4)  == 0
    digits(235) == 15
    """
    product = 1
    odd_count = 0
    for digit in str(n):
        int_digit = int(digit)
        if int_digit%2 == 1:
            product= product*int_digit
            odd_count+=1
    if odd_count ==0:
        return 0
    else:
        return product
# ---
def test_succeeded(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_SUCCEEDED) == JobStatus.SUCCEEDED
# ---
def _console_pwfile_path(uuid):
    """Return the file path for storing the ipmi password for a console."""
    file_name = "%(uuid)s.pw" % {'uuid': uuid}
    return os.path.join(CONF.tempdir, file_name)
# ---
def rad2deg(a: A) -> A:
    return wrap_elemwise_unary(jnp.rad2deg, a)
# ---
def blockwise_fn_flattened(out_key):
        in_keys = key_function(out_key)[1:]  # drop function in position 0
        # flatten (nested) lists indicating contraction
        if isinstance(in_keys[0], list):
            in_keys = list(flatten(in_keys))
        return in_keys
# ---
def dummy_fn(cfg: DummyCfg):
    # write one tiny file so the step "does something"
    out_path = os.path.join(cfg.output_path, "dummy")
    os.makedirs(out_path, exist_ok=True)
    with open(os.path.join(out_path, "done.txt"), "w") as f:
        f.write(str(cfg.x))
    return cfg.x
# ---
def type(self):
        return self._get_profile().user_type
# ---
def add_volume(self, volume):
        ''' add a volume or volume mount to the proper location '''
        exist_volumes = self.get_volumes()
        if not volume:
            return

        if not exist_volumes:
            self.put(DeploymentConfig.volumes_path, [volume])
        else:
            exist_volumes.append(volume)
# ---
def rate_noise(self, t):
    return (1 - self.eps) / (1 - (1 - self.eps) * t)
# ---
def shutdown(self):
        self.llm.shutdown()
# ---
def as_async_dataset(self) -> "AsyncDataset[T_co]":
        raise NotImplementedError("...")
# ---
from array import array
def zero_count(nums):
    n = len(nums)
    n1 = 0
    for x in nums:
        if x == 0:
            n1 += 1
        else:
          None
    return round(n1/n,2)
# ---
def _is_named_or_none(x):
    return x is None or is_named_array(x)
# ---
def gcd(a, b):
    while b != 0:
        (a, b) = (b, a % b)
    return a
# ---
def i0(a: A) -> A:
    return wrap_elemwise_unary(jnp.i0, a)
# ---
def ss_short_motif(feats, random, helix_prob=0.5):
    feats["design_ss_mask"].zero_()
    feats["ss_type"].zero_()
    design_mask = feats["design_mask"].bool()
    seq_len = design_mask.sum().item()
    design_indices = torch.where(design_mask)[0]

    start_pos = random.choice(design_indices[: seq_len - 4 + 1].cpu())
    motif_type = 2 if random.random() < helix_prob else 3

    feats["design_ss_mask"][start_pos : start_pos + 4] = 1
    feats["ss_type"][start_pos : start_pos + 4] = motif_type
# ---
def activate(self):
        exc = self.exc
        if exc is not None:
            self.exc = None
            self.tb = traceback.format_exception(exc.__class__, exc,
                                                 exc.__traceback__)
# ---
def value_match(cls, pattern, value):
        """Determine whether the value matches the pattern. The value
        may have any type.
        """
        return cls.string_match(pattern, util.as_string(value))
# ---
def sort_String(str) : 
    str = ''.join(sorted(str)) 
    return (str)
# ---
def resolve_axis(axis_spec: AxisSpec, axis_selection: AxisSelector) -> Axis: ...
# ---
def http_error_401(self, *args, **kwds):
                self.parent.record("basic")
                urllib.request.HTTPBasicAuthHandler.http_error_401(self,
                                                            *args, **kwds)
# ---
def get_layer(self, index: int) -> M:
        """Return the ``index``th block in this sequential container."""

        return self.blocks[index]
# ---
def test_maybe_reduce_loss_weighted_sum():
    Batch, Pos = hax.make_axes(Batch=2, Pos=2)
    arr = hax.named(jnp.array([[1.0, 2.0], [3.0, 4.0]]), (Batch, Pos))
    weights = hax.named(jnp.array([[0.5, 1.5], [1.0, 0.0]], dtype=jnp.float32), (Batch, Pos))

    reduced = maybe_reduce_loss(arr, hax.sum, Pos, where=None, weight=weights)

    expected = jnp.array(
        [
            1.0 * 0.5 + 2.0 * 1.5,
            3.0 * 1.0 + 4.0 * 0.0,
        ]
    )
    assert jnp.allclose(reduced.array, expected)
# ---
def _get_num_train_steps(param_count: int, batch_size: int, seq_len: int, tpp: int = 20) -> int:
    total_tokens = param_count * tpp
    return max(1, total_tokens // (batch_size * seq_len))
# ---
def changed(self) -> bool:
        return self._changed
# ---
def trapz(y, x=None, axis=-1):
    return _Ntrapz(y, x, axis=axis)
# ---
def strerror(errno_value):
    """
    The string representation of a particular errno
    """
    return "[Errno {}] Invalid argument".format(errno_value)
# ---
def _read_ovf(job_id):
    file_name = os.path.join(_V2V_DIR, "%s.ovf" % job_id)
    try:
        with open(file_name, 'r') as f:
            return f.read()
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
        raise NoSuchOvf("No such ovf %r" % file_name)
# ---
def removeSelected(self):
        self.model.remove_selected()
# ---
def init():
        if not hasattr(DefaultDotGeneralOp, "_instance"):
            DefaultDotGeneralOp._instance = DefaultDotGeneralOp()

        return DefaultDotGeneralOp._instance
# ---
def test_string(self):
        """Store and retrieve a string"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "abc"})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["id"], "abc")
        self.assertTrue(isinstance(item["id"], str))
# ---
def kill_ssh_session(host_alias: str) -> None:
    """Kill SSH session using control socket."""
    try:
        subprocess.run(["ssh", "-O", "exit", host_alias], capture_output=True, check=False, timeout=5)
    except subprocess.TimeoutExpired:
        logger.warning(f"Timeout killing SSH control connection to {host_alias}")
    except Exception as e:
        logger.debug(f"Failed to kill SSH control connection to {host_alias}: {e}")
# ---
def require_permissions(user, *permissions):
    for perm in permissions:
        if not user.has_perm(perm):
            raise PermissionRequired(perm)
# ---
def __call__(self, input_ids: NamedArray, *, key: PRNGKeyArray | None = None):
        """Alias for `embed`. key is ignored."""
        return self.embed(input_ids)
# ---
def reset(self) -> "KvPageCache":
        """Return a reset version of this cache."""
        reset_pages = jnp.zeros_like(self.kv_pages.array)
        return dataclasses.replace(self, kv_pages=NamedArray(reset_pages, self.kv_pages.axes))
# ---
def eos_token_id(self) -> int:
        return 2
# ---
def current_label_start(self):
        if self.current_label and self.message:
            return self.message.get_label_range(self.current_label, self.proto_view, False)[0]
        else:
            return -1
# ---
def test_gufunc_two_inputs(spec):
    def foo(x, y):
        return np.einsum("...ij,...jk->ik", x, y)

    a = xp.ones((2, 3), chunks=100, dtype=int, spec=spec)
    b = xp.ones((3, 4), chunks=100, dtype=int, spec=spec)
    x = apply_gufunc(foo, "(i,j),(j,k)->(i,k)", a, b, output_dtypes=int)
    assert_equal(x, 3 * np.ones((2, 4), dtype=int))
# ---
def sequential_search(dlist, item):
    pos = 0
    found = False
    while pos < len(dlist) and not found:
        if dlist[pos] == item:
            found = True
        else:
            pos = pos + 1
    return found, pos
# ---
def test_shard_scalar_in_module():
    with axis_mapping(resource_map):

        class MyModule(eqx.Module):
            scalar: jnp.ndarray

            def __init__(self):
                self.scalar = jnp.zeros(
                    (),
                )

        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModule)()
            assert mod.scalar.sharding.is_fully_replicated
# ---
def get_env_vars(self):
        '''return a environment variables '''
        return self.get(DeploymentConfig.env_path) or []
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        if not hasattr(self, "_tracker_cm"):
            raise RuntimeError("This tracker is not set as the global tracker")
        self._tracker_cm.__exit__(exc_type, exc_val, exc_tb)
        delattr(self, "_tracker_cm")
# ---
def get_screens(self):
            """Get the available screens.

            A typical multi-monitor workstation comprises one `Display` with
            multiple `Screen` s.  This method returns a list of screens which
            can be enumerated to select one for full-screen display.

            For the purposes of creating an OpenGL config, the default screen
            will suffice.

            :rtype: list of `Screen`
            """
            raise NotImplementedError('deprecated')
# ---
def to_chunksize(chunkset: T_RectangularChunks) -> T_RegularChunks:
    """Convert a chunkset to a chunk size for Zarr.

    Parameters
    ----------
    chunkset: tuple of tuples of ints
        From the ``.chunks`` attribute of an ``Array``

    Returns
    -------
    Tuple of ints
    """

    if not _check_regular_chunks(chunkset):
        raise ValueError(f"Array must have regular chunks, but found chunks={chunkset}")

    return tuple(max(c[0], 1) for c in chunkset)
# ---
def group_tuples(Input): 
	out = {} 
	for elem in Input: 
		try: 
			out[elem[0]].extend(elem[1:]) 
		except KeyError: 
			out[elem[0]] = list(elem) 
	return [tuple(values) for values in out.values()]
# ---
def print_all(self):
        self._call_all('print_all')
# ---
def deploy_rpc_test_contract(deploy_client, name):
    contract_path, contracts = get_test_contract(f"{name}.sol")
    contract_proxy, _ = deploy_client.deploy_solidity_contract(
        name, contracts, libraries=dict(), constructor_parameters=None, contract_path=contract_path
    )

    return contract_proxy
# ---
def wait(self, futures: list[_ImmediateFuture], num_returns: int = 1) -> tuple[list, list]:
        """All futures are immediately ready."""
        return futures[:num_returns], futures[num_returns:]
# ---
def __init__(self):
        self.scope = {}
# ---
def __hash__(self):
        """Hash."""
        return hash(self.verb)
# ---
def extract_symmetric(test_list):
  temp = set(test_list) & {(b, a) for a, b in test_list}
  res = {(a, b) for a, b in temp if a < b}
  return (res)
# ---
def bin_coff(n, r): 
	val = 1
	if (r > (n - r)): 
		r = (n - r) 
	for i in range(0, r): 
		val *= (n - i) 
		val //= (i + 1) 
	return val 
def find_ways(M): 
	n = M // 2
	a = bin_coff(2 * n, n) 
	b = a // (n + 1) 
	return (b)
# ---
def after_batch(self, num_batches_seen: int) -> None:
        if self.cuda_snapshot_frequency is not None:
            if num_batches_seen % self.cuda_snapshot_frequency == 0:
                torch.cuda.memory._dump_snapshot(
                    str(
                        self.output_dir
                        / f"cuda_memory_snapshot_{num_batches_seen}.pickle"
                    )
                )
# ---
def test_wait_create_table(self):
        """Create table shall wait for the table to come online."""
        tablename = "foobar_wait"
        hash_key = DynamoKey("id")
        self.dynamo.create_table(tablename, hash_key=hash_key, wait=True)
        self.assertIsNotNone(self.dynamo.describe_table(tablename))
# ---
def mul_list(nums1,nums2):
  result = map(lambda x, y: x * y, nums1, nums2)
  return list(result)
# ---
def _check_product_lot(self, cr, uid, ids, context=None):
        """ Checks whether move is done or not and production lot is assigned to that move.
        @return: True or False
        """
        for move in self.browse(cr, uid, ids, context=context):
            if move.prodlot_id and move.state == 'done' and (move.prodlot_id.product_id.id != move.product_id.id):
                return False
        return True
# ---
def date(selector):
                return DateGuesser(Regexp(CleanText(selector), r'\w+ (\d{2}/\d{2})'), Env('date_guesser')) | Transaction.Date(selector)
# ---
def display(self, callingWindow, srcContext, mainItem):
        if srcContext not in ("marketItemGroup", "marketItemMisc") or self.mainFrame.getActiveFit() is None:
            return False

        if mainItem is None:
            return False

        for attr in ("emDamage", "thermalDamage", "explosiveDamage", "kineticDamage"):
            if mainItem.getAttribute(attr) is not None:
                return True

        return False
# ---
def _substitute_default_descendant():
        """Replaces default descendant id with input_type."""
        id_ = input_type.get('default_descendant', None)
        if id_ is not None:
            input_type['default_descendant'] = input_types[id_]
# ---
def get_block_metadata(self, header_hash: bytes) -> Optional[BlockMetadata]:
        with self.lock:
            return self._state.get_block_metadata(header_hash)
# ---
def add_list(nums1,nums2):
  result = map(lambda x, y: x + y, nums1, nums2)
  return list(result)
# ---
def split_two_parts(list1, L):
    return list1[:L], list1[L:]
# ---
def enable_chaos(
    key: str,
    failure_rate: float = 1.0,
    error: Exception | None = None,
    delay_seconds: float = 0.0,
    max_failures: int | None = None,
) -> None:
    _rules[key] = ChaosRule(
        failure_rate=failure_rate,
        error=error,
        delay_seconds=delay_seconds,
        max_failures=max_failures,
    )
# ---
def test_eye_not_square():
    a = xp.eye(1, 3, k=2)
    assert_array_equal(a, np.eye(1, 3, k=2))
# ---
def callable_bytes(self) -> bytes | None:
        return self._callable_bytes
# ---
def is_mysql_running():
    try:
        import MySQLdb
        with MySQLdb.connect(host='127.0.0.1', user='root'):
            pass
        return True
    except:
        return False
# ---
def _default_wandb_entity() -> str:
    if _wandb_mode_disabled() or not os.getenv("WANDB_API_KEY"):
        return WANDB_ENTITY
    return wandb.Api().default_entity
# ---
def run_buildoption_test(options):
    for job in JERRY_BUILDOPTIONS:
        ret, _ = create_binary(job, options)
        if ret:
            break

    return ret
# ---
def get_input(self, step: int) -> Input:
        return self[step][0]
# ---
def is_lower(string):
  return (string.lower())
# ---
def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):
        super(LW_AveragePooling3D, self).__init__(pool_size, strides, border_mode, dim_ordering)
# ---
def comb_sort(nums):
    shrink_fact = 1.3
    gaps = len(nums)
    swapped = True
    i = 0
    while gaps > 1 or swapped:
        gaps = int(float(gaps) / shrink_fact)
        swapped = False
        i = 0
        while gaps + i < len(nums):
            if nums[i] > nums[i+gaps]:
                nums[i], nums[i+gaps] = nums[i+gaps], nums[i]
                swapped = True
            i += 1
    return nums
# ---
def _default_trainer_for_run(run_name: str) -> TrainerConfig:
    try:
        trainer = DEFAULT_TRAINER_CONFIGS[run_name]
    except KeyError as exc:
        raise ValueError(
            f"No TrainerConfig known for run '{run_name}'. "
            "Update DEFAULT_TRAINER_CONFIGS in exp1803_convert_32b_phases.py with the training step for this run."
        ) from exc
    return deepcopy(trainer)
# ---
def get_Inv_Count(arr,n): 
    inv_count = 0
    for i in range(n): 
        for j in range(i + 1,n): 
            if (arr[i] > arr[j]): 
                inv_count += 1
    return inv_count
# ---
def allocator():
    """Create PortAllocator with small range for testing."""
    return PortAllocator(port_range=(40000, 40100))
# ---
def chkList(lst): 
    return len(set(lst)) == 1
# ---
def sort(self, axis: AxisSelector) -> Any:  # pragma: no cover
        return haliax.sort(self, axis=axis)
# ---
def question_suffix(cls) -> str:
        """Use Tinker's question suffix format."""
        return TinkerMathEnvBase.question_suffix()
# ---
def test_logout_behaves_correctly(self):
        # Ensure logout behaves correctly - regarding the session.
        with self.client:
            self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="admin_user"),
                follow_redirects=True,
            )
            response = self.client.get("/logout", follow_redirects=True)
            self.assertIn(b"You were logged out. Bye!", response.data)
            self.assertFalse(current_user.is_active)
# ---
def _assert_covariance_matrix(cov: NamedArray, axis: Axis, expected: jnp.ndarray) -> None:
    assert jnp.allclose(cov.array, expected)  # type: ignore[union-attr]
    assert cov.axes[0] == axis  # type: ignore[union-attr]
    assert cov.axes[1].name == f"{axis.name}_cov"  # type: ignore[union-attr]
    assert cov.axes[1].size == axis.size
# ---
def Find_Max(lst): 
    maxList = max((x) for x in lst) 
    return maxList
# ---
def init(cls, Vocab: Axis, config: QwenConfig, *, key) -> "QwenLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = QwenTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)

        return QwenLMHeadModel(transformer, embeddings, lm_head)
# ---
def _testGpuMatmul(self, x, y, transpose_x=False, transpose_y=False):
    x_mat = np.matrix(x).T if transpose_x else np.matrix(x)
    y_mat = np.matrix(y).T if transpose_y else np.matrix(y)
    np_ans = x_mat * y_mat
    with self.test_session(use_gpu=True):
      tf_ans = tf.matmul(x, y, transpose_x, transpose_y).eval()
    self.assertAllClose(np_ans, tf_ans)
    self.assertAllEqual(np_ans.shape, tf_ans.shape)
# ---
def _generate_nonce(ts, secret, salt=None, chars=string.hexdigits.upper()):
	# TODO: Add IP-address to nonce
	if not salt:
		try:
			rng = random.SystemRandom()
		except NotImplementedError:
			rng = random
		salt = ''.join(rng.choice(chars) for i in range(16))
	ctx = hashlib.md5(('%s:%s:%s' % (ts, salt, secret)).encode())
	return ('%s:%s:%s' % (ts, salt, ctx.hexdigest()))
# ---
def test_random_mutation_returns_none_for_invalid_source(bank):
    mutation = random_mutation("this is not python{{{", bank)
    assert mutation is None
# ---
def eqx_max_pool_mean(x):
        pooled = eqx_max_pool(x)
        return pooled.mean()
# ---
def device_address(self):
        if self.device_is_address:
            return self._values['device']
# ---
def test_scheme():

    # does not raise NotImplementedError
    UrlPath('/dev/null').touch()
# ---
def f(x: NamedArray, *args, **kwargs):
        unnamed_x = x.array
        hax_out = hax_mod(x, *args, **kwargs)  # type: ignore
        eqx_out = eqx_mod(unnamed_x, *args, **kwargs)  # type: ignore

        assert jnp.allclose(hax_out.array, eqx_out)
        return hax_out
# ---
def default():
        return CacheOptions()
# ---
def active_at_datetime(self, deadline):
        """Check whether the verification was active at a particular datetime.

        Arguments:
            deadline (datetime): The date at which the verification was active
                (created before and expiration datetime is after today).

        Returns:
            bool

        """
        return (
            self.created_at < deadline and
            self.expiration_datetime > now()
        )
# ---
def eval_model(evaluator: "TaggedEvaluator", model: LmHeadModel, prefix: str = "") -> dict[str, float]:
    with levanter.tracker.capture_time() as time_fn:
        result = evaluator.evaluate(model)
    log_dict = _construct_log_dict(evaluator, result, time_fn(), prefix=prefix)
    return log_dict
# ---
def _return_value():
    return 42
# ---
def __init__(self, **kwargs):
        """
        :param kwargs: passed to torch.nn.ReLU
        """
        super().__init__()
        self.relu = torch.nn.ReLU(**kwargs)
# ---
def raw_array_or_scalar(x: NamedOrNumeric):
    if isinstance(x, NamedArray):
        return x.array
    return x
# ---
def apply_length_weight(H: float, n: int) -> float:
    if n < 8:
        KL = 1.0 - 0.055 * (8 - n)
    elif n > 20:
        KL = 1.0 / (1.0 + 0.027 * (n - 20))
    else:
        KL = 1.0
    return H * KL
# ---
def exists(self):
        '''does the object exist?'''
        if self.deploymentconfig and self.service:
            return True

        return False
# ---
def fake_resize_part_and_fs(dev, start, old, new):
            marker["partition_called"] = True
# ---
def prefix(self) -> str:
        return self._prefix
# ---
def __init__(self, instance: Any, lock: threading.Lock, context):
        self._instance = instance
        self._lock = lock  # Serializes all method calls
        self._context = context
# ---
def __lt__(self, other, /):
        other = self._check_allowed_dtypes(other, "all", "__lt__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.less, self, other, dtype=nxp.bool)
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        return ScaleByAdoptState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)
# ---
def test_exists():
    """`fix.with_fixture` function exists"""
    assert isinstance(with_fixture, FunctionType)
# ---
def tearDown(self):
        super(BaseResourceTestCase, self).tearDown()
        self._service.stop()
        self.session.execute("DROP TABLE IF EXISTS vms;", None)
# ---
import re
def road_rd(street):
  return (re.sub('Road$', 'Rd.', street))
# ---
def stop(self, wait: bool = True):
        self._stop_event.set()
        # I'm getting an error that the thread is threading.current_thread(), which seems impossible
        if self.thread is not None and wait and self.thread != threading.current_thread():
            self.thread.join()
# ---
def test_is_position_token_false_for_base(tok):
    base_token = tok.encode_char("a")
    assert not tok.is_position_token(base_token)
# ---
def lora_state_dict(model: M, prefix: Optional[str] = DEFAULT_DICT_PREFIX) -> StateDict:
    """
    Returns a state dict of the LoRA parameters of the given model without other parameters.
    This method attempts to return a state dict compatible with PEFT's import method.
    """
    state_dict = to_torch_compatible_state_dict(filter_lora_params(model), prefix=prefix)
    return {k: v for k, v in state_dict.items() if v is not None}
# ---
def __init__(self):
    self.softplus = torch.nn.Softplus()
# ---
def find_Average_Of_Cube(n):  
    sum = 0
    for i in range(1, n + 1): 
        sum += i * i * i  
    return round(sum / n, 6)
# ---
def device_address(self):
        if self._values['managed']:
            return None
        return self._values['device_address']
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.field == other.field and self.pattern == other.pattern
# ---
def __init__(self, tableName, tableColumns=[], coreInfo={}):
        self.tableName = tableName
        self.columnsDict = OrderedDict(tableColumns)
        self.dbFile = os.path.join(os.getcwd().replace("python", "metadata"), "libretro.sqlite")
        self.dbFileExists = os.path.isfile(self.dbFile)
        self.coreInfo = coreInfo
# ---
def __init__(self, buffer: LogRingBuffer):
        super().__init__()
        self._buffer = buffer
# ---
def atanh(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in atanh")
    return elemwise(nxp.atanh, x, dtype=x.dtype)
# ---
def test_slice_ref_with_dslice():
    Layer = hax.Axis("layer", 10)
    Head = hax.Axis("head", 4)
    cache_ref = hax.new_ref(hax.zeros((Layer, Head)))

    sub = cache_ref.slice({"layer": hax.ds(3, 3)})
    assert sub.axes == (Layer.resize(3), Head)

    sub[{"layer": 1, "head": slice(None)}] = hax.ones(Head)

    value = cache_ref.value()
    assert jnp.allclose(value[{"layer": 4}].array, jnp.ones((Head.size,), dtype=jnp.float32))
    assert isinstance(sub._prefix[0], type(dslice(0, 1)))
# ---
from itertools import zip_longest, chain, tee
def exchange_elements(lst):
    lst1, lst2 = tee(iter(lst), 2)
    return list(chain.from_iterable(zip_longest(lst[1::2], lst[::2])))
# ---
def send_keys(self, *keys_to_send):
        """Sends keys to current focused element.
        Args:
            keys_to_send: The keys to send.
        """
        self._actions.append(lambda:
            self._driver.switch_to_active_element().send_keys(*keys_to_send))
        return self
# ---
def key_function_with_offset(key_function):
            def wrap(out_key):
                out_coords = out_key[1:]
                offset_in_key = ((offsets.name,) + out_coords,)
                return key_function(out_key) + offset_in_key

            return wrap
# ---
def posts():
    return [make_post() for _ in range(3)]
# ---
def c_identifier(text):
    """Convert input text into an legal identifier in C.

    Example
    -------
    >>> c_identifier("Hello World")
    'HelloWorld'
    >>> c_identifier("Anti-Shake")
    'Antishake'
    """
    if ' ' in text:
        text = camel_case(text)
    text = re.sub(r'\+\d+', lambda x: x.group().replace('+', 'P'), text)
    text = re.sub(r'\-\d+', lambda x: x.group().replace('-', 'N'), text)
    text = replace_punctuations(text)
    return remain_alnum(text)
# ---
def _remove_if_possible(path):
    try:
        os.remove(path)
    except OSError:
        pass
# ---
def grid_size(self) -> GridSize:
        res = self.resolution
        return res[0].shape[0], res[1].shape[0]
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "wte", "position_embeddings": "wpe"}
# ---
def get_volumes(self, mounts=False):
        '''return volume mount information '''
        if mounts:
            return self.get(DeploymentConfig.volume_mounts_path) or []

        return self.get(DeploymentConfig.volumes_path) or []
# ---
def get_Min_Squares(n):
    if n <= 3:
        return n;
    res = n 
    for x in range(1,n + 1):
        temp = x * x;
        if temp > n:
            break
        else:
            res = min(res,1 + get_Min_Squares(n  - temp)) 
    return res;
# ---
def __init__(self, root, cwd, badfn=None, gitignorepaths=None):
        super(gitignorematcher, self).__init__(root, cwd, badfn)
        gitignorepaths = gitignorepaths or []
        self._matcher = pathmatcher.gitignorematcher(root, gitignorepaths)
# ---
def test_diff_3d(shape, n, axis):
    x = np.random.default_rng().integers(0, 10, shape)
    a = xp.asarray(x, chunks=(len(shape) * (5,)))

    assert_array_equal(xp.diff(a, axis=axis, n=n), np.diff(x, axis=axis, n=n))
# ---
def _jit_train_step_fn_no_hook(self):
        return named_jit(
            functools.partial(self._train_step, _no_hooks=True),
            axis_resources=self.parameter_axis_mapping,
            out_axis_resources=self.parameter_axis_mapping,
            donate_args=(True,),
        )
# ---
def post_with_null_author():
    return make_post(with_author=False)
# ---
def default_view_class(self):
		""""""
		if TRACE: print(__name__), self.default_view_class.__doc__

		return BunnyView
# ---
def year_field_is_required_present(self):
        """
        :rtype: bool
        """
        return self._is_element_present(AddMoviePageLocators.YEAR_INPUT_ERROR_LOCATOR)
# ---
def setArtist(self, artist):
        self.__artist = artist
# ---
def _run_event_loop(self):
        """Run the event loop in this thread."""
        self._loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self._loop)

        # Signal that loop is ready
        self._started.set()

        # Run forever until stopped
        self._loop.run_forever()

        # Cleanup
        self._loop.close()
# ---
def test_do_setup(self):
        mox = self.mox
        drv = self._driver

        mox.StubOutWithMock(netapp_nfs.NetAppNFSDriver, 'do_setup')
        mox.StubOutWithMock(drv, '_get_client')
        mox.StubOutWithMock(drv, '_do_custom_setup')

        netapp_nfs.NetAppNFSDriver.do_setup(IgnoreArg())
        drv._get_client()
        drv._do_custom_setup(IgnoreArg())

        mox.ReplayAll()

        drv.do_setup(IsA(context.RequestContext))

        mox.VerifyAll()
# ---
def test_mixed_int_and_selector():
    B, C, V = Axis("batch", 3), Axis("channel", 2), Axis("vocab", 6)
    x = hax.arange((B, C, V))
    idx = hax.arange((B,), dtype=jnp.int32) % V.size
    out = x["channel", 1, "vocab", idx]
    assert out.axes == (B,)
    ref = x.array[:, 1, :][jnp.arange(3), idx.array]
    assert jnp.array_equal(out.array, ref)
# ---
def __pos__(self) -> "NamedArray":  # pragma: no cover
        return haliax.positive(self)
# ---
def poke(self, context):
        c = airflow.hooks.webhdfs_hook.WebHDFSHook(self.webhdfs_conn_id)
        logging.info(
            'Poking for file {self.filepath} '.format(**locals()))
        return c.check_for_path(hdfs_path=self.filepath)
# ---
def weighted_mean_bias(self) -> float:
        return float(
            area_weighted_mean_bias(
                gen=self.gen,
                target=self.target,
                area_weights=self.area_weights,
            )
            .cpu()
            .numpy()
        )
# ---
def to_rows(self, message):
        raise NotImplementedError("Not implemented.")
# ---
def _createNode(self, appName):
        """Create a new root node in the TreeStore model with the name of the
            application.

        Arguments:
        - appName: the name of the TreeStore Node (the same of the application)
        """

        model = self.keyBindingsModel

        myiter = model.append(None)
        model.set_value(myiter, DESCRIP, appName)
        model.set_value(myiter, MODIF, False)

        return myiter
# ---
def dtypes(self, *, device=None, kind=None):
        return nxp.__array_namespace_info__().dtypes(device=device, kind=kind)
# ---
def go(conn, cursor, statement, parameters, context, executemany):
                canary.append((statement, context))
# ---
def upgrade(active_plugins=None, options=None):
    op.add_column('vnf_lcm_subscriptions',
                  sa.Column('tenant_id', sa.String(length=64),
                  nullable=False))

    op.add_column('vnf_lcm_op_occs',
                  sa.Column('tenant_id', sa.String(length=64),
                  nullable=False))
# ---
def delete_floatingip_precommit(self, context, fip_context):
        pass
# ---
def accelerator_type_from_extra(extra: list[str] | None = None) -> AcceleratorType:
    if not extra:
        return AcceleratorType.NONE

    extra_set = set(extra)
    if "tpu" in extra_set:
        return AcceleratorType.TPU
    elif "gpu" in extra_set or "vllm" in extra_set:
        return AcceleratorType.GPU
    elif "cpu" in extra_set:
        return AcceleratorType.CPU
    else:
        return AcceleratorType.NONE
# ---
def test_capabilities():
    capabilities = info.capabilities()
    assert capabilities["boolean indexing"] is False
    assert capabilities["data-dependent shapes"] is False
# ---
def get_driver(dsn_string):
    driver, args, kwargs = parse_dsn(dsn_string)
    return driver(*args, **kwargs)
# ---
def max_sum_subseq(A):
    n = len(A)
    if n == 1:
        return A[0]
    look_up = [None] * n
    look_up[0] = A[0]
    look_up[1] = max(A[0], A[1])
    for i in range(2, n):
        look_up[i] = max(look_up[i - 1], look_up[i - 2] + A[i])
        look_up[i] = max(look_up[i], A[i])
    return look_up[n - 1]
# ---
def dequantize(x, dq_dtype, scale):
    return x.astype(dq_dtype) * jnp.broadcast_to(scale.astype(dq_dtype), x.shape)
# ---
def vocab_size(self) -> int:
        return llama3_tokenizer_vocab_size
# ---
def release_lock(self) -> None:
        """Release the lock if we hold it."""
        try:
            _, lock_data = self._read_lock_with_generation()
            if lock_data and lock_data.worker_id == self.worker_id:
                self.fs.rm(self._lock_path)
                logger.debug("[%s] Released lock", self.worker_id)
        except FileNotFoundError:
            pass
# ---
def playlist_move(self, index1, index2):
        self.command('playlist_move', index1, index2)
# ---
def parse_cookie(self, name):
        if name in self.cookies:
            return self.cookies[name].split("\t")[6]
        else:
            return None
# ---
def test_axis_shapes_force_data_when_other_absorber():
    cfg = MeshConfig(axes={"model": -1})
    ici, _ = cfg.axis_shapes(num_devices=4, num_slices=1)
    # data forced to 1 to keep single absorber
    assert ici["data"] == 1
    # model absorbs the rest
    assert ici["model"] == 4
# ---
def test_get_profiler_not_inited(self):
        profiler.clean()
        self.assertIsNone(profiler.get())
# ---
def resize_embeddings(self, new_size: int, key: PRNGKeyArray | None = None):
        new_weights = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, token_embeddings=new_weights)
# ---
def test_pass_different_length_seq_to_gpt2():
    config = Gpt2Config(
        max_seq_len=64,
        hidden_dim=16,
        num_layers=4,
        num_heads=2,
        gradient_checkpointing=False,
        use_flash_attention=True,
    )
    check_model_works_with_seqlen(Gpt2LMHeadModel, config, 16)
# ---
def test_cancel(self, cr, uid, ids, context=None):
        """ Test whether the move lines are canceled or not.
        @return: True or False
        """
        for pick in self.browse(cr, uid, ids, context=context):
            for move in pick.move_lines:
                if move.state not in ('cancel',):
                    return False
        return True
# ---
def test_vmap_static_args():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Batch, Width, Depth))

    def vmap_fun(x, y):
        return x.sum(Width) if y else x

    selected = hax.vmap(vmap_fun, Batch)(named1, True)

    expected = hax.sum(named1, Width)

    assert jnp.all(jnp.equal(selected.array, expected.array))
    assert selected.axes == expected.axes
# ---
def __len__(self) -> int:
        return len(self._dataloader)
# ---
def items():
        for key, value in d.items():
            if isinstance(value, dict):
                for subkey, subvalue in _flatten_nested_dict(value).items():
                    yield key + "/" + subkey, subvalue
            else:
                yield key, value
# ---
def get(self):
        field, record, value = self.args
        # in order to read the current field's value, remove self from cache
        del record._cache[field]
        # read the current field's value, and update it in cache only
        record._cache[field] = new_value = record[field.name] | value
        return new_value
# ---
def get(self, item):
        """Return the element in the queue identical to `item`.

        Parameters
        ----------
        item :
            The element to search for.

        Returns
        -------
        The element in the queue identical to `item`. If the element
        was not found, None is returned.

        """
        for _, element in self._queue:
            if item == element:
                return element
        return None
# ---
def set_children(self, children):
        self.children = children
# ---
def test_run_streaming_with_retry_calls_on_line_callback():
    """run_streaming_with_retry calls on_line callback for each line."""
    expected_lines = ["line one", "line two", "line three"]
    conn = MagicMock()
    conn.run_streaming.return_value = make_fake_popen(expected_lines)
    lines_received: list[str] = []
    run_streaming_with_retry(conn, "bootstrap script", on_line=lines_received.append)
    assert lines_received == expected_lines
# ---
def test_or_one_true(self):
        expr = (col("a") > 0) | (col("b") > 0)
        assert expr.evaluate({"a": 1, "b": -1}) is True
        assert expr.evaluate({"a": -1, "b": 1}) is True
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' and '.join(str(r) for r in self.rules)
# ---
def __init__(self, string, rind=None):
        CmdText.__init__(self)
        self.insert(string)

        if (rind is not None):
            self.response = rind
# ---
def _format_multiplier_label(mult: float) -> str:
    s = f"{mult:.6g}"
    s = s.rstrip("0").rstrip(".") if "." in s else s
    return s.replace(".", "_")
# ---
def main(): 
    app = QtGui.QApplication(sys.argv)
    window=PreprocessDialog()

    window.show()

    sys.exit(app.exec_())
# ---
def current(self, value):
        self.voltage = self.resistance * value
# ---
def start(
        self,
        *,
        model_name_or_path: str,
        host: str,
        port: int | None,
        timeout_seconds: int,
        extra_cli_args: list[str] | None,
    ) -> VllmServerHandle:
        return _start_vllm_native_server(
            model_name_or_path=model_name_or_path,
            host=host,
            port=port,
            timeout_seconds=timeout_seconds,
            extra_cli_args=extra_cli_args,
        )
# ---
def format_directions(directions):
    return format_line(prefix='directions'.rjust(RJUST), values=directions)
# ---
def __eq__(self, other):
        return (isinstance(other, EntityRef) and
                self.entity_id == other.entity_id)
# ---
def test_unicode_and_str_mixture(self):
        f = capture.CaptureIO()
        if sys.version_info >= (3, 0):
            f.write("\u00f6")
            pytest.raises(TypeError, "f.write(bytes('hello', 'UTF-8'))")
        else:
            f.write(unicode("\u00f6", 'UTF-8'))
            f.write("hello")  # bytes
            s = f.getvalue()
            f.close()
            assert isinstance(s, unicode)
# ---
def batch_axis_at_step(self, step: int):
        size = self.rounded_batch_size_at_step(step)

        return hax.Axis(self.batch_axis_name, size)
# ---
def __setattr__(self, attr, value):
		# Do not preserve the value in DBRow! Use the save method to save.
		if self.initialized and attr in self.structure:
			self._set_value(attr, value)
		return super(DBRow, self).__setattr__(attr, value)
# ---
def get_batch(self, indices: Sequence[int] | np.ndarray) -> Sequence[T_co]:
        pass
# ---
def test_routing_binary_job():
    """Binary entrypoint routes to _launch_binary_job."""
    request = JobRequest(
        name="binary-job",
        entrypoint=Entrypoint.from_binary("echo", ["hello"]),
        resources=ResourceConfig(device=CpuConfig()),
    )
    assert request.entrypoint.binary_entrypoint is not None
    assert not isinstance(request.resources.device, TpuConfig)
# ---
def __get__(self, obj, cls=None):
        if not obj:
            return self
        return getattr(obj, self.slot, None)
# ---
def asCustomKW(self):
        """ @rtype: CustomKW """
        impl_type = EnkfNode.cNamespace().get_impl_type(self)
        assert impl_type == ErtImplType.CUSTOM_KW

        return CustomKW.createCReference(self.valuePointer(), self)
# ---
def skip_if_no_soundlibs(f):
    return pytest.mark.skipif(not has_soundlibs(), reason="soundfile/librosa not installed")(f)
# ---
def slow_create(tags=None):
            time.sleep(0.2)  # Simulate slow VM creation
            return original_create(tags)
# ---
def map_svg():
    return make_map(request, format='svg')
# ---
def get_method(block: gemmi.cif.Block) -> str:
    """Get the method from a gemmi structure.

    Parameters
    ----------
    block : gemmi.cif.Block
        The block to process.

    Returns
    -------
    str
        The method.

    """
    method = ""
    method_key = "_exptl.method"
    with contextlib.suppress(Exception):
        methods = block.find([method_key])
        method = ",".join([m.str(0).lower() for m in methods])

    return method
# ---
def decrement(self, stats, sample_rate=1):
        """
        Decrements one or more stats counters
        """
        self.update_stats(stats, -1, sample_rate)
# ---
def testAssignMultiple(self):
    self.assertEqual((0, 'baz baz\n'), _GrumpRun(textwrap.dedent("""\
        foo = bar = 'baz'
        print foo, bar""")))
# ---
def setup(self, memcached, memcached_cluster):
        self.storage_url = "memcached://localhost:22122"
# ---
def on_key_press(self, symbol, modifiers):
        """Default on_key_press handler."""
        if symbol == key.ESCAPE and not (modifiers & ~(key.MOD_NUMLOCK | 
                                                       key.MOD_CAPSLOCK | 
                                                       key.MOD_SCROLLLOCK)):
            self.dispatch_event('on_close')
# ---
def cross_entropy_loss(
    logits: NamedArray,
    Label: AxisSelector,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: AxisSelection = ...,
) -> NamedArray: ...
# ---
def argsort(self, axis: AxisSelector | None, *, stable: bool = False) -> "NamedArray":  # pragma: no cover
        return haliax.argsort(self, axis=axis, stable=stable)
# ---
def main():
    pass
# ---
def testDeleteNonexistentLocal(self):
    self.assertRaisesRegexp(
        util.ParseError, 'cannot delete nonexistent local',
        _ParseAndVisit, 'def foo():\n  del bar')
# ---
def test_submit_callable_failure_raises(client: LocalClient):
    handle = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    with pytest.raises(RuntimeError, match="intentional failure"):
        handle.wait(raise_on_failure=True)
# ---
def grad_fn(x_in, w_in, y_in):
        return jax.grad(loss_fn, argnums=(0, 1))(x_in, w_in, y_in)
# ---
def _load_files(self, dataset_code):
        url = "http://www.bea.gov/national/nipaweb/GetCSV.asp?GetWhat=SS_Data/Section1All_xls.zip&Section=2"
        self.register_url(url, 
                          self.DATASETS[dataset_code]["filepath"])
# ---
def test_named_ref_jit_plumbing():
    X = hax.Axis("x", 5)
    ref = hax.new_ref(hax.zeros(X))

    @jax.jit
    def write_and_read(ref):
        ref[{"x": 1}] = 4.2
        return ref[{"x": 1}]

    out = write_and_read(ref)
    assert isinstance(out, hax.NamedArray)
    assert out.axes == ()
    assert pytest.approx(out.array.item()) == 4.2
    assert jnp.allclose(ref.value()[{"x": 1}].array, jnp.asarray(4.2))
# ---
def __init__(self, args):
        self.args = args
        self.setattributes(self.args)
# ---
def test_infer_vertical_cell_extent_missing(input_data):
    ds = input_data
    ds = ds.drop("zos")
# ---
def login_required(view):
    @wraps(view)
    def inner(method, notebook, data, *args, **kwargs):
        if not is_logged():
            return views.render('login_form', notebook)
        return view(method, notebook, data, *args, **kwargs)
    return inner
# ---
def GetIntValue( variable ):
  return int( vim.eval( variable ) )
# ---
def _add_block(self, block, batch=None, check_stale=True) -> (bool, bool):
        self.trigger_miner = False

        block_size_limit = self.get_block_size_limit(block)
        if block_size_limit and block.size > block_size_limit:
            logger.info('Block Size greater than threshold limit %s > %s', block.size, block_size_limit)
            return False, False

        return self._try_branch_add_block(block, batch, check_stale)
# ---
def name(self) -> str:
        return "single-threaded"
# ---
def get_job_tasks(self, job_id: JobName) -> list[ControllerTask]:
        """Get all tasks for a job."""
        with self._lock:
            task_ids = self._tasks_by_job.get(job_id, [])
            return [self._tasks[tid] for tid in task_ids if tid in self._tasks]
# ---
R = 3
C = 3
def min_cost(cost, m, n): 
	tc = [[0 for x in range(C)] for x in range(R)] 
	tc[0][0] = cost[0][0] 
	for i in range(1, m+1): 
		tc[i][0] = tc[i-1][0] + cost[i][0] 
	for j in range(1, n+1): 
		tc[0][j] = tc[0][j-1] + cost[0][j] 
	for i in range(1, m+1): 
		for j in range(1, n+1): 
			tc[i][j] = min(tc[i-1][j-1], tc[i-1][j], tc[i][j-1]) + cost[i][j] 
	return tc[m][n]
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.some_table.insert(),
            [
                {"id": 1, "x": 1, "y": 2},
                {"id": 2, "x": 2, "y": 3},
                {"id": 3, "x": 3, "y": 4},
                {"id": 4, "x": 4, "y": 5},
            ],
        )
# ---
def execute(conn, *args, **kw):
            canary.append('execute')
# ---
def create_op(self, op_type, inputs, data_types, **kwargs):
    for i, x in enumerate(inputs):
      if isinstance(x, ops.EagerTensor) or x.graph is not self:
        inputs[i] = self.capture(x)
    return super(_FuncGraph, self).create_op(op_type, inputs, data_types,
                                             **kwargs)
# ---
def _convert_example(inputs: AudioTextDict) -> "AudioTextExample":
            tokens = hax.named(inputs["input_ids"], self.TextPos)
            audio_features = hax.named(inputs["input_features"], self.AudioPos)
            out = AudioTextExample.init(audio_features, tokens, ignore_id=self.ignore_id)
            out = jax.lax.with_sharding_constraint(out, sharding)
            return out
# ---
def action_traceability(self, cr, uid, ids, context=None):
        """ It traces the information of a product
        @param self: The object pointer.
        @param cr: A database cursor
        @param uid: ID of the user currently logged in
        @param ids: List of IDs selected
        @param context: A standard dictionary
        @return: A dictionary of values
        """
        value=self.pool.get('action.traceability').action_traceability(cr,uid,ids,context)
        return value
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        """Convert to proto for RPC APIs."""

        return vm_pb2.SliceInfo(
            slice_id=self._group_id,
            scale_group=self._scale_group,
            created_at=self._created_at.to_proto(),
            vms=[vm.info for vm in self._vms],
        )
# ---
def chip_count(self) -> int:
        return get_tpu_topology(self.variant).chip_count
# ---
def logaddexp2(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logaddexp2(x1, x2)
# ---
def output_code(self, m):
        text = m.group(2)
        return self.renderer.codespan(text)
# ---
def order_clause(self):
        order_strings = []
        for sort in self._sql_sorts():
            order = sort.order_clause()
            order_strings.append(order)

        return ", ".join(order_strings)
# ---
def count(self, eventtype):
        """Tells how many events have been counted of the specified type

        Args:
            eventtype (:obj:`baroque.entities.eventtype.EventType`): the type of events to be counted

        Returns:
            int

        """
        return self.events_count_by_type.get(type(eventtype), 0)
# ---
def test_profiler_get_parent_id(self, mock_generate_uuid):
        mock_generate_uuid.return_value = "42"
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        prof.start("test")
        self.assertEqual(prof.get_parent_id(), "2")
# ---
def __init__(self, *args, **kwargs):
        self.args = args
        self.kwargs = kwargs
# ---
import re
def removezero_ip(ip):
 string = re.sub('\.[0]*', '.', ip)
 return string
# ---
def key():
    return jax.random.PRNGKey(42)
# ---
def axis_size(ax: AxisSpec) -> int:
    """
    Returns the size of the axis or the product of the sizes of the axes in the axis spec
    """

    if isinstance(ax, Axis):
        return ax.size
    elif isinstance(ax, Mapping):
        return prod(ax.values())
    else:
        return prod(axis.size for axis in ax)
# ---
def compute_collinear_mask(v1, v2):
    norm1 = torch.norm(v1, dim=1, keepdim=True)
    norm2 = torch.norm(v2, dim=1, keepdim=True)
    v1 = v1 / (norm1 + 1e-6)
    v2 = v2 / (norm2 + 1e-6)
    mask_angle = torch.abs(torch.sum(v1 * v2, dim=1)) < 0.9063
    mask_overlap1 = norm1.reshape(-1) > 1e-2
    mask_overlap2 = norm2.reshape(-1) > 1e-2
    return mask_angle & mask_overlap1 & mask_overlap2
# ---
def makefile(self, mode='r', bufsize=-1):
        self._makefile_refs += 1
        return socket._fileobject(self, mode, bufsize, close=True)
# ---
def __init__(self, config: InferenceReplConfig):
        self.config = config
        self.server = None
        self.model_name = None

        self.commands: Dict[str, Callable] = {
            "load": self.load,
            "unload": self.unload,
            "chat": self.chat,
            "complete": self.complete,
            "batch": self.batch,
            "help": self.show_help,
            "serve": self.serve,
        }
# ---


def incr_list(l: list):
    """Return list with elements incremented by 1.
    >>> incr_list([1, 2, 3])
    [2, 3, 4]
    >>> incr_list([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [6, 4, 6, 3, 4, 4, 10, 1, 124]
    """
    return [(e + 1) for e in l]
# ---
def __init__(
        self,
        sigma_data: float,
        token_s=384,
        dim_fourier=256,
    ):
        super().__init__()
        self.sigma_data = sigma_data

        self.norm_next = nn.LayerNorm(2 * token_s)
        self.fourier_embed = FourierEmbedding(dim_fourier)
        self.norm_fourier = nn.LayerNorm(dim_fourier)
        self.transition_block = ConditionedTransitionBlock(
            2 * token_s, 2 * token_s + dim_fourier
        )
# ---
def format_headrooms(headrooms):
    return format_line(prefix='headrooms'.rjust(RJUST), values=headrooms)
# ---
def status(self) -> JobStatus: ...
# ---
def matrix_transpose(x, /):
    if x.ndim < 2:
        raise ValueError("x must be at least 2-dimensional for matrix_transpose")

    from cubed.array_api.manipulation_functions import permute_dims

    axes = list(range(x.ndim))
    axes[-1], axes[-2] = axes[-2], axes[-1]  # swap last two axes
    return permute_dims(x, axes)
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[List[int]]:
        # parse the shard name to get the shard number
        shard_num = int(shard_name.split("_")[1])
        return ([shard_num * 10 + i] * 10 for i in range(row, 10))
# ---
def jbor_to_str(val):
    ans = get_job_from_jbor(val) + ':' + get_field_from_jbor(val)
    index = get_index_from_jbor(val)
    if index is not None:
        ans += "." + str(index)
    return ans
# ---
def load(cache_dir: str, exemplar: T, options: Optional["CacheMetadata"] = None) -> "TreeCache":
        logger.info(f"Loading cache from {cache_dir}")
        ledger = CacheLedger.load(cache_dir, options)

        if not ledger.is_finished:
            raise FileNotFoundError(f"Cache at {cache_dir} is not finished. Use build_or_load to build it.")
        return TreeCache(cache_dir, exemplar, ledger)
# ---
def create_actor(self) -> ActorHandle:
        return SliceActor.options(resources={f"TPU-{self._tpu_type}-head": 1}).remote()
# ---
def __init__(self, index):
        self.index = index
# ---
def apply_chat_template(self, messages, tokenize=True, add_generation_prompt=True):
        """Simple chat template support."""
        prompt = "\n".join([m["content"] for m in messages])

        if tokenize:
            return self.encode(prompt)
        return prompt
# ---

def add_elements(arr, k):
    """
    Given a non-empty array of integers arr and an integer k, return
    the sum of the elements with at most two digits from the first k elements of arr.

    Example:

        Input: arr = [111,21,3,4000,5,6,7,8,9], k = 4
        Output: 24 # sum of 21 + 3

    Constraints:
        1. 1 <= len(arr) <= 100
        2. 1 <= k <= len(arr)
    """
    return sum(elem for elem in arr[:k] if len(str(elem)) <= 2)
# ---
def merge_translations(blddir, sources, langs):
    for lang in langs:
        subprocess.call([
            'itstool', '-m', os.path.join(blddir, lang, lang + '.gmo'),
            '-o', os.path.join(blddir, lang)
        ] + sources)
# ---
def dot(
        self,
        axis: AxisSelection | None,
        *b,
        precision: PrecisionLike = None,
        dot_general=jax.lax.dot_general,
    ) -> "NamedArray": ...
# ---
def convert_to_read(self, value, use_name_get=True):
        return "%s,%s" % (value._name, value.id) if value else False
# ---
def meets_length_threshold(record: dict) -> bool:
        """Check if document meets minimum length requirement."""
        if config.min_length is None:
            return True
        length = record.get("metadata", {}).get("length", 0)
        return length >= config.min_length
# ---
def vortex_file(tmp_path):
    """Create a test vortex file with sample data."""
    records = [{"id": i, "name": f"item_{i}", "score": i * 10} for i in range(100)]
    path = tmp_path / "test.vortex"
    write_vortex_file(records, str(path))
    return path
# ---
def find_lucas(n): 
	if (n == 0): 
		return 2
	if (n == 1): 
		return 1
	return find_lucas(n - 1) + find_lucas(n - 2)
# ---
def __post_init__(self):
        self.prognostic = self.prognostic.bool()
        self.boundary = self.boundary.bool()
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        return DefaultRotaryEmbeddings(HeadSize, self)
# ---
def max_sum(arr, n): 
	MSIBS = arr[:] 
	for i in range(n): 
		for j in range(0, i): 
			if arr[i] > arr[j] and MSIBS[i] < MSIBS[j] + arr[i]: 
				MSIBS[i] = MSIBS[j] + arr[i] 
	MSDBS = arr[:] 
	for i in range(1, n + 1): 
		for j in range(1, i): 
			if arr[-i] > arr[-j] and MSDBS[-i] < MSDBS[-j] + arr[-i]: 
				MSDBS[-i] = MSDBS[-j] + arr[-i] 
	max_sum = float("-Inf") 
	for i, j, k in zip(MSIBS, MSDBS, arr): 
		max_sum = max(max_sum, i + j - k) 
	return max_sum
# ---
def __init__(
        self,
        coordinator: OctoprintDataUpdateCoordinator,
        sensor_type: str,
        device_id: str,
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator)
        self._device_id = device_id
        self._attr_name = f"OctoPrint {sensor_type}"
        self._attr_unique_id = f"{sensor_type}-{device_id}"
# ---
def acosh(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in acosh")
    return elemwise(nxp.acosh, x, dtype=x.dtype)
# ---
def first(arr,x,n): 
    low = 0
    high = n - 1
    res = -1  
    while (low <= high):
        mid = (low + high) // 2 
        if arr[mid] > x:
            high = mid - 1
        elif arr[mid] < x:
            low = mid + 1
        else:
            res = mid
            high = mid - 1
    return res
# ---
def client(self) -> IrisClient:
        """IrisClient for this cluster."""
        if self._rpc_client is None:
            self._rpc_client = IrisClient.remote(
                self.controller_url,
                workspace=self._workspace,
            )
        return self._rpc_client
# ---
def fused_func_generator(*args):
        # args are grouped appropriately so they can be called by each predecessor function
        func_args = [
            apply_blockwise_func(pf, iterable_input_blocks[i], *a)
            for i, (pf, a) in enumerate(zip(predecessor_functions, args, strict=True))
        ]
        yield from function(*func_args)
# ---
def __init__(self, allow=None, disallow=None, secure=True, *args, **kwargs):
		super(TemplateField, self).__init__(*args, **kwargs)
		self.validators.append(TemplateValidator(allow, disallow, secure))
# ---
def python_mark_exact_dups_documents(
    batch: pa.RecordBatch,
    text_col: str,
    id_col: str,
    dup_map: dict[str, Any],
    attribute_name: str,
) -> list[dict[str, Any]]:
    results = []
    for record in batch.to_pylist():
        text, record_id = record[text_col], record[id_col]
        h = _str_hash_legacy(text)
        is_dup = h in dup_map and dup_map[h]["canonical"] != record_id
        results.append({"id": record_id, "attributes": {attribute_name: is_dup}})
    return results
# ---
def fmax(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.fmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fmax.html)
    """
    return jnp.fmax(x1, x2)
# ---
def PlotFit(data,BaseName):
    fig = Example_Data.PlotHistograms(data)
    fig.savefig(BaseName + "_Histogram.png")
    fig = Example_Data.PlotLifetimesAndFit(data)
    fig.savefig(BaseName + "_Lifetimes.png")
# ---
def find_checkpoint_root(path):
        """Find the checkpoint root by looking for metadata.json"""
        current = path
        while current and current != os.path.dirname(current):
            metadata_path = os.path.join(current, "metadata.json")
            if fsspec_utils.exists(metadata_path):
                return current
            current = os.path.dirname(current)
        return path
# ---
def __contains__(self, item):
        try:
            self._queue.index(item)
            return True
        except Exception:
            return False
# ---
def rollouts_to_training_batch(rollouts, max_tokens=32, pad_token_id=0):
    """Helper function to convert rollouts to training batch for testing."""
    return train_batch.create_training_batch_from_rollouts(rollouts, max_tokens, pad_token_id)
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"transformer": None, "embeddings": None}
# ---
def query(question, default=None):
    "Ask the user a question and return the response"
    while True:
        if default:
            sys.stdout.write("{} [{}]: ".format(question, default))
        else:
            sys.stdout.write("%s: " % question)
        answer = sys.stdin.readline().strip().replace(" ", "_")
        if answer == "":
            if default:
                return default
        else:
            return answer
# ---
def sum_of_alternates(test_tuple):
  sum1 = 0
  sum2 = 0
  for idx, ele in enumerate(test_tuple):
    if idx % 2:
      sum1 += ele
    else:
      sum2 += ele
  return ((sum1),(sum2))
# ---
def address(self) -> str:
        return self._address or self.vm_id
# ---
def fifth_Power_Sum(n) : 
    sm = 0 
    for i in range(1,n+1) : 
        sm = sm + (i*i*i*i*i) 
    return sm
# ---
def test_create_controller_raises_on_missing_config(gcp_config: config_pb2.IrisClusterConfig):
    """create_controller raises ValueError when no oneof is set."""
    config = config_pb2.IrisClusterConfig()
    config.CopyFrom(gcp_config)
    config.controller.ClearField("gcp")

    with pytest.raises(ValueError, match="No controller config specified"):
        create_controller_vm(config)
# ---
from collections import Counter
def add_dict(d1,d2):
   add_dict = Counter(d1) + Counter(d2)
   return add_dict
# ---
def __eq__(self, other):
        return self.id == other.id
# ---
def check_subset_list(list1, list2): 
    l1, l2 = list1[0], list2[0] 
    exist = True
    for i in list2: 
        if i not in list1: 
            exist = False
    return exist
# ---
def tuple_int_str(tuple_str):
    result = tuple((int(x[0]), int(x[1])) for x in tuple_str)
    return result
# ---
def foo(ref, xs):
        def scan_fn(_, x):
            ref_slice = ref.slice({"x": x})
            ref_slice[...] = (x * x).astype(ref_slice.dtype)
            return None, x * 2

        return hax.scan(scan_fn, X)(None, xs)[1]
# ---
def playlist_clear(self):
        self.command('playlist_clear')
# ---
def _get_gemma_config(use_flash=False, num_kv_heads=4, seq_len=128) -> GemmaConfig:
    return GemmaConfig(
        max_seq_len=seq_len,
        hidden_dim=16,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
        head_dim=4,
    )
# ---
def assign_elements(test_list):
  res = dict()
  for key, val in test_list:
    res.setdefault(val, [])
    res.setdefault(key, []).append(val)
  return (res)
# ---
def join(self, timeout: Duration | None = None) -> None:
        self._thread.join(timeout=timeout.to_seconds() if timeout is not None else None)
# ---
def __init__(self, producer_fn: Callable[[], Iterator[Ex] | AsyncIterator[Ex]], max_capacity: int | None):
        super().__init__(producer_fn, max_capacity)
# ---
def sample(self, logits):
    noise = self._sampling_noise()
    noise = noise[: logits.shape[0], :]
    logits = logits + noise.to(
      dtype=logits.dtype, device=logits.device)
    hard_sample = self._hard_sample(logits)
    soft_sample = self._soft_sample(logits)
    return soft_sample + (hard_sample - soft_sample).detach()
# ---
def create_futures_func(input, **kwargs):
        return [
            (i, asyncio.ensure_future(function.remote.aio(i, **kwargs))) for i in input
        ]
# ---
def array_memory(dtype: T_DType, shape: T_Shape) -> int:
    """Calculate the amount of memory in bytes that an array uses."""
    return np.dtype(dtype).itemsize * prod(shape)
# ---
def test_type(self):
        rset = event.ResultSet()
        with pytest.raises(NotImplementedError):
            rset.extend([])
        with pytest.raises(NotImplementedError):
            rset.extend(False)
# ---
def get_code(self, indices=None):
        cells = self.get_code_cells()
        if indices is None:
            indices = list(range(len(cells)))
        return '\n\n\n'.join([cells[i] for i in indices])
# ---
def cleanup(cache_dir: str):
    """Clean up cached bundles, venvs, and images."""
    cache_path = Path(cache_dir).expanduser()
    if cache_path.exists():
        shutil.rmtree(cache_path)
        click.echo(f"Removed cache directory: {cache_path}")
    else:
        click.echo(f"Cache directory does not exist: {cache_path}")
# ---
def activation_genetic_modification(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'CRISPRa',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def get_lines(string):
    """
    Return list of lines extracted from string.
    """
    line_list = string.split('\n')

    new_list = []
    for l in line_list:
        new_list += [l[i*LINEWIDTH:(i+1)*LINEWIDTH] for i in range(len(l) // LINEWIDTH + 1)]

    return new_list
# ---
def append(self, record: BufferedLogRecord) -> None:
        with self._lock:
            self._buffer.append(record)
# ---
def run_test(function, input, retries, timeout=10, use_backups=False):
    outputs = set()
    with RetryingFunctionExecutor(LocalhostExecutor()) as executor:
        for output in map_unordered(
            executor,
            [function],
            [input],
            ["group0"],
            timeout=timeout,
            retries=retries,
            use_backups=use_backups,
        ):
            outputs.add(output)
    return outputs
# ---
def print_locked_workflow_note():
    print_field('Note',
                'This workflow has an explicit input specification (i.e. it is locked), and as such stage inputs cannot be modified at run-time.')
# ---
def tmpfile(testdir):
    f = testdir.makepyfile("").open('wb+')
    yield f
    if not f.closed:
        f.close()
# ---
def tree_unflatten(treedef, leaves):
    """
    Provided for consistency with tree_flatten.
    """
    return jax.tree_util.tree_unflatten(treedef, leaves)
# ---
def peek(self) -> T_co | None:
        """View the next available item without acquiring a lease."""
        ...
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "google/gemma-2b"
    ) -> HFCheckpointConverter["GemmaConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self,
            reference_checkpoint=ref_checkpoint,
            trust_remote_code=True,
            HfConfigClass=HfGemmaConfig,
        )
# ---
def send_js(filename):
    return static_file(filename, root='js')
# ---
def bmarks():
    return_data = do_edit()
    return return_data
# ---
def _create_blob(self):
        blob_name = self._get_blob_reference()
        blob = self.bsc.get_blob_client(self.container_name, blob_name)
        blob.upload_blob(b'')
        return blob
# ---
def __post_init__(self):
        if self.wandb is not None:
            warnings.warn(
                "wandb is deprecated. use tracker with type wandb instead",
                DeprecationWarning,
            )
            self.tracker = self.wandb
# ---
def service_with_autoscaler(state, scheduler, mock_autoscaler):
    """Service with autoscaler enabled for tests."""
    controller_mock = Mock()
    controller_mock.wake = Mock()
    controller_mock.task_schedule_status = scheduler.task_schedule_status
    controller_mock.autoscaler = mock_autoscaler  # Enable autoscaler
    return ControllerServiceImpl(state, controller_mock, bundle_prefix="file:///tmp/iris-test-bundles")
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        pass
# ---
def __repr__(self):
        return ("{0.__class__.__name__}({0.pattern!r}, {0.fields!r}, "
                "{0.query_class.__name__})".format(self))
# ---
def extent(self):
        from mapproxy.layer import MapExtent

        return MapExtent(self.bbox, self.srs)
# ---
def select_leaf(leaf):
            if isinstance(leaf, haliax.NamedArray):
                if haliax.selects_axis(leaf.axes, self.Block):
                    return leaf[self.Block, index]
                else:
                    return leaf
            if is_jax_or_hax_array_like(leaf):
                if getattr(leaf, "shape", ()) and leaf.shape[0] == self.Block.size:
                    return leaf[index]
                return leaf
            return leaf
# ---
def _get_mesh() -> Mesh | None:
    # Backward-compatible helper for callers that expect a concrete or abstract mesh.
    return _resolve_mesh()
# ---
def writemypid(pidfile):
    pid = str(os.getpid())
    with open(pidfile, 'w') as f:
        f.write(pid)
    f.close
# ---
def test_ring_buffer_query_limit(ring_buffer):
    """Query respects limit parameter, returning most recent records."""
    for i in range(10):
        ring_buffer.append(BufferedLogRecord(float(i), "INFO", "test", f"msg-{i}"))
    results = ring_buffer.query(limit=3)
    assert len(results) == 3
    assert results[0].message == "msg-7"
# ---
def remove_list_range(list1, leftrange, rigthrange):
   result = [i for i in list1 if (min(i)>=leftrange and max(i)<=rigthrange)]
   return result
# ---
def hard_swish(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_swish, a)
# ---
def days_till_due(self):
        """
        Returns the number of days till the due date. Returns a negative number
        of days when the due date is in the past.
        Returns 0 when the task has no due date.
        """
        due = self.due_date()
        if due:
            diff = due - date.today()
            return diff.days
        return 0
# ---
def __init__(self, run: Optional[TrackioRun] = None):
        import trackio

        if run is None:
            logger.warning("Trackio run is not initialized. Initializing a new run.")
            self.run = trackio.init(project="levanter")
        else:
            self.run = run
# ---
def test_lambda_output_shape(self):
    l = keras.layers.Lambda(lambda x: x + 1, output_shape=(1, 1))
    l(keras.backend.variable(np.ones((1, 1))))
    self.assertEqual((1, 1), l.get_config()['output_shape'])
# ---
def writestr(self):
        '''
        generate the midi data header and convert the list of
        midi_track objects in self_tracks into midi data and return it as a string_
        '''
        midi_str = self.write_m_thd_str()
        for trk in self.tracks:
            midi_str = midi_str + trk.get_bytes()
        return midi_str
# ---
def make_job_request():
    """Create a minimal LaunchJobRequest for testing."""

    def _make(name: str = "test-job") -> cluster_pb2.Controller.LaunchJobRequest:
        return cluster_pb2.Controller.LaunchJobRequest(
            name=name,
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=1024**3),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )

    return _make
# ---
def __post_init__(self):
        self.thread = None
        self.stop_flag = threading.Event()
# ---
def unsliced(self) -> "NamedRef":
        """Return a view of the original reference without staged selectors."""
        full_prefix = tuple(slice(None) for _ in self._axes)
        if self._prefix == full_prefix:
            return self
        return NamedRef(self._ref, self._axes, full_prefix)
# ---
def run(cmd: list, **kwargs) -> subprocess.CompletedProcess:
    """Run command with logging."""
    logging.info(f"Running: {' '.join(cmd)}")
    return subprocess.run(cmd, **kwargs)
# ---
def _mesh_axis_totals(self) -> Dict[str, int]:
        ici, dcn = self.mesh.axis_shapes(jax.device_count(), self.num_slices)
        return {name: ici.get(name, 1) * dcn.get(name, 1) for name in set(ici) | set(dcn)}
# ---
def test_cumulative_sum_2d(axis):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2))
    b = xp.cumulative_sum(a, axis=axis)
    assert_array_equal(
        b.compute(),
        np.cumulative_sum(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), axis=axis),
    )
# ---
def _make_toy_inputs():
    key = jax.random.PRNGKey(0)
    key_x, key_w, key_y = jax.random.split(key, 3)

    x = jax.random.normal(key_x, (2, 3, 4), dtype=jnp.float32)
    w = jax.random.normal(key_w, (4, 5), dtype=jnp.float32)
    y = jax.random.randint(key_y, (2, 3), 0, 5, dtype=jnp.int32)
    return x, w, y
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        pass
# ---
def get_all_pool_members(self) -> list[ActorPoolMember]:
        return self._actor_pool.copy()
# ---
def __init__(self, input_channels, hidden_channels, output_channels):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(
            input_channels, hidden_channels, kernel_size=3, padding=1
        )
        self.bn1 = nn.BatchNorm2d(hidden_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(
            hidden_channels, output_channels, kernel_size=3, padding=1
        )
        self.bn2 = nn.BatchNorm2d(output_channels)
# ---
def test_tile(spec, repetitions):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.tile(a, repetitions)
    assert_array_equal(
        b.compute(),
        np.tile(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), repetitions),
    )
# ---
def run_command(cmd, timeout=5):
    """Run a command and return output, or None on failure."""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout,
            shell=True
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except (subprocess.TimeoutExpired, OSError, Exception):
        pass
    return None
# ---
def _display_login_form(request, error_message=''):
    request.session.set_test_cookie()
    return render_to_response('admin/login.html', {
        'title': _('Log in'),
        'app_path': request.get_full_path(),
        'error_message': error_message
    }, context_instance=template.RequestContext(request))
# ---
def apply_blockwise_func(func, is_iterable, *args):
    if is_iterable is False:
        ret = func(*args)
    else:
        # More than one input block is being read from this group of args to primitive op.
        # Note that it is important that a list is not returned to avoid materializing all
        # array blocks at once.
        ret = map(lambda item: func(*item), zip(*args, strict=True))
    return ret
# ---
def test_named_ref_is_pytree_leaf():
    X = hax.Axis("x", 3)
    ref = hax.new_ref(hax.zeros(X))

    leaves = jax.tree_util.tree_leaves(ref)
    assert len(leaves) == 1
    assert leaves[0] is ref._ref

    structure = jax.tree_util.tree_structure(ref)
    rebuilt = jax.tree_util.tree_unflatten(structure, leaves)

    assert isinstance(rebuilt, hax.NamedRef)
    assert rebuilt.axes == ref.axes
    assert rebuilt._prefix == ref._prefix
# ---
def go(*args, **kw):
                canary1.append(name)
# ---
def chip_count(self) -> int:
        """Total number of TPU chips."""
        return get_tpu_topology(self.variant).chip_count
# ---
def __init__(self, endpoints: dict[str, str | list[str]]):
        """Initialize with a mapping of actor names to URLs."""
        self._endpoints: dict[str, list[str]] = {}
        for name, urls in endpoints.items():
            if isinstance(urls, str):
                self._endpoints[name] = [urls]
            else:
                self._endpoints[name] = list(urls)
# ---
def forward(self, x):
        return self.maxpool(x)
# ---
def __iter__(self):
        return self
# ---
def intersects(self, bbox, srs):
        return any(c.intersects(bbox, srs) for c in self.coverages)
# ---
def create_bad_vbd(vm_ref, vdi_ref):
            vbd_rec = {'VM': vm_ref,
               'VDI': vdi_ref,
               'userdevice': 'fake',
               'currently_attached': False}
            vbd_ref = xenapi_fake._create_object('VBD', vbd_rec)
            xenapi_fake.after_VBD_create(vbd_ref, vbd_rec)
            return vbd_ref
# ---
def update(self):
        """Records a new data point at the current time.  This method
        is called automatically when the window buffer is flipped.
        """
        from time import time
        t = time()
        self.count += 1
        self.time += t - self.last_time
        self.last_time = t

        if self.time >= self.update_period:
            self.set_fps(self.count / self.update_period)
            self.time %= self.update_period
            self.count = 0
# ---
def peak_projected_mem(primitive_ops):
    """Calculate the peak projected memory for running a series of primitive ops
    and retaining their return values in memory."""
    memory_modeller = MemoryModeller()
    for p in primitive_ops:
        if p is None:
            continue
        memory_modeller.allocate(p.projected_mem)
        chunkmem = chunk_memory(p.target_array)
        memory_modeller.free(p.projected_mem - chunkmem)
    return memory_modeller.peak_mem
# ---
def skip_in_ci(fn_or_msg):
    if isinstance(fn_or_msg, str):

        def decorator(fn):
            return pytest.mark.skipif("CI" in os.environ, reason=fn_or_msg)(fn)

        return decorator

    return pytest.mark.skipif("CI" in os.environ, reason="skipped in CI")(fn_or_msg)
# ---
def _call_all(self, fun, *args):
        '''
        Call the given function on all backend keys
        '''
        for kback in self.keys:
            print(kback)
            getattr(self.keys[kback], fun)(*args)
# ---
def __getitem__(self, step: int) -> Example:
        """Converts index (step) into (data, label) tuple."""
        return self.example_by_step[step]
# ---
def output_autolink(self, m):
        link = m.group(1)
        if m.group(2) == '@':
            is_email = True
        else:
            is_email = False
        return self.renderer.autolink(link, is_email)
# ---
def _get_host_ip() -> str:
    """Get the routable IP of this host via the default route.

    Opens a UDP socket to a public IP (no traffic sent) and reads back the
    local address the OS selected. With --network=host this returns the real
    machine IP visible to other machines in the same VPC.
    """
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(("8.8.8.8", 80))
        return s.getsockname()[0]
    finally:
        s.close()
# ---
def list_jobs(self, request: cluster__pb2.Controller.ListJobsRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListJobsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def glorot_uniform_init_(weights):
    torch.nn.init.xavier_uniform_(weights, gain=1)
# ---
def inspect(self, container_id: str) -> ContainerStatus:
        c = self._containers.get(container_id)
        if not c:
            return ContainerStatus(running=False, exit_code=1, error="container not found")
        return ContainerStatus(
            running=c._running,
            exit_code=c._exit_code,
            error=c._error,
        )
# ---
def extract_seed(prng_key) -> int:
    """Extract an integer seed from either a JAX PRNG key or an integer."""
    if isinstance(prng_key, int):
        return prng_key
    # It's a JAX key - extract seed using JAX
    return jax.random.randint(prng_key, (), 0, 1_000_000).item()
# ---
def remove_title_page(html: BeautifulSoup):
    # Remove title page since we only care about information after first section
    title_page = html.findAll("div", {"class": "ltx_titlepage"})
    for tp in title_page:
        tp.decompose()
# ---
def hook(*args):
            """Log the call into a list"""
            calls.append(args)
# ---
def selu(a: A) -> A:
    return wrap_elemwise_unary(jnn.selu, a)
# ---
def get_absolute_saagie_url(saagie_url):
    if saagie_url.startswith('/'):
        return SAAGIE_ROOT_URL + saagie_url
    return saagie_url
# ---
def output_text(self):
        return self.renderer.paragraph(self.tok_text())
# ---
def test_empty_rollouts_raises_error():
    """Test that empty rollout list raises ValueError."""
    with pytest.raises(ValueError, match="Cannot create batch from empty rollout list"):
        train_batch.create_training_batch_from_rollouts([], max_tokens=16, pad_token_id=0)
# ---
def test_failure(tmp_path, timing_map, n_tasks, retries, use_backups):
    with pytest.raises(RuntimeError):
        run_test(
            function=partial(deterministic_failure, tmp_path, timing_map),
            input=range(n_tasks),
            retries=retries,
            use_backups=use_backups,
        )

    check_invocation_counts(tmp_path, timing_map, n_tasks, retries)
# ---
def maximum_segments(n, a, b, c) : 
	dp = [-1] * (n + 10) 
	dp[0] = 0
	for i in range(0, n) : 
		if (dp[i] != -1) : 
			if(i + a <= n ): 
				dp[i + a] = max(dp[i] + 1, 
							dp[i + a]) 
			if(i + b <= n ): 
				dp[i + b] = max(dp[i] + 1, 
							dp[i + b]) 
			if(i + c <= n ): 
				dp[i + c] = max(dp[i] + 1, 
							dp[i + c]) 
	return dp[n]
# ---
def wait(self, futures: list, num_returns: int = 1) -> tuple[list, list]:
        """Wait for futures to complete.

        Args:
            futures: List of futures to wait on
            num_returns: Number of futures to wait for

        Returns:
            Tuple of (ready_futures, pending_futures)
        """
        ...
# ---
def escape(self, text):
        """Rendering escape sequence.

        :param text: text content.
        """
        return escape(text)
# ---
def __contains__(self, item):
        for _, element in self._queue:
            if item == element:
                return True
        return False
# ---
def normalize_amplitude(self,z_data,cal_ampdata):
		return z_data/cal_ampdata
# ---
def test_delete_nonexistent_track(self):
        """ Tests that delete_track method succeeds, by design, when deleting a nonexistent track"""
        experiment = self.create_test_experiment()
        self.assertEqual(experiment.tracks.count(), 0)
        response = self.client.post(reverse("ab_testing_tool_delete_track", args=(NONEXISTENT_TRACK_ID,)),
                                    follow=True)
        self.assertEqual(experiment.tracks.count(), 0)
        self.assertOkay(response)
# ---
def test_bank_contains_expected_types(bank):
    # Our sample programs should produce at least some statements and expressions.
    type_names = set(bank.node_types)
    assert type_names & STATEMENT_TYPES, "Expected some statement types"
    assert type_names & EXPRESSION_TYPES, "Expected some expression types"
# ---
def _virtual_offset(base: ts.TensorStore, offset_amount):
    async def do_read(domain: ts.IndexDomain, array: np.ndarray, read_params: ts.VirtualChunkedReadParameters):
        array[...] = (await base[domain].read()) + offset_amount

    return ts.virtual_chunked(do_read, dtype=base.dtype, domain=base.domain, shape=base.shape)
# ---
def send_img(filename):
    return static_file(filename, root='images')
# ---
def parameter_count(model: PyTree):
    # especially with jax.vjp, we get duplicate arrays and want to uniq them
    # NB we need to use object identity here, mostly because of ShapedDtypeStruct
    leaves = {id(x): x for x in jax.tree_util.tree_leaves(model) if is_jax_array_like(x)}
    return sum(x.size for x in leaves.values())
# ---
def test_smoke(cluster):
    _url, client = cluster
    job = submit(client, _quick, "chaos-smoke")
    status = wait(client, job, timeout=30)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def get_advanced_inputs(desc, verbose):
    details = desc.get("details")
    if not verbose and isinstance(details, dict):
        return details.get("advancedInputs", [])
    return []
# ---
def testFutureUnicodeLiterals(self):
    want = "u'foo'\n"
    self.assertEqual((0, want), _GrumpRun(textwrap.dedent("""\
        from __future__ import unicode_literals
        print repr('foo')""")))
# ---
def convert_to_display_name(self, value, record=None):
        """ convert ``value`` from the cache to a suitable display name. """
        return ustr(value)
# ---
def getcash(self):
        return self.cash
# ---
def empty_like(x, /, *, dtype=None, device=None, chunks=None, spec=None) -> "Array":
    return empty(**_like_args(x, dtype, device, chunks, spec))
# ---
def status(self) -> JobStatus:
        iris_status = self._job.status()
        return map_iris_job_state(iris_status.state)
# ---
def glorot_uniform_init_(weights):
    nn.init.xavier_uniform_(weights, gain=1)
# ---
def __repr__(self):
        return f"ReduceOp(local={_get_fn_name(self.local_reducer)}, global={_get_fn_name(self.global_reducer)})"
# ---
def is_woodall(x): 
	if (x % 2 == 0): 
		return False
	if (x == 1): 
		return True
	x = x + 1 
	p = 0
	while (x % 2 == 0): 
		x = x/2
		p = p + 1
		if (p == x): 
			return True
	return False
# ---
def get_extra_vars():
  """Returns the captured variables by the function.

  Returns:
    If the default graph is being used to define a function, the
    returned list of variables are those created inside the function
    body so far. Otherwise, returns an empty list.
  """
  g = ops.get_default_graph()
  if isinstance(g, _FuncGraph):
    return g.extra_vars
  else:
    return []
# ---
def __mul__(self, factor: float) -> "Duration":
        return Duration(int(self._ms * factor))
# ---
def test_regions_passed_through(self):
        resources = ResourceConfig(regions=["us-central1", "us-east1"])
        spec = convert_resources(resources)
        assert list(spec.regions) == ["us-central1", "us-east1"]
# ---
def __rrshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.right_shift(other, self)
# ---
def _get_bias_dropout_scale(self):
      return (
        bias_dropout_add_scale_fused_train
        if self.training
        else bias_dropout_add_scale_fused_inference
      )
# ---
import math
def tn_gp(a,n,r):
  tn = a * (math.pow(r, n - 1))
  return tn
# ---
def removef(filename):
    try:
        os.remove(filename)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise
# ---
def __init__(self, field, pattern, fast=True):
        super().__init__(field, pattern, fast)

        parts = pattern.split('..', 1)
        if len(parts) == 1:
            # No range.
            self.point = self._convert(parts[0])
            self.rangemin = None
            self.rangemax = None
        else:
            # One- or two-sided range.
            self.point = None
            self.rangemin = self._convert(parts[0])
            self.rangemax = self._convert(parts[1])
# ---
def _make_or_expr(self, check1, _or, check2):
        """Create an 'or_expr'.

        Join two checks by the 'or' operator.
        """

        return [('or_expr', OrCheck([check1, check2]))]
# ---
def real(a: A) -> A:
    return wrap_elemwise_unary(jnp.real, a)
# ---
def test_get_with_invalid_election_id_non_existent_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': '69' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified an ID for a non-existent election.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def retrying_create_commit(*args, **kwargs):
    return create_commit(*args, **kwargs)
# ---
def separator(self):
        ''' getter method for separator '''
        return self._separator
# ---
def block_code(self, code, lang=None):
        """Rendering block level code. ``pre > code``.

        :param code: text content of the code block.
        :param lang: language of the given code.
        """
        code = code.rstrip('\n')
        if not lang:
            code = escape(code, smart_amp=False)
            return '<pre><code>%s\n</code></pre>\n' % code
        code = escape(code, quote=True, smart_amp=False)
        return '<pre><code class="lang-%s">%s\n</code></pre>\n' % (lang, code)
# ---
import heapq as hq
def heap_replace(heap,a):
  hq.heapify(heap)
  hq.heapreplace(heap, a)
  return heap
# ---
def main(
    bucket_name="marin-data", directory="processed/fineweb/fw-v1.0/", suffix="_processed_html.jsonl.gz", limit=None
):
    counter = 0
    subdirectories = get_subdirectories(bucket_name, directory, suffix)
    for subdir in subdirectories:
        for braceexpand_path in list_files_in_subdir(bucket_name, subdir, suffix):
            print(f"- {braceexpand_path}")
            counter += 1
            if limit and counter >= limit:
                return
# ---
def Video(self, *args, **kwargs):
        return wandb.Video(*args, **kwargs)
# ---
def __repr__(self):
        return "<subdirmatcher path=%r, matcher=%r>" % (self._path, self._matcher)
# ---
def controller_url(self) -> str:
        if self._remote_url:
            return self._remote_url
        if self._manager:
            url = self._manager.controller.discover()
            if url:
                return url
        raise RuntimeError("No controller URL available. Call __enter__ first.")
# ---
def execute(self, context):
        started_at = datetime.now()
        while not self.poke(context):
            if (datetime.now() - started_at).total_seconds() > self.timeout:
                if self.soft_fail:
                    raise AirflowSkipException('Snap. Time is OUT.')
                else:
                    raise AirflowSensorTimeout('Snap. Time is OUT.')
            sleep(self.poke_interval)
        logging.info("Success criteria met. Exiting.")
# ---
def to_list(x):
  """Normalizes a list/tensor into a list.

  If a tensor is passed, we return
  a list of size 1 containing the tensor.

  Arguments:
      x: target object to be normalized.

  Returns:
      A list.
  """
  if isinstance(x, list):
    return x
  return [x]
# ---
def run_test(program: str, call_expr: str, expected: str) -> bool:
    """Execute a program and test case, return whether it passes."""
    try:
        namespace: dict = {}
        exec(program, namespace)
        result = eval(call_expr, namespace)
        return str(result) == expected
    except Exception:
        return False
# ---
def test_direct_ssh_connection_accepts_duration_connect_timeout():
    """DirectSshConnection accepts Duration for connect_timeout field."""
    from iris.cluster.vm.ssh import DirectSshConnection

    conn = DirectSshConnection(
        host="10.0.0.1",
        connect_timeout=Duration.from_seconds(45),
    )
    cmd = conn._build_cmd("echo hello")
    # The SSH ConnectTimeout option should be the integer seconds value
    assert "ConnectTimeout=45" in " ".join(cmd)
# ---
def exists_volume_mount(self, volume_mount):
        ''' return whether a volume mount exists '''
        exist_volume_mounts = self.get_volume_mounts()

        if not exist_volume_mounts:
            return False

        volume_mount_found = False
        for exist_volume_mount in exist_volume_mounts:
            if exist_volume_mount['name'] == volume_mount['name']:
                volume_mount_found = True
                break

        return volume_mount_found
# ---
def get_date_range_list(self):
        return [d for d in self.doc.xpath('//select[@name="date"]/option/@value') if d]
# ---
def timeout_config() -> config_pb2.TimeoutConfig:
    """Timeout configuration for tests."""
    from iris.time_utils import Duration

    timeout_cfg = config_pb2.TimeoutConfig()
    timeout_cfg.boot_timeout.CopyFrom(Duration.from_seconds(5).to_proto())
    timeout_cfg.init_timeout.CopyFrom(Duration.from_seconds(10).to_proto())
    timeout_cfg.ssh_poll_interval.CopyFrom(Duration.from_seconds(1).to_proto())
    return timeout_cfg
# ---
def convert_to_onchange(self, value):
        return value.id
# ---
def path(self) -> str:
        """Returns the URL path to mount the application to when serving multiple applications."""
        return "/iris.cluster.WorkerService"
# ---
def test_truediv_operator(self):
        """Test the / operator for path joining."""
        base = LocalLocation(path=Path("/base/path"))
        unresolved = UnresolvedLocation(path="subdir/file.zarr")

        resolved = base / unresolved

        assert isinstance(resolved, LocalLocation)
        assert resolved.path == Path("/base/path/subdir/file.zarr")
# ---
def generate_job_name(command: list[str]) -> str:
    """Generate a job name from the command."""
    script_name = "job"
    for arg in command:
        path = Path(arg)
        if path.suffix == ".py":
            script_name = path.stem
            break

    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    username = getpass.getuser()
    return f"iris-run-{username}-{script_name}-{timestamp}"
# ---
def exp(a: A) -> A:
    return wrap_elemwise_unary(jnp.exp, a)
# ---
def inline_html(self, html):
        """Rendering span level pure html content.

        :param html: text content of the html snippet.
        """
        if self.options.get('escape'):
            return escape(html)
        return html
# ---
def array_equiv(a: NamedArray, b: NamedArray) -> bool:
    """Returns True if two arrays are shape-consistent and equal."""
    try:
        a, b = broadcast_arrays(a, b)
    except ValueError:
        return False
    return bool(jnp.array_equal(a.array, b.array))
# ---
def read(self, cr, uid, ids, fields=None, context=None, load='_classic_read'):
        return self.pool.get('stock.picking').read(cr, uid, ids, fields=fields, context=context, load=load)
# ---
def inspect_side_effect(container_id):
        call_count[0] += 1
        if call_count[0] == 1:
            return ContainerStatus(running=True)
        return ContainerStatus(running=False, exit_code=1, error="Container crashed")
# ---

def solution(lst):
    """Given a non-empty list of integers, return the sum of all of the odd elements that are in even positions.
    

    Examples
    solution([5, 8, 7, 1]) ==> 12
    solution([3, 3, 3, 3, 3]) ==> 9
    solution([30, 13, 24, 321]) ==>0
    """
    return sum([x for idx, x in enumerate(lst) if idx%2==0 and x%2==1])
# ---
def resize(self, size) -> "Axis":
        return Axis(self.name, size)
# ---
def exitTransitionToCostume(self):
        pass
# ---
def _sample_t(self, n, device):
    _eps_t = torch.rand(n, device=device)
    if self.antithetic_sampling:
      offset = torch.arange(n, device=device) / n
      _eps_t = (_eps_t / n + offset) % 1
    t = (1 - self.sampling_eps) * _eps_t + self.sampling_eps
    if self.importance_sampling:
      return self.noise.importance_sampling_transformation(t)
    return t
# ---
def ball(key, shape: AxisSpec, D: Axis, p: float = 2.0, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.ball(key=key, shape=jax_shape, d=D.size, p=p, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, concat_axes(shape, D)))
# ---
def test_mem_read_word(self):
        self.mda.video_ram[0x0000] = 0x41
        self.mda.video_ram[0x0001] = 0x08
        self.assertEqual(self.mda.mem_read_word(0x0000), 0x0841)
# ---
def test_get_logs_nonexistent_task(worker):
    """Test getting logs for nonexistent task returns empty list."""
    logs = worker.get_logs(JobName.root("nonexistent-task").task(0).to_wire())
    assert logs == []
# ---
def bytes_free_errcheck(res, func, *args):
    notnull_errcheck(res, func, *args)
    rv = cast(res, c_void_p).value
    _mpv_free(res)
    return rv
# ---
def _assert_result(self, select, result):
        eq_(config.db.execute(select).fetchall(), result)
# ---
def degrees(a: A) -> A:
    return wrap_elemwise_unary(jnp.degrees, a)
# ---
def num_cpus(self) -> float | int:
        """The number of CPUs this processor needs to run."""
        raise NotImplementedError
# ---
def display_error(self, error):
        dialog = QtWidgets.QMessageBox(QtWidgets.QMessageBox.Warning, error.title, error.info, QtWidgets.QMessageBox.Ok, self)
        dialog.exec_()
# ---
def getcapture(self, **kw):
        cap = self.__class__.captureclass(**kw)
        cap.start_capturing()
        try:
            yield cap
        finally:
            cap.stop_capturing()
# ---
def test_spawn_iso_glance(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_ISO, None, None,
                         os_type="windows", architecture="i386")
        self.check_vm_params_for_windows()
# ---
def stop_recording(self):
        self.recording_enabled = False
        self.startrecordingAction.setEnabled(True)
        self.stoprecordingAction.setEnabled(False)
# ---
def _slice_out(Block, i, x):
        if isinstance(x, haliax.core.NamedArray):
            if haliax.selects_axis(x.axes, Block):
                return x[Block, i]
            else:
                return x
        elif haliax.jax_utils.is_jax_array_like(x):
            return x[i]
        else:
            return x
# ---
def __repr__(self):
        return f"MapShardOp(fn={_get_fn_name(self.fn)})"
# ---
def registry(self) -> VmRegistry:
        """Access the underlying registry."""
        return self._registry
# ---
def execute(self):
        with self._volumes(), self._password_file():
            yield self._start_helper()
# ---
def train_shards(self):
    raise NotImplementedError()
# ---
def update_variants(variants, template, publish_progress=True):
	count=0
	for d in variants:
		variant = frappe.get_doc("Item", d)
		copy_attributes_to_variant(template, variant)
		variant.save()
		count+=1
		if publish_progress:
				frappe.publish_progress(count*100/len(variants), title = _("Updating Variants..."))
# ---
def local(cls, config: LocalClientConfig | None = None) -> "IrisClient":
        """Create an IrisClient for local execution using real Controller/Worker.

        Args:
            config: Configuration for local execution

        Returns:
            IrisClient wrapping LocalClusterClient
        """
        cfg = config or LocalClientConfig()
        cluster = LocalClusterClient.create(max_workers=cfg.max_workers)
        return cls(cluster)
# ---
def h_fs_system(_,path,eltName='',cwd=None):
        import subprocess as sp
        import shlex
        data=sp.Popen(shlex.split(path),cwd=cwd,stdout=sp.PIPE, stderr=sp.PIPE).communicate()
        _.ws.send(json.dumps({"method":"fs_system","result":[path,data,eltName]}));
        pass
# ---
def Confirm( message ):
  """Display |message| with Ok/Cancel operations. Returns True if the user
  selects Ok"""
  return bool( PresentDialog( message, [ "Ok", "Cancel" ] ) == 0 )
# ---
def evaluate(self, record: dict) -> bool:
        left_val = self.left.evaluate(record)
        if self.op == "and":
            return bool(left_val and self.right.evaluate(record))
        return bool(left_val or self.right.evaluate(record))
# ---
def predict_dataloader(self) -> DataLoader:
        """Get the training dataloader.

        Returns
        -------
        DataLoader
            The training dataloader.

        """
        return DataLoader(
            self.predict_set,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            shuffle=False,
            collate_fn=collate,
        )
# ---
def spec(tmp_path, reserved_mem):
    return cubed.Spec(tmp_path, allowed_mem=ALLOWED_MEM, reserved_mem=reserved_mem)
# ---
def test_stdfd_functional(self, testdir):
        reprec = testdir.inline_runsource("""
            def test_hello(capfd):
                import os
                os.write(1, "42".encode('ascii'))
                out, err = capfd.readouterr()
                assert out.startswith("42")
                capfd.close()
        """)
        reprec.assertoutcome(passed=1)
# ---
def __getitem__(self, index: int) -> T_co:
        return self._run_coroutine(self.dataset.getitem_async(index))
# ---
def compute_logits(model: LmHeadModel, example: LmExample):
            model = mp.cast_to_compute(model)
            with hax.axis_mapping(compute_axis_mapping):
                activations = model.activations(example.tokens, key=None, attn_mask=example.attn_mask)
                head = model.get_lm_head()
                logits = hax.dot(activations, head, axis=model.Embed)
                return logits
# ---
def test_nested_lists():
    html = """<ul>
    <li>Item 1
        <ul>
            <li>Subitem 1</li>
            <li>Subitem 2</li>
        </ul>
    </li>
    <li>Item 2</li>
</ul>"""

    expected = """* Item 1
    + Subitem 1
    + Subitem 2
* Item 2
"""
    assert to_markdown(html) == expected
# ---
def setUp(self):
        super().setUp()

        self.user = User.objects.create()
        permission = Permission.objects.get(codename='search')
        self.user.user_permissions.add(permission)
        self.client.force_authenticate(user=self.user)
# ---
def trace(
        self, axis1: AxisSelector, axis2: AxisSelector, offset=0, dtype=None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.trace(self, offset=offset, axis1=axis1, axis2=axis2, dtype=dtype)
# ---
def submit(client, fn, name, *args, **kw):
    return client.submit(
        entrypoint=Entrypoint.from_callable(fn, *args),
        name=name,
        resources=ResourceSpec(cpu=1, memory="1g"),
        environment=EnvironmentSpec(),
        **kw,
    )
# ---
def _mean_groupby_func(a, by, axis, intermediate_dtype, num_groups):
    dtype = dict(intermediate_dtype)
    n = npg.aggregate(by, a, func="len", dtype=dtype["n"], axis=axis, size=num_groups)
    total = npg.aggregate(
        by, a, func="sum", dtype=dtype["total"], axis=axis, size=num_groups
    )
    return {"n": n, "total": total}
# ---
def test_random_mutation_changes_source(bank):
    source = CORPUS[0]  # fibonacci
    rng = random.Random(42)

    mutation = random_mutation(source, bank, rng=rng)
    assert mutation is not None
    assert mutation.replacement != mutation.original
# ---
def parse_user_command_line(command: str) -> dict[str, str]:
    """Extract interesting parts from a user command line."""
    parts = command.strip().split()
    entrypoint = None
    for part in parts:
        if Path(part).exists() and "/python" not in part:
            entrypoint = Path(part).name.split(".")[0]
            return {"entrypoint": entrypoint}

    if parts and entrypoint is None:
        entrypoint = parts[0]
    else:
        entrypoint = "unknown"

    return {"entrypoint": entrypoint}
# ---
def scan_aware_map(fn: Callable[..., T], tree: Any, *rest: Any, is_leaf: Callable[[Any], bool] | None = None) -> Any:
    """Alias for :func:`haliax.tree_util.scan_aware_tree_map` with :mod:`jax.tree` style naming."""

    return tree_util.scan_aware_tree_map(fn, tree, *rest, is_leaf=is_leaf)
# ---
def test_if_body_change():
    source = "if x > 0:\n    return x\n"
    target = "if x > 0:\n    return -x\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1
# ---
def heartbeat_loop():
            while not self._stop_event.wait(HEARTBEAT_INTERVAL):
                self._status_file.refresh_lock()
# ---
def _compute_tag_arrays(self):
        tag_arrays = []
        for dataset, tags in self.datasets:
            indexed = [self.tag_to_index[tag] for tag in tags]
            tags = np.zeros(self.Tag.size, dtype=np.int32)
            tags[indexed] = 1
            tags = hax.named(tags, self.Tag)

            tag_arrays.append(tags)
        return tag_arrays
# ---
def add_op(dag, func, inputs, outputs, fusable_with_predecessors=True):
    name = gensym(func.__name__)
    dag.add_node(name, func=func, fusable_with_predecessors=fusable_with_predecessors)
    for n in inputs:
        dag.add_edge(n, name)
    for n in outputs:
        dag.add_node(n)
        dag.add_edge(name, n)

    return name
# ---
def _original_model(old_model):
        return original_model
# ---
def mutagen(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'mutagenesis',
        'purpose': 'repression',
        'method': 'mutagen treatment'
    }
# ---
def __repr__(self):
        return f"LoadFileOp(format={self.format}, columns={self.columns})"
# ---
def iinfo(type, /):
    return nxp.iinfo(type)
# ---
def ListWorkers(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def execute_dag(self, dag: MultiDiGraph, **kwargs) -> None:
        raise NotImplementedError
# ---
def test_with_unflatten_flatten():
    Z = Axis("Z", B.size * C.size)
    assert einops_rearrange(zq, "(Q: B H) d w c -> d (Z: B c) w H", H=H).axes == (D, Z, W, H)
    # make sure the values are right too
    z_t = (
        zq.array.reshape((B.size, H.size, D.size, W.size, C.size))
        .transpose((2, 0, 4, 3, 1))
        .reshape((D.size, Z.size, W.size, H.size))
    )
    assert (einops_rearrange(zq, "(Q: B H) d w c -> d (Z: B c) w H", H=H).array == z_t).all()
# ---
def __repr__(self) -> str:
        return f"Job({self._job_id!r})"
# ---
def test_impl(df):
            B = pd.to_numeric(df.A, errors='coerce')
            return B
# ---
def test_getitem_bool_series(self):
        def test_impl(df):
            return df['A'][df['B']].values

        hpat_func = self.jit(test_impl)
        df = pd.DataFrame({'A': [1, 2, 3], 'B': [True, False, True]})
        np.testing.assert_array_equal(test_impl(df), hpat_func(df))
# ---
def count_no (A,N,L,R): 
    count = 0
    for i in range (L,R + 1): 
        if (i % A != 0): 
            count += 1
        if (count == N): 
            break
    return (i)
# ---
def chunks(self):
        """A tuple containing a sequence of block sizes for each corresponding array dimension."""
        return self._chunks
# ---
def __init__(self, eps=1e-3):
    super().__init__()
    self.eps = eps
# ---
def append(self, input_: Input, label: Prognostic):
        """Add another Example as a new step."""
        self.example_by_step.append((input_, label))
# ---
def read(self, path):
        raise Exception('read called with %r' % path)
# ---
def _pick_free_port(host: str) -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind((host, 0))
        return int(sock.getsockname()[1])
# ---
def __init__(
        self,
        design_dir: str,
    ) -> None:
        super().__init__(write_interval="batch")
        if design_dir is not None:
            self.init_outdir(design_dir)
# ---
def user_to_email(user_profile: UserProfile) -> str:
        return user_profile.email.lower()
# ---
def get_server_mock(self):
        return self._server_mock
# ---
def __call__(self, indices: int | np.ndarray) -> int | np.ndarray:
        """Apply the permutation to the given indices.

        Args:
            indices: An integer or an array of integers to be permuted.

        Returns:
            The permuted value(s).
        """
        pass
# ---
def shuffle_ds(ds, key):
            if self.shuffle is True:
                ds = ds.shuffle(key)
            elif isinstance(self.shuffle, int):
                ds = ds.era_shuffle(self.shuffle, key=key)

            return ds
# ---
def __init__(self, bot, settings):
		self.bot = bot
		self.settings = settings
# ---
def __init__(
        self,
        shape: T_Shape,
        dtype: T_DType,
        chunks: T_RegularChunks,
    ):
        self.shape = shape
        self.dtype = np.dtype(dtype)
        self.chunks = chunks
# ---
def hashivault_approle_role_get(params):
    name = params.get('name')
    client = hashivault_auth_client(params)
    result = client.get_role(name, mount_point=params.get('mount_point'))
    return {'role': result}
# ---
def _initialize_actors(self):
        """Initialize the minimum number of actors."""
        logger.info(f"Initializing {self.min_actors} actors...")
        for _ in range(self.min_actors):
            self._create_and_register_actor()
        logger.info(f"Initialized {len(self.actors)} actors")
# ---
def __init__(self, remote, letter, number, name, colour, *args, **kwargs):
        super(TeamPanel, self).__init__(*args, **kwargs) 
        self.remote = remote
        self.InitUI(letter, number, name, colour)
# ---
def flatten_axes(self, old_axes: AxisSelection, new_axis: AxisSelector) -> "NamedArray":  # pragma: no cover
        return haliax.flatten_axes(self, old_axes=old_axes, new_axis=new_axis)
# ---
def get_job_from_jbor(thing):
    '''
    :returns: Job ID from a JBOR

    Assumes :func:`is_job_ref` evaluates to True
    '''
    if '$dnanexus_link' in thing:
        return thing['$dnanexus_link']['job']
    else:
        return thing['job']
# ---
def __init__(self, input_ids_key: str, loss_weights_key: str | None):
        self.input_ids_key = input_ids_key
        self.loss_weights_key = loss_weights_key
        self._exemplar = {input_ids_key: np.zeros((0,), dtype=np.int32)}
        if loss_weights_key is not None:
            self._exemplar[loss_weights_key] = np.zeros((0,), dtype=np.float32)
# ---
def test_hash_verification_failure(temp_cache_dir, test_bundle):
    """Test that hash verification fails with incorrect hash."""
    cache = BundleCache(temp_cache_dir)

    file_url = f"file://{test_bundle}"

    # Use wrong hash
    wrong_hash = "a" * 64

    # Should raise ValueError
    with pytest.raises(ValueError, match="Bundle hash mismatch"):
        cache.get_bundle(file_url, expected_hash=wrong_hash)
# ---
def test_encode_decode_source_roundtrip(tok):
    source = "x = 1 + 2\n"
    ids = tok.encode_source(source)
    decoded = tok.decode_source(ids)
    assert decoded == source
# ---
def rmse(self) -> float:
        ret = float(
            area_weighted_rmse(
                gen=self.gen,
                target=self.target,
                area_weights=self.area_weights,
            )
            .cpu()
            .numpy()
        )
        return ret
# ---
def _get_radio(self):
		r = Radio(self.i, self._dispatcher, self.send_delay)
		self.radios.append(r)

		self.i += 1

		return r
# ---
def open_door(self, content, door_card):
        return self._do_open
# ---
def check_Consecutive(l): 
    return sorted(l) == list(range(min(l),max(l)+1))
# ---
def decontaminate(config: DeconConfig):
    """Main entry point for decontamination workflows."""
    if config.mode == DeconMode.DECONTAMINATE:
        return _run_decontamination(config)
    elif config.mode == DeconMode.TRAIN_TEST_OVERLAP:
        return _run_train_test_overlap(config)
    else:
        raise ValueError(f"Unknown mode {config.mode}")
# ---
def select_0th(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.take(axis, 0)
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            # other leaves don't matter
            return leaf
# ---
def calculate_alpha(self):
        """
        http://en.wikipedia.org/wiki/Alpha_(investment)
        """
        return alpha(self.algorithm_period_returns,
                     self.treasury_period_return,
                     self.benchmark_period_returns,
                     self.beta)
# ---
def mock_open_for_remote(path, mode="r", **kwargs):
        # Intercept only the remote paths file download
        if "data.commoncrawl.org" in path and "data-jsonl.paths.gz" in path:
            return paths_file.open("rb")
        return original_open(path, mode, **kwargs)
# ---
def evaluate(self, record: dict) -> Any:
        parent_val = self.parent.evaluate(record)
        if isinstance(parent_val, dict):
            return parent_val.get(self.field)
        return None
# ---
def test_entrypoint_from_callable_resolve_roundtrip():
    ep = Entrypoint.from_callable(_add, 3, b=4)
    fn, args, kwargs = ep.resolve()
    assert fn(*args, **kwargs) == 7
# ---
def resolve_axis(self, axis: PartialShapeDict) -> ShapeDict: ...
# ---
def mnist_many_cols_gbm_large():
  train = h2o.import_file(path=pyunit_utils.locate("bigdata/laptop/mnist/train.csv.gz"))
  train.tail()


  gbm_mnist = H2OGradientBoostingEstimator(ntrees=1,
                                           max_depth=1,
                                           min_rows=10,
                                           learn_rate=0.01)
  gbm_mnist.train(x=range(784), y=784, training_frame=train)
  gbm_mnist.show()
# ---
def _test(self, expr, expected):
        some_table = self.tables.some_table

        with config.db.connect() as conn:
            rows = {
                value
                for value, in conn.execute(
                    select([some_table.c.id]).where(expr)
                )
            }

        eq_(rows, expected)
# ---
def test_core_event_deco(self):
        @event.core_event
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.priority is event.Priority.CORE
# ---
def rename(self, renames: Mapping[AxisSelector, AxisSelector]) -> "NamedArray":  # pragma: no cover
        return haliax.rename(self, renames=renames)
# ---
def test_device_flops_for_jax_device(jax_device_kind, expected_flops):
    assert device_flops_for_jax_device(jax_device_kind) == expected_flops
# ---
def get_metrics(self) -> dict:
        return dataclasses.asdict(self.metrics)
# ---
def test_poly_int_der():
    C = Axis("C", 4)
    p = hax.named([1.0, 0.0, -2.0, 1.0], (C,))

    d = hax.polyder(p)
    i = hax.polyint(p)

    assert jnp.allclose(d.array, jnp.polyder(p.array))
    assert jnp.allclose(i.array, jnp.polyint(p.array))
    assert d.axes[0] == C.resize(d.array.shape[0])
    assert i.axes[0] == C.resize(i.array.shape[0])
# ---
def append(self, data: np.ndarray):
        self.extend([data])
# ---
def h_fs_mkdir (_,path): os.mkdir(path)
# ---
def test_flatten(self):
    testing_utils.layer_test(
        keras.layers.Flatten, kwargs={}, input_shape=(3, 2, 4))

    # Test channels_first
    inputs = np.random.random((10, 3, 5, 5)).astype('float32')
    outputs = testing_utils.layer_test(
        keras.layers.Flatten,
        kwargs={'data_format': 'channels_first'},
        input_data=inputs)
    target_outputs = np.reshape(
        np.transpose(inputs, (0, 2, 3, 1)), (-1, 5 * 5 * 3))
    self.assertAllClose(outputs, target_outputs)
# ---
def _on_record_button_pressed(self):
        pass
# ---
def vm_registry(self) -> VmRegistry:
        """Access the VM registry for RPC/status use."""
        return self._vm_registry
# ---
def default(self, obj):
        if isinstance(obj, timedelta):
            return {"days": obj.days, "seconds": obj.seconds, "microseconds": obj.microseconds}
        if isinstance(obj, Path):
            return str(obj)
        if obj in (float32, bfloat16):
            return str(obj)
        try:
            return super().default(obj)
        except TypeError:
            logger.warning(f"Could not serialize object of type {type(obj)}: {obj}")
            return str(obj)
# ---
def setup(*args, **kwargs):
    try:
        ctypes.cdll.LoadLibrary(leptlib)
    except Exception as e:
        raise NidabaPluginException(e.message)
# ---
def _sampling_noise(self):
    return - (1e-10 - (
      torch.rand(* self.shape) + 1e-10).log()).log()
# ---
def launch(self, request: JobRequest) -> JobId:
        """Launch job on Ray cluster, returning job identifier."""
        logger.info("Launching job: %s", request.name)

        if isinstance(request.resources.device, TpuConfig):
            return self._launch_tpu_job(request)

        if request.entrypoint.binary_entrypoint is not None:
            return self._launch_binary_job(request)

        return self._launch_callable_job(request)
# ---
def do_put(self, context, descriptor, reader, writer):
        pass
# ---
def test_simple_limit_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).limit(2).offset(1),
            [(2, 2, 3), (3, 3, 4)],
        )
# ---
def test_model_info_patch_for_fsspec_urls():
    """transformers calls model_info() in _patch_mistral_regex to check if a model is a base Mistral model."""
    import huggingface_hub

    with _patch_hf_hub_download():
        # This should NOT raise or make a network call - it should return a mock
        result = huggingface_hub.hf_api.model_info("memory://some/path")
        assert result.id == "monkeypatched"
        assert result.tags is None
# ---
def test_dontreadfrominput():
    from _pytest.capture import DontReadFromInput
    f = DontReadFromInput()
    assert not f.isatty()
    pytest.raises(IOError, f.read)
    pytest.raises(IOError, f.readlines)
    pytest.raises(IOError, iter, f)
    pytest.raises(UnsupportedOperation, f.fileno)
    f.close()
# ---
def get_extra_inputs():
  """Returns the captured input tensors by the function.

  Returns:
    If the default graph is being used to define a function, the
    returned list of tensors are those accessed inside the function body
    but defined outside the function body so far. Otherwise, returns an
    empty list.
  """
  g = ops.get_default_graph()
  if isinstance(g, _FuncGraph):
    return g.extra_inputs
  else:
    return []
# ---
def test_repr(self):
        assert repr(lit(42)) == "lit(42)"
        assert repr(lit("hello")) == "lit('hello')"
# ---
def test_visualize_shardings_runs(capsys):
    mesh = jax.sharding.Mesh(
        np.array(jax.devices()).reshape(-1, 1, 1),
        (ResourceAxis.DATA, ResourceAxis.MODEL, ResourceAxis.REPLICA),
    )
    with axis_mapping(resource_map), mesh:
        arr = hax.ones((Dim1, Dim2, Dim3))
        visualize_shardings(arr)

    out = capsys.readouterr().out
    assert "dim1" in out and "dim2" in out and "dim3" in out
# ---
def test_get_lines_with_very_long_string():
        assert len(get_lines("a"*(4*LINEWIDTH-1))) == 4
# ---
def _mpv_client_api_version():
    ver = backend.mpv_client_api_version()
    return ver>>16, ver&0xFFFF
# ---
def file_exists(self):
        ''' return whether file exists '''
        if os.path.exists(self.filename):
            return True

        return False
# ---
def compute_axis_mapping(self) -> ResourceMapping:
        return self.config.compute_axis_mapping
# ---
def test_add_entry():
    bank = SubtreeBank()
    entry = SubtreeEntry(source="x + y", node_type="BinOp", stmt_count=0)
    bank.add(entry)
    assert bank.total_entries == 1
    assert bank.has_type("BinOp")
    assert not bank.has_type("Call")
# ---
def test_flatten_scalar_channels(self):
    testing_utils.layer_test(
        keras.layers.Flatten, kwargs={}, input_shape=(3,))

    # Test channels_first
    inputs = np.random.random((10,)).astype('float32')
    outputs = testing_utils.layer_test(
        keras.layers.Flatten,
        kwargs={'data_format': 'channels_first'},
        input_data=inputs)
    target_outputs = np.expand_dims(inputs, -1)
    self.assertAllClose(outputs, target_outputs)
# ---
def get_matching_pattern(self, pattern, name, path):
        pattern = pattern[name]
        if isinstance(pattern, six.string_types):
            return pattern
        else:
            match = pattern.match(path)
            if match:
                return match.group(name)
        return None
# ---
def __init__(self, min_chains: int = 1, max_chains: int = 300) -> None:
        """Initialize the filter.

        Parameters
        ----------
        min_chains : int
            The minimum number of chains allowed.
        max_chains : int
            The maximum number of chains allowed.

        """
        self.min_chains = min_chains
        self.max_chains = max_chains
# ---
def __str__(self):
        return "NoncentralBetaDistr(alpha={0},beta={1},lambda={2})#{3}".format(self.alpha, self.beta, self.lmbda, self.id())
# ---
def _mk(remat_policy: bool | str | "ScanCheckpointPolicy") -> "ScanCheckpointPolicy":
        if isinstance(remat_policy, ScanCheckpointPolicy):
            return remat_policy
        else:
            return ScanCheckpointPolicy.from_bool_or_str(remat_policy)
# ---
def from_cluster(cls, cluster_name: str, ray_init: bool = False) -> "DashboardConfig":
        """Create config for a single cluster by name."""
        return cls(cluster_configs=[cluster_name], ray_init=ray_init)
# ---
def make_post(with_comments=True, with_author=True, with_keywords=True):
    comments = [make_comment() for _ in range(2)] if with_comments else []
    keywords = [make_keyword() for _ in range(3)] if with_keywords else []
    author = make_author() if with_author else None
    return Post(
        id=fake.random_int(),
        title=fake.catch_phrase(),
        author=author,
        author_id=author.id if with_author else None,
        comments=comments,
        keywords=keywords,
    )
# ---
def _ParseAndVisit(source):
  mod = pythonparser.parse(source)
  _, future_features = imputil.parse_future_features(mod)
  importer = imputil.Importer(None, 'foo', 'foo.py', False)
  b = block.ModuleBlock(importer, '__main__', '<test>',
                        source, future_features)
  visitor = stmt.StatementVisitor(b)
  visitor.visit(mod)
  return visitor
# ---
def testAssignAttribute(self):
    self.assertEqual((0, '123\n'), _GrumpRun(textwrap.dedent("""\
        e = Exception()
        e.foo = 123
        print e.foo""")))
# ---
def arrays_to_dag(*arrays):
    from .array import check_array_specs

    check_array_specs(arrays)
    dags = [x.plan.dag for x in arrays if hasattr(x, "plan")]
    return nx.compose_all(dags)
# ---
def test_add_literal(self):
        expr = col("a") + 10
        assert expr.evaluate({"a": 5}) == 15
# ---
def eulerian_num(n, m): 
	if (m >= n or n == 0): 
		return 0 
	if (m == 0): 
		return 1 
	return ((n - m) * eulerian_num(n - 1, m - 1) +(m + 1) * eulerian_num(n - 1, m))
# ---
def axis_spec_to_shape_dict(axis_spec: AxisSelection) -> dict[str, int | None]:  # type: ignore
    ...
# ---
def __repr__(self):
        return self.node + "\nREF: " + str(self.references) + "\nChildren: " + str(self.children.keys()) + "\n"
# ---
def _call_fn(self, index, item):
        if "key" in self._extra_kwargs:
            key = self._maybe_fold_in_key(self._extra_kwargs["key"], index)
            kwargs = {**self._extra_kwargs, "key": key}
        else:
            kwargs = self._extra_kwargs
        return self.fn(item, *self._extra_args, **kwargs)
# ---
def test_iterating_with_next(self):
        stages = iter(['alpha','beta','gamma'])

        try:
            self.assertEqual('alpha', next(stages))
            next(stages)
            self.assertEqual('gamma', next(stages))
            next(stages)
        except StopIteration as ex:
            err_msg = 'Ran out of iterations'

        self.assertRegex(err_msg, 'Ran out')
# ---
def test_filters_for_instance_with_ip_v6(self):
        self.flags(use_ipv6=True)
        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)
        rulesv4, rulesv6 = self.fw._filters_for_instance("fake", network_info)
        self.assertEquals(len(rulesv4), 2)
        self.assertEquals(len(rulesv6), 1)
# ---
def __repr__(self) -> str:
        return f"JobName({str(self)!r})"
# ---
def check_runtime_memory(spec):
    allowed_mem = spec.allowed_mem if spec is not None else None
    if allowed_mem is not None:
        if runtime_memory < allowed_mem:
            raise ValueError(
                f"Runtime memory ({runtime_memory}) is less than allowed_mem ({allowed_mem})"
            )
# ---
def __hash__(self):
        """Hash based on the ID (every object is different)."""
        return hash(id(self))
# ---
def count_Set_Bits(n) :  
    n += 1; 
    powerOf2 = 2;   
    cnt = n // 2;  
    while (powerOf2 <= n) : 
        totalPairs = n // powerOf2;  
        cnt += (totalPairs // 2) * powerOf2;  
        if (totalPairs & 1) : 
            cnt += (n % powerOf2) 
        else : 
            cnt += 0
        powerOf2 <<= 1;    
    return cnt;
# ---
def validate(self, obj):
                if obj.id is None:
                    obj.id = obj.label.replace(' ', '')
                return True
# ---
def program(self):
        return self.startup.current_program() if self._get_startup() else None
# ---
def _engine_fixture(self):
        buf = util.StringIO()
        def dump(sql, *multiparams, **params):
            buf.write(util.text_type(sql.compile(dialect=engine.dialect)))
        engine = create_engine('postgresql://', strategy='mock', executor=dump)
        return engine, buf
# ---
def __enter__(self):
        """Context manager entry - start the worker."""
        self.start()
        return self
# ---
def test_job_name_rejects_invalid_inputs(value: str):
    with pytest.raises(ValueError):
        JobName.from_string(value)
# ---
def __str__(self):
        return f"RunningMean(mean={self.mean}, total={self.total})"
# ---
def test_fold_str_args():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    acc = hax.zeros((Height, Width))

    total = hax.fold(lambda x, y: x + y, "Depth")(acc, named1)

    assert jnp.all(jnp.isclose(total.rearrange(acc.axes).array, jnp.sum(named1.array, axis=2)))
# ---

def multiply(a, b):
    """Complete the function that takes two integers and returns 
    the product of their unit digits.
    Assume the input is always valid.
    Examples:
    multiply(148, 412) should return 16.
    multiply(19, 28) should return 72.
    multiply(2020, 1851) should return 0.
    multiply(14,-15) should return 20.
    """
    return abs(a % 10) * abs(b % 10)
# ---
def get_handler_filename(handler):
    """Shortcut to get the filename from the handler string.

    :param str handler:
      A dot delimited string representing the `<module>.<function name>`.
    """
    module_name, _ = handler.split(".")
    return "{0}.py".format(module_name)
# ---
def string_to_list(string): 
    lst = list(string.split(" ")) 
    return lst
# ---
def started_job(method, notebook, data):
    return {'job': notebook.current_job, 'username': SAAGIE_USERNAME}
# ---
def stop(self) -> None:
        """No-op: TpuVmManager has no background threads to stop."""
        pass
# ---
def tick(self, ts: int) -> None:
        """Advance VM state transitions."""
        for vm in self._vms:
            vm.tick(ts)
# ---
def lcm(a: int, b: int) -> int:  # type: ignore
        """Implementation of `math.lcm()` for all Python versions."""
        # https://stackoverflow.com/a/51716959/809705
        return a * b // math.gcd(a, b)
# ---
def autoscaler(self) -> AutoscalerProtocol | None:
        """Get the autoscaler instance, if autoscaling is enabled."""
        ...
# ---
def add_secret(self, key, value):
        ''' add a secret '''
        if self.secrets:
            self.secrets[key] = value
        else:
            self.put(Secret.secret_path, {key: value})

        return True
# ---
def __exit__(self, exc_type, exc, tb):
        jax_config.abstract_mesh_context_manager.set_local(self._prev)
        return False
# ---
def score_syntax_errors(program_lines):
    points = {')': 3, ']': 57, '}': 1197, '>': 25137}
    s = 0
    scores_auto = []

    for line in program_lines:
        corrupted, stack = corrupted_character(line)

        if corrupted:
            s += points[corrupted]
        else:
            scores_auto.append(score_autocomplete(stack))

    return s, sorted(scores_auto)[floor(len(scores_auto)/2)]
# ---
def _compute_learning_rate(self, batch_size: int, hidden_dim: int) -> float:
        """Compute learning rate from batch size and hidden dim."""
        return (self.lr_constant * math.sqrt(batch_size)) / hidden_dim
# ---
def rights(self):
        del self._rights
# ---
def is_small(self):
    raise NotImplementedError()
# ---
def __repr__(self):
        return '<GeomCoverage %r: %r>' % (self.extent.llbbox, self.geom)
# ---
def wait(self, timeout: float = 60.0) -> str | None:
        """Wait for completion and return result."""
        self._done.wait(timeout=timeout)
        return self.result
# ---
def _get_autoscaler_status(controller_url: str) -> vm_pb2.AutoscalerStatus:
    client = cluster_connect.ControllerServiceClientSync(controller_url)
    request = cluster_pb2.Controller.GetAutoscalerStatusRequest()
    return client.get_autoscaler_status(request).status
# ---
def needs_update_replicas(self, replicas):
        ''' verify whether a replica update is needed '''
        current_reps = self.get(DeploymentConfig.replicas_path)
        return not current_reps == replicas
# ---
def test_maxiters():
    """ensure the iteration ceiling works"""
  #  assert kmeans.should_iter([], [], iterations=29) == True
    assert kmeans.should_iter([], [], iterations=30) == False
    assert kmeans.should_iter([], [], iterations=31) == False
# ---
def ArticleEndpoint():
    """Eventual landing page for searching/retrieving articles"""
    if request.method == 'GET':
        return render_template("articles.html")
# ---
def needAuthForm(self, action):
        """Does this action require an authentication form?"""
        if action not in self.knownActions:
            raise KeyError("unknown action")
        cfg = self.config.get(action, False)
        if cfg == 'auth' or callable(cfg):
            return True
        return False
# ---
def merge_prognostic_and_boundary(self, prognostic: torch.Tensor, step: int):
        input_ = self.get_input(step)
        merged = input_.clone()
        merged[:, : self.num_prognostic_channels] = prognostic
        return merged
# ---
def shift(self, tok, value):
        """Adds one more token to the state.  Calls reduce()."""

        self.tokens.append(tok)
        self.values.append(value)

        # Do a greedy reduce...
        self.reduce()
# ---
def __contains__(self, key):
        """The 'in' operator will return true or false depending on whether
        'key' is an array in the dataset or not.
        """
        return key in self._variables
# ---
def evi(domain, b):
    '''Simple EVI based classifier'''
    #no_clouds = b['b3'].lte(2100).select(['sur_refl_b03'], ['b1'])
    criteria1 = b['EVI'].lte(0.3).And(b['LSWI'].subtract(b['EVI']).gte(0.05)).select(['sur_refl_b02'], ['b1'])
    criteria2 = b['EVI'].lte(0.05).And(b['LSWI'].lte(0.0)).select(['sur_refl_b02'], ['b1'])
    #return no_clouds.And(criteria1.Or(criteria2))
    return criteria1.Or(criteria2)
# ---
def test_capture_is_represented_on_failure_issue128(self, testdir, method):
        p = testdir.makepyfile("""
            def test_hello(cap%s):
                print ("xxx42xxx")
                assert 0
        """ % method)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "xxx42xxx",
        ])
# ---
def __ne__(self, other: object) -> CompareExpr:  # type: ignore[override]
        return CompareExpr(self, _to_expr(other), "ne")
# ---
def fold(
    fn: Callable[[Carry, X], Carry],
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable[[Carry, PyTree[X]], Carry]: ...
# ---
def __call__(self, batch: Sequence[T]) -> Sequence[T]:
        return batch
# ---
def registerPlugin(plugin_instance): 
    """
    @type plugin_instance: L{amsn2.plugins.developers.aMSNPlugin}
    """
    pass
# ---
def pin_memory(self):
        self.raw_data = [
            (
                input_.pin_memory(),
                boundary.pin_memory(),
                label.pin_memory(),
            )
            for input_, boundary, label in self.raw_data
        ]
        return self
# ---
def find_chainlink_dir():
    """Find the .chainlink directory by walking up from cwd."""
    current = os.getcwd()
    for _ in range(10):
        candidate = os.path.join(current, '.chainlink')
        if os.path.isdir(candidate):
            return candidate
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent
    return None
# ---
def to_proto(self) -> cluster_pb2.Constraint:
        """Convert to protobuf representation."""
        proto = cluster_pb2.Constraint(key=self.key, op=self.op.to_proto())
        if self.value is not None:
            proto.value.CopyFrom(AttributeValue(self.value).to_proto())
        return proto
# ---
def test_parse_http_list(self):
        tests = [
            ('a,b,c', ['a', 'b', 'c']),
            ('path"o,l"og"i"cal, example', ['path"o,l"og"i"cal', 'example']),
            ('a, b, "c", "d", "e,f", g, h',
             ['a', 'b', '"c"', '"d"', '"e,f"', 'g', 'h']),
            ('a="b\\"c", d="e\\,f", g="h\\\\i"',
             ['a="b"c"', 'd="e,f"', 'g="h\\i"'])]
        for string, list in tests:
            self.assertEqual(urllib.request.parse_http_list(string), list)
# ---
def tearDown (self):
    utils.rmtemp ()
# ---
def the_object_name1_has_no_boolean_difference_by_name2(name1, name2):
    obj = the_object_name_exists(name1)
    for modifier in obj.modifiers:
        if modifier.type == "BOOLEAN" and modifier.object and modifier.object.name == name2:
            assert False, "A boolean was found"
# ---
def __init__(self, c):
        self.c = c
        self.is_star = False
# ---
def name_get(self, cr, uid, ids, context=None):
        """Append the serial to the name"""
        if not len(ids):
            return []
        res = [ (r['id'], r['serial'] and '%s [%s]' % (r['name'], r['serial'])
                                      or r['name'] )
                for r in self.read(cr, uid, ids, ['name', 'serial'],
                                   context=context) ]
        return res
# ---
def count_vowels(test_str):
  res = 0
  vow_list = ['a', 'e', 'i', 'o', 'u']
  for idx in range(1, len(test_str) - 1):
    if test_str[idx] not in vow_list and (test_str[idx - 1] in vow_list or test_str[idx + 1] in vow_list):
      res += 1
  if test_str[0] not in vow_list and test_str[1] in vow_list:
    res += 1
  if test_str[-1] not in vow_list and test_str[-2] in vow_list:
    res += 1
  return (res)
# ---
def test_perturb_operators_no_ops_returns_none(rng):
    source = "x = 42"
    result = perturb_operators(source, rng, swap_prob=1.0)
    assert result is None
# ---
def test_best_of_n_no_duplicate_sources(params, model_cfg, tokenizer):
    """Best-of-N should deduplicate by source."""
    results = best_of_n(
        params=params,
        source="x = 1 + 2\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(6),
        n=8,
        max_depth=2,
    )
    sources = [c.source for c in results]
    assert len(sources) == len(set(sources))
# ---
def secureheaders():
    headers = cherrypy.response.headers
    headers['X-Frame-Options'] = 'DENY'
    headers['X-XSS-Protection'] = '1; mode=block'
    headers['Content-Security-Policy'] = "default-src='self'"
# ---
def __repr__(self):
        return f"{self.__class__.__name__}()"
# ---
def test_default_returns_local_client():
    """When no context is set, should return LocalClient."""
    with patch("iris.client.client.get_iris_ctx", return_value=None):
        with patch("ray.is_initialized", return_value=False):
            client = current_client()
            assert isinstance(client, LocalClient)
# ---
def eye(N, M=None, k=0, typecode=None, dtype=None):
    """ eye returns a N-by-M 2-d array where the  k-th diagonal is all ones,
        and everything else is zeros.
    """
    dtype = convtypecode(typecode, dtype)
    if M is None: M = N
    m = np.equal(np.subtract.outer(np.arange(N), np.arange(M)),-k)
    if m.dtype != dtype:
        return m.astype(dtype)
# ---
def setUp(self):
        super(TestReviewXBlock, self).setUp()

        for idx, student in enumerate(self.STUDENTS):
            username = 'u{}'.format(idx)
            self.create_account(username, student['email'], student['password'])
            self.activate_user(student['email'])

        self.staff_user = GlobalStaffFactory()
# ---
def rounded_batch_size_at_step(self, step: int) -> int:
        return self._round_batch_size(self.scheduler.batch_size_at_step(step))
# ---
def peek():
        return i < n and pat[i : i + 1]
# ---
def test_start(self):
        p = profiler.init("secret", base_id="1", parent_id="2")
        p.start = mock.MagicMock()
        profiler.start("name", info="info")
        p.start.assert_called_once_with("name", info="info")
# ---
def _check_worker_timeouts(self) -> None:
        """Check for worker timeouts and send kill RPCs for affected tasks."""
        # State computes failed workers and marks them atomically under lock
        tasks_to_kill = self._state.check_worker_timeouts(self._config.worker_timeout)

        # Send kill RPCs outside lock
        if tasks_to_kill:
            self.kill_tasks_on_workers(tasks_to_kill)
# ---
def get_logs(self, label: str = "train") -> Metrics:
        logs: MetricsDict = dict(super().get_logs(label))
        for agg_label in self._aggregators:
            for k, v in self._aggregators[agg_label].get_logs(label=agg_label).items():
                logs[f"{label}/{k}"] = v

        return logs
# ---
def _requeue_task(self, task: ControllerTask, txn: TransactionLog) -> None:
        """Put task back on scheduling queue for retry."""
        task.state = cluster_pb2.TASK_STATE_PENDING
        if task.task_id not in self._task_queue:
            self._task_queue.append(task.task_id)
        txn.log("task_requeued", task.task_id)
# ---
def is_supported(self):
        return not self.capsule_types.isdisjoint(self.SUPPORTED_CAPSULE_TYPES)
# ---
def set_learning_factor(self, learning_factor):
        assert(learning_factor >= 0 and learning_factor <= 1)
        self.lf = learning_factor
# ---
def normalize(self, fill_nan=True, fill_value=0.0) -> xr.Dataset:
        """Normalize input data."""
        norm = (self.data - self.means) / self.stds
        if fill_nan:
            norm = norm.fillna(fill_value)
        return norm
# ---
def test_capacity_format(self):
        """String formatting for Capacity"""
        c = Capacity(1, 3)
        self.assertEqual(str(c), "R:1.0 W:3.0")
        c = Capacity(0, 0)
        self.assertEqual(str(c), "0")
# ---
def test_multiselector_broadcast():
    B, S, V = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 6)
    a = hax.arange((B, S, V))
    idx1 = hax.arange((B, S), dtype=jnp.int32) % V.size
    out = a["vocab", idx1]
    assert out.axes == (B, S)
    assert jnp.array_equal(out.array, _ref_gather(a, V, idx1))
# ---
def loraize_hf_model(model):
            return loraize(model, config.lora, key=lora_key)
# ---
def list(cls, params=None):
        if params is None:
            params = dict()
        response = cls(Api.call('sales/list_sales', params))
        return response.sale_summary
# ---
import re
def start_withp(words):
 for w in words:
        m = re.match("(P\w+)\W(P\w+)", w)
        if m:
            return m.groups()
# ---
def method5(self, k, l):
        return k + l
# ---
def loss_default(x_raw, w_raw):
        return fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
            x_raw,
            y,
            w_raw,
            reduction="mean",
            logsumexp_weight=logsumexp_weight,
            block_sizes=block_sizes,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
        )
# ---
def reject(self, request, *args, **kwargs):
        instance = self.get_object()
        serializer = self.get_serializer(instance)
        instance.reject(processor=request.user)
        return Response(serializer.data)
# ---
def _validate_hello():
        print("Reload validation job OK")
        return 42
# ---
def rate_noise(self, t):
    cos = (1 - self.eps) * (
      torch.cos(t * torch.pi / 2) ** 2)
    sin = (1 - self.eps) * torch.sin(t * torch.pi)
    scale = torch.pi / 2
    return scale * sin / (cos + self.eps)
# ---
def merge_dict(d1,d2):
 d = d1.copy()
 d.update(d2)
 return d
# ---
def __rand__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer or boolean", "__rand__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.bitwise_and, other, self, dtype=result_type(self, other))
# ---
def out_qdq_bwd(compute_dtype, res, g):
    scale, amax_history = res
    q_g, new_scale, new_history = qdq_and_return(g, jnp.float8_e5m2, scale, amax_history, compute_dtype)
    return q_g, new_scale, new_history
# ---
def check_master(self):
        '''
        Log if the master is not running

        :rtype: bool
        :return: Whether or not the master is running
        '''
        if not os.path.exists(
                os.path.join(
                    self.opts['sock_dir'],
                    'publish_pull.ipc'
                    )
                ):
            return False
        return True
# ---
def build(self, axis: AxisSpec) -> RmsNorm:
        return RmsNorm.init(axis, eps=self.eps, use_weight=self.use_weight, use_bias=self.use_bias)
# ---
def quantize_linear_layers(tree: T, config: QuantizationConfig) -> T:
    """
    Converts a module tree to use FP8/INT8 quantization.
    """
    if config.fp8:
        return _quantize_linear_layers(tree, config, Fp8DotGeneralOp, config.amax_history_length, config.compute_dtype)
    elif config.int8:
        return _quantize_linear_layers(tree, config, Int8DotGeneralOp)
    else:
        warnings.warn("Both fp8 and int8 are set to False. `quantize_linear_layers()` is no-op.")
        return tree
# ---
def test_argmax(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.argmax(a, axis=0)
    run_operation(tmp_path, executor, "argmax", b)
# ---
def test_named_param_annotation():
    def foo(x: f32[NamedArray, "batch embed"]):  # type: ignore  # noqa: F722
        pass

    axes = typing.get_args(typing.get_type_hints(foo, include_extras=True)["x"])[1]
    assert axes.before == ("batch", "embed")
# ---
def __init__(self, context: JobContext, config: BackendConfig):
        """Initialize backend with execution context and configuration.

        Args:
            context: Execution context providing put/get/run/wait primitives
            config: Backend configuration
        """
        self.context = context
        self.config = config
        self.dry_run = config.dry_run
# ---
def __init__(self, num_prognostic_channels: int, label_mask: PrognosticMask):
        self.num_prognostic_channels = num_prognostic_channels
        self.label_mask = label_mask
        self.example_by_step: list[Example] = []
        self.load_stats: LoadStats | None = None
# ---
def find_Extra(arr1,arr2,n) : 
    for i in range(0, n) : 
        if (arr1[i] != arr2[i]) : 
            return i 
    return n
# ---
def finished(status: Self) -> bool:
        return status in (JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED)
# ---
def setUp(self):
        fixture_path = "spec/fixtures/responses/whois.nic.pw/status_available.txt"
        host         = "whois.nic.pw"
        part         = yawhois.record.Part(open(fixture_path, "r").read(), host)
        self.record  = yawhois.record.Record(None, [part])
# ---
def GET(self):

        return render.hello_form()
# ---
def test_build_bootstrap_script_no_key_error(self, minimal_bootstrap_config: config_pb2.BootstrapConfig):
        """Template formatting should not raise KeyError."""
        try:
            script = _build_bootstrap_script(minimal_bootstrap_config, vm_address="10.0.0.1")
            assert script
        except KeyError as e:
            pytest.fail(f"Template has unescaped braces: {{{e.args[0]}}}")
# ---
def __len__(self) -> int:
        return len(self.generated_paths) * self.multiplicity
# ---
def getvalue(self, datas=None):
        pos_value = 0.0
        for data in datas or self.positions.keys():
            comminfo = self.getcommissioninfo(data)
            position = self.positions[data]
            pos_value += comminfo.getvalue(position, data.close[0])

        return self.cash + pos_value
# ---
def resolve(self, location: "Location") -> "ResolvedLocation":
        if isinstance(location, UnresolvedLocation):
            return S3Location(
                endpoint_url=self.endpoint_url,
                bucket=self.bucket,
                path=urljoin(self.path + "/", location.path),
            )
        return location
# ---
def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LmDatasetSourceConfigBase:
        """
        Create a Levanter dataset source config from this config and the actual output path.
        """
        pass
# ---
def rel(self, f):
        return self._matcher.rel(self._path + "/" + f)
# ---
def index_multiplication(test_tup1, test_tup2):
  res = tuple(tuple(a * b for a, b in zip(tup1, tup2))
   for tup1, tup2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def test_optimize_map_filter_map():
    """Map, filter, map should be fused into a single stage."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            FilterOp(lambda x: x > 5),
            MapOp(lambda x: x + 1),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    fused_op = plan.stages[0].operations[0]
    assert isinstance(fused_op, Map)
# ---
def get(self, ref):
        return ray.get(ref)
# ---
def validate_cancelled_item(item_code, docstatus=None, verbose=1):
	if docstatus is None:
		docstatus = frappe.db.get_value("Item", item_code, "docstatus")

	if docstatus == 2:
		msg = _("Item {0} is cancelled").format(item_code)
		_msgprint(msg, verbose)
# ---
def common_prefix_util(str1, str2): 
	result = ""; 
	n1 = len(str1) 
	n2 = len(str2) 
	i = 0
	j = 0
	while i <= n1 - 1 and j <= n2 - 1: 
		if (str1[i] != str2[j]): 
			break
		result += str1[i] 
		i += 1
		j += 1
	return (result) 
def common_prefix (arr, n): 
	prefix = arr[0] 
	for i in range (1, n): 
		prefix = common_prefix_util(prefix, arr[i]) 
	return (prefix)
# ---
def from_callable(
        c: Callable[..., Any],
        args: Sequence[Any] = (),
        kwargs: dict[str, Any] | None = None,
    ) -> Self:
        return Entrypoint(callable_entrypoint=CallableEntrypoint(callable=c, args=args, kwargs=kwargs or {}))
# ---
def get_sso_url(self):
        return self.doc['urlSSO']
# ---
def l1(*arg, **kw):
            canary.append("l1")
# ---
def message_post(self, *args, **kwargs):
        """Post the message on stock.picking to be able to see it in the form view when using the chatter"""
        return self.pool.get('stock.picking').message_post(*args, **kwargs)
# ---
def extract_ngrams(text: str, n: int, stride: int) -> Iterator[str]:
    """
    Extract n-grams from text based on config.
    """
    tokens: list[str] = text.split()

    for i in range(0, len(tokens) - n + 1, stride + 1):
        yield " ".join(tokens[i : i + n])
# ---
def __repr__(self):
        return '%s(width=%d, height=%d)' % \
            (self.__class__.__name__, self.width, self.height)
# ---
def count_docs(file_path: str) -> int:
    return len(list(load_file(file_path)))
# ---
def __getitem__(self, key):
        if key not in self._dataset._coord_names:
            return self._dataset[key]
        else:
            raise KeyError(key)
# ---
def delete_comment(self):
        "Delete the selected comment"

        if self.get_selected_item()['type'] == 'Comment':
            self.delete_item()
        else:
            self.term.flash()
# ---
def num_items(self):
        """int: Total items in the page."""
        return self._num_items
# ---
def _build_service(vocab_size=10):
    model = DummyModel(vocab_size=vocab_size, eos_id=3)
    service = InferenceEngine.from_model_with_config(
        model=model,  # type: ignore
        tokenizer=None,
        config=InferenceEngineConfig(
            max_seq_len=32,
            max_pages=64,
            max_seqs=8,
            page_size=8,
            compute_dtype=jnp.float32,
            max_queued_tokens=64,
            max_seqs_in_prefill=4,
        ),
    )
    return service
# ---
def test_scale_up_creates_and_tracks_vm_group(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """scale_up() creates a VM group via VmManager and tracks it."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(scale_group_config, manager)

        new_vm_group = group.scale_up()

        manager.create_vm_group.assert_called_once()
        assert group.slice_count() == 1
        assert new_vm_group in group.vm_groups()
# ---
import re 
regex = '''^(25[0-5]|2[0-4][0-9]|[0-1]?[0-9][0-9]?)\.( 
			25[0-5]|2[0-4][0-9]|[0-1]?[0-9][0-9]?)\.( 
			25[0-5]|2[0-4][0-9]|[0-1]?[0-9][0-9]?)\.( 
			25[0-5]|2[0-4][0-9]|[0-1]?[0-9][0-9]?)$'''
def check_IP(Ip): 
	if(re.search(regex, Ip)): 
		return ("Valid IP address") 
	else: 
		return ("Invalid IP address")
# ---
def closeEvent(self, event):
        """
        Perform necessary operations before closing the window.
        """
        self.saveWindowState()
        #do any other thing before closing...
        event.accept()
# ---
def _get_checkpoint_steps(checkpoint_dir):
    paths = list(pathlib.Path(checkpoint_dir).iterdir())
    return sorted([_load_metadata(f)["step"] for f in paths])
# ---
import re 
def extract_max(input): 
	numbers = re.findall('\d+',input) 
	numbers = map(int,numbers) 
	return max(numbers)
# ---
def grad_func_name(self):
    """Its gradient function's name."""
    return self._grad_func.name if self._grad_func else None
# ---
def poll(poller, result):
    new_array_counts = get_array_counts_from_metrics(result)
    poller.update(new_array_counts)
    state = result.state
    if PipelineState.is_terminal(state):
        return
    else:
        # poll again in 5 seconds
        scheduler = poller.scheduler
        scheduler.enter(5, 1, poll, (poller, result))
# ---
def escape(self, text):
        return minimal_markdown_escape(text)
# ---
def connection_made(self, transport):
        """Called when reader thread is started"""
        self.port = transport.serial.port
        logger.debug('connection_made: `%s` `%s`', self.port, transport)
        self.transport = transport
        self.connected.set()
        self.disconnected.clear()
# ---
def __le__(self, other: "Duration") -> bool:
        return self._ms <= other._ms
# ---
def test_conf_extra_no_section(self):
        with mock.patch.object(memcache, 'ConfigParser',
                               get_config_parser(section='foobar')):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '127.0.0.1:11211')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, False)
        self.assertEqual(
            app.memcache._client_cache['127.0.0.1:11211'].max_size, 2)
# ---
def similar_elements(test_tup1, test_tup2):
  res = tuple(set(test_tup1) & set(test_tup2))
  return (res)
# ---
def exact(self, f):
        """Returns True if f is in .files()."""
        return f in self._fileset
# ---
def compute_advantages(self, rollout_group: list[Rollout]) -> np.ndarray:
        """Compute advantages for a group of rollouts."""
        ...
# ---
def bias_dropout_add_scale(
    x: torch.Tensor,
    bias: typing.Optional[torch.Tensor],
    scale: torch.Tensor,
    residual: typing.Optional[torch.Tensor],
    prob: float,
    training: bool) -> torch.Tensor:
  if bias is not None:
    out = scale * F.dropout(x + bias, p=prob, training=training)
  else:
    out = scale * F.dropout(x, p=prob, training=training)

  if residual is not None:
    out = residual + out
  return out
# ---
def _sympy_parse(expr: str):
    """Parses an expression with sympy."""
    py_expr = expr.replace("^", "**")
    return sympy_parser.parse_expr(
        py_expr,
        transformations=(
            *sympy_parser.standard_transformations,
            sympy_parser.implicit_multiplication_application,
        ),
    )
# ---
def register_rpc(self, handlers, module_name):
        # add the internal methods, note that this means they
        # can get clobbbered by subclass versions
        for meth in self.__base_methods:
            handlers["%s.%s" % (module_name, meth)] = self.__base_methods[meth]

        # register our module's handlers
        for name, handler in self.__list_handlers().items():
            handlers["%s.%s" % (module_name, name)] = handler
# ---
def obj_code_type(self):
                return Investment.CODE_TYPE_ISIN if is_isin_valid(Field('code')(self)) else NotAvailable
# ---
def test_describe_missing(self):
        """Describing a missing table returns None"""
        ret = self.dynamo.describe_table("foobar")
        self.assertIsNone(ret)
# ---
def peek(self):
		oldPos = self.tell()
		b = self.read(1)
		newPos = self.tell()
		if((newPos == (oldPos+1)) and (b != '')):
			r = ord(b)
		else:
			r = None

		self.seek(oldPos, 0)
		return r
# ---
def lugar(self):
        return self.__lugar
# ---
def _get_random_inputs(config: Olmo2Config, override_Pos=None):
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()

    return x, mask
# ---
def test_or_both_false(self):
        expr = (col("a") > 0) | (col("b") > 0)
        assert expr.evaluate({"a": -1, "b": -1}) is False
# ---

def even_odd_count(num):
    """Given an integer. return a tuple that has the number of even and odd digits respectively.

     Example:
        even_odd_count(-12) ==> (1, 1)
        even_odd_count(123) ==> (1, 2)
    """
    even_count = 0
    odd_count = 0
    for i in str(abs(num)):
        if int(i)%2==0:
            even_count +=1
        else:
            odd_count +=1
    return (even_count, odd_count)
# ---
def add_cluster_ip(self, sip):
        '''add cluster ip'''
        self.put(Service.cluster_ip, sip)
# ---
import cmath  
def convert(numbers):    
  num = cmath.polar(numbers)  
  return (num)
# ---
def error_page_401(status, message, traceback, version):
    html = '''<!DOCTYPE html>
<html lang="en">
<head>
  <title>My Radio Web Server</title>
  <meta name="generator" content="Vim">
  <meta charset="UTF-8">
</head>
<body>
   '''
    html += "<h1>%s</h1>" % (status)
    html += "%s<br>" % (message)

    return html
# ---
def stop_job(ctx, job_id):
    """Stop a running Ray job."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        _stop_job(job_id)
        print(f"Job {job_id} stop requested")
# ---
def checkpoint_sort_key(ckpt_dir):
        metadata = json.load(fs.open(os.path.join(ckpt_dir, "metadata.json")))
        return (datetime.datetime.fromisoformat(metadata["timestamp"]), metadata["step"])
# ---
def _mesh_context():
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape((len(devices),))
    mesh = jax.sharding.Mesh(mesh_devices, axis_names=("data",))
    with hax.partitioning.set_mesh(mesh):
        yield
# ---
def exception(self) -> BaseException | None:
        if not self._future.done():
            return None
        return self._future.exception()
# ---
def _iter_mesh_axes(spec_entry):
        if spec_entry is None or spec_entry is PartitionSpec.UNCONSTRAINED:
            return
        if isinstance(spec_entry, str):
            yield spec_entry
        else:
            for item in spec_entry:
                if item is None or item is PartitionSpec.UNCONSTRAINED:
                    continue
                yield item
# ---
def group_id(self) -> str:
        return self._group_id
# ---
def _invalidate_cache(self) -> None:
        self._cached_result = None
        self._client = None
        self._client_url = None
# ---
def check(self):
        pass
# ---
def _get_max_disk_size(populated_size, size):
    if populated_size is None:
        return size
    if size is None:
        return populated_size
    return str(max(int(populated_size), int(size)))
# ---
def json(self):
        if self.parsed_json is None:
            self.load_json()

        return self.parsed_json
# ---
def compute_logprobs_fn(m, b, k):
                    return chunked_compute_logprobs(m, b, k, self.vocab_tile_size)
# ---
def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Get the underlying model (handles DDP wrapping)
        return self._underlying.forward_once(x)
# ---
def test_dropout():
    H = Axis("H", 10)
    key = jrandom.PRNGKey(0)
    hax_dropout = hax.nn.Dropout(0.5)
    eqx_dropout = eqx.nn.Dropout(0.5)

    f = _compare_eqx_and_haliax(hax_dropout, eqx_dropout)
    out = f(hax.random.uniform(jrandom.PRNGKey(0), (H,)), key=key, inference=False)

    assert out.axes == (H,)
# ---
def _read_ovf_from_ova(ova_path):
    """
       virt-v2v support ova in tar, zip formats as well as
       extracted directory
    """
    if os.path.isdir(ova_path):
        return _read_ovf_from_ova_dir(ova_path)
    elif zipfile.is_zipfile(ova_path):
        return _read_ovf_from_zip_ova(ova_path)
    elif tarfile.is_tarfile(ova_path):
        return _read_ovf_from_tar_ova(ova_path)
    raise ClientError('Unknown ova format, supported formats:'
                      ' tar, zip or a directory')
# ---
import math
def surfacearea_sphere(r):
  surfacearea=4*math.pi*r*r
  return surfacearea
# ---
def unpermute_sharded(out_repeat_sort_: Array, sort_idx_: Array):
            inv_sort_idx_ = jnp.argsort(sort_idx_)
            out_repeat_ = jnp.take(out_repeat_sort_, inv_sort_idx_, axis=0)
            out_repeat_unflat_ = jnp.reshape(
                out_repeat_, (-1, self.config.num_experts_per_tok, self.config.hidden_dim)
            )

            return out_repeat_unflat_
# ---
def structure_all(self, tokens: np.ndarray, random: np.random.Generator):
        return [tokens]
# ---
def testDeleteGlobal(self):
    self.assertEqual((0, 'False\n'), _GrumpRun(textwrap.dedent("""\
        foo = 42
        del foo
        print 'foo' in globals()""")))
# ---
def make_bug_cache_key(self, repository, bug_id):
        """Returns a key to use when caching fetched bug information."""
        return 'repository-%s-bug-%s' % (repository.pk, bug_id)
# ---
def test_input_data(input_data):
    ds_input_validate(input_data)
# ---
def mock_the_cursor(cursor, *arg):
            arg[-1].get_result_proxy = Mock(return_value=Mock(context=arg[-1]))
            return retval
# ---
def convert_br(self, el, text, convert_as_inline):
        if convert_as_inline:
            return "<br>"

        if self.options["newline_style"].lower() == markdownify.BACKSLASH:
            return "\\\n"
        else:
            return "  \n"
# ---
def get_vm_logs(self, request: cluster__pb2.Controller.GetVmLogsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetVmLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def ceil(x, /):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in ceil")
    if x.dtype in _integer_dtypes:
        # Note: The return dtype of ceil is the same as the input
        return x
    return elemwise(nxp.ceil, x, dtype=x.dtype)
# ---
def test_sum_axis_0(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.sum(a, axis=0)
    assert_array_equal(b.compute(executor=executor), np.array([12, 15, 18]))
# ---
def convert_to_cache(self, value, record, validate=True):
        if isinstance(value, dict):
            # special case, when an integer field is used as inverse for a one2many
            return value.get('id', False)
        return int(value or 0)
# ---
def xla_impl_batched(x_batched: jax.Array) -> jax.Array:
    """Default implementation (XLA / plain JAX).

    For most kernels, this should be the same as the reference implementation.
    """

    return reference_impl_batched(x_batched)
# ---
def get_score_rank(self):
        return self.weight_rank
# ---
def ListJobs(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def allow_cancel(self, cr, uid, ids, context=None):
        for pick in self.browse(cr, uid, ids, context=context):
            if not pick.move_lines:
                return True
            for move in pick.move_lines:
                if move.state == 'done':
                    raise osv.except_osv(_('Error!'), _('You cannot cancel the picking as some moves have been done. You should cancel the picking lines.'))
        return True
# ---
def __init__(self, payload: bytes):
        super().__init__()
        self._payload = payload
# ---
def __init__(
        self,
    ):
        # Mongo params
        self.mongo_uri = None
        self.client = None
        self._core = None

        # eQ params
        self.CC = ComponentContribution()
        self.lc = None
        self._water = None
# ---
def test_variable_no_encryption(self):
        """
        Test variables without encryption
        """
        Variable.set('key', 'value')
        session = settings.Session()
        test_var = session.query(Variable).filter(Variable.key == 'key').one()
        self.assertFalse(test_var.is_encrypted)
        self.assertEqual(test_var.val, 'value')
# ---
def start_container(self, container_id: str) -> None:
        self._containers[container_id].start()
# ---
def _toy_example(Batch: Axis, Pos: Axis, Vocab: Axis, *, key: PRNGKeyArray) -> LmExample:
    tokens = hax.random.randint(key, (Batch, Pos), 0, Vocab.size)
    loss_weight = hax.ones((Batch, Pos), dtype=jnp.float32).at[Pos, Pos.size - 1].set(0.0)
    return LmExample(tokens=tokens, loss_weight=loss_weight, attn_mask=AttentionMask.causal())
# ---
def _is_int(x: float) -> bool:
    try:
        return abs(x - int(round(x))) <= 1e-7
    except (ValueError, OverflowError):
        return False
# ---
def optionally_tqdm(iterable, use_tqdm=True, **kwargs):
    return tqdm(iterable, **kwargs) if use_tqdm else iterable
# ---
def __contains__(self, key):
        return (key in self._dataset._variables
                and key not in self._dataset._coord_names)
# ---
def resource_spec():
    return cluster_pb2.ResourceSpecProto(cpu=4, memory_bytes=8 * 1024**3, disk_bytes=100 * 1024**3)
# ---
def state(self) -> cluster_pb2.JobState:
        """Get current job state (shortcut for status().state)."""
        return self.status().state
# ---
def init(cls, Vocab: Axis, config: WhisperConfig, *, key) -> "WhisperModel":
        k_t, k_embeddings = haliax.jax_utils.maybe_rng_split(key, 2)
        encoder = WhisperEncoder.init(config, key=k_embeddings)
        decoder = WhisperDecoder.init(config, key=k_t)

        return cls(encoder, decoder)
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if not self._m2.visitdir(dir):
            return self._m1.visitdir(dir)

        if self._m2.visitdir(dir) == "all":
            # There's a bug here: If m1 matches file 'dir/file' and m2 excludes
            # 'dir' (recursively), we should still visit 'dir' due to the
            # exception we have for exact matches.
            return False
        return bool(self._m1.visitdir(dir))
# ---
def text_renderer(node: RenderTreeNode, context: RenderContext) -> str:
        return node.content
# ---
def test_accelerator_type_name_handles_unknown():
    """Unknown accelerator types are marked clearly."""
    assert accelerator_type_name(999).startswith("UNKNOWN(")
# ---
def remember(self, request, principal, *kw):
		return []
# ---
def test_timestamp_uses_wall_clock():
    """Timestamp uses wall-clock time and advances correctly."""
    ts1 = Timestamp.now()
    time.sleep(0.02)  # 20ms
    ts2 = Timestamp.now()

    assert ts2.after(ts1)
    assert ts1.before(ts2)
    assert ts2 > ts1

    # Age should be at least 20ms
    age_ms = ts1.age_ms()
    assert age_ms >= 20
# ---
def test_simple_many_check_open_files(self, testdir):
        with lsof_check():
            with testdir.makepyfile("").open('wb+') as tmpfile:
                self.test_simple_many(tmpfile)
# ---
def test_concat_incompatible_shapes(spec):
    a = xp.full((4, 5), 1, chunks=(3, 2), spec=spec)
    b = xp.full((4, 6), 2, chunks=(3, 2), spec=spec)
    with pytest.raises(
        ValueError,
        match="all the input array dimensions except for the concatenation axis must match exactly",
    ):
        xp.concat([a, b], axis=0)
    xp.concat([a, b], axis=1)
# ---
def init(*, key):
            k1, k2 = jax.random.split(key)
            first = hnn.Linear.init(In, Mid, key=k1, out_first=True)
            second = hnn.Linear.init(Mid, In, key=k2, out_first=True)
            return Module(first, second)
# ---
def from_seconds(cls, epoch_seconds: float) -> "Timestamp":
        """Create timestamp from seconds since epoch."""
        return cls(int(epoch_seconds * 1000))
# ---
def merged_chunk_len_for_indexer(ia, c):
        if not isinstance(ia, ndindex.Slice):
            return c
        if ia.step == 1:
            return c
        if (c // ia.step) < 1:
            return c
        # note that this may not be the same as c
        # but it is guaranteed to be a multiple of the corresponding
        # value returned by chunk_len_for_indexer, which is required
        # by merge_chunks
        return (c // ia.step) * ia.step
# ---
def collate_inference_data(
    data: Sequence[InferenceDataset],
) -> tuple[InferenceDataset, int]:
    # TODO: There is probably a better way to do inference batching
    assert len(data) == 1, "Inference batch size must be 1"
    return data[0][0], data[0][1]
# ---
def testEmpty(self):
        hpcp = HPCP()([], [])
        self.assertEqualVector(hpcp, [0.]*12)
# ---
def search_query_builder(query):
    parsed_query = expr.parseString(query)[0]
    return _parsed_query2dict(parsed_query)
# ---
def _use_controller_table(self):
        """
            Set the resource and the table to be the imported resource
        """

        self.resource = self.controller_resource
        self.table = self.controller_table
        self.tablename = self.controller_tablename
# ---
def __init__(self, prev, as_single_chars=(), as_pattern_chars=()):
        self.prev = prev  # ContentOfGroup or CharClass
        self.single_chars = as_single_chars
        self.pattern_chars = as_pattern_chars
# ---
def matchfn(self, f):
        return True
# ---
def map(fn: Callable[..., T], tree: Any, *rest: Any, is_leaf: Callable[[Any], bool] | None = None) -> Any:
    """Alias for :func:`haliax.tree_util.tree_map` matching :func:`jax.tree.map`."""

    return tree_util.tree_map(fn, tree, *rest, is_leaf=is_leaf)
# ---
def create_zarr(a, /, store, *, dtype=None, chunks=None, path=None):
    # from dask.asarray
    if not isinstance(getattr(a, "shape", None), Iterable):
        # ensure blocks are arrays
        a = np.asarray(a, dtype=dtype)
    if dtype is None:
        dtype = a.dtype

    # write to zarr
    za = open_backend_array(
        store, mode="w", shape=a.shape, dtype=dtype, chunks=chunks, path=path
    )
    za[:] = a
    return za
# ---
def output_exemplar(self):
        return BatchEncoding({})
# ---
def test_composed_int(self):
        table = self.tables.some_table
        lx = (table.c.x + table.c.y).label("lx")
        self._assert_result(select([lx]).order_by(lx), [(3,), (5,), (7,)])
# ---
def test_task_fails_once_then_succeeds(cluster):
    """Container creation fails once, succeeds on retry."""
    _url, client = cluster
    enable_chaos(
        "worker.create_container",
        failure_rate=1.0,
        max_failures=1,
        error=RuntimeError("chaos: transient container failure"),
    )
    job = submit(client, _quick, "retry-once", max_retries_failure=2)
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def num_devices(self) -> int:
        """Get the number of devices."""
        return self.train_config.resources.chip_count()
# ---
def terminate(self) -> None:
        """Terminate this job."""
        self._client._cluster_client.terminate_job(self._job_id)
# ---
def action_traceability(self, cr, uid, ids, context=None):
        """ It traces the information of a product
        @param self: The object pointer.
        @param cr: A database cursor
        @param uid: ID of the user currently logged in
        @param ids: List of IDs selected
        @param context: A standard dictionary
        @return: A dictionary of values
        """
        return self.pool.get('action.traceability').action_traceability(cr,uid,ids,context)
# ---
def mp(self) -> jmp.Policy:
        """Returns the mixed precision policy"""
        return self.config.mp
# ---
def vocab_size(self) -> int:
        """Total vocabulary size including all special tokens."""
        return self.base_token_offset + self.base_vocab_size
# ---
def name(self):
        """Return the name of the sensor."""
        return f"{self.client_name} {self._name}"
# ---
def an_ifc_file_does_not_exist():
    ifc = IfcStore.get_file()
    if ifc:
        assert False, "An IFC is available"
# ---
def t_TASK(self, t):
        r'((?!\(x\))).+'
        return t
# ---
def remove_biblinks(html: BeautifulSoup):
    # Remove the biblinks since we are removing the biblio
    biblinks = html.findAll("a", {"class": "ltx_ref"})
    for biblink in biblinks:
        # Removes reference links
        # biblink.decompose()
        # Removes linking but keeps text
        biblink.unwrap()
# ---
def reset(self):
        self._count = 0
        self.requests = []
# ---
def set_current_client(client: Client) -> Generator[Client, None, None]:
    """Context manager that sets the current client and restores on exit."""
    token = _current_client_var.set(client)
    try:
        yield client
    finally:
        _current_client_var.reset(token)
# ---
def flatten_final_dims(t: torch.Tensor, no_dims: int):
    return t.reshape(t.shape[:-no_dims] + (-1,))
# ---
def status(self) -> VmGroupStatus:
        """Current status computed from VM states."""
        ...
# ---
def fake_get_rrd(host, vm_uuid):
            with open('xenapi/vm_rrd.xml') as f:
                return re.sub(r'\s', '', f.read())
# ---
def _maybe_fold_in_key(self, key, indices: Sequence[int]):
        if key is not None:
            key = _fold_in_key_vmap(key, np.array(indices))
        return key
# ---
def _prefix(kindpats):
    """Whether all the patterns match a prefix (i.e. recursively)"""
    for kind, pat, source in kindpats:
        if kind not in ("path", "relpath"):
            return False
    return True
# ---
def test_reduce_sum(backend):
    """Test basic sum reduction."""
    ds = Dataset.from_list(range(100)).reduce(sum)
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == sum(range(100))
# ---
def patch_sqlalchemy():
    mysqldb_hooks.install_patches()
    try:
        yield
    finally:
        mysqldb_hooks.reset_patches()
# ---
def get_address_balance(self, address: bytes) -> int:
        with self.lock:
            return self._state.get_address_balance(address)
# ---
def test_can_create_random_points(self):
    # Define an arbitrary region in which to compute random points.
    region = ee.Geometry.Rectangle(-119.224, 34.669, -99.536, 50.064)

    # Create 1000 random points in the region.
    random_points = ee.FeatureCollection.randomPoints(region)

    # Note: these random points have no system:index!
    df = dask_ee.read_ee(random_points)

    self.assertEqual(list(df.columns), ['geo'])
    self.assertEqual(df.compute().shape, (1000, 1))
# ---
def _interpret(self):
        if not self.max:
            return

        try:
            count = int("".join(self.max))
        except ValueError:
            assert False, "internal error: cannot convert to number maximum of repetition"
        self.repeat.between.max = count
# ---
def get_concurrency(cfg):
    """Return the Reserved Concurrent Executions if present in the config"""
    concurrency = int(cfg.get("concurrency", 0))
    return max(0, concurrency)
# ---
def _infer_model_name_for_path(model_path: str) -> str:
    """
    Infer model name from model path.
    """
    # path names are like gs://marin-us-central2/checkpoints/dclm_7b2x/hf/dclm_7b0828/dclm_7b0828/step-479999/
    # we want something like: dclm_7b0828_step-479999
    if model_path.endswith("/"):
        model_path = model_path[:-1]

    return "_".join(model_path.split("/")[-2:])
# ---
def get_daily_weight(self,day_of_week):
        return self.daily_weights[day_of_week]
# ---
def __len__(self):
        return self.size
# ---
def test_broadcast_trick():
    a = nxp.ones((10, 10), dtype=nxp.int8)
    b = broadcast_trick(nxp.ones)((10, 10), dtype=nxp.int8)

    assert_array_equal(a, b)
    assert a.nbytes == 100
    assert b.base.nbytes == 1

    a = nxp.ones((), dtype=nxp.int8)
    b = broadcast_trick(nxp.ones)((), dtype=nxp.int8)
    assert_array_equal(a, b)
# ---
def get_bias_dropout_add_scale(training):
  def _bias_dropout_add(x, bias, scale, residual, prob):
    return bias_dropout_add_scale(
      x, bias, scale, residual, prob, training)

  return _bias_dropout_add
# ---
def testImportNative(self):
    self.assertEqual((0, '1 1000000000\n'), _GrumpRun(textwrap.dedent("""\
        from "__go__/time" import Nanosecond, Second
        print Nanosecond, Second""")))
# ---
def test_filter_passing_partial_fail_excluded():
    """A candidate that passes only some tests should be excluded."""
    candidate = _make_candidate("def f(x):\n    return 1\n")
    tests = ["assert f(1) == 1", "assert f(2) == 2"]
    passing = filter_passing([candidate], tests)
    assert passing == []
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.setStyleSheet(self.STYLESHEET)
# ---
def __init__(self, *args, **kwargs):
        global scheduler
        scheduler = "Current scheduler is PyCOMPSs"
        self.task_instance = task(*args, **kwargs)
# ---
def bombardment_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'nucleic_acid_delivery_method': ['bombardment']
    }
# ---
def _unshaped_spec(store: ts.TensorStore) -> ts.Spec:
    spec = store.spec(retain_context=True)
    return spec
# ---
def _stop_job(job_id: str) -> None:
    """Stop a running Ray job.

    Note: This requires RAY_ADDRESS to be set, typically via ray_dashboard context manager.

    Args:
        job_id: The job ID or submission ID to stop
    """
    cmd = ["ray", "job", "stop", job_id]
    subprocess.check_output(cmd, text=True, timeout=60)
# ---
def __init__(self):
        self.mainFrame = gui.mainFrame.MainFrame.getInstance()
# ---
def _run_coroutine(self, coro):
        return thread_utils.blocking_wait(coro)
# ---
def __init__(
        self,
        device: torch.device,
        compute_metric: AreaWeightedFunction,
        n_timesteps: int,
    ):
        self._compute_metric = compute_metric
        self._total: torch.Tensor | None = None
        self._n_batches = torch.zeros(
            n_timesteps, dtype=torch.int32, device=get_device()
        )
        self._device = device
        self._n_timesteps = n_timesteps
# ---
def raw_args(self):
        return {k: v[0] for k, v in self.args.items()}
# ---
def _get_info_path(output_path: str) -> str:
    """Return the `path` of the info file associated with `output_path`."""
    return os.path.join(output_path, ".executor_info")
# ---
def resources(self):
        return self._resources
# ---
def min_of_three(a,b,c): 
      if (a <= b) and (a <= c): 
        smallest = a 
      elif (b <= a) and (b <= c): 
        smallest = b 
      else: 
        smallest = c 
      return smallest
# ---
def test_apply_gufunc_elemwise_01(spec):
    def add(x, y):
        return x + y

    a = cubed.from_array(np.array([1, 2, 3]), chunks=2, spec=spec)
    b = cubed.from_array(np.array([1, 2, 3]), chunks=2, spec=spec)
    z = apply_gufunc(add, "(),()->()", a, b, output_dtypes=a.dtype)
    assert_equal(z, np.array([2, 4, 6]))
# ---
def tearDown(self):
        super(TestMisc, self).tearDown()
        self.dynamo.default_return_capacity = False
# ---
def test_describe(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.describe()

        hpat_func = self.jit(test_impl)
        n = 1001
        hpat_func(n)
        # XXX: test actual output
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def backwards(self, orm):
        # Deleting field 'UserProject.drive_auth'
        db.delete_column(u'user_project', 'drive_auth')
# ---
import re
def check_literals(text, patterns):
  for pattern in patterns:
    if re.search(pattern,  text):
        return ('Matched!')
    else:
        return ('Not Matched!')
# ---
def tracker2(name):
            def go(*args, **kw):
                canary2.append(name)
            return go
# ---
def test_find_path_multi_step():
    source = "a = 1\nb = 2\nc = 3\n"
    target = "a = 10\nb = 20\nc = 30\n"
    path = find_path(source, target)
    assert len(path) >= 1

    current = source
    for mutation in path:
        current = mutation.apply(current)
    ast.parse(current)
# ---
def test_mean_axis_0(spec, executor):
    a = xp.asarray(
        [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], chunks=(2, 2), spec=spec
    )
    b = xp.mean(a, axis=0)
    assert_array_equal(
        b.compute(executor=executor),
        np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]).mean(axis=0),
    )
# ---
def normalize_phase(self,z_data,cal_phase):
		return z_data*np.exp(-1j*cal_phase)
# ---
def gameboard(size, seed=""):
        if size > 20:
            size = 20
        return render_template("gameboard.html", size=size, seed=seed)
# ---
def __init__(self):
        self.hooks = []
        self.jit_hooks = []
# ---
def _initialize(self):
        self._enabled = False
        self._initialized = False
        self.run = None
# ---
def cauchy(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.cauchy(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def dump(self, path: Path) -> None:
        """Dump the object to an NPZ file.

        Parameters
        ----------
        path : Path
            The path to the file.

        """
        np.savez_compressed(str(path), **asdict(self))
# ---
def __len__(self):
        """Calculate total number of batches across all groups."""
        total_batches = 0
        for sampler in self._samplers:
            total_batches += len(sampler)

        return total_batches
# ---
def hardbreak(node: RenderTreeNode, context: RenderContext) -> str:
    if _in_block("heading", node):
        return "<br /> "
    return "\\" + "\n"
# ---
def test_can_pack_simple_case():
    Pos = hax.Axis("pos", size=10)
    packer = SequencePacker(Pos=Pos, max_pack_size=2, pad_token=0)

    assert packer.can_pack([1, 2, 3]) is True
    packer.add_example(ids=[1, 2, 3], loss_weight=[1, 1, 1])
    assert packer.can_pack([4, 5]) is True
    assert packer.can_pack(list(range(6, 16))) is False
# ---
def __init__(self, artist, title, year=None, material=None):
        super(Sculpture, self).__init__(artist, title, year)
        self.__material = material
# ---
def num_primitive_ops(self) -> int:
        """Return the number of primitive operations in this plan."""
        return len(list(visit_nodes(self.dag)))
# ---
def test_asarray(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    assert_array_equal(
        a.compute(executor=executor), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    )
# ---
def run(
        self,
        cmd: list[str],
        *,
        check: bool = True,
        env: dict[str, str] | None = None,
        **kwargs,
    ) -> subprocess.CompletedProcess:
        """Run a command within the venv and wait for completion."""
        if env is None:
            env = self.get_env()
        return self._job_group.run(cmd, check=check, env=env, **kwargs)
# ---
def test_engine_module_name():
    engine = salt.engines.Engine({}, "foobar.start", {}, {}, {}, {}, name="foobar")
    assert engine.name == "foobar"
# ---
def test_apply_blockwise_iterator():
    bw_spec = make_blockwise_spec(
        key_function=make_combine_blocks_iter_key_function(
            "a", numblocks=5, split_every=2
        ),
        function=sum_iter,
    )

    input_data = {"a": [0, 1, 2, 3, 4]}
    out = [apply_blockwise(input_data, [i], bw_spec) for i in range(3)]
    assert out == [1, 5, 4]
# ---
def find_tuples(test_list, K):
  res = [sub for sub in test_list if all(ele % K == 0 for ele in sub)]
  return (str(res))
# ---
def foo(x):
        return nxp.broadcast_to(x[:, np.newaxis], (x.shape[0], 3))
# ---
def test_zero_centrality(self):
        G = nx.path_graph(3)
        prev_cc = nx.closeness_centrality(G)
        edge = self.pick_remove_edge(G)
        test_cc = nx.incremental_closeness_centrality(G, edge, prev_cc, insertion=False)
        G.remove_edges_from([edge])
        real_cc = nx.closeness_centrality(G)
        shared_items = set(test_cc.items()) & set(real_cc.items())
        assert len(shared_items) == len(real_cc)
        assert 0 in test_cc.values()
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: AttentionMask | NamedArray | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        """Compute the activations for the next token in a sequence."""
        x = self.embeddings.embed(input_ids)
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def threeSumClosest(self, nums: List[int], target: int) -> Optional[int]:
        res = None
        nums = sorted(nums)

        for i in range(len(nums)):
            for j in range(i + 1, len(nums)):
                res = bsearch(nums, j + 1, len(nums) - 1, res, i, j, target)
        return res
# ---
def add(x, y):
    assert isinstance(x, int)
    assert isinstance(y, int)
    return x + y
# ---
def __init__(self, want, have=None):
        self.want = want
        self.have = have
# ---
def _add_job(job_id, job):
    with _lock:
        if job_id in _jobs:
            raise JobExistsError("Job %r exists" % job_id)
        _jobs[job_id] = job
# ---
def record(self, tensor: torch.Tensor, i_time_start: int):
        """
        Update metric for a batch of data.
        """
        ...
# ---
def rpc_post(client, method, body=None):
    """Call a Connect RPC method via the test client."""
    return client.post(
        f"/iris.cluster.WorkerService/{method}",
        json=body or {},
        headers={"Content-Type": "application/json"},
    )
# ---
def main():
    print("Hello from ops!")
# ---
def autoscaler(self) -> "Autoscaler | None":
        """The autoscaler instance, if autoscaling is enabled."""
        return self._autoscaler
# ---
def pop(self):
        """Return the element at the front of the queue.

        Returns
        -------
        The first element in the queue.

        """
        return self._queue.pop(0)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        return self.device.device_flops(dtype)
# ---
def imag(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.imag, self.axes)
# ---
def vm_ops(self) -> PlatformOps:
        return _LocalPlatformOps()
# ---
def __iter__(self):
        """Iterate over the dataloader, converting RawTrainData to TrainData."""
        for raw_train_data in self._dataloader:
            dataset = self._datasets[raw_train_data.dataset_id]
            train_data = dataset.to_train_data(raw_train_data)
            yield train_data
# ---
def is_valid_ip(ip):
    """Returns true if the given string is a well-formed IP address.

    Supports IPv4 and IPv6.
    """
    try:
        res = socket.getaddrinfo(ip, 0, socket.AF_UNSPEC,
                                 socket.SOCK_STREAM,
                                 0, socket.AI_NUMERICHOST)
        return bool(res)
    except socket.gaierror as e:
        if e.args[0] == socket.EAI_NONAME:
            return False
        raise
    return True
# ---
def test_olmo3_attention_layer_type_detection(layer_idx):
    """Test that attention correctly detects its layer type."""
    config = _get_olmo3_config()
    attention = _get_olmo3_attention(config, layer_idx=layer_idx, key=random.PRNGKey(0))

    expected_sliding = (layer_idx + 1) % 4 != 0
    if expected_sliding:
        assert attention.config.sliding_window == config.sliding_window
    else:
        assert attention.config.sliding_window is None
# ---
def prepare_output_dirs(self) -> None:
        self.experiment.output_dir.mkdir(parents=True, exist_ok=True)
# ---
import bisect
def left_insertion(a, x):
    i = bisect.bisect_left(a, x)
    return i
# ---
def test_reshape_chunks(spec, executor):
    a = xp.arange(12, chunks=4, spec=spec)
    b = reshape_chunks(a, (2, 6), (2, 2))

    assert b.shape == (2, 6)
    assert b.chunks == ((2,), (2, 2, 2))

    assert_array_equal(
        b.compute(executor=executor),
        np.array([[0, 1, 4, 5, 8, 9], [2, 3, 6, 7, 10, 11]]),
    )
# ---
def backend_array_to_numpy_array(arr):
        return np.asarray(arr)
# ---
def test_llama_param_counts_dont_change_with_seqlen():
    model = LlamaLMHeadModel.init(hax.Axis("v", 2048), _get_llama_config(seq_len=128), key=random.PRNGKey(0))
    model2 = LlamaLMHeadModel.init(hax.Axis("v", 2048), _get_llama_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def close_db(error):
    """Closes connection with Mongo client"""
    if hasattr(g, 'mongo_client'):
        g.mongo_client.close()
# ---
def test_index_2d(spec, ind):
    a = xp.asarray(
        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]],
        chunks=(2, 2),
        spec=spec,
    )
    x = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])
    assert_array_equal(a[ind].compute(), x[ind])
# ---
def default_choice_name(cls) -> Optional[str]:
        return "adam"
# ---
def select_if_missing(missing_leaf, new_value):
            if isinstance(missing_leaf, jax.ShapeDtypeStruct):
                return new_value
            else:
                return None
# ---
def min_sum_path(A): 
	memo = [None] * len(A) 
	n = len(A) - 1
	for i in range(len(A[n])): 
		memo[i] = A[n][i] 
	for i in range(len(A) - 2, -1,-1): 
		for j in range( len(A[i])): 
			memo[j] = A[i][j] + min(memo[j], 
									memo[j + 1]) 
	return memo[0]
# ---
def __init__(self, fget):
        self.fget = fget
        self.func_name = fget.__name__
# ---
def parameter_norm(self, module):
        parameters = [p.norm(p=2) ** 2 for p in module.parameters() if p.requires_grad]
        if len(parameters) == 0:
            return torch.tensor(
                0.0, device="cuda" if torch.cuda.is_available() else "cpu"
            )
        norm = torch.stack(parameters).sum().sqrt()
        return norm
# ---
def _log_libtpu_args_once():
    global _LOGGED_LIBTPU_ARGS
    if not _LOGGED_LIBTPU_ARGS:
        args = os.environ.get("LIBTPU_INIT_ARGS", "<unset>")
        _logger.info("LIBTPU_INIT_ARGS (worker): %s", args)
        _LOGGED_LIBTPU_ARGS = True
# ---
from collections import defaultdict
def max_aggregate(stdata):
    temp = defaultdict(int)
    for name, marks in stdata:
        temp[name] += marks
    return max(temp.items(), key=lambda x: x[1])
# ---
def __eq__(self, other, /):
        other = self._check_allowed_dtypes(other, "all", "__eq__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.equal, self, other, dtype=nxp.bool)
# ---
def do_save(self, e):
        self.remote.do_save(self.get_match())
# ---
def test_compute_is_idempotent(spec, executor):
    a = xp.ones((3, 3), chunks=(2, 2), spec=spec)
    b = xp.negative(a)
    assert_array_equal(b.compute(executor=executor), -np.ones((3, 3)))
    assert_array_equal(b.compute(executor=executor), -np.ones((3, 3)))
# ---
def test_load_datasets_first(self):

        dataset_code = "nipa-section1-10101-a"
        self._load_files(dataset_code)
        self.assertLoadDatasetsFirst([dataset_code])
# ---
def test_parameter(self):
        param1 = ctds.Parameter(b'123', output=True)
        self.assertEqual(param1.value, b'123')
        self.assertTrue(isinstance(param1, ctds.Parameter))

        param2 = ctds.Parameter(b'123')
        self.assertEqual(param1.value, b'123')
        self.assertEqual(type(param1), type(param2))
        self.assertTrue(isinstance(param2, ctds.Parameter))
# ---
def heuristic_is_leaf(x):
    if isinstance(x, list):
        return jnp.isscalar(x[0])
    else:
        return False
# ---
def start_server(self, model: LmHeadModel) -> None:
        with hax.set_mesh(self.mesh), hax.axis_mapping(self.axis_mapping):
            self._inference_server = InferenceServer.create(
                self.inference_server_config,
                model=model,
                tokenizer=self.tokenizer,
            )
        self._inference_thread = threading.Thread(target=lambda: self._inference_server.serve(), daemon=True)
        self._inference_thread.start()
# ---
def _transition(self, new_state: vm_pb2.VmState) -> None:
        """Update state with timestamp and log the transition."""
        old_state = self.info.state
        self.info.state = new_state
        self.info.state_changed_at.CopyFrom(Timestamp.now().to_proto())
        if self._phase:
            self.info.init_phase = self._phase
        logger.info("VM %s: %s -> %s", self.info.vm_id, vm_state_name(old_state), vm_state_name(new_state))
# ---
def test_sentinel_file_reset(tmp_path: Path) -> None:
    """Test SentinelFile.reset removes the file."""
    sentinel = SentinelFile(str(tmp_path / "sentinel.txt"))

    # Signal then reset
    sentinel.signal()
    assert sentinel.is_set()

    sentinel.reset()
    assert not sentinel.is_set()

    # Reset is idempotent
    sentinel.reset()
    assert not sentinel.is_set()
# ---
def check_integer(text):
 text = text.strip()
 if len(text) < 1:
    return None
 else:
     if all(text[i] in "0123456789" for i in range(len(text))):
          return True
     elif (text[0] in "+-") and \
         all(text[i] in "0123456789" for i in range(1,len(text))):
         return True
     else:
        return False
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)

        return x
# ---
def vertical_ohc(ds):
    ohc_raw = raw_ohc(ds)
    ohc_intz = ohc_raw.weighted(ds.dz).sum("lev")
    # multiply by area to get Joules
    ohc_intz = ohc_intz * ds.areacello

    return ohc_intz
# ---
def timed(self, stat, sample_rate=1):
        log.debug('Entering timed context for %r' % (stat,))
        start = time.time()
        yield
        duration = int((time.time() - start) * 1000)
        log.debug('Exiting timed context for %r' % (stat,))
        self.timing(stat, duration, sample_rate)
# ---
def fetch_pipeline_from_server(self):
        """
        Method fetches pipeline from server/cloud
        """
        # TODO
        pass
# ---
def get_layer_types(self) -> Sequence[str]:
        if self.layer_types is not None:
            if len(self.layer_types) != self.num_layers:
                raise ValueError("layer_types must match num_layers")
            return list(self.layer_types)
        return ["sliding_attention" if (i + 1) % 4 != 0 else "full_attention" for i in range(self.num_layers)]
# ---
def foo():
            with profiler.Trace("foo"):
                raise ValueError("bar")
# ---
def get_logger(section, name):
    """
    Fetches a logger.

    Arguments:
        section (string): The section the logger is attributed to.
        name (string): The name of the logger.

    Returns:
        The logger corresponding to the section and name provided.
    """
    section_name = LoggingSection.reverse_mapping[section].lower()

    logger = logging.getLogger('htresearch.{0}.{1}'.format(section_name, name))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

    return logger
# ---
def subject(self, record):
        # in future we can vary the subject depending on the record details
        return 'Updating your Software Carpentry information'
# ---
def bind(self, existing, imported):
        [match1] = [m for m in self.matches if m[0] is existing]
        [match2] = [m for m in self.matches if m[1] is imported]
        assert match1[1] is None
        assert match2[0] is None
        match1[1] = match2[1]
        self.matches.remove(match2)
# ---
def default_window_title(self):
		"""()"""
		if TRACE: print(__name__), self.default_window_title.__doc__

		return "Stanford Bunny"
# ---
def test_client_with_resolver():
    server = ActorServer(host="127.0.0.1")
    server.register("echo", Echo())
    port = server.serve_background()

    try:
        resolver = FixedResolver({"echo": f"http://127.0.0.1:{port}"})
        client = ActorClient(resolver, "echo")

        assert client.echo("hello") == "echo: hello"
    finally:
        server.stop()
# ---
def minimize(self):
        """Minimize the window.
        """
        raise NotImplementedError('abstract')
# ---
def sort_on_occurence(lst): 
	dct = {} 
	for i, j in lst: 
		dct.setdefault(i, []).append(j) 
	return ([(i, *dict.fromkeys(j), len(j)) 
				for i, j in dct.items()])
# ---
def col_clause(self):
        pattern = (self.pattern
                   .replace('\\', '\\\\')
                   .replace('%', '\\%')
                   .replace('_', '\\_'))
        search = '%' + pattern + '%'
        clause = self.field + " like ? escape '\\'"
        subvals = [search]
        return clause, subvals
# ---
def run_func(input, pipeline_func=None, config=None, name=None, compute_id=None):
    result = pipeline_func(input, config=config)
    return result
# ---
def i_rename_the_object_name1_to_name2(name1, name2):
    the_object_name_exists(name1).name = name2
# ---
def __init__(
        self,
        mask: bool = False,
        mask_backbone: bool = False,
        mask_disto: bool = False,
    ) -> None:
        """Initialize the masker.

        Parameters
        ----------
        mask : bool
            Whether or not to mask the input features.
        """
        super().__init__()
        self.mask = mask
        self.mask_backbone = mask_backbone
        self.mask_disto = mask_disto
# ---
def __init__(
        self,
        raw_fn: ComputeLossFunction,
        mp: jmp.Policy,
        compute_axis_mapping: ResourceMapping,
    ):
        """
        Args:
            raw_fn: The underlying loss function
            mp: Mixed precision policy for casting
            compute_axis_mapping: Axis mapping for compute
        """
        self._raw_fn = raw_fn
        self._mp = mp
        self._compute_axis_mapping = compute_axis_mapping
# ---
def _transform(item, meta=dataset_meta):
            return wrap_transform(item, meta, cfg)
# ---
def from_hf_config(cls, hf_config: HfConfig):
        pass
# ---
def transition_task_to_running(state: ControllerState, task: ControllerTask) -> None:
    """Transition a task to RUNNING state via event."""
    state.handle_event(
        TaskStateChangedEvent(
            task_id=task.task_id,
            new_state=cluster_pb2.TASK_STATE_RUNNING,
            attempt_id=task.current_attempt_id,
        )
    )
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> Optional[float]:
        return None
# ---
def make_3d_coordinate_grid(lat, lon) -> torch.Tensor:
    """Makes a 3d Cartesian Coordinates on a unit sphere."""
    lat_lon_grid = lat_lon_meshgrid(lat, lon)  # [2, H, W]
    lat_rad = torch.deg2rad(lat_lon_grid[0])  # [H, W]
    lon_rad = torch.deg2rad(lat_lon_grid[1])  # [H, W]

    x = torch.cos(lat_rad) * torch.cos(lon_rad)
    y = torch.cos(lat_rad) * torch.sin(lon_rad)
    z = torch.sin(lat_rad)

    grid = torch.stack([x, y, z], dim=0)  # [3, H, W]
    return grid.float().unsqueeze(0)
# ---
def odd_Num_Sum(n) : 
    j = 0
    sm = 0
    for i in range(1,n+1) : 
        j = (2*i-1) 
        sm = sm + (j*j*j*j*j)     
    return sm
# ---
def test_accelerator_descriptor():
    from fray.v2.ray_backend.resources import accelerator_descriptor

    assert accelerator_descriptor(ResourceConfig(device=TpuConfig(variant="v4-8"))) == "v4-8"
    assert accelerator_descriptor(ResourceConfig(device=GpuConfig(variant="H100"))) == "H100"
    assert accelerator_descriptor(ResourceConfig(device=CpuConfig())) is None
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None: ...
# ---
def __init__(self, docker_image: str | None, docker_run_args: list[str] | None) -> None:
        self._docker_image = docker_image
        self._docker_run_args = docker_run_args
# ---
def on_load_checkpoint(self, checkpoint):
    if self.ema:
      self.ema.load_state_dict(checkpoint['ema'])
    # Copied from:
    # https://github.com/Dao-AILab/flash-attention/blob/main/training/src/datamodules/language_modeling_hf.py#L41
    self.fast_forward_epochs = checkpoint['loops'][
      'fit_loop']['epoch_progress']['current']['completed']
    self.fast_forward_batches = checkpoint['loops'][
      'fit_loop']['epoch_loop.batch_progress'][
        'current']['completed']
# ---
from collections import defaultdict
def grouping_dictionary(l):
    d = defaultdict(list)
    for k, v in l:
        d[k].append(v)
    return d
# ---
def word_len(s): 
    s = s.split(' ')   
    for word in s:    
        if len(word)%2==0: 
            return True  
        else:
          return False
# ---
def count_nonzero(x, /, *, axis=None, keepdims=False, split_every=None):
    dtype = nxp.__array_namespace_info__().default_dtypes(device=x.device)["indexing"]
    return sum(
        astype(x, nxp.bool),
        axis=axis,
        dtype=dtype,
        keepdims=keepdims,
        split_every=split_every,
    )
# ---
def test_is_cloneable_share_goodformat5(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://netapp.com/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def section(self, title: str):
        """Write a markdown section header."""
        self.log("")
        self._file.write(f"\n## {title}\n\n")
        self._file.flush()
        print("=" * 60, flush=True)
        print(f" {title}", flush=True)
        print("=" * 60, flush=True)
        self.log("")
# ---
def _create(self, fname):
        '''call oc create on a filename'''
        return self.openshift_cmd(['create', '-f', fname])
# ---
def _hard_sample(self, logits):
    pass
# ---
def test_numpy_inputs(self):
    if context.executing_eagerly():
      layer = keras.layers.RepeatVector(2)
      x = np.ones((10, 10))
      self.assertAllEqual(np.ones((10, 2, 10)), layer(x))

      layer = keras.layers.Concatenate()
      x, y = np.ones((10, 10)), np.ones((10, 10))
      self.assertAllEqual(np.ones((10, 20)), layer([x, y]))
# ---
def on_compute_end(self, event):
        self.logger_aware_progress.__exit__(None, None, None)
# ---
def stock_ledger_created(self):
		if not hasattr(self, '_stock_ledger_created'):
			self._stock_ledger_created = len(frappe.db.sql("""select name from `tabStock Ledger Entry`
				where item_code = %s limit 1""", self.name))
		return self._stock_ledger_created
# ---
def add_derived_variables(tensor_out: torch.Tensor) -> dict[str, torch.Tensor]:
    """
    Add derived variables to the output.
    """
    # Ocean heat content
    derived_vars = {}
    tensor_map = TensorMap.get_instance()
    dz = tensor_map.dz.to(tensor_out.device)
    thetao = tensor_out[:, :, tensor_map.VAR_3D_IDX["thetao"]]
    ohct = compute_ocean_heat_content(thetao, dz)
    derived_vars["ocean_heat_content"] = ohct

    return derived_vars
# ---
def _expand_or_placeholder(url):
        expanded = list(expand_glob(url))
        return expanded if expanded else [url]
# ---
def upload_python_script(notebook, data):
    code = notebook.get_code(map(int, data.get('code-lines', '').split('|')))
    files = {'file': (data['job-name'] + '.py', code)}
    return requests.post(
        SCRIPT_UPLOAD_URL_PATTERN % data['saagie-platform'],
        files=files, auth=SAAGIE_BASIC_AUTH_TOKEN).json()['fileName']
# ---
def check_health(self) -> bool:
        """Check if worker is healthy via health endpoint."""
        port = self._bootstrap_config.worker_port or 10001
        try:
            result = self._conn.run(f"curl -sf http://localhost:{port}/health", timeout=Duration.from_seconds(10))
            return result.returncode == 0
        except Exception:
            return False
# ---
def test_var__poorly_conditioned(spec):
    # from https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Example
    npa = np.array([4.0, 7.0, 13.0, 16.0]) + 1e9
    a = xp.asarray(npa, chunks=2, spec=spec)
    b = xp.var(a, axis=0)
    assert_array_equal(b.compute(), npa.var(axis=0))
# ---
def trim_to_size(self, size: int):
        """
        Trim the store to a given size.
        """
        # TODO These all return ts Futures so in theory we could await them all at once
        jtu.tree_map(lambda writer: writer.trim_to_size(size), self.tree, is_leaf=heuristic_is_leaf)
# ---
import re
def text_lowercase_underscore(text):
        patterns = '^[a-z]+_[a-z]+$'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def _drop_vars(self, names):
        self._assert_all_in_dataset(names)
        drop = set(names)
        drop |= set(k for k, v in iteritems(self._variables)
                    if any(name in v.dims for name in names))
        variables = OrderedDict((k, v) for k, v in iteritems(self._variables)
                                if k not in drop)
        coord_names = set(k for k in self._coord_names if k in variables)
        return self._replace_vars_and_dims(variables, coord_names)
# ---
def write_xml(self):
        'Write the XML code for the OPF file.'
        metadata = ''
        for elem in self.meta:
            metadata += elem.write_xml()
        manif = ''
        spine = ''
        guide = ''
        for finfo in self.filelist:
            manif += finfo.manifest_entry()
            spine += finfo.spine_entry()
            guide += finfo.guide_entry()
        return self._opf.format(metadata, manif, spine, guide)
# ---
def _to_torch(x):
        return torch.from_numpy(np.array(x))
# ---
def finalize(self) -> list[PhysicalStage]:
        """Flush remaining ops and return completed stages."""
        self.end_stage()
        return self.stages
# ---
def __init__(self, ssh_config: SshConfig, bootstrap: config_pb2.BootstrapConfig):
        self._ssh_config = ssh_config
        self._bootstrap = bootstrap
# ---
def test_repr_handles_partials():
    """__repr__ should unwrap functools.partial"""
    assert repr(MapOp(partial(int, base=2))) == "MapOp(fn=int)"

    def my_base(n: str, base: int = 10) -> int:
        return int(n, base)

    op = MapOp(partial(my_base, base=2))
    assert repr(op) == "MapOp(fn=test_repr_handles_partials.<locals>.my_base)"
# ---
def _dummy(v):
    return {"p": jnp.array(v, jnp.float32)}
# ---
def finger(self, match):
        self._call_all('finger', match)
# ---
def test_do_setup(self):
        mox = self.mox
        drv = self._driver
        mox.StubOutWithMock(netapp_nfs.NetAppNFSDriver, 'do_setup')
        mox.StubOutWithMock(drv, '_get_client')
        mox.StubOutWithMock(drv, '_do_custom_setup')
        netapp_nfs.NetAppNFSDriver.do_setup(IgnoreArg())
        drv._get_client()
        drv._do_custom_setup(IgnoreArg())

        mox.ReplayAll()

        drv.do_setup(IsA(context.RequestContext))

        mox.VerifyAll()
# ---
def record(self, target: torch.Tensor, gen: torch.Tensor, i_time_start: int):
        """
        Update metric for a batch of data.
        """
        ...
# ---
def __init__(self):
        self.thread_data = threading.local()
        self.thread_data.resource_mapping = None
# ---
from collections import OrderedDict
def remove_duplicate(string):
  result = ' '.join(OrderedDict((w,w) for w in string.split()).keys())
  return result
# ---
def open_builder(tree_path, item):
        item = np.asarray(item)
        rank = item.ndim
        render_tree_path = "/".join(_render_path_elem(x) for x in tree_path)
        return JaggedArrayStore.open(
            os.path.join(path, render_tree_path),
            mode=mode,
            item_rank=rank,
            dtype=item.dtype,
            cache_metadata=cache_metadata,
        )
# ---
import math
import sys
def sd_calc(data):
    n = len(data)
    if n <= 1:
        return 0.0
    mean, sd = avg_calc(data), 0.0
    for el in data:
        sd += (float(el) - mean)**2
    sd = math.sqrt(sd / float(n-1))
    return sd
def avg_calc(ls):
    n, mean = len(ls), 0.0
    if n <= 1:
        return ls[0]
    for el in ls:
        mean = mean + float(el)
    mean = mean / float(n)
    return mean
# ---
def even_Power_Sum(n): 
    sum = 0; 
    for i in range(1,n+1): 
        j = 2*i; 
        sum = sum + (j*j*j*j*j); 
    return sum;
# ---
def newline(self):
        """Rendering newline element."""
        return ''
# ---
def get_file_size(path):
    return os.path.getsize(path)
# ---
def filter_by(self, key, value, multiple=False):
        Util.validate_type(key, "str")
        Util.validate_type(multiple, "bool")
        return self._filter_by(key, value, multiple)
# ---
def setup():
            """Create some temporary files."""
            context.temp_dir = tempfile.mkdtemp()
            context.filenames = ["file_%03d" % i for i in range(files)]
            for filename in context.filenames:
                with open(os.path.join(context.temp_dir, filename), "w") as f:
                    f.write("This is the file %r.\n" % filename)
# ---
def __and__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer or boolean", "__and__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.bitwise_and, self, other, dtype=result_type(self, other))
# ---
def __repr__(self):
        return "<{0}: {1} {2}>".format(
            self.__class__.__name__, self.method, self.path
        )
# ---
def record_batch(self, batch):
        raise NotImplementedError(
            "Call record_validation_batch instead of record_batch"
        )
# ---
def get_num_train_steps(param_count, batch_size, seq_len):
    """Compute the number of steps for Chinchilla optimal training (20x params tokens)."""
    total_tokens = param_count * 20
    tokens_per_step = batch_size * seq_len
    return total_tokens // tokens_per_step
# ---
def test_empty_iteration(max_capacity):
    # Create a BackgroundIterable instance with an empty producer function
    background_iterable = BackgroundIterable(lambda: iter([]), max_capacity=max_capacity)

    # Convert the iterator to a list for comparison
    data = list(background_iterable)

    # Assert that the produced data is empty
    assert data == []
# ---
def unflatten_from_export(self: Mod, template: Mod) -> Mod:
        """
        Unflatten the module after import from torch.

        Template has the proper structure (e.g. articulated named axes) but the values are meaningless.
        """
        del template
        return self
# ---
def __call__(self, x):
                return self.second(self.first(x))
# ---
def test_rust_structs(benchmark: Any, in_memory_table: pa.Table) -> None:
    """
    Python Memory -> Converts to list of Rust 'Document' Classes -> Rust -> List of Rust Classes.
    """

    def _pipeline() -> int:
        docs = [dupekit.Document(row["id"], row["text"]) for row in in_memory_table.to_pylist()]
        return len(dupekit.process_rust_structs(docs))

    assert benchmark(_pipeline) > 0
# ---
def _add_vm_info(vm, params):
    params['vmName'] = vm.name()
    # TODO: use new API: vm.state()[0] == libvirt.VIR_DOMAIN_SHUTOFF
    #       when supported in Xen under RHEL 5.x
    if vm.isActive():
        params['status'] = "Up"
    else:
        params['status'] = "Down"
# ---
def command(self, name, *args):
        """ Execute a raw command """
        args = [name.encode('utf-8')] + [ (arg if type(arg) is bytes else str(arg).encode('utf-8'))
                for arg in args if arg is not None ] + [None]
        _mpv_command(self.handle, (c_char_p*len(args))(*args))
# ---
def streaming_dedup(items: Iterator[T]) -> Iterator[T]:
            """Deduplicate items within a shard."""
            seen = set()
            for item in items:
                k = key(item)
                if k not in seen:
                    seen.add(k)
                    yield item
# ---
def test_step_slicing():
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=2, dtype=jnp.float32)

        data = jnp.array([[1.0, 2.0], [3.0, 4.0]])
        builder.append(data)
# ---
def __repr__(self):
        return f'MultipleSort({self.sorts!r})'
# ---
def __init__(self):
        self.wake = Mock()
        self.kill_tasks_on_workers = Mock()
# ---
def build(
        self,
        bundle_path: Path,
        dockerfile: str,
        job_id: str,
        task_logs: TaskLogs | None = None,
    ) -> BuildResult: ...
# ---
def get_mac():
    """
    Gets a random mac address.
    """
    mac = ("%02x:%02x:%02x:%02x:%02x:%02x" %
                    (random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff),
                     random.randint(0x00, 0xff)))
    return mac
# ---
def specify_binding_not_binding(
        self,
        tokens: np.ndarray,
        contact_mask: np.ndarray,
        design_mask: np.ndarray,
        random: np.random.Generator,
    ):
        self.specify_binding(tokens, contact_mask, design_mask, random)
        self.specify_not_binding(tokens, contact_mask, design_mask, random)
# ---
def tearDown(self):
        crypto._fernet = None
# ---
def parse_timedelta(td_str) -> timedelta:
    td = timedelta(seconds=pytimeparse.parse(td_str))
    if td.total_seconds() < 0:
        raise ValueError("Cannot encode negative timedelta")  # not worth the trouble

    return td
# ---
def test_hf_gradient_fa():
    hf_config = HfGpt2Config.from_pretrained("gpt2")
    config = Gpt2Config.from_hf_config(hf_config)
    # keep block size small to make sure we test the tiling behavior
    config = dataclasses.replace(config, use_flash_attention=True, flash_attention_block_size=128)
    _compare_gpt2_checkpoint_gradients("gpt2", None, config=config)
# ---
def _unstack_matrices(stacked_arrays, revert_indices):
    in_tuple = isinstance(stacked_arrays, tuple)
    unstacked = []
    for arr in stacked_arrays:
        unstacked.extend(jnp.split(arr, arr.shape[0]))
    array_list = [jnp.squeeze(unstacked[i], axis=0) for i in revert_indices]
    if in_tuple:
        return tuple(array_list)
    return array_list
# ---
def run(self, config: OmegaConf) -> None:
        """Run the task.

        Parameters
        ----------
        config : OmegaConf
            The configuration for the task.

        """
        raise NotImplementedError
# ---
def test_multislice_one_slice_fails():
    """3. Run a function where one slice fails, verify retries and eventual failure."""
    num_slices = 2
    tpu_type = "v5litepod-4"

    with pytest.raises(RayTaskError) as excinfo:
        run_on_pod(
            fail_on_slice_0_fn,
            tpu_type,
            num_slices=num_slices,
            max_retries_failure=2,  # low retry
            max_retries_preemption=1,
        )

    assert "DeliberatelyRaisedException" in str(excinfo.value)
# ---
def _slice_multiplier(value) -> int:
    if isinstance(value, int):
        return value
    if value is None:
        return 1
    if isinstance(value, Iterable) and not isinstance(value, (str, bytes)):
        try:
            return max(value)
        except ValueError:
            return 1
    raise TypeError(f"Unsupported slice count value: {value!r}")
# ---
def namespace(self) -> Namespace:
        """Namespace derived from the root job ID.

        All jobs in a hierarchy share the same namespace, enabling actors
        to be discovered across the job tree.
        """
        if self.job_id is None:
            raise RuntimeError("No job id available - ensure IrisContext is initialized from a job")
        return Namespace.from_job_id(self.job_id)
# ---
def loadSelectedProfile(self):
        """Load selected profile"""

        activeProfile = self.getComboBoxList(self.profilesCombo)
        self.loadProfile(activeProfile)
# ---
def testAugAssign(self):
    self.assertEqual((0, '42\n'), _GrumpRun(textwrap.dedent("""\
        foo = 41
        foo += 1
        print foo""")))
# ---
def get_bundle(self, gcs_path: str, expected_hash: str | None = None) -> Path: ...
# ---
def neg_count(list):
  neg_count= 0
  for num in list: 
    if num <= 0: 
      neg_count += 1
  return neg_count
# ---
def updateColumns(self, database, additionalStatement: str = ""):

        if not self.dbFileExists:
            database.createTable(self.tableName, self.columnsDict, additionalStatement)
        else:
            try:
                database.deleteTable(self.tableName)
            except:
                database.createTable(self.tableName, self.columnsDict, additionalStatement)
# ---
def child(self, name: str) -> "JobName":
        """Create a child job name."""
        return JobName((*self._parts, name))
# ---
def accumulate_x_grad():
        res = jax.lax.dot_general(
            xw_scratch_ref[...],
            w_ref[...],
            (((1,), (1,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        x_read_future.wait()
        x_grad_tile_ref[...] += res
        x_write_future.start()
# ---
def loss_chunk(q_arr):
        qn = hax.named(q_arr, q.axes)
        out, _ = chunk_gated_delta_rule(qn, k, v, g, beta, chunk_size=4, output_final_state=False)
        return jnp.sum(out.array)
# ---
def test_build_env_flags_quotes_values_with_spaces(self):
        """Env var values with spaces are properly quoted."""
        config = config_pb2.BootstrapConfig(
            docker_image="gcr.io/test/iris:latest",
            worker_port=10001,
            env_vars={"MSG": "hello world"},
        )
        flags = _build_env_flags(config, vm_address="10.0.0.1")
        assert "MSG='hello world'" in flags or 'MSG="hello world"' in flags
# ---
def test_propagate_engine_to_connection(self):
        engine = testing_engine("sqlite://",
                        options=dict(execution_options={"foo": "bar"}))
        conn = engine.connect()
        eq_(conn._execution_options, {"foo": "bar"})
# ---
def copysign(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "copysign")
    if x1.dtype not in _real_numeric_dtypes or x2.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in copysign")
    return elemwise(nxp.copysign, x1, x2, dtype=result_type(x1, x2))
# ---
def minimum(a,b):   
    if a <= b: 
        return a 
    else: 
        return b
# ---
def KeyPos(self) -> Axis:
        return self.max_Pos.alias("key_position")
# ---
def list_images(self, params=None):
        """Returns a list of all images filtered by any parameters."""
        url = 'images'
        if params:
            url += '?%s' % urllib.urlencode(params)

        resp, body = self.get(url)
        body = json.loads(body)
        self.validate_response(schema.list_images, resp, body)
        return service_client.ResponseBodyList(resp, body['images'])
# ---
def failing_fn():
    print("Executing failing_fn. This should fail.")
    raise deliberately_raised_exception
# ---
def test_with_trace(self, mock_start, mock_stop):

        with profiler.Trace("a", info="a1"):
            mock_start.assert_called_once_with("a", info="a1")
            mock_start.reset_mock()
            with profiler.Trace("b", info="b1"):
                mock_start.assert_called_once_with("b", info="b1")
            mock_stop.assert_called_once_with()
            mock_stop.reset_mock()
        mock_stop.assert_called_once_with()
# ---
def flatten_axes(axis: AxisSpec, new_axis: AxisSelector) -> Axis:
    pass
# ---
def mock_scheduler():
    """Create a mock scheduler with wake() method."""
    return MockSchedulerWake()
# ---
def after_execute(conn, clauseelement, multiparams, params, result):
            assert isinstance(multiparams, (list, tuple))
            assert isinstance(params, dict)
# ---
def name(self):
        return self.name_ctrl.GetValue()
# ---
def __init__(self, handle: IrisActorHandle, method_name: str):
        self._handle = handle
        self._method = method_name
# ---
def _make_causal_mask(seq_len: int) -> Float[Array, "S S"]:
    """Create a causal attention mask.

    Returns a (seq_len, seq_len) boolean mask where True means "allowed to
    attend". Position i can attend to positions 0..i (inclusive).
    """
    return jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))
# ---
def disconnect(self):
        if ( self.connection ):
            try:
                self.connection.quit()
            except:
                pass
        self.connection = None
# ---
def __iter__(self) -> Iterator[T]:
        return iter(self._index_to_obj)
# ---
def scan(
    f: Callable[[Carry, X], tuple[Carry, Y]],
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable[[Carry, PyTree[X]], tuple[Carry, PyTree[Y]]]: ...
# ---
def test_passing_invalid_ip_address_throws_exception(self):
        with pytest.raises(InvalidIpAddressException):
            line = Line(line_contents=get_updated_line_contents({'ip_address': 'foobar'}))
# ---
def __init__(self, connection):
            self.standard = self.Standard(connection)
# ---
def transition(
        self,
        new_state: int,
        *,
        exit_code: int | None = None,
        error: str | None = None,
    ) -> None:
        """Transition this attempt to a new state."""
        self.state = new_state
        now = Timestamp.now()

        if new_state == cluster_pb2.TASK_STATE_RUNNING:
            self.started_at = now

        if new_state in TERMINAL_TASK_STATES:
            self.finished_at = now
            self.exit_code = exit_code
            self.error = error
# ---
def save_file(graph, fn):
    filename = "%s.png" % fn
    graph.write_png(filename)
    graph.write("%s.dot" % fn)
    os.startfile(filename)
# ---
def get_om4_gt(slice_time=True):
    ds_input = xr.open_zarr(
        os.path.join("/pscratch/sd/s/suryad/data", "OM4_5daily_v0.2.1.zarr")
    )
    ds_groundtruth = ds_input.sel(time=slice("1975-01-01", None))
    if slice_time:
        ds_groundtruth = ds_input.isel(time=slice(2903, 2903 + 600))

    return ds_groundtruth.isel(lev=slice(None, levels))
# ---
def address(self):
        """Get the full address the server is running on."""
        for server in self._server.servers:
            for sock in server.sockets:
                addr = sock.getsockname()
                host, port = addr[0], addr[1]
                # handle weird ipv6 localhost address which confuses clients
                if host == "::1" or host == ":1":
                    host = "localhost"
                return f"{host}:{port}"
        return None
# ---
def get_autoscaler_status(self) -> cluster_pb2.Controller.GetAutoscalerStatusResponse:
        """Get autoscaler status including recent actions and group states.

        Returns:
            GetAutoscalerStatusResponse proto with autoscaler status and recent actions
        """
        request = cluster_pb2.Controller.GetAutoscalerStatusRequest()
        return self._client.get_autoscaler_status(request)
# ---
def max_pages_per_seq(self) -> int:
        return (self.max_seq_len + self.page_size - 1) // self.page_size
# ---
def __init__(
        self,
        job_id: JobId,
        request: JobRequest,
        processes: list[subprocess.Popen],
        process_env: TemporaryVenv | None,
        start_time: float,
    ):
        self.job_id = job_id
        self.request = request
        self.processes = processes
        self.process_env = process_env
        self.replica_count = len(processes)
        self.start_time = start_time
        self.log_queue: Queue[str] = Queue()
        self._log_threads: list[Thread] = []
# ---
def setUp(self):
    self.schedule = schedule_parser.Schedule()
    self.schedule.Parse(SCHEDULE_PATH)
# ---
def ListEndpoints(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def count_list(input_list): 
    return (len(input_list))**2
# ---
def test_select_exists_false(self, connection):
        stuff = self.tables.stuff
        eq_(
            connection.execute(
                select([literal(1)]).where(
                    exists().where(stuff.c.data == "no data")
                )
            ).fetchall(),
            [],
        )
# ---
def test_connect_to_host_without_session(self):
        """Can connect to a dynamo host without passing in a session"""
        conn = DynamoDBConnection.connect("us-west-1", host="localhost")
        self.assertIsNotNone(conn.host)
# ---
def author(self):
        self._authors = []
# ---
def test_getitem_out_of_bounds():
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=2, dtype=jnp.float32)

        data = jnp.array([[1.0, 2.0], [3.0, 4.0]])
        builder.append(data)

        with pytest.raises(IndexError):
            builder[2]
# ---
def deposit(account, amount):
    account['balance'] += amount
    return account['balance']
# ---
def lookup_endpoint(self, request: cluster__pb2.Controller.LookupEndpointRequest, ctx: RequestContext) -> cluster__pb2.Controller.LookupEndpointResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def intersection(self, bbox, srs):
        bbox = self._bbox_in_coverage_srs(bbox, srs)
        intersection = (
            max(self.bbox[0], bbox[0]),
            max(self.bbox[1], bbox[1]),
            min(self.bbox[2], bbox[2]),
            min(self.bbox[3], bbox[3]),
        )

        if intersection[0] >= intersection[2] or intersection[1] >= intersection[3]:
            return None
        return BBOXCoverage(intersection, self.srs)
# ---
def nextafter(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.nextafter(x1, x2)
# ---
def from_iris_client(iris_client: IrisClientLib) -> FrayIrisClient:
        """Create a FrayIrisClient by wrapping an existing IrisClient.

        This avoids creating a new connection when we already have an IrisClient
        from the context (e.g., when running inside an Iris task).
        """
        instance = cast(FrayIrisClient, object.__new__(FrayIrisClient))
        instance._iris = iris_client
        return instance
# ---
def maximum_Sum(list1): 
    maxi = -100000
    for x in list1: 
        sum = 0 
        for y in x: 
            sum+= y      
        maxi = max(sum,maxi)     
    return maxi
# ---
def faq():
    """FAQ page for SciNet"""
    return render_template("faq.html")
# ---
def html_inline(node: RenderTreeNode, context: RenderContext) -> str:
    return node.content
# ---
def convert_ccd(name: str) -> Tuple[int, int, int, int, int]:
    """Convert a ccd code to a standard format.

    Parameters
    ----------
    name : str
        The atom name.

    Returns
    -------
    Tuple[int, int, int, int]
        The converted atom name.

    """
    name = name.strip().upper()
    name = [ord(c) - 32 for c in name]
    name = name + [0] * (5 - len(name))
    return tuple(name)
# ---
def main(cfg: DatasetConversionConfig) -> None:
    """CLI entrypoint."""
    hf_dataset_to_jsonl(cfg)
# ---
def __repr__(self):
        return f"IdentityMap({list(repr(x) for x in self._data.values())})"
# ---
def num_chunks(self) -> int:
        """Total number of chunks across all shards."""
        return len(self.source_items)
# ---
def Vocab(self) -> Axis:
        return Axis("vocab", self.grug_config.vocab_size)
# ---
def get_current_idx_list(data):
    idx = []
    for chain in data.chains:
        if chain.in_crop:
            for token in chain.tokens:
                if token.in_crop:
                    idx.extend(
                        [
                            chain.start_idx + token.start_idx + i
                            for i in range(len(token.atoms))
                        ]
                    )
    return idx
# ---
def forward(self, t):
    # Assume time goes from 0 to 1
    return self.total_noise(t), self.rate_noise(t)
# ---
def handle_time(self, channel, data):
        msg = Forseti.Time.decode(data)
        #wx.CallAfter(self.time_text.SetLabel, format_time(msg.game_time_so_far))
        wx.CallAfter(self.match_control.set_time, msg)
# ---
def test_unrescue_not_in_rescue(self):
        instance = self._create_instance()
        conn = xenapi_conn.get_connection(False)
        # Ensure that it will not unrescue a non-rescued instance.
        self.assertRaises(exception.InstanceNotInRescueMode, conn.unrescue,
                          instance, None)
# ---
def click_edit_button(self):
        """
        :rtype: EditMoviePage
        """
        self._click(BrowseMoviePageLocators.EDIT_BUTTON_LOCATOR)
        return EditMoviePage(self._driver)
# ---
def canonicalize(self, step: ExecutorStep) -> ExecutorStep:
        """Multiple instances of `ExecutorStep` might have the same version."""
        return self.version_str_to_step[self.version_strs[step]]
# ---
def remove_Occ(s,ch): 
    for i in range(len(s)): 
        if (s[i] == ch): 
            s = s[0 : i] + s[i + 1:] 
            break
    for i in range(len(s) - 1,-1,-1):  
        if (s[i] == ch): 
            s = s[0 : i] + s[i + 1:] 
            break
    return s
# ---
def _recreate(self, changes):
        """Recreate the window with current attributes.

        :Parameters:
            `changes` : list of str
                List of attribute names that were changed since the last
                `_create` or `_recreate`.  For example, ``['fullscreen']``
                is given if the window is to be toggled to or from fullscreen. 
        """
        raise NotImplementedError('abstract')
# ---
def weighted_mean_gradient_magnitude(
    tensor: torch.Tensor,
    weights: torch.Tensor | None = None,
    dim: tuple[int, ...] = (-2, -1),
) -> torch.Tensor:
    """Compute weighted mean of gradient magnitude across the specified dimensions."""
    return weighted_mean(gradient_magnitude(tensor, dim), weights=weights, dim=dim)
# ---
def _str_to_int(x_str: str) -> int:
    x_str = x_str.replace(",", "")
    x = float(x_str)
    return int(x)
# ---
def output(self, text, rules=None):
        self.tokens = self.block(text, rules)
        self.tokens.reverse()

        self.inline.setup(self.block.def_links, self.block.def_footnotes)

        out = self.renderer.placeholder()
        while self.pop():
            out += self.tok()
        return out
# ---
def build(self, ctx: LrScheduleContext):
        def schedule(step):
            tokens_trained = step * self.batch_size * self.seq_length
            return jnp.minimum(ctx.learning_rate, self.batch_size * self.a * tokens_trained**self.b)

        return schedule
# ---
def key_function(out_key):
        out_coords = out_key[1:]
        offset = block_id_to_offset(out_coords, template.numblocks)
        in_coords = offset_to_block_id(offset, x.numblocks)
        return (
            (x.name, *in_coords),
            (template.name, *out_coords),
        )
# ---
def __call__(self, array: NamedArray, axis: AxisSelector | None = None, **kwargs) -> NamedArray: ...
# ---
def validation_fn(model_weights, round_num):
    del round_num
    return evaluate_fn(model_weights, [cifar_test])
# ---
def tree_flatten(tree, is_leaf=None):
    """
    Version of [jax.tree_util.tree_flatten][] that automatically treats NamedArrays as leaves.
    """
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: is_leaf(x) or is_named_array(x)

    return jax.tree_util.tree_flatten(tree, is_leaf=is_leaf)
# ---
def validate_item_defaults(self):
		companies = list(set([row.company for row in self.item_defaults]))

		if len(companies) != len(self.item_defaults):
			frappe.throw(_("Cannot set multiple Item Defaults for a company."))
# ---
def test_mem_write_byte_char_before_attribute(self):
        self.mda.mem_write_byte(3998, 0xFF)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_GREEN, MDA_BLACK))
        self.mda.mem_write_byte(3999, MDA_ATTR_INTENSITY)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def _isnamedtupleinstance(x):
    t = type(x)
    b = t.__bases__
    if len(b) != 1 or b[0] is not tuple:
        return False
    f = getattr(t, "_fields", None)
    if not isinstance(f, tuple):
        return False
    return all(isinstance(n, str) for n in f)
# ---
def POST(self):
        form = web.input(name="Nobody",greet="Hello")

        greeting = "%s,%s" % (form.greet,form.name)

        return render.index(greeting = greeting)
# ---
def __post_init__(self):
        if len(self.ids) == 0:
            raise ValueError("PromptCompletion must have at least one token")

        # check that there is at least one token in the response
        if len(self.ids) <= self.prompt_length:
            raise ValueError(
                f"PromptCompletion must have strictly more tokens than the prompt length. Got {len(self.ids)} tokens"
                f" and prompt length {self.prompt_length}"
            )
# ---
def scheduler(state):
    return Scheduler(state)
# ---
def dec_refcounts_for_seq(pages_row, ref_counts):
            valid = is_valid(pages_row)

            def body(i, rc):
                def dec(rc):
                    page = pages_row["page", i].scalar()
                    return rc.at["page", page].add(-1)

                return jax.lax.cond(valid["page", i].scalar(), dec, lambda x: x, rc)

            updated = jax.lax.fori_loop(0, pages_row.axis_size("page"), body, ref_counts)
            return hax.maximum(updated, hax.zeros_like(updated))
# ---
def setup(self, env):
        """ Make sure that ``self`` is set up, except for recomputation triggers. """
        if not self.setup_done:
            if self.related:
                self._setup_related(env)
            else:
                self._setup_regular(env)
            self.setup_done = True
# ---
def time_until_next(self) -> float:
        """Get seconds until next allowed run (0.0 if can run now)."""
        if self._last_run is None:
            return 0.0
        elapsed = time.monotonic() - self._last_run
        return max(0.0, self._interval - elapsed)
# ---
def dispatch_loglikelihood(self, packed_request):
        self._send_message(_Message.LOGLIKELIHOOD)
        packed_request = self._send_payload(packed_request)
        return self.process_loglikelihood(packed_request)
# ---
def compute_ppo_loss(
    loss_objective: jax.Array,
    loss_masks: jax.Array,
) -> jax.Array:
    """Compute PPO loss (per-example normalization)."""
    return -1 * jnp.mean(jnp.sum(loss_objective * loss_masks, axis=1) / jnp.sum(loss_masks, axis=1))
# ---
def threadStopped(self, thread_num):
        self.connection_count -= 1
# ---
def open(read_server_info=True):
        return
# ---
def notnull_errcheck(res, func, *args):
    if res is None:
        raise RuntimeError('Underspecified error in MPV when calling {} with args {!r}: NULL pointer returned.'\
                'Please consult your local debugger.'.format(func.__name__, args))
    return res
# ---
def _interrupt_control_thread(self):
        """
        Signal the control flow thread that it needs to abort processing, likely due to a timeout.
        """
        self._ctrl_thread.proc.kill(exception=OperationInterruptedException, block=False)
# ---
def named_jit(
    fn: Callable[Args, R],
    axis_resources: ResourceMapping | None = None,
    *,
    in_axis_resources: ResourceMapping | None = None,
    out_axis_resources: ResourceMapping | None = None,
    donate_args: PyTree | None = None,
    donate_kwargs: PyTree | None = None,
    # args from jit
    keep_unused: bool = False,
    backend: str | None = None,
    inline: bool | None = None,
) -> WrappedCallable[Args, R]: ...
# ---
def trainer_config():
    return TrainerConfig()
# ---
def _field(self, name):
		"""
		Returns the field 'name'
		"""
		index = self.structure.index(name)
		return self.structure[index]
# ---
def test_shard_plain_array_in_module():
    with axis_mapping(resource_map):

        class MyModule(eqx.Module):
            array: jnp.ndarray

            def __init__(self):
                self.array = jnp.zeros((8, 8))

        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModule)()
            assert mod.array.sharding.is_fully_replicated
# ---
def custom_optimize_function(dag, array_names=None):
        # leave DAG unchanged
        return dag
# ---
def testDoubleRandomTranposeBoth(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(k, n, np.float64)
      y = self._randMatrix(m, k, np.float64)
      self._testCpuMatmul(x, y, True, True)
# ---
def to_rows(self, message):
        record_batch = self._parse_arrow_message(message)

        # Iterate through each column simultaneously, and make a dict from the
        # row values
        for row in zip(*record_batch.columns):
            yield dict(zip(self._column_names, row))
# ---
def on_compute_start(self, event):
        # store ops keyed by name
        self.ops = {}
        for name, node in visit_nodes(event.dag):
            primitive_op = node["primitive_op"]
            self.ops[name] = primitive_op

        # count number of times each op exceeds allowed mem
        self.counter = Counter()
# ---
def test_all_options(self):
        for optdict in [dict(nowrap=True),
                        dict(linenos=True),
                        dict(linenos=True, full=True),
                        dict(linenos=True, full=True, noclasses=True)]:

            outfile = StringIO.StringIO()
            fmt = HtmlFormatter(**optdict)
            fmt.format(tokensource, outfile)
# ---
def handle_request(self, request):
        res = super(System, self).handle_request(request)
        if res is not None: return res
# ---
def should_allow_eval(expr: str):
    # we don't want to try parsing unknown text or functions of more than two
    # variables
    if count_unknown_letters_in_expr(expr) > 2:
        return False

    for bad_string in BAD_SUBSTRINGS:
        if bad_string in expr:
            return False

    for bad_regex in BAD_REGEXES:
        if re.search(bad_regex, expr) is not None:
            return False

    return True
# ---
def open(self, request, *args, **kwargs):
        return super().create(request, *args, **kwargs)
# ---
def __init__(self, state: ControllerState):
        self._state = state
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        """Stop cluster and cleanup."""
        del exc_type, exc_val, exc_tb  # unused
        self._stop_jupyter()
        self._rpc_client = None
        if self._manager:
            self._manager.stop()
# ---
def all_equal(a: torch.Tensor, b: torch.Tensor) -> bool:
            return torch.all(a == b).item()
# ---
def __call__(self, model: M_con, *inputs: X, **input_kwargs) -> Scalar: ...
# ---
def test_loggamma():
    a = hax.arange(Width, start=0.1)

    check_gen_is_equal(
        lambda k, s: jax.random.loggamma(k, a.array.reshape(1, -1), shape=s), lambda k, s: hax.random.loggamma(k, s, a)
    )
# ---
def add(m1, m2, inplace):
    # The first operation in a checkpoint can't be in-place, but it's
    # nice to have in-place addition during inference. Thus...
    if not inplace:
        m1 = m1 + m2
    else:
        m1 += m2

    return m1
# ---
def check_health(self) -> bool:
        return True
# ---
def cli(ctx: click.Context) -> None:
    """Benchmark Zephyr deduplication pipeline."""
    if ctx.invoked_subcommand is None:
        # Default to benchmark command with default args
        ctx.invoke(benchmark)
# ---
def test_impl(df):
            return df.A.str.replace('AB*', 'EE', regex=True)
# ---
def test_works_after_dispose(self):
        eng = create_engine(testing.db.url)
        for i in range(3):
            eq_(eng.scalar(select([1])), 1)
            eng.dispose()
# ---
def parallel_lines(line1, line2):
  return line1[0]/line1[1] == line2[0]/line2[1]
# ---
def setup(self):
        self.add_copy_specs([
            "/proc/sysvipc/msg",
            "/proc/sysvipc/sem",
            "/proc/sysvipc/shm"
        ])
        self.add_cmd_output("ipcs")
# ---
def is_scanned_with_axis(leaf):
        if is_named_array(leaf):
            return selects_axis(leaf.axes, axis) and is_scanned(leaf)
        else:
            return is_scanned(leaf)
# ---
def fn(x):
        return x - 1
# ---
def _join_key(prefix: str, key: str) -> str:
    if prefix:
        return f"{prefix}.{key}"
    return key
# ---
import re  
regex = r'^[a-z]$|^([a-z]).*\1$'
def check_char(string): 
	if(re.search(regex, string)): 
		return "Valid" 
	else: 
		return "Invalid"
# ---
def _event_generator(handle):
    while True:
        event = _mpv_wait_event(handle, -1).contents
        if event.event_id.value == MpvEventID.NONE:
            raise StopIteration()
        yield event
# ---
def _default_destination_address(self, cr, uid, context=None):
        user = self.pool.get('res.users').browse(cr, uid, uid, context=context)
        return user.company_id.partner_id.id
# ---
def _eq_loaded(self):
        if self.lc:
            return True
        else:
            print("Load eQulibrator local cache.")
            return False
# ---
def status(self, job_or_id) -> dict:
        job_id = self._to_job_id_str(job_or_id)
        request = cluster_pb2.Controller.GetJobStatusRequest(job_id=job_id)
        assert self._controller_client is not None
        response = self._controller_client.get_job_status(request)
        return {
            "jobId": response.job.job_id,
            "state": cluster_pb2.JobState.Name(response.job.state),
            "exitCode": response.job.exit_code,
            "error": response.job.error,
        }
# ---
def delete_all(self):
        self._call_all('delete_all')
# ---
def test_delete_missing_snapshot(self):
        drv = self._driver
        mox = self._prepare_delete_snapshot_mock(False)

        drv.delete_snapshot(FakeSnapshot())

        mox.VerifyAll()
# ---
def remove_Char(s,c) :  
    counts = s.count(c) 
    s = list(s) 
    while counts :  
        s.remove(c) 
        counts -= 1 
    s = '' . join(s)   
    return (s)
# ---
def __init__(self, state):
        self._state = state
        self.tx_pool = TransactionPool(None)
        self._last_block = Block.deserialize(GenesisBlock().serialize())
        self.current_difficulty = StringToUInt256(str(config.user.genesis_difficulty))

        self.trigger_miner = False
        self.lock = threading.RLock()
# ---
def run(self):
        now = datetime.datetime.now()
        secondsInCurrentDay = now.hour * 3600 + now.minute * 60 + now.second
        if secondsInCurrentDay < self._timeOfDay:
            sleepDuration = self._timeOfDay - secondsInCurrentDay
        else:
            sleepDuration = self._timeOfDay + 3600 * 24 - secondsInCurrentDay
        logging.getLogger("main").info("sleeping for " + str(sleepDuration) + " seconds")
        self._waitIfNotStopped(sleepDuration)
        self._program.run()
# ---
def diff_even_odd(list1):
    first_even = next((el for el in list1 if el%2==0),-1)
    first_odd = next((el for el in list1 if el%2!=0),-1)
    return (first_even-first_odd)
# ---
def description(self):
        return self._description
# ---
def remaining(self):
        """int: Remaining items in the page."""
        return self._remaining
# ---
def main():
    argspec = hashivault_argspec()
    argspec['name'] = dict(required=True, type='str')
    argspec['mount_point'] = dict(required=False, type='str', default='approle')
    module = hashivault_init(argspec)
    result = hashivault_approle_role_get(module.params)
    if result.get('failed'):
        module.fail_json(**result)
    else:
        module.exit_json(**result)
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        return subprocess.run(self._build_cmd(command), capture_output=True, text=True, timeout=timeout.to_seconds() + 5)
# ---
def main(config: Config):
        assert config.data is not None
# ---
def __init__(self, randomize):
    self.idx = 0
    self.files = os.listdir(PARAMS["PATH"])

    if len(self.files) == 0:
      raise EnvironmentError("No file available")

    self.files.sort()

    if randomize:
      print("RANDOMIZE")
      random.shuffle(self.files)
# ---
def func(cls):
                return cls.run_test(code)
# ---
def get_mapping(self, index, doc_type=None):
        return get_indices(self.es).get_mapping(index=index, doc_type=doc_type)
# ---
def sleep_forever():
        while True:
            time.sleep(1)
# ---
def deterministic_failure_modal_long_timeout(
    i, path=None, timing_map=None, *, name=None
):
    return deterministic_failure(path, timing_map, i, name=name)
# ---
def clean_html(html: BeautifulSoup | str) -> str:
    if isinstance(html, str):
        html = BeautifulSoup(html, "html.parser")
    remove_authors(html)
    remove_title_page(html)
    clean_li(html)
    remove_biblio(html)
    remove_footnotes(html)
    remove_biblinks(html)
    linelisting_to_newline(html)
    deconstruct_eqn(html)
    remove_ar5iv_footer(html)
    remove_before_section(html)
    remove_title(html)
    return str(html)
# ---
def service(worker):
    """Create WorkerServiceImpl."""
    return WorkerServiceImpl(provider=worker)
# ---
def polyder(p: NamedArray | ArrayLike, m: int = 1) -> NamedArray:
    """Named version of [jax.numpy.polyder][].

    If ``p`` is not a [haliax.NamedArray][], the differentiated polynomial uses a
    coefficient axis named ``degree``.
    """

    (arr,) = unwrap_namedarrays(p)
    result = jnp.polyder(arr, m=m)
    axis = _poly_axis_from_input(p, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def iter_containing(self, name):
        ctx_dict = object.__getattribute__(self, '__dict__')
        if name in ctx_dict:
            yield self
        parent = ctx_dict.get('_parent')
        if parent is not None:
            for ancestor in parent.iter_containing(name):
                yield ancestor
# ---
def add_dict_to_tuple(test_tup, test_dict):
  test_tup = list(test_tup)
  test_tup.append(test_dict)
  test_tup = tuple(test_tup)
  return (test_tup)
# ---
def atomIndexInResidue(residue):
  """ list of atom index in residue """
  index=[]
  for a in list(residue.atoms()):
    index.append(a.index)
  return index
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> Optional[float]:
        # TODO: implement
        return None
# ---
def output_exemplar(self):
        return {"test": np.array([0], dtype=np.int64)}
# ---
def from_proto(cls, proto: "time_pb2.Duration") -> "Duration":
        """Create from proto Duration message."""
        return cls(proto.milliseconds)
# ---
def clean_li(html: BeautifulSoup):
    # Remove the li tags since they repeat the same information (eg 1. 1.)
    tags = html.findAll("span", {"class": "ltx_tag_item"})
    for tag in tags:
        tag.decompose()
    tags = html.findAll("span", {"class": "ltx_tag_listingline"})
    for tag in tags:
        tag.decompose()
# ---
def upper_ctr(str):
    upper_ctr = 0
    for i in range(len(str)):
          if str[i] >= 'A' and str[i] <= 'Z': upper_ctr += 1
          return upper_ctr
# ---
def _fill_queue_with_batches(self):
        with local_cpu_mesh():
            super()._fill_queue_with_batches()
# ---
def read(self):
        '''
        Read and parse MIDI data stored in a file.
        '''
        self.readstr(self.file.read())
# ---
def filter_lora_params(params: M) -> M:
    """
    Filters LoRA parameters from the given parameter tree.
    """

    return eqx.filter(params, is_lora_param, is_leaf=lambda x: is_lora_param(x) or isinstance(x, haliax.NamedArray))
# ---
def fn(config: Dataclass | None):
        append_log(log, config)
# ---
def dataset(self):
        return self._dataloader.dataset
# ---
def _print_batch(train_ds, valid_ds, tokenizer, k=64):
  for dl_type, dl in [
    ('train', train_ds), ('valid', valid_ds)]:
    print(f'Printing {dl_type} dataloader batch.')
    batch = next(iter(dl))
    print('Batch input_ids.shape', batch['input_ids'].shape)
    first = batch['input_ids'][0, :k]
    last = batch['input_ids'][0, -k:]
    print(f'First {k} tokens:', tokenizer.decode(first))
    print('ids:', first)
    print(f'Last {k} tokens:', tokenizer.decode(last))
    print('ids:', last)
# ---
def replace_list(list1,list2):
 list1[-1:] = list2
 replace_list=list1
 return replace_list
# ---
def assign_task_to_worker(state: ControllerState, task: ControllerTask, worker_id: WorkerId) -> None:
    """Assign a task to a worker via event."""
    state.handle_event(
        TaskAssignedEvent(
            task_id=task.task_id,
            worker_id=worker_id,
        )
    )
# ---
def _get_x_index(self, idx: int, step: int) -> xr.DataArray:
        assert isinstance(idx, int)
        if idx < 0:
            raise IndexError("Sorry, negative indexing is not supported!")
        if idx >= len(self):
            raise IndexError("Index out of range")

        window_index = idx + step * (self.hist + 1) * self.stride
        return self.rolling_indices.isel(window=window_index, drop=True)
# ---
def __getstate__(self) -> dict:
        # Only serialize the endpoint name - client is lazily resolved
        return {"endpoint_name": self._endpoint_name}
# ---
def ship(self, params=None):
        if params is None:
            params = dict()
        params['sale_id'] = self.sale_id
        return Sale(Api.call('sales/mark_shipped', params))
# ---
def test_classify_textfiles_to_db(mock_rfw, mock_rw, mock_jw):
    classify_documents.classify_textfiles_to_db(0, 'test')

    assert mock_rfw.called
    assert mock_rw.called
    assert mock_jw.called
# ---
def execute(self):
        raise NotImplementedError("Subclass must implement this")
# ---
def test_success(tmp_path, timing_map, n_tasks, retries, use_backups):
    path = f"{BASE_PATH}/{tmp_path.name}"
    outputs = asyncio.run(
        run_test(
            app_function=deterministic_failure_modal,
            input=range(n_tasks),
            use_backups=use_backups,
            path=path,
            timing_map=timing_map,
        )
    )

    assert outputs == set(range(n_tasks))
    check_invocation_counts(path, timing_map, n_tasks, retries)
# ---
def test_plain_union(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2)
        s2 = select([table]).where(table.c.id == 3)

        u1 = union(s1, s2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def fake_vdi_resize(*args, **kwargs):
            called['resize'] = True
# ---
def scale_group_config() -> config_pb2.ScaleGroupConfig:
    """A standard scale group configuration for tests."""
    return config_pb2.ScaleGroupConfig(
        name="test-group",
        min_slices=1,
        max_slices=5,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def __init__(self, query, explanation):
        if isinstance(query, list):
            query = " ".join(query)
        message = f"'{query}': {explanation}"
        super().__init__(message)
# ---
def nancumprod(a: NamedArray, axis: AxisSelector, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.nancumprod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nancumprod.html)
    """
    return wrap_axiswise_call(jnp.nancumprod, a, axis, dtype=dtype, single_axis_only=True)
# ---
def vm_name(zone: str) -> str:
    """Generate VM name from zone using fun naming."""
    return f"{config.TPU_VM_PREFIX}-{generate_fun_name(zone)}"
# ---
def __str__(self):
        return '%s (%s)' % (self.name, self.sno.short_name)
# ---
def parse():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument('--bootstrap', action='store_true')
    parser.add_argument('--no-shell-file', action='store_true')
    return parser.parse_args()
# ---
def __getitem__(self, key):
        return self._data[id(key)][1]
# ---
def test_eye(spec, k):
    a = xp.eye(5, k=k, chunks=(2, 2), spec=spec)
    assert_array_equal(a, np.eye(5, k=k))
# ---
def merge(a,b):
    c = []
    while len(a) != 0 and len(b) != 0:
        if a[0] < b[0]:
            c.append(a[0])
            a.remove(a[0])
        else:
            c.append(b[0])
            b.remove(b[0])
    if len(a) == 0:
        c += b
    else:
        c += a
    return c
def merge_sort(x):
    if len(x) == 0 or len(x) == 1:
        return x
    else:
        middle = len(x)//2
        a = merge_sort(x[:middle])
        b = merge_sort(x[middle:])
        return merge(a,b)
# ---
def __init__(self, rules):
        """Initialize the 'and' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules
# ---
def my_dict(dict1):
  if bool(dict1):
     return False
  else:
     return True
# ---
def create_zarr_array(lazy_zarr_array, *, config=None):
    """Stage function for create."""
    lazy_zarr_array.create(mode="a")
# ---
def appvms(self):
        ''' Returns a generator containing all domains based on the current
            TemplateVM.
        '''
        for vm in self.app.domains:
            if hasattr(vm, 'template') and vm.template is self:
                yield vm
# ---
def set_grad_func(self, grad_func):
    """Specifies the gradient function of this function."""
    assert not self._grad_func
    assert isinstance(grad_func, _DefinedFunction)
    self._grad_func = grad_func
# ---
def _build_cluster_images(config) -> None:
    for tag, typ in [(config.defaults.bootstrap.docker_image, "worker"), (config.controller.image, "controller")]:
        if tag:
            params = _extract_image_params(tag, typ)
            if params:
                _build_and_push_image(params)
                click.echo()
# ---
def _arg_aggregate(a):
    # just return index values
    return a["i"]
# ---
def digits(self):
        if callable(self._digits):
            with fields._get_cursor() as cr:
                return self._digits(cr)
        else:
            return self._digits
# ---
def create_configuration():
    configuration = mox.MockObject(conf.Configuration)
    configuration.append_config_values(mox.IgnoreArg())
    configuration.nfs_mount_point_base = '/mnt/test'
    configuration.nfs_mount_options = None
    return configuration
# ---
def gradient_norm(self, module):
        parameters = [
            p.grad.norm(p=2) ** 2
            for p in module.parameters()
            if p.requires_grad and p.grad is not None
        ]
        if len(parameters) == 0:
            return torch.tensor(
                0.0, device="cuda" if torch.cuda.is_available() else "cpu"
            )
        norm = torch.stack(parameters).sum().sqrt()
        return norm
# ---
def copy_to(self, parameters):
    """
    Copy current parameters into given collection of parameters.

    Args:
        parameters: Iterable of `torch.nn.Parameter`; the parameters to be
            updated with the stored moving averages.
    """
    parameters = [p for p in parameters if p.requires_grad]
    for s_param, param in zip(self.shadow_params, parameters):
      if param.requires_grad:
        param.data.copy_(s_param.data)
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        matched = self._matcher.match_relative(dir, True)
        if matched:
            # Everything in the directory is selected (ignored)
            return "all"
        else:
            # Not sure
            return True
# ---
def open(self, req, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
        self.req, self.data, self.timeout = req, data, timeout
# ---
def worker_metadata():
    return cluster_pb2.WorkerMetadata(
        hostname="test-host",
        ip_address="192.168.1.1",
        cpu_count=8,
        memory_bytes=16 * 1024**3,
        disk_bytes=100 * 1024**3,
    )
# ---
def _concatenate(x):
    if isinstance(x[0], np.ndarray):
        return np.concatenate(x)
    return jnp.concatenate(x)
# ---
def testIf(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        if 123:
          print 'foo'
        if '':
          print 'bar'""")))
# ---
def __repr__(self) -> str:
        return f"({self.left} {_COMPARE_SYMBOLS[self.op]} {self.right})"
# ---
def _sum_reduction_func(arr, by, axis, start_group, num_groups, groupby_dtype):
    # change 'by' so it starts from 0 for each chunk
    by = by - start_group
    return npg.aggregate(
        by, arr, func="sum", dtype=groupby_dtype, axis=axis, size=num_groups
    )
# ---
def get_git_commit():
    """Get the current git commit hash."""
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("utf-8").strip()
# ---
def loss_fn(x_in, w_in, y_in):
        return fused_cross_entropy_loss_and_logsumexp_penalty(
            x_in,
            y_in,
            w_in,
            reduction="mean",
            logsumexp_weight=0.0,
            block_sizes=block_sizes,
            dtype=jnp.float32,
            logit_soft_cap=None,
            implementation="xla",
        )
# ---
def num_cpus(self) -> int:
        return 1
# ---
def initialize(self, resolver, mapping):
        self.resolver = resolver
        self.mapping = mapping
# ---
def test_conf_from_extra_conf_bad_max_conn(self):
        with mock.patch.object(memcache, 'ConfigParser', get_config_parser(
                memcache_max_connections='bad42')):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '1.2.3.4:5')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, True)
        self.assertEqual(
            app.memcache._client_cache['1.2.3.4:5'].max_size, 2)
# ---
def _check_worker_healthy(self) -> bool:
        """Check if worker container is already running and healthy."""
        port = self._bootstrap_config.worker_port or 10001
        try:
            result = self._conn.run(f"curl -sf http://localhost:{port}/health", timeout=Duration.from_seconds(10))
            return result.returncode == 0
        except Exception:
            return False
# ---
def setHeight(self, height):
        self.__height = height
# ---
def _handle_func(name, args, restype, errcheck, ctx=MpvHandle):
    func = getattr(backend, name)
    func.argtypes = [ctx] + args if ctx else args
    if restype is not None:
        func.restype = restype
    if errcheck is not None:
        func.errcheck = errcheck
    globals()['_'+name] = func
# ---
def __del__(self):
        """Ensure cleanup when object is destroyed."""
        try:
            self.kill()
        except Exception:
            pass
# ---
def test_run_blocking_with_error():
    with TemporaryVenv() as venv:
        with pytest.raises(subprocess.CalledProcessError):
            venv.run([venv.python_path, "-c", "import sys; sys.exit(1)"])
# ---
def _ensure_is_array(x):
        if isinstance(x, (int, float, bool, complex)):
            return jnp.array(x)
        else:
            return x
# ---
def __init__(self, config: CurriculumConfig):
        self._server = ActorServer(host="localhost")
        self._server.register("curriculum", Curriculum(config))
        self._server.serve_background()
# ---
def summary(self) -> str:
        if self.healthy:
            return "healthy"
        parts = []
        if self.container_status:
            parts.append(f"container={self.container_status}")
        if self.curl_error:
            parts.append(f"curl_error={self.curl_error[:50]}")
        return ", ".join(parts) if parts else "unknown failure"
# ---
def _poll_loop(self):
        """Poll task status every 5 seconds."""
        while not self._stop_event.is_set():
            for job_id in list(self._tracked_jobs):
                try:
                    self._poll_job_tasks(job_id)
                except Exception as e:
                    self._logger.log(f"Error polling job {job_id}: {e}", level="WARN")
            self._stop_event.wait(SCHEDULING_POLL_INTERVAL_SECONDS)
# ---
def flops_per_token(self, vocab_size: int, context_length: int):
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=True,
        )
# ---
def findFirstColumnEmpty(self):
        """
        Implements function to get the first column where a slot remain.
        :return: the column
        """
        for col in xrange(7):
            if self.grille[0][col] == '0':
                return col
        return -1
# ---
def _can_scan(layer_types: Sequence[str]) -> bool:
        """Check if all layers have the same type (can use Stacked scan)."""
        return len(set(layer_types)) <= 1
# ---
def __init__(self, rules=None, default_rule=None):
        """Initialize the Rules store."""

        super(Rules, self).__init__(rules or {})
        self.default_rule = default_rule
# ---
def _get_discount_invoice(self, cr, uid, move_line):
        '''Return the discount for the move line'''
        return 0.0
# ---
def set_loglevel(self, level):
        _mpv_request_log_messages(self._event_handle, level.encode('utf-8'))
# ---
def cleanup(self) -> None:
        """Cleanup transfer server and thread pool."""
        logger.info("Cleaning up JAX transfer client")
        self.executor.shutdown(wait=True)
        self.transfer_server = None
# ---
def is_job_finished(state: int) -> bool:
    return state in (
        cluster_pb2.JOB_STATE_SUCCEEDED,
        cluster_pb2.JOB_STATE_FAILED,
        cluster_pb2.JOB_STATE_KILLED,
        cluster_pb2.JOB_STATE_WORKER_FAILED,
        cluster_pb2.JOB_STATE_UNSCHEDULABLE,
    )
# ---
def create_transaction(node, txid, to_address, *, amount):
    """ Return signed transaction spending the first output of the
        input txid. Note that the node must have a wallet that can
        sign for the output that is being spent.
    """
    raw_tx = create_raw_transaction(node, txid, to_address, amount=amount)
    tx = tx_from_hex(raw_tx)
    return tx
# ---
def execute(self):
        with self._volumes():
            yield self._start_helper()
# ---
def poly(seq_of_zeros: NamedArray | ArrayLike) -> NamedArray:
    """Named version of [jax.numpy.poly][].

    If ``seq_of_zeros`` is not a [haliax.NamedArray][], the returned coefficient axis
    is named ``degree``.
    """

    (roots,) = unwrap_namedarrays(seq_of_zeros)
    result = jnp.poly(roots)
    axis = _poly_axis_from_input(seq_of_zeros, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def init_weights(self):
        """
        If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any
        initialization logic in `_init_weights`.
        """

        # Initialize weights
        self.apply(self._initialize_weights)
# ---
def mini_transform(mean_axis):
                components = []
                components.append(scale_with_mini(self.beta1, self.beta2, self.epsilon, mean_axis=mean_axis))
                if self.weight_decay > 0:
                    components.append(optax.add_decayed_weights(self.weight_decay, self.build_weight_decay_mask()))
                components.append(optax.scale(-learning_rate))
                optimizer = optax.chain(*components)
                return optimizer
# ---
import cmath
def angle_complex(a,b):
  cn=complex(a,b)
  angle=cmath.phase(a+b)
  return angle
# ---
def __init__(self):
        self.flat_map_count = 0
        self.map_count = 0
        self.processed_ids = []
# ---
def shard_names(self) -> Sequence[str]:
        return self._shard_names
# ---
def latest_checkpoint_path(self) -> Path:
        return self.checkpoint_dir / "ckpt.pt"
# ---
def init(layer_idx, *, key):
            k1, k2 = jax.random.split(key)
            up = hax.nn.Linear.init(Embed, Up, key=k1)
            down = hax.nn.Linear.init(Up, Embed, key=k2)

            up = dataclasses.replace(up, weight=up.weight + layer_idx)  # type: ignore
            down = dataclasses.replace(down, weight=down.weight + layer_idx)  # type: ignore

            return Module(up=up, down=down)
# ---
def is_terminal(self) -> bool:
        """True if all VMs are in a terminal state (no further transitions expected)."""
        terminal = {
            vm_pb2.VM_STATE_READY,
            vm_pb2.VM_STATE_FAILED,
            vm_pb2.VM_STATE_TERMINATED,
            vm_pb2.VM_STATE_PREEMPTED,
        }
        return all(v.state in terminal for v in self.vms)
# ---
def _dashboard(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Iris Worker", "/static/worker/app.js"))
# ---
def any_executor(request):
    if request.param.name == "processes" and "cupy" in nxp.__name__:
        pytest.skip(reason="CuPy is not supported with 'processes' executor")
    return request.param
# ---
def _shard_filename(output_path: str, shard_idx: int) -> str:
    return os.path.join(output_path, f"shard_{shard_idx:05d}.jsonl.gz")
# ---
def get_timeline_data(doctype, name):
	'''returns timeline data based on stock ledger entry'''
	out = {}
	items = dict(frappe.db.sql('''select posting_date, count(*)
		from `tabStock Ledger Entry` where item_code=%s
			and posting_date > date_sub(curdate(), interval 1 year)
			group by posting_date''', name))

	for date, count in iteritems(items):
		timestamp = get_timestamp(date)
		out.update({timestamp: count})

	return out
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if self._prefix and dir in self._roots:
            return "all"
        return (
            dir in self._roots
            or dir in self._dirs
            or any(parentdir in self._roots for parentdir in util.finddirs(dir))
        )
# ---
def device_port(self):
        if self._values['managed']:
            return None
        return self._values['device_port']
# ---
def __call__(cls, verb, *args):
        if verb not in cls.verbs:
            cls.verbs[verb] = super(_VerbSingleton, cls).__call__(verb, *args)
        return cls.verbs[verb]
# ---
def key_function(out_key):
        out_coords = out_key[1:]

        # return a tuple with a single item that is an iterator of input keys to be merged
        in_keys = [
            list(
                range(
                    bi * split_every.get(i, 1),
                    min((bi + 1) * split_every.get(i, 1), x.numblocks[i]),
                )
            )
            for i, bi in enumerate(out_coords)
        ]
        return (iter([(x.name,) + tuple(p) for p in product(*in_keys)]),)
# ---
def __init__(self, meth_name, action, handle):
        self.meth_name = meth_name
        self.handle = handle
        self.action = action
# ---
def _make_einsum_spec(arrays, out_axes):
    name_mappings_for_einsum: dict[str, str] = {}
    used_letters: set[str] = set()
    spec = ""
    for operand in arrays:
        if len(spec):
            spec += ","
        for axis in operand.axes:
            letter = _assign_letter_to_name(axis.name, name_mappings_for_einsum, used_letters)
            spec += letter
    spec += "->"
    for out in out_axes:
        letter = name_mappings_for_einsum[axis_name(out)]
        spec += letter
    return spec
# ---
def __call__(self, x: NamedArray, gate: NamedArray) -> NamedArray:
        in_dtype = x.dtype
        x32 = x.astype(jnp.float32)
        var = hax.mean(hax.square(x32), axis=self.axis)
        inv = hax.rsqrt(var + jnp.asarray(self.eps, dtype=jnp.float32))
        y = (x32 * inv).astype(in_dtype)  # RMSNorm (from haliax/nn/normalization.py)
        y = self.weight * y  # learned scale
        gated = y * hnn.silu(gate)  # GDN's output gate
        return gated.astype(in_dtype)
# ---
def _canonicalize_batch(batch: Union[dict, List[dict]]) -> List[dict]:
    if isinstance(batch, pa.RecordBatch):
        batch = dict_from_record_batch(batch)

    if isinstance(batch, dict):
        keys = list(batch.keys())
        values = list(batch.values())
        num_rows = len(values[0]) if values else 0
        return [{key: values[i][j] for i, key in enumerate(keys)} for j in range(num_rows)]
    else:
        return list(batch)
# ---
def remainder(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.remainder(x1, x2)
# ---
def set_current_cluster(cluster: Cluster) -> None:
    _cluster_context.set(cluster)
# ---
def test_check_health_returns_unhealthy_on_failure():
    """check_health returns unhealthy result when curl fails."""
    conn = MagicMock()
    conn.run.return_value = MagicMock(returncode=1, stderr="Connection refused", stdout="")
    result = check_health(conn, port=10001)
    assert result.healthy is False
    assert "exit code 1" in result.curl_error or "Connection refused" in result.curl_error
# ---
def get_record_to_wandb(label: str = ""):
    wandb = WandBLogger.get_instance()
    step = 0

    def record_logs(logs):
        nonlocal step
        for j, log in enumerate(logs):
            if len(log) > 0:
                if label != "":
                    log = {f"{label}/{k}": v for k, v in log.items()}
                wandb.log(log, step=step + j)
        step += len(logs)

    return record_logs
# ---
def extract_hashes(body_contents):
        hashers = {k: HASH_TYPES_DICT[k]() for k in HASH_TYPES_DICT}

        while True:
            chunk = body_contents.read(512 * 16)
            if not chunk:
                break
            for h in hashers.values():
                h.update(chunk)

        return hashers.items()
# ---
def stop_event(self) -> threading.Event:
        return self._stop_event
# ---
from heapq import merge
def combine_lists(num1,num2):
  combine_lists=list(merge(num1, num2))
  return combine_lists
# ---
def offset(self, offset):
        Util.validate_type(offset, "int")
        return self._offset(offset)
# ---
def defineBrowserAgent(uiname, uiversion):
    class AppURLopener(urllib.FancyURLopener):
        version = "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)"
        #version = uiname + " " + uiversion + " / " + sys.platform
    urllib._urlopener = AppURLopener()
# ---
def resetfirst():
            nonlocal first
            first = True
# ---
def is_num_decagonal(n): 
	return 4 * n * n - 3 * n
# ---
def tree_map(fn, tree, leaf_type):
    if isinstance(tree, dict):
        return dict_map(fn, tree, leaf_type)
    elif isinstance(tree, list):
        return [tree_map(fn, x, leaf_type) for x in tree]
    elif isinstance(tree, tuple):
        return tuple([tree_map(fn, x, leaf_type) for x in tree])
    elif isinstance(tree, leaf_type):
        return fn(tree)
    else:
        raise ValueError(f"Tree of type {type(tree)} not supported")
# ---
def map(
        self,
        fn: Callable[[Any], T],
        items: Sequence[Any],
    ) -> list[WorkerFuture[T]]:
        """Map a function over items in parallel.

        Args:
            fn: Function to apply to each item
            items: Items to process

        Returns:
            List of futures, one per item
        """
        return [self.submit(fn, item) for item in items]
# ---
def round(self, decimals=0) -> "NamedArray":  # pragma: no cover
        return haliax.round(self, decimals=decimals)
# ---
import collections as ct
def merge_dictionaries(dict1,dict2):
    merged_dict = dict(ct.ChainMap({}, dict1, dict2))
    return merged_dict
# ---
def get_value(self) -> int:
        return self.value
# ---
def area_weighted_std(
    data: torch.Tensor,
    area_weights: torch.Tensor,
    keepdim: bool = False,
):
    return area_weighted_mean(
        (data - area_weighted_mean(data, area_weights, keepdim=True)) ** 2,
        area_weights,
        keepdim=keepdim,
    ).sqrt()
# ---
def start():
    global backProc
    backProc = multiprocessing.Process(target=testFun, args=(), daemon=True)
    backProc.start()
    return 'started: ' + str(backProc.pid)
# ---
def log(self, metrics: Mapping[str, Any], *, step: Optional[int], commit: Optional[bool] = None):
        del commit
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "log",
                "step": step,
                "metrics": metrics,
            }
        )
        self.logger.info(json.dumps(record))
        if step is not None:
            self._last_metrics.update(_flatten(metrics))
# ---
def test_register(self, dispatcher):
        name = "some_name"

        @event.event(name)
        async def corofunc(*args):
            return True

        h_inst = event.HandlerInstance.from_handler(corofunc)
        dispatcher.register(h_inst)
        assert h_inst in dispatcher.event_map["some_name"]
# ---
def is_valid_parenthese( str1):
        stack, pchar = [], {"(": ")", "{": "}", "[": "]"}
        for parenthese in str1:
            if parenthese in pchar:
                stack.append(parenthese)
            elif len(stack) == 0 or pchar[stack.pop()] != parenthese:
                return False
        return len(stack) == 0
# ---
def _coerce_integer_like_index(value: Any):
        if isinstance(value, np.integer):
            return int(value)
        if type(value) is np.ndarray and value.shape == () and jnp.issubdtype(value.dtype, jnp.integer):
            return int(value.item())
        return value
# ---
def _write_tiny_corpus(path):
    os.makedirs(f"{path}/train", exist_ok=True)
    with open(f"{path}/train/docs.jsonl", "w") as f:
        for i in range(10):
            f.write(json.dumps({"text": f"hello world {i} " * 100}))
            f.write("\n")

    os.makedirs(f"{path}/validation", exist_ok=True)
    with open(f"{path}/validation/docs.jsonl", "w") as f:
        for i in range(10):
            f.write(json.dumps({"text": f"bye world {i} " * 100}))
            f.write("\n")
# ---
def _find_acl_template(conn, acl_template):
    query = (
        sa.sql.select([acl_template_table.c.id])
        .where(acl_template_table.c.template == acl_template)
        .limit(1)
    )
    return conn.execute(query).scalar()
# ---
def unregister(self) -> None:
        Callback.active.remove(self)
# ---
def __init__(self,
                 sname,
                 namespace,
                 kubeconfig,
                 secrets=None):
        ''' constructor for handling secret options '''
        self.kubeconfig = kubeconfig
        self.name = sname
        self.namespace = namespace
        self.secrets = secrets
        self.data = {}

        self.create_dict()
# ---
def test_connection_available_returns_false_on_timeout():
    """connection_available returns False on timeout."""
    conn = MagicMock()
    conn.run.side_effect = subprocess.TimeoutExpired(cmd="ssh", timeout=30)
    assert connection_available(conn) is False
# ---
def backoff_until_ms(self) -> int:
        """Timestamp until which scale-up is blocked due to backoff."""
        return self._backoff_until.epoch_ms()
# ---
def _get_fixed_ips_subnets(self, context, gw_port):
        nw = self._core_plugin.get_network(context, gw_port['network_id'])
        subnets = [{'subnet_id': s} for s in nw['subnets']]
        return subnets
# ---
def normal_serialize(self):
        return super().serialize()
# ---
def test_to_string(self):
        """Can convert from type id to type string."""
        self.assertEquals(
            vm_utils.ImageType.to_string(vm_utils.ImageType.KERNEL),
            vm_utils.ImageType.KERNEL_STR)
# ---
def profile_mean(ds: xr.Dataset) -> xr.Dataset:
    return ds.weighted(ds.areacello).mean(["x", "y"])
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)  # First moment
        second_moment_buffer = otu.tree_zeros_like(params)  # Second moment
        count = jnp.zeros([], jnp.int32)

        return ScaleByMiniState(
            count=count, momentum_buffer=momentum_buffer, second_moment_buffer=second_moment_buffer
        )
# ---
def selection_function(out_key):
            out_coords = out_key[1:]
            return _target_chunk_selection(target_chunks, out_coords, selection)
# ---
def filename(self, value=""):
        self._filename = value
        self.refresh_window_title()
# ---
def test_template_kernel_matches_reference():
    x = jnp.linspace(-3.0, 3.0, 1024, dtype=jnp.float32)
    y_ref = reference_impl_batched(x[None, :])[0]
    y_fast = template_op(x)
    assert jnp.allclose(y_ref, y_fast, atol=0.0, rtol=0.0)

    xb = x.reshape(32, 32)
    yb_ref = reference_impl_batched(xb)
    yb_fast = template_op(xb)
    assert jnp.allclose(yb_ref, yb_fast, atol=0.0, rtol=0.0)
# ---
def scan_via(
        self,
        fn: Callable[[M, CarryT], tuple[CarryT, OutputT_co]],
        *,
        unroll: int | bool | None = None,
    ) -> Callable[[CarryT], tuple[CarryT, OutputT_co]]: ...
# ---
def page_number_gen(self):
        """Generate pages numbers from specified page intervals."""
        last = 0
        for start, end in sorted(self._pages):
            start = max(last, start)
            last = end + 1
            yield from range(start, last)
# ---
def _get_olmo3_attention(config, layer_idx: int, key):
    return config.init_attention(layer_idx, key=key)
# ---
def _expandsets(kindpats, ctx):
    """Returns the kindpats list with the 'set' patterns expanded."""
    fset = set()
    other = []

    for kind, pat, source in kindpats:
        if kind == "set":
            if not ctx:
                raise error.ProgrammingError("fileset expression with no " "context")
            s = ctx.getfileset(pat)
            fset.update(s)
            continue
        other.append((kind, pat, source))
    return fset, other
# ---
def remove_duplicates(self):
        if self.ui.chkBRemoveDuplicates.isChecked():
            for lbl in self.message.message_type:
                seq = lbl.fuzz_values[:]
                seen = set()
                add_seen = seen.add
                lbl.fuzz_values = [l for l in seq if not (l in seen or add_seen(l))]
# ---
def read_batch(self, timeout: float | None = None) -> RolloutBatch | None:
        """Read a single batch with optional timeout.

        Args:
            timeout: Maximum time to wait in seconds. None means no wait.

        Returns:
            RolloutBatch if available, None if timeout or no batches.
        """
        pass
# ---
def extract_missing(test_list, strt_val, stop_val):
  res = []
  for sub in test_list:
    if sub[0] > strt_val:
      res.append((strt_val, sub[0]))
      strt_val = sub[1]
    if strt_val < stop_val:
      res.append((strt_val, stop_val))
  return (res)
# ---
def _partition_trainable_params(model, filter):
    """
    Partitions the model into trainable and non-trainable parameters. This is used internally
    for the gradient calculation and checkpointing, but you can also use it to filter out params for logging
    or something.

    Returns:
        trainable, non-trainable
    """

    filter = make_floating_point_trainable_filter(filter)
    return eqx.partition(model, filter, is_leaf=lambda x: isinstance(x, haliax.NamedArray))
# ---
def test_gemma_param_counts_dont_change_with_seqlen():
    model = GemmaLMHeadModel.init(hax.Axis("v", 2048), _get_gemma_config(seq_len=128), key=random.PRNGKey(0))
    model2 = GemmaLMHeadModel.init(hax.Axis("v", 2048), _get_gemma_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def match(self, item):
        return item.get(self.field) is None
# ---
def prognostic_with_hist(
        self, hist: int
    ) -> Bool[GridMask, " prognostic_vars*({hist}+1)"]:
        return torch.concat([self.prognostic] * (hist + 1), dim=0)
# ---
def from_string(value):
        """ Convert an ORM ``value`` into a :class:`date` value. """
        if not value:
            return None
        value = value[:DATE_LENGTH]
        return datetime.strptime(value, DATE_FORMAT).date()
# ---
def test_normalize_unnormalize_tensor_prognostic(normalize_input):
    normalize, wet_mask = normalize_input
    data = torch.randn([1, normalize._prognostic_std_np.shape[0], *wet_mask.shape])
    input_data = data * wet_mask
    normalized = normalize.normalize_tensor_prognostic(input_data)
    unnormalized = normalize.unnormalize_tensor_prognostic(normalized, fill_value=0.0)
    assert torch.allclose(input_data, unnormalized)
# ---
def max_sum_pair_diff_lessthan_K(arr, N, K): 
	arr.sort() 
	dp = [0] * N 
	dp[0] = 0
	for i in range(1, N): 
		dp[i] = dp[i-1] 
		if (arr[i] - arr[i-1] < K): 
			if (i >= 2): 
				dp[i] = max(dp[i], dp[i-2] + arr[i] + arr[i-1]); 
			else: 
				dp[i] = max(dp[i], arr[i] + arr[i-1]); 
	return dp[N - 1]
# ---
def check_permissions(self, request):
        if not settings.SECURITY_COMMAND_EXECUTION and request.user.is_common_user:
            return self.permission_denied(request, "Command execution disabled")
        return super().check_permissions(request)
# ---
def testTryElse(self):
    self.assertEqual((0, 'foo baz\n'), _GrumpRun(textwrap.dedent("""\
        try:
          print 'foo',
        except:
          print 'bar'
        else:
          print 'baz'""")))
# ---
def __init__(self, status_code):
        self.status_code = status_code
        super(ResponseError, self).__init__(status_code)
# ---
def __init__(self, model_name: str, attribute_name: str, k: int = 2, *args, **kwargs):
        self.model_name = model_name
        self.attribute_name = attribute_name
        self.model = self.load_model()
        self.k = k
# ---
def __call__(self, *args, **kwargs):
    input_types = []
    args = list(args)
    for (i, x) in enumerate(args):
      x = ops.convert_to_tensor(x)
      if not isinstance(x, ops.Tensor):
        raise ValueError("Expect a Tensor but get ", x)
      input_types.append(x.dtype)
      args[i] = x
    return self.instantiate(input_types)(*args, **kwargs)
# ---
def infer_parquet_schema(record: dict[str, Any] | Any):
    """Infer PyArrow schema from a dictionary record."""
    import pyarrow as pa

    if is_dataclass(record):
        record = asdict(record)

    fields = []
    for key, value in record.items():
        fields.append((key, infer_parquet_type(value)))

    return pa.schema(fields)
# ---
def test_perturb_operators_swap_prob_zero_returns_none(rng):
    source = "a + b"
    result = perturb_operators(source, rng, swap_prob=0.0)
    assert result is None
# ---
def test_valid_position_mask_simple_assignment(tok):
    source = "x = 1 + 2\n"
    mask = tok.valid_position_mask(source)
    assert any(mask)
# ---
def device_flops_for_jax_device(jax_device_kind: str, dtype: str = "bf16") -> float | None:
    """Get peak FLOP/s given a JAX device kind string.

    Args:
        jax_device_kind: JAX device kind string (e.g., "TPU v4", "NVIDIA H100 80GB HBM3")
        dtype: Data type (e.g., "bf16", "fp16")

    Returns:
        Peak FLOP/s, or None if device/dtype unknown
    """
    fray_device_type = jax_device_kind_to_fray_device_type(jax_device_kind)
    return device_flops(fray_device_type, dtype)
# ---
def forward_once(self, x):
        x = self.conv1(x)
        x = self.bn1(x)  # BatchNorm2d
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        return x
# ---
def test_impl(df):
            C = df.B == 'two'
            return C.sum()
# ---
def test_mistral_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def validate_uom_conversion_factor(self):
		if self.uoms:
			for d in self.uoms:
				value = get_uom_conv_factor(d.uom, self.stock_uom)
				if value:
					d.conversion_factor = value
# ---
def setUp(self):
        super(XenAPIHostTestCase, self).setUp()
        self.flags(xenapi_connection_url='test_url',
                   xenapi_connection_password='test_pass')
        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)
        xenapi_fake.reset()
        xenapi_fake.create_local_srs()
        self.conn = xenapi_conn.get_connection(False)
# ---
def chordHelper(self, half_steps, tunning, strength):
        notes = [tunning*(2.**(half_steps[i]/12.)) for i in range(len(half_steps))]
        hpcp = HPCP(maxShifted=False)([notes[0], notes[1], notes[2]], strength)
        for i in range(len(hpcp)):
            if i in half_steps: self.assertTrue(hpcp[i]>0)
            elif (i - 12) in half_steps: self.assertTrue(hpcp[i]>0)
            else: self.assertEqual(hpcp[i], 0)
# ---
def __call__(self, host, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
        self.host = host
        self.timeout = timeout
        return self
# ---
def tags(self):
        return self.tag_to_index.keys()
# ---
def max_occurrences(list1):
    max_val = 0
    result = list1[0] 
    for i in list1:
        occu = list1.count(i)
        if occu > max_val:
            max_val = occu
            result = i 
    return result
# ---
def nansum(
    x, /, *, axis=None, dtype=None, keepdims=False, split_every=None, device=None
):
    """Return the sum of array elements over a given axis treating NaNs as zero."""
    dtype = _upcast_integral_dtypes(
        x, dtype, allowed_dtypes=("numeric",), fname="nansum", device=device
    )
    return reduction(
        x,
        nxp.nansum,
        axis=axis,
        dtype=dtype,
        keepdims=keepdims,
        split_every=split_every,
    )
# ---
def test_robust_quad_logx_fits_quadratic():
    """Test that robust_quad_logx recovers known coefficients from synthetic data."""
    x = jnp.array([1e9, 1e10, 1e11, 1e12])
    L = jnp.log10(x)
    # y = 0.1 * L^2 - 2 * L + 20
    y = 0.1 * L**2 - 2 * L + 20

    a, b, c = robust_quad_logx(x, y)

    assert abs(a - 0.1) < 0.01
    assert abs(b - (-2)) < 0.1
    assert abs(c - 20) < 0.5
# ---
def map_fn(p):
        if isinstance(p, hax.nn.Linear):
            if p.weight is None:
                return p
            return f(p)
        elif or_else is not None:
            return or_else(p)
        else:
            return p
# ---
def set_mesh(mesh: MeshLike) -> ContextManager[None]:
    """Compatibility wrapper around `mesh_context` matching the JAX 0.7 API."""

    return mesh_context(mesh)
# ---
def get_directory_size(path):
    return sum([get_file_size(os.path.join(path, fastq)) for fastq in os.listdir(path)])
# ---
def reload_model(self, model: LmHeadModel | None, state_dict: dict) -> None:
        # Serialize numpy arrays to (bytes, dtype, shape) tuples to survive RPC serialization.
        # vLLM's collective_rpc can corrupt numpy arrays during pickling.
        serialized_state_dict = serialize_state_dict_for_rpc(state_dict)
        self.llm.update_weights(serialized_state_dict, self.model_name)
        self.llm.reset_prefix_cache()
# ---
def clear_words(self):
        """Remove all registered words; effectively turns off auto-completion."""
        self._word_freq = []
        self._word_list = defaultdict(lambda: 0)
# ---
def _createPostamble(self):
        """
        """
        ex = []
        ex.append('M30\n') # End of Program, rewind
        return ex
# ---
def remain_alnum(text):
    """Remain digits and English letters of a string.
    """
    return ''.join(c for c in text if c.isalnum()
                                   and ord(' ') <= ord(c) <= ord('z'))
# ---
def setUp(self):
        self.cj = cangjie.Cangjie(self.version, self.language)
# ---
def block_html(self, html):
        """Rendering block level pure html content.

        :param html: text content of the html snippet.
        """
        if self.options.get('skip_style') and \
           html.lower().startswith('<style'):
            return ''
        if self.options.get('escape'):
            return escape(html)
        return html
# ---
def normalize_by_func(self,f_data,z_data,func):
		return z_data/func(f_data)
# ---
def test_entrypoint_params_gpu():
    from fray.v2.ray_backend.backend import get_entrypoint_params

    request = JobRequest(
        name="gpu-job",
        entrypoint=Entrypoint.from_binary("train", []),
        resources=ResourceConfig(device=GpuConfig(variant="H100", count=4)),
    )
    params = get_entrypoint_params(request)
    assert params["entrypoint_num_gpus"] == 4.0
# ---
def release(self, lease: Lease[T_co]) -> None:
        """Release a lease and requeue the item for reprocessing."""
        ...
# ---
def rnai_1(testapp, lab, award, source, target):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'RNAi',
        'reagents': [{'source': source['@id'], 'identifier': 'addgene:12345'}],
        'rnai_sequences': ['ATTACG'],
        'modified_site_by_target_id': target['@id']
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def test_object_bool(tmp_path, executor):
    spec = cubed.Spec(tmp_path, 100000, executor=executor)
    a = xp.asarray(
        [[False, False, False], [False, False, False], [False, False, False]],
        chunks=(2, 2),
        spec=spec,
    )
    b = xp.all(a)
    assert not b

    a = xp.asarray(
        [[True, True, True], [True, True, True], [True, True, True]],
        chunks=(2, 2),
        spec=spec,
    )
    b = xp.all(a)
    assert b
# ---
def __del__(self):
        if self.handle:
            self.terminate()
# ---

def right_angle_triangle(a, b, c):
    '''
    Given the lengths of the three sides of a triangle. Return True if the three
    sides form a right-angled triangle, False otherwise.
    A right-angled triangle is a triangle in which one angle is right angle or 
    90 degree.
    Example:
    right_angle_triangle(3, 4, 5) == True
    right_angle_triangle(1, 2, 3) == False
    '''
    return a*a == b*b + c*c or b*b == a*a + c*c or c*c == a*a + b*b
# ---
def test_impl(df):
            A = df.A.str.split(',')
            B = pd.Series(list(itertools.chain(*A)))
            return B
# ---
def get_curr_value(invalue, val_type):
        '''return the current value'''
        if invalue is None:
            return None

        curr_value = invalue
        if val_type == 'yaml':
            curr_value = yaml.load(invalue)
        elif val_type == 'json':
            curr_value = json.loads(invalue)

        return curr_value
# ---
def autocast(enabled: bool, dtype: torch.dtype) -> torch.autocast:
    if using_gpu():
        return torch.autocast("cuda", dtype=dtype, enabled=enabled)
    else:
        return torch.autocast("cpu", dtype=dtype, enabled=enabled)
# ---
def test_make_blockwise_key_function_elemwise():
    func = lambda x: 0

    key_fn = make_blockwise_key_function(
        func, "z", "ij", "x", "ij", "y", "ij", numblocks={"x": (2, 2), "y": (2, 2)}
    )

    graph = make_blockwise_graph(
        func, "z", "ij", "x", "ij", "y", "ij", numblocks={"x": (2, 2), "y": (2, 2)}
    )
    check_consistent_with_graph(key_fn, graph)
# ---
def name(self) -> str:
        """Get the local name (last component)."""
        return self._parts[-1]
# ---
def get_matching_files(
    patterns: list[str], all_files_list: list[pathlib.Path], exclude_patterns: list[str]
) -> list[pathlib.Path]:
    matched = []
    for file_path in all_files_list:
        if should_exclude(file_path):
            continue
        if matches_pattern(file_path, exclude_patterns):
            continue
        if matches_pattern(file_path, patterns):
            matched.append(file_path)
    return matched
# ---
def case(context):
        """Alter the context."""
        assert context == {"squee": "kapow"}
        context.squee = "boing"
# ---
def fill_customer_code(self):
		""" Append all the customer codes and insert into "customer_code" field of item table """
		cust_code = []
		for d in self.get('customer_items'):
			cust_code.append(d.ref_code)
		self.customer_code = ','.join(cust_code)
# ---
def on_epoch_end(self, model: LightningModule):
        raise NotImplementedError
# ---
def pop(queue_name: str, lease_timeout: float = Body(default=60.0, embed=True)):
            if queue_name not in self.queues:
                return Response(status_code=404)
            lease = self.queues[queue_name].pop(lease_timeout)
            if lease is None:
                return Response(status_code=204)
            return {"lease_id": lease.lease_id, "timestamp": lease.timestamp, "payload": pickle.dumps(lease.item).hex()}
# ---
def cursor_execute(conn, cursor, statement,
                        parameters, context, executemany):
            canary.append('cursor_execute')
            return statement, parameters
# ---
def is_terminal(self) -> bool:
        """True if VM is in a terminal state."""
        return self.info.state in (
            vm_pb2.VM_STATE_READY,
            vm_pb2.VM_STATE_FAILED,
            vm_pb2.VM_STATE_TERMINATED,
            vm_pb2.VM_STATE_PREEMPTED,
        )
# ---
def _get_query_pos_renames(Pos):
    new_Pos: list[Axis] = []
    renames: dict[str, str] = {}
    for i, axis in enumerate(Pos):
        ax_name = axis_name(axis)
        axis = axis.alias(f"q_{ax_name}")
        renames[ax_name] = axis.name
        new_Pos.append(axis)

    return tuple(new_Pos), renames
# ---
def format_ports(ports):
    return format_line(prefix='ports'.rjust(RJUST), values=ports)
# ---
def distance_to_point(self, point):
        point_to_surface = point - self.top_left_corner3d()
        distance_to_surface = self.normal.dot(point_to_surface)
        return distance_to_surface
# ---
def test_cauchy():
    check_gen_is_equal(lambda k, s: jax.random.cauchy(k, s), lambda k, s: hax.random.cauchy(k, s))
# ---
def test_repr(self):
        expr = col("score")
        assert repr(expr) == "col('score')"
# ---
def hypot(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "hypot")
    if x1.dtype not in _real_numeric_dtypes or x2.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in hypot")
    return elemwise(nxp.hypot, x1, x2, dtype=result_type(x1, x2))
# ---
def test_download_indices_for_url(mock_rcw, mock_rw, mock_jw):
    with s.app.test_client() as c:
        resp = c.get('/api/v1/classify_documents/log_only?directory=test')

        assert mock_rcw.called
        assert mock_rw.called
        assert mock_jw.called
# ---
def unregister_event_callback(self, callback):
        self._event_callbacks.remove(callback)
# ---
def _build_query_string(q, default_field=None):
    """
    Builds "query_string" object from 'q'.

    :param: q of type String
    :param: default_field
    :return: dictionary object.
    """

    query_string = {'query_string': {'query': q, 'default_operator': 'AND'}}
    query_string['query_string'].update({'lenient': False} if default_field else {'default_field': default_field})

    return query_string
# ---
def __call__(self, x: NamedArray, mask: Optional[AttentionMask | NamedArray], layer_idx, *, key):
        k1, k2, k3, k4 = haliax.jax_utils.maybe_rng_split(key, 4)

        attn_output = self.attn(self.ln_1(x), mask=mask, layer_idx=layer_idx, key=k1)
        attn_output = self.resid_dropout(attn_output, key=k2)
        x = x + attn_output

        ff_output = self.mlp(self.ln_2(x), key=k3)
        ff_output = self.resid_dropout(ff_output, key=k4)
        x = x + ff_output

        return x
# ---
def output_hrule(self):
        return self.renderer.hrule()
# ---
def insertIndent():
            if self.useTabs:
                cursor.insertText('\t')
            else:  # indent to integer count of indents from line start
                charsToInsert = self.width - (len(self._qpart.textBeforeCursor()) % self.width)
                cursor.insertText(' ' * charsToInsert)
# ---
def get_product(val) : 
	res = 1
	for ele in val: 
		res *= ele 
	return res 
def find_k_product(test_list, K):
  res = get_product([sub[K] for sub in test_list])
  return (res)
# ---
def _run_task(stop_event: threading.Event) -> None:
            attempt.run()
# ---
def run(self):
            with self.default_sess():
                player = get_player_fn(train=False)
                while not self.stopped():
                    try:
                        score = play_one_episode(player, self.func)
                        # print("Score, ", score)
                    except RuntimeError:
                        return
                    self.queue_put_stoppable(self.q, score)
# ---
def test_explicit_client_overrides_auto_detection():
    """Explicitly set client should override auto-detection."""
    mock_ctx = MagicMock()
    mock_iris_client_lib = MagicMock()
    mock_ctx.client = mock_iris_client_lib

    explicit = LocalClient(max_threads=1)
    with patch("iris.client.client.get_iris_ctx", return_value=mock_ctx):
        with set_current_client(explicit):
            # Should return explicit client, not auto-detected Iris client
            assert current_client() is explicit
# ---
def re_arrange_tuples(test_list, ord_list):
  temp = dict(test_list)
  res = [(key, temp[key]) for key in ord_list]
  return (res)
# ---
def get_cluster_hosts(self) -> [Host]:
        return list(self.deployment.hosts)[1:]
# ---
def testParse(self):
        current = self.parser.parse(self.lines, '1024x768')
        for i in range(len(current)):
            self.assertEquals(self.expected[i], current[i], 'Entry incorrect')
# ---
def to_seconds(self) -> float:
        """Convert to seconds."""
        return self._ms / 1000.0
# ---
def test_sqrts(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b, c = sqrts(a)
    # don't optimize graph so we use as much memory as possible (reading from Zarr)
    run_operation(tmp_path, executor, "sqrts", b, c, optimize_graph=False)
# ---
def extract_rear(test_tuple):
  res = list(sub[len(sub) - 1] for sub in test_tuple)
  return (res)
# ---
def from_ms(cls, milliseconds: int) -> "Duration":
        """Create duration from milliseconds."""
        return cls(milliseconds)
# ---
def run_ss_mask_specification(
        self, tokens: np.ndarray, random: np.random.Generator
    ):
        design_mask = tokens["design_mask"].astype(bool)
        if design_mask.sum() > 1 and random.random() < self.ss_condition_prob:
            weights, functions = zip(*self.ss_condition_tasks)
            ss_fn = random.choice(functions, p=weights)
            ss_fn(tokens, random)
# ---
def test_quantile_sequential(self):
        def test_impl(A):
            df = pd.DataFrame({'A': A})
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        A = np.arange(0, n, 1, np.float64)
        np.testing.assert_almost_equal(hpat_func(A), test_impl(A))
# ---
def from_hours(cls, hours: int) -> "Duration":
        """Create duration from hours."""
        return cls(hours * 60 * 60 * 1000)
# ---
def norm_config(self) -> RmsNormConfig:
        return RmsNormConfig(
            eps=self.layer_norm_epsilon,
            use_weight=self.use_layer_norm_weight,
            use_bias=self.use_bias,
        )
# ---
def count_Digit(n):
    count = 0
    while n != 0:
        n //= 10
        count += 1
    return count
# ---
def __is_nice_string_using_old_rules(self, string):
        return (self.__regex_naughty.search(string) is None
            and len(self.__regex_vowels.findall(string)) > 2
            and self.__regex_double_char.search(string))
# ---
def response_tokens_from_choice(self, choice: Choice) -> np.ndarray:
        """Extract response token IDs directly from the choice.

        Uses the response_token_ids attached during vLLM-to-OpenAI conversion,
        avoiding the lossy convert_ids_to_tokens/convert_tokens_to_ids round-trip
        that fails for padding token IDs not in the tokenizer vocabulary.
        """
        return np.array(choice.response_token_ids, dtype=np.int32)
# ---
def test_bench_rust_minhash(benchmark: Any, sample_batch: pa.RecordBatch) -> None:
    batch = sample_batch.slice(length=BENCHMARK_ROWS)
    benchmark(rust_minhash_pipeline, batch)
# ---
def traslado_iva(self):
        return self.__triva
# ---
def add_hook(self, fn: Optional[Callable[[StepInfo], Any] | Callback | JitCallback] = None, *, every: int = 1):
        return self.hooks.add_hook(fn, every=every)
# ---
def custom_fn(pred):
        pred_embeddings, pred_lm_head = pred
        loss = fused_cross_entropy_loss_and_logsumexp_penalty(
            pred_embeddings,
            pred_lm_head,
            Contract=Embed,
            Label=Vocab,
            target_y=true_ids,
            reduction=None,
            logsumexp_weight=0.0,
            block_size=2,
            dtype=pred_embeddings.dtype,
        )

        return loss.mean().scalar()
# ---
def test_selector_adds_new_axis():
    B, S, V, T = Axis("batch", 2), Axis("seq", 3), Axis("vocab", 5), Axis("step", 4)
    logits = hax.arange((B, S, V))
    idx = hax.arange((B, T), dtype=jnp.int32) % V.size
    out = logits["vocab", idx]
    assert set(out.axes) == {B, S, T}
    ref = jnp.transpose(_ref_gather(logits, V, idx), (0, 2, 1))
    assert jnp.array_equal(out.array, ref)
# ---
def conj(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.conj(), self.axes)
# ---
def on_predict_epoch_end(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,  # noqa: ARG002
    ) -> None:
        print(f"Number of failed structure predictions: {self.failed}")
# ---
def get_size_str(size):
    """
    Formats a byte size as a string.

    The returned string is no more than 9 characters long.
    """
    if size is None:
        return "0 " + SIZE_LEVEL[0]
    if size == 0:
        magnitude = 0
        level = 0
    else:
        magnitude = math.floor(math.log(size, 10))
        level = int(min(math.floor(magnitude // 3), 4))
    return ('%d' if level == 0 else '%.2f') % (float(size) / 2**(level*10)) + ' ' + SIZE_LEVEL[level]
# ---
def _unsqueeze(x, reference):
  return x.view(
    * x.shape,
    * ((1,) * (len(reference.shape) - len(x.shape))))
# ---
def test_multiple_outputs():
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), dtype=float)
    b, c = sqrts(a)

    cubed.compute(b, c)

    input = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    assert_array_equal(b, np.sqrt(input))
    assert_array_equal(c, -np.sqrt(input))
# ---
def _parse_lev_from_output_var(prognostic_var_names: PrognosticVarNames) -> list[int]:
    """Parse the `lev` dimension from the output var names. Default: 0 for surface."""
    depth_inds = []
    for var_depth_i in prognostic_var_names:
        # Examples: "so_18", "zos"
        var_split = var_depth_i.split("_")
        if len(var_split) == 1:
            depth_inds.append(0)
        else:
            depth_inds.append(int(var_split[-1]))

    return depth_inds
# ---
def put(self, obj: Any) -> Any:
        """Identity operation - no serialization needed."""
        return obj
# ---
def root():

    return 'Started a background process with PID ' + str(backProc.pid) + " is running: " + str(backProc.is_alive())
# ---
def clear_record(self):
        self.recordWidget.clear()
# ---
def test_identical_programs_no_edits():
    source = "x = 1\n"
    assert tree_diff(source, source) == []
# ---
def test_pickling_and_unpickling_encoded_file():
    # See https://bitbucket.org/pytest-dev/pytest/pull-request/194
    # pickle.loads() raises infinite recursion if
    # EncodedFile.__getattr__ is not implemented properly
    ef = capture.EncodedFile(None, None)
    ef_as_str = pickle.dumps(ef)
    pickle.loads(ef_as_str)
# ---
def special_tokens_injection_check(marin_tokenizer: PreTrainedTokenizer):
    """Test that special tokens are correctly replaced."""
    for token_id, token_str in MARIN_CUSTOM_SPECIAL_TOKENS.items():
        assert marin_tokenizer.decode(token_id) == token_str
        assert marin_tokenizer.convert_tokens_to_ids([token_str]) == [token_id]
# ---
def __colors(self):
        return [key for key in self.__dict__.keys() if not key.startswith('_') and key != 'named']
# ---
def json_loads(data):
            # on Python 3.5 json.loads only supports str not bytes
            return json.loads(data.decode())
# ---
def _sample_prior(self, *batch_dims):
    return self.mask_index * torch.ones(
      * batch_dims, dtype=torch.int64)
# ---
def output(self):
        return luigi.LocalTarget(os.path.join(os.getcwd(), "data", "hola_mundo_desde_R.psv"))
# ---
def nchunks(self) -> int:
        """Number of chunks in array"""
        return reduce(mul, self._cdata_shape, 1)
# ---
def init_outdir(self, outdir):
        self.outdir = Path(outdir)
        self.outdir.mkdir(parents=True, exist_ok=True)
# ---
def local_stats(items):
        items_list = list(items)
        if not items_list:
            return {"sum": 0, "count": 0, "min": float("inf"), "max": float("-inf")}
        return {
            "sum": sum(items_list),
            "count": len(items_list),
            "min": min(items_list),
            "max": max(items_list),
        }
# ---
def native_value(self):
        """Return sensor state."""
        job: OctoprintJobInfo = self.coordinator.data["job"]
        if not job:
            return None

        if not (state := job.progress.completion):
            return 0

        return round(state, 2)
# ---
def test_arange():
    (H,) = hax.make_axes(H=10)

    assert jnp.all(jnp.equal(hax.arange(H).array, jnp.arange(10)))
    assert hax.arange(H).axes == (H,)

    # test stride
    assert jnp.all(jnp.equal(hax.arange(H, step=2).array, jnp.arange(0, 20, 2)))

    # test start and stride
    assert jnp.all(jnp.equal(hax.arange(H, start=2, step=2).array, jnp.arange(2, 22, 2)))
# ---
def test_astype(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.astype(a, xp.int32)
    assert_array_equal(
        b.compute(executor=executor),
        np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
    )
# ---
def next(self, psm: PSM):
        if psm.char.isdigit():
            self.max.append(psm.char)
            return self
        elif psm.char == "}":
            self._interpret()
            return self.repeat.parent
        else:
            psm.error = 'expected digit, "," or "}"'
# ---
def shaped_rng_split(key, split_shape: int | Sequence[int] = 2) -> PRNGKeyArray:
    if isinstance(split_shape, int):
        num_splits = split_shape
        split_shape = (num_splits,) + key.shape
    else:
        num_splits = np.prod(split_shape)
        split_shape = tuple(split_shape) + key.shape

    if num_splits == 1:
        return jnp.reshape(key, split_shape)

    unshaped = maybe_rng_split(key, num_splits)
    return jnp.reshape(unshaped, split_shape)
# ---
def _parse_gcs_path(self, path: str) -> tuple[str, str]:
        """Parse gs://bucket/path into (bucket, blob_path)."""
        path = path[5:]  # Remove gs:// prefix
        bucket, _, blob_path = path.partition("/")
        return (bucket, blob_path)
# ---
def Vocab(self) -> Axis:
        return self.token_embeddings.Vocab
# ---
def full(shape: AxisSpec, fill_value: T, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to `fill_value`"""
    if isinstance(shape, Axis):
        r = NamedArray(jnp.full(shape=shape.size, fill_value=fill_value, dtype=dtype), (shape,))
    else:
        x_shape = to_jax_shape(shape)
        r = NamedArray(jnp.full(shape=x_shape, fill_value=fill_value, dtype=dtype), shape)

    return auto_sharded(r)
# ---
def has_torch():
    try:
        import torch  # noqa F401

        return True
    except ImportError:
        return False
# ---
def gen_keys(self):
        '''
        Generate minion RSA public keypair
        '''
        salt.crypt.gen_keys(
                self.opts['gen_keys_dir'],
                self.opts['gen_keys'],
                self.opts['keysize'])
        return
# ---
def stop(self):
        self._send_message(_Message.STOP)
# ---


def is_palindrome(text: str):
    """
    Checks if given string is a palindrome
    >>> is_palindrome('')
    True
    >>> is_palindrome('aba')
    True
    >>> is_palindrome('aaaaa')
    True
    >>> is_palindrome('zbcd')
    False
    """
    for i in range(len(text)):
        if text[i] != text[len(text) - 1 - i]:
            return False
    return True
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None:
        conn = _direct_ssh(_apply_manual_overrides(self._ssh_config, group_config.manual), slice_id)
        cmd = (
            "sudo docker stop iris-worker 2>/dev/null || "
            "sudo docker kill iris-worker 2>/dev/null || true; "
            "sudo docker rm -f iris-worker 2>/dev/null || true"
        )
        conn.run(cmd, timeout=Duration.from_seconds(60))
# ---
def _compute_value(self, records):
        """ Invoke the compute method on ``records``. """
        # initialize the fields to their corresponding null value in cache
        for field in self.computed_fields:
            records._cache[field] = field.null(records.env)
            records.env.computed[field].update(records._ids)
        self.compute(records)
        for field in self.computed_fields:
            records.env.computed[field].difference_update(records._ids)
# ---
def __nameToWordnetCode(self, name):
        """
        It returns the wordnet code for a given language name
        """
        if not self.__isLanguageAvailable(language_name=name):
            raise Exception("Wordnet code not found for the language name %s " % name)
        name = name.lower()
        languageShortCode = AVAILABLE_LANGUAGES_NAMES[name]

        wordnetCode = self.__shortCodeToWordnetCode(code=languageShortCode)
        return wordnetCode
# ---
def close(self) -> None:
        if self.vllm_server is not None:
            self._backend.stop(self.vllm_server)
            self.vllm_server = None
# ---
def get_context(self, context):
		context.show_search = True
		context.search_link = '/product_search'

		context.parents = get_parent_item_groups(self.item_group)

		self.set_variant_context(context)
		self.set_attribute_context(context)
		self.set_disabled_attributes(context)

		return context
# ---
def tearDown(self):
        Engine.dispatch._clear()
        Engine._has_events = False
# ---
def test_combined_scenarios():
    """Tests with multiple special characters."""
    input_text = r"""This is *bold* and _italic_.
    # Header 1
    - List item 1
    + List item 2
    This has [brackets] and \backslashes\.
    """
    expected_text = r"""This is \*bold\* and \_italic\_.
    \# Header 1
    \- List item 1
    \+ List item 2
    This has \[brackets\] and \backslashes\\.
    """
    assert expected_text == minimal_markdown_escape(input_text)
# ---
def nanargmin(array: NamedArray, axis: AxisSelector | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.nanargmin, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def test_makes_patches__more_variables():
    x = torch.randn(1, 20, 4, 8)

    patch_embed = PerceiverEncoder(
        in_channels=20,
        out_channels=4,
        patch_extent=(180, 180),
        perceiver=make_perceiver(20, 4),
    )

    patches = patch_embed(x, make_resolution(x))

    assert patches.shape == (1, 4, 1, 2)
# ---
def load_item(self, row):
        """
            Load an item from the item table (counterpart to add_item
            when restoring a job from the database)
        """

        item = S3ImportItem(self)
        if not item.restore(row):
            self.error = item.error
            if item.load_parent is None:
                self.error_tree.append(deepcopy(item.element))
        # Update lookup lists
        item_id = item.item_id
        self.items[item_id] = item
        return item_id
# ---
def available_cpu(self) -> int:
        """Available CPU cores after subtracting committed resources."""
        return self.metadata.cpu_count - self.committed_cpu
# ---
def manual_scale_group() -> config_pb2.ScaleGroupConfig:
    """Scale group config for manual hosts."""
    return config_pb2.ScaleGroupConfig(
        name="manual-hosts",
        min_slices=0,
        max_slices=3,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_CPU,
        runtime_version="manual",
        zones=["manual"],
    )
# ---
def _stack_leaves_unchecked(*leaves):
        if is_named_array(leaves[0]):
            return hax.stack(batch_name, leaves)
        else:
            return jnp.stack(leaves)
# ---
def setLDAPDirectory(self, directory=None):
        if directory is None:
                self.directory = []
        else:
            try:
                with open(DIRECTORY, 'w+') as f:
                    f.write(str(directory))
                    self.directory = directory
            except OSError as e:
                raise
# ---
def eliminate_axes(axis_spec: Axis | Sequence[Axis], axes: AxisSelection) -> tuple[Axis, ...]:  # type: ignore
    ...
# ---
def _map_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield fn(item)
# ---
def matchfn(self, f):
        return False
# ---
import math
def degree_radian(radian):
 degree = radian*(180/math.pi)
 return degree
# ---
def __call__(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        ctx: GridContext,
    ) -> torch.Tensor: ...
# ---
def __repr__(self):
        return "<includematcher includes=%r>" % self._pats
# ---
def min_Swaps(s1,s2) :  
    c0 = 0; c1 = 0;  
    for i in range(len(s1)) :  
        if (s1[i] == '0' and s2[i] == '1') : 
            c0 += 1;    
        elif (s1[i] == '1' and s2[i] == '0') : 
            c1 += 1;  
    result = c0 // 2 + c1 // 2;  
    if (c0 % 2 == 0 and c1 % 2 == 0) : 
        return result;  
    elif ((c0 + c1) % 2 == 0) : 
        return result + 2;  
    else : 
        return -1;
# ---
def multiply_num(numbers):  
    total = 1
    for x in numbers:
        total *= x  
    return total/len(numbers)
# ---
def test_set_current_client_context_manager():
    """Explicitly set client should take priority over auto-detection."""
    explicit = LocalClient(max_threads=2)
    with set_current_client(explicit) as c:
        assert c is explicit
        assert current_client() is explicit
    # After exiting, should return a fresh default (or auto-detect)
    assert current_client() is not explicit
# ---
def wait(self, futures: list, num_returns: int = 1) -> tuple[list, list]:
        ready, pending = ray.wait(futures, num_returns=num_returns, fetch_local=False)
        return list(ready), list(pending)
# ---
def __call__(self, batch: dict[str, Any]) -> dict[str, Any]:
        return self.cls.__call__(batch)
# ---
def context(self):
        """The OpenGL context attached to this window.  Read-only.

        :type: `pyglet.gl.Context`
        """
        return self._context
# ---
def harmonic_sum(n):
  if n < 2:
    return 1
  else:
    return 1 / n + (harmonic_sum(n - 1))
# ---
def bitwise_right_shift(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "bitwise_right_shift")
    if x1.dtype not in _integer_dtypes or x2.dtype not in _integer_dtypes:
        raise TypeError("Only integer dtypes are allowed in bitwise_right_shift")
    return elemwise(nxp.bitwise_right_shift, x1, x2, dtype=result_type(x1, x2))
# ---
def compute_num_layers(self, hidden_size: int) -> int:
        """Compute number of layers from hidden size using the depth-width formula."""
        hs_pow = math.log2(hidden_size)
        return round(
            hidden_size
            / (self.base_hidden_layer_ratio + (hs_pow * self.layer_scaling_factor) - self.layer_formula_offset)
        )
# ---
def derivative(x):
        out = x - np.roll(x, 1)
        return out[1:-1]
# ---
def put_number(num, length):
    '''
    Put a single number as a hex number at the end of a string `length` bytes long.

    >>> put_number(3, 4)
    b'\\x00\\x00\\x00\\x03'
    >>> put_number(0, 1)
    b'\\x00'
    '''
    lst = bytearray()

    for i in range(length):
        shift_bits = 8 * (length - 1 - i)
        this_num = (num >> shift_bits) & 0xFF
        lst.append(this_num)
    return bytes(lst)
# ---
def test_submit_callable_succeeds(client: LocalClient):
    handle = client.submit(JobRequest(name="ok", entrypoint=Entrypoint.from_callable(_noop)))
    status = handle.wait()
    assert status == JobStatus.SUCCEEDED
# ---
def deploymentconfig(self):
        ''' deploymentconfig property '''
        return self.dconfig
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        keys = hax.jax_utils.maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.blocks.fold(x, key=keys)
        x = self.ln_f(x)

        return x
# ---
def fused_key_func(out_key):
        return pipeline1.config.key_function(*pipeline2.config.key_function(out_key))
# ---
def save_design_only_structure_to_pdb(atom_design_mask, structure, output_path: Path):
    cif_path = output_path.with_suffix(".cif")
    cif_text = save_design_only_structure_to_cif(atom_design_mask, structure, cif_path)

    cif_io = io.StringIO(cif_text)
    mmcif_parser = PDB.MMCIFParser()
    pdb_writer = PDB.PDBIO()
    parsed_structure = mmcif_parser.get_structure("des_only", cif_io)
    pdb_writer.set_structure(parsed_structure)
    pdb_writer.save(str(output_path))
# ---
def sorted_dict(dict1):
  sorted_dict = {x: sorted(y) for x, y in dict1.items()}
  return sorted_dict
# ---
def gradient_detach_interval(request):
    """Parametrized fixture for gradient detach intervals. `0` means no detaching."""
    return (i := request.param), "no detaching!" if i == 0 else f"detaching every {i}"
# ---
def selection_function(out_key):
        out_coords = out_key[1:]
        return get_item(normalized_copy_chunks, out_coords)
# ---
def get_attname(self):
		return "%s_json" % self.name
# ---
def filter_checkpoint(
    fun: Callable,
    *,
    prevent_cse: bool = True,
    policy: Callable[..., bool] | None = None,
):
    """As `jax.checkpoint`, but allows any Python object as inputs and outputs"""

    warnings.warn(
        "filter_checkpoint is deprecated, use eqx.filter_checkpoint instead",
        DeprecationWarning,
    )

    return eqx.filter_checkpoint(fun, prevent_cse=prevent_cse, policy=policy)
# ---
def myroot():
    return_data = get_index()
    return return_data
# ---
def _download(self, gcs_path: str, local_path: Path) -> None:
        with fsspec.open(gcs_path, "rb") as src:
            with open(local_path, "wb") as dst:
                dst.write(src.read())
# ---
def sort_matrix(M):
    result = sorted(M, key=sum)
    return result
# ---
def scanned_f(*args, **kwargs):
        return scan_preconfig(None, *args, **kwargs)[1]
# ---
def _setup_parameter(self, file_name, **kwargs):
        for key, value in kwargs.items():
            origin = "%s =" %key
            new = "%s = %s" %(key, value)
            sudo('sed -i "/%s/ c\%s" %s' %(origin, new, file_name))
# ---
def test_success(tmp_path, timing_map, n_tasks, retries, use_backups):
    outputs = run_test(
        function=partial(deterministic_failure, tmp_path, timing_map),
        input=range(n_tasks),
        retries=retries,
        use_backups=use_backups,
    )

    assert outputs == set(range(n_tasks))

    check_invocation_counts(tmp_path, timing_map, n_tasks, retries)
# ---
def min(x, axis=0):
    return _Nmin(x, axis)
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)
        return x
# ---
def test_beam_search_respects_beam_size(params, model_cfg, tokenizer):
    """Should not return more candidates than beam_size."""
    beam_size = 3
    results = beam_search(
        params=params,
        initial_programs=["x = 1\n", "y = 2\n", "z = 3\n", "w = 4\n"],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(2),
        beam_size=beam_size,
        expansions_per_beam=2,
        max_depth=2,
    )
    assert len(results) <= beam_size
# ---
def test_multiply(self):
        expr = col("a") * col("b")
        assert expr.evaluate({"a": 4, "b": 5}) == 20
# ---
def round_to(c, s):
    """Return a chunk dimension that is close to an even multiple or factor

    We want values for c that are nicely aligned with s.

    If c is smaller than s we use the original chunk size and accept an
    uneven chunk at the end.

    If c is larger than s then we want the largest multiple of s that is still
    smaller than c.
    """
    if c <= s:
        return max(1, int(c))
    else:
        return c // s * s
# ---
def _add_missing_coords_inplace(self):
        """Add missing coordinates to self._variables
        """
        for dim, size in iteritems(self.dims):
            if dim not in self._variables:
                # This is equivalent to np.arange(size), but
                # waits to create the array until its actually accessed.
                data = indexing.LazyIntegerRange(size)
                coord = Coordinate(dim, data)
                self._variables[dim] = coord
# ---
def terminate(self) -> None:
        if self._terminated:
            return
        self._terminated = True
        for worker in self._workers:
            worker.stop()
        for vm in self._managed_vms:
            self._vm_registry.unregister(vm.info.vm_id)
# ---
def modify(id, radio, url, genre) :
    db = cherrypy.session['database']

    sql = "UPDATE Radio SET radio='%s', url='%s', genre='%s', exist=1 WHERE id = %s" % (radio, url, genre, id)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def __init__(self, fn_id: str, preempt_until_n_calls: int):
        self.fn_id = fn_id
        self.preempt_until_n_calls = preempt_until_n_calls
        self.call_count = 0
# ---
def get_file_details_dir(filename):
  return filename[filename.rindex("/")+1:]
# ---
def validate_with_braceexpand(braceexpand_paths, paths):
    be_paths = []
    for be_path in braceexpand_paths:
        be_paths.extend(list(braceexpand(be_path)))

    if set(be_paths) != set(paths):
        print("Braceexpand paths are not equal to the original paths")
        print("Braceexpand paths:")
        print(be_paths)
        print("Original paths:")
        print(paths)
# ---
def __init__(self, pool_size=2, strides=None, padding='valid'):
        if strides is None:
            strides = pool_size
        assert padding in {'valid', 'same'}, 'border_mode must be in {valid, same}'
        self.pool_length = pool_size
        self.stride = strides
        self.border_mode = padding
# ---
def get_indices(es):
    return elasticsearch.client.IndicesClient(es)
# ---
def task(self, index: int) -> "JobName":
        """Create a task name for this job.

        Tasks are job names with a numeric suffix.

        Example:
            JobName.from_string("/my-job").task(0) -> JobName(("my-job", "0"))
        """
        return JobName((*self._parts, str(index)))
# ---
def count(lst):   
    return sum(lst)
# ---
def model_type(cls) -> Type["Gpt2HyenaModel"]:
        return Gpt2HyenaModel
# ---
def normalize_and_mask(
        self, normalize_before_mask: bool, masked_fill_value: float
    ) -> Float[torch.Tensor, "batch time var lat lon"]:
        """Normalize and mask tensors."""
        tensor = self.data
        if normalize_before_mask:
            tensor = self._normalize(tensor)
        tensor = torch.where(self.mask, tensor, masked_fill_value)
        if not normalize_before_mask:
            tensor = self._normalize(tensor)
        return tensor
# ---
def __call__(self, target, creds, enforcer):
        """Check an individual match.

        Matches look like:

            tenant:%(tenant_id)s
            role:compute:admin
        """

        # TODO(termie): do dict inspection via dot syntax
        match = self.match % target
        if self.kind in creds:
            return match == six.text_type(creds[self.kind])
        return False
# ---
def stub_add_volume_type_access(context, type_id, project_id):
            self.assertEqual(fake.VOLUME_TYPE4_ID, type_id, "type_id")
            self.assertEqual(PROJ2_UUID, project_id, "project_id")
# ---
def zeros_like(x, /, *, dtype=None, device=None, chunks=None, spec=None) -> "Array":
    return zeros(**_like_args(x, dtype, device, chunks, spec))
# ---
def poke(self, context):
        '''
        Function that the sensors defined while deriving this class should
        override.
        '''
        raise AirflowException('Override me.')
# ---
def __call__(
        self,
        array: NamedArray,
        axis: AxisSelection | None = None,
        where: NamedArray | None = None,
        **kwargs,
    ) -> NamedArray: ...
# ---
def placeholder_func(*x):
    return 1
# ---
def _now_ms() -> int:
    """Return current Unix timestamp in milliseconds."""
    return int(time.time() * 1000)
# ---
def __init__(self, futures: list[tuple[ResolvedEndpoint, Future]]):
        self._futures = futures
# ---
def rombus_area(p,q):
  area=(p*q)/2
  return area
# ---
def poll(self) -> int | None:
        if self.is_alive():
            return None
        # Return non-zero exit code if thread raised an exception
        return 1 if self._exception is not None else 0
# ---
def log_data_to_wandb(data):
    # Generate the week ID
    week_id = int(datetime.now().strftime("%W"))  # e.g., "02"
    project_name = "marin-monitoring"
    id_ = "weekly-metrics-final-final"
    wandb.init(project=project_name, id=id_, resume="allow")
    # Log the data for this week
    log_weekly_data(data=data, week_id=week_id, project_name=project_name, id_=id)
    wandb.finish()
# ---
def extract_features(text: str, ngram_config: NGramConfig | None) -> Iterator[str]:
    """
    Extract features (paragraphs or n-grams) from text.
    """
    paragraphs = text.split("\n")

    for para in paragraphs:
        if ngram_config:
            yield from extract_ngrams(para, ngram_config.ngram_length, ngram_config.stride)
        else:
            # Exact paragraph matching
            yield para
# ---
def tokenize_nemotron_subset(name: str, tokenizer: str | None = None) -> ExecutorStep[TokenizeConfig]:
    """Get a specific nemotron split tokenization step."""
    assert name in NEMOTRON_DATASETS, f"Split {name} not found in NEMOTRON_DATASETS"
    return tokenize_nemotron(tokenizer=tokenizer)[f"nemotron_cc/{name}"]
# ---
def datasource_from_hf(id: str, *, split, **kwargs) -> ShardedDataSource[dict]:
    """
    Create a ShardedDataset from a HuggingFace dataset. Arguments are passed to load_dataset.
    """
    return WrappedHFDataSource(id, split=split, **kwargs)
# ---
def init(cls, axis: AxisSpec, eps: float = 1e-6, use_weight: bool = True, use_bias: bool = False, dtype=None):
        assert use_weight, "GemmaRMSNorm does not support use_weight=False"
        assert not use_bias, "GemmaRMSNorm does not support use_bias=True"

        weight = hax.zeros(axis)
        bias = None

        return GemmaRMSNorm(axis, weight, bias, eps, dtype=jnp.float32)
# ---
def bmarks():
    return_data = add_tags()
    return return_data
# ---
def fabs(a: A) -> A:
    return wrap_elemwise_unary(jnp.fabs, a)
# ---
def test_int_array_index_1d(spec, ind):
    a = xp.arange(12, chunks=(3,), spec=spec)
    b = a.rechunk((4,))  # force materialization to test indexing against zarr
    assert_array_equal(b[ind].compute(), np.arange(12)[ind])
# ---
def rotate_half(x):
  x1, x2 = (
    x[..., : x.shape[-1] // 2],
    x[..., x.shape[-1] // 2 :],
  )
  return torch.cat((-x2, x1), dim=-1)
# ---
def start_transfer_server() -> jax_transfer.TransferServer:
    """Start JAX transfer server."""
    ip = get_local_ip_from_hostname()
    backend_client = jax.devices()[0].client

    # Use random port binding for proper network resource management
    server = jax_transfer.start_transfer_server(
        backend_client,
        f"{ip}:0",  # Random port binding
        [f"{ip}:0"] * jax.device_count(),
    )
    return server
# ---
def _filter_token(tok):
	return str(tok).translate(_TOKEN_FILTER_MAP)
# ---
def no_alloc(state):
                    # No-op; leave index INVALID so downstream gets INVALID destinations
                    state = eqx.error_if(state, jnp.zeros(()) < 4, "INVALID!")
                    return state
# ---
def plugin_on_paused(self):
        if self.__cookie is None:
            return

        try:
            bus = dbus.SessionBus()
            obj = bus.get_object(self.DBUS_NAME, self.DBUS_PATH)
            iface = dbus.Interface(obj, self.DBUS_INTERFACE)
            iface.Uninhibit(self.__cookie)
            self.__cookie = None
        except dbus.DBusException:
            pass
# ---
def _description_sortable(self):
        return self.store or (self.inherited and self.related_field._description_sortable)
# ---
def mock_image_cache():
    """Create mock ImageBuilder."""
    builder = Mock(spec=ImageBuilder)
    builder.build = Mock(
        return_value=BuildResult(
            image_tag="test-image:latest",
            build_time_ms=1000,
            from_cache=False,
        )
    )
    builder.protect = Mock()
    builder.unprotect = Mock()
    return builder
# ---
def test_impl(A, B):
            df = pd.DataFrame({'A': A, 'B': B})
            df2 = df.groupby('A', as_index=False)['B'].sum()
            # TODO: fix handling of df setitem to force match of array dists
            # probably with a new node that is appended to the end of basic block
            # df2['C'] = np.full(len(df2.B), 3, np.int8)
            # TODO: full_like for Series
            df2['C'] = np.full_like(df2.B.values, 3, np.int8)
            return df2
# ---
def output_exemplar(self):
        return self.transforms[-1].output_exemplar
# ---
def __init__(self, refresh_rate: int = 1, process_position: int = 0):
        """Initialize the pipeline progress bar.

        Parameters
        ----------
        refresh_rate : int
            Refresh rate for the progress bar
        process_position : int
            Position of the process for multi-processing
        """
        super().__init__(refresh_rate=refresh_rate, process_position=process_position)
        self._original_tqdm = None
        self._patch_tqdm()
# ---
def valid_date(s):
    try:
      return datetime.strptime(s, _DATE_FORMAT).date()
    except ValueError:
      raise argparse.ArgumentTypeError(f'Invalid date specified: "{s}".')
# ---
def test_nested_dict(self):
        """Store and retrieve a nested dict"""
        self.make_table()
        data = {
            "s": "abc",
            "d": {
                "i": 42,
            },
        }
        self.dynamo.put_item("foobar", {"id": "abc", "d": data})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["d"], data)
# ---
def photo_preview(self):
        img = get_thumbnail(self.photo, '75x75', crop='center')
        return format_html('<a href="{}" target="_blank"><img style="width:75px; height:75px;" src="{}"></a>',
                           self.photo.url, img.url)
# ---
def get_col_str(col_desc):
    return col_desc['name'] + DELIMITER(" (") + col_desc['type'] + DELIMITER(")")
# ---
def primary_source(self) -> DataSource:
        return self.sources[0]
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[dict]:
        url = self._shard_name_to_url_mapping[shard_name]
        with fsspec.open(url, "r", compression="infer") as f:
            # TODO: would be nice if we could seek faster than this. Can't even skip json parsing
            data = json.load(f)
            return iter(data[row:])
# ---
def increment_counting(self, event):
        """Counts an event

        Args:
            event (:obj:`baroque.entities.event.Event`): the event to be counted

        """
        assert isinstance(event, Event)
        self.events_count += 1
        t = type(event.type)
        if t in self.events_count_by_type:
            self.events_count_by_type[t] += 1
        else:
            self.events_count_by_type[t] = 1
# ---
def test_logging_stream_ownership(self, testdir):
        p = testdir.makepyfile("""
            def test_logging():
                import logging
                import pytest
                stream = capture.CaptureIO()
                logging.basicConfig(stream=stream)
                stream.close() # to free memory/release resources
        """)
        result = testdir.runpytest_subprocess(p)
        assert result.stderr.str().find("atexit") == -1
# ---
def match(self, item):
        return self.value_match(self.pattern, item.get(self.field))
# ---
def test_ravel():
    H, W, D = hax.make_axes(Height=2, Width=3, Depth=4)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))
    raveled = named1.ravel("Z")

    assert raveled.size == H.size * W.size * D.size
    assert hax.all(hax.equal(raveled, named1.flatten_axes((H, W, D), "Z")))
    assert jnp.all(jnp.equal(raveled.array, jnp.ravel(named1.array)))
# ---
def decomposed_mse(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """Standard MSE loss (l2) computed per channel."""
    return F.mse_loss(pred, target, reduction="none").mean(dim=(0, 2, 3))
# ---
def test_annotate_image(self, annotator_client_mock):
        # Given
        annotate_image_method = annotator_client_mock.annotate_image

        # When
        self.hook.annotate_image(request=ANNOTATE_IMAGE_REQUEST)
        # Then
        # Product ID was provided explicitly in the method call above, should be returned from the method
        annotate_image_method.assert_called_once_with(
            request=ANNOTATE_IMAGE_REQUEST, retry=None, timeout=None
        )
# ---
def _out_first(self):
        """
        Returns: bool: Whether the output axes are first in the weight matrix
        """
        # We do it this way because of scan layers
        if isinstance(self.Out, hax.Axis):
            return self.weight.axes[-1] != self.Out
        else:
            return self.weight.axes[-len(self.Out) :] != self.Out
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["Olmo3Config"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfOlmo3Config,
        )
# ---
def t_DONE(self, t):
        r'(\(x\))'
        return t
# ---
def beta(self):
        return self.__data['bool_tester']
# ---
def pop(self, lease_timeout: float = 60.0) -> Lease[T_co] | None:
        """Acquire a lease on the next available item."""
        ...
# ---
def __repr__(self):
        return "<xormatcher matchers=%r>" % self._matchers
# ---
def get_bootstrap_host(self) -> Host:
        return list(self.deployment.hosts)[0]
# ---
def __complex__(self) -> complex:  # pragma: no cover
        return complex(self.array)
# ---
import collections as ct
def merge_dictionaries_three(dict1,dict2, dict3):
    merged_dict = dict(ct.ChainMap({},dict1,dict2,dict3))
    return merged_dict
# ---
def frequency_Of_Largest(n,arr): 
    mn = arr[0] 
    freq = 1
    for i in range(1,n): 
        if (arr[i] >mn): 
            mn = arr[i] 
            freq = 1
        elif (arr[i] == mn): 
            freq += 1
    return freq
# ---
def to_proto(self) -> cluster_pb2.CoschedulingConfig:
        """Convert to protobuf representation."""
        return cluster_pb2.CoschedulingConfig(group_by=self.group_by)
# ---
def multiply_elements(test_tup):
  res = tuple(i * j for i, j in zip(test_tup, test_tup[1:]))
  return (res)
# ---
def kill_tasks_on_workers(self, task_ids: set[JobName]) -> None:
        """Send KILL RPCs to workers for tasks that were running."""
        ...
# ---
def decomposed_mae(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    """Standard MAE loss (l1) computed per channel."""
    return F.l1_loss(pred, target, reduction="none").mean(dim=(0, 2, 3))
# ---
def __str__(self) -> str:
        return f"{self.start} to {self.end}"
# ---
def test_encrypt_simple_message(self):
        self._test_encryption('This is a simple message.')
# ---
def is_Isomorphic(str1,str2):          
    dict_str1 = {}
    dict_str2 = {}
    for i, value in enumerate(str1):
        dict_str1[value] = dict_str1.get(value,[]) + [i]        
    for j, value in enumerate(str2):
        dict_str2[value] = dict_str2.get(value,[]) + [j]
    if sorted(dict_str1.values()) == sorted(dict_str2.values()):
        return True
    else:
        return False
# ---
def _setup_pkgresources():
    import pkg_resources
    import os
    import plistlib

    pl = plistlib.readPlist(os.path.join(
        os.path.dirname(os.getenv('RESOURCEPATH')), "Info.plist"))
    appname = pl.get('CFBundleIdentifier')
    if appname is None:
        appname = pl['CFBundleDisplayName']
    path = os.path.expanduser('~/Library/Caches/%s/python-eggs' % (appname,))
    pkg_resources.set_extraction_path(path)
# ---
def test_task_lifecycle_phases(worker):
    """Test task transitions through PENDING -> BUILDING -> RUNNING -> SUCCEEDED."""
    request = create_run_task_request()
    task_id = worker.submit_task(request)

    task = worker.get_task(task_id)
    task.thread.join(timeout=15.0)

    final_task = worker.get_task(task_id)
    assert final_task.status == cluster_pb2.TASK_STATE_SUCCEEDED
    assert final_task.exit_code == 0
# ---
def huber(residual):
        abs_r = jnp.abs(residual)
        quad = 0.5 * residual**2
        linear = delta * (abs_r - 0.5 * delta)
        return jnp.where(abs_r <= delta, quad, linear)
# ---
def markdownify_kwargs(self) -> dict:
        exclude = {*list(self.resiliparse_kwargs.keys()), "prepend_title"}
        return {f.name: getattr(self, f.name) for f in fields(self.markdownify_config) if f.name not in exclude}
# ---
def main(config):
  """Main entry point for training."""
  L.seed_everything(config.seed)
  _print_config(config, resolve=True, save_cfg=True)

  logger = utils.get_logger(__name__)
  tokenizer = dataloader.get_tokenizer(config)

  if config.mode == 'sample_eval':
    generate_samples(config, logger, tokenizer)
  elif config.mode == 'ppl_eval':
    _ppl_eval(config, logger, tokenizer)
  else:
    _train(config, logger, tokenizer)
# ---
def get_service(name: str) -> ServiceInfo | None:
    """Get a service by name."""
    register_services()
    return SERVICES.get(name)
# ---
def _get_olmo2_config(use_flash=False, num_kv_heads=4, seq_len=128) -> Olmo2Config:
    return Olmo2Config(
        max_seq_len=seq_len,
        hidden_dim=16,
        intermediate_dim=32,
        num_layers=4,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def test_corrupt_program_produces_valid_python(bank):
    source = CORPUS[0]  # fibonacci
    rng = random.Random(42)

    corrupted, mutations = corrupt_program(source, num_steps=3, bank=bank, rng=rng)

    assert len(mutations) > 0
    assert corrupted != source

    try:
        ast.parse(corrupted)
    except SyntaxError:
        pytest.fail(f"corrupt_program produced invalid Python:\n{corrupted}")
# ---
def Embed(self) -> Axis:
        return self.hyena.Embed
# ---
def clause(self):
        """Generate an SQLite expression implementing the query.

        Return (clause, subvals) where clause is a valid sqlite
        WHERE clause implementing the query and subvals is a list of
        items to be substituted for ?s in the clause.
        """
        return None, ()
# ---
def __truediv__(self, other: str) -> "InputName":
        """Alias for `cd` that looks more Pythonic."""
        return self.cd(other)
# ---
def open(self, chunks: dict[str, int] | None = None) -> xr.Dataset:
        # TODO(jder): could consider passing credentials here
        # rather than relying on the environment

        return xr.open_dataset(
            self.url(),
            backend_kwargs={"storage_options": {"endpoint_url": self.endpoint_url}},
            engine="zarr",
            chunks=chunks,
        )
# ---
def find_packages():
        return list(_find_packages(mn.__path__, mn.__name__))
# ---
def test_fixed_window_with_elastic_expiry(self):
        storage = MemcachedStorage("memcached://localhost:22122")
        limiter = FixedWindowElasticExpiryRateLimiter(storage)
        per_sec = RateLimitItemPerSecond(2, 2)

        assert limiter.hit(per_sec)
        time.sleep(1)
        assert limiter.hit(per_sec)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert limiter.test(per_sec)
# ---
def terminate(self) -> None:
        self._terminated.set()
        self._future.cancel()
# ---
def _assert_no_data(self):
        eq_(
            testing.db.scalar(self.table.count()), 0
        )
# ---
def itemsize(self) -> int:
        """Length of one array element in bytes"""
        return self.dtype.itemsize
# ---
def output_exemplar(self) -> dict[str, np.ndarray]:
        return {"data": np.array([0], dtype=np.int64)}
# ---
def axes(self) -> tuple[Axis, ...]:
        shape = jnp.shape(self.array)
        if len(shape) != len(self.axis_names):
            raise ValueError(
                f"Shape of underlying array {shape} does not match number of axes {self.axis_names}. {self.array}"
            )

        return tuple(Axis(name, size) for name, size in zip(self.axis_names, shape))
# ---
def are_equal_under_sympy(ground_truth_normalized: str, given_normalized: str):
    if not given_normalized:
        return False

    are_equal = False
    try:
        expr = f"({ground_truth_normalized})-({given_normalized})"
        if should_allow_eval(expr):
            sympy_diff = _sympy_parse(expr)
            simplified = sympy.simplify(sympy_diff)
            if simplified == 0:
                are_equal = True
    except BaseException:
        pass
    return are_equal
# ---
def axis_indices(self, axis: AxisSelection) -> tuple[int | None, ...]: ...
# ---
def target_fn(queue, args, kwargs):
        try:
            # Call the original function
            result = underlying_function(*args, **kwargs)
            queue.put((True, result))  # Success, put the result
        except Exception as e:
            # Capture and return the full traceback in case of an exception
            exc_info = sys.exc_info()
            exception_info = ExceptionInfo(ex=e, tb=tblib.Traceback(exc_info[2]))
            queue.put((False, exception_info))
# ---
def created_at_ms(self) -> int:
        """Timestamp when this VM group was created (milliseconds since epoch)."""
        return self._created_at.epoch_ms()
# ---
def single_to_keys(single, indexing_matrix, W, H):
    B, N, D = single.shape
    K = N // W
    single = single.view(B, 2 * K, W // 2, D)
    return torch.einsum("b j i d, j k -> b k i d", single, indexing_matrix).reshape(
        B, K, H, D
    )
# ---
def result(self) -> bytes | None:
        """Serialized task result (cloudpickle), if available."""
        ...
# ---
def is_main_process():
    return get_rank() == 0
# ---
def compute_hash(doc):
        content = doc["content"]
        return hashlib.md5(content.encode()).hexdigest()
# ---
def forget(self, request):
		return [('WWW-Authenticate', _generate_digest_challenge(
			round(time.time()),
			self.secret,
			self.realm,
			'NPDIGEST'
		))]
# ---
def get_all_regions() -> list[str]:
    """Extract unique regions from TPU_ZONES_CONFIG."""
    regions = set()
    for zone in TPU_ZONES_CONFIG.keys():
        # Zone format: us-west4-a -> region: us-west4
        region = zone.rsplit("-", 1)[0]
        regions.add(region)
    return sorted(regions)
# ---
def fsspec_dir_only_contains_files(dir_path):
    """
    Check if a directory only contains files in a fsspec filesystem.
    """
    fs, _ = fsspec.core.url_to_fs(dir_path)
    ls_res = fs.ls(dir_path, detail=True)
    if len(ls_res) == 0:
        return False
    return all(item["type"] == "file" for item in ls_res)
# ---
def service(state, scheduler):
    controller_mock = Mock()
    controller_mock.wake = Mock()
    controller_mock.task_schedule_status = scheduler.task_schedule_status
    controller_mock.autoscaler = None  # No autoscaler by default
    return ControllerServiceImpl(state, controller_mock, bundle_prefix="file:///tmp/iris-test-bundles")
# ---
def pending(self) -> int:
        return len(self.leases)
# ---
def centres(request):
	#PythonCenter
	return render(request, 'centres/centres.html')
# ---
def test_column_map_arg(self):
        def test_impl(df):
            df['B'] = df.A.map(lambda a: 2 * a)
            return

        n = 121
        df1 = pd.DataFrame({'A': np.arange(n)})
        df2 = pd.DataFrame({'A': np.arange(n)})
        hpat_func = self.jit(test_impl)
        hpat_func(df1)
        self.assertTrue(hasattr(df1, 'B'))
        test_impl(df2)
        np.testing.assert_equal(df1.B.values, df2.B.values)
# ---
def never(root, cwd):
    return nevermatcher(root, cwd)
# ---
def resolve_axis(self, axis: AxisSelector) -> Axis:
        """Resolve an axis selector to the corresponding axis in the current view."""
        name = axis_name(axis)
        for ax in self.axes:
            if ax.name == name:
                return ax
        raise ValueError(f"Axis {name} is not present in this reference view")
# ---
def point_in_poly(x,y,poly):

    n = len(poly)
    inside = False

    p1x,p1y = poly[0]
    for i in range(n+1):
        p2x,p2y = poly[i % n]
        if y > min(p1y,p2y):
            if y <= max(p1y,p2y):
                if x <= max(p1x,p2x):
                    if p1y != p2y:
                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x
                    if p1x == p2x or x <= xints:
                        inside = not inside
        p1x,p1y = p2x,p2y

    return inside
# ---
def _is_string_feature(feature) -> bool:
    """Return True if the feature (possibly nested) stores strings."""
    dtype = getattr(feature, "dtype", None)
    if dtype == "string":
        return True
    nested = getattr(feature, "feature", None)
    if nested is not None:
        return _is_string_feature(nested)
    return False
# ---
def seek(self, amount, reference="relative", precision="default-precise"):
        self.command('seek', amount, reference, precision)
# ---
def test_make_token_requires_login(self):
        token1 = csrf.make_token()
        self.assertIsNone(token1)
        self.login()
        token2 = csrf.make_token()
        self.assertIsNotNone(token2)
# ---
def remove_tuple(test_list):
  res = [sub for sub in test_list if not all(ele == None for ele in sub)]
  return (str(res))
# ---
def find_date_and_format(string):
            for ord, format in enumerate(cls.date_formats):
                for format_option in format:
                    try:
                        date = datetime.strptime(string, format_option)
                        return date, ord
                    except ValueError:
                        # Parsing failed.
                        pass
            return (None, None)
# ---
def spawn(
        self,
        target: Callable[..., Any],
        *,
        name: str | None = None,
        args: tuple = (),
        on_stop: Callable[[], None] | None = None,
    ) -> ManagedThread:
        thread = ManagedThread(target=target, name=name, args=args, on_stop=on_stop, _container=self)
        with self._lock:
            self._threads.append(thread)
        thread.start()
        return thread
# ---
def clear_requesting(self) -> None:
        """Clear REQUESTING state (scale-up completed or failed)."""
        self._requesting_until = Timestamp.from_ms(0)
# ---
def KVHeads(self) -> Axis:
        return Axis("kv_head", self.num_kv_heads)
# ---
def cleanup(self) -> None:
        """No cleanup needed for GCS checkpoints."""
        pass
# ---
def the_object_name_is_not_an_ifc_element(name):
    id = the_object_name_exists(name).BIMObjectProperties.ifc_definition_id
    assert id == 0, f"The ID is {id}"
# ---
def sum_even_and_even_index(arr,n):  
    i = 0
    sum = 0
    for i in range(0,n,2): 
        if (arr[i] % 2 == 0) : 
            sum += arr[i]  
    return sum
# ---
def set_job_info(info: JobInfo) -> None:
    _job_info.set(info)
# ---
def _thrift_binary(self):
    thrift_binary = ThriftBinary.Factory.scoped_instance(self).create()
    return thrift_binary.path
# ---
def LogPABotError(message):
    _pabotlog.error(message)
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "RotaryEmbeddingsConfig":
        pass
# ---
def forward(self, labels):
    embeddings = self.embedding_table(labels)
    return embeddings
# ---
def toggle_connect(self):
        if self.serial.isOpen():
            self.disconnect()
        else:
            self.connect()
# ---
def _filter_blacklist(package):
        blacklist = ["-i", "#", "Python==", "python-lambda=="]
        return all(package.startswith(entry) is False for entry in blacklist)
# ---
def initialized_wrapped_optimizer():
    mock_opt = mock_optimizer_transform()
    skip_conf = SkipStepConfig(rolling_interval_length=10)  # smaller interval for tests
    wrapped_opt = skip_conf.wrap(mock_opt)
    dummy_params = {"w": jnp.array([1.0, 2.0, 3.0]), "b": jnp.array([0.5])}
    initial_state = wrapped_opt.init(dummy_params)
    return wrapped_opt, initial_state, dummy_params, skip_conf
# ---
def compute(na, inp):
        return hax.nn.softmax(
            model(na, inp),
            axis=model.Vocab,
        )
# ---
def device_mesh(self) -> Mesh:
        return self.config.device_mesh
# ---
def bench():
        for sample in loader:
            dataset, n = sample
            for X, y in dataset:
                _, _ = X, y
# ---
def find_speedrun_results(base_path: str) -> list[str]:
    fs = fsspec.filesystem(base_path.split("://", 1)[0] if "://" in base_path else "file")
    pattern = f"{base_path}/**/speedrun_results.json"
    all_results = fs.glob(pattern)

    # Filter out excluded speedruns by checking the run name (directory name)
    return [path for path in all_results if Path(path).parent.name not in EXCLUDED_SPEEDRUNS]
# ---
def on_signal(sig, frame):
        click.echo("\nClosing tunnel...")
        stop.set()
# ---
def test_float(self):
        """Store and retrieve a float"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "num": 1.1})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertAlmostEqual(float(item["num"]), 1.1)
# ---
def take(x, indices, /, *, axis):
    return x[(slice(None),) * axis + (indices,)]
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        """Stop batch generation and wait for thread."""
        self.stop_flag.set()
        if self.thread:
            self.thread.join(timeout=2)
        return False
# ---
def _get_vm_logs(controller_url: str, vm_id: str, tail: int) -> tuple[str, str, int]:
    client = cluster_connect.ControllerServiceClientSync(controller_url)
    request = cluster_pb2.Controller.GetVmLogsRequest(vm_id=vm_id, tail=tail)
    response = client.get_vm_logs(request)
    return response.logs, response.vm_id, response.state
# ---
def update(self, request, *args, **kwargs):
        raise MethodNotAllowed(self.action)
# ---
def unique_test_name(config_name: str) -> str:
    global _NEXT_TEST_ID
    _NEXT_TEST_ID += 1
    return f"test_{config_name}_{_NEXT_TEST_ID}"
# ---
def testFloatBasic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.float32)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def global_reducer(sketches: Iterator[DDSketch]) -> DDSketch:
        """Merge all shard sketches into one."""
        combined = DDSketch()
        for sketch in sketches:
            combined.merge(sketch)
        return combined
# ---
def sqrt(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in sqrt")
    return elemwise(nxp.sqrt, x, dtype=x.dtype)
# ---
def test_weibull_min():
    scale = hax.arange(Width, start=0.1)
    concentration = hax.arange(Height, start=0.1)

    check_gen_is_equal(
        lambda k, s: jax.random.weibull_min(
            k, scale.array.reshape(1, -1), concentration.array.reshape(-1, 1), shape=s
        ),
        lambda k, s: hax.random.weibull_min(k, s, scale, concentration),
    )
# ---
def count_alpha_dig_spl(string):
  alphabets=digits = special = 0
  for i in range(len(string)):
    if(string[i].isalpha()):
        alphabets = alphabets + 1
    elif(string[i].isdigit()):
        digits = digits + 1
    else:
        special = special + 1
  return (alphabets,digits,special)
# ---
def get_default_screen(self):
            """Get the default screen as specified by the user's operating system
            preferences.

            :rtype: `Screen`
            """
            raise NotImplementedError('deprecated')
# ---
def test_deduplication(bank):
    """Identical subtrees from different programs should be deduplicated."""
    for node_type, entries in bank.entries.items():
        sources = [e.source for e in entries]
        assert len(sources) == len(set(sources)), (
            f"Duplicates found in {node_type}: " f"{len(sources)} entries but {len(set(sources))} unique"
        )
# ---
def union_axes(a1: AxisSpec, a2: ShapeDict) -> ShapeDict: ...
# ---
def _pipeline() -> int:
        return sum(len(dupekit.process_arrow_batch(b)) for b in in_memory_table.to_batches(max_chunksize=batch_size))
# ---
def _random(x, numblocks=None, root_seed=None, dtype=nxp.float64, block_id=None):
    stream_id = block_id_to_offset(block_id, numblocks)
    rg = Generator(Philox(key=root_seed + stream_id))
    out = rg.random(x.shape, dtype=dtype)
    out = numpy_array_to_backend_array(out)
    return out
# ---
def string_to_unresolved(data: Any) -> Any:
    """Turns a string into an UnresolvedLocation."""
    # TODO(jder): we could support other fsspec or universal_pathlib URLs here
    if isinstance(data, str):
        return UnresolvedLocation(path=data)
    return data
# ---
def test_convert_to_bytes_error(input_value):
    with pytest.raises(ValueError, match="Invalid value"):
        cubed.Spec(allowed_mem=input_value)
# ---
def create_mesh(devices=None):
    """Create a simple JAX mesh for testing."""
    if devices is None:
        devices = jax.local_devices()[:1]  # Use just one device for tests
    return Mesh(np.array(devices), axis_names=("batch",))
# ---
def is_big_name(item):
            return len(item) > 4
# ---
def test_repr_html():
    pytest.importorskip("jinja2")
    assert ones([])._repr_html_()
    assert ones(10)[:0]._repr_html_()
    assert ones(10)._repr_html_()
    assert ones((10, 10))._repr_html_()
    assert ones((10, 10, 10))._repr_html_()
    assert ones((10, 10, 10, 10))._repr_html_()
# ---
def __call__(self, batch: List[T]) -> PyTree:
        return jtu.tree_map(
            lambda _, *xs: PreparedBatch.from_batch([np.asarray(x) for x in xs]),
            self.exemplar,
            *batch,
            is_leaf=heuristic_is_leaf,
        )
# ---
def check_last (arr,n,p): 
    _sum = 0
    for i in range(n): 
        _sum = _sum + arr[i] 
    if p == 1: 
        if _sum % 2 == 0: 
            return "ODD"
        else: 
            return "EVEN"
    return "EVEN"
# ---
def NumLinesInBuffer( buffer_object ):
  # This is actually less than obvious, that's why it's wrapped in a function
  return len( buffer_object )
# ---
def test_reset_twice_error(self):
        with self.getcapture() as cap:
            print("hello")
            out, err = cap.readouterr()
        pytest.raises(ValueError, cap.stop_capturing)
        assert out == "hello\n"
        assert not err
# ---
def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
# ---
def cosh(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in cosh")
    return elemwise(nxp.cosh, x, dtype=x.dtype)
# ---
def setUp(self):
        super(XenAPISRSelectionTestCase, self).setUp()
        xenapi_fake.reset()
# ---
def __init__(self, subtype, data, msg=None):
        m = "Database is missing data {} for {}".format(data, subtype)
        super(PhylotyperError, self).__init__(subtype, m)
        self.data = data
# ---
def load_or_initialize(cache_dir: str, source: ShardedDataSource, processor: BatchProcessor):
        metadata = CacheMetadata(preprocessor_metadata=processor.metadata)
        try:
            return CacheLedger.load(cache_dir, metadata)
        except FileNotFoundError:
            return CacheLedger(
                total_num_rows=0,
                shard_rows={shard: 0 for shard in source.shard_names},
                is_finished=False,
                metadata=metadata,
            )
# ---
def predict_dataloader(self) -> DataLoader:
        return DataLoader(
            self._val_set,
            batch_size=self.cfg.val_batch_size,
            num_workers=self.cfg.num_workers,
            pin_memory=self.cfg.pin_memory,
            shuffle=False,
            collate_fn=collate,
        )
# ---
def reset_input_style(self):
        """Reset regex input line background"""
        if self._input_styled:
            self.inputLine.setStyleSheet(self.styleSheet())
            self._input_styled = False
# ---
def do_alloc(carry):
                return _alloc_pages_for_seq(i, carry)
# ---
def filter(self, record: Record) -> bool:
        """Filter complexes based on their resolution.

        Parameters
        ----------
        record : Record
            The record to filter.

        Returns
        -------
        bool
            Whether the record should be filtered.

        """
        structure = record.structure
        return self.minimum_resolution <= structure.resolution <= self.resolution
# ---
def _jit_stub(fn, *args, **kwargs):
        def _wrapped(x):
            return fn(x)

        return _wrapped
# ---
def connected(self):
        return self._connected
# ---
def shutdown(self, wait: bool = True) -> None:
        """Shutdown the client and all managed resources."""
        ...
# ---
def __len__(self) -> int:
        """Get the length of the dataset.

        Returns
        -------
        int
            The length of the dataset.

        """
        return self.samples_per_epoch
# ---
def stop(self):
        """Stop worker if running."""
        if self.worker:
            self.worker.stop()
# ---
def remove_replica(test_tup):
  temp = set()
  res = tuple(ele if ele not in temp and not temp.add(ele) 
				else 'MSP' for ele in test_tup)
  return (res)
# ---
def test_permute_errors_on_invalid_starting_dims_index(self):
    with self.assertRaisesRegexp(ValueError, r'Invalid permutation .*dims.*'):
      testing_utils.layer_test(
          keras.layers.Permute,
          kwargs={'dims': (0, 1, 2)}, input_shape=(3, 2, 4))
# ---
def polyCongr(coefficients, m):
        solutions = []
        for i in xrange(m):
                value = 0
                for degree in xrange(len(coefficients)):
                        value += coefficients[degree] * (i ** (len(coefficients) - degree - 1))
                if value % m == 0:
                        solutions.append(i)

        return solutions
# ---
def __getitem__(self, key):
            if key in self:
                return super(defaultdict, self).__getitem__(key)
            else:
                return self.__factory()
# ---
def perimeter_triangle(a,b,c):
  perimeter=a+b+c
  return perimeter
# ---
def convert_to_cache(self, value, record, validate=True):
        """ convert ``value`` to the cache level in ``env``; ``value`` may come from
            an assignment, or have the format of methods :meth:`BaseModel.read`
            or :meth:`BaseModel.write`

            :param record: the target record for the assignment, or an empty recordset

            :param bool validate: when True, field-specific validation of
                ``value`` will be performed
        """
        return value
# ---
def _list_jobs(filters: list[str] | None = None) -> list[dict]:
    """Fetch the list of jobs using the Ray CLI."""
    cmd = ["ray", "list", "jobs", "--detail", "--format=json", "--limit=10000"]
    for f in filters or []:
        cmd.extend(["--filter", f])

    result = subprocess.check_output(cmd, text=True, timeout=60)
    try:
        return json.loads(result)
    except json.JSONDecodeError:
        logger.warning(f"Failed to parse JSON output from ray list jobs: {result}")
        return []
# ---
def clear_basic_auth_token():
    global SAAGIE_BASIC_AUTH_TOKEN
    SAAGIE_BASIC_AUTH_TOKEN = None
# ---
def __setitem__(self, key, value):
        self.array.__setitem__(key, value)
# ---
def connection_lost(self, exception):
        """\
        Called when the serial port is closed or the reader loop terminated
        otherwise.
        """
        if isinstance(exception, Exception):
            logger.debug('Connection to port `%s` lost: %s', self.port,
                         exception)
        else:
            logger.debug('Connection to port `%s` closed', self.port)
        self.connected.clear()
        self.disconnected.set()
# ---
def negative(a: A) -> A:
    return wrap_elemwise_unary(jnp.negative, a)
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()
# ---
def __init__(self):
        self.verbose: bool = False
        self.config_file: str | None = None
        self.config_obj: RayClusterConfig | None = None
        self.tpu_name: str | None = None
        self.config_data: dict | None = None
# ---
def count_unset_bits(n): 
    count = 0
    x = 1
    while(x < n + 1): 
        if ((x & n) == 0): 
            count += 1
        x = x << 1
    return count
# ---
def output_exemplar(self):
        return {
            "input_ids": np.zeros((0,), dtype=np.int32),
            "assistant_masks": np.zeros((0,), dtype=np.int32),
        }
# ---
def get_timestamps(self, backend, dn):
        return ObjectBackendRegistry.backends[backend].get_timestamps(dn)
# ---
def visit_mrow(self, element):
        return self._visit_children(element)
# ---
def _ensure_batched(x):
        if len(x) == 0:
            return list(x)
        elif isinstance(x[0], Sequence) or isinstance(x[0], np.ndarray):
            return list(x)
        else:
            return [x]
# ---
def dequeue():
    try:
        dequeue_task()
    except Exception as e:
        return make_response(dumps(dict(status=e.message)), 202)

    return make_response(dumps(dict(status="ok")), 202)
# ---
def decorator(func):
        _checks[name] = func
        return func
# ---
def __init__(self, c_in, eps=1e-5):
        super(LayerNorm, self).__init__()

        self.c_in = (c_in,)
        self.eps = eps

        self.weight = nn.Parameter(torch.ones(c_in))
        self.bias = nn.Parameter(torch.zeros(c_in))
# ---
def _no_ssl_required_on_debug(app, **kwargs):
    if app.debug or app.testing:
        os.environ['AUTHLIB_INSECURE_TRANSPORT'] = '1'
# ---
def axis_indices(self, axis: AxisSelector) -> int | None:  # type: ignore
        ...
# ---
def matchsubinclude(f):
            for prefix, matcherargs in subincludes:
                if f.startswith(prefix):
                    mf = submatchers.get(prefix)
                    if mf is None:
                        mf = match(*matcherargs)
                        submatchers[prefix] = mf

                    if mf(f[len(prefix) :]):
                        return True
            return False
# ---
def fsspec_get_atomic_directories(dir_path):
    """
    Get all directories under this directory that only contains files within them
    """
    subdirectories = []

    if fsspec_isdir(dir_path):
        for subdir in fsspec_get_curr_subdirectories(dir_path):
            if fsspec_dir_only_contains_files(subdir):
                subdirectories.append(subdir)
            else:
                subdirectories.extend(fsspec_get_atomic_directories(subdir))

    return subdirectories
# ---
def compute_document_hashes(batch: pa.RecordBatch) -> pa.RecordBatch:
        pipeline = [
            Transformation.ResolveIds(text_col=config.text_field, id_col="id", output_col="resolved_id"),
            Transformation.Hash(input_col=config.text_field, output_col="hash", algo=dupekit.HashAlgorithm.Xxh3_128),
            Transformation.SelectColumns(columns=["hash", "resolved_id"]),
        ]
        return dupekit.transform(batch, pipeline)
# ---
def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):
        super(LW_MaxPooling2D, self).__init__(pool_size, strides, padding, data_format)
# ---
def test_apply_blockwise():
    bw_spec = make_blockwise_spec(
        key_function=make_map_blocks_key_function("a"),
        function=negative,
    )

    input_data = {"a": [0, 1, 2, 3, 4]}
    out = [apply_blockwise(input_data, [i], bw_spec) for i in range(5)]
    assert out == [0, -1, -2, -3, -4]
# ---
def broadcast_arrays_and_return_axes(
    *arrays: NamedArray, require_subset: bool = True, ensure_order: bool = True
) -> tuple[tuple[NamedArray, ...], tuple[Axis, ...]]: ...
# ---
def _nannumel(x, **kwargs):
    """A reduction to count the number of elements, excluding nans"""
    return nxp.sum(~(nxp.isnan(x)), **kwargs)
# ---
def take_L_and_F_set_bits(n) : 
    n = n | n >> 1
    n = n | n >> 2
    n = n | n >> 4
    n = n | n >> 8
    n = n | n >> 16 
    return ((n + 1) >> 1) + 1      
def toggle_F_and_L_bits(n) :  
    if (n == 1) : 
        return 0 
    return n ^ take_L_and_F_set_bits(n)
# ---
def stack_add(*a):
        return nxp.sum(nxp.stack(a), axis=0)
# ---
def cached_token_count(cache_path: str, field: str = "input_ids") -> int:
    """Return the total number of tokens stored in a finished TreeCache."""
    cache = TreeCache.load(cache_path, {field: np.zeros((0,), dtype=np.int32)})
    return cache.store.tree[field].data_size
# ---
def max_Pos(self) -> Axis:
        return Axis("position", self.max_seq_len)
# ---
def presentToolTipsChecked(self, widget):
        """Signal handler for the "toggled" signal for the
           presentToolTipsCheckButton GtkCheckButton widget.
           The user has [un]checked the 'Present ToolTips'
           checkbox. Set the 'presentToolTips'
           preference to the new value if the user can present tooltips.

        Arguments:
        - widget: the component that generated the signal.
        """

        self.prefsDict["presentToolTips"] = widget.get_active()
# ---
def __iter__(self):
        """Iterator for each row in all pages."""
        for page in self.pages:
            for row in page:
                yield row
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"backbone": None, "embeddings": None}
# ---
def __init__(self, eps=1e-3):
    super().__init__()
    self.eps = eps
    self.sigma_max = self.total_noise(torch.tensor(1.0))
    self.sigma_min = self.eps + self.total_noise(torch.tensor(0.0))
# ---
def __delitem__(self, item):
		if item in self._values:
			del self._values[item]
		del self._addresses[item]
# ---
def as_sync_dataset(self) -> "SyncDataset[T_co]":
        raise NotImplementedError("...")
# ---
def abs(self, f):
        """Convert a repo path back to path that is relative to the root of the
        matcher."""
        return f
# ---
def Table(self, *args, **kwargs):
        return wandb.Table(*args, **kwargs)
# ---
def compute_random_augmentation(
    multiplicity,
    s_trans=1.0,
    device=None,
    dtype=torch.float32
):
    R = random_rotations(multiplicity, dtype=dtype, device=device)
    random_trans = torch.randn((multiplicity, 1, 3), dtype=dtype, device=device) * s_trans
    return R, random_trans
# ---
def an_empty_ifc_project():
    bpy.ops.bim.create_project()
# ---
def get_calibration_widget(self):
        return self._calibration_widget
# ---
def first_Element(arr,n,k): 
    count_map = {}; 
    for i in range(0, n): 
        if(arr[i] in count_map.keys()): 
            count_map[arr[i]] += 1
        else: 
            count_map[arr[i]] = 1
        i += 1
    for i in range(0, n):  
        if (count_map[arr[i]] == k): 
            return arr[i] 
        i += 1 
    return -1
# ---
def __init__(self,
                 config,
                 debug):
        ''' Constructor for OCVersion '''
        super(OCVersion, self).__init__(None, config)
        self.debug = debug
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        """Invoke the method remotely. Returns a future."""
        ...
# ---
def setUp(self):
        self.escala = Escala('fixtures/escala.xml')
        self.dir = dirs.TestDir()
        self.maxDiff = None
# ---
def bitwise_and(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_and](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_and.html)
    """
    return jnp.bitwise_and(x1, x2)
# ---
def get_full_name(self):
        return self.nickname
# ---
def flatten(self, new_axis_name: AxisSelector) -> "NamedArray":  # pragma: no cover
        return haliax.flatten(self, new_axis_name=new_axis_name)
# ---
def has_a_finalist_role(self):
        return len(self.finalist_user_roles()) > 0
# ---
def set_decoder(self, decoder):
        """Set decoder (backbone) for the model."""
        self.bimamba = decoder
# ---
def test_resolve_unresolved_location(self):
        """Test resolving an UnresolvedLocation against an S3Location."""
        base = S3Location(bucket="test-bucket", path="base/path")
        unresolved = UnresolvedLocation(path="subdir/file.zarr")

        resolved = base.resolve(unresolved)

        assert isinstance(resolved, S3Location)
        assert resolved.bucket == "test-bucket"
        assert resolved.path == "base/path/subdir/file.zarr"
        assert resolved.endpoint_url == base.endpoint_url
# ---
def recur_gcd(a, b):
	low = min(a, b)
	high = max(a, b)
	if low == 0:
		return high
	elif low == 1:
		return 1
	else:
		return recur_gcd(low, high%low)
# ---
def wandb_xla_logger(config: WandbConfig):
    import wandb

    last_mtime = wandb.run and wandb.run.start_time or time.time()

    def log_xla_to_wandb(step: StepInfo):
        nonlocal last_mtime
        save_xla_dumps_to_wandb(last_mtime)
        # update time to now
        last_mtime = time.time()

    if config.save_xla_dumps:
        return log_xla_to_wandb
    else:
        return lambda x: None
# ---
def cluster():
    """Boots a local cluster via ClusterManager, yields (url, client)."""
    config = load_config(DEFAULT_CONFIG)
    config = make_local_config(config)
    manager = ClusterManager(config)
    with manager.connect() as url:
        client = IrisClient.remote(url, workspace=IRIS_ROOT)
        yield url, client
# ---
def find_Parity(x): 
    y = x ^ (x >> 1); 
    y = y ^ (y >> 2); 
    y = y ^ (y >> 4); 
    y = y ^ (y >> 8); 
    y = y ^ (y >> 16); 
    if (y & 1): 
        return ("Odd Parity"); 
    return ("Even Parity");
# ---
def do_fold(init: CarryT, *args, **kwargs) -> CarryT:
            return haliax.fold(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *args, **kwargs)
# ---
def is_lora_param(node):
    return isinstance(node, LowRankLinear)
# ---
def keys(self):
        """ @rtype: StringList """
        return SummaryKeyMatcher.cNamespace().keys(self)
# ---
def Vocab(self) -> Axis:
        return self.decoder.embeddings.Vocab
# ---
def matches(step: ExecutorStep) -> bool:
            # track which regexes have been used
            for i, regex in enumerate(regexes):
                if regex.search(step.name):
                    used_regexes.add(i)
                    return True

            return False
# ---
def test_filter_passing_empty_input():
    passing = filter_passing([], ["assert True"])
    assert passing == []
# ---
def get_secrets(self):
        ''' returns all of the defined secrets '''
        return self.get(Secret.secret_path) or {}
# ---
def local_tag(self) -> str:
        return f"{self.image_name}:{self.version}"
# ---
def __exit__(self, except_type, except_val, traceback):
        'Call finalize() and close the file.'
        try:
            self.finalize()
        finally:
            # Close again in case an exception happened in finalize()
            self.epub_f.close()
        return False
# ---
def _get_extension(file_path: str) -> str:
    for ext in sorted(SUPPORTED_EXTENSIONS, key=len, reverse=True):
        if file_path.endswith(ext):
            return ext
    raise ValueError(f"Unsupported extension: {file_path}.")
# ---
def __init__(self, config):
        self.config = config
# ---
def test_is_cloud_storage_path():
    assert not is_cloud_storage_path("relative_path/path")
    assert not is_cloud_storage_path("/absolute_path/path")
    assert not is_cloud_storage_path("file:relative_path/path")
    assert not is_cloud_storage_path("file://absolute_path/path")
    assert not is_cloud_storage_path("file:///absolute_path/path")
    assert is_cloud_storage_path("s3://host/path")
    assert is_cloud_storage_path("gs://host/path")
# ---
def __init__(self, x=0.0, y=0.0, z=0.0):
        super(Vector3D, self).__init__(x, y, z)
# ---
def test_parse_chat_completion_tokens_empty_response():
    """Test handling of empty response."""
    tokenizer = AutoTokenizer.from_pretrained("gpt2")

    chat_completion = create_mock_chat_completion_with_logprobs("", [], [])

    parsed_tokens = parse_chat_completion_tokens_from_bytes(chat_completion, tokenizer)
    assert parsed_tokens == []
# ---
def get_tpu_topology(tpu_type: str) -> TpuTopologyInfo:
    """Get TPU topology by type name."""
    for config in TPU_TOPOLOGIES:
        if config.name == tpu_type:
            return config
    raise ValueError(f"Unknown TPU type: {tpu_type}")
# ---
def loss_fn(m, x):
        y = m.fold(x)
        return hax.sum(y).scalar()
# ---
def __init__(self, model_name: str, attribute_name: str, model_type: str | None, *args, **kwargs):
        self.model_name = model_name
        self.model_type = model_type
        self.attribute_name = attribute_name
        self.cls = self.from_model_path(model_name, attribute_name, model_type, *args, **kwargs)
# ---
def dec(rc):
                page = seq_pages["page", i].scalar()
                return rc.at["page", page].add(-1)
# ---
def percentage(self) -> float:
        """Calculate usage percentage, handling unit conversions."""
        try:
            used_val = float(self.used.replace("TiB", "").replace("GiB", "").replace("MiB", "").replace("KiB", ""))
            total_val = float(self.total.replace("TiB", "").replace("GiB", "").replace("MiB", "").replace("KiB", ""))
            return (used_val / total_val * 100) if total_val > 0 else 0.0
        except (ValueError, ZeroDivisionError):
            return 0.0
# ---
def structure(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> Any:
    """Alias for :func:`haliax.tree_util.tree_structure` matching :func:`jax.tree.structure`."""

    return tree_util.tree_structure(tree, is_leaf=is_leaf)
# ---
def test___cmp__gt(self):
        self._test__cmp__(
            lambda left, right: left > right,
            (
                True,
                False,
                False,
                False,
                False,
                TypeError if PY3 else True,
                TypeError if PY3 else False,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
            ),
            '>'
        )
# ---
def getManagerInfo(atomFeed):
	try:
		entries = atomFeed.getElementsByTagName('entry')[1]
	except:
		return None
	try:
		managerId = entries.getElementsByTagName('snx:userid')[0]
		return managerId.firstChild.data
	except:
		return None
# ---
def test_order(self):
        assert (event.Priority.PRE_CORE
                > event.Priority.CORE
                > event.Priority.POST_CORE
                > event.Priority.PRE_DEFAULT
                > event.Priority.DEFAULT
                > event.Priority.POST_DEFAULT)
# ---
def best_inference_checkpoint_path(self) -> Path:
        return self.checkpoint_dir / "best_inference_ckpt.pt"
# ---
def init(cls, Vocab: Axis, config: MConfig, *, key: PRNGKeyArray) -> "ModelWithHfSerializationMixin":
        pass
# ---
def div_of_nums(nums,m,n):
 result = list(filter(lambda x: (x % m == 0 and x % n == 0), nums)) 
 return result
# ---
def unprotect(self, tag: str) -> None:
        """Remove protection from an image (job completed)."""
        ...
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        object_ref = self._ray_method.remote(*args, **kwargs)
        return RayActorFuture(object_ref)
# ---
def __init__(self, hits=None, docs=None):
        """Parse hits into docs."""
        self.hits = hits if hits else self.no_hits
        self.docs = docs if docs else []
# ---
def apply_mask(ds: xr.Dataset, mask: xr.DataArray):
    """Applies mask to same and lower dimensional data"""
    ds_out = xr.Dataset(attrs=ds.attrs)
    for var in ds.data_vars:
        data = ds[var]
        mask_pruned = _pick_first_element_of_missing_dims(mask, data)
        ds_out[var] = data.where(mask_pruned)
    return ds_out
# ---
def request(self, method, url, body=None, headers=None):
        self.method = method
        self.selector = url
        if headers is not None:
            self.req_headers += headers.items()
        self.req_headers.sort()
        if body:
            self.data = body
        if self.raise_on_endheaders:
            import socket
            raise socket.error()
# ---
def token(self):
        """Attempt to return the auth header token.

        :return: token related to request
        """
        prefixes = ("Bearer", "Token")
        auth_header = self.headers.get("Authorization")

        if auth_header is not None:
            for prefix in prefixes:
                if prefix in auth_header:
                    return auth_header.partition(prefix)[-1].strip()

        return auth_header
# ---
def cumprod(self, axis: AxisSelector, *, dtype=None) -> "NamedArray":  # pragma: no cover
        return haliax.cumprod(self, axis=axis, dtype=dtype)
# ---
def create_qwen_tokenizer():
    return AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
# ---
def play_one_episode(player, func, verbose=False):
    def f(s):
        spc = player.get_action_space()
        act = func([[s]])[0][0].argmax()
        if random.random() < 0.001:
            act = spc.sample()
        if verbose:
            print(act)
        return act
    return np.mean(player.play_one_episode(f))
# ---
def callback(ctx: click.Context, json_str: str | None, **kwargs):
            field_values = {k: v for k, v in kwargs.items() if v is not None}
            request = build_request(method, json_str, field_values)
            response = call_rpc(self.service_name, method.name, controller_url, request)
            click.echo(format_response(response))
# ---
def _solve_puzzle_parts(self):
        old_nice_count = 0
        new_nice_count = 0
        for string in self._puzzle_input:
            if not string:
                continue
            if self.__is_nice_string_using_old_rules(string):
                old_nice_count += 1
            if self.__is_nice_string_using_new_rules(string):
                new_nice_count += 1
        return (old_nice_count, new_nice_count)
# ---
def read_parquet_file(filepath: str) -> list[dict]:
    """Helper function to read a Parquet file"""
    import pandas as pd

    df = pd.read_parquet(filepath)
    return df.to_dict("records")
# ---
def __init__(self, fs: AbstractFileSystem):
        super().__init__()
        self._fs = fs
        import concurrent.futures

        self._executor = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENT_CHUNKS)
# ---
def contains(self, bbox, srs):
        bbox = self._bbox_in_coverage_srs(bbox, srs)
        return bbox_contains(self.bbox, bbox)
# ---
def test_vm_manager_create_returns_vm_group(
    mock_run: MagicMock,
    vm_manager_factory: tuple[str, object],
):
    """VmManager.create_vm_group() returns a VmGroup with VMs."""
    mock_run.return_value = MagicMock(returncode=0, stdout="", stderr="")
    _platform_type, manager = vm_manager_factory

    vm_group = manager.create_vm_group()

    assert vm_group is not None
    assert len(vm_group.vms()) >= 1
# ---
def Vocab(self) -> Axis:
        return self._Vocab
# ---
def __init__(self, notebook, job_data):
        self.notebook = notebook
        self.data = job_data
        self.platform_id = job_data['platform_id']
        self.capsule_type = job_data['capsule_code']
        self.id = job_data['id']
        self.name = job_data['name']
        self.last_run = None
# ---
def arrays_only(x):
    return eqx.filter(x, eqx.is_array_like)
# ---
def finish(self):
        self._run.finish()
# ---
def getPluginsWithStatus(): pass
# ---
def __init__(self, perm):
        self.perm = perm
# ---
def binary_search(item_list,item):
	first = 0
	last = len(item_list)-1
	found = False
	while( first<=last and not found):
		mid = (first + last)//2
		if item_list[mid] == item :
			found = True
		else:
			if item < item_list[mid]:
				last = mid - 1
			else:
				first = mid + 1	
	return found
# ---
def __init__(self, text):
        self.text = text
        self.clicked = Signal()
# ---
def _denoiser_update(self, x, t):
    sigma, _ = self.noise(t)
    score = self.get_score(x, sigma)
    stag_score = self._staggered_score(score, sigma)
    probs = stag_score * self._transp_transition(x, sigma)
    probs[..., self.mask_index] = 0
    samples = _sample_categorical(probs)
    return samples
# ---
def neg_nos(list1):
  for num in list1: 
    if num < 0: 
       return num
# ---
def check_monthnumber(monthname3):
  if monthname3 =="April" or monthname3== "June" or monthname3== "September" or monthname3== "November":
    return True
  else:
    return False
# ---
def _is_parseable(source: str, node_type: str) -> bool:
    """Check that a code fragment parses back successfully.

    Statements are parsed as-is. Expressions are wrapped in an assignment
    to form a valid statement for ast.parse().
    """
    try:
        if node_type in STATEMENT_TYPES:
            ast.parse(source)
        else:
            # Expressions need a statement wrapper to parse.
            ast.parse(f"__x = {source}")
        return True
    except SyntaxError:
        return False
# ---
def Odd_Length_Sum(arr):
    Sum = 0
    l = len(arr)
    for i in range(l):
        Sum += ((((i + 1) *(l - i) + 1) // 2) * arr[i])
    return Sum
# ---
def repeat_tuples(test_tup, N):
  res = ((test_tup, ) * N)
  return (res)
# ---
def terminate(self) -> None: ...
# ---
def visit_mo(self, element):
        text = element.get_text().strip()
        if text in _MATH_OPERATORS:
            return MacroNode(_MATH_OPERATORS[text])
        return OperatorNode(text)
# ---
def test_random_mutation_produces_valid_python(bank):
    source = CORPUS[0]  # fibonacci
    rng = random.Random(42)

    mutation = random_mutation(source, bank, rng=rng)
    assert mutation is not None

    mutated = mutation.apply(source)
    # The result must be valid Python.
    try:
        ast.parse(mutated)
    except SyntaxError:
        pytest.fail(f"Mutation produced invalid Python:\n{mutated}")
# ---
def add_volume_mount(self, volume_mount):
        ''' add a volume or volume mount to the proper location '''
        exist_volume_mounts = self.get_volume_mounts()

        if not exist_volume_mounts and volume_mount:
            self.put(DeploymentConfig.volume_mounts_path, [volume_mount])
        else:
            exist_volume_mounts.append(volume_mount)
# ---
def disconnect(self):
        self.worker.quit()
        self.serial.close()
        self.connectAction.setText(self.tr("Connect"))
        self.connectAction.setIcon(QIcon(pixmap("network-connect-3.png")))
        self.serialdlgAction.setEnabled(True)
        self.connectionstateLabel.setText(self.tr("Not connected"))
        self._connected = False
        self.objectexplorer.refresh()
# ---
def _make_grug_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape(1, 1, 1, len(devices))
    return Mesh(
        mesh_devices,
        axis_names=("replica_dcn", "replica", "data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit, AxisType.Explicit, AxisType.Explicit),
    )
# ---
def test_pspec_for_plain_array_axis_names_nested_module():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        mod = NestedArrayModule(ArrayModule(jnp.ones((Dim2.size, Dim3.size))))

        specs: NestedArrayModule = pspec_for(mod)

        assert specs.inner.arr == PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        """Map from Levanter model names to HF."""
        return {"transformer": "model", "embeddings": None}
# ---
def copy_specification_from_item_group(self):
		self.set("website_specifications", [])
		if self.item_group:
			for label, desc in frappe.db.get_values("Item Website Specification",
										   {"parent": self.item_group}, ["label", "description"]):
				row = self.append("website_specifications")
				row.label = label
				row.description = desc
# ---
def npartitions(self):
        """Number of chunks in the array."""
        return reduce(mul, self.numblocks, 1)
# ---
def _dashboard(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Iris Controller", "/static/controller/app.js"))
# ---
def finish(self):
        """Finish the wandb run."""
        if self._enabled:
            wandb.finish()
# ---
def push_to_gcp(local_id, project_id, region, repository) -> str:
    """Pushes a local Docker image to Artifact Registry."""
    configure_gcp_docker(project_id, region, repository)

    artifact_repo = f"{region}-docker.pkg.dev/{project_id}/{repository}"

    full_image_name = f"{artifact_repo}/{local_id}"
    _run(["docker", "tag", local_id, full_image_name])
    _run(["docker", "push", full_image_name])

    return f"{artifact_repo}/{local_id}"
# ---
def _get_size(n_items):
    """
    Calculate the size of the subplot layouts based on number of items.
    """
    n_cols = math.ceil(math.sqrt(n_items))
    n_rows = math.floor(math.sqrt(n_items))
    if n_cols * n_rows < n_items:
        n_cols += 1
    return int(n_rows), int(n_cols)
# ---
def BufferIsUsable( buffer_object ):
  return not BufferModified( buffer_object ) or HiddenEnabled( buffer_object )
# ---
def union(matches, root, cwd):
    """Union a list of matchers.

    If the list is empty, return nevermatcher.
    If the list only contains one non-None value, return that matcher.
    Otherwise return a union matcher.
    """
    matches = list(filter(None, matches))
    if len(matches) == 0:
        return nevermatcher(root, cwd)
    elif len(matches) == 1:
        return matches[0]
    else:
        return unionmatcher(matches)
# ---
def test_voter_get_requests_redirected_to_index(self):
        self.client.logout()
        self.client.login(username='user0', password='voter')

        response = self.client.get(reverse('results-export'), follow=True)
        self.assertRedirects(response, reverse('index'))
# ---
def __getitem__(self, idx: int) -> Dict:
        """Get an item from the dataset.

        Returns
        -------
        Dict[str, Tensor]
            The sampled data features.

        """
        path = Path(self.yaml_paths[idx % len(self.yaml_paths)])
        feat = self.get_sample(path)
        data_sample_idx = idx // len(self.yaml_paths) + self.skip_offset
        if self.dataset.multiplicity > 1:
            feat["data_sample_idx"] = data_sample_idx
        return feat
# ---
def __setitem__(self, key, value):
        self.subqueries[key] = value
# ---
def __call__(self, view_or_request, *args, **kwargs):
        if not self.wrapped:
            self.view = view_or_request

        def dec(*args, **kwargs):
            try:
                return self.view(*args, **kwargs)
            except PermissionRequired as e:
                kwargs['_perm'] = e.perm
                kwargs['_view'] = self.view
                return self.error_view(*args, **kwargs)

        return dec(view_or_request, *args, **kwargs) if self.wrapped else dec
# ---
def fullscreen(self):
        """True if the window is currently fullscreen.  Read-only.

        :type: bool
        """
        return self._fullscreen
# ---
def _chunk_sum(a, axis=None, dtype=None, keepdims=None):
    return nxp.sum(a, axis=axis, dtype=dtype, keepdims=True)
# ---
def device_info(self):
        """Device info."""
        return self.coordinator.device_info
# ---
def send_config_change_event(msg, error=EventError.ErrorTypes.NO_ERROR, pubsub=INJECTED):
        # type: (str, Dict[str, Any], PubSub) -> None
        event = EsafeEvent(EsafeEvent.Types.CONFIG_CHANGE, {'type': 'apartment', 'msg': msg}, error=error)
        pubsub.publish_esafe_event(PubSub.EsafeTopics.CONFIG, event)
# ---
def cond(state: tuple[GenState, _DecodeOutputs, jax.Array]):
        _gen_state, _outputs, step = state
        return (
            (step < max_rounds)
            & (_gen_state.decode_state.num_queued_tokens > 0)
            & (~hax.all(_gen_state.decode_state.finished)).scalar()
        )
# ---
def test_arange(spec, executor):
    a = xp.arange(12, chunks=(5,), spec=spec)
    assert_array_equal(a.compute(executor=executor), np.arange(12))
# ---
def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)  # BatchNorm2d
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        return x
# ---
def mock_open(path, mode="rb"):
            if path in files:
                return io.BytesIO(files[path])
            raise FileNotFoundError(f"File not found: {path}")
# ---
def on_ok(self, *args):
        pass
# ---
def test_many(self, capfd):
        with lsof_check():
            for i in range(10):
                cap = StdCaptureFD()
                cap.stop_capturing()
# ---
def datasource_from_jsonl(urls_or_paths: Sequence[str]) -> ShardedDataSource[dict]:
    return JsonlDataSource(urls_or_paths)
# ---
def write_int_to_file(path, i):
    with fsspec.open(path, "w") as f:
        f.write(str(i))
# ---
def test_function_body_difference():
    source = "def f(x):\n    return x + 1\n"
    target = "def f(x):\n    return x * 2\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1

    # The edit should target the body, not the whole function.
    for edit in edits:
        assert edit.node_type != "FunctionDef", "Should target a specific sub-expression, not the whole function"
# ---
def cluster_backup_jobs(ctx, backup_dir):
    """Backup Ray jobs to specified directory."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        Path(backup_dir).mkdir(parents=True, exist_ok=True)
        _backup_jobs(ctx.obj.config_file, backup_dir)
        print(f"Jobs backed up successfully to {backup_dir}")
# ---
def arrays_to_plan(*arrays):
    plans = [x.plan for x in arrays if hasattr(x, "plan")]
    if len(plans) == 0:
        raise ValueError(f"No plans found for arrays: {arrays}")
    return plans[0].arrays_to_plan(*arrays)
# ---
def scan_checkpoint_policy_decode(policy: str | dict | bool):
        if not isinstance(policy, dict):
            return ScanCheckpointPolicy.from_bool_or_str(policy)

        from draccus.parsers.decoding import decode_dataclass

        return decode_dataclass(ScanCheckpointPolicy, policy)
# ---
def test_map_overlap_trim():
    x = np.array([1, 1, 2, 3, 5, 8, 13, 21])
    a = xp.asarray(x, chunks=5)

    def derivative(x):
        out = x - np.roll(x, 1)
        return out[1:-1]  # manual trim

    b = cubed.map_overlap(
        derivative,
        a,
        dtype=a.dtype,
        chunks=a.chunks,
        depth=1,
        boundary=0,
        trim=False,
    )

    assert_array_equal(b.compute(), np.array([1, 0, 1, 1, 2, 3, 5, 8]))
# ---
def size(self) -> int:
        """
        The total number of blocks in the array.
        """
        return math.prod(self.shape)
# ---
def _(self):
        return self.translator.ugettext
# ---
def Convert(string): 
    li = list(string.split(" ")) 
    return li
# ---
def __init__(
        self, metadata: dict[str, dict[str, str]] | None = None, hist: int = 0
    ):
        """
        Args:
            metadata: Mapping of variable names their metadata that will
                used in generating logged image captions.
            hist: Number of history steps to include in the snapshot.
        """
        if metadata is None:
            metadata = {}
        else:
            self._metadata = metadata
        self.hist = hist
# ---
def batched(iterable: Iterable[T], batch_size: int) -> Iterator[List[T]]:
    """Yields batches of the given size from the given iterable."""
    batch = []
    for item in iterable:
        batch.append(item)
        if len(batch) == batch_size:
            yield batch
            batch = []

    if len(batch) > 0:
        yield batch
# ---
def clear(self):
        """Clear the window.

        This is a convenience method for clearing the color and depth
        buffer.  The window must be the active context (see `switch_to`).
        """
        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)
# ---
def mark_exact_dups_paragraphs(batches: Iterator[pa.RecordBatch]) -> Iterator[pa.RecordBatch]:
        """Mark duplicate paragraphs in a single record using exact hash matching."""

        dup_map = _load_dupe_map_shard(duplicate_key_shards)

        for batch in batches:
            yield dupekit.mark_paragraph_duplicates(
                batch,
                dup_map,
                attribute_name=str(DedupMode.EXACT_PARAGRAPH),
                algorithm=dupekit.HashAlgorithm.Xxh3_128,
            )
# ---
def filter(self, label):
                    for pattern, actype in AccountsPage.TYPES.items():
                        if label.startswith(pattern):
                            return actype
                    return Account.TYPE_UNKNOWN
# ---
def _do_jit_log(metrics, *, step=None):
    try:
        if _global_tracker is None:
            warnings.warn("No global tracker set")
        else:
            _global_tracker.log(metrics, step=step, commit=False)
    except Exception:
        logger.exception("Error logging metrics")
# ---
def prediction_data(input_data):
    return input_data[["so", "thetao", "uo", "vo", "zos"]].drop_vars("wetmask")
# ---
def fn():
        if done:
            return end - start
        else:
            return time.perf_counter() - start
# ---
def arc_length(d,a):
    pi=22/7
    if a >= 360:
        return None
    arclength = (pi*d) * (a/360)
    return arclength
# ---
def default_window_title(self):
		"""()"""
		if TRACE: print(__name__), self.default_window_title.__doc__

		return "Wasp"
# ---
def is_remote_branch(git_path, module, dest, remote, version):
    cmd = '%s ls-remote %s -h refs/heads/%s' % (git_path, remote, version)
    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)
    if to_native(version, errors='surrogate_or_strict') in out:
        return True
    else:
        return False
# ---
def test_trim_exact_length_no_change():
    """Test that array of exact length is unchanged."""
    ary = np.array([1, 2, 3, 4, 5], dtype=np.int32)
    result = train_batch.trim_and_pad(ary, max_seq_len=5, pad_to=5, padding_value=999)

    np.testing.assert_array_equal(result, ary)
# ---
def lateralsurface_cuboid(l,w,h):
  LSA = 2*h*(l+w)
  return LSA
# ---
def create_test_entrypoint():
    """Create a simple test entrypoint."""
    from dataclasses import dataclass

    @dataclass
    class Entrypoint:
        callable: object
        args: tuple = ()
        kwargs: dict | None = None

        def __post_init__(self):
            if self.kwargs is None:
                self.kwargs = {}

    def test_fn():
        print("Hello from test")

    return Entrypoint(callable=test_fn)
# ---
def __del__(self):
        if self.tb:
            self.hndl(self.cls, self.tb)
# ---
def resolve(self, location: "Location") -> "ResolvedLocation":
        pass
# ---


def modp(n: int, p: int):
    """Return 2^n modulo p (be aware of numerics).
    >>> modp(3, 5)
    3
    >>> modp(1101, 101)
    2
    >>> modp(0, 101)
    1
    >>> modp(3, 11)
    8
    >>> modp(100, 101)
    1
    """
    ret = 1
    for i in range(n):
        ret = (2 * ret) % p
    return ret
# ---
def set_image_metadata(self, image_id, meta):
        """Sets the metadata for an image."""
        post_body = json.dumps({'metadata': meta})
        resp, body = self.put('images/%s/metadata' % str(image_id), post_body)
        body = json.loads(body)
        self.validate_response(schema.image_metadata, resp, body)
        return service_client.ResponseBody(resp, body['metadata'])
# ---
def scale_group_config() -> config_pb2.ScaleGroupConfig:
    """A standard scale group configuration."""
    return config_pb2.ScaleGroupConfig(
        name="test-group",
        min_slices=0,
        max_slices=5,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-8",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def stop(self) -> None:
        """Stop controller."""
        ...
# ---
def test_preemptible_true_produces_no_constraints(self):
        resources = ResourceConfig(preemptible=True)
        constraints = convert_constraints(resources)
        assert constraints == []
# ---
def next_interval(self) -> float:
        interval = self._initial * (self._factor**self._attempt)
        interval = min(interval, self._maximum)

        if self._jitter > 0:
            jitter = interval * self._jitter * (2 * random.random() - 1)
            interval = max(0.001, interval + jitter)

        self._attempt += 1
        return interval
# ---
def __init__(self, tracker: "Tracker"):
        self.tracker = tracker
# ---
def remove_spaces(str1):
  str1 = str1.replace(' ','')
  return str1
# ---
def even_bit_toggle_number(n) : 
    res = 0; count = 0; temp = n    
    while(temp > 0 ) : 
        if (count % 2 == 0) : 
            res = res | (1 << count)      
        count = count + 1
        temp >>= 1 
    return n ^ res
# ---
def get_inherited(self, name, default = UnboundLocalError, depth = 1):
        ctx = self.get_containing(name, depth = depth)
        if ctx is None:
            if default is UnboundLocalError:
                raise AttributeError('Attribute %s not found in %s' % (name, self))
            return default
        return object.__getattribute__(ctx, name)
# ---
def broadcast_arrays(
    *arrays: NamedOrNumeric | None, require_subset: bool = True, ensure_order: bool = True
) -> tuple[NamedOrNumeric | None, ...]: ...
# ---
def test_direct_instantiation_raises():
    with pytest.raises(TypeError):
        DummyMultiton()
# ---
def get_values(self, min_value, max_value, step=1):
        decimal_step = Decimal(str(step))
        value = Decimal(str(min_value))
        while value <= max_value:
            yield value
            value += decimal_step
# ---
def is_Product_Even(arr,n): 
    for i in range(0,n): 
        if ((arr[i] & 1) == 0): 
            return True
    return False
# ---
def _validate_configuration(self):
    assert not (self.change_of_variables
                and self.importance_sampling)
    if self.parameterization == 'sedd':
      assert not self.importance_sampling
      assert not self.change_of_variables
    if self.parameterization == 'd3pm':
      assert self.T > 0
    if self.T > 0:
      assert self.parameterization in {'d3pm', 'subs'}
    if self.subs_masking:
      assert self.parameterization == 'd3pm'
# ---
def all(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.all](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.all.html#jax.numpy.all).
    """
    return wrap_reduction_call(jnp.all, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def test_no_elements():
    partial_order = (...,)
    candidates = ()
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ()
# ---
def test_exception_wrapping_dbapi(self):
        conn = testing.db.connect()
        for _c in testing.db, conn:
            assert_raises_message(
                tsa.exc.DBAPIError,
                r"not_a_valid_statement",
                _c.execute, 'not_a_valid_statement'
            )
# ---
def rombus_perimeter(a):
  perimeter=4*a
  return perimeter
# ---
def vocab_size(self) -> int:
        return self._vocab_size
# ---
def product(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.product(self, axis=axis, dtype=dtype, where=where)
# ---
def predecessors_unordered(dag, name):
    """Return a node's predecessors in no particular order, with repeats for multiple edges."""
    for pre, _ in dag.in_edges(name):
        yield pre
# ---
def get_fn_name(fn: ExecutorFunction, short: bool = False):
    """Just for debugging: get the name of the function."""
    if fn is None:
        return "None"
    import ray

    if isinstance(fn, ray.remote_function.RemoteFunction):
        return fn._function.__name__
    if short:
        return f"{fn.__name__}"
    else:
        return str(fn)
# ---
def dump(sql, *multiparams, **params):
            buf.write(util.text_type(sql.compile(dialect=engine.dialect)))
# ---
def product(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.product, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def __iter__(self):
        for i in range(self.begin, self.end, self.stride):
            yield self[i]
# ---
def test_position_from_token_non_position(tok):
    with pytest.raises(ValueError):
        tok.position_from_token(tok.sos_token_id)
# ---
def Postprocessing (self):
        self.Log ("Incoming calls test case: Cleaning up after test...")

        if not self.Caller is None:
            self.Caller.release ()
        if not self.Receptionist is None:
            self.Receptionist.release ()
        if not self.Receptionist_2 is None:
            self.Receptionist_2.release ()
        if not self.Callee is None:
            self.Callee.release ()
# ---
def body(i, cmap):
            tid = target_slot_ids["position", i].scalar()

            def do(c):
                return c.at["seq", tid].set(INVALID)

            return jax.lax.cond(is_valid(tid), do, lambda c: c, cmap)
# ---
def sum_Of_Primes(n): 
    prime = [True] * (n + 1)  
    p = 2
    while p * p <= n: 
        if prime[p] == True:  
            i = p * 2
            while i <= n: 
                prime[i] = False
                i += p 
        p += 1    
    sum = 0
    for i in range (2,n + 1): 
        if(prime[i]): 
            sum += i 
    return sum
# ---
def successors_unordered(dag, name):
    """Return a node's successors in no particular order, with repeats for multiple edges."""
    for _, succ in dag.out_edges(name):
        yield succ
# ---
def EvalBatch(self):
        return Axis(self.batch_axis_name, self.eval_batch_size)
# ---
def test_uniform_with_bounds_broadcast_and_scalar():
    key = jax.random.PRNGKey(0)
    lb = hax.full(Height, -3.0)
    ub = 0.5
    u = hax.random.uniform(key, shape=(Height, Width), minval=lb, maxval=ub)

    assert u.axes == (Height, Width)

    assert hax.all(u >= -3.0)
    assert hax.all(u <= 0.5)
# ---
def list_cluster_workers(zone: str, project: str) -> list[str]:
    result = subprocess.run(
        [
            "gcloud",
            "compute",
            "tpus",
            "tpu-vm",
            "list",
            f"--zone={zone}",
            f"--project={project}",
            "--format=value(name)",
        ],
        capture_output=True,
        text=True,
        check=True,
    )
    return [name for name in result.stdout.strip().split("\n") if name]
# ---
def get_default_zone() -> Optional[str]:
    try:
        result = subprocess.run(["gcloud", "config", "get-value", "compute/zone"], stdout=subprocess.PIPE, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        return None
# ---
def _ViewEventHandler(f):
    f._view = True
    return f
# ---
def terminate(self):
        """
        Tells the logger process to exit immediately. If you do not call 'flush' method before, you may lose some
        messages of progresses that have not been displayed yet. This method blocks until logger process has stopped.
        """
        self.queue.put(dill.dumps(ExitCommand()))

        if self.process:
            self.process.join()
# ---
def check_subset(list1,list2): 
    return all(map(list1.__contains__,list2))
# ---
from itertools import groupby 
def extract_elements(numbers, n):
    result = [i for i, j in groupby(numbers) if len(list(j)) == n] 
    return result
# ---
def layers(self) -> Sequence[Olmo3DecoderLayer]:
        return cast(Sequence[Olmo3DecoderLayer], self._layers.unstacked())
# ---
def rsqrt(a: A) -> A:
    return wrap_elemwise_unary(jax.lax.rsqrt, a)
# ---
def intersect_axes(ax1: AxisSpec, ax2: AxisSelection) -> AxisSpec:  # type: ignore
    ...
# ---
def test_log_summary():
    with tempfile.TemporaryDirectory() as tmpdir:
        with SummaryWriter(logdir=tmpdir) as writer:
            tracker = TensorboardTracker(writer)
            tracker.log_summary({"float": 2.0})
            tracker.log_summary({"str": "test"})
            tracker.log_summary({"scalar_jax_array": jnp.array(3.0)})
            tracker.log_summary({"scalar_np_array": np.array(3.0)})
# ---
def add(self, x: Arrayish, total: Arrayish) -> "RunningMean":
        delta = x - self.mean
        # careful: total and self.total can be 0
        new_total = self.total + total
        ratio = hax.where(new_total, total / new_total, 0.0)
        new_mean = self.mean + delta * ratio
        new_total = self.total + total
        return RunningMean(new_mean, new_total)
# ---
def noop():
    pass
# ---
def caption(self):
        """The window caption (title).  Read-only.

        :type: str
        """
        return self._caption
# ---
def lista_valores(self):
        v  = [self.__emisornombre,self.__fechatimbrado, self.__tipo, self.__emisorrfc ]
        v += [self.__uuid, self.__folio, self.__receptornombre, self.__receptorrfc ]
        v += [self.__subtotal, self.__trieps, self.__triva]
        v += [self.__retiva, self.__retisr, self.__tcambio, self.__total]
        return v
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        for tracker in self.loggers:
            tracker.log_artifact(artifact_path, name=name, type=type)
# ---
def load_cache(cache_dir: str, *, seq_len: int) -> TreeCache[dict]:
    exemplar = {"input_ids": np.zeros(seq_len, dtype=np.int32)}
    return TreeCache.load(cache_dir, exemplar)
# ---
def check(string) :
    p = set(string) 
    s = {'0', '1'} 
    if s == p or p == {'0'} or p == {'1'}: 
        return ("Yes") 
    else : 
        return ("No")
# ---
def forward_once(self, fts):
        raise NotImplementedError()
# ---
def quantize(x, q_dtype, scale, compute_dtype):
    # Explicitly cast the max values to the compute dtype to avoid unnecessary
    # casting to FP32 during the subsequent math operations."
    dtype_max = get_fp8_max(q_dtype, compute_dtype)
    scaled_x = x / jnp.broadcast_to(scale.astype(compute_dtype), x.shape)
    clipped_x = jnp.clip(scaled_x, -dtype_max, dtype_max)
    return clipped_x.astype(q_dtype)
# ---
def test_forward_output_dtype_is_float32(params, tiny_cfg):
    """Logits should always be float32 regardless of compute_dtype."""
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (1, 8), 1, 100)
    logits = forward(params, token_ids, tiny_cfg)
    assert logits.dtype == jnp.float32
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype):
        # Use trivial cache dimensions; the cache is unused by this dummy model
        kv_heads = Axis("kv_head", 1)
        head_size = Axis("embed", 1)
        return KvPageCache.init(spec, kv_heads, head_size, dtype=dtype)
# ---
def the_void_name_is_not_filled_by_filling(name, filling):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(rel.RelatedBuildingElement.Name == filling for rel in element.HasFillings):
        assert False, "A filling was found"
# ---
def test_str_representation(self):
        """Test string representation of LocalLocation."""
        loc = LocalLocation(path=Path("/tmp/data"))
        assert str(loc) == "/tmp/data"
# ---
def config_options(self):
        ''' return config options '''
        return self._options
# ---
def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Kill all tracked processes and clean up venv directory."""
        self._job_group.__exit__(exc_type, exc_val, exc_tb)
        try:
            self._temp_dir.cleanup()
        except Exception as e:
            logger.warning(f"Failed to cleanup temporary directory: {e}")
# ---
def internal_shortname(self):
        return self.__data['str_shortname']
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids)
        x = self.norm(x)
        return x
# ---
def __del__(self):
        # Always try to clean up the window when it is dereferenced.
        # Makes sure there are no dangling pointers or memory leaks.
        # If the window is already closed, pass silently.
        try:
            self.close()
        except:   # XXX  Avoid a NoneType error if already closed.
            pass
# ---
def job_id(self) -> JobName:
        return self.task_id.parent or self.task_id
# ---
def __missing__(self, key):
        try:
            return super().__missing__(key)
        except KeyError:
            return NotImplemented
# ---
def remove_even(str1):
 str2 = ''
 for i in range(1, len(str1) + 1):
    if(i % 2 != 0):
        str2 = str2 + str1[i - 1]
 return str2
# ---
def run(
        self,
        cmd: list[str],
        *,
        check: bool = True,
        env: dict[str, str] | None = None,
        **kwargs,
    ) -> subprocess.CompletedProcess:
        """Run a command and wait for completion."""
        self._check_entered()

        if "cwd" not in kwargs and self._cwd:
            kwargs["cwd"] = self._cwd

        logger.info("Running %s", " ".join(cmd))
        return subprocess.run(cmd, env=env, check=check, **kwargs)
# ---
def _get_go_namespace(self, source):
    with open(source) as thrift:
      namespace = self.NAMESPACE_PARSER.search(thrift.read())
      if not namespace:
        raise TaskError('Thrift file {} must contain "namespace go "', source)
      return namespace.group(1)
# ---
def get(self, ref):
        """Retrieve an object from Ray's object store."""
        return ray.get(ref)
# ---
def remove_widget(self, widget):
        """Remove a widget (currently only L{TextBox}s are accepted) from
            the list of widgets to do auto-correction for.
            """
        if isinstance(widget, TextBox) and widget in self.widgets:
            self._remove_textbox(widget)
# ---
def test_composed_int_desc(self):
        table = self.tables.some_table
        lx = (table.c.x + table.c.y).label("lx")
        self._assert_result(
            select([lx]).order_by(lx.desc()), [(7,), (5,), (3,)]
        )
# ---
def generate_job_name(command: str) -> str:
    """Generate job name from command."""
    parts = command.split()
    entrypoint = parts[0] if parts else "unknown"
    if "/" in entrypoint:
        entrypoint = entrypoint.split("/")[-1]
    if "." in entrypoint:
        entrypoint = entrypoint.split(".")[0]

    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    return f"fray-{getpass.getuser()}-{entrypoint}-{timestamp}"
# ---
def load_results_file(path: str) -> dict:
    fs = fsspec.filesystem(path.split("://", 1)[0] if "://" in path else "file")
    with fs.open(path, "r") as f:
        data = json.load(f)
        return {"runs": [data]} if "runs" not in data else data
# ---
def start(self):
        self.thread.start()
        self._wait_for_server()
# ---
def _sleep_then_succeed():
    time.sleep(0.1)
# ---
def strip_lines(self):
        """
        Remove excessive number of lines. This deletes the oldest half.
        """
        if (self.num_lines > MAX_NUM_STORED_LINES):
            for i in range(MAX_NUM_STORED_LINES // 2):
                self.lines.pop(i)
# ---
def No_of_Triangle(N,K):
    if (N < K):
        return -1;
    else:
        Tri_up = 0;
        Tri_up = ((N - K + 1) *(N - K + 2)) // 2;
        Tri_down = 0;
        Tri_down = ((N - 2 * K + 1) *(N - 2 * K + 2)) // 2;
        return Tri_up + Tri_down;
# ---
def testWithAs(self):
    self.assertEqual((0, '1 2 3\n'),
                     _GrumpRun(textwrap.dedent("""\
        class ContextManager(object):
          def __enter__(self):
            return (1, (2, 3))
          def __exit__(self, *args):
            pass
        with ContextManager() as [x, (y, z)]:
          print x, y, z
        """)))
# ---
def vmap_fun(x):
        return x.take(Width, 2)
# ---
def __init__(self, slice_info: SliceInfo):
        self._awaitable: ray.ObjectRef | None = None
        self._host_info: TPUHostInfo | None = None
        self._slice_info = slice_info
# ---
def get(self, name, default=None):
        """Return the first value, either the default or actual"""
        return super().get(name, [default])[0]
# ---
def shard_names(self) -> Sequence[str]:
        return [f"shard_{i}" for i in range(self._num_shards)]
# ---
def divide(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "divide")
    if x1.dtype not in _floating_dtypes or x2.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in divide")
    return elemwise(nxp.divide, x1, x2, dtype=result_type(x1, x2))
# ---
def drop_vars(self, *names):  # pragma: no cover
        warnings.warn('the Dataset method `drop_vars` has been deprecated; '
                      'use `drop` instead',
                      FutureWarning, stacklevel=2)
        return self.drop(names)
# ---
def load_dataset_with_backoff(
    *,
    context: str,
    max_attempts: int = 6,
    initial_delay: float = 2.0,
    max_delay: float = 120.0,
    logger: logging.Logger | None = None,
    **dataset_kwargs: Any,
):
    return call_with_hf_backoff(
        lambda: datasets.load_dataset(**dataset_kwargs),
        context=context,
        max_attempts=max_attempts,
        initial_delay=initial_delay,
        max_delay=max_delay,
        logger=logger,
    )
# ---
def do_wrap(self) -> bool:
        wrap_mode = self.options.get("mdformat", {}).get("wrap", DEFAULT_OPTS["wrap"])
        return isinstance(wrap_mode, int) or wrap_mode == "no"
# ---
def Source(candidate: str) -> pathlib.Path | str:
    """Data Source can either be a local file or a remote URL."""
    if "://" in candidate:
        return candidate
    return pathlib.Path(candidate)
# ---
def __init__(self, config: dict, env=None):
        creds_string, _ = get_credentials(env)
        self.gcp_wrapper = gcp.GcpWrapper(json.loads(creds_string))
        self.config = config
# ---
def test_random_dtype(spec, executor):
    a = cubed.random.random((10, 10), dtype=xp.float32, chunks=(4, 5), spec=spec)

    assert a.shape == (10, 10)
    assert a.chunks == ((4, 4, 2), (5, 5))
    assert a.dtype == xp.float32

    x = nxp.unique_values(a.compute(executor=executor))
    assert x.dtype == xp.float32
    assert len(x) > 90
# ---
def get_frequency_range(self):
		return self.frequency_range
# ---
def _ensure_encoding(possibly_bytes):
    return possibly_bytes.decode('utf-8') if type(possibly_bytes) is bytes else possibly_bytes
# ---
def __xor__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer or boolean", "__xor__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.bitwise_xor, self, other, dtype=result_type(self, other))
# ---
def test_wait_all_no_raise(client: LocalClient):
    h_ok = client.submit(JobRequest(name="ok", entrypoint=Entrypoint.from_callable(_noop)))
    h_fail = client.submit(JobRequest(name="fail", entrypoint=Entrypoint.from_callable(_fail)))
    statuses = wait_all([h_ok, h_fail], raise_on_failure=False)
    assert statuses[0] == JobStatus.SUCCEEDED
    assert statuses[1] == JobStatus.FAILED
# ---
def listify_list(list1):
  result = list(map(list,list1)) 
  return result
# ---
def __init__(self,name,arguments,as_written="",position=0):
        self.arguments = arguments
        self.number_of_arguments = len(arguments)
        self.name = name
        self.as_written = as_written
        self.arguments_list = arguments
        self.position = position
# ---
def get_orthogonal_matrix(GG: List[Union[Array, None]], epsilon: float) -> List[Union[Array, None]]:
    Q: List[Union[Array, None]] = []
    for gg in GG:
        if gg is None:
            Q.append(None)
        else:
            _, eigh = jnp.linalg.eigh(gg + epsilon * jnp.eye(gg.shape[0]))
            Q.append(jnp.flip(eigh, axis=1))
    return Q
# ---
def on_train_epoch_end(self):
        if self.confidence_prediction:
            self.log(
                "train/confidence_loss",
                self.train_confidence_loss_logger,
                prog_bar=False,
                on_step=False,
                on_epoch=True,
            )
            for k, v in self.train_confidence_loss_dict_logger.items():
                self.log(f"train/{k}", v, prog_bar=False, on_step=False, on_epoch=True)
# ---
def test_str_split(self):
        def test_impl(df):
            return df.A.str.split(',')

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(hpat_func(df), test_impl(df), check_names=False)
# ---
def to_t(arr: jnp.ndarray):
        return torch.from_numpy(np.array(arr))
# ---
def sort(self, items):
        return items
# ---
def setTitle(self, title):
        self.__title = title
# ---
def __init__(self, *args, **kwargs):
        super(UsuarioForm, self).__init__(*args, **kwargs)
        self.fields['primeiro_telefone'].widget.attrs['class'] = 'telefone'
        self.fields['segundo_telefone'].widget.attrs['class'] = 'telefone'
# ---
def _service_deps(self):
    service_deps = self.get_options().get('service_deps')
    return list(self.resolve_deps(service_deps)) if service_deps else self._deps
# ---
def drain_actor_pool(self) -> None:
        logger.info(f"{self.get_actor_pool_name()} actor pool members draining.")
        self._remove_members_from_actor_pool(0)
        logger.info(f"{self.get_actor_pool_name()} actor pool drained.")
# ---
def alibi_attention_bias(Heads: Axis, KPos: Axis, bias_max: float = 8, dtype=jnp.float32) -> NamedArray:
    """
    Creates an attention bias for alibi attention.

    :param KPos: Axis of (key) sequence length
    :param Heads: Axis of heads
    :return: NamedArray of shape (Heads, KPos)
    """
    slopes = haliax.named(np.array(_get_alibi_slopes(Heads.size, bias_max)), Heads)
    positions = haliax.arange(KPos).broadcast_axis(Heads)

    biases = slopes * positions
    return biases.astype(dtype)
# ---
def task_index(self) -> int:
        """0-indexed task number within the job."""
        return self._task_name.require_task()[1]
# ---
def _logs_page(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Iris Logs", "/static/worker/logs-page.js"))
# ---
def _hello_tpu_job():
    """Simple job that prints and returns."""
    print("Hello from TPU!")
    return 42
# ---
def test_literal(self):
        expr = lit(42)
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def register_endpoint(self, request: cluster__pb2.Controller.RegisterEndpointRequest, ctx: RequestContext) -> cluster__pb2.Controller.RegisterEndpointResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def resize_embeddings(self, new_size: int, key: Optional[PRNGKeyArray] = None):
        new_weights = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, token_embeddings=new_weights)
# ---
def extra_flags(extra: list[str] | None = None) -> list[str]:
    if not extra:
        extra = []

    accel_type = accelerator_type_from_extra(extra)
    if accel_type == AcceleratorType.NONE:
        extra.append("cpu")

    extra_set = set(extra)

    cmd = []
    for ex in extra_set:
        if ex.strip():
            cmd.append(f"--extra={ex}")
    return cmd
# ---
def test_conditional_escaping():
    """Tests *, _ with and without surrounding spaces."""
    test_cases = [
        ("word*bold*", r"word\*bold\*"),
        ("* this is not bold *", "* this is not bold *"),
        ("normal *bold*", "normal \\*bold\\*"),
        ("_italic_", "\\_italic\\_"),
        ("normal _italic_", "normal \\_italic\\_"),
    ]
    for text, expected in test_cases:
        assert minimal_markdown_escape(text) == expected
# ---
def union_elements(test_tup1, test_tup2):
  res = tuple(set(test_tup1 + test_tup2))
  return (res)
# ---
def remove_odd(str1):
 str2 = ''
 for i in range(1, len(str1) + 1):
    if(i % 2 == 0):
        str2 = str2 + str1[i - 1]
 return str2
# ---
def test_max(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.max(a, axis=0)
    run_operation(tmp_path, executor, "max", b)
# ---
def vm_groups(self) -> list[VmGroupProtocol]:
        """All VM groups in this scale group."""
        with self._vm_groups_lock:
            return list(self._vm_groups.values())
# ---
def __init__(self, objs: Iterable[T] = ()):
        self._index_to_obj: list[T] = []
        self._obj_to_index: dict[T, int] = {}
        for obj in objs:
            self.append(obj)
# ---
def test_instance_auto_disk_config_passes_fail_safes(self):
        """Should partition if instance is marked as auto_disk_config=True and
        virt-layer specific fail-safe checks pass.
        """
        self.instance_values['auto_disk_config'] = True

        def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4')]
        self.stubs.Set(vm_utils, "_get_partitions",
                       fake_get_partitions)

        self.assertIsPartitionCalled(True)
# ---
def _d3pm_parameterization(self, logits):
    if self.subs_masking:
      logits[:, :, self.mask_index] += self.neg_infinity
    logits = logits - torch.logsumexp(logits, dim=-1,
                                      keepdim=True)
    return logits
# ---
def addStringFromLineEdit(self):
        text = self.inputLine.text()
        if not text:
            return
        try:
            self.model.add(text)
        except AlreadyThereException:
            self.app.show_message("Expression already in the list.")
            return
        except Exception as e:
            self.app.show_message(f"Expression is invalid: {e}")
            return
        self.inputLine.clear()
# ---
def __init__(self, info: vm_pb2.VmInfo):
        # Don't call super().__init__ - just set the minimal attributes
        self.info = info
        self._log_lines: list[str] = []
# ---
def load_state_dict(self, state_dict):
    self.epoch = state_dict['epoch']
    self.counter = state_dict['counter']
    self.restarting = True
# ---
def test_cannot_scale_down_at_min_slices(self, scale_group_config: config_pb2.ScaleGroupConfig):
        """can_scale_down() returns False when at min_slices."""
        discovered = [make_mock_vm_group("slice-001")]
        manager = make_mock_vm_manager(vm_groups_to_discover=discovered)
        group = ScalingGroup(scale_group_config, manager)
        group.reconcile()

        assert group.slice_count() == 1  # min_slices
        assert not group.can_scale_down()
# ---
def get_gpg_fingerprint(output):
    """Return a fingerprint of the primary key.

    Ref:
    https://git.gnupg.org/cgi-bin/gitweb.cgi?p=gnupg.git;a=blob;f=doc/DETAILS;hb=HEAD#l482
    """
    for line in output.splitlines():
        data = line.split()
        if data[1] != 'VALIDSIG':
            continue

        # if signed with a subkey, this contains the primary key fingerprint
        data_id = 11 if len(data) == 11 else 2
        return data[data_id]
# ---
def result(self, timeout: float | None = None) -> T:
        """Block until result is available.

        Args:
            timeout: Maximum time to wait in seconds

        Returns:
            The return value of the submitted callable

        Raises:
            TimeoutError: If result not available within timeout
            Exception: Any exception raised by the callable
        """
        return self._future.result(timeout=timeout)
# ---
def count_Pairs(arr,n): 
    cnt = 0; 
    for i in range(n): 
        for j in range(i + 1,n): 
            if (arr[i] == arr[j]): 
                cnt += 1; 
    return cnt;
# ---
def divide(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.divide](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.divide.html)
    """
    return jnp.divide(x1, x2)
# ---
def test_url_generation(self):
        """Test URL generation for S3Location."""
        loc = S3Location(bucket="test-bucket", path="data/test.zarr")
        assert loc.url() == "s3://test-bucket/data/test.zarr"
# ---
def diff_consecutivenums(nums):
    result = [b-a for a, b in zip(nums[:-1], nums[1:])]
    return result
# ---
def teardown():
            """Delete the temporary files created in `setup()`."""
            shutil.rmtree(context.temp_dir)
# ---
def main():
    sol = Solution()
    print(sol.threeSumClosest([-111, -111, 3, 6, 7, 16, 17, 18, 19], 13))
    return 0
# ---
def get_actor_name_from_actor_info(self, actor_info) -> str:
        raise NotImplementedError()
# ---
def max(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.max, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def tpus_per_node(tpu_type: str) -> int:
    """Return the number of TPU chips per node for a given TPU type."""
    if tpu_type in {"v4-8", "v5p-8"}:
        return 4
    match = re.search(r"-(\d+)$", tpu_type)
    if not match:
        raise ValueError(f"Cannot parse TPU type: {tpu_type}")
    chips = int(match.group(1))
    if chips > 8:
        raise ValueError("Only single tpu nodes are supported with the CLI")
    return chips
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "hparams",
                "hparams": hparams,
            }
        )
        try:
            self.logger.info(json.dumps(record))
        except TypeError as e:
            logger.info(f"Oh noes... {e}")
# ---
def parse_paragraph(self, m):
        text = m.group(1).rstrip('\n')
        self.tokens.append({'type': 'paragraph', 'text': text})
# ---
def test_shutdown_prevents_new_submissions(self, local_client):
        """After shutdown, submit() raises RuntimeError."""
        config = WorkerPoolConfig(
            num_workers=1,
            resources=ResourceSpec(cpu=1, memory="512m"),
        )

        pool = WorkerPool(local_client, config, timeout=30.0)
        pool.__enter__()

        pool.shutdown(wait=False)

        with pytest.raises(RuntimeError, match="shutdown"):
            pool.submit(lambda: 42)
# ---
def from_single_ref(ref: Any, context: JobContext, idx: int, count: int) -> Shard:
        """Wrap a single ref as a Shard.

        Args:
            ref: Reference to wrap (type depends on context)
            context: Execution context for get operations
            idx: Shard index
            count: Number of items in the ref

        Returns:
            Shard containing the single ref
        """
        return Shard(idx=idx, chunks=[Chunk(count=count, data=ref)], context=context)
# ---
def sub_step(self, skip):
        self.command('sub_step', skip)
# ---
def get_moldir_atomic_numbers(moldir: str) -> list[int]:
    counter = set()
    mols = load_all_molecules(moldir)
    z_set = set()
    for mol in mols.values():
        z_set.update(a.GetAtomicNum() for a in mol.GetAtoms())
    z_set.discard(0)
    whitelist = sorted(z_set)
    return whitelist
# ---
def _poll_ref(self) -> JobStatus:
        ready, _ = ray.wait([self._ref], timeout=0)
        if not ready:
            return JobStatus.RUNNING
        try:
            ray.get(self._ref)
            return JobStatus.SUCCEEDED
        except Exception:
            return JobStatus.FAILED
# ---
def l2(*arg, **kw):
            canary.append("l2")
# ---
def _dataset_of_id(self, id):
        return self.datasets[self.dataset_index[id]]
# ---
def list_commands(self, _ctx: click.Context) -> list[str]:
        svc = get_service(self.service_name)
        if not svc:
            return []
        self.available_methods = svc.methods
        return [to_kebab_case(m) for m in sorted(svc.methods.keys())]
# ---
def register_worker(
    state: ControllerState,
    worker_id: str,
    address: str,
    metadata: cluster_pb2.WorkerMetadata,
) -> WorkerId:
    """Register a worker via event."""
    wid = WorkerId(worker_id)
    state.handle_event(
        WorkerRegisteredEvent(
            worker_id=wid,
            address=address,
            metadata=metadata,
            timestamp=Timestamp.now(),
        )
    )
    return wid
# ---
def _get_provider_uuid_by_host(self, host):
        # We have to temporarily mutate to 2.53 to get the hypervisor UUID.
        with utils.temporary_mutation(self.admin_api, microversion='2.53'):
            return super(ComputeStatusFilterTest211,
                         self)._get_provider_uuid_by_host(host)
# ---
def __next__(self):
        """Iterate through the future's result."""
        if self._iterator is None:
            self._iterator = iter(self.result())
        return next(self._iterator)
# ---
def tan(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in tan")
    return elemwise(nxp.tan, x, dtype=x.dtype)
# ---
def update_str(s):
      update_num(len(s))
      hasher.update(compat.as_bytes(s))
# ---
def logical_or(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "logical_or")
    if x1.dtype not in _boolean_dtypes or x2.dtype not in _boolean_dtypes:
        raise TypeError("Only boolean dtypes are allowed in logical_or")
    return elemwise(nxp.logical_or, x1, x2, dtype=nxp.bool)
# ---
def fold_via(
        self, fn: Callable[[M, CarryT], CarryT], *, unroll: int | bool | None = None
    ) -> Callable[[CarryT], CarryT]: ...
# ---
def get_spendable_output(self):
        self.log.debug("getting spendable output %s" % self.spendable_outputs[0].vtx[0])
        return self.spendable_outputs.pop(0).vtx[0]
# ---
def initialize(self, io_loop=None, executor=None):
        self.io_loop = io_loop or IOLoop.current()
        self.executor = executor or dummy_executor
# ---
def trim_tuple(test_list, K):
  res = []
  for ele in test_list:
    N = len(ele)
    res.append(tuple(list(ele)[K: N - K]))
  return (str(res))
# ---
def resolve_trainer_config(self, override: TrainerConfig | None = None) -> TrainerConfig:
        if override is not None:
            return override
        trainer_run = self.trainer_run_name or self.run_name
        return _default_trainer_for_run(trainer_run)
# ---
from typing import List


def rescale_to_unit(numbers: List[float]) -> List[float]:
    """ Given list of numbers (of at least two elements), apply a linear transform to that list,
    such that the smallest number will become 0 and the largest will become 1
    >>> rescale_to_unit([1.0, 2.0, 3.0, 4.0, 5.0])
    [0.0, 0.25, 0.5, 0.75, 1.0]
    """
    min_number = min(numbers)
    max_number = max(numbers)
    return [(x - min_number) / (max_number - min_number) for x in numbers]
# ---
def get_vocab(self) -> typing.Dict[str, int]:
    return self._vocab_str_to_int
# ---
def _get_clean_children(self, element):
        return [c for c in element.children if not (isinstance(c, NavigableString) and not c.strip())]
# ---
def get_batch_sync(self, indices_or_slice, *, timeout: Optional[float] = None):
        if isinstance(indices_or_slice, slice):
            indices_or_slice = range(
                indices_or_slice.start or 0,
                indices_or_slice.stop or len(self),
                indices_or_slice.step or 1,
            )
        return self.store.get_batch_sync(indices_or_slice)
# ---
def check_triplet(A, n, sum, count):
    if count == 3 and sum == 0:
        return True
    if count == 3 or n == 0 or sum < 0:
        return False
    return check_triplet(A, n - 1, sum - A[n - 1], count + 1) or\
           check_triplet(A, n - 1, sum, count)
# ---
def __hash__(self) -> int:
        return hash(self._ms)
# ---
def __call__(self, env, start_response):
        return env
# ---
def GetLogStreamAsString(region, stream_name, log_group):
  """Returns the messages of the log stream as a string."""
  log_lines = []
  token = None
  events = []
  while token is None or events:
    response = GetLogs(region, stream_name, log_group, token)
    events = response['events']
    token = response['nextForwardToken']
    for event in events:
      log_lines.append(event['message'])
  return '\n'.join(log_lines)
# ---
def test_scan_doesnt_scan_scalars():
    Height = Axis("Height", 10)
    named1 = hax.random.uniform(PRNGKey(0), (Height,))

    def scan_fun(acc, z, x):
        return (acc + z * x).scalar(), x * z

    total, selected = hax.scan(scan_fun, Height)(0.0, 4.0, named1)

    assert jnp.all(jnp.isclose(total, jnp.sum(named1.array * 4.0)))
    assert jnp.all(jnp.equal(selected.array, named1.array * 4.0))
# ---
def test_flip(tmp_path, spec, executor):
    # Note 'a' has one fewer element in axis=0 to force chunking to cross array boundaries
    a = cubed.random.random(
        (9999, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.flip(a, axis=0)
    run_operation(tmp_path, executor, "flip", b)
# ---
def resize_embeddings(self, new_size: int, key: Optional[PRNGKeyArray] = None):
        new_weights = self.token_embeddings.resize_embeddings(new_size, key=key)
        return dataclasses.replace(self, Vocab=self.Vocab.resize(new_size), token_embeddings=new_weights)
# ---
def getheaders(self, name):
        return list(self.values())
# ---
import re
def find_adverb_position(text):
 for m in re.finditer(r"\w+ly", text):
    return (m.start(), m.end(), m.group(0))
# ---
def geomspace(
    axis: AxisSelector, *, start: float, stop: float, endpoint: bool = True, dtype: DTypeLike | None = None
) -> NamedArray:
    """
    Version of jnp.geomspace that returns a NamedArray.
    If `axis` is a string, the default number of samples (50, per numpy) will be used.
    """
    if isinstance(axis, str):
        axis = Axis(axis, 50)
    return NamedArray(jnp.geomspace(start, stop, axis.size, endpoint=endpoint, dtype=dtype), (axis,))
# ---
def _convert_objectGUID(item):
    item = uuid.UUID("{{{0!s}}}".format(item)).bytes_le
    item = escape_bytes(item)
    return item
# ---
def shutdown(self) -> None:
        """Terminate the actor job."""
        client = self._get_client()
        client.terminate(self._job_id)
# ---
def test_qr_chunking():
    A = xp.ones((32, 4), chunks=(4, 2))
    with pytest.raises(
        ValueError,
        match=r"qr only supports tall-and-skinny \(single column chunk\) arrays.",
    ):
        xp.linalg.qr(A)
# ---
def _safe_sharding_constraint(x, sharding):
    if sharding is None:
        return x
    else:
        return with_sharding_constraint(x, sharding)
# ---
def reload_old(ng=False):
    if ng:
        bl_idnames = {n.bl_idname for n in ng.nodes if n.bl_idname in old_bl_idnames} 
        for bl_id in bl_idnames:
            mod = register_old(bl_id)
            if mod:
                importlib.reload(mod)
            else:
                print("Couldn't reload {}".format(bl_id))
    else:
        for ng in bpy.data.node_groups:
            reload_old(ng)
# ---
def pop(self):
        """Pop an element from the queue."""
        raise NotImplementedError
# ---
def test_hidden_linear_init_matches_linear_scaling(out_first: bool):
    In = hax.Axis("I", 6)
    Out = hax.Axis("O", 5)
    key = jrandom.PRNGKey(0)

    linear = Linear.init(In, Out, key=key, use_bias=False, out_first=out_first)
    hidden = Linear.init(
        In,
        Out,
        key=key,
        use_bias=False,
        out_first=out_first,
        reparam_cls=HiddenLinearMup,
    )

    assert jnp.allclose(hidden.weight.array, linear.weight.array)
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["Qwen3Config"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfQwen3Config,
        )
# ---
def add(self, n, values=None):
    self.update(self._seen_so_far + n, values)
# ---
def sum_even_odd(list1):
    first_even = next((el for el in list1 if el%2==0),-1)
    first_odd = next((el for el in list1 if el%2!=0),-1)
    return (first_even+first_odd)
# ---
def is_remote_tag(git_path, module, dest, remote, version):
    cmd = '%s ls-remote %s -t refs/tags/%s' % (git_path, remote, version)
    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)
    if to_native(version, errors='surrogate_or_strict') in out:
        return True
    else:
        return False
# ---
def test_arrow_marshaling(benchmark: Any, in_memory_table: pa.Table, batch_size: int | None) -> None:
    """
    Benchmarks Python Memory -> Rust -> Arrow Batches with different chunk sizes.
    batch_size=None simulates the "Giant" case (one massive batch).
    """

    def _pipeline() -> int:
        return sum(len(dupekit.process_arrow_batch(b)) for b in in_memory_table.to_batches(max_chunksize=batch_size))

    assert benchmark(_pipeline) > 0
# ---
def __len__(self):
                return len(self.l)
# ---


def greatest_common_divisor(a: int, b: int) -> int:
    """ Return a greatest common divisor of two integers a and b
    >>> greatest_common_divisor(3, 5)
    1
    >>> greatest_common_divisor(25, 15)
    5
    """
    while b:
        a, b = b, a % b
    return a
# ---
def _compileddirpats(self):
        pat, matchfunc = _buildregexmatch(
            [("glob", p, "") for p in self._globdirpats], "$"
        )
        return matchfunc
# ---
def test_spatial_dropout_2d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout2D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4, 5))

    testing_utils.layer_test(
        keras.layers.SpatialDropout2D,
        kwargs={'rate': 0.5, 'data_format': 'channels_first'},
        input_shape=(2, 3, 4, 5))
# ---
def sort_mixed_list(mixed_list):
    int_part = sorted([i for i in mixed_list if type(i) is int])
    str_part = sorted([i for i in mixed_list if type(i) is str])
    return int_part + str_part
# ---
def get_img_info(self, format):
        class img_info(object):
            def __init__(self, fmt):
                self.file_format = fmt

        return img_info(format)
# ---
def test_fillna_inplace(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            df.A.fillna(5.0, inplace=True)
            return df.A.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
# ---
def get_listbox_items(drinkers):
    items = []

    for drinker in drinkers:
        items.append(unicode('%s, %d drinks, %s' % (drinker.name, drinker.drinks, drinker.idle)))

    return items
# ---
def max_path_sum(tri, m, n): 
	for i in range(m-1, -1, -1): 
		for j in range(i+1): 
			if (tri[i+1][j] > tri[i+1][j+1]): 
				tri[i][j] += tri[i+1][j] 
			else: 
				tri[i][j] += tri[i+1][j+1] 
	return tri[0][0]
# ---
def onchange_partner_in(self, cr, uid, ids, partner_id=None, context=None):
        return {}
# ---
def In(self) -> AxisSpec:
        return self.layers[0].In
# ---
def start(self) -> None:
        """Start background thread for loading data."""
        if self._thread is not None:
            raise RuntimeError("ReplayDataLoader already running")

        self._stop_event.clear()
        self._thread = threading.Thread(target=self._worker_loop, daemon=True)
        self._thread.start()
        logger.info("Started ReplayDataLoader background thread")
# ---
def path(self):
        return self._parsed_url.path.decode("utf-8")
# ---
def wake(self) -> None:
        """Signal the controller loop to run immediately.

        Called when events occur that may make scheduling possible:
        - New job submitted
        - New worker registered
        - Task finished (freeing capacity)
        """
        self._wake_event.set()
# ---
def __repr__(self) -> str:
        return f"~({self.child})"
# ---
def _makeglobrecursive(pat):
    """Make a glob pattern recursive by appending "/**" to it"""
    if pat.endswith("/") or not pat:
        return pat + "**"
    else:
        return pat + "/**"
# ---
def starr_seq(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'episome',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transient transfection']
    }
# ---
def __init__(self, shape: T_Shape):
        dtype = nxp.int32
        chunks = (1,) * len(shape)
        super().__init__(shape, dtype, chunks)
# ---
def __init__(
            self,
            filepath,
            webhdfs_conn_id='webhdfs_default',
            *args, **kwargs):
        super(WebHdfsSensor, self).__init__(*args, **kwargs)
        self.filepath = filepath
        self.webhdfs_conn_id = webhdfs_conn_id
# ---
def find_remainder(arr, lens, n): 
    mul = 1
    for i in range(lens):  
        mul = (mul * (arr[i] % n)) % n 
    return mul % n
# ---
def has_len(self) -> bool:
        """
        Whether the data store currently has a known length. If this returns False, then the length of the data store
        may change in the future.
        """
        pass
# ---
def test_corrupt_program_returns_mutations_in_order(bank):
    source = CORPUS[0]
    rng = random.Random(42)

    corrupted, mutations = corrupt_program(source, num_steps=3, bank=bank, rng=rng)

    # Replay the mutations to verify they produce the same result.
    current = source
    for m in mutations:
        current = m.apply(current)
    assert current == corrupted
# ---
def test_actor_group_shutdown(client: LocalClient):
    group = client.create_actor_group(Counter, name="counters", count=2)
    handles = group.wait_ready()
    assert len(handles) == 2
    group.shutdown()
# ---
def _is_printer_printing(printer: OctoprintPrinterInfo) -> bool:
    return (
        printer
        and printer.state
        and printer.state.flags
        and printer.state.flags.printing
    )
# ---
def first(self) -> ResolvedEndpoint:
        """Get the first endpoint.

        Returns:
            The first resolved endpoint

        Raises:
            ValueError: If no endpoints are available
        """
        if not self.endpoints:
            raise ValueError(f"No endpoints for '{self.name}'")
        return self.endpoints[0]
# ---
def test_pass_different_length_seq(num_kv_heads):
    config = LlamaConfig(
        max_seq_len=64,
        hidden_dim=64,
        intermediate_dim=32,
        num_heads=2,
        num_kv_heads=num_kv_heads,
    )
    check_model_works_with_seqlen(LlamaLMHeadModel, config, 16)
# ---
def _escape(s: str) -> str:
    out = html.escape(s, quote=False)
    # we want newlines and other special characters to be visible
    # we also want strings of spaces to be visible
    out = out.replace("\n", "").replace("\t", "")
    out = re.sub(r"  +", lambda m: " " + "" * (len(m.group(0)) - 1), out)
    return out
# ---
def always(self):
        return True
# ---
def pytest_configure(config):
    config.addinivalue_line("markers", "cloud: mark test as needing cloud to run")
    config.addinivalue_line("markers", "slow: mark test as slow to run")
# ---
def list_available_configs() -> list[str]:
    """List all available cluster configurations."""
    infra_dir = Path("infra")
    if not infra_dir.exists():
        return []

    configs = []
    for yaml_file in infra_dir.glob("marin-*.yaml"):
        # Skip template file
        if yaml_file.name == "marin-cluster-template.yaml":
            continue
        configs.append(str(yaml_file))

    return sorted(configs)
# ---
def split_sizes(self):
        return self._split_sizes
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("data", String(50)),
        )
# ---
def test_reduce_with_pipeline(backend):
    """Test reduce integrated with other operations."""
    ds = Dataset.from_list(range(1, 21)).filter(lambda x: x % 2 == 0).map(lambda x: x * 2).reduce(sum)

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    expected = sum(x * 2 for x in range(1, 21) if x % 2 == 0)
    assert results[0] == expected
# ---
def from_proto(cls, proto: cluster_pb2.Worker.LogEntry) -> "LogEntry":
        return cls(
            timestamp=Timestamp.from_proto(proto.timestamp),
            source=proto.source,
            data=proto.data,
        )
# ---
def test_suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.makeSuite(TestFuncs))
    return suite
# ---
def reducer(*tokens):
    """Decorator for reduction methods.

    Arguments are a sequence of tokens, in order, which should trigger running
    this reduction method.
    """

    def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func

    return decorator
# ---
def test_make_blockwise_key_function_flip():
    func = lambda x: 0

    key_fn = make_blockwise_key_function(
        func, "z", "ij", "x", "ij", "y", "ji", numblocks={"x": (2, 2), "y": (2, 2)}
    )

    graph = make_blockwise_graph(
        func, "z", "ij", "x", "ij", "y", "ji", numblocks={"x": (2, 2), "y": (2, 2)}
    )
    check_consistent_with_graph(key_fn, graph)
# ---
def log_summary(self, metrics: typing.Mapping[str, Any]):
        import trackio

        to_log = {f"summary/{k}": _convert_value_to_loggable_rec(v) for k, v in metrics.items()}
        trackio.log(to_log)
# ---
def withdraw(self, amount):
        if not isinstance(amount, int):
            raise ValueError
        if self._balance < amount:
            raise WithdrawError(amount)
        self._balance -= amount
        return self._balance
# ---
def _on_worker_heartbeat(self, txn: TransactionLog, event: WorkerHeartbeatEvent) -> None:
        worker = self._workers[event.worker_id]
        worker.last_heartbeat = event.timestamp
        worker.healthy = True
        worker.consecutive_failures = 0
        txn.log("heartbeat", event.worker_id)
# ---
def _get_image_ids() -> set[str]:
    """Get all image IDs."""
    result = subprocess.run(["docker", "images", "-q"], capture_output=True, text=True, check=False)
    return set(result.stdout.strip().split()) if result.stdout.strip() else set()
# ---
def setUp(self):
        for _index_name, index_class in settings.ELASTICSEARCH_INDEXES['default'].items():
            doctype = locate(index_class)
            alias_migration.setup_index(doctype)
# ---
def compute(model, input):
        model_output = model(input, attn_mask=attn_mask)
        return model_output
# ---
def get_level(self, nick):
        return self.data[nick.lower()]['level']
# ---
def _validate_accelerator_types(config: config_pb2.IrisClusterConfig) -> None:
    """Validate that scale groups have explicit accelerator types."""
    for name, sg_config in config.scale_groups.items():
        if sg_config.accelerator_type == config_pb2.ACCELERATOR_TYPE_UNSPECIFIED:
            raise ValueError(f"Scale group '{name}' must set accelerator_type to cpu, gpu, or tpu.")
# ---
def set_power(self, power):
        if power > self._default_calibration_power + 10:
            raise ValueError("Power can be % dBm max, requested %d dBm" % (
                self._default_calibration_power + 10, power))

        self._power = power
        self._requested_cal = self.get_calibration(self._frequency,
                                                   self._power)
        self._lo.set_power(self._requested_cal.get_lo_power())
        self._output_SSB()
# ---
def test_rolling3(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            Ac = df.A.rolling(3, center=True).apply(lambda a: a[0] + 2 * a[1] + a[2])
            return Ac.sum()

        hpat_func = self.jit(test_impl)
        n = 121
        self.assertEqual(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def test_spatial_dropout_3d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout3D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4, 4, 5))

    testing_utils.layer_test(
        keras.layers.SpatialDropout3D,
        kwargs={'rate': 0.5, 'data_format': 'channels_first'},
        input_shape=(2, 3, 4, 4, 5))
# ---
def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n
# ---
import math
def round_up(a, digits):
    n = 10**-digits
    return round(math.ceil(a / n) * n, digits)
# ---
def get(self, ref: Any) -> Any:
        """Get result, unwrapping _ImmediateFuture if needed."""
        if isinstance(ref, _ImmediateFuture):
            return ref.result()
        return ref
# ---
def from_now(cls, duration: "Duration") -> "Deadline":
        """Create deadline from a Duration offset from now."""
        return cls(time.monotonic() + duration.to_seconds())
# ---
def __init__(self):
        self._start = time.monotonic()
# ---
def _expand_and_check_shape(expected_len: int, spec: T | Sequence[T], name: str) -> tuple[T, ...]:
    spec = ensure_tuple(spec)
    if len(spec) == 1:
        spec = spec * expected_len
    if len(spec) != expected_len:
        raise ValueError(f"Expected {expected_len} elements for {name}, got {len(spec)}")

    return spec
# ---
def __get__(self, record, owner):
        if record is None:
            return self         # the field is accessed through the class owner
        if not record:
            return False
        return record.ensure_one()._ids[0]
# ---
def interaction_choosers():
    return pd.DataFrame({
        'attr': ['a', 'b', 'c', 'b']},
        index=['w', 'x', 'y', 'z'])
# ---
def _submit_simple_job(client: IrisClient, tpu_type: str) -> cluster_pb2.JobStatus:
    def hello():
        print("Hello from validation job!")
        return 42

    job = client.submit(
        entrypoint=Entrypoint.from_callable(hello),
        name="validate-hello",
        resources=ResourceSpec(device=tpu_device(tpu_type)),
        environment=EnvironmentSpec(),
    )
    return job.wait(timeout=DEFAULT_VALIDATION_TIMEOUT, raise_on_failure=False)
# ---
def __init__(self, vocab, clusters):
        self.vocab = vocab
        self.clusters = clusters
# ---
def __init__(self, examples):
        self.examples = examples
# ---
def test_arrow_io_pipeline(benchmark: Any, small_parquet_path: str, batch_size: int) -> None:
    """
    Python End-to-End: Python reads file -> Stream of RecordBatches -> Rust (called per batch).
    Includes Parquet parsing overhead and Python loop overhead.
    """

    def _pipeline() -> int:
        batches = pq.ParquetFile(small_parquet_path).iter_batches(batch_size=batch_size)
        return sum(len(dupekit.process_arrow_batch(b)) for b in batches)

    assert benchmark(_pipeline) > 0
# ---
def cos(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in cos")
    return elemwise(nxp.cos, x, dtype=x.dtype)
# ---
def _num_train_steps(*, param_count: int, batch_size: int, max_seq_len: int, tpp: int = 20) -> int:
    total_tokens = param_count * tpp
    return max(1, total_tokens // (batch_size * max_seq_len))
# ---
def progressBarUpdateIntervalValueChanged(self, widget):
        """Signal handler for the "value_changed" signal for the
           progressBarUpdateIntervalSpinButton GtkSpinButton widget.

        Arguments:
        - widget: the component that generated the signal.
        """

        self.prefsDict["progressBarUpdateInterval"] = widget.get_value_as_int()
# ---
def test_log(monkeypatch):
    monkeypatch.setenv("HF_HUB_OFFLINE", "1")
    run = trackio.init(project="test-log")
    tracker = TrackioTracker(run)
    tracker.log({"float": 2.0}, step=0)
    tracker.log({"str": "test"}, step=0)
    tracker.log({"scalar_jax_array": jnp.array(3.0)}, step=0)
    tracker.log({"scalar_np_array": np.array(3.0)}, step=0)
    tracker.log({"histogram": Histogram.from_array(jnp.array([1.0, 2.0, 3.0]))}, step=0)
    trackio.finish()
# ---
def use_test_mesh(tensor_parallelism: int = 1, *, mesh: Optional[Mesh] = None, skip_ok: bool = False):
    """Context manager that activates a data/model Mesh and sets haliax's global mesh."""
    if mesh is None:
        mesh = create_test_mesh(tensor_parallelism, skip_ok=skip_ok)
    with set_mesh(mesh):
        yield mesh
# ---
def testMatMul_OutEmpty_B(self):
    n, k, m = 3, 8, 0
    x = self._randMatrix(n, k, np.float32)
    y = self._randMatrix(k, m, np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def test_capacity_math(self):
        """Capacity addition and equality"""
        cap = Capacity(2, 4)
        s = set([cap])
        self.assertIn(Capacity(2, 4), s)
        self.assertNotEqual(Capacity(1, 4), cap)
        self.assertEqual(Capacity(1, 1) + Capacity(2, 2), Capacity(3, 3))
# ---
def getStatus(self):
        return self.status
# ---
def config_dump(self):
        path = bottle.request.body.getvalue().decode('utf-8')
        self.config.dump(path)
# ---
def done(self, lease: Lease[T]) -> None:
        try:
            self.fs.rm(self.processing_dir / lease.lease_id)
        except FileNotFoundError:
            raise ValueError(f"Invalid lease: {lease.lease_id} not found (already done or expired)") from None
# ---
def on_load(self):
        if self.doc.xpath('//p[contains(text(), "incident technique")]'):
            raise BrowserIncorrectPassword("Vous n'avez aucun compte sur cet espace. " \
                                           "Veuillez choisir un autre type de compte.")
# ---
def values(self):
		"""
		Return a list of the file's values
		"""
		return [self[id] for id in self]
# ---
def get_eval_examples(self, n_examples: int) -> list[dict[str, Any]]:
        """Get evaluation examples."""
        return self.eval_examples[:n_examples]
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        if self._prefix and dir in self._fileset:
            return "all"
        if not self._prefix:
            return True
        return (
            dir in self._fileset
            or dir in self._dirs
            or any(parentdir in self._fileset for parentdir in util.finddirs(dir))
        )
# ---
def sum(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.sum(
            self,
            axis=axis,
            dtype=dtype,
            where=where,
        )
# ---
def test_writeorg(self, tmpfile):
        data1, data2 = tobytes("foo"), tobytes("bar")
        cap = capture.FDCapture(tmpfile.fileno())
        cap.start()
        tmpfile.write(data1)
        tmpfile.flush()
        cap.writeorg(data2)
        scap = cap.snap()
        cap.done()
        assert scap == totext(data1)
        with open(tmpfile.name, 'rb') as stmp_file:
            stmp = stmp_file.read()
            assert stmp == data2
# ---
def SetQuickFixList( quickfix_list ):
  """Populate the quickfix list and open it. List should be in qflist format:
  see ":h setqflist" for details."""
  vim.eval( 'setqflist( {0} )'.format( json.dumps( quickfix_list ) ) )
# ---
def is_primitive_op(node_dict):
    """Return True if a node is a primitive op"""
    return "primitive_op" in node_dict
# ---
def _compute_top2_gap_on_device(logit_fn, model, batch: B, Vocab) -> jnp.ndarray:
    with jax.named_scope("logits"):
        logits = logit_fn(model, batch)
    gaps = top2_gap_from_logits(logits, axis=Vocab)
    return gaps.flatten("token").array
# ---
def cd(self, name: str) -> "InputName":
        return InputName(self.step, name=os.path.join(self.name, name) if self.name else name)
# ---
def _find_repo_root(start_file: Path) -> Path | None:
    """Return the repository root that also contains the experiments directory if possible."""
    for parent in start_file.parents:
        if (parent / "pyproject.toml").exists() and (parent / "experiments").exists():
            return parent
    return None
# ---
def setupUi(self, Dialog):
        Dialog.setObjectName(_fromUtf8("Dialog"))
        Dialog.resize(1000, 400)

        self.gridLayout = QtGui.QGridLayout(Dialog)
        self.gridLayout.setObjectName(_fromUtf8("gridLayout"))

        # list of Events
        self.prepare_form(Dialog)

        self.retranslateUi(Dialog)
        QtCore.QMetaObject.connectSlotsByName(Dialog)
# ---
def is_set(self) -> bool:
        return os.path.exists(self._path)
# ---
def strong(node: RenderTreeNode, context: RenderContext) -> str:
    text = make_render_children(separator="")(node, context)
    indicator = node.markup
    return indicator + text + indicator
# ---
def _py_blake2b(text: bytes) -> bytes:
    return hashlib.blake2b(text).digest()
# ---
def get(ref: NamedRef, idx: SliceSpec | EllipsisType = Ellipsis) -> NamedArray:
    """Functional helper equivalent to `ref[idx]`."""
    return ref[idx]
# ---
def code_block(node: RenderTreeNode, context: RenderContext) -> str:
    return fence(node, context)
# ---
def _ensure_verifiers_installed(self):
        """Ensure verifiers package is installed."""
        try:
            import verifiers  # noqa: F401
        except ImportError as e:
            raise ImportError(
                "The 'verifiers' package is required to use PrimeIntellectEnv. "
                "Please install it with: uv pip install 'marin[rl]' or uv pip install verifiers"
            ) from e
# ---
def compute_success_ratio(stats: LessonStats, current_step: int, max_staleness: int = 1000) -> float:
    """Get success rate for a lesson."""
    return compute_smoothed_success(stats.training_stats.reward_history)
# ---
def validation_set(self) -> Optional[ProcessedAudioCache]:
        return self.build_or_load_cache(self.validation_split)
# ---
def _materialize_mask_slice(mask, i, j, QPos, KPos, block_size):
    return materialize_mask(
        mask,
        QPos,
        KPos,
        q_slice=hax.ds.block(i, block_size),
        k_slice=hax.ds.block(j, block_size),
    )
# ---
def method_not_allowed(error):
    return make_response(jsonify( { 'error': 'Method Not Allowed' } ), 405)
# ---
def __init__(self, system, capForce, particleGroup = None):
        if not (pmi._PMIComm and pmi._PMIComm.isActive()) or pmi._MPIcomm.rank in pmi._PMIComm.getMPIcpugroup():
            if (particleGroup == None) or (particleGroup.size() == 0):
                cxxinit(self, integrator_CapForce, system, capForce)
            else:
                cxxinit(self, integrator_CapForce, system, capForce, particleGroup)
# ---
def _radius(res_name: str, atom_name: str, element: str) -> float:
    """
    ProtOr radius with element fallback.
    """
    try:
        r = vdw_radius_protor(res_name, atom_name)
        if r is not None:
            return r
    except KeyError:
        pass
    r = vdw_radius_single(element)
    return r if r is not None else 1.8
# ---
def _job_status_to_result(name: str, status: cluster_pb2.JobStatus, duration: float) -> ValidationResult:
    if status.state == cluster_pb2.JOB_STATE_SUCCEEDED:
        return ValidationResult(name, True, f"Job completed in {duration:.1f}s", duration)
    state_name = cluster_pb2.JobState.Name(status.state)
    return ValidationResult(name, False, f"Job ended with state {state_name}: {status.error}", duration)
# ---
def __le__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.less_equal(self, other)
# ---
def _nanmean_combine(a, **kwargs):
    n = nxp.nansum(a["n"], **kwargs)
    total = nxp.nansum(a["total"], **kwargs)
    return {"n": n, "total": total}
# ---
def is_decimal(num):
    import re
    dnumre = re.compile(r"""^[0-9]+(\.[0-9]{1,2})?$""")
    result = dnumre.search(num)
    return bool(result)
# ---
def now(cls, source: str, data: str) -> "LogLine":
        return cls(timestamp=datetime.now(timezone.utc), source=source, data=data)
# ---
def test_default_dtypes():
    dtypes = info.default_dtypes()
    assert dtypes["real floating"] == xp.asarray(0.0).dtype
    assert dtypes["complex floating"] == xp.asarray(0.0j).dtype
    assert dtypes["integral"] == xp.asarray(0).dtype
    assert dtypes["indexing"] == xp.argmax(xp.zeros(10)).dtype
# ---
def date(self, val):
        self.opt_meta['date'] = _EpubDate(val)
# ---
def get_drinker_list():
    data = urllib2.urlopen("http://192.168.11.5:8080/drinkcounter/get_datas/").read().split("\n")
    drinkers = []

    for data_row in data:
        if data_row == '': continue

        fields = data_row.split('|')

        drinker = Drinker()
        drinker.id = int(fields[0])
        drinker.name = fields[1]
        drinker.drinks = int(fields[2])
        drinker.prom = float(fields[3])
        drinker.idle = fields[4]

        drinkers.append(drinker)

    return drinkers
# ---
def __repr__(self) -> str:
        return f"({self.left} {_ARITHMETIC_SYMBOLS[self.op]} {self.right})"
# ---
def __call__(
        self,
        tensor: torch.Tensor,
    ) -> torch.Tensor: ...
# ---
def test_apply_blockwise_multiple_inputs():
    bw_spec = make_blockwise_spec(
        key_function=make_map_blocks_key_function("a", "b"), function=add
    )

    input_data = {"a": [0, 1, 2, 3, 4], "b": [5, 6, 7, 8, 9]}
    out = [apply_blockwise(input_data, [i], bw_spec) for i in range(5)]
    assert out == [5, 7, 9, 11, 13]
# ---
def build(self, data_root: ResolvedLocation) -> VizRun:
        return VizRun(
            name=self.name,
            data=data_root.resolve(self.location).open(chunks={}),
            variables=self.variables,
        )
# ---
def terminate(self) -> None:
        self._job.terminate()
# ---
def filter_data(students,h,w):
    result = {k: s for k, s in students.items() if s[0] >=h and s[1] >=w}
    return result
# ---
def to_proto(self) -> cluster_pb2.Entrypoint:
        """Convert to protobuf representation."""
        proto = cluster_pb2.Entrypoint()
        if self._callable_bytes is not None:
            proto.callable = self._callable_bytes
        elif self.command is not None:
            proto.command.argv[:] = self.command
        return proto
# ---
def _deps(self):
    thrift_import_target = self.get_options().thrift_import_target
    thrift_imports = self.context.resolve(thrift_import_target)
    return thrift_imports
# ---
def _make_demo_config() -> config_pb2.IrisClusterConfig:
    config = config_pb2.IrisClusterConfig()
    cpu_sg = config.scale_groups["cpu"]
    cpu_sg.name = "cpu"
    cpu_sg.accelerator_type = config_pb2.ACCELERATOR_TYPE_CPU
    cpu_sg.min_slices = 0
    cpu_sg.max_slices = 1
    return IrisConfig(config).as_local().proto
# ---
def on_task_end(self, event):
        result = event.result
        if result is None:
            return
        else:
            self.sessions.append(merge_sessions(*[store.session for store in result]))
# ---
def unique_id(self):
        """Return the unique id of this switch."""
        return self._unique_id
# ---
def test_vocab_size(tok):
    expected = 3 + tok.max_seq_len + tok.base_vocab_size
    assert tok.vocab_size == expected
# ---
def __enter__(self):
        global _global_tracker
        self.old_tracker = _global_tracker
        _global_tracker = self.tracker

        return self.tracker
# ---
def set_rebus_controller(self, rebus_controller):
        self.rebus_controller = rebus_controller
# ---
def map_jpg():
    return make_map(request, format='jpg')
# ---
def abs(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in abs")
    if x.dtype == complex64:
        dtype = float32
    elif x.dtype == complex128:
        dtype = float64
    else:
        dtype = x.dtype
    return elemwise(nxp.abs, x, dtype=dtype)
# ---
def resolve_path(path: str) -> str:
    """Resolve a path to an absolute path, except for cloud storage paths."""
    return path if path.startswith(CLOUD_STORAGE_PREFIXES) else os.path.realpath(path)
# ---
def _configure_logging():
    """Configure logging to show all iris module output."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        stream=sys.stdout,
        force=True,
    )
    # Ensure iris modules are visible
    logging.getLogger("iris").setLevel(logging.INFO)
# ---
def as_dict(self):
        return {'reason': self.value}
# ---
def yesno(question, default="y"):
    "Ask the user a yes/no question"
    while True:
        sys.stdout.write("{} (y/n) [{}]: ".format(question, default))
        answer = sys.stdin.readline().strip().lower()
        if len(answer) == 0:
            answer = default
        if answer == "y":
            return True
        elif answer == "n":
            return False
# ---
def _create_instance_ref(self):
        return db.instance_create(self.context,
                                  {'user_id': self.user_id,
                                   'project_id': self.project_id,
                                   'instance_type_id': 1})
# ---
def getLoadedPlugins(): pass
# ---
def setServer( self, url ):
    self.serverURL = url
# ---
def is_Power_Of_Two (x): 
    return x and (not(x & (x - 1))) 
def differ_At_One_Bit_Pos(a,b): 
    return is_Power_Of_Two(a ^ b)
# ---
def wait(self):
        """ Waits for the deployment to complete: first, the network that will contain the cluster is deployed. Once
        the network is deployed, a firewall for the network and an instance template are deployed. Finally,
        once the instance template is deployed, an instance group manager and all its instances are deployed.
        """
        self.deployment.wait_for_completion()
# ---
def triu(array: NamedArray, axis1: Axis, axis2: Axis, k=0) -> NamedArray:
    """Compute the upper triangular part of an array along two named axes."""
    array = array.rearrange((..., axis1, axis2))

    inner = jnp.triu(array.array, k=k)
    return NamedArray(inner, array.axes)
# ---
def calc_time_indices(num_time_samples):
    indices = np.linspace(0, 10311, num_time_samples+2, dtype=int)[1:-1]
    dif = indices[1]-indices[0]
    for i, val in enumerate(indices):
        offset = np.random.randint(int(-dif/2.5), int(dif/2.5)) # divide by 2.5 keeps indices more centered
        indices[i] = val + offset
    return indices
# ---
def test_index_zero_dim(shape, chunks, ind):
    a = xp.ones(shape, chunks=chunks)
    b = a[ind]
    assert_array_equal(b.compute(), np.ones(shape)[ind])
# ---
def _update_file(path: Path, *, dry_run: bool) -> bool:
    with path.open() as f:
        data = json.load(f)

    changed = False
    for entry in data.get("runs", []):
        run_info = entry.get("run_info")
        if isinstance(run_info, dict) and _correct_run_info(run_info):
            changed = True

    if changed and not dry_run:
        with path.open("w") as f:
            json.dump(data, f, indent=2, sort_keys=True)

    return changed
# ---
def is_on(self):
        """Return true if device is on."""
        return self._state
# ---
def reparam(self) -> AbstractEmbeddingReparam:
        return self._reparam_cls(self.Embed, self.Vocab)
# ---
def get_github_issues():
    """Get all issues with the `experiment` label."""
    g = Github()
    repo = g.get_repo("marin-community/marin")
    return repo.get_issues(labels=["experiment"], state="all")
# ---
def get_value_from_remote(obj):  # Gather
    obj = compss_wait_on(obj)
    return obj
# ---
def hardcoded(path: str) -> "InputName":
        """
        Sometimes we want to specify a path that is not part of the pipeline but is still relative to the prefix.
        Try to use this sparingly.
        """
        return InputName(None, name=path)
# ---
def timerules():
    """Adds, Updates or deletes time rule for the given element"""
    rules = request.get_json()

    if len(rules) == 0:
        raise Exception("No elements in the list")

    for rule in rules:
        if 'id' not in rule:
            rule['id'] = None

    home_services.save_time_rules(rules)
    return 'OK'
# ---
def __init__(self, log_tree: LogTree):
        self._start_time = time.monotonic()
        self._start_datetime = datetime.now()
        self._log_tree = log_tree
        summary_path = log_tree.get_writer("summary.md", "Execution summary")
        self._file: TextIO = open(summary_path, "w")
# ---
def custom_model_info(repo_id, *args, **kwargs) -> ModelInfo:
            if _is_url_like(repo_id):
                # `tags=None` makes is_base_mistral return False, skipping the problematic code path
                return ModelInfo(id="monkeypatched", tags=None)
            return original_model_info(repo_id, *args, **kwargs)
# ---
def read_cfg(path_to_config_file, profile_name):
    cfg = read(path_to_config_file, loader=yaml.full_load)
    if profile_name is not None:
        cfg["profile"] = profile_name
    elif "AWS_PROFILE" in os.environ:
        cfg["profile"] = os.environ["AWS_PROFILE"]
    return cfg
# ---
import re
def pass_validity(p):
 x = True
 while x:  
    if (len(p)<6 or len(p)>12):
        break
    elif not re.search("[a-z]",p):
        break
    elif not re.search("[0-9]",p):
        break
    elif not re.search("[A-Z]",p):
        break
    elif not re.search("[$#@]",p):
        break
    elif re.search("\s",p):
        break
    else:
        return True
        x=False
        break

 if x:
    return False
# ---
def exists(v):
    return v is not None
# ---
def __init__(
        self, resolution: float = 9.0, minimum_resolution: float = 0.0
    ) -> None:
        """Initialize the filter.

        Parameters
        ----------
        resolution : float, optional
            The maximum allowed resolution.

        """
        self.resolution = resolution
        self.minimum_resolution = minimum_resolution
# ---
def _check_deadline(self) -> bool:
        """Returns True if the global deadline has passed. Sets interrupted flag."""
        if self._deadline is not None and time.monotonic() > self._deadline:
            self.logger.log(
                f"Global timeout ({self.config.timeout_seconds}s) exceeded!",
                level="ERROR",
            )
            self._interrupted = True
            return True
        return False
# ---
def eliminate_axes(axis_spec: PartialShapeDict, axes: AxisSelection) -> PartialShapeDict:  # type: ignore
    ...
# ---
def num_cpus(self) -> int:
        return num_cpus_used_by_tokenizer(self.tokenizer)
# ---
def test_accelerator_types_are_distinguishable():
    """Different accelerator types produce different display names."""
    types = [
        config_pb2.ACCELERATOR_TYPE_UNSPECIFIED,
        config_pb2.ACCELERATOR_TYPE_CPU,
        config_pb2.ACCELERATOR_TYPE_GPU,
        config_pb2.ACCELERATOR_TYPE_TPU,
    ]
    friendly_names = [accelerator_type_friendly(t) for t in types]
    assert len(friendly_names) == len(set(friendly_names)), "All accelerator types must have unique display names"
# ---
def log_summary(self, metrics: dict[str, Any]):
        self.metrics["summary"] = metrics
# ---
def settings_customise_sources(
        cls,
        settings_cls: type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> tuple[PydanticBaseSettingsSource, ...]:
        # We don't need env/dotenv/secrets, yaml & CLI are injected in from_yaml_and_cli
        return (init_settings,)
# ---
def __lt__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.less(self, other)
# ---
def indent(self, text, level):
        return markdownify.line_beginning_re.sub("    " * level, text) if text else ""
# ---
def _refresh_swap_list_items(self):
        if not self.panes:
            return
        items = []
        basefmt = self.selected_pane.parsing_date_format
        for first, second in [(DAY, MONTH), (MONTH, YEAR), (DAY, YEAR)]:
            swapped = swap_format_elements(basefmt, first, second)
            items.append("{} --> {}".format(basefmt.iso_format, swapped.iso_format))
        self.swap_type_list[:3] = items
# ---
def __init__(self):
        self.file = None
        self.format = 1
        self.tracks = []
        self.ticks_per_quarter_note = 1024
        self.ticks_per_second = None
# ---
def accumulated(cls, stats: list["LoadStats"]) -> "LoadStats":
        """Accumulate the stats across multiple LoadStats objects in a batch."""
        return cls(sum(s.load_time_seconds for s in stats))
# ---
def test_neutron_resolved(self):
        self.compare_stacks('Neutron.template', 'Neutron.yaml', {})
# ---
def test_plain_ndarray_selector():
    B, V = Axis("batch", 3), Axis("vocab", 5)
    x = hax.arange((B, V))
    idx = jnp.array([0, 2, 4], dtype=jnp.int32)
    out = x["vocab", idx]
    assert out.axes == (B,)
    assert jnp.array_equal(out.array, x.array[jnp.arange(3), idx])
# ---
def pick_add_edge(g):
        u = nx.utils.arbitrary_element(g)
        possible_nodes = set(g.nodes())
        neighbors = list(g.neighbors(u)) + [u]
        possible_nodes.difference_update(neighbors)
        v = nx.utils.arbitrary_element(possible_nodes)
        return (u, v)
# ---
def build(self, axis: AxisSpec) -> "GemmaRMSNorm":
        return GemmaRMSNorm.init(axis, eps=self.eps, use_weight=self.use_weight, use_bias=self.use_bias)
# ---
def extract_braced_content(text: str, start: int = 0) -> str | None:
    """Extract content from {content} starting at position start."""
    if start >= len(text) or text[start] != "{":
        return None
    end = find_matching_brace(text, start)
    return text[start + 1 : end] if end else None
# ---
def get_anomalies_vars(var_names: BoundaryVarNames) -> tuple[str, ...]:
    """Get the variables that need to be computed for anomalies."""
    return tuple([var for var in var_names if var.endswith("_anomalies")])
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.int32)})
            return df.A.quantile(.25)
# ---
def make_resolution(x: torch.Tensor) -> tuple[Lat, Lon]:
    lat = torch.linspace(start=-90, end=90, steps=x.shape[-2])
    lon = torch.linspace(start=0, end=360, steps=x.shape[-1])
    return lat, lon
# ---
def __repr__(self):
        return ("{0.__class__.__name__}({0.field!r}, {0.pattern!r}, "
                "{0.fast})".format(self))
# ---
def test_get_secret_key(self):
        first_key = csrf._get_secret_key()
        self.assertEqual(len(first_key), 32)
        second_key = csrf._get_secret_key()
        self.assertEqual(first_key, second_key)
# ---
def writeManagerFileThread(managerFilename, qin):
	m = open(managerFilename + ".csv", "w")
	m.write("Source,Target\n")
	while True:
		data = qin.get()
		if data == None:
			break
		m.write(str(data) + "\n")
		qin.task_done()
# ---
def fetch_task_logs(
        self,
        task_id: JobName,
        start: "Timestamp | None" = None,
        max_lines: int = 0,
    ) -> list[cluster_pb2.Worker.LogEntry]:
        return self._remote_client.fetch_task_logs(task_id, start, max_lines)
# ---
def release(self, lease: Lease[T_co]) -> None:
        if lease.lease_id in self.leases:
            item, _, _ = self.leases[lease.lease_id]
            self.queue.insert(0, item)
            del self.leases[lease.lease_id]
# ---
def resolve(self, location: "Location") -> "ResolvedLocation":
        if isinstance(location, UnresolvedLocation):
            return LocalLocation(path=self.path / location.path)
        return location
# ---
def rear_extract(test_list):
  res = [lis[-1] for lis in test_list]
  return (res)
# ---
def opener_has_handler(self, opener, handler_class):
        self.assertTrue(any(h.__class__ == handler_class
                            for h in opener.handlers))
# ---
def test_failure(tmp_path, timing_map, n_tasks, retries, use_backups):
    with pytest.raises(RuntimeError):
        asyncio.run(
            run_test(
                function=partial(deterministic_failure, tmp_path, timing_map),
                input=range(n_tasks),
                retries=retries,
                use_backups=use_backups,
            )
        )

    check_invocation_counts(tmp_path, timing_map, n_tasks, retries)
# ---
def template_input(input):
    input = os.path.abspath(input)
    # input, input_cmd
    return "input\t{}".format(input), ["--input", input]
# ---
def _select_gen(stream: Iterator, columns: tuple[str, ...]) -> Iterator:
    cols_set = set(columns)
    for item in stream:
        yield {k: item[k] for k in cols_set if k in item}
# ---
def subscribe(ch, method, properties, body):
        """
        prints the body message. It's the default callback method
        :param ch: keep null
        :param method: keep null
        :param properties: keep null
        :param body: the message
        :return:
        """
# ---
def bitwise_xor(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "bitwise_xor")
    if (
        x1.dtype not in _integer_or_boolean_dtypes
        or x2.dtype not in _integer_or_boolean_dtypes
    ):
        raise TypeError("Only integer or boolean dtypes are allowed in bitwise_xor")
    return elemwise(nxp.bitwise_xor, x1, x2, dtype=result_type(x1, x2))
# ---
def __eq__(self, other: object) -> bool:
        if not isinstance(other, Duration):
            return NotImplemented
        return self._ms == other._ms
# ---
def test_hash_verification_success(temp_cache_dir, test_bundle, test_bundle_hash):
    """Test that hash verification passes with correct hash."""
    cache = BundleCache(temp_cache_dir)

    file_url = f"file://{test_bundle}"

    # Get bundle with correct hash - should succeed without raising
    extract_path = cache.get_bundle(file_url, expected_hash=test_bundle_hash)

    # Verify path is valid by checking we got something back
    assert extract_path is not None
# ---
def _apply_logit_soft_cap(logits: jax.Array, logit_soft_cap: Optional[float]) -> jax.Array:
    if logit_soft_cap is None:
        return logits
    return jnp.tanh(logits / logit_soft_cap) * logit_soft_cap
# ---
def find(n,m):
  r = n%m
  return (r)
# ---
def list_tasks(
        self,
        request: cluster_pb2.Worker.ListTasksRequest,
    ) -> cluster_pb2.Worker.ListTasksResponse: ...
# ---
def __init__(self, dim, dim_single_cond):
        super().__init__()
        self.a_norm = LayerNorm(dim, elementwise_affine=False, bias=False)
        self.s_norm = LayerNorm(dim_single_cond, bias=False)
        self.s_scale = Linear(dim_single_cond, dim)
        self.s_bias = LinearNoBias(dim_single_cond, dim)
# ---
def counting_fn(x):
        nonlocal call_count
        call_count += 1
        return x * 2
# ---
def base_field(self):
        """ Return the base field of an inherited field, or ``self``. """
        return self.related_field.base_field if self.inherited else self
# ---
def loss_fn(model, data):
            m = jax.vmap(model)
            return jnp.mean(jnp.square(m(data)))
# ---
def logical_not(a: A) -> A:
    return wrap_elemwise_unary(jnp.logical_not, a)
# ---
def _make_dummy_batch(EvalBatch, EvalPos):
    dummy_batch = hax.vmap(LmExample.causal, EvalBatch)(
        hax.zeros(EvalPos, dtype=jnp.int32),
        loss_weight=hax.zeros(EvalPos, dtype=jnp.float32),
        segment_ids=hax.zeros(EvalPos, dtype=jnp.int32),
    )
    out = hax.shard(dummy_batch, {})
    return out
# ---
def __init__(self, endpoint_name: str):
        self._endpoint_name = endpoint_name
        self._client: Any = None
# ---
def make_coiled_function(func, coiled_kwargs):
    return coiled.function(**coiled_kwargs)(execution_stats(func))
# ---
def test_count_basic(backend):
    """Test basic count operation."""
    ds = Dataset.from_list(range(100)).count()
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == 100
# ---
def tearDown(self):
        import shutil
        shutil.rmtree(self.repo_path)
# ---
def mk_treeview(frame, side='top', sbars='y'):
    BORDER = 0
    COLOR = 'grey'

    treeview_frame = tkinter.Frame(frame, bg=COLOR, bd=BORDER)
    treeview_frame.pack(side=side, fill='both', expand=True)

    treeview = tkinter.ttk.Treeview(treeview_frame)
    mk_scrollable_area(treeview, treeview_frame, sbars)

    return treeview
# ---
def material(self):
        return self.__material
# ---
def on_remove_duplicates_state_changed(self):
        self.fuzz_table_model.remove_duplicates = self.ui.chkBRemoveDuplicates.isChecked()
        self.fuzz_table_model.update()
        self.remove_duplicates()
# ---
def _print_deleted(matches, after_match):
            deleted = []
            for keydir in (self.key.ACC, self.key.PEND, self.key.REJ):
                deleted.extend(list(
                    set(matches.get(keydir, [])).difference(
                        set(after_match.get(keydir, []))
                    )
                ))
            for key in sorted(deleted):
                print('Key for minion {0} deleted.'.format(key))
# ---
def w_read():
        w_read_future.start()
# ---
def secrets(self):
        '''secret property getter'''
        if self._secrets is None:
            self._secrets = self.get_secrets()
        return self._secrets
# ---
def minimal_bootstrap_config() -> config_pb2.BootstrapConfig:
    """Minimal valid bootstrap config."""
    return config_pb2.BootstrapConfig(
        docker_image="gcr.io/test/iris-worker:latest",
        worker_port=10001,
        cache_dir="/var/cache/iris",
    )
# ---
def __getattr__(self, method_name: str) -> "ThreadActorMethod":
        method = getattr(self._instance, method_name)
        return ThreadActorMethod(method, self._lock, self._context)
# ---
def make_batches(size, batch_size):
  """Returns a list of batch indices (tuples of indices).

  Arguments:
      size: Integer, total size of the data to slice into batches.
      batch_size: Integer, batch size.

  Returns:
      A list of tuples of array indices.
  """
  num_batches = int(np.ceil(size / float(batch_size)))
  return [(i * batch_size, min(size, (i + 1) * batch_size))
          for i in range(0, num_batches)]
# ---
def error(self, proto, *args):
        self.proto, self.args = proto, args
# ---
def _make_grug_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    # We only require a mesh context here so the loss can provide `out_sharding=...`.
    mesh_devices = np.array(devices).reshape(len(devices), 1)
    return Mesh(
        mesh_devices,
        axis_names=("data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit),
    )
# ---
from operator import itemgetter 
def index_minimum(test_list):
  res = min(test_list, key = itemgetter(1))[0]
  return (res)
# ---
def getapiname(self):
		return 'taobao.subusers.get'
# ---
def serve():
    if os.environ.get("DEV") == "true":
        return proxy_to_dev_server("")
    return send_from_directory(app.static_folder, "index.html")
# ---
def test_makes_patches():
    x = torch.randn(3, 10, 4, 8)

    patch_embed = PerceiverEncoder(
        in_channels=10,
        out_channels=4,
        patch_size=4,
        perceiver=make_perceiver(10, 4),
        lat=torch.linspace(start=-90, end=90, steps=x.shape[-2]),
        lon=torch.linspace(start=0, end=360, steps=x.shape[-1]),
    )

    patches = patch_embed(x)

    assert patches.shape == (3, 4, 1, 2)
# ---
def get_local_ip_from_hostname():
    hostname = socket.gethostname()
    ip_address = socket.gethostbyname(hostname)
    return ip_address
# ---
def test_task_timeout(cluster, sentinel):
    """Task times out, marked FAILED."""
    _url, client = cluster
    job = submit(client, _block, "timeout-test", sentinel, timeout=Duration.from_seconds(5))
    status = wait(client, job, timeout=30)
    assert status.state == cluster_pb2.JOB_STATE_FAILED
# ---
def __init__(
        self,
        problem: str,
        answer: str,
        grader: Literal["sympy", "math_verify"] = "sympy",
        timeout: float = 1.0,
    ):
        self.problem = problem
        self.answer = answer
        self.grader = grader
        self.timeout = timeout
# ---
def close(self, request, *args, **kwargs):
        instance = self.get_object()
        serializer = self.get_serializer(instance)
        instance.close(processor=request.user)
        return Response(serializer.data)
# ---
def test_roll(spec, executor, chunks, shift, axis):
    x = np.arange(4 * 6).reshape((4, 6))
    a = cubed.from_array(x, chunks=chunks, spec=spec)

    if _maybe_len(shift) != _maybe_len(axis):
        with pytest.raises(TypeError if axis is None else ValueError):
            xp.roll(a, shift, axis=axis)
    else:
        assert_array_equal(
            xp.roll(a, shift, axis=axis).compute(executor=executor),
            np.roll(x, shift, axis),
        )
# ---
def __init__(self, **kwargs):
        super(ApplicationGatewaySslPredefinedPolicy, self).__init__(**kwargs)
        self.name = kwargs.get('name', None)
        self.cipher_suites = kwargs.get('cipher_suites', None)
        self.min_protocol_version = kwargs.get('min_protocol_version', None)
# ---
def test_impl(df):
            A = df.A.str.split(',')
            return pd.Series(list(itertools.chain(*A)))
# ---
def _is_simple_field(field: FieldDescriptor) -> bool:
    """Check if a protobuf field is a simple scalar type that maps to Click."""
    if field.label == FieldDescriptor.LABEL_REPEATED:
        return False
    if field.message_type is not None:
        return False
    return field.type in PROTO_TYPE_TO_CLICK
# ---
def resolve_axis(self, axis: tuple[AxisSelector, ...]) -> tuple[Axis, ...]: ...
# ---
def test_from_files_empty_glob_ok(tmp_path):
    """Test from_files with empty_glob_ok=True."""
    input_dir = tmp_path / "input"
    input_dir.mkdir()

    # No error when empty_glob_ok=True
    ds = Dataset.from_files(f"{input_dir}/*.txt", empty_glob_ok=True)
    files = list(ds.source)
    assert len(files) == 0
# ---
def coerce_boundary(ndim, boundary):
    default = "none"
    if boundary is None:
        boundary = default
    if not isinstance(boundary, (tuple, dict)):
        boundary = (boundary,) * ndim
    if isinstance(boundary, tuple):
        boundary = dict(zip(range(ndim), boundary))
    if isinstance(boundary, dict):
        boundary = {ax: boundary.get(ax, default) for ax in range(ndim)}
    return boundary
# ---
def materialize_mask(
    mask: NamedArray | AttentionMask,
    QPos: Axis,
    KPos: Axis,
    q_slice: Optional[haliax.dslice] = None,
    k_slice: Optional[haliax.dslice] = None,
) -> NamedArray: ...
# ---
def wait_for_playback(self):
        """ Waits until playback of the current title is paused or done """
        with self._playback_cond:
            self._playback_cond.wait()
# ---
def test_conf_set_no_read(self):
        with mock.patch.object(memcache, 'ConfigParser', ExcConfigParser):
            exc = None
            try:
                memcache.MemcacheMiddleware(
                    FakeApp(), {'memcache_servers': '1.2.3.4:5',
                                'memcache_serialization_support': '2',
                                'memcache_max_connections': '30'})
            except Exception as err:
                exc = err
        self.assertIsNone(exc)
# ---
def on_key_release(symbol, modifiers):
            """A key on the keyboard was released.

            :Parameters:
                `symbol` : int
                    The key symbol pressed.
                `modifiers` : int
                    Bitwise combination of the key modifiers active.

            :event:
            """
# ---

def iscube(a):
    '''
    Write a function that takes an integer a and returns True 
    if this ingeger is a cube of some integer number.
    Note: you may assume the input is always valid.
    Examples:
    iscube(1) ==> True
    iscube(2) ==> False
    iscube(-1) ==> True
    iscube(64) ==> True
    iscube(0) ==> True
    iscube(180) ==> False
    '''
    a = abs(a)
    return int(round(a ** (1. / 3))) ** 3 == a
# ---
def clear_cmd():
    """Clear the screen."""
    import os

    os.system("clear" if os.name == "posix" else "cls")
# ---
def create_tx(self, spend_tx, n, value, script=CScript([OP_TRUE, OP_DROP] * 15 + [OP_TRUE])):
        return create_tx_with_script(spend_tx, n, amount=value, script_pub_key=script)
# ---
def weighted_cross_entropy(logits, y, loss_weights, ignore_index=-100):
    """Weighted cross entropy loss (discounts certain tokens)."""
    logits = logits.view(-1, logits.shape[-1])
    y = y.view(-1)
    ce = F.cross_entropy(logits, y, ignore_index=ignore_index, reduction='none')
    loss_weights = loss_weights.view(-1)
    loss_weights[y == ignore_index] = 0.0
    # TODO: Follows GPN implementation, but should we remove weight normalization?
    return (ce * (loss_weights / loss_weights.sum())).sum()
# ---
def surfacearea_cylinder(r,h):
  surfacearea=((2*3.1415*r*r) +(2*3.1415*r*h))
  return surfacearea
# ---
def wait_until_or_raise(
        self,
        condition: Callable[[], bool],
        timeout: Duration,
        error_message: str,
    ) -> None:
        if not self.wait_until(condition, timeout):
            raise TimeoutError(error_message)
# ---
def __str__(self):
        return str(self.__shortname)
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step: Optional[int], commit: Optional[bool] = None):
        if step is not None:
            self.metrics[f"step_{step}"] = metrics
        else:
            self.metrics.update(metrics)
# ---
def config(self) -> HackableTransformerConfig:
        return self.transformer.config
# ---
def test_column_select_pushdown(self, sync_backend, vortex_file):
        """Test column selection pushdown."""
        ds = Dataset.from_files(str(vortex_file)).load_vortex().select("id", "score")

        results = list(Backend.execute(ds, context=sync_backend))
        assert len(results) == 100
        assert set(results[0].keys()) == {"id", "score"}
# ---
def __init__(self, job_id, command):
        self._id = job_id
        self._command = command
        self._thread = None

        self._status = STATUS.STARTING
        self._description = ''
        self._disk_progress = 0
        self._disk_count = 1
        self._current_disk = 1
        self._aborted = False
        self._proc = None
# ---
def _map_fn(lax_map, bs, n_maps, fn, *args):
    """Maybe map a fn along multiple leading axes."""
    if n_maps <= 0:
        return fn(*args)

    if lax_map:
        mapped_fn = lambda xs: _map_fn(lax_map, bs, n_maps - 1, fn, *xs)
        return jax.lax.map(mapped_fn, xs=args, batch_size=bs if bs > 1 else None)
    else:
        mapped_fn = lambda *xs: _map_fn(lax_map, bs, n_maps - 1, fn, *xs)
        return vmap(mapped_fn)(*args)
# ---
def zigzag(n, k): 
	if (n == 0 and k == 0): 
		return 1
	if (k == 0): 
		return 0
	return zigzag(n, k - 1) + zigzag(n - 1, n - k)
# ---
def generate_subsequences(n):
    subsequences = []
    combinations_list = []
    index = 4
#Generate all combinations
    while index > 0:
        combinations_list.append(list(combinations(str(n), index)))
        index -= 1
#Formatting combinations
    for index in combinations_list:
        for combination in index:
            subsequences.append(''.join(combination))
    return subsequences
# ---
def wrapped_scan(*args, **kwargs):
        scan_calls.append(kwargs.get("unroll"))
        return original_scan(*args, **kwargs)
# ---
def is_inside_jit():
    """Returns True if we're currently inside a jit"""
    return isinstance(jnp.zeros(()), jax.core.Tracer)
# ---
def to_proto(self) -> vm_pb2.SliceInfo:
        """Convert to proto for RPC APIs."""
        return vm_pb2.SliceInfo(
            slice_id=self._slice_id,
            scale_group=self._scale_group,
            created_at=self._created_at.to_proto(),
            vms=[vm.info for vm in self._vms],
        )
# ---
def __enter__(self):
        gui, backend = self.shell.enable_matplotlib(self.new_backend)
# ---
def mask_to_bias(mask: NamedArray, mask_value: float = -1e9) -> NamedArray:
    return mask * mask_value
# ---
def gen_header(data):
    if data['start_year'] == data['end_year']:
        data['dates'] = data['start_year']
    else:
        data['dates'] = '{} - {}'.format(data['start_year'], data['end_year'])
    return '\n'.join(line.rstrip() for line in data['header'].format(**data).strip().splitlines())
# ---
def reject_all(self, include_accepted=False):
        '''
        Reject all keys

        :param bool include_accepted: Whether or not to accept a matched key that was formerly accepted
        '''
        self.reject('*', include_accepted=include_accepted)
# ---
def arctan(a: A) -> A:
    return wrap_elemwise_unary(jnp.arctan, a)
# ---
def get_program_changes(self):
        '''Get all unique program changes used in this Track, sorted.
        '''
        post = []
        for event in self.events:
            if event.type_ == 'PROGRAM_CHANGE':
                if event.data not in post:
                    post.append(event.data)
        return post
# ---
def _get_llama_config(use_flash=False, num_kv_heads=4, seq_len=128) -> LlamaConfig:
    return LlamaConfig(
        max_seq_len=seq_len,
        hidden_dim=32,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        attn_backend=AttentionBackend.DEFAULT if use_flash else AttentionBackend.VANILLA,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def dummy_fn():
      # pylint: disable=pointless-statement
      value
# ---
def sort_numeric_strings(nums_str):
    result = [int(x) for x in nums_str]
    result.sort()
    return result
# ---
def ncr_modp(n, r, p): 
    C = [0 for i in range(r+1)]   
    C[0] = 1
    for i in range(1, n+1): 
        for j in range(min(i, r), 0, -1): 
            C[j] = (C[j] + C[j-1]) % p   
    return C[r]
# ---
def _arange(x, size, start, stop, step, arange_dtype, block_id=None):
    i = block_id[0]
    blockstart = start + (i * size * step)
    blockstop = start + ((i + 1) * size * step)
    return nxp.arange(blockstart, min(blockstop, stop), step, dtype=arange_dtype)
# ---
def peek(self):
        if self.tokens:
            return self.tokens[-1]
        return None
# ---
def dirty(self, value):
        self._dirty = value
        self.refresh_window_title()
# ---
def _get_hf_kernels():
    pytest.importorskip("torch")
    pytest.importorskip("transformers")
    from transformers.models.qwen3_next.modular_qwen3_next import (
        torch_chunk_gated_delta_rule as hf_chunk,
        torch_recurrent_gated_delta_rule as hf_recur,
    )

    return hf_chunk, hf_recur
# ---
def mod_ndwi_learned(domain, b):
    if domain.unflooded_domain == None:
        print('No unflooded training domain provided.')
        return None
    unflooded_b = modis_utilities.compute_modis_indices(domain.unflooded_domain)
    water_mask  = modis_utilities.get_permanent_water_mask()
    threshold   = modis_utilities.compute_binary_threshold(get_mod_ndwi(unflooded_b), water_mask, domain.bounds)
    return mod_ndwi(domain, b, threshold)
# ---
def _native_logs_tail(log_dir: str | None, *, max_lines: int = 200) -> str:
    if not log_dir:
        return "<no log directory available for native vLLM server>"
    stdout_path = os.path.join(log_dir, "stdout.log")
    stderr_path = os.path.join(log_dir, "stderr.log")
    return (
        "--- stdout (tail) ---\n"
        f"{_tail_file(stdout_path, max_lines)}\n"
        "--- stderr (tail) ---\n"
        f"{_tail_file(stderr_path, max_lines)}"
    )
# ---
def __exit__(self, type, value, tb):
        gui, backend = self.shell.enable_matplotlib(self.old_backend)
# ---
def elapsed_seconds(self) -> float:
        """Get elapsed time in seconds."""
        return time.monotonic() - self._start
# ---
def __init__(self, name, crypto, description, pluginMethod):
		self.name = name #No Spaces please...
		self.cryptoMethod = crypto 
		self.description = description
		self.plugin = pluginMethod
# ---
def _dnsname_to_pat(dn):
        pats = []
        for frag in dn.split(r'.'):
            if frag == '*':
                # When '*' is a fragment by itself, it matches a non-empty dotless
                # fragment.
                pats.append('[^.]+')
            else:
                # Otherwise, '*' matches any dotless fragment.
                frag = re.escape(frag)
                pats.append(frag.replace(r'\*', '[^.]*'))
        return re.compile(r'\A' + r'\.'.join(pats) + r'\Z', re.IGNORECASE)
# ---
def metric_wrapper(
        target: torch.Tensor,
        gen: torch.Tensor,
    ) -> torch.Tensor:
        if source == "gen":
            return metric(gen)
        elif source == "target":
            return metric(target)
# ---
def test_greater_than(self):
        """Test that cmp_version compares a as greater than b"""
        self.assertTrue(vmops.cmp_version('1.2.3.5', '1.2.3.4') > 0)
# ---
def __init__(self, async_iter):
        self.async_iter = async_iter
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self._exhausted = False
# ---
def test_complex_logical(self):
        # (a > 0 AND b > 0) OR c > 0
        expr = ((col("a") > 0) & (col("b") > 0)) | (col("c") > 0)
        assert expr.evaluate({"a": 1, "b": 1, "c": -1}) is True
        assert expr.evaluate({"a": -1, "b": -1, "c": 1}) is True
        assert expr.evaluate({"a": -1, "b": -1, "c": -1}) is False
# ---
def LaunchJob(self, request, context):
        """Job lifecycle"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def launch_job(
        self,
        request: cluster_pb2.Controller.LaunchJobRequest,
    ) -> cluster_pb2.Controller.LaunchJobResponse:
        """Submit a job to the controller."""
        return self._service.launch_job(request, None)
# ---
def __init__(self, code, headers):
        self.code = code
        self.headers = headers
        self.reset()
# ---
def _patch_source_config(
    input_config: LmDatasetSourceConfigBase, output_path: str, extra_tags: list[str]
) -> LmDatasetSourceConfigBase:
    """
    Patch the source config to point to the new cache.

    TODO: would be better to make this more explicit somehow...
    """
    base_tags = input_config.tags or []
    return dataclasses.replace(input_config, cache_dir=output_path, tags=base_tags + extra_tags)
# ---
def example_reading_spec(self):
    data_fields = {
        "image/encoded": tf.FixedLenFeature((), tf.string),
        "image/format": tf.FixedLenFeature((), tf.string),
    }

    data_items_to_decoders = {
        "inputs":
            contrib.slim().tfexample_decoder.Image(
                image_key="image/encoded",
                format_key="image/format",
                channels=self.num_channels),
    }

    return data_fields, data_items_to_decoders
# ---
def test_all_indices_covered(self, sampler_from_datasets):
        """All indices should be covered when iterating."""
        datasets = [MockDataset(10), MockDataset(10)]
        sampler = sampler_from_datasets(
            datasets=datasets,
            group_key=lambda ds: ds.grid_size,
            batch_size=2,
            shuffle=False,
            drop_last=False,
        )

        all_indices = [idx for batch in sampler for idx in batch]
        assert set(all_indices) == set(range(20))
# ---
def bmarks():
    return_data = get_import_bm()
    return return_data
# ---
def finger_all(self):
        '''
        Print out all fingerprints
        '''
        matches = self.key.finger('*')
        salt.output.display_output(
                matches,
                'key',
                self.opts)
# ---
def decodeFinished(self):
        self.status.completed = True
# ---
def list_slices(self) -> list[FakeVmGroup]:
        """List all VM groups."""
        with self._lock:
            return list(self._slices.values())
# ---
def test_task_with_ports(worker):
    """Test task with port allocation."""
    request = create_run_task_request(ports=["http", "grpc"])
    task_id = worker.submit_task(request)

    task = worker.get_task(task_id)
    assert len(task.ports) == 2
    assert "http" in task.ports
    assert "grpc" in task.ports
    assert task.ports["http"] != task.ports["grpc"]

    task.thread.join(timeout=15.0)
# ---
def astype(self, dtype) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.astype(dtype), self.axes)
# ---
def sync_backend(request):
    with fray_default_job_ctx(create_job_ctx("sync")):
        yield
# ---
def test_wait_for_condition_timeout() -> None:
    """Test wait_for_condition raises TimeoutError when condition never becomes true."""
    import pytest

    start = time.monotonic()
    with pytest.raises(TimeoutError, match="did not become true within"):
        wait_for_condition(lambda: False, timeout=Duration.from_seconds(0.1))
    elapsed = time.monotonic() - start

    # Should wait approximately the timeout duration
    assert 0.09 < elapsed < 0.2
# ---
def tanh(a: A) -> A:
    return wrap_elemwise_unary(jnp.tanh, a)
# ---
def __init__(self, min_len: int = 4, max_len: int = 5000) -> None:
        """Initialize the filter.

        Parameters
        ----------
        min_len : float, optional
            The minimum allowed length.
        max_len : float, optional
            The maximum allowed length.

        """
        self._min = min_len
        self._max = max_len
# ---
def clear_tuple(test_tup):
  temp = list(test_tup)
  temp.clear()
  test_tup = tuple(temp)
  return (test_tup)
# ---
def get_result_proxy(self):
                return cls(self)
# ---
def pop(self):
        assert isinstance(self.ast, ast.CharClass)
        last = self.ast.elems[-1]
        self.ast.elems = self.ast.elems[:-1]
        return last
# ---
def get_test_client(nowait=False):
    client = get_es_connection('default')

    # wait for yellow status
    for _ in range(1 if nowait else 5):
        try:
            client.cluster.health(wait_for_status="yellow")
            return client
        except ConnectionError:
            time.sleep(0.1)
    else:
        # timeout
        raise SkipTest("Elasticsearch failed to start")
# ---
def __init__(self, value):
        self.value = value
# ---
def test_is_cloneable_share_goodformat1(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://10.61.222.333/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def get_test_contract(name):
    contract_path = os.path.abspath(
        os.path.join(os.path.dirname(__file__), "..", "smart_contracts", name)
    )
    contracts = compile_files_cwd([contract_path])

    return contract_path, contracts
# ---
def corofunc2():
            called.append(corofunc2)
# ---
def unbind(self, axis: AxisSelector) -> Sequence["NamedArray"]:  # pragma: no cover
        return haliax.unbind(self, axis=axis)
# ---
def rearrange_bigger(n):
    nums = list(str(n))
    for i in range(len(nums)-2,-1,-1):
        if nums[i] < nums[i+1]:
            z = nums[i:]
            y = min(filter(lambda x: x > z[0], z))
            z.remove(y)
            z.sort()
            nums[i:] = [y] + z
            return int("".join(nums))
    return False
# ---
def _parse_hits(self, hits, resource):
        """Parse hits response into documents."""
        datasource = self._datasource(resource)
        schema = config.DOMAIN[datasource[0]]['schema']
        dates = get_dates(schema)
        docs = []
        for hit in hits.get('hits', {}).get('hits', []):
            docs.append(format_doc(hit, schema, dates))
        return ElasticCursor(hits, docs)
# ---
def set_node(self, node):
        self.node = node
# ---
def print_all(self):
        '''
        Print out all managed keys
        '''
        self.print_key('*')
# ---
def test_argmax_axis_0(spec):
    a = xp.asarray([[11, 12, 13], [11, 11, 14], [10, 13, 11]], chunks=(2, 2), spec=spec)
    b = xp.argmax(a, axis=0)
    assert_array_equal(
        b.compute(),
        np.array([[11, 12, 13], [11, 11, 14], [10, 13, 11]]).argmax(axis=0),
    )
# ---
def errno_value():
    """
    A particular errno.
    """
    return errno.EINVAL
# ---
def loggamma(key, shape: AxisSpec, a: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    a = broadcast_to(a, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.loggamma(key, a.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def finish(self):
        self.wandb_logger.finish()
# ---
def test_searchsorted(x1, x1_chunks, x2, x2_chunks, side):
    x1 = np.array(x1)
    x2 = np.array(x2)

    x1d = xp.asarray(x1, chunks=x1_chunks)
    x2d = xp.asarray(x2, chunks=x2_chunks)

    out = xp.searchsorted(x1d, x2d, side=side)

    assert out.shape == x2d.shape
    assert out.chunks == x2d.chunks
    assert_array_equal(out.compute(), np.searchsorted(x1, x2, side=side))
# ---
def uint_type(self) -> np.dtype:
        """A uint type that is the same size as the field's value."""
        # Yes, this is in bytes, so u4 == uint32
        return np.dtype(f"u{self.size_in_bytes()}")
# ---
def date(self):
        '''Publication date. (optional)

        Must be given in "YYYY[-MM[-DD]]" format.'''
        try:
            return self.opt_meta['date']
        except KeyError:
            return None
# ---
def job_request():
    return cluster_pb2.Controller.LaunchJobRequest(
        name=JobName.root("test-job").to_wire(),
        entrypoint=_make_test_entrypoint(),
        resources=cluster_pb2.ResourceSpecProto(cpu=2, memory_bytes=4 * 1024**3),
        environment=cluster_pb2.EnvironmentConfig(),
        replicas=1,
    )
# ---
def output(self):
        return luigi.LocalTarget(os.path.join(os.getcwd(), "data", "hola_mundo_desde_python.json"))
# ---
def test_deeply_nested(self):
        expr = col("data")["level1"]["level2"]
        assert expr.evaluate({"data": {"level1": {"level2": "value"}}}) == "value"
# ---
def connect_client():
    """Connects to Mongo client"""
    try:
        return MongoClient(app.config['DB_HOST'], int(app.config['DB_PORT']))
    except errors.ConnectionFailure as e:
        raise e
# ---
import re
def text_uppercase_lowercase(text):
        patterns = '[A-Z]+[a-z]+$'
        if re.search(patterns, text):
                return 'Found a match!'
        else:
                return ('Not matched!')
# ---
def advance_time(delta_seconds):
        nonlocal fake_now
        fake_now += timedelta(seconds=delta_seconds)
# ---
def _relative_positions(seg_ids):
        idx = jnp.arange(seg_ids.shape[0])
        is_start = jnp.concatenate([jnp.array([True]), seg_ids[1:] != seg_ids[:-1]])
        start_idx = idx * is_start.astype(idx.dtype)
        seg_start = jax.lax.associative_scan(jnp.maximum, start_idx)
        return idx - seg_start
# ---
def __init__(self, fn, batch_size, num_cpus, num_gpus, resources, output_exemplar=None):
        self.fn = fn
        self.batch_size = batch_size
        self.num_cpus = num_cpus
        self.num_gpus = num_gpus
        self.resources = resources
        self.output_exemplar = output_exemplar
# ---
def transform(self, x, scalar):
            return x + self.w + scalar
# ---
def __call__(self, target, cred):
        """Check the policy."""

        return True
# ---
def app_settings():

    app_settings = {'GRAPHITE_HOST': settings.GRAPHITE_HOST,
                    'OCULUS_HOST': settings.OCULUS_HOST,
                    'FULL_NAMESPACE': settings.FULL_NAMESPACE,
                    }

    resp = json.dumps(app_settings)
    return resp, 200
# ---
def removeLink( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def compute_loss(model: LmHeadModel, example: LmExample):
            with hax.axis_mapping(compute_axis_mapping):
                model = inference_mode(model, True)
                model = mp.cast_to_compute(model)
                return model.compute_next_token_loss(example, key=None)
# ---
def test_combine_blocks_iter_key_function():
    key_function = make_combine_blocks_iter_key_function(
        "a", numblocks=5, split_every=2
    )

    check_key_function(key_function, (0,), "(<('a', 0), ('a', 1)>,)")
    check_key_function(key_function, (1,), "(<('a', 2), ('a', 3)>,)")
    check_key_function(key_function, (2,), "(<('a', 4)>,)")
# ---
def stop(self) -> None:
        """Terminate controller GCE VM."""
        self._delete_vm()
# ---
def col_clause(self):
        return self.field + " = ?", [self.buf_pattern]
# ---
def log(self, metrics: typing.Mapping[str, typing.Any], *, step: Optional[int], commit: Optional[bool] = None):
        """
        Log metrics to the tracker. Step is always required.

        Args:
            metrics: Metrics to log
            step: Step to log at
            commit: Whether to commit the metrics. If None, uses the default for the tracker.
        """
        pass
# ---
def key_str(self, match):
        '''
        Return the specified public key or keys based on a glob
        '''
        ret = {}
        for status, keys in six.iteritems(self.name_match(match)):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                ret[status][key] = self._get_key_str(key, status)
        return ret
# ---
def __setattr__(self, name, value):
        """ Set slot or non-slot field attribute. """
        try:
            object.__setattr__(self, name, value)
        except AttributeError:
            if self._attrs:
                self._attrs[name] = value
            else:
                self._attrs = {name: value}
# ---
def local_client():
    """Create a LocalClusterClient-backed IrisClient for true E2E testing.

    This fixture starts a real Controller and Worker with in-process execution,
    ensuring WorkerPool tests go through the full job submission infrastructure.
    """
    client = IrisClient.local()
    yield client
    client.shutdown(wait=True)
# ---
def debug_print(self, prefix: str = ""):

        def callback(self):
            print(f"{prefix}JitScheduler State:")
            print(f"{prefix}Queued Tokens: {self.queued_tokens}")
            print(f"{prefix}Queued Slot IDs: {self.queued_slot_ids}")
            print(f"{prefix}Num Queued Tokens: {self.num_queued_tokens}")

        jax.experimental.io_callback(callback, None, ordered=True, self=self)
# ---
def is_active(self):
        """Returns `True` if dropout is active (and therefore needs a key), `False` otherwise."""
        return not self.inference and self.pdrop > 0
# ---
def testExprNameLocal(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        foo = 42
        def bar():
          foo
        bar()""")))
# ---
def _get_blob_reference(self):
        return self.get_resource_name(TEST_BLOB_PREFIX)
# ---
def do_clone(gs_clone: GenState) -> GenState:
                new_state, _ = gs_clone.clone_sequence(
                    parent_val,
                    child_local_id=slot_val,
                    seq_params=seq_params,
                )
                return new_state
# ---
def test_count_addition(self):
        """Count addition"""
        count = Count(4, 2)
        self.assertEqual(count + 5, 9)
# ---
def render(self, mystery):
        return json.dumps(mystery.encode(), indent=4)
# ---
def __init__(self,**kwargs):
        self.register_event_type('on_answer')
        super(EditorPopup,self).__init__(**kwargs)
# ---
def __init__(self, width, height, depth=None):
        self.__width = width
        self.__height = height
        self.__depth = depth
# ---
def _pipeline() -> int:
        docs = [dupekit.Document(row["id"], row["text"]) for row in in_memory_table.to_pylist()]
        return len(dupekit.process_rust_structs(docs))
# ---
def __init__(self, field, ascending=True, case_insensitive=True):
        self.field = field
        self.ascending = ascending
        self.case_insensitive = case_insensitive
# ---
def click_and_hold(self, on_element):
        """Holds down the left mouse button on an element.
        Args:
            on_element: The element to mouse down.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.MOUSE_DOWN, {}))
        return self
# ---
def testImportNativeType(self):
    self.assertEqual((0, "<type 'Duration'>\n"), _GrumpRun(textwrap.dedent("""\
        from "__go__/time" import Duration
        print Duration""")))
# ---
def handle(self, *args, **options):

        if options['profile_file']:
            profiler = Profile()
            profiler.runcall(self._handle, *args, **options)
            profiler.dump_stats(options['profile_file'])
        else:
            self._handle(*args, **options)
# ---
def __exit__(self, et, ev, tb):
    if self.level is not None:
      self.logger.setLevel(self.old_level)
    if self.handler:
      self.logger.removeHandler(self.handler)
    if self.handler and self.close:
      self.handler.close()
# ---
def even_position(nums):
	return all(nums[i]%2==i%2 for i in range(len(nums)))
# ---
def unique_values(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> NamedArray:
    """Shortcut for :func:`unique` that returns only unique values."""

    return typing.cast(
        NamedArray,
        unique(
            array,
            Unique,
            axis=axis,
            fill_value=fill_value,
        ),
    )
# ---
def test_limit_offset_selectable_in_unions(self):
        table = self.tables.some_table
        s1 = (
            select([table])
            .where(table.c.id == 2)
            .limit(1)
            .order_by(table.c.id)
        )
        s2 = (
            select([table])
            .where(table.c.id == 3)
            .limit(1)
            .order_by(table.c.id)
        )

        u1 = union(s1, s2).limit(2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def create_tokenization_step(dataset_identifier: str, short_name: str) -> ExecutorStep:
    dataset_config = INSTRUCTION_DATASET_NAME_TO_CONFIG[dataset_identifier]
    dataset = get_instruction_dataset(dataset_identifier, splits=dataset_config.splits)
    return default_tokenize(
        name=f"{short_name}_marin_tokenizer",
        dataset=dataset / "**/*.jsonl.gz",
        tokenizer=marin_tokenizer,
        format=ChatLmDatasetFormat(),
    )
# ---
def finalize_loss():
        lse_ref[...] = (jnp.log(l_scratch_ref[...]) + m_scratch_ref[...]).astype(lse_ref.dtype)
        label_logits_ref[...] = label_logits_scratch_ref[...].astype(label_logits_ref.dtype)
# ---
def list_item(node: RenderTreeNode, context: RenderContext) -> str:
    """Return one list item as string.

    This returns just the content. List item markers and indentation are
    added in `bullet_list` and `ordered_list` renderers.
    """
    block_separator = "\n" if is_tight_list_item(node) else "\n\n"
    text = make_render_children(block_separator)(node, context)

    if not text.strip():
        return ""
    return text
# ---
def env_for_accel(self, accel_type: str) -> dict[str, str]:

        base_env = self.env.copy()

        if "-" in accel_type:
            base_env.update(self.accel_env.get(accel_type.split("-")[0], {}))

        if accel_type in self.accel_env:
            base_env.update(self.accel_env[accel_type])

        return base_env
# ---
def add(self, duration: Duration) -> "Timestamp":
        """Return new timestamp offset by duration."""
        return Timestamp(self._epoch_ms + duration.to_ms())
# ---
def balanced_accuracy_scoring(clf, X, y):
    """Scoring function that computes the balanced accuracy to be used
    internally in the cross-validation procedure.
    """
    y_pred = clf.predict(X)
    conf_mat = confusion_matrix(y, y_pred)
    bal_acc = 0.
    for i in range(len(conf_mat)):
        bal_acc += (float(conf_mat[i, i])) / np.sum(conf_mat[i])

    bal_acc /= len(conf_mat)
    return bal_acc
# ---
def _get_or_create_queue(queue_name: str, maxlen: int | None = None) -> "InMemoryRolloutQueue":
    """Get or create a named in-memory queue."""
    if queue_name not in _MEMORY_QUEUES:
        _MEMORY_QUEUES[queue_name] = InMemoryRolloutQueue(maxlen=maxlen)
    return _MEMORY_QUEUES[queue_name]
# ---
def __invert__(self) -> NotExpr:
        return NotExpr(self)
# ---
def test_iter(self):
        prio_set_list = event._PrioritizedSetList()
        objs = [(i,) for i in range(5)]
        for i, obj in enumerate(objs):
            prio_set_list.add(-i, obj)

        for i, set_ in enumerate(prio_set_list):
            assert set_ == (-i, {objs[i]})
# ---
def test_pass_different_length_seq(num_kv_heads):
    config = GemmaConfig(
        max_seq_len=64,
        hidden_dim=64,
        intermediate_dim=32,
        num_heads=2,
        num_kv_heads=num_kv_heads,
        use_flash_attention=True,
        head_dim=4,
    )
    check_model_works_with_seqlen(GemmaLMHeadModel, config, 16)
# ---
def add_jobs(thread_id: int):
        try:
            barrier.wait()
            for i in range(jobs_per_thread):
                job_id = f"t{thread_id}_j{i}"
                req = job_request(f"job-{job_id}")
                submit_job(state, job_id, req)
        except Exception as e:
            errors.append(e)
# ---
def __init__(self, dim, vocab_dim):
    super().__init__()
    self.embedding = nn.Parameter(
      torch.empty((vocab_dim, dim))
    )
    torch.nn.init.kaiming_uniform_(
      self.embedding, a=math.sqrt(5)
    )
# ---
def build(self, ctx: LrScheduleContext):
        return optax.linear_schedule(ctx.learning_rate, ctx.min_lr, ctx.decay_steps)
# ---
def terminate(self, job_id: JobId) -> None:
        """Terminate all replicas of a job."""
        self._get_job(job_id).cleanup()
# ---
def __repr__(self):
        return "<nevermatcher>"
# ---
def get_cpu_millicores(self) -> int | None:
        if not self.resources or not self.resources.cpu:
            return None
        return self.resources.cpu * 1000
# ---
def get_bundle(self, gcs_path: str, expected_hash: str | None = None) -> Path:
        del gcs_path, expected_hash
        return self._bundle_path
# ---
def get_global_buffer() -> LogRingBuffer:
    return _global_buffer
# ---
def __init__(self, *args, **kwargs):
        super(UsuarioEditForm, self).__init__(*args, **kwargs)
        self.fields['primeiro_telefone'].widget.attrs['class'] = 'telefone'
        self.fields['segundo_telefone'].widget.attrs['class'] = 'telefone'
# ---
def __module_version(self):
        return self.version
# ---
def test_count_add_none_capacity(self):
        """Count addition with one None consumed_capacity"""
        cap = Capacity(3, 0)
        count = Count(4, 2)
        count2 = Count(5, 3, cap)
        ret = count + count2
        self.assertEqual(ret, 9)
        self.assertEqual(ret.scanned_count, 5)
        self.assertEqual(ret.consumed_capacity, cap)
# ---
def __iter__(self):
        return iter(self.subqueries)
# ---


def derivative(xs: list):
    """ xs represent coefficients of a polynomial.
    xs[0] + xs[1] * x + xs[2] * x^2 + ....
     Return derivative of this polynomial in the same form.
    >>> derivative([3, 1, 2, 4, 5])
    [1, 4, 12, 20]
    >>> derivative([1, 2, 3])
    [2, 6]
    """
    return [(i * x) for i, x in enumerate(xs)][1:]
# ---
def eliminate_axes(axis_spec: AxisSelection, axes: AxisSelection) -> AxisSelection:  # type: ignore
    ...
# ---
def average_tuple(nums):
    result = [sum(x) / len(x) for x in zip(*nums)]
    return result
# ---
def get_supported_boot_devices(self, task):
        """Get a list of the supported boot devices.

        :param task: a task from TaskManager.
        :returns: A list with the supported boot devices defined
                  in :mod:`ironic.common.boot_devices`.

        """
        return [boot_devices.PXE, boot_devices.DISK, boot_devices.CDROM,
                boot_devices.BIOS, boot_devices.SAFE]
# ---
def check_tuples(test_tuple, K):
  res = all(ele in K for ele in test_tuple)
  return (res)
# ---
def multi_list(rownum,colnum):
  multi_list = [[0 for col in range(colnum)] for row in range(rownum)]
  for row in range(rownum):
    for col in range(colnum):
        multi_list[row][col]= row*col
  return multi_list
# ---
def test_bound_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).offset(bindparam("o")),
            [(3, 3, 4), (4, 4, 5)],
            params={"o": 2},
        )
# ---
def __str__(self):
        return self.name
# ---
def remove(self, resource, lookup=None):
        args = self._es_args(resource)
        if lookup:
            try:
                return self.es.delete(id=lookup.get('_id'), refresh=True, **args)
            except elasticsearch.NotFoundError:
                return
        else:
            query = {'query': {'match_all': {}}}
            return self.es.delete_by_query(body=query, **args)
# ---
def visit_mfrac(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            num = self._visit(children[0])
            den = self._visit(children[1])
            return BracedNode(f"\\frac{{{num}}}{{{den}}}")
        return TextNode("")
# ---
def do_alloc(state):
                    indices, ref_counts = state
                    # choose a page with the smallest ref count; when has_free, argmin will pick a zero-ref page
                    free_page_idx = hax.argmin(ref_counts, "page")
                    ref_counts = ref_counts.at["page", free_page_idx].add(1)
                    indices = indices.at["seq", seq_id, "page", page_idx].set(free_page_idx)
                    return indices, ref_counts
# ---
def __enter__(self):
    self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()
    for objects in self.custom_objects:
      _GLOBAL_CUSTOM_OBJECTS.update(objects)
    return self
# ---
def _call_remote(*args, **kw):
                return ray.get(step.fn.remote(*args, **kw))
# ---
def chunks(self) -> tuple[int, ...]:
        return self.array.chunk_layout.read_chunk.shape or ()
# ---
def train_dataloader(self) -> DataLoader:
        """Get the training dataloader.

        Returns
        -------
        DataLoader
            The training dataloader.

        """
        return DataLoader(
            self._train_set,
            batch_size=self.cfg.batch_size,
            num_workers=self.cfg.num_workers,
            pin_memory=self.cfg.pin_memory,
            shuffle=False,
            collate_fn=collate,
        )
# ---
def test_wait_all_all_succeed(client: LocalClient):
    handles = [client.submit(JobRequest(name=f"ok-{i}", entrypoint=Entrypoint.from_callable(_noop))) for i in range(3)]
    statuses = wait_all(handles)
    assert all(s == JobStatus.SUCCEEDED for s in statuses)
# ---
def test_repeat(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.repeat(a, 3, axis=0)
    run_operation(tmp_path, executor, "repeat", b)
# ---
def forward(self, x):
        return self.upsampler(x)
# ---
def test_legacy_dbapi_error(self):
        engine = engines.testing_engine()
        canary = Mock()

        event.listen(engine, "dbapi_error", canary)

        with engine.connect() as conn:
            try:
                conn.execute("SELECT FOO FROM I_DONT_EXIST")
                assert False
            except tsa.exc.DBAPIError as e:
                eq_(canary.mock_calls[0][1][5], e.orig)
                eq_(canary.mock_calls[0][1][2], "SELECT FOO FROM I_DONT_EXIST")
# ---
def _setup_regular(self, env):
        super(Float, self)._setup_regular(env)
        self._setup_digits(env)
# ---
def checkPause(self):
    pass
# ---
def load_apartment_by_doorbell_id(doorbell_id):
        # type: (int) -> Optional[ApartmentDTO]
        apartment_orm = Apartment.select().where(Apartment.doorbell_rebus_id == doorbell_id).first()
        if apartment_orm is None:
            return None
        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
        return apartment_dto
# ---
def run(self):
        logger.debug("Guardando en {} el catlogo {}".format(self.output().path, self.catalog_name))

        with closing(requests.get(self.catalog_url, stream= True)) as response, \
             self.output().open('w') as output_file:
            for chunk in response.iter_lines(chunk_size=1024*8):
                if chunk:
                    output_file.write(chunk.decode('utf-8') + '\n')
# ---
def i_add_a_cube_of_size_size_at_location(size, location):
    bpy.ops.mesh.primitive_cube_add(size=float(size), location=[float(co) for co in location.split(",")])
# ---
def bad(self, f, msg):
        self._matcher.bad(self._path + "/" + f, msg)
# ---
def __init__(self, **kwargs):
        env = Environment(loader=PackageLoader('hotzenplotz.worker','templates'))
        self.template =  env.get_template('cron')
        self.dir_path = None
# ---


def add(x: int, y: int):
    """Add two numbers x and y
    >>> add(2, 3)
    5
    >>> add(5, 7)
    12
    """
    return x + y
# ---
def get_job(self, job_id: JobName) -> ControllerJob | None:
        with self._lock:
            return self._jobs.get(job_id)
# ---
def _autoscaling_monitor_loop(self):
        """Start a background thread that monitors the load and scales the number of actors accordingly."""
        while not self._monitor_stop_event.is_set():
            try:
                time.sleep(self.scale_check_interval)
                self._check_and_scale()
            except Exception:
                # Best-effort; do not crash the monitor
                pass
# ---
def rate_noise(self, t):
    """
    Rate of change of noise ie g(t)
    """
    pass
# ---
def round(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in round")
    return elemwise(nxp.round, x, dtype=x.dtype)
# ---
def __repr__(self):
        return_str = "<MidiFile %d tracks\n" % len(self.tracks)
        for track in self.tracks:
            return_str = return_str + "  " + track.__repr__() + "\n"
        return return_str + ">"
# ---
def setUp(self):
        super(XenAPIDiffieHellmanTestCase, self).setUp()
        self.alice = vmops.SimpleDH()
        self.bob = vmops.SimpleDH()
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        k_gate, k_up, k_down = maybe_rng_split(key, 3)
        h = self.act(self.gate_proj(x, key=k_gate)) * self.up_proj(x, key=k_up)
        return self.down_proj(h, key=k_down)
# ---
def position_max(list1):
    max_val = max(list1)
    max_result = [i for i, j in enumerate(list1) if j == max_val]
    return max_result
# ---
def the_tokenizer(self) -> PreTrainedTokenizerBase:
        if self.tokenizer is None:
            return self.the_processor.tokenizer
        else:
            return load_tokenizer(self.tokenizer)
# ---
def string_match(cls, pattern, value):
        return pattern.lower() == value.lower()
# ---
def cluster_list_configs(ctx):
    """List available cluster configurations."""
    configs = list_available_configs()
    if not configs:
        print("No cluster configurations found in infra/")
        return

    print("Available cluster configurations:")
    for config_path in configs:
        print(f"  {config_path}")
# ---
def log_step_info_inner(step: StepInfo):
        metrics = {"train/loss": step.loss, "global_step": step.step}
        if total_steps:
            metrics["run_progress"] = step.step / total_steps
        log_optimizer_hyperparams(step.opt_state, step=step.step, prefix="optim")
        levanter.tracker.log(metrics, step=step.step)
# ---
def test_position_tokens_dont_overlap_base(tok):
    """Position tokens and base vocab tokens should not overlap."""
    pos_range = set(range(tok.position_token_offset, tok.position_token_offset + tok.num_position_tokens))
    base_range = set(range(tok.base_token_offset, tok.base_token_offset + tok.base_vocab_size))
    assert not pos_range & base_range
# ---
def _normalize_unroll(unroll: int | bool | None, block_size: int) -> int | bool:
    """Convert user-provided ``unroll`` values into something understood by ``jax.lax.scan``."""

    if unroll is None:
        return 1

    if isinstance(unroll, bool):
        return unroll

    resolved = int(unroll)
    if resolved < 1:
        raise ValueError(f"unroll must be >= 1; got {resolved}.")

    return resolved
# ---
def KeyPos(self) -> Axis:
        return self.config.KeyPos
# ---
def total_tokens(self) -> int:
        return sum(len(req.prompt_tokens) + req.max_tokens for req in self)
# ---
def _create_zarr_indexer(selection, shape, chunks):
    if zarr.__version__[0] == "3":
        from zarr.core.chunk_grids import RegularChunkGrid
        from zarr.core.indexing import OrthogonalIndexer

        return OrthogonalIndexer(selection, shape, RegularChunkGrid(chunk_shape=chunks))
    else:
        from zarr.indexing import OrthogonalIndexer

        return OrthogonalIndexer(selection, ZarrArrayIndexingAdaptor(shape, chunks))
# ---
def extract_unique(test_dict):
  res = list(sorted({ele for val in test_dict.values() for ele in val}))
  return res
# ---
def test_jax_device_kind_to_fray_device_type(jax_device_kind, expected_fray_type):
    assert jax_device_kind_to_fray_device_type(jax_device_kind) == expected_fray_type
# ---
def put(self, obj: Any) -> Any:
        """Identity operation - in-process, no serialization needed."""
        return obj
# ---
def test_compute_next_token_loss_reduction_returns_scalar():
    Vocab = Axis("vocab", 32)
    cfg = ToyLmConfig(max_seq_len=8, embed_dim=16)
    model = ToyLmHeadModel.init(Vocab, cfg, key=jax.random.PRNGKey(0))

    Batch = Axis("batch", 4)
    Pos = cfg.max_Pos.resize(8)
    example = _toy_example(Batch, Pos, Vocab, key=jax.random.PRNGKey(1))

    loss = model.compute_next_token_loss(example)
    assert loss.axes == ()
    assert jnp.shape(loss.array) == ()
# ---
def arcsin(a: A) -> A:
    return wrap_elemwise_unary(jnp.arcsin, a)
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "WhisperModel":
        new_decoder = self.decoder.resize_vocab(new_size, key)
        return dataclasses.replace(self, decoder=new_decoder)
# ---
def compute_action(self, state, t):
        u_in_base, u_out = self.base_controller(state, t)

        err = self.sim.pressure - state
        self.I = self.I * (1 - self.decay) + err * self.decay

        pid_correction = self.K[0] * err + self.K[1] * self.I

        u_in = torch.clamp(u_in_base + pid_correction, min=0.0, max=100.0)
        self.sim(u_in, u_out, t)

        return u_in, u_out
# ---
def test_mutation_apply():
    m = Mutation(start=4, end=9, replacement="world", node_type="Name", original="hello")
    result = m.apply("say hello there")
    assert result == "say world there"
# ---
def replace_dot_general(x):
        if isinstance(x, DefaultDotGeneralOp):
            return PreciseDotGeneralOp()
        return x
# ---
def first_Missing_Positive(arr,n): 
    ptr = 0
    for i in range(n):
        if arr[i] == 1:
            ptr = 1
            break
    if ptr == 0:
        return(1)
    for i in range(n):
        if arr[i] <= 0 or arr[i] > n:
            arr[i] = 1
    for i in range(n):
        arr[(arr[i] - 1) % n] += n
    for i in range(n):
        if arr[i] <= n:
            return(i + 1)
    return(n + 1)
# ---
def test_groups_created_from_dataset_sizes(self):
        """Groups should partition indices based on cumulative dataset boundaries."""
        sampler = EquivalenceGroupBatchSampler.from_dataset_sizes(
            dataset_sizes=[3, 5, 2],
            batch_size=2,
            shuffle=False,
        )

        assert sampler.groups == [
            [0, 1, 2],  # first dataset: indices 0-2
            [3, 4, 5, 6, 7],  # second dataset: indices 3-7
            [8, 9],  # third dataset: indices 8-9
        ]
# ---


def encode_shift(s: str):
    """
    returns encoded string by shifting every character by 5 in the alphabet.
    """
    return "".join([chr(((ord(ch) + 5 - ord("a")) % 26) + ord("a")) for ch in s])


def decode_shift(s: str):
    """
    takes as input string encoded with encode_shift function. Returns decoded string.
    """
    return "".join([chr(((ord(ch) - 5 - ord("a")) % 26) + ord("a")) for ch in s])
# ---
def test_sequence_table_reserve_and_release_slot():
    pt = _make_table()
    sequences = SequenceTable.init(pt.max_seqs, pt.pages_per_seq, pt.page_size)

    sequences, slot_arr = sequences.reserve_slot()
    slot = int(slot_arr)
    assert slot == 0
    assert bool(sequences.used_mask.array[slot])

    sequences = sequences.release_slot(slot)
    assert not bool(sequences.used_mask.array[0])
# ---
def test_count_multiplication(self):
        """Count multiplication"""
        count = Count(4, 2)
        self.assertEqual(2 * count, 8)
# ---
def match(self, item):
        if self.field not in item:
            return False
        value = item[self.field]
        if isinstance(value, str):
            value = self._convert(value)

        if self.point is not None:
            return value == self.point
        else:
            if self.rangemin is not None and value < self.rangemin:
                return False
            if self.rangemax is not None and value > self.rangemax:
                return False
            return True
# ---
def check_access_rights(self, cr, uid, operation, raise_exception=True):
        #override in order to redirect the check of acces rights on the stock.picking object
        return self.pool.get('stock.picking').check_access_rights(cr, uid, operation, raise_exception=raise_exception)
# ---
def exists(results, _name):
        ''' Check to see if the results include the name '''
        if not results:
            return False

        if Utils.find_result(results, _name):
            return True

        return False
# ---
def is_finite(self) -> bool:
        return self.dataset.has_len()
# ---
def test_profile__inference_loader__1gb(inference_loader_pair, benchmark):
    cfg, loader = inference_loader_pair

    @benchmark
    def bench():
        for sample in loader:
            dataset, n = sample
            for X, y in dataset:
                _, _ = X, y
# ---
def test_write_vortex_single_record(self, tmp_path):
        """Test writing single record."""
        records = [{"key": "value", "number": 42}]
        output_path = tmp_path / "single.vortex"

        result = write_vortex_file(records, str(output_path))
        assert result["count"] == 1

        loaded = list(load_vortex(str(output_path)))
        assert loaded == records
# ---
def _match_greater_than_or_equal(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) >= str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def test_unstack_zero_arrays(spec):
    a = xp.full((0, 4, 6), 1, chunks=(1, 2, 3), spec=spec)
    assert xp.unstack(a) == ()
# ---
def device_password(self):
        if self._values['managed']:
            return None
        return self._values['device_password']
# ---
def test_simple_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).offset(2),
            [(3, 3, 4), (4, 4, 5)],
        )
# ---
def path_is_jnetfs(path):
  #check if PATH is VFS or not

  df_output_lines = os.popen("df -Ph '%s'" % path).read().splitlines()

  return df_output_lines and "JnetFS" in df_output_lines[1]
# ---
import heapq as hq
def heap_queue_largest(nums,n):
  largest_nums = hq.nlargest(n, nums)
  return largest_nums
# ---
def test_pspec_for_named_axes():
    mesh = Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping(resource_map), mesh:
        mod = MyModule(named=hax.ones((Dim1, Dim2, Dim3)), unnamed1=jnp.ones(Dim2.size), static_field=1)

        specs: MyModule = pspec_for(mod)

        spec = PartitionSpec(None, ResourceAxis.DATA, ResourceAxis.MODEL)

        assert specs.named == spec
        assert specs.unnamed1 == PartitionSpec(None)
# ---
def target_space_id(self):
    raise NotImplementedError()
# ---
def test_kill_nonexistent_task(worker):
    """Test killing a nonexistent task returns False."""
    result = worker.kill_task(JobName.root("nonexistent-task").task(0).to_wire())
    assert result is False
# ---
def test_build_runtime_env_cpu_sets_jax_platforms():
    from fray.v2.ray_backend.backend import build_runtime_env

    request = JobRequest(
        name="cpu-test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=CpuConfig()),
    )
    env = build_runtime_env(request)
    assert env["env_vars"]["JAX_PLATFORMS"] == "cpu"
# ---
def test_list_migrations_in_flavor_resize_situation(self):
        """Admin can get the migrations list containing the resized server"""
        server = self.create_test_server(wait_until="ACTIVE")
        server_id = server['id']

        self.resize_server(server_id, self.flavor_ref_alt)

        body = self.client.list_migrations()['migrations']

        instance_uuids = [x['instance_uuid'] for x in body]
        self.assertIn(server_id, instance_uuids)
# ---
def cleanup(self) -> None:
        """Cleanup Flight server resources."""
        # shutdown servers in parallel in threads to avoid blocking on shutdown
        for flight_server in self._flight_servers:
            logger.debug(f"Shutting down Arrow Flight server at {flight_server._location}...")
            threading.Thread(target=flight_server.shutdown, daemon=True).start()
# ---
def log2(a: A) -> A:
    return wrap_elemwise_unary(jnp.log2, a)
# ---
def floor_divide(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "floor_divide")
    if x1.dtype not in _real_numeric_dtypes or x2.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in floor_divide")
    return elemwise(nxp.floor_divide, x1, x2, dtype=result_type(x1, x2))
# ---
def on_compute_start(self, event):
        self.events = []
# ---
def __init__(self):
        self.beforeToRtl = []
        self.beforeToRtlImpl = []
        self.afterToRtlImpl = []

        self.beforeHdlArchGeneration = [
            extract_part_drivers,
            removeUnconnectedSignals,
            markVisibilityOfSignalsAndCheckDrivers,
        ]
        self.afterToRtl = []
# ---
def convert_math(self, el, text, convert_as_inline):
        try:
            x = mathml_to_markdown(el)
            return x
        except Exception as e:
            logger.exception(f"Error converting math: {e}")
            return text
# ---
def check_valid_bot_type(user_profile: UserProfile, bot_type: int) -> None:
    if bot_type not in user_profile.allowed_bot_types:
        raise JsonableError(_("Invalid bot type"))
# ---
def __post_init__(self):
        if not self._parts:
            raise ValueError("JobName cannot be empty")
        for part in self._parts:
            if "/" in part:
                raise ValueError(f"JobName component cannot contain '/': {part}")
            if not part or not part.strip():
                raise ValueError("JobName component cannot be empty or whitespace")
# ---
def find_project_root(file_path, marker_files):
    """Walk up from file_path looking for project root markers."""
    current = os.path.dirname(os.path.abspath(file_path))
    for _ in range(10):  # Max 10 levels up
        for marker in marker_files:
            if os.path.exists(os.path.join(current, marker)):
                return current
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent
    return None
# ---
def test_list_tables_page(self):
        """Call to ListTables should page results"""
        hash_key = DynamoKey("id")
        for i in range(120):
            self.dynamo.create_table("table%d" % i, hash_key=hash_key)
        tables = list(self.dynamo.list_tables(110))
        self.assertEqual(len(tables), 110)
# ---
def remove(self, tag: str) -> None: ...
# ---
def asGenData(self):
        """ @rtype: GenData """
        impl_type = EnkfNode.cNamespace().get_impl_type(self)
        assert impl_type == ErtImplType.GEN_DATA

        return GenData.createCReference(self.valuePointer(), self)
# ---
def collect_workdir_size_mb(workdir: Path) -> int:
    """Calculate workdir size in MB using du -sm."""
    if not workdir.exists():
        return 0

    result = subprocess.run(
        ["du", "-sm", str(workdir)],
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        return 0

    # du -sm output format: "SIZE\tPATH"
    output = result.stdout.strip()
    size_str = output.split("\t")[0]

    return int(size_str)
# ---
def lazy_zarr_array(
    store: T_Store,
    shape: T_Shape,
    dtype: T_DType,
    chunks: T_RegularChunks,
    path: Optional[str] = None,
    **kwargs,
) -> LazyZarrArray:
    return LazyZarrArray(
        store,
        shape,
        dtype,
        chunks,
        path=path,
        **kwargs,
    )
# ---
def max_seqs(self) -> int:
        return self._max_seqs
# ---
def create(self, run_id) -> Checkpointer:
        keeps = [CheckpointInterval(**k) for k in self.keep]
        return Checkpointer(
            base_path=self.expanded_path(run_id),
            save_interval=self.save_interval,
            step_policies=keeps,
            delete_old_temp_checkpoints=self.delete_old_temp_checkpoints,
        )
# ---
def __call__(self, carry: hax.NamedArray) -> tuple[hax.NamedArray, hax.NamedArray]:
            updated = carry + self.weight
            return updated, updated
# ---
def status(self) -> TaskState:
        """Current task state (PENDING, RUNNING, SUCCEEDED, etc.)."""
        ...
# ---
def create_floatingip_precommit(self, context, fip_context):
        pass
# ---
def test(b):
    if not b:
        raise RuntimeError('test assertion failed')
# ---
def __repr__(self):
        return f"cubed.core.CoreArray<{self.name}, shape={self.shape}, dtype={self.dtype}, chunks={self.chunks}>"
# ---
def test_successor(self):
        try:
            self.test_patterns.create(Successor)
            raise Exception("Recursive structure did not explode.")
        except RuntimeError as re:
            assert str(re).startswith("maximum recursion depth")
# ---
def unregister_old(bl_id):
    global imported_mods
    mod = imported_mods.get(bl_id)
    if mod:
        #print("Unloaded old node type {}".format(bl_id)) 
        mod.unregister()
        del imported_mods[bl_id]
# ---
def test_deduplicate_empty(backend):
    """Test deduplication on empty dataset."""
    ds = Dataset.from_list([]).deduplicate(key=lambda x: x["id"])
    results = list(Backend.execute(ds, context=backend))
    assert results == []
# ---
def _remove_right_units(string: str) -> str:
    # "\\text{ " only ever occurs (at least in the val set) when describing units
    if "\\text{ " in string:
        splits = string.split("\\text{ ")
        assert len(splits) == 2
        return splits[0]
    else:
        return string
# ---
def test_index_1d_step(spec, shape, chunks, ind, new_chunks_expected):
    a = xp.arange(shape, chunks=chunks, spec=spec)
    b = a[ind]
    assert_array_equal(b.compute(), np.arange(shape)[ind])
    assert b.chunks == new_chunks_expected
# ---
def on_task_end(self, event):
        """Called when the a task ends.

        Parameters
        ----------
        event : TaskEndEvent
            Information about the task execution.
        """
        pass
# ---
def test_object_dtype():
    a = xp.asarray(["a", "b"], dtype=object, chunks=2)
    cubed.to_zarr(a, store=None)
# ---
def test_subtract(self):
        expr = col("a") - col("b")
        assert expr.evaluate({"a": 10, "b": 3}) == 7
# ---
def test_final_ellipsis():
    partial_order = ("apple", "banana", ...)
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("apple", "banana", "cherry")
# ---
def __repr__(self):
        return '<BBOXCoverage %r/%r>' % (self.extent.llbbox, self.bbox)
# ---
def __init__(self, dataset):
        self._dataset = dataset
# ---
def _create_thread(self, target=None, **kwargs):
        return IonProcessThread(target=target, heartbeat_secs=self.heartbeat_secs, **kwargs)
# ---
def autocomplete(self, word):
        for w in self._word_list:
            if w.startswith(word):
                return w, w[len(word):]
        return None, u''
# ---
def replace_char(str1,ch,newch):
 str2 = str1.replace(ch, newch)
 return str2
# ---
def __repr__(self) -> str:
        return f"col({self.name!r})"
# ---
def __init__(self, model: "BaseModel | DistributedDataParallel") -> None:
        super().__init__()
        self._underlying: BaseModel = getattr(model, "module", model)
# ---
def the_object_name_is_not_a_void(name):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(element.VoidsElements):
        assert False, "A void was found"
# ---
def email(self):
        """
        Shortcut property for finding the e-mail address or bot URL.
        """
        if "profile" in self._raw:
            email = self._raw["profile"].get("email")
        elif "bot_url" in self._raw:
            email = self._raw["bot_url"]
        else:
            email = None
        if not email:
            logging.debug("No email found for %s", self._raw.get("name"))
        return email
# ---
def serve_weights(self, weight_id: int, model) -> None:
        """Serve weights to clients.

        Args:
            weight_id: Unique identifier for this weight update
            model: Levanter model parameters (PyTree of NamedArrays)
        """
        pass
# ---
def _class_name(self):
        return "LicenseInfo"
# ---
def __init__(self, max_workers: int):
        """Initialize thread pool context.

        Args:
            max_workers: Maximum number of worker threads
        """
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self._actors: dict[str, Any] = {}  # name -> instance
        self._actor_locks: dict[str, threading.Lock] = {}  # per-actor locks
        self._actors_lock = threading.Lock()
# ---
def OnQuit(self, e):
        self.Close()
# ---
def area_weighted_gradient_magnitude_percent_diff(
    target: torch.Tensor, gen: torch.Tensor, area_weights: torch.Tensor
):
    area_weights = area_weights.to(target.device)
    return gradient_magnitude_percent_diff(target, gen, weights=area_weights)
# ---
def fray_default_job_ctx(ctx: JobContext):
    """Set the default job context for the duration of the context.

    Examples:
        >>> ctx = create_job_ctx("threadpool", max_workers=8)
        >>> with fray_default_job_ctx(ctx):
        ...     results = execute(ds)
    """
    old_ctx = _job_context.get()
    _job_context.set(ctx)
    try:
        yield ctx
    finally:
        _job_context.set(old_ctx)
# ---
def _strip_wrapped_partial(fun):
    if hasattr(fun, "__wrapped__"):  # ft.wraps
        return _strip_wrapped_partial(fun.__wrapped__)
    if isinstance(fun, ft.partial):
        return _strip_wrapped_partial(fun.func)
    return fun
# ---
def parse_text(self, m):
        text = m.group(0)
        self.tokens.append({'type': 'text', 'text': text})
# ---
def launch_job(self, request: cluster__pb2.Controller.LaunchJobRequest, ctx: RequestContext) -> cluster__pb2.Controller.LaunchJobResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def normalize_zdata(self,z_data,cal_z_data):
		return z_data/cal_z_data
# ---
def cancel_units (self, uids) :
        """
        Cancel given unit(s)
        """

        raise Exception ("%s.cancel_unit() is not implemented" % self.__class__.__name__)
# ---
def reader(self) -> "InMemoryRolloutReader":
        """Create a reader for this queue."""
        return InMemoryRolloutReader(self)
# ---
def first_Repeated_Char(str): 
    h = {}
    for ch in str:
        if ch in h: 
            return ch;
        else: 
            h[ch] = 0
    return '\0'
# ---
def calc_hydrophobicity(seq: str) -> float:
    s = (seq or "").strip().upper()
    if not s or "X" in s:
        return float("nan")
    base = calc_base_h(s)
    base = apply_length_weight(base, len(s))
    return round(overall_penalty(base), 4)
# ---
def python_grad_func(self):
    """Python gradient function callable."""
    return self._python_grad_func
# ---
def test_spawn_with_network_qos(self):
        self._create_instance()
        for vif_ref in xenapi_fake.get_all('VIF'):
            vif_rec = xenapi_fake.get_record('VIF', vif_ref)
            self.assertEquals(vif_rec['qos_algorithm_type'], 'ratelimit')
            self.assertEquals(vif_rec['qos_algorithm_params']['kbps'],
                              str(3 * 1024))
# ---
def test_take_overlapping_names():
    Height, Width, Depth = hax.make_axes(Height=20, Width=30, Depth=40)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    Height2 = Axis("Height", 10)
    indices_to_take = hax.arange(Height2, dtype=jnp.int32)
    named2 = hax.take(named1, Height, indices_to_take)

    assert named2.axes == (Height2, Width, Depth)
    assert named2.array.shape == (10, 30, 40)

    assert jnp.all(jnp.equal(named2.array, named1.array[:10]))
# ---
def __enter__(self):
        self.register()
        return self
# ---
def area(self):
        raise NotImplemented
# ---
def __enter__(self) -> "SerialCacheWriter":
        return self
# ---
def producer_with_exception():
        raise ValueError("Something went wrong!")
# ---
def atan(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in atan")
    return elemwise(nxp.atan, x, dtype=x.dtype)
# ---
def test_transaction_engine_ctx_begin_fails(self):
        engine = engines.testing_engine()

        mock_connection = Mock(
            return_value=Mock(
                        begin=Mock(side_effect=Exception("boom"))
                    )
        )
        engine._connection_cls = mock_connection
        assert_raises(
            Exception,
            engine.begin
        )

        eq_(
            mock_connection.return_value.close.mock_calls,
            [call()]
        )
# ---
def matrix_replacer(content: str) -> str:
        return f"[{content}]"
# ---
def zip_list(list1,list2):  
 result = list(map(list.__add__, list1, list2)) 
 return result
# ---
def slow_fn(x):
        time.sleep(0.2)
        return x * 2
# ---
def test_available_when_no_constraints(self, unbounded_config: config_pb2.ScaleGroupConfig):
        """Group is AVAILABLE when not in backoff, quota ok, and under capacity."""
        from iris.cluster.vm.scaling_group import GroupAvailability

        manager = make_mock_vm_manager()
        group = ScalingGroup(unbounded_config, manager)

        state = group.availability()
        assert state.status == GroupAvailability.AVAILABLE
# ---
def test_limit_complete(self):
        """A limit with item_capacity = 0 is 'complete'"""
        limit = Limit(item_limit=0)
        self.assertTrue(limit.complete)
# ---
def get_template(name: str) -> Template:
    try:
        return get_environment().get_template(name)
    except TemplateNotFound as e:
        raise TemplateNotFound(
            f"Unable to find {name} in dask.widgets.TEMPLATE_PATHS {TEMPLATE_PATHS}"
        ) from e
# ---
def compute(model, input):
            model_output = model(input, attn_mask=attn_mask)
            return model_output
# ---
def perimeter(diameter,height) : 
    return 2*(diameter+height)
# ---
def request_context():
    """Create a mock RequestContext for RPC calls."""
    return Mock(spec=RequestContext)
# ---
def KillTask(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def shutdown(self) -> None:
        """Terminate all local actors."""
        for job in self._jobs:
            job.terminate()
        for handle in self._handles:
            handle._executor.shutdown(wait=False)
# ---
def num_classes(self):
    raise NotImplementedError()
# ---
def filter_all(items):
        for _ in items:
            pass  # Consume but don't yield
        return iter([])
# ---
def _fn(a):
        return jnp.square(jnn.relu(a))
# ---
def is_odd(n) : 
    if (n^1 == n-1) :
        return True; 
    else :
        return False;
# ---
def iscomplex(a: A) -> A:
    return wrap_elemwise_unary(jnp.iscomplex, a)
# ---
def _jit_train_step_fn(self):
        return named_jit(
            self._train_step,
            axis_resources=self.parameter_axis_mapping,
            out_axis_resources=self.parameter_axis_mapping,
            donate_args=(True,),
        )
# ---
def get_task_status(self, task_name: JobName) -> cluster_pb2.TaskStatus:
        return self._remote_client.get_task_status(task_name)
# ---
def calc(A):
        A = A / max_abs
        aa = A * A
        aa_sum0 = jnp.sum(aa, axis=0)
        i = jnp.argmax(aa_sum0, 0)
        x = jax.lax.dynamic_index_in_dim(A, i, 1, keepdims=False)
        x = x @ A
        return max_abs * jnp.linalg.norm((x / jnp.linalg.norm(x)) @ A.T)
# ---
def find_rotation_count(A):
    (left, right) = (0, len(A) - 1)
    while left <= right:
        if A[left] <= A[right]:
            return left
        mid = (left + right) // 2
        next = (mid + 1) % len(A)
        prev = (mid - 1 + len(A)) % len(A)
        if A[mid] <= A[next] and A[mid] <= A[prev]:
            return mid
        elif A[mid] <= A[right]:
            right = mid - 1
        elif A[mid] >= A[left]:
            left = mid + 1
    return -1
# ---
def get_random_samples(self, num_samples):
        num_samples = min(num_samples, len(self.examplers))
        return random.sample(tuple(self.examplers), num_samples)
# ---
def fetch_task_logs(self, request: cluster__pb2.Worker.FetchTaskLogsRequest, ctx: RequestContext) -> cluster__pb2.Worker.FetchTaskLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
import re
def text_match(text):
  patterns = '^[a-z]+_[a-z]+$'
  if re.search(patterns,  text):
    return ('Found a match!')
  else:
    return ('Not matched!')
# ---
def eval_loss(model, *batch, **batch_kwargs):
                model = self.mp.cast_to_compute(model)
                return self.loss_fn(model, *batch, **batch_kwargs, key=None)
# ---
def test_encode_source_ascii(tok):
    source = "abc"
    ids = tok.encode_source(source)
    assert len(ids) == 3
    # Each should be in the base token range.
    for tid in ids:
        assert tid >= tok.base_token_offset
        assert tid < tok.base_token_offset + tok.base_vocab_size
# ---
def __len__(self):
        return len(self.indices)
# ---
def make_table(self):
        """Convenience method for making a table"""
        hash_key = DynamoKey("id")
        self.dynamo.create_table("foobar", hash_key=hash_key)
# ---
def __getattr__(self, method_name: str) -> "ActorMethod":
        """Get a callable method wrapper for the actor."""
        raise NotImplementedError
# ---
def test_unstack(tmp_path, spec, executor):
    a = cubed.random.random(
        (2, 10000, 10000), chunks=(2, 5000, 5000), spec=spec
    )  # 400MB chunks
    b, c = xp.unstack(a)
    run_operation(tmp_path, executor, "unstack", b, c)
# ---
def __sub__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.subtract(self, other)
# ---
def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    cudnn.benchmark = (
        True  # False # Set to True for better performance but lose reproducibility
    )
# ---
def reset_prefix_cache(self):
        return self.bridge.run(self.engine.reset_prefix_cache())
# ---
def __len__(self):
        return self.num_rows
# ---
def init_x_grad():
        x_grad_tile_ref[...] = jax.lax.dot_general(
            xw_scratch_ref[...],
            w_ref[...],
            (((1,), (1,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        x_write_future.start()
# ---
def QHeadSize(self) -> Axis:
        return Axis("q_head_dim", self.qk_rope_head_dim + self.qk_nope_head_dim)
# ---
def config(self):
        return self.backbone.config
# ---
def _():
        counts_ref[...] = jnp.zeros_like(counts_ref)
# ---
def test_swap_returns_previous_value():
    X = hax.Axis("x", 4)
    ref = hax.new_ref(hax.zeros(X))
    prev = hax.swap(ref, {"x": slice(1, 3)}, hax.ones(X.resize(2)))
    assert isinstance(prev, hax.NamedArray)
    assert prev.axes == (X.resize(2),)
    assert jnp.allclose(prev.array, 0.0)
    assert jnp.allclose(ref.value()[{"x": slice(1, 3)}].array, jnp.ones((2,), dtype=jnp.float32))
# ---
def inc(rc):
                    return rc.at["page", page].add(1)
# ---
def test_empty_set_against_integer(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.in_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def _block(s):
    """Block until sentinel is signalled. Pass a SentinelFile instance."""
    s.wait()
# ---
def loss_fn(lin):
            y = lin(x)
            loss = y * dy.astype(y.dtype)
            return hax.sum(loss).scalar()
# ---
def _compute_proc_stats(self, start_proc_time):
        cur_time = get_ion_ts_millis()
        self._record_proc_time(cur_time)
        proc_time = cur_time - start_proc_time
        self._proc_time += proc_time
# ---
def test_horizontal_retrace_toggles(self):
        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF0)
        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF1)
        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF0)
# ---
def max_seqs(self) -> int:
        """Number of sequences in the buffer."""
        return self.tokens.axis_size("seq")
# ---
def start(self) -> None:
        if self.cuda_snapshot_frequency is not None:
            torch.cuda.memory._record_memory_history()
# ---
def pool_id(self) -> str:
        return self._pool_id
# ---
def get_process_status_cache(self, process):
        if not self.process_status_cache.get(process.id):
            self.process_status_cache[process.id] = {
                'status': None,
                'id': process.vpnservice['id'],
                'updated_pending_status': False,
                'ipsec_site_connections': {}}
        return self.process_status_cache[process.id]
# ---
def rng():
    return random.Random(42)
# ---
def release(self, lease: Lease[Any]) -> None:
        with httpx.Client() as client:
            client.post(
                f"http://{self.host}:{self.port}/queues/{self.queue_name}/release",
                json={"lease_id": lease.lease_id, "timestamp": lease.timestamp},
            )
# ---
def destroy(self):
        """Remove all signal-connections."""
        self.autocomp.clear_words()
        self.autocomp.clear_widgets()
        self.main_controller.store_controller.disconnect(self._store_loaded_id)
        if getattr(self, '_cursor_changed_id', None):
            self.store_cursor.disconnect(self._cursor_changed_id)
        if self._unitview_id:
            self.main_controller.unit_controller.view.disconnect(self._unitview_id)
# ---
def __module_description(self):
        return self.description
# ---
def extract_data(pipeline_response):
            deserialized = self._deserialize("ManagedInstanceQueryStatistics", pipeline_response)
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)
            return deserialized.next_link or None, iter(list_of_elem)
# ---
def get_vpn_services_on_host(self, context, host):
        """Get list of vpnservices.

            The vpnservices including related ipsec_site_connection,
            ikepolicy and ipsecpolicy on this host
        """
        cctxt = self.client.prepare()
        return cctxt.call(context, 'get_vpn_services_on_host', host=host)
# ---
def process(self, message):
        """
        Process a message as arriving based on a subscription.
        """
        pass
# ---
def drag_and_drop(self, source, target):
        """Holds down the left mouse button on the source element,
           then moves to the target element and releases the mouse button.
        Args:
            source: The element to mouse down.
            target: The element to mouse up.
        """
        self.click_and_hold(source)
        self.release(target)
        return self
# ---
def set_rt(self, rt):
        assert(len(rt) == self.num_features)
        self.rt = rt
# ---
def monitor_loop(tpu_client: tpu_v2.TpuClient):
    """Main monitoring loop - runs indefinitely."""
    while True:
        try:
            for zone, count in config.TPU_ZONES_CONFIG.items():
                ensure_tpu_vms(tpu_client, zone, count)
        except Exception as e:
            logging.error(f"Error: {e}", exc_info=True)

        time.sleep(600)
# ---
def get_measurement(self, block_timestamp, parent_headerhash, parent_metadata: BlockMetadata):
        with self.lock:
            return self._state.get_measurement(block_timestamp, parent_headerhash, parent_metadata)
# ---
def get_max_occuring_char(str1):
  ASCII_SIZE = 256
  ctr = [0] * ASCII_SIZE
  max = -1
  ch = ''
  for i in str1:
    ctr[ord(i)]+=1;
  for i in str1:
    if max < ctr[ord(i)]:
      max = ctr[ord(i)]
      ch = i
  return ch
# ---
def publisher(self):
        del self._publisher
# ---
def __str__(self):
        return "%s.%s" % (self.model_name, self.name)
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"transformer": "model", "embeddings": None}
# ---
def test_create_actor_with_args(client: LocalClient):
    actor = client.create_actor(Counter, 42, name="counter")
    assert actor.get.remote().result() == 42
# ---
def test_load_mock_environment():
    """Test loading MockEnv via EnvConfig."""
    config = EnvConfig(env_class="marin.rl.environments.mock_env.MockEnv", env_args={"task_type": "cats", "seed": 42})

    env = load_environment_from_spec(config)

    assert isinstance(env, MockEnv)
    assert env.task_type == "cats"
    assert len(env.train_examples) > 0
    assert len(env.eval_examples) > 0
# ---
def profile_mean(ds: xr.Dataset) -> xr.Dataset:
    return ds.weighted(ds.areacello).mean(["y", "x"])
# ---
import math
def wind_chill(v,t):
 windchill = 13.12 + 0.6215*t -  11.37*math.pow(v, 0.16) + 0.3965*t*math.pow(v, 0.16)
 return int(round(windchill, 0))
# ---
def count_charac(str1):
 total = 0
 for i in str1:
    total = total + 1
 return total
# ---
def set_device(device: torch.device) -> None:
    global _CHOSEN_DEVICE
    _CHOSEN_DEVICE = device
# ---
def recursive_list_sum(data_list):
	total = 0
	for element in data_list:
		if type(element) == type([]):
			total = total + recursive_list_sum(element)
		else:
			total = total + element
	return total
# ---
def forward(
        self,
        x,  #: Float['... d']
    ):  # -> Float[' ... (d//2)']:
        x, gates = x.chunk(2, dim=-1)
        return F.silu(gates) * x
# ---
def output_double_emphasis(self, m):
        text = m.group(2) or m.group(1)
        text = self.output(text)
        return self.renderer.double_emphasis(text)
# ---
def _like_args(x, dtype=None, device=None, chunks=None, spec=None):
    if dtype is None:
        dtype = x.dtype
    if chunks is None:
        chunks = x.chunks
    if spec is None:
        spec = x.spec
    return dict(shape=x.shape, dtype=dtype, device=device, chunks=chunks, spec=spec)
# ---
def free(self):
        EnkfNode.cNamespace().free(self)
# ---
def matchfn(self, f):
        return bool(self.m1(f)) ^ bool(self.m2(f))
# ---
def setvalue(self, s):
		self.erase()
		self.write(s)
		return
# ---
def _attention(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    biases: List[torch.Tensor],
) -> torch.Tensor:
    # [*, H, C_hidden, K]
    key = permute_final_dims(key, (1, 0))

    # [*, H, Q, K]
    a = torch.matmul(query, key)

    for b in biases:
        a += b

    a = softmax_no_cast(a, -1)

    # [*, H, Q, C_hidden]
    a = torch.matmul(a, value)

    return a
# ---
def t_ID(self, t):
        r'\d+\.([uU]|[lL]|[uU][lL]|[lL][uU])?'
        t.value = int(t.value[:-1])
        return t
# ---
def setdata(self, request, response):
        self._calls.append(Call(request, response))
# ---
def get_latest_weight_id(self) -> int | None:
        """Get the latest weight ID."""
        with self._lock:
            return self._latest_weight_id
# ---
def metrics_loss_fn(model, batch, key=None):
    """Loss function returning (loss, metrics) tuple."""
    loss = hax.sum(batch * model.weight)
    metrics = {"accuracy": jnp.array(0.95), "perplexity": jnp.array(2.5)}
    return loss, metrics
# ---
def _all_done() -> bool:
            for rid, n_kids in expected_children.items():
                kid_map = self.results.get(rid, {})
                for cid in range(n_kids):
                    dr = kid_map.get(cid)
                    if dr is None or not dr.done:
                        return False
            return True
# ---
def find_closest_divisible_int_to_sqrt(n: int) -> int:
    """
    Find the closest integer to the square root of n (less than or equal to sqrt(n)) that divides n.
    """
    assert n > 0, f"Expected n > 0, got {n}"
    for i in range(int(n**0.5), 0, -1):
        if n % i == 0:
            return i

    return 1
# ---
def _cumulative_sum_func(a, /, *, axis=None, dtype=None, include_initial=False):
    out = nxp.cumulative_sum(a, axis=axis, dtype=dtype, include_initial=include_initial)
    if include_initial:
        # we don't yet support including the final element as it complicates chunk sizing
        ind = tuple(
            slice(a.shape[i]) if i == axis else slice(None) for i in range(a.ndim)
        )
        out = out[ind]
    return out
# ---
def get_lm_head(self) -> NamedArray:
        return self.lm_head
# ---
def remove_redundant_parens(text: str) -> str:
    """Remove redundant parentheses around simple numbers and expressions."""
    # Handle mixed numbers first: 11(2)/(3) -> 11+2/3
    text = re.sub(r"(\d+)\((\d+)\)/\((\d+)\)", r"\1+\2/\3", text)

    # Remove parens around simple fractions: (5)/(3) -> 5/3
    text = re.sub(r"\((\d+)\)/\((\d+)\)", r"\1/\2", text)

    # Remove parens around standalone numbers
    text = re.sub(r"(?<![a-zA-Z()])\((\d+(?:\.\d+)?)\)(?![a-zA-Z()])", r"\1", text)

    return text
# ---
def extent(self):
        from mapproxy.layer import MapExtent
        return MapExtent(self.bbox, self.srs)
# ---
def slice_count(self) -> int:
        """Total number of VM groups (regardless of state)."""
        with self._vm_groups_lock:
            return len(self._vm_groups)
# ---
def __init__(
        self,
        device: torch.device,
        compute_metric: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
    ):
        self._compute_metric = compute_metric
        self._total: torch.Tensor | None = None
        self._device = device
# ---
def _fix_fracs(string):
    """Convert \\frac{a}{b} to (a)/(b), handling all variants. (Legacy - use process_latex_fractions)"""
    # Use new abstraction
    return process_latex_fractions(string)
# ---
def axis_size(self, axis: AxisSelector) -> int:  # type: ignore
        ...
# ---
def __init__(self, cfg=None, service_url=None):
        self.cfg = cfg or Config()
        self.service_url = service_url or self.cfg.get('service', 'url')
        user = self.cfg.get('service', 'user', default=None)
        password = self.cfg.get('service', 'password', default=None)
        self.session = requests.Session()
        if user and password:
            self.session.auth = (user, password)
# ---
def testFormatTag(self):
    """Tests the _FormatTag function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    tag_string = test_helper._FormatTag(None)
    self.assertEqual(tag_string, '-')

    event_tag = events.EventTag()
    event_tag.AddLabel('one')
    event_tag.AddLabel('two')

    tag_string = test_helper._FormatTag(event_tag)
    self.assertEqual(tag_string, 'one two')
# ---
def indexes(self):
        """OrderedDict of pandas.Index objects used for label based indexing
        """
        return Indexes(self)
# ---
def reverse_list_lists(lists):
    for l in lists:
        l.sort(reverse = True)
    return lists
# ---
def _mesh_axis_size(axis_name) -> int:
        if mesh is None:
            return 1
        return mesh.shape[axis_name]
# ---
def accept(self):
        sock, addr = self._sock.accept()
        client = OpenSSL.SSL.Connection(sock._context, sock)
        return client, addr
# ---
def model_capture_hook(info):
                # Make a copy of the model on the CPU.
                self.trained_model = jax.device_get(info.state.model)
# ---
def shard_names(self) -> Sequence[str]:
        return list(self._shard_name_to_url_mapping.keys())
# ---
def pages_per_seq(self) -> int:
        return self._pages_per_seq
# ---
def around(a: A) -> A:
    return wrap_elemwise_unary(jnp.around, a)
# ---
def record(self, target: torch.Tensor, gen: torch.Tensor):
        """Add a batch of data to the metric.

        Args:
            target: Target data. Should have shape [batch, time, height, width].
            gen: Generated data. Should have shape [batch, time, height, width].
        """
        new_value = self._compute_metric(target, gen).mean(dim=0)
        if self._total is None:
            self._total = torch.zeros_like(new_value, device=self._device)
        self._total += new_value
# ---
def is_fp16_enabled():
    # Autocast world
    fp16_enabled = torch.get_autocast_gpu_dtype() == torch.float16
    fp16_enabled = fp16_enabled and torch.is_autocast_enabled()

    return fp16_enabled
# ---
def validate_item_type(self):
		if self.has_serial_no == 1 and self.is_stock_item == 0 and not self.is_fixed_asset:
			msgprint(_("'Has Serial No' can not be 'Yes' for non-stock item"), raise_exception=1)

		if self.has_serial_no == 0 and self.serial_no_series:
			self.serial_no_series = None
# ---
def test_multiple_int_array_indexes(spec):
    a = xp.asarray(
        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]],
        chunks=(2, 2),
        spec=spec,
    )
    with pytest.raises(NotImplementedError):
        a[[1, 2, 1], [2, 1, 0]]
# ---
def load_state_dict(self, state_dict):
        self.__dict__.update(state_dict)
# ---
def broadcast_axis(a: NamedArray, axis: AxisSpec) -> NamedArray:
    """
    Broadcasts `a`, ensuring that it has all the axes in `axis`.
     `broadcast_axis` is an alias for `broadcast_to(a, axis, enforce_no_extra_axes=False, ensure_order=True)`

     You typically use this function when you want to broadcast an array to a common set of axes.
    """
    if isinstance(axis, Axis) and axis in a.axes:
        return a

    return broadcast_to(a, axis, enforce_no_extra_axes=False, ensure_order=True)
# ---
def __repr__(self):
        return f"cubed.Array<{self.name}, shape={self.shape}, dtype={self.dtype}, chunks={self.chunks}>"
# ---
def _iter_speedrun_results(paths: list[Path]) -> Iterable[Path]:
    for path in paths:
        if path.is_dir():
            yield from path.rglob("speedrun_results.json")
        elif path.name == "speedrun_results.json":
            yield path
# ---
def f(c, x, y):
        return c, x + y
# ---
def tuple_modulo(test_tup1, test_tup2):
  res = tuple(ele1 % ele2 for ele1, ele2 in zip(test_tup1, test_tup2)) 
  return (res)
# ---
def hostloc(self):
        '''return host:port'''
        hostloc = self.hostname
        if self.port:
            hostloc = '{}:{}'.format(hostloc, self.port)

        return hostloc
# ---
def test_capturing_reset_simple(self):
        with self.getcapture() as cap:
            print("hello world")
            sys.stderr.write("hello error\n")
            out, err = cap.readouterr()
        assert out == "hello world\n"
        assert err == "hello error\n"
# ---
def do_init(self, match):
        self.lc.publish('Match/Init', match.encode())
# ---
def _recv(self, addr, bindata, frequency, bandwidth):
		if self.frequency == frequency and self.bandwidth == bandwidth and self.addr == addr:
			self.q.put(bindata)
# ---
def rgb_to_hsv(r, g, b):
    r, g, b = r/255.0, g/255.0, b/255.0
    mx = max(r, g, b)
    mn = min(r, g, b)
    df = mx-mn
    if mx == mn:
        h = 0
    elif mx == r:
        h = (60 * ((g-b)/df) + 360) % 360
    elif mx == g:
        h = (60 * ((b-r)/df) + 120) % 360
    elif mx == b:
        h = (60 * ((r-g)/df) + 240) % 360
    if mx == 0:
        s = 0
    else:
        s = (df/mx)*100
    v = mx*100
    return h, s, v
# ---
def __init__(self):
        self.group = ast.Group()
        self.is_alt = False
# ---
def test_add_already_added(self):
        prio_set_list = event._PrioritizedSetList()
        obj = object()
        prio_set_list.add(0, obj)

        with pytest.raises(ValueError) as excinfo:
            prio_set_list.add(0, obj)
        excinfo.match(r"has already been added")

        with pytest.raises(ValueError) as excinfo:
            prio_set_list.add(1, obj)
        excinfo.match(r"has already been added")
# ---
def __or__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_or(self, other)
# ---
def __call__(
        self, pred: torch.Tensor, target: torch.Tensor, wet: PrognosticMask
    ) -> torch.Tensor: ...
# ---
def __repr__(self):
        return_str = "<MidiTrack %d -- %d events\n" % (self.index, len(self.events))
        for event in self.events:
            return_str = return_str + "    " + event.__repr__() + "\n"
        return return_str + "  >"
# ---
def wait_for_interrupt(self):
        """Wait for Ctrl+C, keeping the cluster running."""
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            pass
# ---
def kernel_triangular_attn(q, k, v, tri_bias, mask, scale):
    from cuequivariance_torch.primitives.triangle import triangle_attention
    return triangle_attention(q, k, v, tri_bias, mask=mask, scale=scale)
# ---
def min_val(listval):
     min_val = min(i for i in listval if isinstance(i, int))
     return min_val
# ---
def bar(x: i32["batch"]):  # type: ignore  # noqa: F722
        pass
# ---
def mpra(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transduction']
    }
# ---
def _add_tiny(x):
    return x + jnp.finfo(x.dtype).tiny
# ---
def get_decoder(self):
        """Get decoder (backbone) for the model."""
        return self.bimamba
# ---
def template_flash(stitches, do_outies):
    return "stitches: %s, outies: %s" % (stitches, do_outies), ["--flash", str(stitches), "--allow_outies", str(do_outies)]
# ---
def col_clause(self):
        search = (self.pattern
                  .replace('\\', '\\\\')
                  .replace('%', '\\%')
                  .replace('_', '\\_'))
        clause = self.field + " like ? escape '\\'"
        subvals = [search]
        return clause, subvals
# ---
def __call__(self, carry: Carry, *args: Args.args, **kwargs: Args.kwargs) -> tuple[Carry, Y]: ...
# ---
def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):
        sema = threading.Semaphore(value=0)
        def observer(val):
            if cond(val):
                sema.release()
        self.observe_property(name, observer)
        if not level_sensitive or not cond(getattr(self, name.replace('-', '_'))):
            sema.acquire()
        self.unobserve_property(name, observer)
# ---
def strikethrough(self, text):
        """Rendering ~~strikethrough~~ text.

        :param text: text content for strikethrough.
        """
        return '<del>%s</del>' % text
# ---
def min(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.min, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def clear_caches():
    """Clears internal Equinox caches.

    Best used before calling `jax.clear_caches()` or `jax.clear_backends()`.

    **Arguments:**

    None.

    **Returns:**

    None.
    """
    for cache in internal_caches:
        cache.clear()
    for cache in internal_lru_caches:
        cache.cache_clear()
# ---
def argmax(x, /, *, axis=None, keepdims=False, split_every=None):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in argmax")
    if axis is None:
        x = reshape(x, (-1,))
        axis = 0
        keepdims = False
    return arg_reduction(
        x,
        nxp.argmax,
        axis=axis,
        keepdims=keepdims,
        split_every=split_every,
    )
# ---
def __len__(self):
        return len(self._dataset._variables) - len(self._dataset._coord_names)
# ---
def getresponse(self):
        return MockHTTPResponse(MockFile(), {}, 200, "OK")
# ---
def as_dict(self):
        return { 'prefix': self.prefix.decode('utf-8'),
                 'level':  self.level.decode('utf-8'),
                 'text':   self.text.decode('utf-8').rstrip() }
# ---
def the_object_name_should_display_as_mode(name, mode):
    assert the_object_name_exists(name).display_type == mode
# ---
def parabola_focus(a, b, c): 
  focus= (((-b / (2 * a)),(((4 * a * c) - (b * b) + 1) / (4 * a))))
  return focus
# ---
def sum_difference(n):
    sumofsquares = 0
    squareofsum = 0
    for num in range(1, n+1):
        sumofsquares += num * num
        squareofsum += num
    squareofsum = squareofsum ** 2
    return squareofsum - sumofsquares
# ---
def normalised_allowed_extensions(self) -> set[str]:
        return _normalise_allowed_extensions(self.allowed_extensions, casefold=self.casefold)
# ---
def init(HeadDim, config):
        return YarnRotaryEmbeddings(HeadDim, config)
# ---
def doMonteCarloNP(pointa, pointb, weights, nopoint):
    #print "weights ", weight
    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    #print dataset_Monte
    return getMisMatches(dataset_Monte, weights)
# ---
def _flip_num_input_blocks(axis, shape, chunksizes):
    num = 1
    for ax in axis:
        if shape[ax] % chunksizes[ax] != 0:
            num *= 2
    return num
# ---
def scenario_debug(function):
    def subfunction(self):
        run_debug(function(self))

    return subfunction
# ---
def set_item_default(item_code, company, fieldname, value):
	item = frappe.get_cached_doc('Item', item_code)

	for d in item.item_defaults:
		if d.company == company:
			if not d.get(fieldname):
				frappe.db.set_value(d.doctype, d.name, fieldname, value)
			return

	# no row found, add a new row for the company
	d = item.append('item_defaults', {fieldname: value, "company": company})
	d.db_insert()
	item.clear_cache()
# ---
def subsample_fastqs(path_fastqs, num_files=10, num_sequences=1000):
    for i, path_fastq in enumerate(path_fastqs):
        if i >= num_files:
            return
        with open(path_fastq) as fastq_inf:
            fastq_gen = read_fastq(fastq_inf)
            yield limit_fastq(fastq_gen, num_sequences=num_sequences)
# ---
def __module_api_version(self):
        return self.api_version
# ---
def find_max(test_list):
  res = max(int(j) for i in test_list for j in i)
  return (res)
# ---
def accept_all(self, include_rejected=False):
        '''
        Accept all keys

        :param bool include_rejected: Whether or not to accept a matched key that was formerly rejected
        '''
        self.accept('*', include_rejected=include_rejected)
# ---
def linCongr(a, b, m):
        solutions = set()
        if (b % gcd(a, m) == 0):
                numSols = gcd(a, m)
                sol = (b * egcd(a, m)[0] / numSols) % m
                for i in xrange(0, numSols):
                        solutions.add((sol + m * i / numSols) % m)
        return solutions
# ---
def test_dispatch_intermittent_failure(cluster):
    """Test intermittent heartbeat failure during dispatch (30%). Task assignments are
    buffered and retried on next heartbeat cycle. Task should eventually succeed.
    """
    _url, client = cluster
    enable_chaos("controller.heartbeat", failure_rate=0.3)
    job = submit(client, _quick, "intermittent-dispatch")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def __init__(self, config: LocalClusterConfig = LocalClusterConfig()):
        """Initialize local cluster."""
        self._jobs: dict[JobId, _LocalJob] = {}
        self.config = config
# ---
def unobserve_property(self, name, handler):
        handlers = self._property_handlers[name]
        handlers.remove(handler)
        if not handlers:
            _mpv_unobserve_property(self._event_handle, hash(name)&0xffffffffffffffff)
# ---
def __len__(self):
        return len(self.subqueries)
# ---
def test_apply_bracket_constraints(tokenizer):
    logits = jnp.ones(tokenizer.vocab_size)
    constrained = apply_bracket_constraints(logits, "f(x", tokenizer)

    # ) should be allowed.
    assert float(constrained[tokenizer.encode_char(")")]) == 1.0
    # ] should be masked to -inf.
    assert float(constrained[tokenizer.encode_char("]")]) < -1e8
# ---
def test_wait_all_empty():
    assert wait_all([]) == []
# ---
def vocab_size(self) -> int:
        return self.core.vocab_size
# ---
def log(self, metrics: Mapping[str, Any], *, step: int | None = None):
        self._run.log(metrics, step=step)
# ---
def _round_to_nearest_multiple(x: int, multiple: int) -> int:
    return ((x + multiple - 1) // multiple) * multiple
# ---
def num_new_tokens(self) -> jnp.ndarray:
        return self.cu_q_lens["seq", self.num_seqs].scalar()
# ---
def connect_ftp(self, user, passwd, host, port, dirs,
                            timeout=socket._GLOBAL_DEFAULT_TIMEOUT):
                self.user, self.passwd = user, passwd
                self.host, self.port = host, port
                self.dirs = dirs
                self.ftpwrapper = MockFTPWrapper(self.data)
                return self.ftpwrapper
# ---
def __call__(self, x, key=None):
        if key is not None:
            k1, k2 = jax.random.split(key)
            return self.lora(x, key=k2) + self.wrapped(x, key=k1)
        else:

            return self.lora(x) + self.wrapped(x)
# ---
def __init__(self):
        self.message = "Downloading.."
        self.total_bytes = 0
        self.current_bytes = 0
        self.completed = False
        self.error_occured = False
        self.start_time = 0
        self.file_name = ""
        self.kbps = 0
        self.assembly = False
        self.assembly_percent = 0
# ---
def tril(array: NamedArray, axis1: Axis, axis2: Axis, k=0) -> NamedArray:
    """Compute the lower triangular part of an array along two named axes."""
    array = array.rearrange((..., axis1, axis2))

    inner = jnp.tril(array.array, k=k)
    return NamedArray(inner, array.axes)
# ---
def post(self):
            pass
# ---
def get_readme():
    dirname = os.path.dirname(os.path.abspath(__file__))
    with open(os.path.join(dirname, "README.md"), "r") as fp:
        long_description = fp.read()
    return long_description
# ---
def test_blake3_vector():
    # Catch un-intentional regressions
    assert bytes(hash_blake3(b"hello")).hex() == "ea8f163db38682925e4491c5e58d4bb3506ef8c14eb78a86e908c5624a67200f"
# ---
def scan(
    f: Callable,
    axis: AxisSelector,
    *,
    remat: ScanCheckpointSpec = False,
    reverse: bool = False,
    unroll: int = 1,
    is_scanned: BoolAxisSpec = is_named_or_shaped_array_like,
) -> Callable: ...
# ---
def state(self) -> ControllerState:
        return self._state
# ---
def __init__(self, name, zarray, spec, plan):
        super().__init__(name, zarray, spec, plan)
# ---
def get_iqawg(self):
        self._iqawg.set_parameters(
            {'calibration': self._current_cal})  # ensure
        return self._iqawg
# ---
def test_failure(self, apply_failing_clock_call, errno_value, strerror):
        """
        A failure in C{clock_getres} results in an L{OSError} that
        presents the failure's errno.
        """
        calls = apply_failing_clock_call('_clock_getres', errno_value)

        with pytest.raises(OSError) as exc:
            get_clock_info("monotonic")

        assert len(calls) == 1
        assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC

        assert str(exc.value) == strerror
# ---
def __init__(self, main_model):
        self.main_view = None
        self.main_model = main_model

        self.main_model.begin_job_fetch.connect(self.on_begin_job_fetch)
        self.main_model.update_job_fetch_progress.connect(self.on_job_fetch_update)
        self.main_model.fetched_job.connect(self.on_fetched_job)
# ---
def destroy_process(self, process_id):
        """Destroy process.

        Disable the process and remove the process
        manager for the processes that no longer are running vpn service.
        """
        if process_id in self.processes:
            process = self.processes[process_id]
            process.disable()
            if process_id in self.processes:
                del self.processes[process_id]
# ---
def list_endpoints(self, prefix: str) -> list[cluster_pb2.Controller.Endpoint]:
        return self._remote_client.list_endpoints(prefix)
# ---
def isplayfile(pathname) :
    if os.path.isfile(pathname) == False:
        return False
    ext = os.path.splitext(pathname)[1]
    ext = ext.lower()
    if (ext == '.mp2') : return True;
    if (ext == '.mp3') : return True;
    if (ext == '.ogg') : return True;
    return False
# ---
def always(self):
        return self._always
# ---
def test_get_lines_with_long_string():
        assert len(get_lines("a"*(2*LINEWIDTH-1))) == 2
# ---
def setup_env(ctx):
    """Set up the remote environment on the development TPU."""
    tpu_name = ctx.obj.tpu_name
    host_alias = f"dev-tpu-{tpu_name}"
    setup_remote_environment(host_alias)
# ---
def _parse_fastavro(self):
        """Convert parsed Avro schema to fastavro format."""
        self._parse_avro_schema()
        self._fastavro_schema = fastavro.parse_schema(self._avro_schema_json)
# ---
def update_po(srcdir, project_id, langs):
    potfile = os.path.join(srcdir, project_id + '.pot')
    for lang in langs:
        pofile = os.path.join(srcdir, lang, lang + '.po')
        subprocess.call(['msgmerge', '-q', '-o', pofile, pofile, potfile])
# ---
def test_add_data(self):
        self.assertFalse(self.get.has_data())
        self.assertEqual("GET", self.get.get_method())
        self.get.add_data("spam")
        self.assertTrue(self.get.has_data())
        self.assertEqual("POST", self.get.get_method())
# ---
def test_technical_contacts(self):
        eq_(self.record.technical_contacts.__class__.__name__, 'list')
        eq_(self.record.technical_contacts, [])
# ---
def test_create_experiment_view_unauthorized(self):
        """ Tests edit_experiment template does not render for url 'create_experiment'
            when unauthorized """
        self.set_roles([])
        response = self.client.get(reverse("ab_testing_tool_create_experiment"), follow=True)
        self.assertTemplateNotUsed(response, "ab_tool/create_experiment.html")
        self.assertTemplateUsed(response, "ab_tool/not_authorized.html")
# ---
def __init__(self, proto: config_pb2.IrisClusterConfig):
        """Create IrisConfig from proto.

        Args:
            proto: Cluster configuration proto (defaults will be applied)
        """
        self._proto = apply_defaults(proto)
# ---
def get_preference_adapter(source: str) -> PreferenceTransformAdapter | None:
    return preference_transform_templates.get(source)
# ---
def can_retry_preemption(self) -> bool:
        return self.preemption_count < self.max_retries_preemption
# ---
def _reshape(x):
        if is_jax_array_like(x) and x.shape != ():
            return x.reshape([outer_size, inner_size, *x.shape[1:]])
        else:
            return x
# ---
def __array_namespace__(self, /, *, api_version=None):
        if api_version is not None and api_version not in (
            "2021.12",
            "2022.12",
            "2023.12",
        ):
            raise ValueError(f"Unrecognized array API version: {api_version!r}")
        import cubed

        return cubed
# ---
def addgen(id, genre) :
    db = cherrypy.session['database']

    sql = "UPDATE Radio SET genre='%s' WHERE id = %s" % (genre, id)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def terminate_job(self, request: cluster__pb2.Controller.TerminateJobRequest, ctx: RequestContext) -> cluster__pb2.Empty:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def render_execdepends(thing):
    rendered = []
    for item in thing:
        dep = copy.copy(item)
        dep.setdefault('package_manager', 'apt')
        dep['version'] = ' = '+dep['version'] if 'version' in dep else ''
        rendered.append("{package_manager}: {name}{version}".format(**dep))
    return rendered
# ---
def add_pilot (self, pid) :
        """
        add (Compute or Data)-Pilot(s) to the pool
        """

        raise Exception ("%s.add_pilot() is not implemented" % self.__class__.__name__)
# ---
def traslado_ieps(self):
        return self.__trieps
# ---
def logs_url(self):
        return self.admin_url + '/logs'
# ---
import math
def area_tetrahedron(side):
  area = math.sqrt(3)*(side*side)
  return area
# ---
def __enter__(self):
        # TODO(jder): Could make this thread/async-safe if needed.
        self.previous_scope = Multiton._current_scope
        Multiton._current_scope = self.scope
        return self
# ---
def load_variable_data(var: xr.Variable) -> None:
        var.load()
# ---
def maybe_cast_to_bf16(arr):
            if jnp.issubdtype(arr.dtype, jnp.floating):
                return arr.astype(jnp.bfloat16)
            return arr
# ---
def _add_text_box(self, textbox):
        """Add the given L{TextBox} to the list of widgets to do auto-
            correction on."""
        if not hasattr(self, '_textbox_insert_ids'):
            self._textbox_insert_ids = {}
        handler_id = textbox.connect('text-inserted', self._on_insert_text)
        self._textbox_insert_ids[textbox] = handler_id
        self.widgets.add(textbox)
# ---
def save(self):

        if self.instance.pk:
            return super(FarmworkForm, self).save()

        instance = super(FarmworkForm, self).save(commit=False)
        instance.slug = slugify(instance.get_job_fruit_display() + '-' + instance.get_job_role_display() + '-in-' + instance.loc_city)
        instance.save()

        return instance
# ---
def _alias_names(artifact: "wandb.sdk.Artifact") -> set[str]:  # type: ignore[name-defined]
    names: set[str] = set()
    for alias in artifact.aliases or []:
        name = getattr(alias, "name", alias)
        if name is not None:
            names.add(str(name))
    return names
# ---
def test_quantile_parallel_int(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.int32)})
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def send_acknowledgement_notification(acknowledgement):
    subject = _("%s has acknowledged you on Villages.cc") % (
        acknowledgement.payer)
    send_notification(subject, acknowledgement.payer, acknowledgement.recipient,
                      'acknowledgement_notification_email.txt',
                      {'acknowledgement': acknowledgement})
# ---
def __init__(self, manager, task_id, sub_ids=None, dry_run=False, resubmit=False):
		self.__manager = manager
		self.__task	= self.__manager.load_task(task_id)
		self.__sub_ids = sub_ids
		self.__dry_run = dry_run
		self.__resubmit = resubmit

		self.__logger = logging.getLogger('JSUB')
		if self.__sub_ids==None:
			self.__sub_ids=range(len(self.__task.data['jobvar']))

		self.__initialize_manager()
# ---
def _resolve_save_reference_code(self, save_reference_code: Optional[bool]) -> bool:
        """Determine whether reference code should be bundled with the checkpoint."""
        #  the way we determine this is if the config class is in the HF package or not
        if save_reference_code is None:
            return not self.HfConfigClass.__module__.startswith("transformers.")

        return save_reference_code
# ---
def _new_private_method(self, m):
        return 2 * m
# ---
def _compute_full(Vocab, pred_embeddings, pred_lm_head, true_ids):
    logits_full = hax.dot(pred_embeddings, pred_lm_head, axis="embed")
    target_y_full = hax.nn.one_hot(true_ids, Vocab, dtype=pred_embeddings.dtype)
    loss_full, sumexp_full = cross_entropy_loss_and_log_normalizers(logits_full, Vocab, target_y_full)
    return loss_full, sumexp_full
# ---
def relevant_keys(mapping):
            return [k for k, v in mapping.items()
                    if any(d in indexer_dims for d in v.dims)]
# ---
def _get_account_analytic_invoice(self, cr, uid, picking, move_line):
        return False
# ---
def transfection_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'nucleic_acid_delivery_method': ['stable transfection']
    }
# ---
def clear_sample_outputs(self):
        """
        Clear all stored sample outputs.

        Removes all previously collected sample outputs from memory.
        """
        self.sample_outputs.clear()
# ---
def list_images(self, pattern: str) -> list[ImageInfo]: ...
# ---
def cleanup_log(path: str):
    os.unlink(path)
# ---
def get_job_status(self, job_id: JobName) -> cluster_pb2.JobStatus:
        return self._remote_client.get_job_status(job_id)
# ---
def get_default_config_path(region: str) -> str:
    """Get default config path for a region."""
    return f"infra/marin-{region}.yaml"
# ---
def get_ova_info(ova_path):
    ns = {'ovf': _OVF_NS, 'rasd': _RASD_NS}

    try:
        root = ET.fromstring(_read_ovf_from_ova(ova_path))
    except ET.ParseError as e:
        raise V2VError('Error reading ovf from ova, position: %r' % e.position)

    vm = {}
    _add_general_ovf_info(vm, root, ns, ova_path)
    _add_disks_ovf_info(vm, root, ns)
    _add_networks_ovf_info(vm, root, ns)

    return response.success(vmList=vm)
# ---
def __len__(self):
        return len(self.datasets)
# ---
def test_tril(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.tril(a)
    run_operation(tmp_path, executor, "tril", b)
# ---
def vocab_size(self):
    """Number of pixel values."""
    return 256
# ---
def _visit(self, element):
        if isinstance(element, NavigableString):
            text = element.strip()
            return TextNode(text) if text else TextNode("")

        if not element.name:
            return TextNode("")

        method_name = f"visit_{element.name}"
        if hasattr(self, method_name):
            return getattr(self, method_name)(element)

        # Default recursive visit
        return self._visit_children(element)
# ---
def get_remote_url(git_path, module, dest, remote):
    '''Return URL of remote source for repo.'''
    command = [git_path, 'ls-remote', '--get-url', remote]
    (rc, out, err) = module.run_command(command, cwd=dest)
    if rc != 0:
        # There was an issue getting remote URL, most likely
        # command is not available in this version of Git.
        return None
    return to_native(out).rstrip('\n')
# ---
def maybe_rng_split(key: PRNGKeyArray | None, num: int = 2):
    """Splits a random key into multiple random keys. If the key is None, then it replicates the None. Also handles
    num == 1 case"""
    if key is None:
        return [None] * num
    elif num == 1:
        return jnp.reshape(key, (1,) + key.shape)
    else:
        return jrandom.split(key, num)
# ---
def fake_generate_swap(cls, *args, **kwargs):
            self.called = True
# ---
def test_window(backend):
    """Test window operation (same as batch)."""
    ds = Dataset.from_list([[1, 2, 3, 4, 5]]).flat_map(lambda x: x).window(2)
    windows = list(Backend.execute(ds, context=backend))
    assert windows == [[1, 2], [3, 4], [5]]
# ---
def __init__(
        self,
        config: BiMambaConfig,
        input_dim=None,
        device=None,
        dtype=None,
    ):
        super().__init__()
        factory_kwargs = {'device': device, 'dtype': dtype}
        if input_dim is None:
            input_dim = config.vocab_size
        self.word_embeddings = nn.Embedding(
            input_dim, config.d_model, **factory_kwargs
        )
# ---
def write(self, buf, flags=0):
        return self.sendall(buf, flags)
# ---
def get_platforms(self):
        return [SaagiePlatform(self, platform_data)
                for platform_data in requests.get(PLATFORMS_URL, auth=SAAGIE_BASIC_AUTH_TOKEN).json()]
# ---
def parameterize_with_configs(pattern, config_path=None):
    test_path = os.path.dirname(os.path.abspath(__file__))
    if config_path is None:
        config_path = os.path.join(test_path, "..", "config")

    configs = glob.glob(os.path.join(config_path, pattern))
    return pytest.mark.parametrize("config_file", configs, ids=lambda x: f"{os.path.basename(x)}")
# ---
def test_impl(df):
            C = df.A.str.split(',')
            return C[df.B == 'aa']
# ---
def last(arr,x,n):
    low = 0
    high = n - 1
    res = -1  
    while (low <= high):
        mid = (low + high) // 2 
        if arr[mid] > x:
            high = mid - 1
        elif arr[mid] < x:
            low = mid + 1
        else:
            res = mid
            low = mid + 1
    return res
# ---
def _workflow_signal(self, cr, uid, ids, signal, context=None):
        #override in order to fire the workflow signal on given stock.picking workflow instance
        #instead of it's own workflow (which is not existing)
        return self.pool.get('stock.picking')._workflow_signal(cr, uid, ids, signal, context=context)
# ---
def __init__(self, queue: InMemoryRolloutQueue):
        self._queue = queue
# ---
def __truediv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.true_divide(self, other)
# ---
def dict_from_parsed_args(parsed_args, attrs):
    d = {}
    for attr in attrs:
        value = getattr(parsed_args, attr)
        if value is not None:
            d[attr] = value
    return d
# ---
def test_deadline_timeout_integration():
    """Deadline can be used to enforce timeouts in operations."""
    deadline = Deadline.from_seconds(0.2)  # 200ms timeout
    iterations = 0

    while not deadline.expired():
        iterations += 1
        time.sleep(0.05)  # 50ms per iteration

    # Should have run ~4 iterations (200ms / 50ms)
    assert 3 <= iterations <= 5
# ---
def _soft_sample(self, logits):
    return torch.nn.functional.softmax(
      logits / self.temperature, dim=-1)
# ---
def handle_schedule(self, channel, data):
        msg = Forseti.Schedule.decode(data)
        for i in range(msg.num_matches):
            self.match_list_box.Insert(format_match(msg.matches[i]), i,
                    msg.matches[i])
# ---
def read_all_jsonl_gz():
    """Fixture to read all JSONL gzipped files from a directory."""

    def _read_all(directory: Path, pattern: str = "*.jsonl.gz") -> list[dict]:
        records = []
        for file_path in sorted(directory.glob(pattern)):
            with gzip.open(file_path, "rt", encoding="utf-8") as handle:
                for line in handle:
                    if line.strip():
                        records.append(json.loads(line))
        return records

    return _read_all
# ---
def test_classify_textfiles_to_db_no_connection(mock_db, mock_jw):
    mock_db.connected_to_db.return_value = False
    classify_documents.classify_textfiles_to_db(0, None)
    assert not mock_jw.called
# ---
def draft_force_assign(self, cr, uid, ids, *args):
        """ Confirms picking directly from draft state.
        @return: True
        """
        wf_service = netsvc.LocalService("workflow")
        for pick in self.browse(cr, uid, ids):
            if not pick.move_lines:
                raise osv.except_osv(_('Error!'),_('You cannot process picking without stock moves.'))
            wf_service.trg_validate(uid, 'stock.picking', pick.id,
                'button_confirm', cr)
        return True
# ---
def __init__(self, pool_size=2, strides=None, padding='valid'):
        super(LW_AveragePooling1D, self).__init__(pool_size, strides, padding)
# ---
def fail(returncode, e):
    sys.stderr.write("ERROR: %s\n" % e)
    sys.exit(returncode)
# ---
def __init__(self, ser: serial.Serial, parent=None):
        super().__init__(parent)
        self.serial = ser
        self._quit = False
# ---
def get_initial_input(self):
        data = self.__getitem__(0)[0]
        return data
# ---
def versioned(value: T_co) -> VersionedValue[T_co]:
    if isinstance(value, VersionedValue):
        raise ValueError("Can't nest VersionedValue")
    elif isinstance(value, InputName):
        # TODO: We have also run into Versioned([InputName(...), ...])
        raise ValueError("Can't version an InputName")

    return VersionedValue(value)
# ---
def __getitem__(self, key):
        return self.subqueries[key]
# ---
def read_all_available(self) -> list[RolloutBatch]:
        """Read all currently available batches without blocking.

        Returns:
            List of all available batches (may be empty).
        """
        pass
# ---
def devices(self):
        return nxp.__array_namespace_info__().devices()
# ---
def test_visualize_shardings_model_axis(capsys):
    devices = jax.devices()
    mesh = jax.sharding.Mesh(np.array(devices).reshape(-1, 2), (ResourceAxis.DATA, ResourceAxis.MODEL))
    with axis_mapping({"dim1": ResourceAxis.DATA, "dim2": ResourceAxis.MODEL}), mesh:
        arr = hax.ones((Dim1, Dim2))
        visualize_shardings(arr)

    out = capsys.readouterr().out
    assert "dim2" in out
# ---
def __init__(self, data: list[T], is_complete: bool = False):
        super().__init__()
        self.data = data
        self.is_complete = is_complete
        if not is_complete:
            self.complete_promise: Optional[asyncio.Future[None]] = asyncio.Future()
            self.length_updated: Optional[asyncio.Condition] = asyncio.Condition()
        else:
            self.complete_promise = None
            self.length_updated = None
# ---
def test_warmup_and_cosine_decay():
    optimizer = AdamConfig(
        learning_rate=1e-2,
        weight_decay=0.0,
        warmup=0.1,  # 10% of steps
        min_lr_ratio=0.1,
        lr_schedule="cosine",
        cycles=None,
    )

    sched_fn = optimizer.lr_scheduler(1000)

    # Warmup phase
    assert np.isclose(sched_fn(0), 0.0)
    assert np.isclose(sched_fn(50), 0.5e-2)
    assert np.isclose(sched_fn(100), 1e-2)

    # Decay phase
    assert np.isclose(sched_fn(999), 1e-3, atol=1e-5)
# ---
def is_enabled(self) -> bool:
        return len(self.watch_targets) > 0 and self.interval > 0 and (self.include_norms or self.include_histograms)
# ---
def __init__(self, array):
        self.array = array
# ---
def test_raw_metric_host_network(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'net_usage_rate_average',
        host_name)

    for record in query:
        if record.net_usage_rate_average is not None:
            assert record.net_usage_rate_average > 0, 'Zero Host Network IO'
            break
# ---
def evaluate(self, record: dict) -> Any:
        return self.value
# ---
def make(self):
        comps = {}
        for name, component in self.components.items():
            comps[name] = eval(component['class'])(config=eval(component['config']))
        return Compound(linkages=self.linkages, **comps)
# ---
def scopes_del(self):
        del self._scopes
        if self.req is not None and self.req.environ.get('wenoit_etalage') is not None \
                and '_scopes' in self.req.environ['wenoit_etalage']:
            del self.req.environ['wenoit_etalage']['_scopes']
# ---
def convert_to_cache(self, value, record, validate=True):
        if value is None or value is False:
            return False
        if validate and self.sanitize:
            return html_sanitize(value, strip_style=self.strip_style)
        return value
# ---
def reset(self) -> "ListCache[PageCacheT]":
        return ListCache(tuple(cache.reset() for cache in self.caches))
# ---
def name_get(self, cr, uid, ids, context=None):
        res = []
        for line in self.browse(cr, uid, ids, context=context):
            name = line.location_id.name+' > '+line.location_dest_id.name
            # optional prefixes
            if line.product_id.code:
                name = line.product_id.code + ': ' + name
            if line.picking_id.origin:
                name = line.picking_id.origin + '/ ' + name
            res.append((line.id, name))
        return res
# ---
def camel_case(string):
    """Return camel case string from a space-separated string.

    Example
    -------
    >>> camel_case('good job')
    'GoodJob'
    """
    return ''.join(w.capitalize() for w in string.split())
# ---
def _custom_setup(self):
        self._driver = netapp_nfs.NetAppDirect7modeNfsDriver(
            configuration=create_configuration())
# ---
def __call__(self, target, cred):
        """Check the policy.

        Requires that all rules accept in order to return True.
        """

        for rule in self.rules:
            if not rule(target, cred):
                return False

        return True
# ---
def _get_random_inputs(config: GemmaConfig):
    Embed = config.Embed
    Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()
    return x, mask
# ---
def put(self, obj: Any):
        return ray.put(obj)
# ---
def test_get_task_status_not_found(service, worker, request_context):
    """Test get_task_status raises NOT_FOUND for nonexistent task."""
    status_request = cluster_pb2.Worker.GetTaskStatusRequest(task_id=JobName.root("nonexistent").task(0).to_wire())

    with pytest.raises(ConnectError) as exc_info:
        service.get_task_status(status_request, request_context)

    assert exc_info.value.code == Code.NOT_FOUND
    assert "nonexistent" in str(exc_info.value)
# ---
def parse_newline(self, m):
        length = len(m.group(0))
        if length > 1:
            self.tokens.append({'type': 'newline'})
# ---
def _retire_overused_rollouts(self):
        """Remove rollouts that exceeded max_samples usage."""
        if self.max_samples < 0:
            return

        for env_name in self.rollout_storage:
            rollouts = self.rollout_storage[env_name]
            # Keep only rollouts under usage limit
            self.rollout_storage[env_name] = [r for r in rollouts if r.usage_count < self.max_samples]
# ---
def device_compatible(job_device_type: str, worker_device_type: str) -> bool:
    """Check if a job's device requirement is compatible with a worker's device.

    CPU jobs can run on any worker since every host has a CPU.
    Accelerator jobs (GPU, TPU) require the specific hardware.
    """
    if job_device_type == "cpu":
        return True
    return job_device_type == worker_device_type
# ---
def frame_save(fig, frame, odir=None, frame_pattern="frame_%05d.png", dpi=100):
    fig.savefig(
        os.path.join(odir, frame_pattern % (frame)),
        dpi=dpi,
        facecolor=fig.get_facecolor(),
        transparent=True,
    )
    # I am trying everything to *wipe* this figure, hoping that it could
    # help with the dask glitches I experienced earlier.
    # TBD if this is all needed...how this might affect performance.
    plt.close(fig)
    del fig
    gc.collect(2)
# ---
def get_block_size_limit(self, block: Block):
        with self.lock:
            return self._state.get_block_size_limit(block)
# ---
def positive(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in positive")
    return elemwise(nxp.positive, x, dtype=x.dtype)
# ---
def current_len(self) -> Optional[int]:
        """
        Returns the current length of the data store. If the length is infinite or not known, returns None.
        """
        pass
# ---
def ensure_parent_dir(path: str) -> None:
    """Create directories for `path` if necessary."""
    # Use os.path.dirname for local paths, otherwise use fsspec
    if "://" in path:
        output_dir = path.rsplit("/", 1)[0]
        fs, dir_path = fsspec.core.url_to_fs(output_dir)
        if not fs.exists(dir_path):
            fs.mkdirs(dir_path, exist_ok=True)
    else:
        output_dir = os.path.dirname(path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
# ---
def _transp_transition(self, i, sigma):
    sigma = _unsqueeze(sigma, reference=i[..., None])
    edge = torch.exp(-sigma) * F.one_hot(
      i, num_classes=self.vocab_size)
    edge += torch.where(i == self.mask_index,
                        1 - torch.exp(-sigma).squeeze(-1),
                        0)[..., None]
    return edge
# ---
def max_pool_mean(x):
        pooled = max_pool(
            (hax.Axis("H", 2), hax.Axis("W", 2), hax.Axis("D", 2)), x, stride=1, padding=((0, 1), (0, 1), (0, 1))
        )
        return hax.mean(pooled).scalar()
# ---
def get_match_urls(response):
    for match in response.xpath("//a[contains(@href, 'stats/games/')]/@href").extract():
                yield response.urljoin(match)
# ---
def Check_Solution(a,b,c): 
    if (2*b*b == 9*a*c): 
        return ("Yes"); 
    else: 
        return ("No");
# ---
def fake_join_subordinate(id, compute_uuid, host, url, user, password):
            fake_join_subordinate.called = True
# ---
def fun_wrapped(dynamic_donated, dynamic_reserved, static):
        dynamic = eqx.combine(dynamic_donated, dynamic_reserved)
        dynamic_fun, dynamic_spec = dynamic
        static_fun, static_spec = static

        fun = hashable_combine(dynamic_fun, static_fun)
        args, kwargs = hashable_combine(dynamic_spec, static_spec)
        out = fun(*args, **kwargs)
        out_dynamic, out_static = hashable_partition(out, is_array)
        return out_dynamic, Static(out_static)
# ---
def longest_increasing_subsequence(arr): 
	n = len(arr) 
	longest_increasing_subsequence = [1]*n 
	for i in range (1 , n): 
		for j in range(0 , i): 
			if arr[i] > arr[j] and longest_increasing_subsequence[i]< longest_increasing_subsequence[j] + 1 : 
				longest_increasing_subsequence[i] = longest_increasing_subsequence[j]+1
	maximum = 0
	for i in range(n): 
		maximum = max(maximum , longest_increasing_subsequence[i]) 
	return maximum
# ---
def unregister(self, vm_id: str) -> None:
        """Unregister a VM by ID.

        Called when a VM is terminated. Safe to call if the VM is not registered.
        """
        with self._lock:
            self._vms.pop(vm_id, None)
# ---
def __goForAWalk(self, task):
        self.notify.debug('going for a walk')
        self.fsm.request('Walk')
        return Task.done
# ---
def test_impl(df):
            df2 = df[['A']]
            df2['A'] += 10
            return df2.A, df.A
# ---
def read_jsonl_gz_file(filepath: str) -> list[dict]:
    """Helper function to read a JSONL.GZ file"""
    results = []
    with fsspec.open(filepath, "rt", encoding="utf-8", compression="infer") as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    return results
# ---
def metadata(self) -> dict:
        return construct_metadata(self.data)
# ---
def test_dropout(self):
    testing_utils.layer_test(
        keras.layers.Dropout, kwargs={'rate': 0.5}, input_shape=(3, 2))

    testing_utils.layer_test(
        keras.layers.Dropout,
        kwargs={'rate': 0.5,
                'noise_shape': [3, 1]},
        input_shape=(3, 2))
# ---
def test_raw_metric_host_memory(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'derived_memory_used',
        host_name)

    for record in query:
        if record.derived_memory_used is not None:
            assert record.derived_memory_used > 0, 'Zero Host Memory Usage'
            break
# ---
def sum_three_smallest_nums(lst):
	return sum(sorted([x for x in lst if x > 0])[:3])
# ---
def with_tpu(tpu_type: str, *, slice_count: int = 1, **kwargs: Any) -> ResourceConfig:
        device = TpuConfig(variant=tpu_type)
        return ResourceConfig(device=device, replicas=slice_count, **kwargs)
# ---
def main():
    unittest.main()
# ---
def open_shard(self, shard_name: str) -> Iterator[T_co]:
        return self.open_shard_at_row(shard_name, 0)
# ---
def _get_first_task_error(self, job_id: JobName) -> str | None:
        """Get the first error message from failed/killed tasks in a job."""
        for task_id in self._tasks_by_job.get(job_id, []):
            task = self._tasks.get(task_id)
            if task and task.error:
                return task.error
        return None
# ---
def log(message: str) -> None:
    """Log to stderr (visible in MCP server logs)."""
    print(f"[safe-fetch] {message}", file=sys.stderr)
# ---
def convert_to_write(self, value, target=None, fnames=None):
        return value.id
# ---
def __call__(self, env):
        response = self.proxy(env)
        if response:
            return response

        response = super(ProxyArchivalRouter, self).__call__(env)
        if response:
            return response
# ---
def area_weighted_mean(
    data: torch.Tensor,
    area_weights: torch.Tensor,
    dim: tuple[int, ...] = (-2, -1),
    keepdim: bool = False,
) -> torch.Tensor:
    return weighted_mean(data, area_weights, dim=dim, keepdim=keepdim)
# ---
def match_entries_by_date_and_amount(self, threshold):
        self.selected_pane.match_entries_by_date_and_amount(threshold)
        self.import_table.refresh()
# ---
def init(cls, Vocab: Axis, config: Gpt2HyenaConfig, *, key) -> "Gpt2HyenaModel":
        k_t, k_embeddings = jrandom.split(key, 2)
        backbone = Gpt2HyenaBackbone.init(config, key=k_t)
        embeddings = Gpt2Embeddings.init(
            Vocab,
            # Our config type has everything it needs, but is not a subclass of Gpt2Config
            config,  # type: ignore
            key=k_embeddings,
        )

        return Gpt2HyenaModel(backbone, embeddings)
# ---
def get_device_variant(device: cluster_pb2.DeviceConfig) -> str | None:
    """Extract device variant (e.g., GPU model) from config."""
    if device.HasField("gpu"):
        return device.gpu.variant if device.gpu.variant else None
    elif device.HasField("tpu"):
        return device.tpu.variant if device.tpu.variant else None
    return None
# ---
def test_load_vortex_empty_file(self, tmp_path):
        """Test loading an empty vortex file."""
        empty_path = tmp_path / "empty.vortex"
        write_vortex_file([], str(empty_path))

        records = list(load_vortex(str(empty_path)))
        assert records == []
# ---
def _get_profile(self):
        if self.profile:
            return self.profile
        self.profile = self.get_profile()
        return self.profile
# ---
def device_type(self) -> str:
        """Device type from worker metadata."""
        return get_device_type(self.metadata.device)
# ---
def test_csv(self):
        body, mime_type = self._run_handler(
            EXPERIMENT, SESSION_GROUPS, download_data.OutputFormat.CSV
        )
        self.assertEqual("text/csv", mime_type)
        self.assertEqual(EXPECTED_CSV, body)
# ---
def kill_all():
    proc = multiprocessing.active_children()
    for p in proc:
        p.terminate()
    return 'killed all'
# ---
def get_field_from_jbor(thing):
    '''
    :returns: Output field name from a JBOR

    Assumes :func:`is_job_ref` evaluates to True
    '''
    if '$dnanexus_link' in thing:
        return thing['$dnanexus_link']['field']
    else:
        return thing['field']
# ---
def setYear(self, year):
        self.__year = year
# ---
def test_ones_like(spec, executor):
    a = xp.ones((3, 3), chunks=(2, 2), spec=spec)
    b = xp.ones_like(a)
    assert_array_equal(b.compute(executor=executor), np.ones_like(np.ones((3, 3))))
# ---
def setUp(self):
        super().setUp()

        permission = Permission.objects.get(codename='search')
        self.user = User.objects.create()
        self.user.user_permissions.add(permission)
        self.group.add_member(self.user.essauth_member)

        self.client.force_authenticate(user=self.user)
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        for tracker in self.loggers:
            tracker.log_hyperparameters(hparams)
# ---
def generate_hash_from_messages(messages: list[dict[str, str]]) -> str:
    """Generate a hash from a list of messages.

    Args:
        messages (List[Dict[str, str]]): A list of messages.

    Returns:
        str: A hash of the messages.
    """
    return hashlib.sha256(str(messages).encode()).hexdigest()
# ---
def _output_ckpt_name(self):
        return f"BlockSeq[{self.Block}, {self.blocks[0].__class__.__name__}].outputs"
# ---
def __post_init__(self):
        if self.workspace and self.docker_image:
            raise ValueError("Cannot specify both workspace and docker_image")
        if not self.workspace and not self.docker_image:
            raise ValueError("Must specify either workspace or docker_image")
# ---
def session(self):
        return self.req.environ.get('beaker.session') if self.req is not None else None
# ---
def predict(self, s):
        return self.model.eval([s])
# ---
def _spec_from_serialized_config(ser: str):
    config = donfig.deserialize(ser)
    spec_dict = expand_environment_variables(config["spec"])
    return Spec(**spec_dict)
# ---
def log_cb(step, metrics):
        logged_metrics.append((step, {k: float(v) for k, v in metrics.items()}))
# ---
def _update(self, records, value):
        """ Update the cached value of ``self`` for ``records`` with ``value``. """
        records._cache[self] = value
# ---
def pad_non_aligned_v_block():
        if v_dim % v_block_size != 0:
            rem = v_dim % v_block_size
            w_ref[:, rem:] = jnp.zeros((w_ref.shape[0], w_ref.shape[1] - rem), dtype=w_ref.dtype)
# ---
def convert_to_onchange(self, value):
        """ convert ``value`` from the cache to a valid value for an onchange
            method v7.
        """
        return self.convert_to_write(value)
# ---
def shard_names(self) -> Sequence[str]:
        raise NotImplementedError
# ---
def __iter__(self):
        return BackgroundIterator(self._producer_fn, self.max_capacity)
# ---
def delete(self):
        """
            Delete this job and all its items from the job table
        """

        db = current.db

        _debug("Deleting job ID=%s" % self.job_id)
        self.__define_tables()
        item_table = self.item_table
        query = item_table.job_id == self.job_id
        db(query).delete()
        job_table = self.job_table
        query = job_table.job_id == self.job_id
        db(query).delete()
# ---
def find_triplet_array(A, arr_size, sum): 
	for i in range( 0, arr_size-2): 
		for j in range(i + 1, arr_size-1): 
			for k in range(j + 1, arr_size): 
				if A[i] + A[j] + A[k] == sum: 
					return  A[i],A[j],A[k] 
					return True
	return False
# ---
def quit(self):
        self._quit = True
# ---
def get_task_status(self, request: cluster__pb2.Worker.GetTaskStatusRequest, ctx: RequestContext) -> cluster__pb2.TaskStatus:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def redraw(self, ctx):
        ctx.set_source(self.fill.pattern)
        self.indicator.redraw(ctx, self.metric.value)
# ---
def the_void_name_is_filled_by_filling(name, filling):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(rel.RelatedBuildingElement.Name == filling for rel in element.HasFillings):
        return True
    assert False, "No filling found"
# ---
def testAugAssignBitAnd(self):
    self.assertEqual((0, '3\n'), _GrumpRun(textwrap.dedent("""\
        foo = 7
        foo &= 3
        print foo""")))
# ---
def __init__(self, internal_name, main_controller):
        self.internal_name = internal_name
        self.main_controller = main_controller

        self._init_plugin()
# ---
def __init__(self, op, leaving):
      self.op = op
      self.leaving = leaving
# ---
def getPoints(numberOfPoints):
    pointList = list(zip(np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)))
    return pointList
# ---
def key_function(out_key):
        out_coords = out_key[1:]
        return tuple((name, *out_coords) for name in names)
# ---
def maximum_product(nums):
    import heapq
    a, b = heapq.nlargest(3, nums), heapq.nsmallest(2, nums)
    return max(a[0] * a[1] * a[2], a[0] * b[0] * b[1])
# ---
def float_power(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.float_power](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.float_power.html)
    """
    return jnp.float_power(x1, x2)
# ---
def __nonzero__(self):
        return self.__bool__()
# ---
def _with_mesh(mesh):
        set_mesh = getattr(jax, "set_mesh", None)
        if set_mesh is None:
            return mesh
        set_mesh(mesh)
        return None
# ---
def health_check(self, request: actor__pb2.Empty, ctx: RequestContext) -> actor__pb2.HealthResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def _read_stored(self):
        # Only works if the array has been computed
        if self.size > 0:
            # read back from zarr
            zarray = open_if_lazy_zarr_array(self._zarray)
            return numpy_array_to_backend_array(zarray[...])
        else:
            # this case fails for zarr, so just return an empty array of the correct shape
            return nxp.empty(self.shape, dtype=self.dtype)
# ---
def check_min_heap(arr, i):
    if 2 * i + 2 > len(arr):
        return True
    left_child = (arr[i] <= arr[2 * i + 1]) and check_min_heap(arr, 2 * i + 1)
    right_child = (2 * i + 2 == len(arr)) or (arr[i] <= arr[2 * i + 2] 
                                      and check_min_heap(arr, 2 * i + 2))
    return left_child and right_child
# ---
def task_state_name(state: int) -> str:
    """Return enum name like 'TASK_STATE_RUNNING'."""
    try:
        return _TASK_STATE.values_by_number[state].name
    except KeyError:
        return f"UNKNOWN({state})"
# ---
def max_sum_of_three_consecutive(arr, n): 
	sum = [0 for k in range(n)] 
	if n >= 1: 
		sum[0] = arr[0] 
	if n >= 2: 
		sum[1] = arr[0] + arr[1] 
	if n > 2: 
		sum[2] = max(sum[1], max(arr[1] + arr[2], arr[0] + arr[2])) 
	for i in range(3, n): 
		sum[i] = max(max(sum[i-1], sum[i-2] + arr[i]), arr[i] + arr[i-1] + sum[i-3]) 
	return sum[n-1]
# ---
def __init__(self, upsampling: int | tuple[int, int] = 2):
        super().__init__()
        if isinstance(upsampling, int):
            upsampling = (upsampling, upsampling)
        if tuple(upsampling) != (2, 2):
            raise ValueError(
                "ZonallyPeriodicBilinearUpsample only supports 2x upsampling"
            )
        self.scale_h, self.scale_w = upsampling
# ---
def name(self):
        return EnkfNode.cNamespace().get_name(self)
# ---
def setReplicaHost( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndict = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.setReplicaHost( lfndict )
# ---
from copy import deepcopy
def colon_tuplex(tuplex,m,n):
  tuplex_colon = deepcopy(tuplex)
  tuplex_colon[m].append(n)
  return tuplex_colon
# ---
def shape(self) -> tuple[int, ...]:
        """
        The number of blocks per axis.
        """
        return self.array.numblocks
# ---
def task_index(self) -> int | None:
        """If this is a task (last component is numeric), return the index."""
        try:
            return int(self._parts[-1])
        except ValueError:
            return None
# ---
def test_failure(tmp_path, timing_map, n_tasks, retries, use_backups):
    path = f"{BASE_PATH}/{tmp_path.name}"
    with pytest.raises(RuntimeError):
        asyncio.run(
            run_test(
                app_function=deterministic_failure_modal,
                input=range(n_tasks),
                use_backups=use_backups,
                path=path,
                timing_map=timing_map,
            )
        )

    check_invocation_counts(path, timing_map, n_tasks, retries)
# ---
def reciprocal(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in reciprocal")
    return elemwise(nxp.reciprocal, x, dtype=x.dtype)
# ---
def unregister_endpoint(self, request: cluster__pb2.Controller.UnregisterEndpointRequest, ctx: RequestContext) -> cluster__pb2.Empty:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def ptp(self, axis: AxisSelection | None = None) -> "NamedArray":  # pragma: no cover
        return haliax.ptp(self, axis=axis)
# ---
def state():
    return ControllerState()
# ---
def underscore(txt):
        """Return an under_scores text from a CamelCase text.

        This function will leave a CamelCase text unchanged.
        """
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', txt)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
# ---
def modular_sum(arr, n, m): 
	if (n > m): 
		return True
	DP = [False for i in range(m)] 
	for i in range(n): 
		if (DP[0]): 
			return True
		temp = [False for i in range(m)] 
		for j in range(m): 
			if (DP[j] == True): 
				if (DP[(j + arr[i]) % m] == False): 
					temp[(j + arr[i]) % m] = True
		for j in range(m): 
			if (temp[j]): 
				DP[j] = True
		DP[arr[i] % m] = True
	return DP[0]
# ---
def pipeline_to_stream(
    create_futures_func: Callable,
    name: str,
    pipeline: CubedPipeline,
    **kwargs,
) -> Stream:
    """
    Turn a pipeline into an asynchronous stream of results.
    """
    return stream.iterate(
        async_map_unordered(
            create_futures_func,
            pipeline.mappable,
            return_stats=True,
            name=name,
            func=pipeline.function,
            config=pipeline.config,
            **kwargs,
        )
    )
# ---
def uninstall():
    paths = LAUNCHER_PATH, DATA_PATH
    for p in paths:
        print("Deleting", p, "...", end=" ", flush=True)
        shutil.rmtree(p)
        print("Success!")
    print("Removing desktop shortcut...", end=" ", flush=True)
    desktop = os.path.join(os.path.expanduser('~'), 'Desktop')
    shortcut_path = os.path.join(desktop, "Augur Launcher.lnk")
    os.remove(shortcut_path)
    print("Success!")
# ---
def _cycle_property(self, name, direction='up'):
        self.command('cycle_property', name, direction)
# ---
def init(config: GemmaConfig, *, key) -> "GemmaTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, GemmaDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return GemmaTransformer(config, layers, ln_f)
# ---
def finished(self, name):
    print("==========")
# ---
def create_actor(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> ActorHandle:
        group = self.create_actor_group(actor_class, *args, name=name, count=1, resources=resources, **kwargs)
        return group.wait_ready()[0]
# ---
def p_error(self, p):
        if p:
            raise SyntaxError(
                "Character '%s' at line %d" % (p.value[0], p.lineno)
            )
        else:
            raise SyntaxError("SyntaxError at EOF")
# ---
def testFunctionDefGeneratorReturnValue(self):
    self.assertRaisesRegexp(
        util.ParseError, 'returning a value in a generator function',
        _ParseAndVisit, 'def foo():\n  yield 1\n  return 2')
# ---
def catalan_number(num):
    if num <=1:
         return 1   
    res_num = 0
    for i in range(num):
        res_num += catalan_number(i) * catalan_number(num-i-1)
    return res_num
# ---
def _check_step(v: str) -> str:
    if v not in _all_steps():
        raise ValueError(
            f"Invalid step: '{v}', expected one of: {', '.join(_ordered_steps())}"
        )
    return v
# ---
def __mul__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__mul__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.multiply, self, other, dtype=result_type(self, other))
# ---
def the_object_name_is_at_location(name, location):
    obj_location = the_object_name_exists(name).location
    assert (
        obj_location - Vector([float(co) for co in location.split(",")])
    ).length < 0.1, f"Object is at {obj_location}"
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        examples = []
        for _ in range(n_examples):
            word, opposite = self.OPPOSITES[rng.integers(len(self.OPPOSITES))]
            prompt = f"Opposite of {word}? One word:"
            answer = opposite
            examples.append({"prompt": prompt, "answer": answer})
        return examples
# ---
def loss_fn(x_in, w_in, y_in):
            return fused_cross_entropy_loss_and_logsumexp_penalty(
                x_in,
                y_in,
                w_in,
                reduction="mean",
                logsumexp_weight=0.0,
                block_sizes=block_sizes,
                dtype=jnp.float32,
                logit_soft_cap=None,
                implementation="pallas_tpu",
            )
# ---
def test_column_list_select2(self):
        # make sure SDC copies the columns like Pandas does
        def test_impl(df):
            df2 = df[['A']]
            df2['A'] += 10
            return df2.A, df.A

        hpat_func = self.jit(test_impl)
        n = 11
        df = pd.DataFrame(
            {'A': np.arange(n), 'B': np.ones(n), 'C': np.random.ranf(n)})
        np.testing.assert_array_equal(hpat_func(df.copy())[1], test_impl(df)[1])
# ---
def is_Word_Present(sentence,word): 
    s = sentence.split(" ") 
    for i in s:  
        if (i == word): 
            return True
    return False
# ---
def local_cpu_mesh():
    """Temporarily sets the default device to CPU and creates a mesh with a single CPU device"""
    cpu = jax.local_devices(backend="cpu")[0]
    mesh = create_mesh_from_axis_specs(
        ici_axes={
            ResourceAxis.REPLICA: 1,
            ResourceAxis.DATA: 1,
            ResourceAxis.MODEL: 1,
            ResourceAxis.CONTEXT: 1,
        },
        dcn_axes={},
        devices=[cpu],
    )
    with use_cpu_device(), haliax.partitioning.set_mesh(mesh):
        yield mesh
# ---
def __getitem__(self, key):
        return self._raw[key]
# ---
def mpra_1(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transduction'],
        'introduced_elements': 'synthesized DNA',
        'modified_site_nonspecific': 'random'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def ema_checkpoint_path(self) -> Path:
        return self.checkpoint_dir / "ema_ckpt.pt"
# ---
def __hash__(self):
        """Since subqueries are mutable, this object should not be hashable.
        However and for conveniences purposes, it can be hashed.
        """
        return reduce(mul, map(hash, self.subqueries), 1)
# ---
def get(self):
        self.finish()
# ---
def find_Min_Sum(num): 
    sum = 0
    i = 2
    while(i * i <= num): 
        while(num % i == 0): 
            sum += i 
            num /= i 
        i += 1
    sum += num 
    return sum
# ---
def __init__(
        self,
        service: WorkerServiceImpl,
        host: str = "0.0.0.0",
        port: int = 8080,
    ):
        self._service = service
        self._host = host
        self._port = port
        self._app = self._create_app()
        self._server: uvicorn.Server | None = None
# ---
def get_widget_id(self, prefix, name, key=''):
        if self.instance:
            opts = self.instance._meta
            widget_id = '%s-%s-%s_%s-%s' % (prefix, name, opts.app_label, opts.module_name, self.instance.pk)
        else:
            widget_id = '%s-%s' % (prefix, name)
        if key:
            widget_id = '%s_%s' % (widget_id, slugify(key))
        return widget_id
# ---
def flatten_axes(axis: Axis, old_axes: Axis, new_axis: AxisSelector) -> Axis:
    pass
# ---
def get_vocab(self):
        return self._vocab
# ---
def _initialize(self, value=0):
        self.value = value
# ---
def testSizeNonmultiple12(self):
        self.assertConfigureFails(HPCP(), {'size':13})
# ---
def cancel_assign(self, cr, uid, ids, *args):
        """ Cancels picking and moves.
        @return: True
        """
        wf_service = netsvc.LocalService("workflow")
        for pick in self.browse(cr, uid, ids):
            move_ids = [x.id for x in pick.move_lines]
            self.pool.get('stock.move').cancel_assign(cr, uid, move_ids)
            wf_service.trg_write(uid, 'stock.picking', pick.id, cr)
        return True
# ---
def axis_spec_to_tuple(axis_spec: AxisSpec) -> tuple[Axis, ...]: ...
# ---
def __init__(
            self,
            filepath,
            hdfs_conn_id='hdfs_default',
            *args, **kwargs):
        super(HdfsSensor, self).__init__(*args, **kwargs)
        self.filepath = filepath
        self.hdfs_conn_id = hdfs_conn_id
# ---
def __init__(self):
            raise NotImplementedError('deprecated')
# ---
def is_not_a_branch(git_path, module, dest):
    branches = get_branches(git_path, module, dest)
    for branch in branches:
        if branch.startswith('* ') and ('no branch' in branch or 'detached from' in branch or 'detached at' in branch):
            return True
    return False
# ---
def test_job_id_contains_name(client: LocalClient):
    handle = client.submit(JobRequest(name="my-job", entrypoint=Entrypoint.from_callable(_noop)))
    assert "my-job" in handle.job_id
    handle.wait()
# ---
def _multislice_info_from_head(head: SliceInfo, slice_id: int, num_slices: int) -> MultisliceInfo:
    """
    Create a MultisliceInfo object from the head slice info and the slice ID and number of slices.
    """
    return MultisliceInfo(
        coordinator_ip=head.ip_address,
        slice_id=slice_id,
        num_slices=num_slices,
        port=8081,  # default port for megascale
    )
# ---
def available_memory(self) -> int:
        """Available memory bytes after subtracting committed resources."""
        return self.metadata.memory_bytes - self.committed_mem
# ---
def get_power_range(self):
		return self.power_range
# ---
def __hash__(self) -> int:
        """Hash based on repr for use in sets/dicts."""
        return hash(repr(self))
# ---
def move_first(test_list):
  test_list = test_list[-1:] + test_list[:-1]  
  return test_list
# ---
def has_axis(self, axis: AxisSelection) -> bool:
        """Returns true if the given axis is present in this NamedArray."""
        return self.axis_indices(axis) is not None
# ---
def access_elements(nums, list_index):
    result = [nums[i] for i in list_index]
    return result
# ---
def is_not_null(self) -> NotExpr:
        return NotExpr(IsNullExpr(self))
# ---
def default_window_title(self):
		"""()"""
		if TRACE: print(__name__), self.default_window_title.__doc__

		return "Dragon"
# ---
def with_prefix(prefix: str | None, leaf: str | None) -> str | None:
    """Joins two optional path strings in a way compatible with pytorch state dict serialization"""
    if prefix is None:
        return leaf
    elif leaf is None:
        return prefix
    else:
        return f"{prefix}.{leaf}"
# ---
def __delitem__(self, key):
        del self.subqueries[key]
# ---
def _spatial_gradients(
    tensor: torch.Tensor, *, pad_mode: str
) -> tuple[torch.Tensor, torch.Tensor]:
    """Compute forward differences along y and x axes with configurable x padding."""
    grad_y = tensor[:, :, 1:, :] - tensor[:, :, :-1, :]
    grad_y = F.pad(grad_y, (0, 0, 0, 1), mode="constant")

    padded_x = F.pad(tensor, (0, 1, 0, 0), mode=pad_mode)
    grad_x = padded_x[:, :, :, 1:] - padded_x[:, :, :, :-1]

    return grad_y, grad_x
# ---
def relu_squared(x: A) -> A:
    """ReLU squared activation function. jnp.square(jnp.maximum(0, x))"""

    def _fn(a):
        return jnp.square(jnn.relu(a))

    return typing.cast(A, wrap_elemwise_unary(_fn, x))
# ---
def append(self, ex: T):
        return self.extend([ex])
# ---
def output(self):
        output_path = '{}/catalogs/{}.csv'.format(self.root_path,
                                                  self.catalog_name)
        return luigi.s3.S3Target(path=output_path)
# ---
def name(self) -> str:
        return "modal"
# ---
def Offer_To_Pick_Up_Call (self, Call_Flow_Control, Call_ID):
        self.Step (Message = "Client offers to answer call...")

        try:
            Call_Flow_Control.PickupCall (call_id = Call_ID)
        except:
            self.Log (Message = "Pick-up call returned an error of some kind.")
# ---
def openai_client(self) -> AsyncOpenAI:
        return AsyncOpenAI(
            base_url=f"http://{self._inference_server.address()}/v1",
            api_key="marin",
        )
# ---
def set_slotted(self, slotted):
        self.slotted = slotted
# ---
def _assert_can_put_with_sharding(array, sharding):
    try:
        jax.device_put(array, sharding)
    except ValueError:
        # assert False, f"Could not put array with shape {array.shape} with sharding {sharding}"
        raise AssertionError(f"Could not put array with shape {array.shape} with sharding {sharding}")
# ---
def init_test_tqdm(self) -> None:
        """Initialize the test progress bar."""
        bar = super().init_test_tqdm()
        return self._update_bar_description(bar)
# ---
def highest_Power_of_2(n): 
    res = 0; 
    for i in range(n, 0, -1):  
        if ((i & (i - 1)) == 0): 
            res = i; 
            break;      
    return res;
# ---
def spec(test_data):
    return test_data['spec']
# ---
def observer(val):
            if cond(val):
                sema.release()
# ---
def output_link(self, m):
        return self._process_link(m, m.group(3), m.group(4))
# ---
def arrays_to_plan(cls, *arrays):
        return Plan(arrays_to_dag(*arrays))
# ---
def _pipeline() -> int:
        if batch_size is None:
            batches = [in_memory_table.to_pylist()]
        else:
            batches = (b.to_pylist() for b in in_memory_table.to_batches(max_chunksize=batch_size))
        return sum(len(dupekit.process_dicts_batch(batch)) for batch in batches)
# ---
def discover_latest(self) -> bool:
        return self.checkpoint_step is None
# ---
def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()
# ---
def test_fdfuncarg_skips_on_no_osdup(testdir):
    testdir.makepyfile("""
        import os
        if hasattr(os, 'dup'):
            del os.dup
        def test_hello(capfd):
            pass
    """)
    result = testdir.runpytest_subprocess("--capture=no")
    result.stdout.fnmatch_lines([
        "*1 skipped*"
    ])
# ---
def tearDown(self):
        self.master.delete()
        self.master_status.delete()
        self.datastore.delete()
        self.datastore_version.delete()
        models.create_nova_client = self.safe_nova_client
        super(TestReplication, self).tearDown()
# ---
def __getitem__(self, field):
        return getattr(self, field, None)
# ---
def unembed_active_scale(self):
        return 1 / hax.axis_size(self.Embed)
# ---
def in_qdq_bwd(compute_dtype, res, g):
    new_scale, new_history = res
    q_g = g
    return q_g, new_scale, new_history
# ---
def mock_requests_get(url, **kwargs):
        from unittest.mock import Mock

        response = Mock()
        response.status_code = 200
        if "file1" in url:
            response.headers = {"content-length": str(len(file1_data))}
            response.raw = BytesIO(file1_data)
        else:
            response.headers = {"content-length": str(len(file2_data))}
            response.raw = BytesIO(file2_data)
        return response
# ---
def fn(config: MyConfig):
            pass
# ---
def test_bank_has_return(bank):
    assert bank.has_type("Return"), "Should extract Return statements"
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: AttentionMask | NamedArray | None = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        x = self.embeddings.embed(input_ids)
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def test_can_restore_from_backup_with_almost_equal_size(self):
        # target size equals to "1Gb"
        self.backup.size = 0.99
        self.backup.save()
        instance = models.Instance.create(
            self.context, self.name, self.flavor_id,
            self.image_id, self.databases, self.users,
            self.datastore, self.datastore_version,
            self.volume_size, self.backup_id,
            self.az, self.nics, self.configuration)
        self.assertIsNotNone(instance)
# ---
def remove_job(self, index):
        job_num = int(self.main_view.ui.jobs_tab_widget.tabText(index)[1:])
        self.main_model.jobs.pop(job_num, None)
        self.main_view.remove_tab(index)
# ---
def upload_photo_id_image(self, img):
        raise NotImplementedError
# ---
def __rpow__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__rpow__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.pow, other, self, dtype=result_type(self, other))
# ---
def __str__(self):
        return self.as_written
# ---
def test_llama_params():
    # Check that the computed number of trainable params is close to the actual number of params
    hf_config = transformers.LlamaConfig.from_pretrained("NousResearch/Llama-2-7b-hf")
    llama_config = LlamaConfig.from_hf_config(hf_config)
    actual_params = 6.738415616e9
    params = llama_config.total_trainable_params(hf_config.vocab_size)
    assert np.isclose(actual_params, params, rtol=1e-2)
# ---
def _has_expert_details(self):
        if self._is_expert():
            profile = self._get_profile()
            return True if profile.title or profile.company else False
# ---
def run_remotely(input, func=None, config=None, name=None, compute_id=None):
            # note we can't use the execution_stat decorator since it doesn't work with ray decorators
            result, stats = execute_with_stats(func, input, config=config)
            return result, stats
# ---
import re
def replace_max_specialchar(text,n):
 return (re.sub("[ ,.]", ":", text, n))
# ---
def class_method(cls, arg):
        return arg
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "artifact",
                "path": artifact_path,
                "name": name,
                "artifact_type": type,
            }
        )
        self.logger.info(json.dumps(record))
# ---
def team_member_id(self):
        return self.team_member.id if self._get_member() else ''
# ---
def maybe_unstrip_protocol(path: str):
        base_path_protocol = urllib.parse.urlparse(str(checkpoint_path)).scheme
        if base_path_protocol != "" and not urllib.parse.urlparse(path).scheme != "":
            return f"{base_path_protocol}://{path}"
        return path
# ---
def update(self, new_model: M, step: int) -> "EmaDecaySqrtModelAveraging[M]":
        w = self._raw_weight(step)
        new_tot_w = self.tot_weight + w
        alpha = 0.0 if new_tot_w == 0.0 else w / new_tot_w
        updated = optax.incremental_update(new_model, self.model, alpha)
        return dataclasses.replace(self, model=updated, tot_weight=new_tot_w)
# ---
def power(a,b):
	if b==0:
		return 1
	elif a==0:
		return 0
	elif b==1:
		return a
	else:
		return a*power(a,b-1)
# ---
def cross_entropy_loss(
    logits: NamedArray,
    Label: AxisSelector,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: None = None,
) -> jnp.ndarray | NamedArray: ...
# ---
def remove_boxed(s):
    """Remove \\boxed wrapper and return content."""
    if "\\boxed " in s:
        left = "\\boxed "
        if s.startswith(left):
            return s[len(left) :]

    idx = s.find("\\boxed{")
    if idx >= 0:
        content = extract_braced_content(s, idx + 6)
        return content if content is not None else s

    return s
# ---
def out_first(self):
        """
        Returns: bool: Whether the output axes are first in the weight matrix
        """
        # We do it this way because of scan layers
        if isinstance(self.Out, hax.Axis):
            return self.weight.axes[-1] != self.Out
        else:
            return self.weight.axes[-len(self.Out) :] != self.Out
# ---
def nanprod(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanprod, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def __init__(
        self,
        dataloader: torch.utils.data.DataLoader[RawTrainData],
        datasets: list[TorchTrainDataset],
        device: torch.device,
    ):
        self._dataloader = dataloader
        self._datasets = {dataset.id: dataset for dataset in datasets}
        self._device = device
# ---
def testFormatSourceShort(self):
    """Tests the _FormatSourceShort function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))

    source_short_string = test_helper._FormatSourceShort(
        event, event_data, event_data_stream)

    self.assertEqual(source_short_string, 'FILE')
# ---
def encode_char(self, char: str) -> int:
        """Encode a single character to a base vocabulary token ID."""
        code = ord(char)
        if code < self.base_vocab_size:
            return self.base_token_offset + code
        return self.base_token_offset
# ---
def unLoadPlugin(plugin_name): 
    """
    @type plugin_name: str
    """
    pass
# ---
def stop(self) -> None:
        """No-op: FakeVmManager has no background threads to stop."""
        pass
# ---
def click_remove_button(self):
        """
        :rtype: HomePage
        """
        self._click(EditMoviePageLocators.REMOVE_BUTTON_LOCATOR)
        self.alert_accept()
        from .home import HomePage
        return HomePage(self._driver)
# ---
def create_vm_side_effect(**kwargs):
        vm = MagicMock()
        vm.info = vm_pb2.VmInfo(
            vm_id=kwargs["vm_id"],
            slice_id=kwargs["slice_id"],
            scale_group=kwargs["scale_group"],
            zone=kwargs["zone"],
            address=kwargs.get("address", ""),
            state=vm_pb2.VM_STATE_BOOTING,
        )
        return vm
# ---
def shell_sort(my_list):
    gap = len(my_list) // 2
    while gap > 0:
        for i in range(gap, len(my_list)):
            current_item = my_list[i]
            j = i
            while j >= gap and my_list[j - gap] > current_item:
                my_list[j] = my_list[j - gap]
                j -= gap
            my_list[j] = current_item
        gap //= 2

    return my_list
# ---
def getCurrentColor(self):
        return self._program.getCurrentColor()
# ---
def getReplicaStatus( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndict = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.getReplicaStatus( lfndict )
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "google/gemma-2-2b"
    ) -> HFCheckpointConverter["Gemma2Config"]:  # type: ignore
        return HFCheckpointConverter(
            self,
            reference_checkpoint=ref_checkpoint,
            trust_remote_code=True,
            HfConfigClass=HfGemma2Config,
        )
# ---
def count_Squares(m,n):
    if(n < m):
        temp = m
        m = n
        n = temp
    return ((m * (m + 1) * (2 * m + 1) / 6 + (n - m) * m * (m + 1) / 2))
# ---
def store(self, parameters: Iterable[nn.Parameter]):
        """
        Save the current parameters for restoring later.

        Args:
            parameters: The parameters to be stored for later restoration by `restore`
        """
        self._stored_params = [param.clone() for param in parameters]
# ---
def init(hidden, mlp):
            return MLP(w1=hax.ones((hidden, mlp)), w2=hax.ones((mlp, hidden)))
# ---
def config_name(request: pytest.FixtureRequest) -> str:
    return request.param
# ---
import re
def text_match_zero_one(text):
        patterns = 'ab?'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        """Block until the job completes."""
        if self._ref is not None:
            return self._wait_ref(timeout, raise_on_failure)
        return self._wait_submission(timeout, raise_on_failure)
# ---
def get_x_from_y(self, y):
        if self.a == 0:
            return 0.0

        return 1.0 * (-self.c - self.b * y) / self.a
# ---
def _get_device_type(self) -> DeviceType:
        """Get device type from config."""
        accel = self._config.accelerator_type
        if accel == config_pb2.ACCELERATOR_TYPE_GPU:
            return DeviceType.GPU
        elif accel == config_pb2.ACCELERATOR_TYPE_TPU:
            return DeviceType.TPU
        return DeviceType.CPU
# ---
def load_dedup_outputs(output_dir: str) -> dict[str, dict]:
    """Load all dedupe output files and return as id->doc mapping.

    Args:
        output_dir: Directory containing .jsonl.gz output files

    Returns:
        Dictionary mapping document IDs to document records
    """
    output_files = list(Path(output_dir).glob("**/*.jsonl.gz"))
    results = []
    for output_file in output_files:
        results.extend(load_jsonl(str(output_file)))
    return {r["id"]: r for r in results}
# ---
def __init__(self, prev):
        self.prev = prev  # ContentOfGroup or CharClass
        self.pattern = ast.PatternChar()
        self.pattern.type = ast.PatternChar.Ascii

        self.prev.add(self.pattern)
# ---
def build(self, ctx: LrScheduleContext):
        return optax.constant_schedule(ctx.learning_rate)
# ---
def process_file(input_path):
        """Read file and return transformed record."""
        with open(input_path) as f:
            content = f.read()
        # Return a single record with uppercased content
        return {"content": content.upper(), "path": input_path}
# ---
def do_pause(self):
        self.do_time_ctrl('pause')
# ---
def test_column_map(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df['B'] = df.A.map(lambda a: 2 * a)
            return df.B.sum()

        n = 121
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# ---
def get(self):
        return self.value
# ---
def find_missing(ar,N): 
    l = 0
    r = N - 1
    while (l <= r):  
        mid = (l + r) / 2
        mid= int (mid) 
        if (ar[mid] != mid + 1 and ar[mid - 1] == mid): 
            return (mid + 1)  
        elif (ar[mid] != mid + 1): 
            r = mid - 1 
        else: 
            l = mid + 1
    return (-1)
# ---
def test_sample_edit_with_validation_invalid(tokenizer):
    source = "x = 1 + 2\n"
    replacement_tokens = tokenizer.encode_source("if :")

    mutation = sample_edit_with_validation(
        source=source,
        edit_position=4,
        original_span_end=9,
        replacement_tokens=replacement_tokens,
        tokenizer=tokenizer,
    )
    assert mutation is None
# ---
def index():
        form = ExampleForm()
        form.validate_on_submit()  # to get error messages to the browser
        flash('critical message', 'critical')
        flash('error message', 'error')
        flash('warning message', 'warning')
        flash('info message', 'info')
        flash('debug message', 'debug')
        flash('different message', 'different')
        flash('uncategorized message')
        return render_template('index.html', form=form)
# ---
def query(self, *, prefix: str | None = None, limit: int = 200) -> list[BufferedLogRecord]:
        with self._lock:
            items = list(self._buffer)
        if prefix:
            items = [r for r in items if r.logger_name.startswith(prefix)]
        return items[-limit:]
# ---
def init(weight):
            return Module(weight=weight)
# ---
def _wait_for_process(self):
        if self._proc.returncode is not None:
            return
        logging.debug("Job %r waiting for virt-v2v process", self._id)
        if not self._proc.wait(timeout=self.PROC_WAIT_TIMEOUT):
            raise V2VProcessError("Job %r timeout waiting for process pid=%s",
                                  self._id, self._proc.pids)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float32)})
            df.A[0:100] = np.nan
            df.A[200:331] = np.nan
            return df.A.quantile(.25)
# ---
def output_exemplar(self):
        return self.exemplar
# ---
import math
def volume_tetrahedron(num):
	volume = (num ** 3 / (6 * math.sqrt(2)))	
	return round(volume, 2)
# ---
def test_iris_not_detected_when_no_context():
    """Should not detect Iris when get_iris_ctx() returns None."""
    with patch("iris.client.client.get_iris_ctx", return_value=None):
        with patch("ray.is_initialized", return_value=False):
            client = current_client()
            assert isinstance(client, LocalClient)
# ---
def address(self) -> str:
        return self.host
# ---
def from_ms(cls, timeout_ms: int) -> "Deadline":
        """Create deadline from milliseconds in the future."""
        return cls(time.monotonic() + timeout_ms / 1000.0)
# ---
def test_impl(n):
            df1 = pd.DataFrame({'key1': np.arange(n), 'A': np.arange(n) + 1.0})
            df2 = pd.DataFrame({'key2': n - np.arange(n), 'A': n + np.arange(n) + 1.0})
            A3 = pd.concat([df1.A, df2.A])
            return A3.sum()
# ---
def sleep(self, seconds):
        """Blocks thread until chronos.tick() advances past wake time."""
        if seconds <= 0:
            return

        event = threading.Event()
        with self._lock:
            wake_time = self._current_time + seconds
            heapq.heappush(self._sleepers, SleepEvent(wake_time, event))

        # Block WITHOUT holding lock - will be woken by tick()
        event.wait()
# ---
def as_local(self) -> "IrisConfig":
        """Create local variant of this config.

        Returns:
            New IrisConfig configured for local testing
        """
        local_proto = make_local_config(self._proto)
        return IrisConfig(local_proto)
# ---
def uuid2dn(self, backend, uuid, from_db_only=False):
        dn = ObjectBackendRegistry.backends[backend].uuid2dn(uuid)
        if dn is None and from_db_only is True:
            # fallback to db
            if self.__index is None:
                self.__index = PluginRegistry.getInstance("ObjectIndex")
            res = self.__index.search({'uuid': uuid}, {'dn': 1})
            if len(res) == 1:
                dn = res[0]['dn']
        return dn
# ---
def setDepth(self, depth):
        self.__depth = depth
# ---
def _reshape_axes_for_bshd_bins(q, q_class, output_order=("B", "S", "H", "D")):
    """
    Reshape the axes of a qkv as BSHD to match the bins in q_class
    """

    q = _maybe_flatten(q, q_class["B"], "B")
    q = _maybe_flatten(q, q_class["S"], "S")
    q = _maybe_flatten(q, q_class["H"], "H")
    q = _maybe_flatten(q, q_class["D"], "D")
    q = q.rearrange(output_order)
    return q
# ---
def write(self):
        '''
        Write MIDI data as a file to the file opened with `.open()`.
        '''
        self.file.write(self.writestr())
# ---
def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LmDatasetSourceConfigBase:
        return HfDatasetSourceConfig(
            id=self.id,
            name=self.name,
            tags=self.tags,
            cache_dir=actual_output_path,
            format=self.format,
        )
# ---
def from_proto(proto: cluster_pb2.AttributeValue) -> "AttributeValue":
        """Convert from protobuf representation."""
        if proto.HasField("string_value"):
            return AttributeValue(proto.string_value)
        elif proto.HasField("int_value"):
            return AttributeValue(proto.int_value)
        elif proto.HasField("float_value"):
            return AttributeValue(proto.float_value)
        # Default to empty string if no value set
        return AttributeValue("")
# ---
def trainable_model(self) -> M:
        return trainables_only(self.model, self.is_trainable)
# ---
def testGradientInput0(self):
    with self.test_session(use_gpu=False):
      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2],
                   dtype=tf.float64, name="x")
      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],
                   shape=[2, 4], dtype=tf.float64, name="y")
      m = tf.matmul(x, y, name="matmul")
      err = gc.ComputeGradientError(x, [3, 2], m, [3, 4])
    print("matmul input0 gradient err = ", err)
    self.assertLess(err, 1e-10)
# ---
def test_admin_contacts(self):
        eq_(self.record.admin_contacts.__class__.__name__, 'list')
        eq_(self.record.admin_contacts, [])
# ---
def create_temp_fasta(sequences, names):
    """Create a temporary FASTA file from sequences."""
    with tempfile.NamedTemporaryFile(mode="w", suffix=".fasta", delete=False) as f:
        for i, (seq, name) in enumerate(zip(sequences, names)):
            f.write(f">{name}\n{seq}\n")
        return f.name
# ---
def __int__(self) -> int:  # pragma: no cover
        return int(self.array)
# ---
def pipelines_to_plan(self, pipelines: ParallelPipelines) -> T:
        """Convert pipeline specifications into a plan."""
        raise NotImplementedError
# ---
def test_measure_reserved_mem(executor):
    if executor.name not in ("processes", "lithops"):
        pytest.skip(f"{executor.name} executor does not support measure_reserved_mem")

    reserved_memory = cubed.measure_reserved_mem(executor=executor)
    assert reserved_memory > 1_000_000
# ---
def shutdown(self):
        """Terminate remaining jobs."""
        logger.info(f"Shutting down cluster with {len(self._jobs)} jobs")
        for job_id, job in self._jobs.items():
            try:
                job.cleanup()
            except Exception as e:
                logger.warning(f"Error cleaning up job {job_id}: {e}")
        self._jobs.clear()
# ---
def url_dequery(url):
    """Return a URL with the query component removed.

    :param url: URL to dequery.
    :type url: str
    :rtype: str
    """
    url = urlparse(url)
    return urlunparse((url.scheme,
                                url.netloc,
                                url.path,
                                url.params,
                                '',
                                url.fragment))
# ---
def __contains__(self, key):
        try:
            self._fallback(key)
            return True
        except KeyError:
            return False
# ---
def __init__(self):
        self.queue = []
        self.leases = {}
# ---
def logs_tail(self, handle: VllmServerHandle, *, max_lines: int = 200) -> str:
        raise NotImplementedError
# ---
def intersection(self, bbox, srs):
        bbox = self._geom_in_coverage_srs(bbox, srs)
        return GeomCoverage(self.geom.intersection(bbox), self.srs)
# ---
def init(self, run_id: Optional[str]) -> Tracker:
        raise NotImplementedError
# ---
def test_activation(self):
    # with string argument
    testing_utils.layer_test(
        keras.layers.Activation,
        kwargs={'activation': 'relu'},
        input_shape=(3, 2))

    # with function argument
    testing_utils.layer_test(
        keras.layers.Activation,
        kwargs={'activation': keras.backend.relu},
        input_shape=(3, 2))
# ---
def log1p(a: A) -> A:
    return wrap_elemwise_unary(jnp.log1p, a)
# ---
def block_function(out_key):
        return ((x.name,) + out_key[1:],)
# ---
def empty_list(length):
 empty_list = [{} for _ in range(length)]
 return empty_list
# ---
def __add__(self, other):
        assert self.n_classes == other.n_classes, "Number of classes does not match"
        return MulticlassContingencyTable(self.table + other.table,
                                          n_classes=self.n_classes,
                                          class_names=self.class_names)
# ---
def process_bind_param(self, value, dialect):
                raise nope
# ---
def eliminate_axes(axis_spec: ShapeDict, axes: AxisSelection) -> ShapeDict:  # type: ignore
    ...
# ---
def test_nested_field_missing(self):
        expr = col("meta")["score"]
        assert expr.evaluate({"meta": {"other": 1}}) is None
# ---
def restoreDefaults(self):
        self.model.restore_defaults()
# ---
def test_cache(self):
        conn = testing.db.connect()
        cache = {}
        cached_conn = conn.execution_options(compiled_cache=cache)

        ins = users.insert()
        cached_conn.execute(ins, {'user_name':'u1'})
        cached_conn.execute(ins, {'user_name':'u2'})
        cached_conn.execute(ins, {'user_name':'u3'})
        assert len(cache) == 1
        eq_(conn.execute("select count(*) from users").scalar(), 3)
# ---
def func(self, *args, **kwargs):
            ds = self.coords.to_dataset()
            for k in self.data_vars:
                ds._variables[k] = f(self._variables[k], *args, **kwargs)
            return ds
# ---
def chunk_tuples(test_tup, N):
  res = [test_tup[i : i + N] for i in range(0, len(test_tup), N)]
  return (res)
# ---
def tuple_to_int(nums):
    result = int(''.join(map(str,nums)))
    return result
# ---
def _fn_fwd(x: jax.Array, labels: jax.Array, w: jax.Array):
        loss, lse = _forward(x, labels, w)
        return (loss, lse), (x, labels, w, lse)
# ---
def relationships(request):
    accounts = ripple.get_user_accounts(request.profile)
    return locals()
# ---
def c_in(self, sigma):
        return 1 / torch.sqrt(sigma**2 + self.sigma_data**2)
# ---
def register_endpoint(
        self,
        name: str,
        address: str,
        job_id: JobName,
        metadata: dict[str, str] | None = None,
    ) -> str:
        request = cluster_pb2.Controller.RegisterEndpointRequest(
            name=name,
            address=address,
            job_id=job_id.to_wire(),
            metadata=metadata or {},
        )
        response = self._client.register_endpoint(request)
        return response.endpoint_id
# ---
def _init_buffers():
        new_model = lm_model_cls.init(Vocab, config, key=key)

        def select_if_missing(missing_leaf, new_value):
            if isinstance(missing_leaf, jax.ShapeDtypeStruct):
                return new_value
            else:
                return None

        return jax.tree.map(select_if_missing, dtype_structs, new_model, is_leaf=lambda x: x is None)
# ---
def kill():
    backProc.terminate()
    return 'killed: ' + str(backProc.pid)
# ---
def setUp(self):
        testdata.run()
# ---
def score_autocomplete(stack):
    points_autocomplete = {')': 1, ']': 2, '}': 3, '>': 4}
    s_auto = 0

    for char in stack:
        s_auto *= 5
        s_auto += points_autocomplete[char]

    return s_auto
# ---
def test_simple(self, tmpfile):
        fd = tmpfile.fileno()
        cap = capture.FDCapture(fd)
        data = tobytes("hello")
        os.write(fd, data)
        s = cap.snap()
        cap.done()
        assert not s
        cap = capture.FDCapture(fd)
        cap.start()
        os.write(fd, data)
        s = cap.snap()
        cap.done()
        assert s == "hello"
# ---
def _is_lora_compatible_module(module):
    # TODO: more modules
    return isinstance(module, hnn.Linear)
# ---
def brailleFlashTimeValueChanged(self, widget):
        self.prefsDict["brailleFlashTime"] = widget.get_value_as_int() * 1000
# ---
def total_nbytes_written(self) -> int:
        """Return the total number of bytes written for all materialized arrays in this plan."""
        nbytes = 0
        for _, d in self.dag.nodes(data=True):
            if d.get("type") == "array":
                target = d["target"]
                if isinstance(target, LazyZarrArray):
                    nbytes += target.nbytes
        return nbytes
# ---
def worker(mock_bundle_cache, mock_image_cache, mock_runtime):
    """Create Worker with mocked dependencies."""
    config = WorkerConfig(
        port=0,
        port_range=(50000, 50100),
    )
    return Worker(
        config,
        bundle_provider=mock_bundle_cache,
        image_provider=mock_image_cache,
        container_runtime=mock_runtime,
    )
# ---
def test_dicts_loop(benchmark: Any, in_memory_table: pa.Table) -> None:
    """
    Benchmarks Python Memory -> List[dict] -> Python Loop calls Rust per item -> List[dict].
    """

    def _pipeline() -> int:
        return len([dupekit.process_dicts_loop(d) for d in in_memory_table.to_pylist()])

    assert benchmark(_pipeline) > 0
# ---
def __init__(self, hidden_size, out_channels, cond_dim):
    super().__init__()
    self.norm_final = LayerNorm(hidden_size)
    self.linear = nn.Linear(hidden_size, out_channels)
    self.linear.weight.data.zero_()
    self.linear.bias.data.zero_()

    self.adaLN_modulation = nn.Linear(cond_dim,
                                      2 * hidden_size,
                                      bias=True)
    self.adaLN_modulation.weight.data.zero_()
    self.adaLN_modulation.bias.data.zero_()
# ---
def test_alibi_attention_compared_to_hf():
    import torch
    from transformers.models.bloom.modeling_bloom import build_alibi_tensor

    L, H = hax.make_axes(L=1, H=16)

    # Returns tensor shaped (batch_size * num_heads, 1, max_seq_len)
    torch_tensor = (
        build_alibi_tensor(torch.ones(1, L.size), H.size, dtype=torch.float32).numpy().reshape(H.size, L.size)
    )

    hax_tensor = np.array(alibi_attention_bias(H, L).array)

    assert np.allclose(torch_tensor, hax_tensor)
# ---
def update_floatingip_precommit(self, context, fip_context):
        pass
# ---
def addDirectory( self, path, force = False, rpc = '', url = '', timeout = None ):
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.addDirectory( path, force )
# ---
def __init__(self, training_worker_config):
        super().__init__(training_worker_config)
        self.training_worker_config = training_worker_config

        self.steps_completed = 0
        self.losses = []
        self.trained_model = None
        self.reference_model = None
        self.all_steps_seen = []
# ---
def __init__(self, matcher):
        self._matcher = matcher
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        return 1
# ---
def __init__(self, shape):
    self.shape = shape
# ---
def run_inference_mode(args):
    """Run in inference worker mode."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger = logging.getLogger("rollout_worker")

    logger.info("Starting inference worker mode...")

    # cleanup()
    worker_config = llama_small_rollout_worker_config(CHECKPOINT_DIR, str(RUN_ID))
    worker = RolloutWorker(
        config=worker_config,
    )

    worker.run()
    logger.info("Inference worker completed")
# ---
def erase(self):
		self.truncate(0)
		self.seek(0, 0)
		return
# ---
def __init__(
        self,
        cpu: int = 1000,
        memory_gb: int = 1000,
        attributes: dict[str, str | int | float] | None = None,
        device: cluster_pb2.DeviceConfig | None = None,
    ):
        self._cpu = cpu
        self._memory_gb = memory_gb
        self._attributes = attributes or {}
        self._device = device
# ---
def gmm_impl(lhs, rhs, group_sizes):
        return gmm_sharded(lhs.array, rhs.array, group_sizes.array, ar=ar)
# ---
def _fail():
    raise RuntimeError("intentional failure")
# ---
def scan_fn(_, x):
            ref_slice = ref.slice({"x": x})
            ref_slice[...] = (x * x).astype(ref_slice.dtype)
            return None, x * 2
# ---
def output(self):
        return luigi.s3.S3Target('{}/{}/{}/YEAR={}/{}.psv'.format(self.root_path,
                                                                  self.etl_path,
                                                                  self.task_name,
                                                                  self.year,
                                                                  str(self.month).zfill(2)))
# ---
def test_show(self):
        resp = FakeResponse()
        self.type_action_controller.show(self.req, resp, fake.VOLUME_TYPE_ID)
        self.assertEqual({'id': fake.VOLUME_TYPE_ID,
                          'os-volume-type-access:is_public': True},
                         resp.obj['volume_type'])
# ---
def scientific_papers_detokenizer(x):
  x = wt_detokenizer(x)
  x = lm1b_detokenizer(x)
  return x
# ---
def get_indexable(self):
        raise NotImplementedError
# ---
def test_list_type_with_no_admin_default(self):
        expected = {'volume_types': [{'id': fake.VOLUME_TYPE_ID},
                                     {'id': fake.VOLUME_TYPE2_ID}]}
        req = fakes.HTTPRequest.blank('/v2/%s/types' % fake.PROJECT_ID,
                                      use_admin_context=False)
        result = self.type_controller_v2.index(req)
        self.assertVolumeTypeListEqual(expected['volume_types'],
                                       result['volume_types'])
# ---
def test_metric_fold_associativity(reduction):
    """fold is associative for all reduction types."""
    m1 = Metric.from_value(10.0, reduction)
    m2 = Metric.from_value(20.0, reduction)
    m3 = Metric.from_value(30.0, reduction)

    # (m1 + m2) + m3
    result1 = fold(fold(m1, m2), m3)

    # m1 + (m2 + m3)
    result2 = fold(m1, fold(m2, m3))

    assert jnp.allclose(result1.value(), result2.value())
# ---
def Diff(li1,li2):
    return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))
# ---
def extract_data(pipeline_response):
            deserialized = self._deserialize("PolicyAssignmentListResult", pipeline_response)
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)
            return deserialized.next_link or None, iter(list_of_elem)
# ---
def _f32(x):
            return np.asarray(x, dtype=np.float32)
# ---
def _count_reduce(key: str, items: Iterator[pa.StructScalar], *, canonical_id: str) -> DupeReduceResult:
    head = next(items)
    doc_cnt = sum(map(lambda _: 1, items)) + 1
    if doc_cnt == 1:
        return {
            "hash": None,
            "cnt": 1,
            "canonical": None,
        }

    return {
        "hash": key,
        "cnt": doc_cnt,
        "canonical": head[canonical_id],
    }
# ---
def get_lm_head(self) -> hax.NamedArray:
        return self.embeddings.token_embeddings.weight if self.lm_head is None else self.lm_head.weight
# ---
def ohc_map(ohc_intz):
    # return last 1 year - first 1 year
    return ohc_intz.isel(time=slice(-73, None)).mean("time") - ohc_intz.isel(
        time=slice(0, 73)
    ).mean("time")
# ---
def pytest_addoption(parser):
    parser.addoption(
        "--runcloud", action="store_true", default=False, help="run cloud tests"
    )
    parser.addoption(
        "--runslow", action="store_true", default=False, help="run slow tests"
    )
# ---
def test_start_with_ellipsis():
    partial_order = (..., "apple", "banana")
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("cherry", "apple", "banana")
# ---
def reset(self):
        return DecodeState.init(
            page_table=self.page_table.reset(),
            pad_token_id=self.pad_token_id,
            max_stop_seqs=self.stop_tokens.shape["stop_seq"] if self.stop_tokens is not None else 0,
            max_stop_tokens=self.stop_tokens.shape["position"] if self.stop_tokens is not None else 0,
            max_queued_tokens=self.tqueue.max_queued_tokens,
        )
# ---
def common_pile_tokenized(*, tokenizer: str = llama3_tokenizer) -> dict[str, TokenizerStep]:
    """Return tokenization steps for the Common Pile filtered datasets."""
    tokenized: dict[str, TokenizerStep] = {}
    for dataset, step in COMMON_PILE_DATASETS.items():
        tokenized[f"common_pile/{dataset}"] = default_tokenize(
            name=f"common_pile/{dataset}",
            dataset=step,
            tokenizer=tokenizer,
        )
    return tokenized
# ---
def created_at_ms(self) -> int:
        """Timestamp when this VM group was created (milliseconds since epoch)."""
        ...
# ---
def choose_match(self, event):
        self.match_control.set_match(event.GetClientData())
# ---
def _on_Server(self, host, port, use_ssl, connect_timeout, get_info=None,
                   tls=None):
        # mangle request packet

        return "FakeServerObject"
# ---
def test_sigv4(self):
        CliRunner().invoke(rubberjack, ['--sigv4-host', 'foo', 'deploy'], catch_exceptions=False)
# ---
def editingKey(self, cell, editable, path, treeModel):
        """Starts user input of a Key for a selected key binding"""

        self._presentMessage(messages.KB_ENTER_NEW_KEY)
        orca_state.capturingKeys = True
        editable.connect('key-press-event', self.kbKeyPressed)
        return
# ---
def test_list_migrations(self):
        """Test admin user can get the migrations list"""
        self.client.list_migrations()
# ---
def wait_async_writes():
        w_write_future.wait()
# ---
def f(olmo2_model, input_ids, mask):
        out = olmo2_model(input_ids, mask)
        return hax.sum(out).scalar()
# ---
def mathml_to_markdown(mathml_node):
    converter = MathMLToLatex()
    return converter.convert(mathml_node)
# ---
def __exit__(self, *args):
        self.stop()
        self.reset()
# ---
def _last_or_fail(self, psm: PSM):
        if self.parent.g.group.seq:
            return self.parent.g.group.seq[-1]
        else:
            psm.error = "nothing to repeat"
# ---
def merge_after(self, obj):
        """
        Merge with another CmdText object by appending the input objects content.
        """
        self.lines
# ---
def _shardmap_histogram(a: NamedArray, bins):
    spec = hax.partitioning.pspec_for_axis(a.axes)
    flattened_spec = _flattened_spec(spec)

    def _wrapped_hist(arr):
        return _single_shard_histogram(arr, bin_edges=bins, reduce_mesh=flattened_spec)

    shard_h = shard_map(_wrapped_hist)
    res = shard_h(a)

    # the filter misses the last bin, so we need to add it
    if res.size >= 1:
        res = res.at[-1].add(1)
    return res
# ---
def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Get the underlying model (handles DDP wrapping)
        return self._underlying.forward_once(x, self._ctx)
# ---
def test_split_into():
    assert list(split_into([1, 2, 3, 4, 5, 6], [1, 2, 3])) == [[1], [2, 3], [4, 5, 6]]
    assert list(split_into([1, 2, 3, 4, 5, 6], [2, 3])) == [[1, 2], [3, 4, 5]]
    assert list(split_into([1, 2, 3, 4], [1, 2, 3, 4])) == [[1], [2, 3], [4], []]
# ---
def __init__(
        self,
        distance_gaussian_dim: int,
        token_z: int,
        out_dim: int,
    ):
        super().__init__()
        self.distance_gaussian_smearing = GaussianSmearing(
            start=0.0, stop=2.0, num_gaussians=distance_gaussian_dim
        )
        input_dim = distance_gaussian_dim + 1 + token_z
        self.distance_token_bias_trans = Transition(
            dim=input_dim, hidden=token_z, out_dim=out_dim
        )
# ---
def test_full_end_to_end_cache():
    td = tempfile.TemporaryDirectory()
    with td as tmpdir:
        cache = build_or_load_cache(
            tmpdir,
            SimpleShardSource(num_shards=15),
            TestProcessor(),
        )

        expected = simple_process(TestProcessor(), SimpleShardSource(num_shards=15))

        all_data = cache[:]

        check_datasets_equal(all_data, expected)
# ---
def reserved_mem(executor):
    res = cubed.measure_reserved_mem(executor) * 1.1  # add some wiggle room
    return round_up_to_multiple(res, 10_000_000)
# ---
def test_1dim_distance():
    """See if this contraption works in 1 dimension"""
    num1 = random.random()
    num2 = random.random()
    assert kmeans.ndim_euclidean_distance(num1, num2) == abs(num1-num2)
# ---
def imageTag(self, imgName) :
        imgBuilder = self.images.get(imgName, None)
        if imgBuilder :
            return imgBuilder.buildTag()
        return None
# ---
def __str__(self):
        return "NoncentralChiSquare(df={0},lambda={1})#{2}".format(self.df, self.lmbda, self.id())
# ---
def __eq__(self, other):
        if not isinstance(other, BBOXCoverage):
            return NotImplemented

        if self.srs != other.srs:
            return False

        if self.bbox != other.bbox:
            return False

        return True
# ---
def matchfn(self, f):
        return self._m1(f) and self._m2(f)
# ---
def gridstr(src: DataSource) -> str:
    """Makes a human-readable string about the grid from a DataSource."""
    grid = src.grid_size
    return f"{grid[0]}x{grid[1]}"
# ---
def double_click(self, on_element):
        """Double-clicks an element.
        Args:
            on_element: The element to double-click.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.DOUBLE_CLICK, {}))
        return self
# ---
def convert_pre(self, el, text, convert_as_inline):
        if not text:
            return ""
        code_language = self.options["code_language"]

        if self.options["code_language_callback"]:
            code_language = self.options["code_language_callback"](el) or code_language

        if "```" in text:  # have to use <pre>
            return f"\n<pre><code>{text}</code></pre>\n"
        else:
            return f"\n```{code_language}\n{text}\n```\n"
# ---
def test_anonymous_get_requests_redirected_to_index(self):
        self.client.logout()

        response = self.client.get(reverse('results-export'), follow=True)
        self.assertRedirects(response, '/?next=%2Fadmin%2Fresults')
# ---
def _environment(self):
        # Provide some sane environment
        env = os.environ.copy()

        # virt-v2v specific variables
        env['LIBGUESTFS_BACKEND'] = 'direct'
        if 'virtio_iso_path' in self._vminfo:
            env['VIRTIO_WIN'] = self._vminfo['virtio_iso_path']
        return env
# ---
def __getitem__(self, index):
        """Get the value at the given index.

        Parameters
        ----------
        index : int
            The index into the array.

        """
        return self._data[index]
# ---
def add_hop(self, actor_id: str):
        """Record this actor in the path."""
        self.path.append(actor_id)
# ---
def num_rows(self):
        if self._cached_num_rows is not None:
            return self._cached_num_rows
        result = int(self.offsets[0].read().result())
        if self._cache_metadata:
            self._cached_num_rows = result
        return result
# ---
def __init__(
        self,
        controller_address: str,
        timeout: float = 5.0,
        namespace: Namespace | None = None,
    ):
        self._address = controller_address.rstrip("/")
        self._timeout = timeout
        self._explicit_namespace = namespace
        self._client = ControllerServiceClientSync(
            address=self._address,
            timeout_ms=int(timeout * 1000),
        )
# ---
def send_token(self, token: Token) -> str:
        """Queue token for processing - returns immediately."""
        self._queue.put(token)
        return "queued"
# ---
def test_deadline_expires():
    """Deadline with short timeout expires after sleeping."""
    deadline = Deadline.from_seconds(0.1)
    time.sleep(0.15)
    assert deadline.expired()
# ---
def check_short_name(short_name_raw: str) -> str:
    short_name = short_name_raw.strip()
    if len(short_name) == 0:
        raise JsonableError(_("Bad name or username"))
    return short_name
# ---
def setUpClass(cls):
        if os.path.exists(db):
            os.remove(db)
        cls.conn = sqlite3.connect(db, detect_types=sqlite3.PARSE_DECLTYPES)
        st0 = option.CsvStore(kid='/base/')
        st0.merge_file(c1)
        st0.validate()
        cls.desc = st0.desc
# ---
def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / ((lr / min_lr - 1) * jnp.maximum(count, 1) / decay_steps + 1))
        return jnp.maximum(lr * decay, min_lr)
# ---
def test_main(verbose=None):
    from test import test_urllib2
    support.run_doctest(test_urllib2, verbose)
    support.run_doctest(urllib.request, verbose)
    tests = (TrivialTests,
             OpenerDirectorTests,
             HandlerTests,
             MiscTests,
             RequestTests)
    support.run_unittest(*tests)
# ---
def test_ragged_paged_attention_multi_seq(seq_lens):
    rng = jr.PRNGKey(hash(tuple(seq_lens)))
    q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs = _build_random_case(rng, seq_lens)

    ragged = jit_rpa(q, kv_pages, kv_lens, page_indices, cu_q_lens, num_seqs, sm_scale=SM_SCALE)
    ref = _reference_attention(q, kv_pages, kv_lens, page_indices, cu_q_lens, seq_lens)

    assert ragged.axes == ref.axes
    tol = _rpa_tol()
    assert_trees_all_close(ragged.array, ref.array, atol=tol, rtol=tol)
# ---
def _invert_results(self, candidates):
        inverted_candidates = list(self.directory)

        for candidate in candidates:
            try:
                inverted_candidates.remove(candidate)
            except ValueError:
                pass

        return inverted_candidates
# ---
def _zeros_like_if_needed(ct, like):
    if isinstance(ct, ad_util.Zero):
        return jnp.zeros_like(like)
    return ct
# ---
def beta_step_scale_schedule(self, num_sampling_steps=None):
        t = np.linspace(0, 1, num_sampling_steps)
        beta_cdf_weights = torch.from_numpy(
            beta.cdf(t, self.step_scale_alpha, self.step_scale_beta)
        )
        return (
            self.min_step_scale
            + (self.max_step_scale - self.min_step_scale) * beta_cdf_weights
        )
# ---
def list_tasks(self, job_id: JobName) -> list[cluster_pb2.TaskStatus]:
        return self._remote_client.list_tasks(job_id)
# ---
def main():
    spec = ArgumentSpec()

    module = AnsibleModule(
        argument_spec=spec.argument_spec,
        supports_check_mode=spec.supports_check_mode,
        required_if=spec.required_if
    )

    try:
        mm = ModuleManager(module=module)
        results = mm.exec_module()
        module.exit_json(**results)
    except F5ModuleError as ex:
        module.fail_json(msg=str(ex))
# ---
def get_strength(self, nick):
        return self.data[nick.lower()]['strength']
# ---
def convert_sub(self, el, text, convert_as_inline):
        if not text:
            return ""
        return f"<sub>{text}</sub>"
# ---
def load_module(old, name):
        module = old(name)
        if module.__name__ in _DISTUTILS_PATCH:
            patch_dist(module)
        return module
# ---
def _span(values: Iterable[float]) -> float:
    seq = list(values)
    return max(seq) - min(seq)
# ---
def test_column_definition(self):
        s = option.SqlStore()
        print(s.column_definition(go('Integer'))[1])
        print(s.column_definition(go('String'))[1])
        print(s.column_definition(go('Point'))[1])
        print(s.column_definition(go('Role'))[1])
        print(s.column_definition(go('RoleIO'))[1])
        print(s.column_definition(go('Log'))[1])
        print(s.column_definition(go('Meta'))[1])
# ---
from typing import List


def string_xor(a: str, b: str) -> str:
    """ Input are two strings a and b consisting only of 1s and 0s.
    Perform binary XOR on these inputs and return result also as a string.
    >>> string_xor('010', '110')
    '100'
    """
    def xor(i, j):
        if i == j:
            return '0'
        else:
            return '1'

    return ''.join(xor(x, y) for x, y in zip(a, b))
# ---
def segComplete(self, seg):
        if ( seg == None ): return

        if ( seg.data ): 
            data_size = len("".join(seg.data))

            current_time = time.time()
            if ( (current_time - self.speedTime) > 1 ):
                self.status.kbps = self.speedCounter
                self.speedCounter = 0
                self.speedTime = current_time
            else:
                self.speedCounter += (data_size/1024)

            self.cache.append(seg)
# ---
def test_transaction_engine_ctx_rollback(self):
        fn = self._trans_rollback_fn()
        ctx = testing.db.begin()
        assert_raises_message(
            Exception,
            "breakage",
            testing.run_as_contextmanager, ctx, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def get_status(self) -> vm_pb2.AutoscalerStatus:
        """Build status for the status API."""
        from iris.rpc import time_pb2

        return vm_pb2.AutoscalerStatus(
            groups=[g.to_status() for g in self._groups.values()],
            current_demand={g.name: g.current_demand for g in self._groups.values()},
            last_evaluation=time_pb2.Timestamp(epoch_ms=0),  # Controlled by controller now
            recent_actions=list(self._action_log),
        )
# ---
def is_finite(self) -> bool:
        """
        Returns whether the dataset will have a known length in the future (e.g. if it's being constructed).
        If this returns False, the length of the dataset is infinite or unknowable.
        """
        raise NotImplementedError
# ---
def main():
    try:
        conf = config.get_config_object()
        paste_file = conf.find_file(conf.paste_deploy.config_file)
        wsgi_app = os_pastedeploy.paste_deploy_app(paste_file,
                                                   'staccato-api',
                                                   conf)
        server = os_wsgi.Service(wsgi_app, conf.bind_port)
        server.start()
        server.wait()
    except RuntimeError as e:
        fail(1, e)
# ---
def test_impl(df):
            df['B'] = df.A.map(lambda a: 2 * a)
            return
# ---
def the_object_name_has_number_vertices(name, number):
    total = len(the_object_name_exists(name).data.vertices)
    assert total == int(number), f"We found {total} vertices"
# ---
def get_memory_mb(self) -> int | None:
        if not self.resources or not self.resources.memory_bytes:
            return None
        return self.resources.memory_bytes // (1024 * 1024)
# ---
def _pack_requests(
    requests: list[Instance],
    tokenizer: HfTokenizer,
    Pos: hax.Axis,
    max_pack_size: int,
) -> list[LmExample]:
    packed_iterator = _iterate_tokenized_requests(requests, tokenizer, Pos.size, batch_size=128)
    # TODO: use a better packing algorithm?
    return greedy_pack_prompt_completions(
        Pos,
        packed_iterator,
        max_segments_per_example=max_pack_size,
        pad_token=tokenizer.pad_token_id,
    )
# ---
def load_lm_dataset_cache(
    cache_dir: str,
    format: LmDatasetFormatBase,
    tokenizer: HfTokenizer,
    enforce_eos: bool = True,
) -> TreeCache[dict]:
    """Load an existing cache, raising if not present."""
    processor = preprocessor_for_format(format, tokenizer, enforce_bos=True, enforce_eos=enforce_eos)
    cache = TreeCache.load(
        cache_dir,
        exemplar=processor.output_exemplar,
        options=CacheMetadata(preprocessor_metadata=processor.metadata),
    )
    return cache
# ---
def discriminant_value(x,y,z):
    discriminant = (y**2) - (4*x*z)
    if discriminant > 0:
        return ("Two solutions",discriminant)
    elif discriminant == 0:
        return ("one solution",discriminant)
    elif discriminant < 0:
        return ("no real solution",discriminant)
# ---
def backend(request) -> TrainBackendConfig:
    return request.param
# ---
def run(self, fn: Callable, *args, name: str | None = None) -> Future | GeneratorFuture:
        """Submit function to thread pool, returning GeneratorFuture for generator functions."""
        if inspect.isgeneratorfunction(fn):
            future = self.executor.submit(lambda: list(fn(*args)))
            return GeneratorFuture(future)
        else:
            return self.executor.submit(fn, *args)
# ---
def discover_vm_groups(self) -> list[VmGroupProtocol]:
        """Find and adopt existing VM groups from cloud.

        Called once at startup to recover state from a previous controller.
        Returns ready-to-use VmGroup objects with their VMs already started.

        Returns:
            List of discovered VmGroup objects
        """
        ...
# ---
def make_from_hf_config(cls, rope_theta: float, config: dict) -> "YarnRotaryEmbeddingsConfig":
        return YarnRotaryEmbeddingsConfig(
            theta=rope_theta,
            factor=float(config.get("factor", 1.0)),
            beta_fast=float(config.get("beta_fast", 32.0)),
            beta_slow=float(config.get("beta_slow", 1.0)),
            original_max_position_embeddings=int(config.get("original_max_position_embeddings", 2048)),
            mscale=float(config.get("mscale", 1.0)),
        )
# ---
def big_diff(nums):
     diff= max(nums)-min(nums)
     return diff
# ---
def log_sigmoid(a: A) -> A:
    return wrap_elemwise_unary(jnn.log_sigmoid, a)
# ---
def test_with_exception(self, mock_start, mock_stop):

        self.assertRaises(ValueError, test_fn_exc)
        expected_info = {
            "function": {
                "name": "osprofiler.tests.unit.test_profiler.test_fn_exc"
            }
        }
        expected_stop_info = {"etype": "ValueError", "message": ""}
        mock_start.assert_called_once_with("foo", info=expected_info)
        mock_stop.assert_called_once_with(info=expected_stop_info)
# ---
def save(self, context):
        """In addition to save the pf, it should also save the
        vfs associated with this pf
        """
        # To ensure the saving type is PF
        if self.type != 'pf':
            raise exception.InvalidDeployType()

        for exist_vf in self.virtual_function_list:
            exist_vf.save(context)
        super(PhysicalFunction, self).save(context)
# ---
def deterministic_hash(obj: object) -> int:
    """Compute a deterministic hash for an object."""
    s = msgspec.msgpack.encode(obj, order="deterministic")
    return zlib.adler32(s)
# ---
def transform_to(self, srs):
        return MultiCoverage([c.transform_to(srs) for c in self.coverages])
# ---
def rotate_half(x):
  x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]
  return torch.cat((-x2, x1), dim=-1)
# ---
def __repr__(self) -> str:
        return f"cubed.storage.zarr.LazyZarrArray<shape={self.shape}, dtype={self.dtype}, chunks={self.chunks}>"
# ---
def interleave_lists(list1,list2,list3):
    result = [el for pair in zip(list1, list2, list3) for el in pair]
    return result
# ---
def mk_LayerNorm(self, axis: AxisSpec) -> LayerNormBase:
        return hnn.RmsNorm.init(
            axis, eps=self.layer_norm_epsilon, use_weight=self.use_layer_norm_weight, use_bias=self.use_bias
        )
# ---
def log_softmax(a: A, axis: AxisSelection | None = None) -> A:
    return wrap_axiswise_call(jnn.log_softmax, a, axis=axis, single_axis_only=False)
# ---
def switch_func(txn):
            txn.description, txn.payee = txn.payee, txn.description
# ---
def test_metric_jit():
    """Metrics work through JIT."""

    @jax.jit
    def fold_metrics_jit(m1, m2):
        return fold(m1, m2)

    m1 = Metric.from_value(10.0, ReductionType.SUM)
    m2 = Metric.from_value(20.0, ReductionType.SUM)
    result = fold_metrics_jit(m1, m2)

    assert jnp.allclose(result.value(), 30.0)
# ---
def __init__(self):
                self.inner = MyModuleInit()
# ---
def emit(self, record: logging.LogRecord) -> None:
        self._buffer.append(
            BufferedLogRecord(
                timestamp=record.created,
                level=record.levelname,
                logger_name=record.name,
                message=self.format(record),
            )
        )
# ---
def k(self):
        html = "<h2>Stopping</h2>"
        killall()
        return html
# ---
def get_unit (self, uids) :
        """
        Reconnect to and return (Compute or Data)-Unit object(s)
        """

        raise Exception ("%s.get_unit() is not implemented" % self.__class__.__name__)
# ---
def destroy_router(self, process_id):
        pass
# ---
def in_jit(x, pspec):
        if isinstance(x, hax.NamedArray):
            arr = x.array
        else:
            arr = x
        arr = jax.lax.with_sharding_constraint(arr[valid_device_for_process], pspec)

        if isinstance(x, hax.NamedArray):
            return hax.named(arr, x.axis_names)
        else:
            return arr
# ---
def beta_noise_scale_schedule(self, num_sampling_steps):
        t = np.linspace(0, 1, num_sampling_steps)
        beta_cdf_weights = torch.from_numpy(
            beta.cdf(1 - t, self.noise_scale_alpha, self.noise_scale_beta)
        )
        return (
            self.max_noise_scale
            + (self.min_noise_scale - self.max_noise_scale) * beta_cdf_weights
        )
# ---
def test_extend_with_multiple(cache_metadata):
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=2, dtype=jnp.float32, cache_metadata=cache_metadata)

        data1 = jnp.array([[1.0, 2.0], [3.0, 4.0]])
        data2 = jnp.array([[5.0]])

        builder.extend([data1, data2])

        assert len(builder) == 2

        result1 = builder[0]
        assert jnp.all(result1 == data1)

        result2 = builder[1]
        assert jnp.all(result2 == data2)
# ---
def _parse_arrow_schema(self):
        raise NotImplementedError("Not implemented.")
# ---
def KHeadDim(self) -> Axis:
        return Axis("k_head_dim", self.head_k_dim)
# ---
def _check_value(value):
    """ Return ``value``, or call its getter if ``value`` is a :class:`SpecialValue`. """
    return value.get() if isinstance(value, SpecialValue) else value
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)
        return ScaleByMuonState(momentum_buffer=momentum_buffer)
# ---
def __getitem__(self, key):
        return self._dict[key]
# ---
def _is_float(num: str) -> bool:
    try:
        float(num)
        return True
    except (ValueError, OverflowError):
        return False
# ---
def simple_process(processor, source):
    result = []
    for shard_name in source.shard_names:
        for batch in source.open_shard(shard_name):
            result.append(processor([batch])[0])

    return result
# ---
def test_connection_host(self):
        """Connection can access host of endpoint"""
        urlparse(self.dynamo.host)
# ---
def test_iterators_are_a_type(self):
        it = iter(range(1,6))

        total = 0

        for num in it:
            total += num

        self.assertEqual(15 , total)
# ---


def truncate_number(number: float) -> float:
    """ Given a positive floating point number, it can be decomposed into
    and integer part (largest integer smaller than given number) and decimals
    (leftover part always smaller than 1).

    Return the decimal part of the number.
    >>> truncate_number(3.5)
    0.5
    """
    return number % 1.0
# ---
def testRaiseInstance(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        try:
          raise RuntimeError('foo')
          print 'bad'
        except RuntimeError as e:
          print e""")))
# ---
def peak_demand(self) -> int:
        """Peak demand seen."""
        return self._peak_demand
# ---
def __init__(self, port_range: tuple[int, int] = (30000, 40000)):
        self._range = port_range
        self._allocated: set[int] = set()
        self._lock = threading.Lock()
# ---
def _get(delta, start):
            p = start
            while delta:
                p = p.next
                delta -= 1
            return p.val
# ---
def __len__(self) -> int:
        return self.size
# ---
def center(atom_coords, atom_mask):
    atom_mean = torch.sum(
        atom_coords * atom_mask[:, :, None], dim=1, keepdim=True
    ) / torch.sum(atom_mask[:, :, None], dim=1, keepdim=True)
    atom_coords = atom_coords - atom_mean
    return atom_coords
# ---
from typing import List


def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:
    """ Filter an input list of strings only for ones that start with a given prefix.
    >>> filter_by_prefix([], 'a')
    []
    >>> filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a')
    ['abc', 'array']
    """
    return [x for x in strings if x.startswith(prefix)]
# ---
def __init__(self, opts):
        Key.__init__(self, opts)
        self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)
        self.serial = salt.payload.Serial(self.opts)
# ---
def axes(self) -> tuple[Axis, ...]:
        """Axes visible from this view after applying staged selectors."""
        return _axes_after_prefix(self._axes, self._prefix)
# ---
def reset():
            model.solver.objective = reverse_value
            model.solver.objective.direction = reverse_value.direction
# ---
def shard_map(
    *,
    in_specs: Any = None,
    out_specs: Any = None,
    mesh: Mesh | None = None,
    axis_mapping: ResourceMapping | None = None,
    check_rep: bool = False,
    **kwargs,
) -> typing.Callable[[Callable[Args, R]], Callable[Args, R]]: ...
# ---
def test_not(self):
        expr = ~(col("flag") == True)  # noqa: E712
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def __abs__(self) -> "NamedArray":  # pragma: no cover
        return haliax.absolute(self)
# ---
def _balance_Q(Q):
                        norms = jnp.array([jnp.max(jnp.abs(q)) for q in Q], dtype=jnp.float32)
                        gmean = jnp.exp(jnp.mean(jnp.log(norms)))
                        to_mul = gmean / norms
                        return [q * x.astype(q.dtype) for q, x in zip(Q, to_mul)]
# ---
def dem_threshold(domain, b):
    '''Just use a height threshold on the DEM!'''

    heightLevel = float(domain.algorithm_params['dem_threshold'])
    dem         = domain.get_dem().image
    return dem.lt(heightLevel).select(['elevation'], ['b1'])
# ---
def fn(config: Config):
            time.sleep(config.wait)
            with open(config.path, "r") as f:
                number = int(f.read())
            with open(config.path, "w") as f:
                f.write(str(number + config.number))
# ---
def __repr__(self):
        return '<{0.__class__.__name__!s} {0.verb}>'.format(self)
# ---
def test_random_seed(spec, executor):
    random.seed(42)
    a = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    a_result = a.compute(executor=executor)

    random.seed(42)
    b = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    b_result = b.compute(executor=executor)

    assert_array_equal(a_result, b_result)
# ---
def _teardown(self, file_name):
        if path.isfile(file_name):
            try:
                remove(file_name)
            except:
                pass
# ---
def test_makes_rectangular_patches():
    x = torch.randn(1, 10, 4, 8)

    patch_embed = PerceiverEncoder(
        in_channels=10,
        out_channels=4,
        patch_size=(4, 2),
        perceiver=make_perceiver(10, 4),
        lat=torch.linspace(start=-90, end=90, steps=x.shape[-2]),
        lon=torch.linspace(start=0, end=360, steps=x.shape[-1]),
    )

    patches = patch_embed(x)

    assert patches.shape == (
        1,
        4,
        1,
        4,
    )
# ---
def VHeadSize(self) -> Axis:
        return Axis("v_head_dim", self.v_head_dim)
# ---
def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]
# ---
def quit(self, code=None):
        self.command('quit', code)
# ---
def validate_is_stock_item(item_code, is_stock_item=None, verbose=1):
	if not is_stock_item:
		is_stock_item = frappe.db.get_value("Item", item_code, "is_stock_item")

	if is_stock_item != 1:
		msg = _("Item {0} is not a stock Item").format(item_code)

		_msgprint(msg, verbose)
# ---
def status(self) -> ControllerStatus:
        if self._controller:
            return ControllerStatus(
                running=True,
                address=self._controller.url,
                healthy=True,
            )
        return ControllerStatus(running=False, address="", healthy=False)
# ---
def __init__(self, shape, temperature=1.0):
    super().__init__(shape)
    self.temperature = temperature
# ---
def sum_positivenum(nums):
  sum_positivenum = list(filter(lambda nums:nums>0,nums))
  return sum(sum_positivenum)
# ---
def __init__(self, parent=None, datafolder=None):
        """
        Constructor
        """
        QtGui.QDialog.__init__(self, parent)

        # self.filelist = filelist
        self.datafolder = datafolder

        # labels font
        self.font_labels = QtGui.QFont("Arial", 12, QtGui.QFont.Bold)
        self.font_edits = QtGui.QFont("Arial", 12)
        self.font_buttons = QtGui.QFont("Arial", 10, QtGui.QFont.Bold)


        self.setupUi(self)
        self.exec_()
# ---
def to_state_dict(tree: PyTree, prefix: str | None = None) -> StateDict:
        warnings.warn("Ignore all int8 states (if any) for now.")
        return {}
# ---
def test_random_data(self):
        data = MockData()
        a_set = data.get_random_elements(10)
        self.assertTrue(len(a_set) == 10, "the data should have 10 elements!")
# ---
def find_Max(arr,low,high): 
    if (high < low): 
        return arr[0] 
    if (high == low): 
        return arr[low] 
    mid = low + (high - low) // 2 
    if (mid < high and arr[mid + 1] < arr[mid]): 
        return arr[mid] 
    if (mid > low and arr[mid] < arr[mid - 1]): 
        return arr[mid - 1]  
    if (arr[low] > arr[mid]): 
        return find_Max(arr,low,mid - 1) 
    else: 
        return find_Max(arr,mid + 1,high)
# ---
import re
def end_num(string):
    text = re.compile(r".*[0-9]$")
    if text.match(string):
        return True
    else:
        return False
# ---
def execute_with_timing(function, *args, **kwargs):
    """Invoke function and measure timing information.

    Returns the result of the function call and a stats dictionary.
    """

    function_start_tstamp = time.time()
    result = function(*args, **kwargs)
    function_end_tstamp = time.time()
    return result, dict(
        function_start_tstamp=function_start_tstamp,
        function_end_tstamp=function_end_tstamp,
    )
# ---
def test_endpoints_exist(test_client):
    """Test that the endpoints are properly defined"""
    _, server = test_client
    routes = [route.path for route in server.app.routes]
    assert "/health" in routes
    assert "/v1/completions" in routes
    assert "/v1/chat/completions" in routes
# ---
def create_user(self, email=None, password=None, **extra_fields):
        return self._create_user(email, password, False, False,
                                 **extra_fields)
# ---
def test_check_share_in_use_no_conn(self):
        drv = self._driver
        share = drv._check_share_in_use(None, '/dir')
        if share:
            self.fail('Unexpected share detected.')
# ---
def on_lower_bound_checked_changed(self):
        if self.ui.checkBoxLowerBound.isChecked():
            self.ui.spinBoxLowerBound.setEnabled(True)
            self.ui.spinBoxBoundaryNumber.setEnabled(True)
        elif not self.ui.checkBoxUpperBound.isChecked():
            self.ui.spinBoxLowerBound.setEnabled(False)
            self.ui.spinBoxBoundaryNumber.setEnabled(False)
        else:
            self.ui.spinBoxLowerBound.setEnabled(False)
# ---
def var(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.var, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def train_batch(
        model: torch.nn.Module, batch: TrainData, loss_fn: Callable
    ) -> TrainBatchOutput:
        loss_per_channel = model(batch, loss_fn=partial(loss_fn, wet=batch.label_mask))
        loss = torch.mean(loss_per_channel)
        return TrainBatchOutput(loss, loss_per_channel)
# ---
def parse_rule(rule):
    """Parses a policy rule into a tree of Check objects."""

    # If the rule is a string, it's in the policy language
    if isinstance(rule, basestring):
        return _parse_text_rule(rule)
    return _parse_list_rule(rule)
# ---
def model_type(self):  # noqa: D401
        return Gemma3LMHeadModel
# ---
def _is_float(num: str) -> bool:
    try:
        float(num)
        return True
    except ValueError:
        return False
# ---
def Request_Information (self, Reception_ID):
        self.Step (Message = "Requesting (updated) information about reception " + str (Reception_ID))

        Data_On_Reception = self.Reception_Database.Single (Reception_ID)

        self.Step (Message = "Received information on reception " + str (Reception_ID))

        return Data_On_Reception
# ---
def __call__(self, f):
        return self.task_instance.__call__(f)
# ---
def __str__(self):
        return f'[{self.start}, {self.end})'
# ---
def test_error_attribute_issue555(testdir):
    testdir.makepyfile("""
        import sys
        def test_capattr():
            assert sys.stdout.errors == "strict"
            assert sys.stderr.errors == "strict"
    """)
    reprec = testdir.inline_run()
    reprec.assertoutcome(passed=1)
# ---
def test_lambda_multiple_inputs(self):
    ld = keras.layers.Lambda(lambda x: x[0], output_shape=lambda x: x[0])
    x1 = np.ones([3, 2], np.float32)
    x2 = np.ones([3, 5], np.float32)
    out = ld([x1, x2])
    self.assertAllEqual(out.shape, [3, 2])
# ---
def test_string_to_unresolved_conversion(self):
        """Test that strings are automatically converted to UnresolvedLocation."""
        from pydantic import BaseModel

        class TestModel(BaseModel):
            location: Location

        # Test string conversion
        model = TestModel.model_validate({"location": "data/test.zarr"})
        assert isinstance(model.location, UnresolvedLocation)
        assert model.location.path == "data/test.zarr"
# ---
def test_find_span_end_no_match():
    source = "x = 1\n"
    # Offset 99 doesn't correspond to any node.
    end = _find_span_end(source, 99)
    assert end is None
# ---
def JumpToTab( tab_number ):
  """Jump to Vim tab with corresponding number """
  vim.command( 'silent! tabn {0}'.format( tab_number ) )
# ---
def get_process_logs(self, request: cluster__pb2.Worker.GetProcessLogsRequest, ctx: RequestContext) -> cluster__pb2.Worker.GetProcessLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def test_all(spec, executor):
    a = xp.asarray(
        [[True, True, True], [True, True, True], [True, True, True]],
        chunks=(2, 2),
        spec=spec,
    )
    b = xp.all(a)
    assert b.ndim == 0
    assert b.size == 1
    assert b.compute(executor=executor)
# ---
def model_type(self) -> Type["Gpt2LMHeadModel"]:
        return Gpt2LMHeadModel
# ---
def __init__(self, username: str, tpu_name: str, tpu_type: str = "v4-8"):
        """Initialize the actor."""
        self.username = username
        self.tpu_name = tpu_name
        self.tpu_type = tpu_type
# ---
def roots(p: NamedArray | ArrayLike) -> NamedArray:
    """Named version of [jax.numpy.roots][].

    If ``p`` is not a [haliax.NamedArray][], the root axis is named ``degree``.
    """

    (arr,) = unwrap_namedarrays(p)
    result = jnp.roots(arr)
    axis = _poly_axis_from_input(p, result.shape[0])
    return NamedArray(result, (axis,))
# ---
def fold_and_compare(ax: matplotlib.axes._axes.Axes, seam_data: xr.DataArray):
        middle = len(seam_data.x) // 2
        left = seam_data.isel(y=-1, x=slice(0, middle)).data
        right_flipped = seam_data.isel(y=-1, x=slice(middle, None)).data[::-1]

        ax.plot(left)
        ax.plot(right_flipped, ls="--")
# ---
def ohc_map(ohc_intz):
    ohc_intz = remove_climatology(ohc_intz)
    # return last 1 year - first 1 year
    return ohc_intz.isel(time=slice(-73, None)).mean("time") - ohc_intz.isel(
        time=slice(0, 73)
    ).mean("time")
# ---
def __call__(self, view_func):
        def decorator(request, *args, **kwargs):
            if not request.user.has_perm(self.perm):
                raise PermissionRequired(self.perm)
            return view_func(request, *args, **kwargs)
        return checks_permissions(self.error_view)(decorator)
# ---
def _get_current_node_tpu_worker_id() -> int | None:
    """Return the TPU worker ID for the current node across Ray versions."""
    if hasattr(TPUAcceleratorManager, "_get_current_node_tpu_worker_id"):
        return TPUAcceleratorManager._get_current_node_tpu_worker_id()
    if hasattr(TPUAcceleratorManager, "get_current_node_tpu_worker_id"):
        return TPUAcceleratorManager.get_current_node_tpu_worker_id()
    raise AttributeError("TPUAcceleratorManager is missing TPU worker ID helpers")
# ---
def to_hf_batched(x):
        if len(x) == 0:
            return list(x)
        elif isinstance(x[0], Sequence) or isinstance(x[0], np.ndarray):
            if all(len(y) == len(x[0]) for y in x):
                return np.stack(x)
            else:
                return list(x)
        else:
            return x
# ---
def tiny_cfg():
    return TreeDiffusionConfig(
        vocab_size=128,
        hidden_dim=64,
        intermediate_dim=128,
        num_layers=2,
        num_heads=4,
        num_kv_heads=4,
        max_seq_len=64,
    )
# ---
def test_add_top_level_namespace(spec, executor):
    a = cubed.ones((10, 10), chunks=(10, 2), spec=spec)
    b = cubed.ones((10, 10), chunks=(2, 10), spec=spec)
    c = cubed.add(a, b)
    assert_array_equal(
        c.compute(executor=executor), np.ones((10, 10)) + np.ones((10, 10))
    )
# ---
def _normalize_weights(weights, scale=1.0):
    total = sum(weights.values())
    return {k: v / total * scale for k, v in weights.items()}
# ---
def test_load_datasets_update(self):

        dataset_code = "nipa-section1-10101-a"
        self._load_files(dataset_code)
        self.assertLoadDatasetsUpdate([dataset_code])
# ---
def save_data(self):
        try:
            f = open('data/er_nick-data.csv', 'wt')
            writer = csv.writer(f)
            for u in self.data:
                writer.writerow([u, self.data[u]['id'], self.data[u]['nick'], self.data[u]['level'], self.data[u]['strength'], self.data[u]['rank_points'], self.data[u]['citizenship']])
            f.close()
        except:
            pass
# ---
def _resolve_zones(
    group_config: config_pb2.ScaleGroupConfig,
    platform: config_pb2.GcpPlatformConfig,
) -> list[str]:
    if group_config.zones:
        return list(group_config.zones)
    if platform.default_zones:
        return list(platform.default_zones)
    if platform.zone:
        return [platform.zone]
    raise ValueError(f"No zones configured for scale group {group_config.name}")
# ---
def the_object_name_is_placed_in_the_collection_collection(name, collection):
    obj = the_object_name_exists(name)
    [c.objects.unlink(obj) for c in obj.users_collection]
    bpy.data.collections.get(collection).objects.link(obj)
# ---
def scale_group(self) -> str:
        return self._scale_group
# ---
def output_exemplar(self) -> dict:
        return self._exemplar
# ---
def test_reinitialize_some_tokens_invalid_tokens(local_gpt2_tokenizer):
    # Test with tokens not in vocabulary
    tokenizer = local_gpt2_tokenizer
    model = None

    with pytest.raises(ValueError, match="One or more tokens are not in the tokenizer vocabulary"):
        reinitialize_some_tokens(model, tokenizer, ["<mklamnfljkaf>"], jax.random.PRNGKey(0))
# ---
def test_tree_diff_syntax_error_returns_empty():
    assert tree_diff("invalid{{{", "x = 1\n") == []
    assert tree_diff("x = 1\n", "invalid{{{") == []
# ---
def mean_groupby_reduction(x, by, axis, num_groups):
    intermediate_dtype = [("n", nxp.int64), ("total", nxp.float64)]
    dtype = x.dtype

    return groupby_reduction(
        x,
        by,
        func=_mean_groupby_func,
        combine_func=_mean_groupby_combine,
        aggregate_func=_mean_groupby_aggregate,
        axis=axis,
        intermediate_dtype=intermediate_dtype,
        dtype=dtype,
        num_groups=num_groups,
    )
# ---
def __setitem__(self, item, value):
		if not isinstance(item, int):
			raise TypeError("DBFile indices must be integers, not %s" % (type(item)))

		if isinstance(value, DBRow):
			self._values[item] = value
			self._addresses[item] = -1
		else:
			# FIXME technically we should allow DBRow, but this is untested and will need resetting parent
			raise TypeError("Unsupported type for DBFile.__setitem__: %s" % (type(value)))
# ---
def is_Diff(n): 
    return (n % 11 == 0)
# ---
def test_exponential():
    check_gen_is_equal(lambda k, s: jax.random.exponential(k, s), lambda k, s: hax.random.exponential(k, s))
# ---
def __init__(self, indices):
        super().__init__()
        self.indices = indices
# ---
def with_output(self, x, scalar):
            out = x + self.w + scalar
            return out, x * scalar
# ---
def mk_listbox(frame, side='top', sbars='y', sel_mode=tkinter.EXTENDED):
    BORDER = 0
    COLOR = 'grey'

    listbox_frame = tkinter.Frame(frame, bg=COLOR, bd=BORDER)
    listbox_frame.pack(side=side, fill='both', expand=True)

    listbox = tkinter.Listbox(listbox_frame, selectmode=sel_mode)
    mk_scrollable_area(listbox, listbox_frame, sbars)
    return listbox
# ---
def chunkmem(self):
        # take broadcast trick into account
        return array_memory(self.dtype, (1,))
# ---
def test_new_websocket_client_novnc_token_invalid(self, check_token):
        check_token.return_value = False

        self.wh.path = "http://127.0.0.1/"
        self.wh.headers.getheader.return_value = "token=XXX"

        self.assertRaises(exception.InvalidToken,
                          self.wh.new_websocket_client)
        check_token.assert_called_with(mock.ANY, token="XXX")
# ---
def test_format_accelerator_display_handles_unknown():
    """Unknown accelerator types are handled gracefully."""
    result = format_accelerator_display(999, "some-variant")
    assert "unknown" in result.lower()
# ---
def gram_schmidt(y):
    """ Modified Gram-Schmidt orthonormalization of the matrix y(n,n) """

    n = y.shape[0]
    if y.shape[1] != n:
        raise ValueError("Invalid shape: {}".format(y.shape))
    mo = np.zeros(n)

    # Main loop
    for i in range(n):
        # Remove component in direction i
        for j in range(i):
            esc = np.sum(y[j]*y[i])
            y[i] -= y[j]*esc

        # Normalization
        mo[i] = np.linalg.norm(y[i])
        y[i] /= mo[i]

    return mo
# ---
def min_Ops(arr,n,k): 
    max1 = max(arr) 
    res = 0
    for i in range(0,n):  
        if ((max1 - arr[i]) % k != 0): 
            return -1 
        else: 
            res += (max1 - arr[i]) / k 
    return int(res)
# ---
def assert_equal_out(hax_out, torch_out: torch.Tensor):
        assert np.isclose(
            torch_out.numpy(), np.array(hax_out.array), rtol=1e-2, atol=1e-2
        ).all(), f"{torch_out} != {hax_out}"
# ---
def Vocab(self) -> Axis:
        return self.embeddings.Vocab
# ---
def _get_data(self):
        return self._parameter1
# ---
def callback(response):
            current = response.context['upload_stream_current']
            total = response.context['data_stream_total']
            if current is not None:
                progress.append((current, total))
# ---
def _get_data_dir(self, db_version):
        # Try to get from svc first
        output = run('svcprop -p config/data postgresql')
        if output.stdout and exists(output.stdout, use_sudo=True):
            return output.stdout
        return base_postgres.PostgresInstall._get_data_dir(self, db_version)
# ---
def hparams(self, defaults, unused_model_hparams):
    p = defaults
    p.modality = {"inputs": modalities.ModalityType.IMAGE,
                  "targets": modalities.ModalityType.SYMBOL}
    p.vocab_size = {"inputs": 256,
                    "targets": self._encoders["targets"].vocab_size}
    p.batch_size_multiplier = 256
    p.loss_multiplier = 1.0
    p.input_space_id = problem.SpaceID.IMAGE
    p.target_space_id = self.target_space_id
# ---
def test_reductions_produce_scalar_named_arrays_when_None_axis():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)

    named1 = hax.random.uniform(PRNGKey(0), (Height, Width))

    assert isinstance(hax.mean(named1, axis=None), NamedArray)

    # But if we specify axes, we always get a NamedArray, even if it's a scalar
    assert isinstance(hax.mean(named1, axis=("Height", "Width")), NamedArray)
    assert hax.mean(named1, axis=("Height", "Width")).axes == ()
# ---
def test_llama_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def resolve(self, url):
        return url
# ---
def bad(self, f, msg):
        """Callback from dirstate.walk for each explicit file that can't be
        found/accessed, with an error message."""
# ---
def __bool__(self):
        return False
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetGroupList(0), ['011', '012', '013', '014', '015', '016', '017', '018', '019'])
    self.assertEqual(self.schedule.GetGroupList(1), ['021', '022', '023', '024', '025', '026', '028'])
    self.assertEqual(self.schedule.GetGroupList(3), ['041', '042'])
    self.assertEqual(self.schedule.GetGroupList(8), ['0111', '0112', '0113', '0114'])
# ---
def convert_constraints(resources: ResourceConfig) -> list[Constraint]:
    """Build Iris scheduling constraints from fray v2 ResourceConfig."""
    from iris.cluster.types import preemptible_constraint

    constraints: list[Constraint] = []
    if not resources.preemptible:
        constraints.append(preemptible_constraint(False))
    return constraints
# ---
def nanmax(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.nanmax, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def task_index(self) -> int:
        return self.task_id.require_task()[1]
# ---
def test_get_profiler_and_init(self):
        p = profiler.init("secret", base_id="1", parent_id="2")
        self.assertEqual(profiler.get(), p)

        self.assertEqual(p.get_base_id(), "1")
        # NOTE(boris-42): until we make first start we don't have
        self.assertEqual(p.get_id(), "2")
# ---
def test_namedarray_runtime_check_with_dtype():
    Batch = Axis("batch", 2)
    arr = NamedArray(jnp.zeros((Batch.size,), dtype=jnp.float32), (Batch,))
    assert arr.matches_axes(f32["batch"])  # type: ignore
    assert not arr.matches_axes(i32["batch"])
# ---
def sin(a: A) -> A:
    return wrap_elemwise_unary(jnp.sin, a)
# ---
def bottom_left_corner3d(self):
        return self.edge_points3d[3]
# ---
def process_mask(mask):
    mask = mask.where(mask != 0, np.nan)
    mask = mask.transpose("lat", "lon")
    mask = mask.assign_coords(lat=data.y.values, lon=data.x.values)
    mask = mask.rename({"lat": "y", "lon": "x"})
    return mask
# ---
def test_auto_picking(self, cr, uid, ids):
        # TODO: Check locations to see if in the same location ?
        return True
# ---
def is_elastic(datasource):
    """Detect if given resource uses elastic."""
    return datasource.get('backend') == 'elastic' or datasource.get('search_backend') == 'elastic'
# ---
def count_bidirectional(test_list):
  res = 0
  for idx in range(0, len(test_list)):
    for iidx in range(idx + 1, len(test_list)):
      if test_list[iidx][0] == test_list[idx][1] and test_list[idx][1] == test_list[iidx][0]:
        res += 1
  return (str(res))
# ---
def test_sample_returns_entry(bank):
    rng = random.Random(42)
    entry = bank.sample("If", rng)
    assert entry is not None
    assert entry.node_type == "If"
    assert len(entry.source) > 0
# ---
def test_filter_with_expression(backend):
    """Test filter with expression on in-memory data."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"name": "alice", "score": 80},
            {"name": "bob", "score": 60},
            {"name": "charlie", "score": 90},
        ]
    ).filter(col("score") > 70)

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert all(r["score"] > 70 for r in results)
# ---
def forward(self, z: Tensor) -> Tensor:
        """Perform the forward pass.

        Parameters
        ----------
        z : Tensor
            The pairwise embeddings

        Returns
        -------
        Tensor
            The predicted distogram.

        """
        z = z + z.transpose(1, 2)
        return self.distogram(z).reshape(
            z.shape[0], z.shape[1], z.shape[2], 1, self.num_bins
        )
# ---
def trunc_normal_init_(weights, scale=1.0, fan="fan_in"):
    shape = weights.shape
    f = _calculate_fan(shape, fan)
    scale = scale / max(1, f)
    a = -2
    b = 2
    std = math.sqrt(scale) / truncnorm.std(a=a, b=b, loc=0, scale=1)
    size = _prod(shape)
    samples = truncnorm.rvs(a=a, b=b, loc=0, scale=std, size=size)
    samples = np.reshape(samples, shape)
    with torch.no_grad():
        weights.copy_(torch.tensor(samples, device=weights.device))
# ---
def block_size():
    return 10
# ---
def value_dim(self) -> int:
        return self.num_v_heads * self.head_v_dim
# ---
def first_odd(nums):
  first_odd = next((el for el in nums if el%2!=0),-1)
  return first_odd
# ---
def isexact(self):
        return self._m1.isexact()
# ---
def logical_xor(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logical_xor(x1, x2)
# ---
def sync_and_track():
            result = _sync_weights_original()
            if result:
                self.weight_transfers += 1
            return result
# ---
def rate_noise(self, t):
    return self.sigma_max - self.sigma_min
# ---
def points_in_radius(x,y,target_x, target_y,radius):
    inside=np.zeros(x.size,dtype=bool)
    d2=(x-target_x)**2+(y-target_y)**2
    inside = d2<=radius**2
    return x[inside],y[inside], inside
# ---
def unsize_axes(axis_spec: PartialShapeDict) -> PartialShapeDict: ...
# ---
def my_base(n: str, base: int = 10) -> int:
        return int(n, base)
# ---
def squeeze(x, /, axis):
    if not isinstance(axis, tuple):
        axis = (axis,)

    if any(x.shape[i] != 1 for i in axis):
        raise ValueError("cannot squeeze axis with size other than one")

    axis = validate_axis(axis, x.ndim)

    chunks = tuple(c for i, c in enumerate(x.chunks) if i not in axis)

    return map_blocks(
        nxp.squeeze, x, dtype=x.dtype, chunks=chunks, drop_axis=axis, axis=axis
    )
# ---
def check_tuplex(tuplex,tuple1): 
  if tuple1 in tuplex:
    return True
  else:
     return False
# ---
def __init__(
        self,
        model: "BaseModel | DistributedDataParallel",
        ctx: "GridContext",
    ) -> None:
        super().__init__()
        self._underlying: BaseModel = getattr(model, "module", model)  # type: ignore
        self._ctx = ctx
# ---
def make_widgets(self, main_frame):
        pass
# ---
def trim_and_pad(ary: np.ndarray, max_seq_len: int, pad_to: int, padding_value: int | float) -> np.ndarray:
    """Trim to max_seq_len and pad to pad_to."""
    ary = ary[:max_seq_len]
    if pad_to < len(ary):
        raise ValueError(f"pad_to ({pad_to}) must be >= trimmed length ({len(ary)})")
    ary = np.pad(ary, (0, pad_to - len(ary)), mode="constant", constant_values=padding_value)
    return ary
# ---
def height(self, new_height):
        self.set_size(self.width, new_height)
# ---
def run_streaming(self, command: str) -> subprocess.Popen:
        return subprocess.Popen(
            self._build_cmd(command),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
        )
# ---
def transgene_insertion_2(testapp, lab, award, ctcf):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'transgene insertion',
        'purpose': 'in vivo enhancer characterization',
        'nucleic_acid_delivery_method': ['mouse pronuclear microinjection'],
        'modified_site_by_gene_id': ctcf['@id'],
        'introduced_sequence': 'ATCGTA'
    }
# ---
def clip(array: NamedOrNumeric, a_min: NamedOrNumeric, a_max: NamedOrNumeric) -> NamedArray:
    """Like jnp.clip, but with named axes. This version currently only accepts the three argument form."""
    (array, a_min, a_max), axes = broadcast_arrays_and_return_axes(array, a_min, a_max)
    array = raw_array_or_scalar(array)
    a_min = raw_array_or_scalar(a_min)
    a_max = raw_array_or_scalar(a_max)

    return NamedArray(jnp.clip(array, a_min, a_max), axes)
# ---
def __init__(self, m1, m2):
        super(xormatcher, self).__init__(m1._root, m1._cwd)
        self.traversedir = m1.traversedir
        self.m1 = m1
        self.m2 = m2
# ---
def test_demo_notebook_hello_world_submit(demo_client: IrisClient) -> None:
    # Notebook cell snippet (verbatim structure).
    def hello_world():
        print("Hello from the cluster!")
        return 42

    job = demo_client.submit(
        entrypoint=Entrypoint.from_callable(hello_world),
        name="notebook-hello",
        resources=ResourceSpec(cpu=1, memory="512m"),
    )
    status = job.wait(timeout=30.0, raise_on_failure=False)
    assert status is not None
# ---
def _dummy_fn(x: int) -> int:
    return x + 1
# ---
def port(self) -> int:
        """Actual bound port (may differ from config if port=0 was specified)."""
        if self._server and self._server.servers:
            # Get actual port from the first server socket
            sockets = self._server.servers[0].sockets
            if sockets:
                return sockets[0].getsockname()[1]
        return self._config.port
# ---
def model_config(self):
        if self.model_name == "llama32":
            return llama_32b_remat
        if self.model_name == "qwen32":
            return qwen3_32b_remat
        raise ValueError(f"Unknown model alias '{self.model_name}'")
# ---
def __call__(self, x):
            return self.second(self.first(x))
# ---
def now(*args):
        """ Return the current day and time in the format expected by the ORM.
            This function may be used to compute default values.
        """
        return datetime.now().strftime(DATETIME_FORMAT)
# ---
def playlist_remove(self, index='current'):
        self.command('playlist_remove', index)
# ---
def count_With_Odd_SetBits(n): 
    if (n % 2 != 0): 
        return (n + 1) / 2
    count = bin(n).count('1') 
    ans = n / 2
    if (count % 2 != 0): 
        ans += 1
    return ans
# ---
def fn(config: Config):
            random_str = str(random.randint(0, 1000))
            time.sleep(2)
            with open(os.path.join(config.path, random_str), "w") as f:
                f.write("1")
# ---
def test_count_division(self):
        """Count division"""
        count = Count(4, 2)
        self.assertEqual(count / 2, 2)
# ---
def __call__(self, x):
            y = tree_checkpoint_name(hax.sin(x + self.named), "sin")
            y = tree_checkpoint_name(hax.cos(y + x), "cos")
            return y + x
# ---
def __init__(self, instance: Any):
        self._instance = instance
        self._executor = ThreadPoolExecutor(max_workers=16)
# ---
def test_basic_rearrange_transpose():
    assert einops_rearrange(z, "b d h w c -> b h w d c").axes == (B, H, W, D, C)
    # make sure the values are right too
    z_t = z.array.transpose((0, 2, 3, 1, 4))
    assert (einops_rearrange(z, "b d h w c -> b h w d c").array == z_t).all()
# ---
from itertools import groupby
def pack_consecutive_duplicates(list1):
    return [list(group) for key, group in groupby(list1)]
# ---
def test_simple_fail_second_start(self, tmpfile):
        fd = tmpfile.fileno()
        cap = capture.FDCapture(fd)
        cap.done()
        pytest.raises(ValueError, cap.start)
# ---
def _get_container_ids() -> set[str]:
    """Get all container IDs (running and stopped)."""
    result = subprocess.run(["docker", "ps", "-aq"], capture_output=True, text=True, check=False)
    return set(result.stdout.strip().split()) if result.stdout.strip() else set()
# ---
def test_rechunk(spec, executor, new_chunks, expected_chunks, use_new_impl):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 1), spec=spec)
    b = a.rechunk(new_chunks, use_new_impl=use_new_impl)
    assert b.chunks == expected_chunks
    assert_array_equal(
        b.compute(executor=executor),
        np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
    )
# ---
def assign(d1, d2):
                for k, v in d1.items():
                    if type(v) is dict:
                        assign(v, d2[k])
                    else:
                        if _add_into_out:
                            v[i : i + chunk_size] += d2[k]
                        else:
                            v[i : i + chunk_size] = d2[k]
# ---
def sampler(self):
        return self._dataloader.sampler
# ---
def next(self, psm: PSM):
        if psm.char in string.hexdigits:
            self.pattern.pattern += psm.char
            count = len(self.pattern.pattern)
            return self.prev if count >= 2 else self
        else:
            psm.error = "expected ASCII hexadecimal character"
# ---
def create_router_postcommit(self, context, router_context):
        pass
# ---
def _repr_inline_(self, max_width):
        """
        Format to a single line with at most max_width characters. Used by xarray.
        """
        return f"cubed.Array<chunksize={self.chunksize}>"
# ---
def testImportConflictingPackage(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        import time
        from "__go__/time" import Now""")))
# ---
def __init__(self, exception):
        self.exception = exception
# ---
def state_dict(self) -> dict:
        state_dict = {k: v for k, v in self.__dict__.items() if k not in ["optimizer"]}
        return state_dict
# ---
def jit_vmap_hist(a):
        """
        This function will be JIT compiled and VMapped.
        """
        # Call the sharded histogram function
        hist, bins = hax.vmap(levanter.tracker.histogram.sharded_histogram, Layer)(a, bins=32)
        return hist, bins
# ---
def __repr__(self):
        return f"WrappedAsyncDataset({repr(self.dataset)})"
# ---
def __str__(self) -> str:
        return repr(self)
# ---
def name_to_path(name: str) -> str:
        if protocol is not None:
            return f"{protocol}://{name}"
        return name
# ---
def result(self) -> Any:
        """Get the underlying result from the future."""
        return self._future.result()
# ---
def convert_to_export(self, value, env):
        return value.name_get()[0][1] if value else ''
# ---
def push(self, item: T) -> None:
        timestamp = time.time()
        unique_id = uuid.uuid4()
        filename = f"{timestamp:.6f}_{unique_id}.pkl"

        with self.fs.open(self.pending_dir / filename, "wb") as f:
            pickle.dump(item, f)
# ---
def the_tokenizer(self) -> HfTokenizer:
        if self.tokenizer == "passthrough":
            return PassthroughTokenizer(self.vocab_size)
        else:
            return load_tokenizer(self.tokenizer)
# ---
def execute_pipeline(pipeline, executor):
    """Executes a pipeline"""
    dag = nx.MultiDiGraph()
    dag.add_node("node", pipeline=pipeline)
    executor.execute_dag(dag)
# ---
def submit_job(
    state: ControllerState,
    job_id: str,
    request: cluster_pb2.Controller.LaunchJobRequest,
) -> JobName:
    """Submit a job via event."""
    jid = JobName.from_string(job_id) if job_id.startswith("/") else JobName.root(job_id)
    request.name = jid.to_wire()
    state.handle_event(
        JobSubmittedEvent(
            job_id=jid,
            request=request,
            timestamp=Timestamp.now(),
        )
    )
    return jid
# ---
def ones(shape, *, dtype=None, device=None, chunks="auto", spec=None) -> "Array":
    dtypes = __array_namespace_info__().default_dtypes(device=device)

    if dtype is None:
        dtype = dtypes["real floating"]
    return full(shape, 1, dtype=dtype, device=device, chunks=chunks, spec=spec)
# ---
def __init__(self):
        self.stdin_path = '/dev/null'
        self.stdout_path = settings.LOG_PATH + '/webapp.log'
        self.stderr_path = settings.LOG_PATH + '/webapp.log'
        self.pidfile_path = settings.PID_PATH + '/webapp.pid'
        self.pidfile_timeout = 5
# ---
def modulate(x: torch.Tensor,
             shift: torch.Tensor,
             scale: torch.Tensor) -> torch.Tensor:
  return x * (1 + scale) + shift
# ---
def __call__(self, target, cred):
        """Check the policy.

        Returns the logical inverse of the wrapped check.
        """

        return not self.rule(target, cred)
# ---
def load_checkpoint_weights(self, checkpoint_path):
        checkpoint = torch.load(
            checkpoint_path, map_location=self.device, weights_only=False
        )
        self.load_state_dict(checkpoint["state_dict"], strict=False)
        print(f"Loaded weights from {checkpoint_path}")
# ---
def test_create_actor_synchronous_call(client: LocalClient):
    actor = client.create_actor(Counter, start=10, name="counter")
    assert actor.get() == 10
    actor.increment(3)
    assert actor.get() == 13
# ---
def decode(self, token_ids):
            return "".join(chr(tid) for tid in token_ids)
# ---
def CurrentFiletypes():
  return VimExpressionToPythonType( "&filetype" ).split( '.' )
# ---
def _list_relative_files(directory: str) -> set[str]:
            rel_files: set[str] = set()
            for root, _, filenames in os.walk(directory):
                for filename in filenames:
                    full_path = os.path.join(root, filename)
                    rel_files.add(os.path.relpath(full_path, directory))
            return rel_files
# ---
def check_valid_interface_type(interface_type: Optional[int]) -> None:
    if interface_type not in Service.ALLOWED_INTERFACE_TYPES:
        raise JsonableError(_("Invalid interface type"))
# ---
def setUp(self):

        self.data = MockData()
# ---
def remove(self, item):
        """Remove an element from the queue.

        Parameters
        ----------
        item :
            The element to remove.

        """
        self._queue.remove(item)
# ---
def main(config: ServerConfig):
    print("ServerConfig:", config)

    config = standardize_config(config)

    global server
    server = Server(config)

    debug = os.environ.get("DEV") == "true"
    assert debug, "This function must be run in debug mode"
    port = config.port if config.port is not None else (5000 if debug else 80)
    app.run(host="0.0.0.0", port=port, debug=debug)
# ---
def qCleanupResources():
    QtCore.qUnregisterResourceData(rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data)
# ---
def size(self) -> int:
        """Get total number of rollouts across all environments."""
        with self._lock:
            return sum(len(rollouts) for rollouts in self.rollout_storage.values())
# ---
def __eq__(self, other):
        if not isinstance(other, GeomCoverage):
            return NotImplemented

        if self.srs != other.srs:
            return False

        if self.bbox != other.bbox:
            return False

        if not self.geom.equals(other.geom):
            return False

        return True
# ---
def delete_secret(self, key):
        ''' delete secret'''
        try:
            del self.secrets[key]
        except KeyError as _:
            return False

        return True
# ---
def __init__(self, table=None, n_classes=2, class_names=("1", "0")):
        self.table = table
        self.n_classes = n_classes
        self.class_names = class_names
        if table is None:
            self.table = np.zeros((self.n_classes, self.n_classes), dtype=int)
# ---
def floor(a: A) -> A:
    return wrap_elemwise_unary(jnp.floor, a)
# ---
def volume_cuboid(l,w,h):
  volume=l*w*h
  return volume
# ---
def test_brackets_unbalanced():
    assert not brackets_balanced("(a + b")
    assert not brackets_balanced("a + b)")
    assert not brackets_balanced("[1, 2)")
# ---
def delete(self):
            pass
# ---
def test_unschedulable_maps_to_failed(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_UNSCHEDULABLE) == JobStatus.FAILED
# ---
def max_run_uppercase(test_str):
  cnt = 0
  res = 0
  for idx in range(0, len(test_str)):
    if test_str[idx].isupper():
      cnt += 1
    else:
      res = cnt
      cnt = 0
  if test_str[len(test_str) - 1].isupper():
    res = cnt
  return (res)
# ---
def create_test_rollout_storage_config() -> RolloutStorageConfig:
    """Create in-memory storage config for testing."""
    from marin.rl.rollout_storage import StorageType

    test_id = uuid.uuid4().hex[:8]
    return RolloutStorageConfig(storage_type=StorageType.IN_MEMORY, queue_name=f"test_{test_id}")
# ---
def cli(ctx, dry_run, executor_info_base_path, prefix):
    """Manage pretraining datasets: download, tokenize, and list available datasets."""
    # These arguments are passed by the test framework but used by executor_main, not this CLI
    if ctx.invoked_subcommand is None:
        ctx.invoke(_list)
# ---
def _get_memory_total_bytes() -> int:
    try:
        with open("/proc/meminfo") as f:
            for line in f:
                if line.startswith("MemTotal:"):
                    return int(line.split()[1]) * 1024  # kB to bytes
    except FileNotFoundError:
        pass
    # Fallback for non-Linux
    return 8 * 1024**3
# ---
def always(self):
        return self._m1.always() and self._m2.always()
# ---
def register_rpc_commands(iris_group: click.Group) -> None:
    """Register RPC service commands on the top-level iris group."""
    iris_group.add_command(ServiceCommands("controller", name="controller-rpc", help="Controller service RPC methods"))
    iris_group.add_command(ServiceCommands("worker", name="worker-rpc", help="Worker service RPC methods"))
    iris_group.add_command(ServiceCommands("actor", name="actor-rpc", help="Actor service RPC methods"))
# ---
def __init__(
        self,
        cache_dir: str,
        exemplar: T,
        metadata: Optional["CacheMetadata"] = None,
        shard_name: str = "",
    ):
        self.cache_dir = cache_dir
        self.metadata = metadata
        self._exemplar = exemplar
        self._shard_name = shard_name
        self._tree_store = TreeStore.open(exemplar, self.cache_dir, mode="w", cache_metadata=True)
        self._is_closed = False
# ---
import re
def extract_values(text):
 return (re.findall(r'"(.*?)"', text))
# ---
def setUp(self):
        self.db = self.opendb()
        self.sample_data()
# ---
def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig)
# ---
def test_bank_has_call(bank):
    assert bank.has_type("Call"), "Should extract Call expressions"
# ---
def get_autoscaler_status(self, request: cluster__pb2.Controller.GetAutoscalerStatusRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetAutoscalerStatusResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def vref(self):
        """Reference voltage used by the chip. You need to set this. It defaults to 3.3V"""
        return self._Vref
# ---
def query(self, name):
        ret = []
        obj = getattr(self.ndb.query, name)
        for line in obj():
            ret.append(line)
        return bottle.template('{{!ret}}', ret=json.dumps(ret))
# ---
def test_actor_unnamed_isolation(job_context):
    """Test that unnamed actors are isolated instances."""
    actor1 = job_context.create_actor(SimpleActor, 10)
    actor2 = job_context.create_actor(SimpleActor, 20)

    job_context.get(actor1.increment.remote(5))
    job_context.get(actor2.increment.remote(3))

    assert job_context.get(actor1.get_value.remote()) == 15
    assert job_context.get(actor2.get_value.remote()) == 23
# ---
def log_image(self, name, path):
        if self.logger is not None:
            try:
                self.logger.log_image(name, images=[str(path)])
            except:
                import traceback

                traceback.print_exc()  # noqa: T201
                print(f"Image logging failed for {name} {str(path)}.")
# ---
def retrfile(self, filename, filetype):
                self.filename, self.filetype = filename, filetype
                return io.StringIO(self.data), len(self.data)
# ---
def on_step(self, info: StepInfo[S], force: bool = False): ...
# ---
def test_creation_with_path_object(self):
        """Test LocalLocation creation with Path object."""
        path = Path("/tmp/data")
        loc = LocalLocation(path=path)
        assert loc.path == path
# ---
def weibull_min(key, shape: AxisSpec, scale: NamedOrNumeric, concentration: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    scale = broadcast_to(scale, shape)
    concentration = broadcast_to(concentration, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.weibull_min(key, scale.array, concentration.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def _thrift_cmd(self):
    cmd = [self._thrift_binary]
    thrift_import = 'thrift_import={}'.format(self.get_options().thrift_import)
    gen_options = self.get_options().gen_options
    if gen_options:
      gen_options += ',' + thrift_import
    else:
      gen_options = thrift_import
    cmd.extend(('--gen', 'go:{}'.format(gen_options)))

    if self.get_options().strict:
      cmd.append('-strict')
    if self.get_options().level == 'debug':
      cmd.append('-verbose')
    return cmd
# ---
def hasMore(I):
	return (I.peek() != None)
# ---
def StdCaptureFD(out=True, err=True, in_=True):
    return capture.MultiCapture(out, err, in_, Capture=capture.FDCapture)
# ---
def _UNSPECIFIED():
    raise ValueError("unspecified")
# ---
def from_binary(command: str, args: Sequence[str]) -> Self:
        return Entrypoint(binary_entrypoint=BinaryEntrypoint(command=command, args=args))
# ---
def _handle_gl_func(name, args=[], restype=None):
    _handle_func(name, args, restype, errcheck=None, ctx=MpvOpenGLCbContext)
# ---
def test_unresolved_location_passthrough(self):
        """Test that UnresolvedLocation objects pass through unchanged."""
        from pydantic import BaseModel

        class TestModel(BaseModel):
            location: Location

        unresolved = UnresolvedLocation(path="data/test.zarr")
        model = TestModel(location=unresolved)
        assert model.location == unresolved
# ---
def __init__(self, config: FakeVmManagerConfig):
        self._config = config
        self._lock = threading.Lock()
        self._slices: dict[str, FakeVmGroup] = {}
        self._slice_counter = 0
# ---
def testFor(self):
    self.assertEqual((0, '1\n2\n3\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1, 2, 3):
          print i""")))
# ---
def fsdp(fn: F, parameter_mapping: ResourceMapping, compute_mapping: ResourceMapping) -> F: ...
# ---
def _wrapped(x):
            return fn(x)
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetHoursColumn(), 1)
# ---
def profit_amount(actual_cost,sale_amount): 
 if(actual_cost > sale_amount):
    amount = actual_cost - sale_amount
    return amount
 else:
    return None
# ---
def head_size(self) -> int:
        if self.head_dim is not None:
            return self.head_dim
        return self.Embed.size // self.num_heads
# ---
def init(key):
        return SimpleModel(hax.random.normal(key, (Embed,)))
# ---
def test_submit_rejects_name_with_slash(local_client):
    """Verify submit raises ValueError for names containing '/'."""
    entrypoint = Entrypoint.from_callable(dummy_entrypoint)
    resources = ResourceSpec(cpu=1, memory="1g")

    with pytest.raises(ValueError) as exc_info:
        local_client.submit(entrypoint, "invalid/name", resources)

    assert "/" in str(exc_info.value)
# ---
def _is_task_timed_out(self, task: ControllerTask, job: ControllerJob) -> bool:
        """Check if a task has exceeded its scheduling timeout."""
        return job.scheduling_deadline is not None and job.scheduling_deadline.expired()
# ---
def offsets_virtual_array(shape, spec=None) -> "Array":
    name = gensym()
    target = virtual_offsets(shape)

    from .array_object import Array

    plan = Plan._new(name, "block_ids", target, hidden=True)
    return Array(name, target, spec, plan)
# ---
import re
def search_literal(pattern,text):
 match = re.search(pattern, text)
 s = match.start()
 e = match.end()
 return (s, e)
# ---
def name(self):
        return os.path.splitext(os.path.basename(self.path))[0]
# ---
def scanned_f(init, *args, **kwargs):
        return scan_preconfig(init, *args, **kwargs)[0]
# ---
def matches(self, b: int, h: int, v: int) -> bool:
        return self.b_min <= b <= self.b_max and self.h_min <= h <= self.h_max and self.v_min <= v <= self.v_max
# ---
def cache_dir(pytestconfig: pytest.Config) -> pathlib.Path:
    dir = pytestconfig.rootpath / ".data_cache"
    dir.mkdir(parents=True, exist_ok=True)
    return dir
# ---
def update_config(self) -> Optional[WebConfig]:
        if self.config_arg is None:
            return None
        config, web_config = parse_config(self.config_arg)
        self.cron_jobs = OrderedDict((job.name, job) for job in config)
        return web_config
# ---
def docker_runtime(docker_cleanup_scope):
    """DockerRuntime that cleans up containers and images created during test."""
    yield DockerRuntime()
# ---
def _deDuplicate(results):
        found = dict()
        deDuped = list()
        for entry in results:
            dn = entry.get("dn")
            if not dn in found:
                found[dn] = 1
                deDuped.append(entry)

        return deDuped
# ---
from itertools import combinations
def sub_lists(my_list):
	subs = []
	for i in range(0, len(my_list)+1):
	  temp = [list(x) for x in combinations(my_list, i)]
	  if len(temp)>0:
	    subs.extend(temp)
	return subs
# ---
def test_remove(self):
        prio_set_list = event._PrioritizedSetList()
        obj = (1,)

        prio_set_list.add(1, obj)
        assert prio_set_list
        prio_set_list.remove(obj)
        assert not prio_set_list

        with pytest.raises(ValueError) as excinfo:
            prio_set_list.remove(obj)
        excinfo.match(r"can not be found")
# ---


def fizz_buzz(n: int):
    """Return the number of times the digit 7 appears in integers less than n which are divisible by 11 or 13.
    >>> fizz_buzz(50)
    0
    >>> fizz_buzz(78)
    2
    >>> fizz_buzz(79)
    3
    """
    ns = []
    for i in range(n):
        if i % 11 == 0 or i % 13 == 0:
            ns.append(i)
    s = ''.join(list(map(str, ns)))
    ans = 0
    for c in s:
        ans += (c == '7')
    return ans
# ---
def first_message(database, write):
    messages = [
            firestore_pb2.WriteRequest(database = database, writes = [])
    ]
    for msg in messages:
            yield msg
# ---
def query_string(self):
        if self._parsed_url.query:
            return self._parsed_url.query.decode("utf-8")
        else:
            return ""
# ---
def fused_func_single(*args):
        # args are grouped appropriately so they can be called by each predecessor function
        func_args = [
            apply_blockwise_func(pf, iterable_input_blocks[i], *a)
            for i, (pf, a) in enumerate(zip(predecessor_functions, args, strict=True))
        ]
        return function(*func_args)
# ---
def _tee_handler(cmd, *ignore_args, **ignore_kwargs):
            self._tee_executed = True
            return '', ''
# ---
def _batch_sizes() -> dict[str, int]:
    return {"130m": 128, "300m": 128, "520m": 128, "1_2b": 128}
# ---
def _rotate_half(x: NamedArray, HeadSize: Axis) -> NamedArray:
    """Rotates half of the hidden dims of the input and concatenates them."""
    x1 = x[HeadSize, : HeadSize.size // 2]
    x2 = x[HeadSize, HeadSize.size // 2 :]
    out = hax.concatenate(HeadSize, (-x2, x1))
    return out
# ---
def is_team_member(self):
        return True if self._get_member() else False
# ---
def test_binary_force_string(self):
        """Binary must wrap a string type"""
        with self.assertRaises(TypeError):
            Binary(2)
# ---
def trace_me():
            pass
# ---
def _maybe_flatten(q, axes, name):
    if axes:
        q = q.flatten_axes(axes, name)
    else:
        q = q.broadcast_axis(Axis(name, 1))
    return q
# ---
def vocab_size(self):
        return self.tokenizer.vocab_size
# ---
def test_construct_image_url_direct(self):
        drv = self._driver
        img_loc = ("nfs://host/path/image-id", None)
        location = drv._construct_image_nfs_url(img_loc)
        if location != "nfs://host/path/image-id":
            self.fail("Unexpected direct url.")
# ---
def _swap_description_payee(self, apply_to_all):
        if apply_to_all:
            panes = self.panes
        else:
            panes = [self.selected_pane]

        def switch_func(txn):
            txn.description, txn.payee = txn.payee, txn.description

        self._swap_fields(panes, switch_func)
# ---
def get_all_actors_in_pool(self) -> list[ActorHandle]:
        return [member.actor for member in self._actor_pool]
# ---
def elapsed(self):
        return self._elapsed
# ---
def access_key(ditionary,key):
  return list(ditionary)[key]
# ---
def _identity_loss_weight(loss_weight: np.ndarray) -> np.ndarray:
    return loss_weight
# ---
def unfrackgitpath(path):
    if path is None:
        return None

    # copied from ansible.utils.path
    return os.path.normpath(os.path.realpath(os.path.expanduser(os.path.expandvars(path))))
# ---
def test_impl(df):
            return df.B.sum()
# ---
def __enter__(self):
        self.server_thread = ServerThread(self.server, self.host, self.port)
        self.server_thread.start()
        return self
# ---
def make_keyword():
    return Keyword(keyword=fake.domain_word())
# ---
def delete(self, match):
        self._call_all('delete', match)
# ---
def output_path_inverse_fold(input_path):
                assert self.output_dir is not None
                return [
                    self.output_dir / f"{input_path.stem}.cif",
                    self.output_dir / f"{input_path.stem}.npz",
                ]
# ---
def submit_unit (self, description) :
        """
        Instantiate and return (Compute or Data)-Unit object(s)
        """

        raise Exception ("%s.submit_unit() is not implemented" % self.__class__.__name__)
# ---
def mark_old(node):
    if node.parent and node.parent.label == "Deprecated node!":
        return
    ng = node.id_data
    frame = ng.nodes.new("NodeFrame")
    if node.parent:
        frame.parent = node.parent
    node.parent = frame
    frame.label = "Deprecated node!"
    frame.use_custom_color = True
    frame.color = (.8, 0, 0)
    frame.shrink = True
# ---
def clause(self):
        return self.clause_with_joiner('or')
# ---
from operator import itemgetter
def index_on_inner_list(list_data, index_no):
    result = sorted(list_data, key=itemgetter(index_no))
    return result
# ---
def sorted_models(models):
 sorted_models = sorted(models, key = lambda x: x['color'])
 return sorted_models
# ---
def _resolve_config_path(config_path: str) -> Path:
    path = Path(config_path)
    if not path.is_absolute():
        path = ROOT / path
    return path
# ---
def to_hf_config(self) -> tuple[float, dict | None]:
        if self.factor == 1.0:
            return self.theta, None
        return self.theta, {"factor": self.factor}
# ---
def swap_List(newList): 
    size = len(newList) 
    temp = newList[0] 
    newList[0] = newList[size - 1] 
    newList[size - 1] = temp   
    return newList
# ---
def h_fs_put(_,path,data):
        f=open(path,'w')
        for x in data: f.write(unescape(x))
        f.close()
        pass
# ---
def config(self) -> ToyLmConfig:
        return self._config
# ---
def __repr__(self) -> str:
        return f"{self.child}.is_null()"
# ---
def test_passing_non_parseable_status_throws_exception(self):
        with pytest.raises(InvalidStatusException):
            Line(line_contents=get_updated_line_contents({'status': 'foobar'}))
# ---
def camel_to_snake(text):
        import re
        str1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', str1).lower()
# ---
def _create_ssh_connection(self, host: str) -> DirectSshConnection:
        """Create an SSH connection for the given host."""
        return DirectSshConnection(
            host=host,
            user=self._ssh_config.user,
            port=self._ssh_config.port,
            key_file=self._ssh_config.key_file,
            connect_timeout=self._ssh_config.connect_timeout,
        )
# ---
def map(self, *args, **kwargs):
        return map(*args, **kwargs)
# ---
def seralize(self) -> Any:
        return self.path
# ---
def get(self) -> torch.Tensor:
        """
        Get the total metric value, not divided by number of recorded batches.
        """
        ...
# ---
def testFloatRandom(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.float32)
      y = self._randMatrix(k, m, np.float32)
      self._testCpuMatmul(x, y)
      self._testGpuMatmul(x, y)
# ---
def counting_flat_map(self, path):
        self.flat_map_count += 1
        return load_file(path)
# ---
def uri(self):
        """
        Gets the uri of this ContributorOrcid.

        :return: The uri of this ContributorOrcid.
        :rtype: str
        """
        return self._uri
# ---
def DeleteSession(self, request, context):
        """Ends a session, releasing server resources associated with it. This will
    asynchronously trigger cancellation of any operations that are running with
    this session.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def area(self):
        raise NotImplementedError
# ---
def __len__(self):
        if not self.has_len():
            raise ValueError("DataLoader has no length")
        total_length = blocking_wait(self.data_store.current_len())
        step = self.scheduler.find_step_containing_offset(total_length) + 1
        return step
# ---
def register_event_callback(self, callback):
        self._event_callbacks.append(callback)
# ---
def test_zero_duration():
    """Zero duration is handled correctly."""
    zero = Duration.from_ms(0)

    assert zero.to_ms() == 0
    assert zero.to_seconds() == 0.0

    # Proto roundtrip
    proto = zero.to_proto()
    restored = Duration.from_proto(proto)
    assert restored.to_ms() == 0
# ---
def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take(Width, 2)
# ---
def test_perturb_operators_arithmetic(rng):
    source = "x + y"
    # Try many times since perturbation is probabilistic.
    found_different = False
    for seed in range(50):
        r = random.Random(seed)
        result = perturb_operators(source, r, swap_prob=1.0)
        if result is not None:
            found_different = True
            ast.parse(result)
            assert result != source
            break
    assert found_different, "Expected at least one operator perturbation"
# ---
def reset_fray_context():
    """Reset fray context between tests for isolation."""
    _job_context.set(None)
    yield
    _job_context.set(None)
# ---
def start_side_effect(container_id):
        call_count[0] += 1
        if call_count[0] == 1:
            raise RuntimeError("failed to bind host port: address already in use")
        return None
# ---
def __init__(self, service_name: str, **attrs):
        super().__init__(**attrs)
        self.service_name = service_name
        self.available_methods = {}
# ---
def _set_panel_match(self, match, team_idx, panel_idx):
        self.team_panels[panel_idx].number = match.team_numbers[team_idx] 
        self.team_panels[panel_idx].name = match.team_names[team_idx]
# ---
def __init__(self, parent: OpeningOfGroup):
        self.parent = parent
# ---
def numblocks(self):
        """A tuple indicating the number of blocks (chunks) in each corresponding array dimension."""
        return tuple(map(len, self.chunks))
# ---
def __radd__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__radd__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.add, other, self, dtype=result_type(self, other))
# ---
def create_task(coro: Awaitable) -> asyncio.Task:
    return asyncio.get_event_loop().create_task(coro)
# ---
def create_qwen_config():
    return Qwen3Config(
        seq_len=4096,
        hidden_dim=1024,
        intermediate_dim=3072,
        num_heads=16,
        num_kv_heads=8,
        num_layers=28,
        rope=Llama3RotaryEmbeddingsConfig(),
        tie_word_embeddings=True,
    )
# ---
def test_merge_chunks_fails_dim_mismatch(spec, target_chunks):
    a = xp.ones((10, 10), dtype=np.uint8, chunks=(2, 3), spec=spec)
    with pytest.raises(
        ValueError, match=r"Chunks .* must have same number of dimensions as array"
    ):
        merge_chunks(a, target_chunks)
# ---
def _dest_images(self):
        ret = []
        for vol in self._prepared_volumes:
            ret.append(vol['path'])
        return ret
# ---
def guides_transduction_GM(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'nucleic_acid_delivery_method': ['transduction'],
        'introduced_elements': 'gRNAs and CRISPR machinery',
        'MOI': 'high',
        'guide_type': 'sgRNA'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def _infer_core_grid(b_dim: int, block_sizes: BlockSizes) -> tuple[int, int]:
    num_cores = _infer_num_tensorcores()
    if num_cores > 1:
        if b_dim % num_cores != 0:
            num_cores = 1
        else:
            b_per_core = b_dim // num_cores
            if b_per_core % block_sizes.b_block_size != 0:
                num_cores = 1
    num_b_blocks_per_core = b_dim // (num_cores * block_sizes.b_block_size)
    return num_cores, num_b_blocks_per_core
# ---
def init_hotkeys(self):
        self.main_model.hotkey_model.add_hotkey(["Lcontrol", "Lmenu", "J"], self.main_view.focus_job_num_edit)
        self.main_model.hotkey_model.add_hotkey(["Lcontrol", "Lmenu", "O"], self.main_view.open_current_job_folder)
        self.main_model.hotkey_model.add_hotkey(["Lcontrol", "Lmenu", "B"], self.main_view.open_current_job_basecamp)
        self.main_model.hotkey_model.start_detection()
# ---
def nanargmax(array: NamedArray, axis: AxisSelector | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.nanargmax, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def get(self) -> torch.Tensor:
        """Returns the metric."""
        assert self._total is not None
        return self._total
# ---
def route(method, path):
    def decorator(f):
        f.http_route = path
        f.http_method = method
        return f
    return decorator
# ---
def on_step(self, info: StepInfo[S], force: bool = False):
        self.fn(info)
# ---
def __call__(self, x):
        x = hax.dot(self.w_in, x, axis=self.In)
        x = hnn.relu(x)
        x = hax.dot(self.w_out, x, axis=self.Mid)
        return x
# ---
from itertools import groupby 
def group_element(test_list):
  res = dict()
  for key, val in groupby(sorted(test_list, key = lambda ele: ele[1]), key = lambda ele: ele[1]):
    res[key] = [ele[0] for ele in val] 
  return (res)
# ---
def bias_dropout_add_scale_fused_inference(
  x: torch.Tensor,
  bias: typing.Optional[torch.Tensor],
  scale: torch.Tensor,
  residual: typing.Optional[torch.Tensor],
  prob: float,
) -> torch.Tensor:
  return bias_dropout_add_scale(
    x, bias, scale, residual, prob, False
  )
# ---
def Receptionist_Places_Call (self, Number):
        self.Step (Message = "Receptionist places call to " + str (Number) + "...")

        self.Log (Message = "Dialling through receptionist agent...")
        self.Receptionist.dial (Number)
# ---
def test_spec_compressor(tmp_path, compressor):
    spec = cubed.Spec(tmp_path, allowed_mem=100000, zarr_compressor=compressor)
    a = xp.ones((3, 3), chunks=(2, 2), spec=spec)
    b = xp.negative(a)
    assert_array_equal(b.compute(), -np.ones((3, 3)))
# ---
def ReportTaskState(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def execute_plan(self, plan: T, **kwargs):
        """Execute a plan."""
        raise NotImplementedError
# ---
def count_statements(node: ast.AST) -> int:
    """Count the number of statement nodes in an AST subtree.

    This is the Python analogue of the paper's 'sigma' (primitive count).
    Used to control the size of extracted and replaced subtrees.
    """
    count = 0
    for child in ast.walk(node):
        if isinstance(child, ast.stmt):
            count += 1
    return count
# ---
def delete(self, path):
        ''' remove path from a dict'''
        try:
            entry = Yedit.get_entry(self.yaml_dict, path, self.separator)
        except KeyError:
            entry = None

        if entry is None:
            return (False, self.yaml_dict)

        result = Yedit.remove_entry(self.yaml_dict, path, self.separator)
        if not result:
            return (False, self.yaml_dict)

        return (True, self.yaml_dict)
# ---
def dev_shards(self):
    return 1
# ---
def on_random_range_min_changed(self):
        self.ui.spinBoxRandomMaximum.setMinimum(self.ui.spinBoxRandomMinimum.value())
# ---
def test_amin_alias():
    Height, Width = hax.make_axes(Height=2, Width=3)
    data = jnp.arange(6.0).reshape(2, 3)
    arr = hax.named(data, (Height, Width))
    assert jnp.array_equal(hax.amin(arr).array, jnp.amin(data))
    assert jnp.array_equal(hax.amin(arr, axis=Height).array, jnp.amin(data, axis=0))
    assert hax.amin(arr, axis=Height).axes == (Width,)
# ---
def test_single_statement_replacement():
    source = "return a + b\n"
    target = "return a * b\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1
    assert edits[0].node_type in ("BinOp", "Return")
# ---
def visit_mtable(self, element):
        rows = []
        for child in element.children:
            if child.name == "mtr":
                cells = []
                for cell in child.children:
                    if cell.name == "mtd":
                        cells.append(str(self._visit(cell)))
                rows.append(" & ".join(cells))
        content = " \\\\ ".join(rows)
        return BracedNode(f"\\begin{{matrix}} {content} \\end{{matrix}}")
# ---
def _set_pitch(self, value):
        self._parameter1 = value
# ---
def with_output(self, x):
            out = x + self.w
            return out, 2 * self.w
# ---
def foo(x, y):
        return np.einsum("...ij,...jk->ik", x, y)
# ---
def _test_context(self):
        """
        The context for running validation/inference.
        In this context, the stepper uses the EMA model if
        `self.test_using_ema` is True.
        """
        if self.test_using_ema:
            with self._ema_context():
                yield
        else:
            yield
# ---
def _filter_gen(stream: Iterator, predicate: Callable) -> Iterator:
    for item in stream:
        if predicate(item):
            yield item
# ---
import re
def text_match(text):
        patterns = 'ab*?'
        if re.search(patterns,  text):
                return ('Found a match!')
        else:
                return ('Not matched!')
# ---
def print_hi(name):
    # Use a breakpoint in the code line below to debug your script.
    print(f'Hi, {name}')
# ---
def queue(request):
    """Parameterized fixture providing FileQueue and HttpQueue implementations."""
    if request.param == "file":
        with tempfile.TemporaryDirectory() as tmpdir:
            yield FileQueue(path=tmpdir)
    elif request.param == "http":
        with HttpQueueServer(host="127.0.0.1", port=9999) as server:
            yield server.new_queue("test-queue")
    else:
        raise ValueError(f"Unknown queue type: {request.param}")
# ---
def setReplicaStatus( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndict = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.setReplicaStatus( lfndict )
# ---
def init(wrapped: hnn.Linear, r: int, alpha: float, dropout: float = 0.0, *, key):
        """
        Initializes a LoraLinear module.
        """
        lora = LowRankLinear.init(wrapped.In, wrapped.Out, r, alpha, dropout, key=key)
        return LoraLinear(wrapped, lora)
# ---
def __init__(self):
        super(TestAppModelSimple, self).__init__()
        self.my_key = ""
        self.stringField = ""
# ---
def listeEuler(f, x0, y0, pas, n):
    x, y, L = x0, y0, []
    for k in range(n):
        L += [(x, y)]
        x += pas
        y += pas * f(x, y)
    return L
# ---
def _extract_node_name(self, resource_name: str) -> str:
        """Extract node name from GCP resource path.

        GCP returns 'projects/proj/locations/zone/nodes/my-tpu'
        but gcloud delete expects just 'my-tpu'.
        """
        if "/" in resource_name:
            return resource_name.split("/")[-1]
        return resource_name
# ---
def evaluate(self, record: dict) -> Any:
        return record.get(self.name)
# ---
def fake_raise(*args, **kwargs):
            raise exception.MigrationError(reason='test failure')
# ---
def _parse_avro_schema(self):
        raise NotImplementedError("Not implemented.")
# ---
def number_ctr(str):
      number_ctr= 0
      for i in range(len(str)):
          if str[i] >= '0' and str[i] <= '9': number_ctr += 1     
      return  number_ctr
# ---
def optimized_func(self, r):
        result = 0
        M = len(self.data)
        pool = Pool(processes=4)

        for m in range(M):
            Nm = self.data[m].shape[0] - 1

            k_args = range(Nm + 1)
            self_args = [self] * len(k_args)
            m_args = [m] * len(k_args)
            r_args = [r] * len(k_args)

            result += sum(pool.map(worker_func,
                                   zip(self_args, m_args, k_args, r_args)))

        return result
# ---
def encrypt(pw):
    from hashlib import md5
    return md5(pw).hexdigest()
# ---
def __init__(self, status):
        """Initialize FakeResponse.

        :param status: Either 'failed' or 'passed'
        """
        self.Status = status

        if status == 'failed':
            self.Reason = 'Sample error'
# ---
def pages_per_seq(self) -> int:
        return self.page_indices.axis_size("page")
# ---
def get_address(self) -> str:
        return f"http://{self._server.address}"
# ---
def latest_weight_id(self) -> int | None:
        """
        Returns the latest weight ID that has been transferred.
        """
        return self._latest_weight_id
# ---
def test_get_task_logs_not_found_for_missing_job(client):
    """GetTaskLogs returns NOT_FOUND when the job doesn't exist."""
    resp = client.post(
        "/iris.cluster.ControllerService/GetTaskLogs",
        json={"taskId": JobName.root("nonexistent").task(0).to_wire()},
        headers={"Content-Type": "application/json"},
    )
    assert resp.status_code != 200
    assert "not found" in resp.text.lower()
# ---
def make_json_serializable(row: dict) -> dict:
    """Make a row JSON serializable"""
    for key, value in row.items():
        if isinstance(value, dict):
            row[key] = make_json_serializable(value)
        if isinstance(value, datetime.datetime):
            row[key] = value.isoformat()
        if isinstance(value, np.ndarray):
            row[key] = value.tolist()
        if isinstance(value, np.float32 | np.float64):
            row[key] = float(value)
    return row
# ---
def _use_import_item_table(self, job_id):
        """
            Set the resource and the table to being s3_import_item 
        """

        if self.item_resource == None:
            from s3resource import S3Resource
            (prefix, name) = S3ImportJob.ITEM_TABLE_NAME.split("_",1)
            self.item_resource = S3Resource(prefix, name)
        self.resource = self.item_resource
        self.tablename = S3ImportJob.ITEM_TABLE_NAME
        self.table = S3ImportJob.define_item_table()
# ---
def _quick_task():
    """A simple task that returns immediately."""
    return 42
# ---
def yaml_dict(self):
        ''' getter method for yaml_dict '''
        return self.__yaml_dict
# ---
def matches_constraints(self, constraints: Sequence[cluster_pb2.Constraint]) -> bool:
        """Check if this worker matches all given constraints."""
        for constraint in constraints:
            attr = self.attributes.get(constraint.key)
            if not _evaluate_constraint(attr, constraint):
                return False
        return True
# ---
def __init__(self, format: Optional[str] = "svg") -> None:
        self.format = format
# ---
def _from_array(block, input_array, outchunks=None, asarray=None, block_id=None):
    out = input_array[get_item(outchunks, block_id)]
    if asarray:
        out = np.asarray(out)
    out = numpy_array_to_backend_array(out)
    return out
# ---
def on_compute_end(self, event):
        if len(self.sessions) == 0:
            self.merged_sessions = None
        else:
            self.merged_sessions = merge_sessions(self.sessions)
# ---
def test_greater_than(self):
        expr = col("score") > 100
        assert expr.evaluate({"score": 50}) is False
        assert expr.evaluate({"score": 100}) is False
        assert expr.evaluate({"score": 150}) is True
# ---
def __exit__(self, *_):
        self.shutdown(wait=False)
# ---
def mean(self) -> Scalar:
        return self.sum / self.num
# ---
def run_ansible(params):
        '''run the idempotent ansible code'''
        oc_version = OCVersion(params['kubeconfig'], params['debug'])

        if params['state'] == 'list':

            #pylint: disable=protected-access
            result = oc_version.get()
            return {'state': params['state'],
                    'results': result,
                    'changed': False}
# ---
def resources(self) -> Dict[str, float]:
        """Any resources that this processor needs to run. Ray uses this to schedule tasks."""
        return {}
# ---
def test_running(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_RUNNING) == JobStatus.RUNNING
# ---
def to_proto(self) -> cluster_pb2.AttributeValue:
        """Convert to protobuf representation."""
        proto = cluster_pb2.AttributeValue()
        if isinstance(self.value, str):
            proto.string_value = self.value
        elif isinstance(self.value, int):
            proto.int_value = self.value
        elif isinstance(self.value, float):
            proto.float_value = self.value
        return proto
# ---
def __init__(self, lang):
        '''lang must be a lower-case two-letter language code,
        optionally followed by a "-" and a upper-case two-letter country
        code. (e.g., "en", "en-US", "en-UK", "de", "de-DE", "de-AT")'''
        if self._lang_re.match(lang):
            self.tag = 'dc:language'
            self.text = lang
            self.attr = ()
        else:
            raise ValueError('invalid language format')
# ---
def astype(self, dtype):
        return PreparedBatch(self.data.astype(dtype), self.offsets, self.shapes)
# ---
def apply_mask(ds: xr.Dataset, mask: xr.DataArray):
    """applies mask to same and lower dimensional data"""
    ds_out = xr.Dataset(attrs=ds.attrs)
    for var in ds.data_vars:
        data = ds[var]
        mask_pruned = _pick_first_element_of_missing_dims(mask, data)
        ds_out[var] = data.where(mask_pruned)
    return ds_out
# ---
def get_ludic(n):
	ludics = []
	for i in range(1, n + 1):
		ludics.append(i)
	index = 1
	while(index != len(ludics)):
		first_ludic = ludics[index]
		remove_index = index + first_ludic
		while(remove_index < len(ludics)):
			ludics.remove(ludics[remove_index])
			remove_index = remove_index + first_ludic - 1
		index += 1
	return ludics
# ---
def greater_equal(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "greater_equal")
    return elemwise(nxp.greater_equal, x1, x2, dtype=nxp.bool)
# ---

def prime_length(string):
    """Write a function that takes a string and returns True if the string
    length is a prime number or False otherwise
    Examples
    prime_length('Hello') == True
    prime_length('abcdcba') == True
    prime_length('kittens') == True
    prime_length('orange') == False
    """
    l = len(string)
    if l == 0 or l == 1:
        return False
    for i in range(2, l):
        if l % i == 0:
            return False
    return True
# ---
def skip_if_not_enough_devices(count: int):
    return pytest.mark.skipif(len(jax.devices()) < count, reason=f"Not enough devices ({len(jax.devices())})")
# ---
def setColorSetter(self, colorSetter):
        self._colorSetter = colorSetter
        self._program.setColorSetter(colorSetter)
# ---
def unload(self):
        """Unload the current model."""
        if self.server:
            console.print(f"[blue]Unloading {self.model_name}...[/blue]")
            self.server.shutdown()
            self.server = None
            console.print("[green] Model unloaded[/green]")
        else:
            console.print("[yellow]No model loaded[/yellow]")
# ---
def list_iris_containers(self, all_states: bool = True) -> list[str]:
        """List all containers with iris.managed=true label."""
        cmd = ["docker", "ps", "-q", "--filter", "label=iris.managed=true"]
        if all_states:
            cmd.append("-a")
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        if result.returncode != 0:
            return []
        return [cid for cid in result.stdout.strip().split("\n") if cid]
# ---
def flatten_tuple(test_list):
  res = ' '.join([idx for tup in test_list for idx in tup])
  return (res)
# ---
import math 
def count_Divisors(n) : 
    count = 0
    for i in range(1, (int)(math.sqrt(n)) + 2) : 
        if (n % i == 0) : 
            if( n // i == i) : 
                count = count + 1
            else : 
                count = count + 2
    if (count % 2 == 0) : 
        return ("Even") 
    else : 
        return ("Odd")
# ---
def __init__(
        self,
        job_id: str,
        *,
        ref: ray.ObjectRef | None = None,
        submission_id: str | None = None,
        dashboard_address: str | None = None,
    ):
        self._job_id = job_id
        self._ref = ref
        self._submission_id = submission_id
        self._dashboard_address = dashboard_address
# ---
def string_to_bytes( text ):
    return bin(int.from_bytes(text.encode(), 'big'))
# ---
def match_info(self):
        """return matched info after resolving route"""
        return self.app.router.get(self)[2]
# ---
def set_image_metadata_item(self, image_id, key, meta):
        """Sets the value for a specific image metadata key."""
        post_body = json.dumps({'meta': meta})
        resp, body = self.put('images/%s/metadata/%s' % (str(image_id), key),
                              post_body)
        body = json.loads(body)
        self.validate_response(schema.image_meta_item, resp, body)
        return service_client.ResponseBody(resp, body['meta'])
# ---
from typing import List, Any


def filter_integers(values: List[Any]) -> List[int]:
    """ Filter given list of any python values only for integers
    >>> filter_integers(['a', 3.14, 5])
    [5]
    >>> filter_integers([1, 2, 3, 'abc', {}, []])
    [1, 2, 3]
    """
    return [x for x in values if isinstance(x, int)]
# ---
def _getPoint(self, point):
        """
        Converts a Point type into an Excellon coordinate
        """
        return "X%.6fY%.6f\n" % (point.x, -point.y)
# ---
def remaining_ms(self) -> int:
        """Get remaining milliseconds until deadline (0 if expired)."""
        remaining_seconds = self._deadline - time.monotonic()
        return max(0, int(remaining_seconds * 1000))
# ---
def test_stop_not_inited(self):
        profiler.clean()
        profiler.stop()
# ---
def unique_name(prefix: str) -> str:
    """Generate a unique job name with the given prefix."""
    return f"{prefix}-{uuid.uuid4().hex[:8]}"
# ---
def axis_size(self, axis: Sequence[AxisSelector]) -> tuple[int, ...]:  # type: ignore
        ...
# ---
def get_config(self) -> omegaconf.DictConfig:
        config = omegaconf.OmegaConf.create({})
        config_file = omegaconf.OmegaConf.load(self.config_path)
        config = omegaconf.OmegaConf.merge(config, config_file)
        args_cfg = omegaconf.OmegaConf.from_dotlist(self.args)
        config = omegaconf.OmegaConf.merge(config, args_cfg)
        return config
# ---
def __iter__(self) -> Iterator[PageCacheT]:
        return iter(self.caches)
# ---
def join_protocol(path):
        return f"{protocol}://{path}" if protocol else path
# ---
def list_jobs(filters: list[str] | None = None) -> list[dict]:
    """Fetch the list of jobs using the Ray CLI."""
    cmd = ["ray", "list", "jobs", "--detail", "--format=json", "--limit=10000"]
    for f in filters or []:
        cmd.extend(["--filter", f])

    result = run_ray_command(cmd)
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError:
        logger.warning(f"Failed to parse JSON output from ray list jobs: {result.stdout} -- {result.stderr}")
        return []
# ---
def setup_triggers(self, env):
        """ Add the necessary triggers to invalidate/recompute ``self``. """
        model = env[self.model_name]
        for path in self.depends:
            self._setup_dependency([], model, path.split('.'))
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step, commit: Optional[bool] = None):
        pass
# ---
def ss_all(self, tokens, random):
        design_mask = tokens["design_mask"].astype(bool)
        tokens["design_ss_mask"][design_mask] = 1
# ---
def add_ports(self, inc_ports):
        ''' add a port object to the ports list '''
        if not isinstance(inc_ports, list):
            inc_ports = [inc_ports]

        ports = self.get_ports()
        if not ports:
            self.put(Service.port_path, inc_ports)
        else:
            ports.extend(inc_ports)

        return True
# ---
def get_role_name(region, account_id, role):
    """Shortcut to insert the `account_id` and `role` into the iam string."""
    prefix = ARN_PREFIXES.get(region, "aws")
    return "arn:{0}:iam::{1}:role/{2}".format(prefix, account_id, role)
# ---
def test_binary_set(self):
        """Store and retrieve a binary set"""
        self.make_table()
        item = {
            "id": "a",
            "datas": set([Binary("a"), Binary("b")]),
        }
        self.dynamo.put_item("foobar", item)
        ret = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(ret, item)
# ---
def next_sleep_interval() -> float:
    now = get_now(datetime.timezone.utc)
    target = now.replace(second=0) + WAKEUP_INTERVAL
    return (target - now).total_seconds()
# ---
def n_conceptos(self):
        return self.__n_conceptos
# ---
def test_actor_named_without_get_if_exists(job_context):
    """Test that named actors without get_if_exists create new instances."""
    actor1 = job_context.create_actor(SimpleActor, 10, name="actor", get_if_exists=False)
    job_context.get(actor1.increment.remote(5))

    with pytest.raises(ValueError):
        job_context.create_actor(SimpleActor, 20, name="actor", get_if_exists=False)
# ---
def __init__(self, num_prognostic_channels: int, ctx: GridContext):
        self.num_prognostic_channels = num_prognostic_channels
        self.ctx = ctx
        self.example_by_step: list[Example] = []
        self.load_stats: LoadStats | None = None
# ---
def handle_shutdown(_signum, _frame):
        logger.info("Shutdown signal received, stopping controller...")
        controller.stop()
        logger.info("Controller stopped")
        stop_event.set()
# ---
def find_gcd(x, y): 
	while(y): 
		x, y = y, x % y 
	return x 
def get_gcd(l):
  num1 = l[0]
  num2 = l[1]
  gcd = find_gcd(num1, num2)
  for i in range(2, len(l)):
    gcd = find_gcd(gcd, l[i])
  return gcd
# ---
def dealias_binding(self, binding: str) -> AxisSelector | None:
        return self.bindings.get(binding, None)
# ---
def selected_target_account(self):
        return self.selected_pane.selected_target
# ---
def __setitem__(self, index: int, obj: T):
        self._index_to_obj[index] = obj
        self._obj_to_index[obj] = index
# ---
def terminate_all(self) -> None:
        """Terminate all VM groups in this scale group."""
        with self._vm_groups_lock:
            snapshot = list(self._vm_groups.values())
            self._vm_groups.clear()
        for vm_group in snapshot:
            vm_group.terminate()
# ---
def list_slices(self, group_config: config_pb2.ScaleGroupConfig) -> list[str]: ...
# ---
def local_config():
    """Create weight transfer config for local filesystem."""
    with tempfile.TemporaryDirectory() as temp_dir:
        config = WeightTransferConfig(
            mode=WeightTransferMode.GCS_CHECKPOINT,
            checkpoint_dir=temp_dir,
        )
        yield config
# ---
def static_files_mount() -> Mount:
    """Mount for serving static JS/CSS assets (vendor libs, shared utils, app components)."""
    return Mount("/static", app=StaticFiles(directory=STATIC_DIR), name="static")
# ---
def test_laplace():
    check_gen_is_equal(lambda k, s: jax.random.laplace(k, s), lambda k, s: hax.random.laplace(k, s))
# ---
def __init__(self, ray_options: dict | None = None):
        self.ray_options = ray_options or {}
# ---
def is_delta_time(self):
        '''
        Return a boolean if this is a DeltaTime subclass.

        >>> mt = MidiTrack(1)
        >>> dt = DeltaTime(mt)
        >>> dt.is_delta_time()
        True
        '''
        if self.type_ == "DeltaTime":
            return True
        return False
# ---
def __post_init__(self):
        if len(self.configs) == 0:
            raise ValueError("At least one dataset must be provided")

        if set(self.configs.keys()) != set(self.train_weights.keys()):
            raise ValueError(
                f"The keys in configs and weights must be the same;got {self.configs.keys()} and"
                f" {self.train_weights.keys()}"
            )
# ---
def test_from_list(sample_data, backend):
    """Test creating dataset from list."""
    ds = Dataset.from_list(sample_data)
    assert list(Backend.execute(ds, context=backend)) == sample_data
# ---
def to_dataframe(self, message, dtypes=None):
        raise NotImplementedError("Not implemented.")
# ---
def _write_jsonl_gz(path: Path, rows: list[dict]) -> None:
    with gzip.open(path, "wt", encoding="utf-8") as handle:
        for row in rows:
            handle.write(json.dumps(row) + "\n")
# ---
def GetBufferNumberForFilename( filename, open_file_if_needed = True ):
  return GetIntValue( u"bufnr('{0}', {1})".format(
      EscapeForVim( os.path.realpath( filename ) ),
      int( open_file_if_needed ) ) )
# ---
def all_inputs_const(op):
    # If all inputs of an op are guaranteed constants, then we can infer that
    # the op produces a constant as well.
    return op.inputs and all(inp.op in constants for inp in op.inputs)
# ---
def _hard_sample(self, logits):
    assert logits.ndim == 2
    indices = torch.argmax(logits, dim=-1)
    zeros = logits * 0
    ones = torch.ones_like(logits[:, :, :1])
    return torch.scatter(zeros, -1, indices[:, :, None],
                         ones)
# ---
def render(self, task: "Task") -> Text:
        """Show completed/total."""
        completed = int(task.completed)
        total = int(task.total) if task.total is not None else "?"
        total_width = len(str(total))
        return Text(
            f"{completed}{self.separator}{total}".ljust(total_width + 1 + total_width),
            style="progress.download",
        )
# ---
def _get_nnx_key_name(split_key: list[str]) -> str:
    """
    Determine the NNX key name from the split Levanter key.
    If the key ends in 'bias', append '_bias' to the parameter name.
    Otherwise (e.g. 'weight'), use the parameter name directly.
    """
    key_name = split_key[-2]
    if split_key[-1] == "bias":
        key_name = f"{key_name}_bias"
    return key_name
# ---
def inc(x):
    global count
    count = count + 1
    return x + 1
# ---
def native_value(self) -> datetime | None:
        """Return sensor state."""
        job: OctoprintJobInfo = self.coordinator.data["job"]

        if (
            not job
            or not job.progress.print_time
            or not _is_printer_printing(self.coordinator.data["printer"])
        ):
            return None

        read_time = self.coordinator.data["last_read_time"]

        return read_time - timedelta(seconds=job.progress.print_time)
# ---
def forward(self, fts_input: Input, fts: Prognostic) -> Prognostic:
        """Apply correction to the input features.

        Args:
            fts_input: Input tensor to correct
            fts: Output tensor to correct

        Returns:
            Corrected output tensor
        """
        raise NotImplementedError
# ---
def remove(url, *, recursive=False, **kwargs):
    """Remove a file from a remote filesystem."""
    # TODO: better to use a STS deletion policy or job for this one.
    fs, path = fsspec.core.url_to_fs(url, **kwargs)

    fs.rm(path, recursive=recursive)
# ---
def screenshot(self, includes='subtitles', mode='single'):
        self.command('screenshot', includes, mode)
# ---
def _add_snapshot_info(conn, vm, params):
    # Snapshot related API is not yet implemented in the libvirt's Xen driver
    if conn.getType() == 'Xen':
        return

    try:
        ret = vm.hasCurrentSnapshot()
    except libvirt.libvirtError:
        logging.exception('Error checking for existing snapshots.')
    else:
        params['has_snapshots'] = ret > 0
# ---
def test_from_files():
    with tempfile.TemporaryDirectory() as tmpdir:
        for i, program in enumerate(SAMPLE_PROGRAMS[:2]):
            (Path(tmpdir) / f"prog_{i}.py").write_text(program)

        # Also write a non-Python file that should be skipped.
        (Path(tmpdir) / "readme.txt").write_text("not python")

        paths = list(Path(tmpdir).glob("*"))
        bank = SubtreeBank.from_files(paths)
        assert bank.total_entries > 0
# ---
def set_failure_mode(self, mode: FailureMode) -> None:
        """Set the failure mode for subsequent operations."""
        self._config.failure_mode = mode
# ---
def option_info(self, name):
        return self._get_property('option-info/'+name)
# ---
def dot(
        self,
        *args: "NamedArray",
        axis: AxisSelection | None,
        precision: PrecisionLike = None,
        dot_general=jax.lax.dot_general,
    ) -> "NamedArray": ...
# ---
import sys 
def tuple_size(tuple_list):
  return (sys.getsizeof(tuple_list))
# ---
def init(In, Out, key):
            blocks = hax.nn.Stacked.init(Layer, Block)(In, Out, key=jax.random.split(key, Layer.size))
            return Tformer(blocks)
# ---
def get_windows(self):
            """Get the windows currently attached to this display.

            :rtype: sequence of `Window`
            """
            raise NotImplementedError('deprecated')
# ---
def microbatch_size(self) -> int | None:
        if self.per_device_parallelism < 0:
            return None
        return self.per_device_parallelism * self.data_axis_size
# ---
def decode(x):
                text = x[self.text_key]
                audio_pointer = x[self.audio_key]
                audio = AudioTextUrlDataSource.resolve_audio_pointer(audio_pointer, self.sampling_rate)
                return (audio["array"], audio["sampling_rate"], text)
# ---
def test_and_both_true(self):
        expr = (col("a") > 0) & (col("b") > 0)
        assert expr.evaluate({"a": 1, "b": 1}) is True
# ---
def args(self):
        if self.parsed_args is None:
            if self.query_string:
                self.parsed_args = RequestParameters(
                    parse_qs(self.query_string)
                )
            else:
                self.parsed_args = RequestParameters()
        return self.parsed_args
# ---
def readline(self, count=None): pass
# ---
def calculate_information(self):
        """
        http://en.wikipedia.org/wiki/Information_ratio
        """
        return information_ratio(self.algorithm_returns,
                                 self.benchmark_returns)
# ---
def test_plain(self):
        self._test_proxy(_result.ResultProxy)
# ---
def on_predict_epoch_end(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,  # noqa: ARG002
    ) -> None:
        """Print the number of failed examples."""
        print(f"Number of failed examples: {self.failed}")
# ---
def round_num(n,m):
    a = (n //m) * m
    b = a + m
    return (b if n - a > b - n else a)
# ---
def _es_args(self, resource, refresh=None):
        """Get index and doctype args."""
        datasource = self._datasource(resource)
        args = {
            'index': self.index,
            'doc_type': datasource[0],
            }
        if refresh:
            args['refresh'] = refresh
        return args
# ---
def named_array_to_tensor(named_array):
        return torch.from_numpy(np.array(named_array.array))
# ---
def set_left_most_unset_bit(n): 
    if not (n & (n + 1)): 
        return n 
    pos, temp, count = 0, n, 0 
    while temp: 
        if not (temp & 1): 
            pos = count      
        count += 1; temp>>=1
    return (n | (1 << (pos)))
# ---
def fold_fun(acc, x, static1, *, static2):
        assert static1 is True
        assert static2 is False
        return NamedArray(acc.array + x.rearrange(acc.axes).array, acc.axes)
# ---
def http_error_401(self, *args, **kwds):
                self.parent.record("digest")
                urllib.request.HTTPDigestAuthHandler.http_error_401(self,
                                                             *args, **kwds)
# ---
def err(context):
            stmt = context.statement
            exception = context.original_exception
            if "ERROR ONE" in str(stmt):
                return MyException("my exception")
            elif "ERROR TWO" in str(stmt):
                return exception
            else:
                return None
# ---
def numeric_to_string(
    name: Union[Tuple[int, int, int, int], Tuple[int, int, int, int, int]],
) -> str:
    name = [chr(c + 32) for c in name if c != 0]
    name = "".join(name)
    return name
# ---
def open(self, filename, attrib="rb"):
        '''
        Open a MIDI file path for reading or writing.

        For writing to a MIDI file, `attrib` should be "wb".
        '''
        if attrib not in ['rb', 'wb']:
            raise MidiException('cannot read or write unless in binary mode, not:', attrib)
        self.file = open(filename, attrib)
# ---
def write_binary_file(records: Iterable[bytes], output_path: str) -> dict:
    """Write binary records to a file."""
    ensure_parent_dir(output_path)

    count = 0
    with atomic_rename(output_path) as temp_path:
        with fsspec.open(temp_path, "wb", block_size=64 * 1024 * 1024) as f:
            for record in records:
                f.write(record)
                count += 1

    return {"path": output_path, "count": count}
# ---
def _add_log(self, cluster: str, level: str, message: str, details: str | None = None) -> None:
        entry = LogEntry(
            timestamp=datetime.now(),
            cluster=cluster,
            level=level,
            message=message,
            details=details,
        )
        with self._logs_lock:
            self._logs.append(entry)
# ---
def project_back(
    grad: Array,
    Q: List[Union[Array, None]],
    precision: jax.lax.PrecisionLike = jax.lax.Precision.HIGHEST,
) -> Array:
    for mat in Q:
        if mat is not None:  # noqa: SIM108
            grad = jnp.tensordot(
                grad,
                mat,
                axes=((0,), (1,)),
                precision=precision,
            )
        else:
            grad = jnp.moveaxis(grad, 0, -1)

    return grad
# ---
def test_simple_limit(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).limit(2),
            [(1, 1, 2), (2, 2, 3)],
        )
# ---
def _is_slice_like(value: Any) -> bool:
    return isinstance(value, (slice, range, HaliaxDSlice)) or is_pallas_dslice(value)
# ---
def test_load_math_environment():
    """Test loading MathEnv via EnvConfig."""
    config = EnvConfig(env_class="marin.rl.environments.math_env.MathEnv", env_args={"seed": 42})

    env = load_environment_from_spec(config)

    assert isinstance(env, MathEnv)
    assert len(env.train_examples) > 0
    assert len(env.eval_examples) > 0
# ---
def worker_func_der(args):
    self = args[0]
    m = args[1]
    k = args[2]
    r = args[3]
    i = args[4]

    return ((self.eval_func(m, k, r) -
             self.eval_func(m, k, self.rt) -
             self.temporal_diff_sum(m, k)) * 2 *
            self.eval_func_der(m, k, r, i))
# ---
def modal_executor(request):
    return request.param
# ---
def name(self):
    """Function name."""
    self._create_definition_if_needed()
    return self._func_name
# ---
def skip_if_module_missing(module: str):
    def try_import_module(module):
        try:
            __import__(module)
        except ImportError:
            return False
        else:
            return True

    return pytest.mark.skipif(not try_import_module(module), reason=f"{module} not installed")
# ---
def _slice_cache_in_ray(cfg: SliceCacheConfig):
    logging.basicConfig(level=logging.INFO)
    logger.info(f"Starting slice cache with config: {cfg}")
    return _do_slice_cache(cfg)
# ---
def __init__(self, loggers: List[Tracker]):
        self.loggers = loggers
# ---
def __str__(self):
        return "<" + ", ".join(str(v) for v in self.values) + ">"
# ---
def from_named_array(array: hax.NamedArray, num_bins: int = 31) -> "Histogram":
        raw_array = array.array
        min = raw_array.min()
        max = raw_array.max()
        num = array.size
        sum = raw_array.sum()
        sum_squares = (raw_array**2).sum()
        counts, edges = sharded_histogram(array, bins=num_bins)
        return Histogram(min, max, num, sum, sum_squares, edges, counts)
# ---
def severity_score(name):
    return const.liability_severity.get(name, const.default_severity)
# ---
def do_main():
  fix_args()

  updateCB.newExec()
  target = PARAMS["TARGET"]
  if not(PARAMS["PUT_RANDOM"]):
    photowall(target)
  else:
    random_wall(target)
# ---
def __init__(self, title, info):
        self.title = title
        self.info = info
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.query_class == other.query_class
# ---
def _flatmap_gen(stream: Iterator, fn: Callable) -> Iterator:
    for item in stream:
        yield from fn(item)
# ---
def slice_any_failed(slice_info: vm_pb2.SliceInfo) -> bool:
    """Compute any_failed from vms[] in proto."""
    return any(vm.state in (vm_pb2.VM_STATE_FAILED, vm_pb2.VM_STATE_PREEMPTED) for vm in slice_info.vms)
# ---
def start(self) -> str:
        """Start the controller. Returns the controller address.

        For GCP: creates a GCE VM, bootstraps, returns internal IP.
        For local: starts in-process Controller, returns localhost URL.
        """
        self._controller = create_controller_vm(self._config, threads=self._threads)
        address = self._controller.start()
        logger.info("Controller started at %s (local=%s)", address, self.is_local)
        return address
# ---
def get_queryset(self):
        return CommandExecution.objects.filter(
            user_id=str(self.request.user.id)
        )
# ---
def get_stop_sequences(self) -> list[str] | list[int]:
        raise NotImplementedError
# ---
def test_as_remote_kwargs_gpu_auto():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=GpuConfig(variant="auto", count=1))
    kwargs = as_remote_kwargs(config)
    assert kwargs["num_gpus"] == 1
    assert "accelerator_type" not in kwargs
# ---
def apply(self, source: str) -> str:
        """Apply this mutation to a source string."""
        return source[: self.start] + self.replacement + source[self.end :]
# ---
def __enter__(self) -> "JobGroup":
        self._entered = True
        return self
# ---
def test_is_stop_signal_exact_match():
    # tail_tokens matches stop_sequence exactly
    tail_tokens = hax.named(jnp.array([5, 6, 7], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[5, 6, 7]], dtype=jnp.int32), axis=("seq", "position"))
    assert is_stop_signal(tail_tokens, stop_sequences)
# ---
def cursor_execute(conn, cursor, statement, parameters,
                                context, executemany):
            cursor_stmts.append((str(statement), parameters, None))
# ---
def error_messages(self) -> list[str]:
        """Collect non-empty error messages from VMs."""
        return [v.init_error for v in self.vms if v.init_error]
# ---
def _vm_detail_page(self, _request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("VM Detail", "/static/controller/vm-detail.js"))
# ---
def commit_callback():
        elapsed = time.time() - start_time
        logger.info(f"Checkpoint committed to Tensorstore successfully! Total time: {elapsed:.2f}s")
# ---
from typing import List


def filter_by_substring(strings: List[str], substring: str) -> List[str]:
    """ Filter an input list of strings only for ones that contain given substring
    >>> filter_by_substring([], 'a')
    []
    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')
    ['abc', 'bacd', 'array']
    """
    return [x for x in strings if substring in x]
# ---
def batch_completions(self, prompts, temperature, n, max_tokens=None, stop=None, system_prompt=None):
            completions = []
            for prompt in prompts:
                responses = [f"mock_response_{i}" for i in range(n)]
                completion = create_test_chat_completion(prompt, responses)
                completions.append(completion)
            return completions
# ---
def test_ar_loss_zero_mask_gives_zero_loss(params, tiny_cfg):
    batch_size, seq_len = 2, 16
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (batch_size, seq_len), 1, 100)
    loss_mask = jnp.zeros((batch_size, seq_len))

    loss, _ = ar_loss(params, token_ids, loss_mask, tiny_cfg)
    assert float(loss) == 0.0
# ---
def click(self, on_element=None):
        """Clicks an element.
        Args:
            on_element: The element to click.
                        If None, clicks on current mouse position.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.CLICK, {'button': 0}))
        return self
# ---
def inside_step(self, state: S, inside_info: InsideJitInfo[M]) -> CBInfo:
        """
        This function is called inside the JIT-compiled function. You have access to the `inside_info` which contains
        information about the gradients, updates, and other information that was computed during the step.
        Args:
            state:
            inside_info:

        Returns:

        """
        ...
# ---
def on_validation_end(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,
    ) -> None:
        """Restore original weights after validation.

        Parameters
        ----------
        trainer: Trainer
            The Trainer instance.
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._on_eval_end(pl_module)
# ---
def gcs_config():
    """Create weight transfer config for GCS checkpoint mode."""
    return WeightTransferConfig(
        mode=WeightTransferMode.GCS_CHECKPOINT,
        checkpoint_dir="gs://marin-eu-west4/rl_testing/llama-1b-math-rl-test-0-35d621/policy_checkpoints",
    )
# ---
def addFile( self, lfn, force = False, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfndicts = res['Value']
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.addFile( lfndicts, force )
# ---
def pages_and_slots(self):
        token_dests = self.new_token_dests

        t_pages = hax.where(is_valid(token_dests), token_dests // self.page_size, INVALID)
        t_slots = hax.where(is_valid(token_dests), token_dests % self.page_size, INVALID)

        return t_pages, t_slots
# ---
def convert_to_export(self, value, env):
        if value or value == 0.0:
            return value if env.context.get('export_raw_data') else ustr(value)
        return ''
# ---
def __repr__(self) -> str:
        return f"Deadline(remaining={self.remaining_seconds():.3f}s)"
# ---
def test_as_array_fails(spec):
    a = np.ones((1000, 1000))
    with pytest.raises(
        ValueError,
        match="Size of in memory array is 8.0 MB which exceeds maximum of 1.0 MB.",
    ):
        xp.asarray(a, chunks=(100, 100), spec=spec)
# ---
def flatten_for_export(self) -> "XIELUActivation":
        """Expand scalar parameters to [1] shape for HF checkpoint compatibility."""
        Param = Axis("xielu_param", 1)
        alpha_p = hax.named(self.alpha_p.array.reshape(1), Param)
        alpha_n = hax.named(self.alpha_n.array.reshape(1), Param)
        return XIELUActivation(alpha_p, alpha_n, self.beta, self.eps)
# ---
def inspect(self, container_id: str) -> ContainerStatus: ...
# ---
def HR_knockout(lab, award, target):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'knockout',
        'purpose': 'repression',
        'method': 'homologous recombination',
        'modified_site_by_target_id': target['@id']
    }
# ---
def _default_jax_compilation_cache_dir() -> str:
    marin_prefix = os.environ.get("MARIN_PREFIX")
    if marin_prefix:
        return os.path.join(marin_prefix, "compilation-cache")
    return "/tmp/marin-jax-compilation-cache"
# ---
def test_works_after_dispose_testing_engine(self):
        eng = engines.testing_engine()
        for i in range(3):
            eq_(eng.scalar(select([1])), 1)
            eng.dispose()
# ---
def test_str_representation(self):
        """Test string representation of S3Location."""
        loc = S3Location(bucket="test-bucket", path="data/test.zarr")
        assert str(loc) == "s3://test-bucket/data/test.zarr"
# ---
def count_unknown_letters_in_expr(expr: str):
    expr = expr.replace("sqrt", "")
    expr = expr.replace("frac", "")
    letters_in_expr = set([x for x in expr if x.isalpha()])
    return len(letters_in_expr)
# ---
def multiply(self, a: int, b: int) -> int:
        return a * b
# ---
def _files(self):
        if self.isexact():
            return [f for f in self._m1.files() if self(f)]
        # If m1 is not an exact matcher, we can't easily figure out the set of
        # files, because its files() are not always files. For example, if
        # m1 is "path:dir" and m2 is "rootfileins:.", we don't
        # want to remove "dir" from the set even though it would match m2,
        # because the "dir" in m1 may not be a file.
        return self._m1.files()
# ---
def compute_axis_mapping(self) -> ResourceMapping:
        """Mapping from logical axis to physical axis for compute."""
        return self.mesh.resolved_compute_mapping
# ---
def training_data(self) -> Iterator[MockEnvExample]:
        """Stream training data."""
        for example in self.train_examples:
            yield MockEnvExample(
                raw_prompt=example["prompt"],
                raw_answer=example["answer"],
                processed_prompt=example["prompt"],
                processed_answer=example["answer"],
                metadata={"task_type": self.task_type},
            )
# ---
def number(self, val):
        self.num_ctrl.SetValue(str(val))
# ---
def push(self, item: Any) -> None:
        with httpx.Client() as client:
            client.post(f"http://{self.host}:{self.port}/queues/{self.queue_name}/push", content=pickle.dumps(item))
# ---
def pretty_print(self):
        for i, step in enumerate(self.steps):
            print(f"[{i + 1}] {step.name:25s}")
# ---
def minimum(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.minimum(x1, x2)
# ---
def __is_nice_string_using_new_rules(self, string):
        return (self.__regex_double_pair.search(string)
            and self.__regex_triplet.search(string))
# ---
def test_change_permission(self):
        """
        Tests that only staff can change entries
        """
        self.assertTrue(self.creator_admin.has_change_permission(self.request))

        self.request.user = self.user
        self.assertFalse(self.creator_admin.has_change_permission(self.request))
# ---
def list_tasks(self, request: cluster__pb2.Controller.ListTasksRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListTasksResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def get_committed_resources(self) -> tuple[int, int, int]:
        """Return committed (cpu, memory_bytes, gpu_count) for this worker."""
        return (self.committed_cpu, self.committed_mem, self.committed_gpu)
# ---
def _metric_flatten(m: Metric):
    """Flatten Metric for JAX - reduction is aux_data, value/count are children."""
    return (m._value, m._count), m.reduction
# ---
def __init__(self):
        self.priority = 1
        self.language = ['en']
        self.domains = ['coolmoviezone.online']
        self.base_link = 'https://coolmoviezone.online'
        self.scraper = cfscrape.create_scraper()
# ---
def __call__(self, q: NamedArray, position_ids: NamedArray) -> NamedArray:
        inv_freq_llama = self._compute_inv_freq_llama()
        freqs = position_ids * inv_freq_llama.broadcast_axis(position_ids.axes)
        emb = hax.concatenate(self.HeadDim, (freqs, freqs))
        cos = hax.cos(emb).astype(q.dtype)
        sin = hax.sin(emb).astype(q.dtype)

        q_embed = q * cos + _rotate_half(q, self.HeadDim) * sin
        return q_embed
# ---
def determine_draft_value(self, record):
        """ Determine the value of ``self`` for the given draft ``record``. """
        if self.compute:
            self._compute_value(record)
        else:
            record._cache[self] = SpecialValue(self.null(record.env))
# ---
def test_package_installation():
    with TemporaryVenv(pip_install_args=["six"]) as venv:
        result = venv.run(
            [venv.python_path, "-c", "import six; print(six.__version__)"],
            capture_output=True,
            text=True,
        )
        assert result.returncode == 0
        assert len(result.stdout.strip()) > 0
# ---
def test_current_pixel_wraps_right(self):
        self.mda.current_pixel = [719, 0]
        self.mda.io_read_byte(0x3BA)
        self.assertEqual(self.mda.current_pixel, [0, 1])
# ---
def accumulate_w_grad():
        res = jax.lax.dot_general(
            x_ref[...],
            xw_scratch_ref[...],
            (((0,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        w_read_future.wait()
        w_grad_tile_ref[...] += res
        w_write_future.start()
# ---
def _staggered_score(self, score, dsigma):
    score = score.clone()
    extra_const = (1 - dsigma.exp()) * score.sum(dim=-1)
    score *= dsigma.exp()[:, None]
    score[..., self.mask_index] += extra_const
    return score
# ---
def style_css(self):
        '''CSS stylesheet for the files that are generated by the EpubBuilder
        instance. Can be overwritten or extended, but not deleted.'''
        return self._style_css
# ---
def default_choice_name(cls) -> Optional[str]:
        return "wandb"
# ---
def without_axes(axis_spec: PartialShapeDict, to_remove: AxisSelection, allow_mismatched_sizes=False) -> PartialShapeDict:  # type: ignore
    ...
# ---
def init(config: MistralConfig, *, key) -> "MixtralTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, MixtralDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return MixtralTransformer(config, layers, ln_f)
# ---
def FetchTaskLogs(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def set_frequency(self, freq):
        self._frequency = freq
        self._lo.set_frequency(self._frequency + self._if_frequency)
        self._requested_cal = self.get_calibration(self._frequency,
                                                   self._power)
        self._output_SSB()
# ---
def is_checkpoint_dir(path: str):
        return fs.exists(os.path.join(path, "metadata.json"))
# ---
def string_list_to_tuple(str1):
    result = tuple(x for x in str1 if not x.isspace()) 
    return result
# ---
def forward(
        self,
        z: Tensor,
        pair_mask: Tensor,
        use_kernels: bool = False,
    ) -> Tensor:
        for layer in self.layers:
            if self.activation_checkpointing:
                z = torch.utils.checkpoint.checkpoint(layer, z, pair_mask)
            else:
                z = layer(z, pair_mask)
        return z
# ---
def test_len():
    with tempfile.TemporaryDirectory() as tmpdir:
        exemplar = {"a": np.array([0], dtype=np.float64), "b": np.array([0], dtype=np.float64)}
        builder = TreeStore.open(exemplar, tmpdir)

        assert len(builder) == 0

        batch = [
            {"a": np.array([1.0, 2.0]), "b": np.array([3.0, 4.0])},
            {"a": np.array([5.0, 6.0]), "b": np.array([7.0, 8.0])},
        ]
        builder.extend(batch)

        assert len(builder) == 2
# ---
def exitChatty(self):
        self.ignore(self.chattyDoneEvent)
        self.chatty.exit()
# ---
import re
pattern = 'fox'
text = 'The quick brown fox jumps over the lazy dog.'
def find_literals(text, pattern):
  match = re.search(pattern, text)
  s = match.start()
  e = match.end()
  return (match.re.pattern, s, e)
# ---
def get_properties(self):
        d = COMMON_PROPERTIES.copy()
        d.update(CONSOLE_PROPERTIES)
        return d
# ---
def do_get_name(self, event):
        self.name = configurator.get_team_name(self.number)
# ---
def front_and_rear(test_tup):
  res = (test_tup[0], test_tup[-1])
  return (res)
# ---
def step_impl(context, user_name):
    user_exists = context.user_service.exists(user_name)
    assert context.base_user.username == user_exists['username']
    assert context.base_user.password == user_exists['password']
    assert context.base_user.email == user_exists['email']
    assert context.base_user.first_name == user_exists['first_name']
    assert context.base_user.last_name == user_exists['last_name']
    assert user_exists['_id'] is not None
# ---
def intersects(self, bbox, srs):
        bbox = self._geom_in_coverage_srs(bbox, srs)
        with self._prep_lock:
            return self.prepared_geom.intersects(bbox)
# ---
def test_actor_exception_propagation():
    """Test that exceptions from actor methods propagate to the client."""
    server = ActorServer(host="127.0.0.1")
    server.register("calc", Calculator())
    port = server.serve_background()

    try:
        resolver = FixedResolver({"calc": f"http://127.0.0.1:{port}"})
        client = ActorClient(resolver, "calc")
        with pytest.raises(ZeroDivisionError):
            client.divide(1, 0)
    finally:
        server.stop()
# ---
def era_shuffle(self, era_length: int, key: PRNGKeyArray, *, perm_type: PermType = "feistel"):
        import levanter.data.permutation as permutation

        return permutation.EraShufflingDataset(self, era_length, key=key, perm_type=perm_type)
# ---
def _build_cmd(self, command: str) -> list[str]:
        return [
            "gcloud",
            "compute",
            "tpus",
            "tpu-vm",
            "ssh",
            self.vm_id,
            f"--zone={self._zone}",
            f"--project={self.project_id}",
            f"--worker={self.worker_index}",
            "--quiet",
            "--command",
            command,
        ]
# ---
def check_files(context):
        """Return the number of present and absent files."""
        present = 0
        absent = 0
        for filename in context.filenames:
            if os.path.exists(os.path.join(context.temp_dir, filename)):
                present += 1
            else:
                absent += 1
        return context.temp_dir, present, absent
# ---
def current_in_milliamps(self):
        return self.current * 1000
# ---
def __iter__(self):
        return iter(self.indices)
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        return YarnRotaryEmbeddings.init(HeadSize, self)
# ---
from typing import List


def concatenate(strings: List[str]) -> str:
    """ Concatenate list of strings into a single string
    >>> concatenate([])
    ''
    >>> concatenate(['a', 'b', 'c'])
    'abc'
    """
    return ''.join(strings)
# ---
def expm1(a: A) -> A:
    return wrap_elemwise_unary(jnp.expm1, a)
# ---
def parabola_directrix(a, b, c): 
  directrix=((int)(c - ((b * b) + 1) * 4 * a ))
  return directrix
# ---
def chat(self, message: Optional[str] = None):
        """Chat with the model."""
        if not self.server:
            console.print("[red]No model loaded. Use 'load' command first[/red]")
            return

        if message:
            messages = [ChatMessage(role="user", content=message)]
            self._run_chat_completion(messages)
        else:
            self._run_chat_session()
# ---
def loss_fn(model, data):
        m = jax.vmap(model)
        return jnp.mean(jnp.square(m(data)))
# ---
def test_using_capsys_fixture_works_with_sys_stdout_encoding(capsys):
    test_text = 'test text'

    print(test_text.encode(sys.stdout.encoding, 'replace'))
    (out, err) = capsys.readouterr()
    assert out
    assert err == ''
# ---
def test_capture_results_accessible_by_attribute(self):
        with self.getcapture() as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            capture_result = cap.readouterr()
        assert capture_result.out == "hello"
        assert capture_result.err == "world"
# ---
def out_degree_unique(dag, name):
    """Returns number of unique out edges"""
    return len(set(post for _, post in dag.out_edges(name)))
# ---
def scopes_set(self, scopes):
        self._scopes = scopes
        if self.req is not None:
            self.req.environ.setdefault('wenoit_etalage', {})['_scopes'] = scopes
# ---
def subtotal(self):
        return self.__subtotal
# ---
def send_font(filename):
    return static_file(filename, root='fonts')
# ---
def __rmod__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__rmod__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.remainder, other, self, dtype=result_type(self, other))
# ---
def test_connect_as_ctx_noautocommit(self):
        fn = self._trans_fn()
        self._assert_no_data()
        ctx = testing.db.connect().execution_options(autocommit=False)
        testing.run_as_contextmanager(ctx, fn, 5, value=8)
        # autocommit is off
        self._assert_no_data()
# ---
def __eq__(self, other):
        return type(self) == type(other)
# ---
def build_map(hashes: list[str], ids: list[str]) -> dict[str, Any]:
    """
    Builds a duplicate map where ~50% of items are marked as canonical and 50% as duplicates.
    """
    dup_map = {}
    for i, (h, rec_id) in enumerate(zip(hashes, ids, strict=True)):
        # Every other element is treated as a duplicate of "other_canonical_id"
        canon = rec_id if i % 2 != 0 else "other_canonical_id"
        dup_map[h] = {"canonical": canon}
    return dup_map
# ---
def test_concat_str(self):
        def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1, df2])
            return (A3.two == 'foo').sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def _MakeModuleBlock():
  return block.ModuleBlock(None, '__main__', '<test>', '',
                           imputil.FutureFeatures())
# ---
def lcm(a, b):
        return a * b / gcd(a, b)
# ---
def test_reduce_will_blow_your_mind(self):
        import functools
        # As of Python 3 reduce() has been demoted from a builtin function
        # to the functools module.

        result = functools.reduce(self.add, [2, 3, 4])
        self.assertEqual(int, result.__class__)
        # Reduce() syntax is same as Python 2

        self.assertEqual(9, result)

        result2 = functools.reduce(self.multiply, [2, 3, 4], 1)
        self.assertEqual(24, result2)
# ---
def _print_accepted(matches, after_match):
            if self.key.ACC in after_match:
                accepted = sorted(
                    set(after_match[self.key.ACC]).difference(
                        set(matches.get(self.key.ACC, []))
                    )
                )
                for key in accepted:
                    print('Key for minion {0} accepted.'.format(key))
# ---
def method4(self, j):
        return j
# ---
def _copy_target_attributes(self):
    """Override `_copy_target_attributes` to exclude `provides`."""
    return [a for a in super(GoThriftGen, self)._copy_target_attributes if a != 'provides']
# ---
def test_lambda_output_shape_tuple_with_none(self):

    def lambda_fn(x):
      return x

    l = keras.layers.Lambda(lambda_fn, output_shape=(None, 10))
    output_shape = l.compute_output_shape((5, 10, 20))
    self.assertAllEqual([5, None, 10], output_shape.as_list())
# ---
def fit_baseline_phase(self,z_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		'''
		return self._baseline_als(np.angle(z_data),lam,p,niter=niter)
# ---
def _command(self):
        raise NotImplementedError("Subclass must implement this")
# ---
def _apply_timespec(name, goal_timespec):
        calls = []

        def _fake_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            timespec[0] = goal_timespec[0]
            return 0

        monkeypatch.setattr(_api, name, _fake_clock_call)

        return calls
# ---
def getMisMatches(data, weights):
    #print data
    list1 = np.empty(len(data))
    list1.fill(weights[0])
    results = list1+ weights[1]*data[:,0]+weights[2]*data[:,1]
    results = -1 * results
    return float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data)
# ---
def FiletypesForBuffer( buffer_object ):
  # NOTE: Getting &ft for other buffers only works when the buffer has been
  # visited by the user at least once, which is true for modified buffers
  return GetBufferOption( buffer_object, 'ft' ).split( '.' )
# ---
def test_spawn_empty_dns(self):
        """Test spawning with an empty dns list"""
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_VHD, None, None,
                         os_type="linux", architecture="x86-64",
                         empty_dns=True)
        self.check_vm_params_for_linux()
# ---
def test_spawn_vhd_glance_linux(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_VHD, None, None,
                         os_type="linux", architecture="x86-64")
        self.check_vm_params_for_linux()
# ---
def main(config: FilterDolminoConfig):
    filter_dolmino(config)
# ---
def corofunc2():
            called[1] += 1
            return event.ReturnValue(insert_events=[evt3], append_events=[evt2])
# ---
def forward(
        self,
        s: Tensor,
        z: Tensor,
        mask: Tensor,
        pair_mask: Tensor,
        use_kernels: bool = False,
    ) -> Tuple[Tensor, Tensor]:
        for layer in self.layers:
            if self.activation_checkpointing:
                s, z = torch.utils.checkpoint.checkpoint(layer, s, z, mask, pair_mask)
            else:
                s, z = layer(s, z, mask, pair_mask)
        return s, z
# ---
def convert_tokens_to_ids(self, token):
            # In our simple test tokenizer, tokens are single chars
            return ord(token[0]) if token else 0
# ---
def get_tracker(name: Literal["trackio"]) -> TrackioTracker: ...
# ---
def check_monthnum_number(monthnum1):
  if monthnum1 == 2:
    return True
  else:
    return False
# ---
def moneda(self):
        return self.__moneda
# ---
def fetch_startup_logs(self, tail_lines: int = 100) -> str | None:
        """Fetch startup script logs for debugging. Returns None if unavailable."""
        ...
# ---
def test_cache_middleware(self):
        req = Request.blank('/something', environ={'REQUEST_METHOD': 'GET'})
        resp = self.app(req.environ, start_response)
        self.assertTrue('swift.cache' in resp)
        self.assertTrue(isinstance(resp['swift.cache'], MemcacheRing))
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: Optional[AttentionMask | NamedArray] = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray | tuple[NamedArray, NamedArray | float]:
        del attn_mask, key, pos_ids
        hidden = self.embed_weight.take(self.Vocab, input_ids)
        return hidden, self.aux_loss
# ---
def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):
        super(LW_AveragePooling2D, self).__init__(pool_size, strides, padding, data_format)
# ---
def main_basename(path):
    r"""Return a main name of a basename of a given file path.

    Example
    -------
    >>> main_basename('c:\code\langconv\MsgID.h')
    'MsgID.h'
    """
    base = os.path.basename(path)
    base_main, _base_ext = os.path.splitext(base)
    return base_main
# ---
def _(a):
        return a.get()
# ---
def url(self) -> str:
        return f"http://{self._config.host}:{self.port}"
# ---
def reparam(self) -> AbstractLinearReparam:
        return self._reparam_cls(self.In, self.Out)
# ---
def union_axes(a1: AxisSpec, a2: AxisSpec) -> AxisSpec: ...
# ---
def _detect_nvidia_gpu_environment() -> bool:
    for key in ("NVIDIA_VISIBLE_DEVICES", "CUDA_VISIBLE_DEVICES"):
        value = os.environ.get(key)
        if not value:
            continue
        if value:
            return True
    return bool(glob.glob("/dev/nvidia[0-9]*"))
# ---
def connect(self):
        # Open either an SSL or regular NNTP connection.
        try:
            if ( self.ssl ):
                self.connection = NNTP_SSL(self.server, self.port, self.username, self.password, False, True, timeout=15)
            else:
                self.connection = NNTP(self.server, self.port, self.username, self.password, False, True, timeout=15)
        except:
            pass

        if ( self.connection ): return True
        return False
# ---
def test_capturing_done_simple(self):
        with self.getcapture() as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            out, err = cap.readouterr()
        assert out == "hello"
        assert err == "world"
# ---
def on_fuzzing_end_changed(self, value: int):
        self.ui.spinBoxFuzzingStart.setMaximum(self.ui.spinBoxFuzzingEnd.value())
        new_end = self.message.convert_index(value - 1, self.proto_view, 0, False)[1] + 1
        self.current_label.end = new_end
        self.current_label.fuzz_values[:] = []
        self.update_message_data_string()
        self.fuzz_table_model.update()
        self.ui.tblFuzzingValues.resize_me()
# ---
def test_synthetic_subtrees_have_correct_types(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=10)
    for entry in entries:
        assert entry.node_type in ("Return", "If", "For", "Assign", "BinOp", "Call", "Compare", "UnaryOp")
# ---
def combinations_list(list1):
    if len(list1) == 0:
        return [[]]
    result = []
    for el in combinations_list(list1[1:]):
        result += [el, el+[list1[0]]]
    return result
# ---
def test_api_suite() -> None:
    assert repr(Bloom(27_000, 0.0317)) == "<Bloom size_in_bits=193960 approx_items=0.0>"

    _test_bloom(Bloom(13242, 0.0000001))
    _test_bloom(Bloom(9874124, 0.01))
    _test_bloom(Bloom(2837, 0.5))

    operations_with_self()
    test_chunked_write()

    print("All API tests passed")
# ---
def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> list[int]:
        raise NotImplementedError
# ---
def test_raises_when_no_batches_recorded(self, mock_data_source):
        """Verify _get_data raises when no batches have been recorded."""
        srcs = [mock_data_source]
        agg = MeanAggregator(srcs, target_time=0)

        with pytest.raises(ValueError, match="No batches have been recorded"):
            agg._get_data()
# ---
def QHeadsPerGroup(self) -> Axis:
        """Axis for query heads per group."""
        return Axis("q_heads_per_group", self.q_heads_per_group)
# ---
def TrainBatch(self):
        if not isinstance(self.train_batch_size, int):
            raise ValueError("TrainBatch is only valid for a single batch size. Use batch_axis_at_step instead")
        return Axis(self.batch_axis_name, self.train_batch_size)
# ---
def len_log(list1):
    max=len(list1[0])
    for i in list1:
        if len(i)>max:
            max=len(i)
    return max
# ---
def test_ray_not_detected_when_not_initialized():
    """Should not detect Ray when ray.is_initialized() is False."""
    with patch("iris.client.client.get_iris_ctx", return_value=None):
        with patch("ray.is_initialized", return_value=False):
            client = current_client()
            assert isinstance(client, LocalClient)
# ---
def divide(self, a: int, b: int) -> float:
        return a / b
# ---
def array_equal(a: NamedArray, b: NamedArray) -> bool:
    """Returns True if two arrays have the same shape and elements."""
    if set(a.axes) != set(b.axes):
        return False
    b = b.rearrange(a.axes)
    return bool(jnp.array_equal(a.array, b.array))
# ---
def test_default_tokenize_with_dataset_name():
    step = default_tokenize(
        name="dummy",
        dataset=HfDatasetSpec(id="cnn_dailymail", name="3.0.0"),
        tokenizer="gpt2",
    )
    assert isinstance(step.config, HfTokenizeConfig)
    assert step.config.id == "cnn_dailymail"
    assert step.config.name == "3.0.0"
# ---
def _kill_agent(self):
        rc, out, err = execCmd([_SSH_AGENT.cmd, '-k'],
                               env={'SSH_AGENT_PID': self._agent_pid})
        if rc != 0:
            logging.error('Error killing ssh-agent (PID=%r), exit code: %r'
                          ', out: %r, err: %r' %
                          (self._agent_pid, rc, out, err))
# ---
def do_viz_lm(config: LevanterVizLmConfig) -> None:
    """
    Visualizes log probabilities of a language model.

    Args:
        config (VizLmConfig): The configuration for visualizing log probabilities.
    """
    # Levanter can read `gs://` checkpoints directly via fsspec/tensorstore, and HF
    # checkpoints via fsspec as well. Avoid staging large directories locally.
    execute_in_subprocess(viz_lm_main, (config,), {})
# ---
def _get_nemotron_split_paths(split: str):
    """Helper to get file paths for a nemotron split."""
    patterns = NEMOTRON_DATASETS[split]
    return [_nemotron_cc_path / pattern for pattern in patterns]
# ---
def __invert__(self) -> "NamedArray":  # pragma: no cover
        return haliax.invert(self)
# ---
def max_product(arr, n ): 
	mpis =[0] * (n) 
	for i in range(n): 
		mpis[i] = arr[i] 
	for i in range(1, n): 
		for j in range(i): 
			if (arr[i] > arr[j] and
					mpis[i] < (mpis[j] * arr[i])): 
						mpis[i] = mpis[j] * arr[i] 
	return max(mpis)
# ---
def _create_app(self) -> Starlette:
        rpc_wsgi_app = WorkerServiceWSGIApplication(service=self._service)
        rpc_app = WSGIMiddleware(rpc_wsgi_app)

        routes = [
            Route("/health", self._health),
            Route("/", self._dashboard),
            Route("/task/{task_id:path}", self._task_detail_page),
            Route("/logs", self._logs_page),
            static_files_mount(),
            Mount(rpc_wsgi_app.path, app=rpc_app),
        ]
        return Starlette(routes=routes)
# ---
def set_output_embeddings(self, new_embeddings):
        """Overrides output embeddings."""
        self.lm_head = new_embeddings
# ---
def test_add_dicts(self):
        """Merge two dicts of values together"""
        a = {
            "a": 1,
            "b": 2,
        }
        b = {
            "a": 3,
            "c": 4,
        }
        ret = add_dicts(a, b)
        self.assertEqual(
            ret,
            {
                "a": 4,
                "b": 2,
                "c": 4,
            },
        )
# ---
def test_repeat(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.repeat(a, 3, axis=1)
    assert_array_equal(
        b.compute(),
        np.repeat(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), 3, axis=1),
    )
# ---
def to_dataframe(self):
        """Convert this dataset into a pandas.DataFrame.

        Non-index variables in this dataset form the columns of the
        DataFrame. The DataFrame is be indexed by the Cartesian product of
        this dataset's indices.
        """
        return self._to_dataframe(self.dims)
# ---
def friendly_time(msecs):
    secs, msecs = divmod(msecs, 1000)
    mins, secs = divmod(secs, 60)
    hours, mins = divmod(mins, 60)
    if hours:
        return '%dh%dm%ds' % (hours, mins, secs)
    elif mins:
        return '%dm%ds' % (mins, secs)
    elif secs:
        return '%ds%dms' % (secs, msecs)
    else:
        return '%.2fms' % msecs
# ---
def sample_data(self):
        pass
# ---
def test_remove_from_empty_aggregate(self):
        values = {"name": 'fake_aggregate',
                  "availability_zone": 'fake_zone'}
        result = db.aggregate_create(self.context, values)
        self.assertRaises(exception.AggregateError,
                          self.conn._pool.remove_from_aggregate,
                          None, result, "test_host")
# ---
def get_short_name(self):
        return self.name
# ---
def _get_random_inputs(config: LlamaConfig, override_Pos=None):
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()
    return x, mask
# ---
def linelisting_to_newline(html: BeautifulSoup):
    # Turn new line listings into new lines
    linelisting = html.findAll("div", {"class": "ltx_listingline"})
    for fn in linelisting:
        fn.append(BeautifulSoup("<br>", "html.parser"))
# ---
def push(self, item: T_co) -> None:
        self.queue.append(item)
# ---
def __init__(self, volume_size=0):
        self.volume_name = None
        self.name = None
        self.volume_id = None
        self.volume_size = volume_size
        self.user_id = None
        self.status = None
# ---
def __init__(
        self,
        state: ControllerState,
        scheduler: SchedulerProtocol,
        bundle_prefix: str,
        log_buffer: LogBuffer | None = None,
    ):
        self._state = state
        self._scheduler = scheduler
        self._bundle_store = BundleStore(bundle_prefix)
        self._log_buffer = log_buffer
# ---
def __call__(self, *args, **kwargs):
        return self._call(False, *args, **kwargs)
# ---
def test_find_span_end_invalid_python():
    end = _find_span_end("def (broken", 0)
    assert end is None
# ---
def mean(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.mean, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype)
# ---
def count_sources(batch):
        counts = {"ds1": 0, "ds2": 0, "ds3": 0}
        for x in batch:
            if x < 10:
                counts["ds1"] += 1
            elif x < 100:
                counts["ds2"] += 1
            else:
                counts["ds3"] += 1
        return counts
# ---
def _resolved_shared_axis_mapping(self):
        mapping = dict(DEFAULT_SHARED_MAPPING)
        for logical, physical in self.shared_mapping.items():
            mapping[logical] = _norm(physical)

        return mapping
# ---
def __init__(self, provider: TaskProvider, log_buffer: LogBuffer | None = None):
        self._provider = provider
        self._log_buffer = log_buffer
        self._timer = Timer()
# ---
def _get_shared_executor() -> ThreadPoolExecutor:
    global _shared_executor
    if _shared_executor is None:
        _shared_executor = ThreadPoolExecutor(max_workers=32)
    return _shared_executor
# ---
def stop(self) -> None:
        """Stop background thread."""
        if self._thread is not None:
            self._stop_event.set()
            self._thread.join(timeout=5.0)
            self._thread = None
            logger.info("Stopped ReplayDataLoader background thread")
# ---
def test_sample_nonexistent_type_returns_none(bank):
    rng = random.Random(42)
    assert bank.sample("NonexistentType", rng) is None
# ---
def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"
# ---
def r(a=4):
            for _ in range(a):
                yield rx.randint(1, 10)
# ---
def is_named_or_shaped_array_like(x):
    return (is_jax_array_like(x) and x.ndim >= 1) or is_named_array(x)
# ---
def __post_init__(self):
        object.__setattr__(self, "caches", tuple(self.caches))
# ---
def test_masking(self):
    testing_utils.layer_test(
        keras.layers.Masking, kwargs={}, input_shape=(3, 2, 3))
# ---
def lambda_fn(x):
      return x
# ---
def _ensure_scalar(x: hax.types.Scalar | hax.NamedArray) -> hax.types.Scalar:
    if isinstance(x, hax.NamedArray):
        return x.scalar()
    else:
        return x
# ---
def tree_map(fn, tree, *rest, is_leaf=None):
    """
    Version of [jax.tree_util.tree_map][] that automatically treats NamedArrays as leaves.
    """
    old_is_leaf = is_leaf
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: old_is_leaf(x) or is_named_array(x)

    return jax.tree.map(fn, tree, *rest, is_leaf=is_leaf)
# ---
def get_queryset(self):
        """
        Filter providers by current user
        """
        user = self.request.user
        if (type(user) == AnonymousUser):
            return Identity.objects.none()

        identities = user.current_identities()
        return identities
# ---
def session():
    Session = sessionmaker()
    engine = create_engine(MYSQL_CONNECTION_STRING)
    Session.configure(bind=engine)
    metadata.create_all(engine)
    try:
        yield Session()
    except:
        pass
# ---
def test_update_demand_tracks_peak(self, unbounded_config: config_pb2.ScaleGroupConfig):
        """update_demand() tracks peak demand."""
        manager = make_mock_vm_manager()
        group = ScalingGroup(unbounded_config, manager)

        group.update_demand(5)
        group.update_demand(10)
        group.update_demand(3)

        assert group.current_demand == 3
        assert group.peak_demand == 10
# ---
def __call__(
        self, model: Any, batch: Any, **batch_kwargs: dict[str, Any]
    ) -> tuple[jax.Array, dict[str, Metric]]: ...
# ---
def executor_options(self) -> Optional[dict]:
        return self._executor_options
# ---
def apply_update(m_hat, v_hat):
            return m_hat / (jnp.sqrt(v_hat) + epsilon)
# ---
def choosers_dm(choosers, spec):
    return eval_variables(spec.index, choosers)
# ---
def should_log(self) -> bool:
        """Return True if any sample logging should occur."""
        if self.log_all:
            return True
        return self.max_samples_per_benchmark is not None and self.max_samples_per_benchmark > 0
# ---
def batch_flatten(data):
    """BatchFlatten.

    This operator flattens all the dimensions except for the batch dimension.
    which results a 2D output.

    For data with shape ``(d1, d2, ..., dk)``
    batch_flatten(data) returns reshaped output of shape ``(d1, d2*...*dk)``.


    Parameters
    ----------
    data : relay.Expr
        The input data to the operator.

    Returns
    -------
    result: relay.Expr
        The Flattened result.
    """
    return _make.batch_flatten(data)
# ---
def test_transaction_connection_fn_commit(self):
        fn = self._trans_fn()
        conn = testing.db.connect()
        conn.transaction(fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def dummy_server():
    return DummyInferenceServer()
# ---
def test_patch_from__full_globe():
    # Full globe extent should equal grid dimensions
    patch_h, patch_w = patch_from(
        patch_extent=(180.0, 360.0), input_height=4, input_width=8
    )
    assert patch_h == 4
    assert patch_w == 8
# ---
def finish(self):
        import trackio

        logger.info("Finishing trackio run...")
        trackio.finish()
# ---
def make_target(self) -> Callable[..., None]:
        """Create a thread target that carries the current context.

        Must be called from the main thread so copy_context() captures
        the right contextvars (iris_ctx, etc.).
        """
        ctx = copy_context()

        def target(stop_event: threading.Event) -> None:
            ctx.run(self._run, stop_event)

        return target
# ---
def increment(self, amount: int = 1) -> int:
        self.call_count += 1
        self.value += amount
        return self.value
# ---
def on_epoch_end(self, model):
        # Take the common epoch end for folding and affinity
        self.common_on_epoch_end(model)

        # Take the affinity specific epoch end
        self.on_epoch_end_design(model)
# ---
def _set_data(self, value):
        if value is not None and not isinstance(value, bytes):
            if isinstance(value, str):
                value = value.encode('utf-8')
        self._parameter1 = value
# ---
def unembed(self, x: NamedArray):
        return self.token_embeddings.unembed(x)
# ---
def __init__(self, callback, debounce_seconds=0.5):
        super().__init__()
        self.callback = callback
        self.debounce_seconds = debounce_seconds
        self.last_trigger = 0
        self._timer = None
# ---
def scan_compatible_fn(_, x):
        del _
        return None, fn(x)
# ---
def get_reward(self, m, s):
        """
        Get reward for moving from state s to state (s + 1)
        """
        return self.rewards[m][s + 1][0]
# ---
def test_filters_for_instance_without_ip_v6(self):
        self.flags(use_ipv6=False)
        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)
        rulesv4, rulesv6 = self.fw._filters_for_instance("fake", network_info)
        self.assertEquals(len(rulesv4), 2)
        self.assertEquals(len(rulesv6), 0)
# ---
def draw(self, x, y):
        gl.glPushAttrib(gl.GL_ENABLE_BIT | gl.GL_CURRENT_BIT)
        gl.glColor4f(1, 1, 1, 1)
        gl.glEnable(gl.GL_BLEND)
        gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE_MINUS_SRC_ALPHA)
        self.texture.blit(x - self.hot_x, y - self.hot_y, 0)
        gl.glPopAttrib()
# ---
from array import array
def negative_count(nums):
    n = len(nums)
    n1 = 0
    for x in nums:
        if x < 0:
            n1 += 1
        else:
          None
    return round(n1/n,2)
# ---
def remove_tuples(test_list, K):
  res = [ele for ele in test_list if len(ele) != K]
  return (res)
# ---
def utilities(choosers_dm, spec, test_data):
    utils = choosers_dm.dot(spec).astype('float')
    return pd.DataFrame(
        utils.as_matrix().reshape(test_data['probabilities'].shape),
        columns=test_data['probabilities'].columns)
# ---
def __call__(self, x):
            hidden = hax.dot(Hidden, x, self.w1)
            hidden = hax.tanh(hidden)
            return hax.dot(Mlp, hidden, self.w2)
# ---
def __init__(self, fmt):
                self.file_format = fmt
# ---
def init_fn(params):
        # Initialize mean_square and momentum buffers to zero trees
        mean_square = otu.tree_zeros_like(params)
        momentum_buf = otu.tree_zeros_like(params)
        return ScaleByRMSPropMomState(
            count=jnp.zeros([], jnp.int32),
            mean_square=mean_square,
            momentum=momentum_buf,
        )
# ---
def name(self) -> str:
        raise NotImplementedError
# ---
def movie(self, imdb, title, localtitle, aliases, year):
        try:
            title = cleantitle.geturl(title)
            url = self.base_link + '/%s-%s' % (title, year)
            return url
        except:
            return
# ---
def bias_dropout_add_scale_fused_inference(
    x: torch.Tensor,
    bias: typing.Optional[torch.Tensor],
    scale: torch.Tensor,
    residual: typing.Optional[torch.Tensor],
    prob: float) -> torch.Tensor:
  return bias_dropout_add_scale(
    x, bias, scale, residual, prob, False)
# ---
def get_ports(self):
        ''' get a list of ports '''
        return self.get(Service.port_path) or []
# ---
def test_vmap_unmapped_args():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Width, Depth))

    def vmap_fun(x):
        return x.take(Width, 2)

    selected = hax.vmap(vmap_fun, Batch)(named1)

    expected_jax = jnp.array([named1.take(Width, 2).array for _ in range(Batch.size)])
    expected_names = (Batch, Depth)

    assert jnp.all(jnp.equal(selected.array, expected_jax))
    assert selected.axes == expected_names
# ---
def parsed_error_msg(self):
        """
        Sometimes, the error message we've received needs to be parsed into
        something more human readable

        The default behavior is to return the current error message as is.
        """
        return self.error_msg
# ---
def num_devices_per_slice(self):
        """number of devices within a slice"""
        return jax.device_count() // self.num_slices
# ---
def crispr_tag_1(testapp, lab, award, ctcf):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'method': 'CRISPR',
        'modified_site_by_gene_id': ctcf['@id'],
        'introduced_tags': [{'name': 'mAID-mClover', 'location': 'C-terminal'}]
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def Pos(self) -> Axis:
        return self.config.max_Pos
# ---
def smallest_missing(A, left_element, right_element):
    if left_element > right_element:
        return left_element
    mid = left_element + (right_element - left_element) // 2
    if A[mid] == mid:
        return smallest_missing(A, mid + 1, right_element)
    else:
        return smallest_missing(A, left_element, mid - 1)
# ---
def get_process_logs(self, request: cluster__pb2.Controller.GetProcessLogsRequest, ctx: RequestContext) -> cluster__pb2.Controller.GetProcessLogsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def job_id(self) -> JobId | None:
        return self._job_id
# ---
def restart(self) -> str:
        """Stop then start controller."""
        ...
# ---
def key_str_all(self):
        '''
        Return all managed key strings
        '''
        ret = {}
        for status, keys in six.iteritems(self.list_keys()):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                ret[status][key] = self._get_key_str(key, status)
        return ret
# ---
def shape(self) -> dict[str, int]:
        if not len(self.axis_names) == jnp.ndim(self.array):
            raise ValueError(
                f"Number of axes {len(self.axes)} does not match number of dimensions {jnp.ndim(self.array)} of array"
            )
        return {axis.name: axis.size for axis in self.axes}
# ---
def main():
    """Main entry point."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    cli()
# ---
def test_impl(df):
            return df['A'][df['B']].values
# ---
def action_assign_wkf(self, cr, uid, ids, context=None):
        """ Changes picking state to assigned.
        @return: True
        """
        self.write(cr, uid, ids, {'state': 'assigned'})
        return True
# ---
def getPreprocessFunction(preprocessType):

    if preprocessType == "dummy":
        return dummyPreprocessInput
    elif preprocessType == "mobilenet":
        return mobilenet.preprocess_input
    elif preprocessType == "imagenet":
        return imagenet_utils.preprocess_input
    else:
        raise Exception(preprocessType + " not supported")
# ---
def add_to_dup_map(record: dict):
        shard_dup_map[record["id"]] = record["fuzzy_duplicate"]
# ---
def test_brackets_balanced_empty():
    assert brackets_balanced("")
# ---
def recombination_knockout(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'knockout',
        'purpose': 'repression',
        'method': 'site-specific recombination',
        'modified_site_by_coordinates': {
            "assembly": "GRCh38",
            "chromosome": "11",
            "start": 60000,
            "end": 62000
        }
    }
# ---
def supports_fork(self) -> bool:
        return False
# ---
def _redact_docker_run_command(cmd: list[str]) -> str:
    redacted = list(cmd)
    i = 0
    while i < len(redacted):
        if redacted[i] == "-e" and i + 1 < len(redacted):
            kv = redacted[i + 1]
            if "=" in kv:
                key, value = kv.split("=", 1)
                if key in _SENSITIVE_ENV_KEYS and value:
                    redacted[i + 1] = f"{key}=<redacted>"
        i += 1
    return shlex.join(redacted)
# ---
def test_limit_offset_nobinds(self):
        """test that 'literal binds' mode works - no bound params."""

        table = self.tables.some_table
        stmt = select([table]).order_by(table.c.id).limit(2).offset(1)
        sql = stmt.compile(
            dialect=config.db.dialect, compile_kwargs={"literal_binds": True}
        )
        sql = str(sql)

        self._assert_result(sql, [(2, 2, 3), (3, 3, 4)])
# ---
def _custom_setup(self):
        kwargs = {}
        kwargs['netapp_mode'] = 'proxy'
        kwargs['configuration'] = create_configuration()
        self._driver = netapp_nfs.NetAppDirectCmodeNfsDriver(**kwargs)
        self._driver.ssc_enabled = True
        self._driver.configuration.netapp_copyoffload_tool_path = 'cof_path'
# ---
def activation_genetic_modification_2(testapp, lab, award):
    return{
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'activation',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def eval_data(self) -> Iterator[MockEnvExample]:
        """Stream evaluation data."""
        for example in self.eval_examples:
            yield MockEnvExample(
                raw_prompt=example["prompt"],
                raw_answer=example["answer"],
                processed_prompt=example["prompt"],
                processed_answer=example["answer"],
                metadata={"task_type": self.task_type},
            )
# ---
def send_serialdata(self, node):
        if isinstance(node, JsonItem):
            if self.serial.isOpen():
                s = node.to_json()
                self.serial.write(utf8_to_bytearray(s + '\n'))
                self.loggingWidget.log_output(s.strip())
# ---
def wait_until_finished(self):
        self._manager.wait_until_finished()
        if jax.process_index() == 0:
            while self._checkpoint_being_removed is not None or not self._async_checkpoint_remover_queue.empty():
                time.sleep(0.2)
# ---
def set_visible(self, visible=True):    
        """Show or hide the window.

        :Parameters:
            `visible` : bool
                If True, the window will be shown; otherwise it will be
                hidden.

        """
        raise NotImplementedError('abstract')
# ---
def argmax(self, axis: AxisSelector | None = None) -> "NamedArray":  # pragma: no cover
        return haliax.argmax(self, axis=axis)
# ---
def log(self, metrics: Metrics, step: int | None, **kwargs):
        """Log metrics to wandb."""
        if self._enabled:
            # Really, this should take a mapping, not a dict
            # (so it is covariant) but it doens't so we convert
            wandb.log(dict(metrics), step=step, **kwargs)
# ---
def set_design_dir(self, design_dir):
        if design_dir != self.design_dir:
            # Set directories for the writer
            self.design_dir = design_dir
            self.writer.init_outdir(design_dir)
            self.aff_writer.init_outdir(design_dir)
            self.analyze_task.init_datasets(design_dir, load_dataset=True)
# ---
def hello():
            return 42
# ---
def _axis_size(self, axis: str) -> int:
        return self._mesh_axis_totals.get(axis, 1)
# ---
def _is_trivial_index(idx: Any) -> bool:
    return isinstance(idx, slice) and idx.start is None and idx.stop is None and idx.step is None
# ---
def __init__(self,**kwargs):
        self.register_event_type('on_answer')
        super(ConfirmPopup,self).__init__(**kwargs)
# ---
def convert_to_display_name(self, value, record=None):
        return ustr(value.display_name)
# ---
def test_repr(self):
        expr = ~(col("flag") == True)  # noqa: E712
        assert repr(expr) == "~((col('flag') == lit(True)))"
# ---
def run(argv):
    try:
        return subprocess.check_output(argv, stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        print(e.output.decode(), file=sys.stderr)
        raise
# ---
def run():
        trainer.run()
# ---
def __init__(self, handles: list[ActorHandle], jobs: list[LocalJobHandle]):
        self._handles = handles
        self._jobs = jobs
        self._yielded = False
# ---
def test_fused_cross_entropy_xla_matches_reference():
    x, w, y = _make_toy_inputs()

    loss = fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
        x.reshape(6, 4),
        y.reshape(6),
        w,
        reduction=None,
        logsumexp_weight=0.0,
        implementation="xla",
    )

    loss_ref, _ = linear_softmax_cross_entropy_loss_reference(
        x.reshape(6, 4),
        y.reshape(6),
        w,
    )

    assert jnp.allclose(loss, loss_ref, atol=1e-5, rtol=1e-5)
# ---
def _extend_path(path: Optional[str], extra: str):
    if path == "memory" or path is None:
        return path
    else:
        return os.path.join(path, extra)
# ---
def test_member_route_requires_login(self):
        # Ensure member route requres logged in user.
        response = self.client.get("/members", follow_redirects=True)
        self.assertIn(b"Please log in to access this page", response.data)
# ---
def celu(a: A) -> A:
    return wrap_elemwise_unary(jnn.celu, a)
# ---
def match(self, item):
        return any(q.match(item) for q in self.subqueries)
# ---
def testDeleteSubscript(self):
    self.assertEqual((0, '{}\n'), _GrumpRun(textwrap.dedent("""\
        foo = {'bar': 'baz'}
        del foo['bar']
        print foo""")))
# ---
def norm_config(self) -> RmsNormConfig:
        """Return the normalization configuration for OLMo2."""
        return RmsNormConfig(
            eps=self.layer_norm_epsilon,
            use_weight=self.use_layer_norm_weight,
            use_bias=self.use_bias,
        )
# ---
def plugin_on_unpaused(self):
        xid = dbus.UInt32(get_toplevel_xid())
        flags = dbus.UInt32(InhibitFlags.IDLE)

        try:
            bus = dbus.SessionBus()
            obj = bus.get_object(self.DBUS_NAME, self.DBUS_PATH)
            iface = dbus.Interface(obj, self.DBUS_INTERFACE)
            self.__cookie = iface.Inhibit(
                self.APPLICATION_ID, xid, self.INHIBIT_REASON, flags)
        except dbus.DBusException:
            pass
# ---
def inject_profiler():
        return dict(profiler_includes=templatetags.profiler_includes())
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"wrapped": None, "lora": None}
# ---
def parse_nptable(self, m):
        item = self._process_table(m)

        cells = re.sub(r'\n$', '', m.group(3))
        cells = cells.split('\n')
        for i, v in enumerate(cells):
            cells[i] = re.split(r' *\| *', v)

        item['cells'] = cells
        self.tokens.append(item)
# ---
def _reload(current_model: LmHeadModel) -> LmHeadModel:
                return weight_loader(self.server, self.config.server, current_model)
# ---
def is_empty(self) -> bool:
        return len(self.endpoints) == 0
# ---
def dif_Square(n): 
    if (n % 4 != 2): 
        return True
    return False
# ---
def to_proto(self) -> cluster_pb2.TaskStatus:
        """Convert to protobuf TaskStatus message."""
        ...
# ---
def fold_step(block: FoldModule, carry: hax.NamedArray) -> hax.NamedArray:
        return block(carry)
# ---
def loss_fn_with_ctx(
        pred: torch.Tensor,
        target: torch.Tensor,
        ctx: GridContext,
    ) -> torch.Tensor:
        wet = ctx.label_mask.to(device=pred.device)
        pred = pred * wet
        target = target * wet
        return loss_fn(pred, target)
# ---
def empty_queue_space(self) -> jnp.ndarray:
        return self.tokens.axis_size("position") - self.num_tokens
# ---
def _arg_combine(a, arg_func=None, **kwargs):
    # convert axis from single value tuple to int
    axis = kwargs.pop("axis")[0]

    i = a["i"]
    v = a["v"]

    # find indexes of values in v and apply to i and v
    vi = arg_func(v, axis=axis, **kwargs)
    # note that the array API doesn't have take_along_axis, so this may fail
    i_combined = nxp.take_along_axis(i, vi, axis=axis)
    v_combined = nxp.take_along_axis(v, vi, axis=axis)
    return {"i": i_combined, "v": v_combined}
# ---
def zone(self) -> str:
        """Zone/location, or empty string if not applicable."""
        ...
# ---
def notify(self, order):
        self.notifs.append(order.clone())
# ---
def disable_noisy_loggers():
    """Disable verbose INFO logging from inference engine and HTTP clients."""
    noisy_loggers = [
        "levanter.inference.engine",
        "levanter.inference.openai",
        "httpx",
        "uvicorn.access",
    ]
    for logger_name in noisy_loggers:
        logging.getLogger(logger_name).setLevel(logging.WARNING)
# ---
def apply_rotary_pos_emb(qkv, cos, sin):
  cos = cos[0, :, 0, 0, : cos.shape[-1] // 2]
  sin = sin[0, :, 0, 0, : sin.shape[-1] // 2]
  return flash_attn.layers.rotary.apply_rotary_emb_qkv_(
    qkv, cos, sin
  )
# ---
def __getitem__(self, key):
        if not utils.is_dict_like(key):
            raise TypeError('can only lookup dictionaries from Dataset.loc')
        return self.dataset.sel(**key)
# ---
def save_dt_model(model_name, model, folder):
    """
    Save model using Pickle binary format.

    :param dataframe model: model reference
    :param string model_name: title for the model used on the output filename
    :param string folder: location of model output
    """
    print("Saving model...")
    model_file = folder + '/models/' + model_name + '.pkl'
    path = open(model_file, 'wb')
    pickle.dump(model, path)
    print("Model saved location:", model_file)
# ---
def max_length(self) -> int:
        """Maximum sequence length the model supports for inputs."""
        return self.config.max_seq_len
# ---
def check_Equality(s): 
    return (ord(s[0]) == ord(s[len(s) - 1])); 
def count_Substring_With_Equal_Ends(s): 
    result = 0; 
    n = len(s); 
    for i in range(n): 
        for j in range(1,n-i+1):  
            if (check_Equality(s[i:i+j])): 
                result+=1; 
    return result;
# ---
def test_metadata_works():
    processor = AutoProcessor.from_pretrained("openai/whisper-tiny")
    tokenizer = AutoTokenizer.from_pretrained("openai/whisper-tiny")
    batch_processor = BatchAudioProcessor(processor, tokenizer)
    # test this doesn't throw
    assert len(batch_processor.metadata)
# ---
def isinf(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in isinf")
    return elemwise(nxp.isinf, x, dtype=nxp.bool)
# ---
def test_remove_project_access_with_no_admin_user(self):
        req = fakes.HTTPRequest.blank('/v2/%s/types/%s/action' % (
            fake.PROJECT_ID, fake.VOLUME_TYPE3_ID), use_admin_context=False)
        body = {'removeProjectAccess': {'project': PROJ2_UUID}}
        self.assertRaises(exception.PolicyNotAuthorized,
                          self.type_action_controller._removeProjectAccess,
                          req, fake.VOLUME_TYPE3_ID, body)
# ---
def on_operation_start(self, event):
        pass
# ---
def _indices_to_selector(axes: Sequence[Axis], indices: Sequence[Any]) -> dict[AxisSelector, Any]:
    selector: dict[AxisSelector, Any] = {}
    for axis, idx in zip(axes, indices):
        if _is_trivial_index(idx):
            continue
        selector[axis] = idx
    return selector
# ---
def register(self, type_):
        obj = type_(self)
        self.types[obj._type] = obj
# ---
def _effective_pack(component: DatasetComponent) -> bool | int | Literal["pad"]:
    if component.pack is not None:
        return component.pack
    fmt = component.format
    if isinstance(fmt, TextLmDatasetFormat):
        return False
    if isinstance(fmt, ChatLmDatasetFormat):
        return True if fmt.pack is None else fmt.pack
    return False
# ---
def bsearch(nums, left, right, res, i, j, target):
    while left <= right:
        middle = (left + right) // 2
        candidate = nums[i] + nums[j] + nums[middle]
        if res is None or abs(candidate - target) < abs(res - target):
            res = candidate
        if candidate == target:
            return res
        elif candidate > target:
            right = middle - 1
        else:
            left = middle + 1
    return res
# ---
def set_basic_selection(self, selection, value, fields=None):
        self[fields][selection] = value
# ---
def _check_plotfunc_output(func, da, framedim="time", **kwargs):
            timestep = 0
            timestep_value = da[framedim].data[timestep]
            fig = plt.figure()
            oargs = func(da, fig, timestep, timestep_value, framedim, **kwargs)
            # I just want the number of output args, delete plot
            plt.close(fig)
            if oargs is None:
                return 0
            else:
                return len(oargs)
# ---
def _str_is_int(x_str: str) -> bool:
    try:
        x_str = _strip_properly_formatted_commas(x_str)
        x = float(x_str)
        return abs(x - int(round(x))) <= 1e-7
    except (ValueError, OverflowError):
        return False
# ---
def transform_conversation_to_dolma(row: dict):
    dolma_row = row
    text = ""
    for message in dolma_row["messages"]:
        text += message["role"] + ": "
        text += message["content"] + "\n\n"

    text = text.strip()

    dolma_row["text"] = text
    del dolma_row["messages"]
    return dolma_row
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        """Generate examples."""
        ...
# ---
import heapq
def small_nnum(list1,n):
  smallest=heapq.nsmallest(n,list1)
  return smallest
# ---
def build(self, ctx: LrScheduleContext):
        return _inv_sqrt_decay_schedule(ctx.learning_rate, ctx.min_lr, ctx.warmup_steps, self.timescale)
# ---
def __getitem__(self, name, file_local=False):
        """ Get an option value """
        prefix = 'file-local-options/' if file_local else 'options/'
        return self._get_property(prefix+name)
# ---
def is_command(self) -> bool:
        return self.command is not None
# ---
def corofunc3():
            called[2] += 1

            async def corofunc():
                pass

            return event.ReturnValue(append_events=[evt3], schedule={corofunc()})
# ---
def enterChatty(self):
        self.chatty.enter()
        self.acceptOnce(self.chattyDoneEvent, self.__decideNextState)
# ---
def get_sample_outputs(self) -> dict[str, list[dict]]:
        """
        Get all stored sample outputs.

        Returns:
            Dictionary mapping task names to lists of sample outputs
        """
        return self.sample_outputs
# ---
def position_token_id(self, pos: int) -> int:
        """Get the token ID for <POS pos>."""
        if pos < 0 or pos >= self.num_position_tokens:
            raise ValueError(f"Position {pos} out of range [0, {self.num_position_tokens})")
        return self.position_token_offset + pos
# ---
def __exit__(self, *args, **kwargs):
    _GLOBAL_CUSTOM_OBJECTS.clear()
    _GLOBAL_CUSTOM_OBJECTS.update(self.backup)
# ---
def __getitem__(self, key):
        # read eagerly
        return self.array.__getitem__(key).read().result()
# ---
def to_arrow(self, message):
        raise NotImplementedError("Not implemented.")
# ---
def __str__(self):
        materialString = ""
        if self.__material is not None:
            materialString = " ({0})".format(self.__material)
        return "{0}{1}".format(super(Sculpture, self).__str__(),
                               materialString)
# ---
def discover_new(self) -> list[ActorHandle]:
        """Return handles that are ready but haven't been yielded yet.

        After wait_ready(count=1), subsequent calls to discover_new() will
        return the remaining handles as they become available. For LocalClient
        all handles are ready immediately, so this returns whatever wait_ready
        didn't return on its first call.
        """
        ...
# ---
def test_dupfile_on_textio():
    io = py.io.TextIO()
    f = capture.safe_text_dupfile(io, "wb")
    f.write("hello")
    assert io.getvalue() == "hello"
    assert not hasattr(f, 'name')
# ---
def dummyPreprocessInput(image):
    image -= 127.5
    return image
# ---
def test_lon_lat_output(self):
        """Asserts that the vertices in the lat-lon output are in the
        right order (lat before long)."""
        for vertex in self.polycircle.to_lon_lat():
            assert_almost_equal(vertex[0], self.longitude, places=2)
            assert_almost_equal(vertex[1], self.latitude, places=2)
# ---
def tok(self):
        t = self.token['type']

        # sepcial cases
        if t.endswith('_start'):
            t = t[:-6]

        return getattr(self, 'output_%s' % t)()
# ---
def evaluate(self, record: dict) -> bool:
        return not self.child.evaluate(record)
# ---
def join_fn(left: Iterator, right: Iterator) -> Iterator:
        return _sorted_merge_join(left, right, left_key_fn, right_key_fn, combiner_fn, join_type)
# ---
def _array_slices(offsets, start, stop):
    """Return pairs of array index and array slice to slice from start to stop in the concatenated array."""
    slice_start = start
    while slice_start < stop:
        # find array that slice_start falls in
        i = bisect(offsets, slice_start) - 1
        slice_stop = min(stop, offsets[i + 1])
        yield i, slice(slice_start - offsets[i], slice_stop - offsets[i])
        slice_start = slice_stop
# ---
def test_roll_bad_named_shift():
    H = Axis("H", 4)
    W = Axis("W", 3)

    arr = hax.arange((H, W))
    shift = hax.arange((Axis("dummy", 2),))

    with pytest.raises(TypeError):
        hax.roll(arr, shift, H)
# ---
def test_calculo_horas_voadas(self):
        s_horas = {
            'h_diurno': '6:40',
            'h_noturno': '6:47',
            'h_total_voo': '13:27',
            'h_faixa2': '0:00',
            'h_sobreaviso': '40:00',
            'h_reserva': '29:13'
        }
        self.assertEqual(self.escala.soma_horas(), s_horas)
# ---
def get_question(self) -> str:
        return self.problem + self.question_suffix()
# ---
def bottom_right_corner3d(self):
        return self.edge_points3d[2]
# ---
def get_device_type_enum(device: cluster_pb2.DeviceConfig) -> DeviceType:
    """Extract device type as enum from DeviceConfig.

    Args:
        device: DeviceConfig proto from job request

    Returns:
        DeviceType enum value
    """
    if device.HasField("gpu"):
        return DeviceType.GPU
    elif device.HasField("tpu"):
        return DeviceType.TPU
    return DeviceType.CPU
# ---
def close(self):
        self.__root.destroy()
        self.__root.quit()
# ---
def busy_loop():
        while True:
            print("test")
# ---
def abort_job(job_id):
    try:
        job = _get_job(job_id)
        job.abort()
    except ClientError as e:
        logging.info('Cannot abort job, error: %s', e)
        return errCode[e.err_name]
    return {'status': doneCode}
# ---
def test_main_wrapper_loads_from_fsspec():
    with fsspec.open("memory://test.yaml", "w") as f:
        f.write(
            """
        project: test
        """
        )

    args = ["--config_path", "memory://test.yaml", "--x", "2"]

    @dataclasses.dataclass
    class Config:
        project: str
        x: int = 1

    @levanter.config.main(args=args)
    def main(config: Config):
        assert config.project == "test"
        assert config.x == 2

    main()
# ---
def zero_update(updates, nu_hat):
            # For the first step, return zero updates
            return jax.tree_util.tree_map(
                lambda u: None if u is None else jnp.zeros_like(u),
                updates,
                is_leaf=lambda x: x is None,
            )
# ---
def build(self) -> optax.GradientTransformation:
        return clip_update_by_historical_norm(self.rolling_interval_length, self.sigma_factor)
# ---
def discover_vm_groups(self) -> list[VmGroupProtocol]:
        """Return empty list - no recovery for local demo."""
        return []
# ---
def get_best_folding_sample(folded):
    confidence = 0.8 * folded["design_to_target_iptm"] + 0.2 * folded["design_ptm"]
    best_idx = np.argmax(confidence)

    # TODO: remove the "if k in folded"
    best_sample = {
        k: folded[k][best_idx] for k in const.eval_keys_confidence if k in folded
    }
    best_sample["coords"] = folded["coords"][best_idx]
    return best_sample
# ---
def make_state(key):
            model = MLP(in_size=2, out_size=1, width_size=2, depth=3, key=key)
            opt_state = optim.init(arrays_only(model))

            return model, opt_state, key
# ---
def _schedule(n):
        """Exponential anneal with flat start."""
        return jnp.clip(max_prob * jnp.exp(-decay * (n - flat_start)), min_prob, max_prob)
# ---
def all(
        self, axis: AxisSelection | None = None, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.all(self, axis=axis, where=where)
# ---
def mask_fn(model):
                decayed_paths = []
                mask = jax.tree_util.tree_map(
                    partial(_apply_on, decayed_paths, from_class_keypath=None),
                    model,
                    leaf_key_paths(model, is_leaf=is_leaf),
                    is_leaf=is_leaf,
                )

                # log all decayed weights
                levanter.tracker.log_hyperparameters({"decayed_weights": sorted(decayed_paths)})

                return mask
# ---
def _read_ovf_from_tar_ova(ova_path):
    with tarfile.open(ova_path) as tar:
        for member in tar:
            if member.name.endswith('.ovf'):
                with closing(tar.extractfile(member)) as ovf:
                    return ovf.read()
        raise ClientError('OVA does not contains file with .ovf suffix')
# ---
def teardown():
    """Destroy all TPU CI infrastructure."""
    delete_controller_vm()
    delete_all_tpu_vms()
# ---
def get_page(url):
    """Retrieve the given page."""
    return urllib2.urlopen(url).read()
# ---
def LookupEndpoint(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def transform_abstract(html: BeautifulSoup):
    # Transform the abstract from h6 to h2
    abstract = html.findAll("h6", {"class": "ltx_title_abstract"})
    for ab in abstract:
        ab.name = "h2"
    return html
# ---
def _infer_tpu_type_from_config(config_data: dict | None) -> str | None:
    if not config_data:
        return None

    try:
        return config_data["available_node_types"]["tpu_worker"]["node_config"]["acceleratorType"]
    except KeyError:
        return None
# ---
def num_pages(self) -> int:
        return self.page_ref_counts.axis_size("page")
# ---
def vm_count(self) -> int:
        """Total number of VMs in the group."""
        return len(self.vms)
# ---
def test_glob (self):
    import glob
    pattern = os.path.join (utils.TEST_ROOT, "*")
    self.assertEquals (list (fs.glob (pattern)), glob.glob (pattern))
# ---
def refund(self, params=None):
        if params is None:
            params = dict()
        if hasattr(self, 'lineitem_id'):
            params['lineitem_id'] = self.lineitem_id
            url = 'sales/refund_lineitem'
        elif hasattr(self, 'invoice_id'):
            params['invoice_id'] = self.invoice_id
            url = 'sales/refund_invoice'
        else:
            params['sale_id'] = self.sale_id
            url = 'sales/refund_invoice'
        return Sale(Api.call(url, params))
# ---
def _tpu_rpa_available() -> bool:
        if tpu_ragged_paged_attention is None:
            return False
        if jax.default_backend() != "tpu":
            return False
        kind = str(getattr(jax.devices()[0], "device_kind", "")).lower()
        if "tpu v2" in kind or "tpu v3" in kind:
            return False
        return True
# ---
def _merge_chunk_streams(exec_ctx, futures: list):
    active = {id(f): f for f in futures}

    while active:
        ready, _ = exec_ctx.wait(list(active.values()), num_returns=1)
        for gen in ready:
            try:
                items = exec_ctx.get(next(gen))
                yield from items
            except StopIteration:
                del active[id(gen)]
# ---
def __init__(self, inputfiles, grid):
        """
        :param inputfiles: list of pdb files of aligned models
        :param grid: 2d-array coordinates of each point of a grid, fourth column full of zeros
        """
        self.inputfiles = inputfiles
        self.models = []
        self.header = []
        self.radius = None
        self.atoms = []
        self.grid = grid
# ---
def model_config():
    config = ModelConfig(
        name="test-llama-200m",
        path="gs://marin-us-east5/gcsfuse_mount/perplexity-models/llama-200m",
        engine_kwargs={"enforce_eager": True, "max_model_len": 1024},
        generation_params={"max_tokens": 16},
    )
    return config
# ---
def __call__(self, x: NamedArray, attn_mask: Optional[AttentionMask | NamedArray], *, key=None) -> NamedArray:
        keys = hax.jax_utils.maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.blocks.fold(x, attn_mask, hax.arange(self.config.Layers), key=keys)
        x = self.ln_f(x)

        return x
# ---
def __call__(self, batch: dict[str, Any]):
        raise NotImplementedError
# ---
def key(item):
            field_val = item.get(self.field, '')
            if self.case_insensitive and isinstance(field_val, str):
                field_val = field_val.lower()
            return field_val
# ---
def __ge__(self, other, /):
        other = self._check_allowed_dtypes(other, "all", "__ge__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.greater_equal, self, other, dtype=nxp.bool)
# ---
def test_int_array_index_2d_no_op(spec, ind):
    a = xp.asarray(
        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]],
        chunks=(3, 3),
        spec=spec,
    )
    assert a is a[ind]
# ---
def _PostCreate(self):
    """Set the retention policy."""
    put_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'put-retention-policy',
        '--log-group-name', self.name,
        '--retention-in-days', str(self.retention_in_days)
    ]
    vm_util.IssueCommand(put_cmd)
# ---
def should_display_status_to_user(self):
        """Whether or not the status from this attempt should be displayed to the user."""
        return False
# ---
def testRaiseExitStatus(self):
    self.assertEqual(1, _GrumpRun('raise Exception')[0])
# ---
def _dslice_params(value: Any) -> tuple[Any, int, Any] | None:
    """
    For slice like objects, return (start, size, step).
    """
    if isinstance(value, HaliaxDSlice):
        return value.start, value.size, 1
    if is_pallas_dslice(value):
        start = value.start
        size = value.size
        step = value.stride
        return start, size, step

    return None
# ---
def process_latex_sqrt(text: str) -> str:
    """Convert \\sqrt{x} or \\sqrt x to sqrt(x)."""
    return replace_latex_command(text, "sqrt", lambda args: f"sqrt({args[0]})" if args else "sqrt", max_args=1)
# ---
def artist(self):
        return self.__artist
# ---
def setUp (self):
    utils.mktemp ()
    for filename in self.filenames:
      with open (os.path.join (utils.TEST_ROOT, filename), "w"):
        pass
# ---
def test_just_err_capture(self):
        with self.getcapture(out=False, err=True) as cap:
            sys.stdout.write("hello")
            sys.stderr.write("world")
            out, err = cap.readouterr()
        assert err == "world"
        assert not out
# ---
def even_bit_set_number(n): 
    count = 0;res = 0;temp = n 
    while(temp > 0): 
        if (count % 2 == 1): 
            res |= (1 << count)
        count+=1
        temp >>= 1
    return (n | res)
# ---
def partition_lora_params(params: M) -> Tuple[M, M]:
    """
    Partitions the given parameter tree into base/non-LoRA parameters and non-LoRA parameters.

    Returns:
        (base_params, lora_params)
    """
    lora_params, base_params = eqx.partition(params, is_lora_param, is_leaf=is_lora_param)
    return base_params, lora_params
# ---
def test_argmax(spec):
    a = xp.asarray([[11, 12, 13], [11, 11, 14], [10, 13, 11]], chunks=(2, 2), spec=spec)
    b = xp.argmax(a)
    assert_array_equal(
        b.compute(),
        np.array([[11, 12, 13], [11, 11, 14], [10, 13, 11]]).argmax(),
    )
# ---
def reference_impl_batched(x_batched: jax.Array) -> jax.Array:
    """Reference implementation (oracle).

    Replace this with a readable, correct baseline for your kernel. This should operate on batched inputs.
    """

    return jnp.tanh(x_batched)
# ---
def generate_pip_freeze():
    from importlib.metadata import distributions

    dists = distributions()
    return "\n".join(f"{dist.name}=={dist.version}" for dist in dists)
# ---
def __init__(self, dataset):
        self.dataset = dataset
# ---
def __floordiv__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.floor_divide(self, other)
# ---
def scan_fn(_, i):
            slice = x_ref.slice({"x": i})
            slice[...] = jnp.sin(x * i)
            return None, None
# ---
def _rm_thread(path: str) -> None:
    try:
        fs, _ = fsspec.core.url_to_fs(path)
        fs.rm(path, recursive=True)
    except Exception as e:
        logger.error(f"Failed to delete old checkpoint at {path}: {e}", exc_info=True)
# ---
def format_ambients(ambients):
    return format_line(prefix='ambients'.rjust(RJUST), values=ambients)
# ---
def publish(c):
    """Publish to production via rsync"""
    c.run('pelican -s publishconf.py')
    c.run(
        'rsync --delete --exclude ".DS_Store" -pthrvz -c '
        '{} {production}:{dest_path}'.format(
            CONFIG['deploy_path'].rstrip('/') + '/',
            **CONFIG))
# ---
def _read_jsonl(path: Path) -> list[dict]:
    """Read records from a JSONL file."""
    records = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            if line.strip():
                records.append(json.loads(line))
    return records
# ---
def __exit__(self, *_) -> None:
        self.shutdown()
# ---
def sl(start, size):
            return hax.ds(start, size)
# ---
def sum_nums(x, y,m,n):
    sum_nums= x + y
    if sum_nums in range(m, n):
        return 20
    else:
        return sum_nums
# ---
def __call__(self, msg):
        self._saved_msg.append(msg)
# ---
def save_apartment(self, apartment_dto, send_event=True):
        # type: (ApartmentDTO, bool) -> ApartmentDTO
        self._check_rebus_ids(apartment_dto)
        apartment_orm = ApartmentMapper.dto_to_orm(apartment_dto)
        apartment_orm.save()
        if send_event:
            ApartmentController.send_config_change_event('save')
        return ApartmentMapper.orm_to_dto(apartment_orm)
# ---
def stop(self):
        self.running = False
        self.articleDecoder.stop()
        for thread in self.threads:
            thread.stop()
        self.clearCache()
# ---
def test_endswith_autoescape(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e%fg", autoescape=True), {6})
# ---
def subset(ar, n): 
    res = 0
    ar.sort() 
    for i in range(0, n) : 
        count = 1
        for i in range(n - 1): 
            if ar[i] == ar[i + 1]: 
                count+=1
            else: 
                break 
        res = max(res, count)  
    return res
# ---
def parse_block_quote(self, m):
        self.tokens.append({'type': 'block_quote_start'})
        # clean leading >
        cap = _block_quote_leading_pattern.sub('', m.group(0))
        self.parse(cap)
        self.tokens.append({'type': 'block_quote_end'})
# ---
def isposinf(a: A) -> A:
    return wrap_elemwise_unary(jnp.isposinf, a)
# ---
def _assert_result(self, select, result, params=()):
        eq_(config.db.execute(select, params).fetchall(), result)
# ---
def fake_stream_disk(*args, **kwargs):
            pass
# ---
from typing import List


def remove_duplicates(numbers: List[int]) -> List[int]:
    """ From a list of integers, remove all elements that occur more than once.
    Keep order of elements left the same as in the input.
    >>> remove_duplicates([1, 2, 3, 2, 4])
    [1, 3, 4]
    """
    import collections
    c = collections.Counter(numbers)
    return [n for n in numbers if c[n] <= 1]
# ---
def testSmallMinMaxRange(self):
        self.assertConfigureFails(HPCP(), {'bandPreset':False, 'maxFrequency':200, 'minFrequency':1})
# ---
def test_svd():
    A = np.reshape(np.arange(32, dtype=np.float64), (16, 2))

    U, S, Vh = xp.linalg.svd(xp.asarray(A, chunks=(4, 2)), full_matrices=False)
    U, S, Vh = cubed.compute(U, S, Vh)

    assert_allclose(U * S @ Vh, A, atol=1e-08)
    assert_allclose(U.T @ U, np.eye(2, 2), atol=1e-08)  # U must be orthonormal
    assert_allclose(Vh @ Vh.T, np.eye(2, 2), atol=1e-08)
# ---
def declared_input_types(self):
    """Returns the list of data types of explicit declared inputs."""
    return self._input_types
# ---
def e(inp, dif):
            for i in inp:
                yield 2 if i == 10 else (1 if i >= dif else 0)
# ---
def top_left_corner3d(self):
        return self.edge_points3d[0]
# ---
def angle(a: A) -> A:
    return wrap_elemwise_unary(jnp.angle, a)
# ---
def remove_duplic_list(l):
    temp = []
    for x in l:
        if x not in temp:
            temp.append(x)
    return temp
# ---
def prefix(self):
        return self._matcher.prefix() and not self._always
# ---
def new_ref(value: NamedArray) -> NamedRef:
    """Construct a `NamedRef` from a `NamedArray`."""
    if isinstance(value, NamedArray):
        base_axes = value.axes
        impl = jax.new_ref(value.array)
    else:
        raise TypeError("new_ref only supports NamedArray inputs")

    prefix = tuple(slice(None) for _ in base_axes)
    return NamedRef(impl, base_axes, prefix)
# ---
def test_logout_route_requires_login(self):
        # Ensure logout route requres logged in user.
        response = self.client.get("/logout", follow_redirects=True)
        self.assertIn(b"Please log in to access this page", response.data)
# ---
def add_endpoint(self, endpoint: ControllerEndpoint, task_id: JobName | None = None) -> None:
        """Add an endpoint, optionally associating it with a task."""
        with self._lock:
            self._endpoints[endpoint.endpoint_id] = endpoint
            if task_id:
                self._endpoints_by_task.setdefault(task_id, set()).add(endpoint.endpoint_id)
# ---
def __init__(self, registry: str):
        self._registry = registry
# ---
def sqrt(a: A) -> A:
    return wrap_elemwise_unary(jnp.sqrt, a)
# ---
def func_name(self):
            pass
# ---
def check_greater(arr, number):
  arr.sort()
  if number > arr[-1]:
    return ('Yes, the entered number is greater than those in the array')
  else:
    return ('No, entered number is less than those in the array')
# ---
def set_process_stats_callback(stats_cb):
    """ Sets a callback function (hook) to push stats after a process operation call. """
    global stats_callback
    if stats_cb is None:
        pass
    elif stats_callback:
        log.warn("Stats callback already defined")
    stats_callback = stats_cb
# ---
def is_Sum_Of_Powers_Of_Two(n): 
    if (n % 2 == 1): 
        return False
    else: 
        return True
# ---
def _go(conn):
            assert_raises_message(
                MyException,
                "nope",
                conn.execute,
                    select([1]).\
                        where(
                            column('foo') == literal('bar', MyType())
                        )
            )
# ---
def vm_count(self) -> int:
        """Number of VMs in the TPU pod."""
        return get_tpu_topology(self.variant).vm_count
# ---
def get_hf_config(self):
        return self.config.to_hf_config(self.Vocab.size)
# ---
def __init__(self, difficulty: str = "medium"):
        # Difficulty not used for this task
        pass
# ---
def unflatten_masks(data: xr.Dataset) -> xr.Dataset:
    """Adds a "wetmask" `xarray.DataArray` with dimensions (lev, y, x)."""
    data_ = data.copy()
    if "wetmask" not in data_.variables:
        assert MASK_VARS[0] in data_.variables, "Wet mask must have masks as data vars!"

        wetmask = data_[MASK_VARS].to_array(dim="lev", name="wetmask")

        data_["wetmask"] = wetmask.assign_coords(lev=data_.lev)
        data_ = data_.drop_vars(MASK_VARS)

    return data_
# ---
def Repeat(x): 
    _size = len(x) 
    repeated = [] 
    for i in range(_size): 
        k = i + 1
        for j in range(k, _size): 
            if x[i] == x[j] and x[i] not in repeated: 
                repeated.append(x[i]) 
    return repeated
# ---
def run(self, command: str, timeout: Duration = Duration.from_seconds(30)) -> subprocess.CompletedProcess:
        """Run command synchronously and wait for completion."""
        ...
# ---
def test_cpu_device_produces_no_device(self):
        resources = ResourceConfig(device=CpuConfig())
        spec = convert_resources(resources)
        assert spec.device is None
# ---
def getapiname(self):
		return 'rds.aliyuncs.com.CheckAccountNameAvailable.2014-08-15'
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n), 'B': np.random.ranf(n)})
            Ac = df.A.rolling(3).sum()
            return Ac.sum()
# ---
def wrapper(*args, **kwargs):
            with _remove_tpu_lockfile_on_exit_cm():
                return fn(*args, **kwargs)
# ---
def get_runtime_env(self) -> dict:
        """
        Returns the runtime environment to run the evaluator on the Ray cluster.
        """
        return build_runtime_env_for_packages(
            extra=["eval"],
            env_vars={"HF_ALLOW_CODE_EVAL": "1"},
            # Human eval tests code from the model which requires permission to run.
        )
# ---
def test_capsysbinary(self, testdir):
        reprec = testdir.inline_runsource("""
            def test_hello(capsysbinary):
                import sys
                # some likely un-decodable bytes
                sys.stdout.buffer.write(b'\\xfe\\x98\\x20')
                out, err = capsysbinary.readouterr()
                assert out == b'\\xfe\\x98\\x20'
                assert err == b''
        """)
        reprec.assertoutcome(passed=1)
# ---
def tup_string(tup1):
  str =  ''.join(tup1)
  return str
# ---
def get_odd_occurence(arr, arr_size):
  for i in range(0, arr_size):
    count = 0
    for j in range(0, arr_size):
      if arr[i] == arr[j]:
        count += 1
    if (count % 2 != 0):
      return arr[i]
  return -1
# ---
def getSolution(self,solverId):
    path = "{base}/{solverId}/solution".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.get(path,
                       params=self.params,
                       headers=self.headers)
    self.validateReply(req)
    return req.json()
# ---
def on_show():
            """The window was shown.

            This event is triggered when a window is restored after being
            minimised, or after being displayed for the first time.

            :event:
            """
# ---
def build(self, axis: AxisSpec) -> LayerNormBase:
        """Build the normalization layer."""
        raise NotImplementedError
# ---
def _identity(_):
        return state.cache
# ---
def live_checker(text):
            regex_error.setStyleSheet("")
            regex_error.setText("")
            try:
                check()
            except OptionsCheckError as e:
                regex_error.setStyleSheet(self.STYLESHEET_ERROR)
                regex_error.setText(e.info)
# ---
def test_apply_blockwise_fused():
    bw_spec1 = make_blockwise_spec(
        key_function=make_map_blocks_key_function("a"), function=negative
    )
    bw_spec2 = make_blockwise_spec(
        key_function=make_map_blocks_key_function("b"), function=negative
    )

    bw_spec = fuse_blockwise_specs(bw_spec2, bw_spec1)

    input_data = {"a": [0, 1, 2, 3, 4]}
    out = [apply_blockwise(input_data, [i], bw_spec) for i in range(5)]
    assert out == [0, 1, 2, 3, 4]
# ---
def same_order(l1, l2):
    common_elements = set(l1) & set(l2)
    l1 = [e for e in l1 if e in common_elements]
    l2 = [e for e in l2 if e in common_elements]
    return l1 == l2
# ---
def is_slotted(self):
        return self.slotted
# ---
def run_func_processes(input, func=None, config=None, name=None, compute_id=None):
    return func(input, config=config)
# ---
def matchfn(self, f):
        # XXX: is_dir is set to True here for performance.
        # It should be set to whether "f" is actually a directory or not.
        return self._matcher.match_relative(f, True)
# ---
def onchange_date(self, cr, uid, ids, date, date_expected, context=None):
        """ On change of Scheduled Date gives a Move date.
        @param date_expected: Scheduled Date
        @param date: Move Date
        @return: Move Date
        """
        if not date_expected:
            date_expected = time.strftime('%Y-%m-%d %H:%M:%S')
        return {'value':{'date': date_expected}}
# ---
def get_size(self):
        """Return the current size of the window.

        The window size does not include the border or title bar.

        :rtype: (int, int)
        :return: The width and height of the window, in pixels.
        """
        raise NotImplementedError('abstract')
# ---
def _get_ref(self, ref) -> Tuple[str, Optional[str]]:
        if ref is None:
            if self.reference_checkpoint is None:
                raise ValueError("Must provide a reference checkpoint to load HFConfig from")
            ref = self.reference_checkpoint
        ref = _coerce_to_rr(ref)
        return ref.model_name_or_path, ref.revision
# ---
def __init__(self, comodel_name=None, inverse_name=None, string=None, **kwargs):
        super(One2many, self).__init__(
            comodel_name=comodel_name,
            inverse_name=inverse_name,
            string=string,
            **kwargs
        )
# ---
def sum_Square(n) : 
    i = 1 
    while i*i <= n : 
        j = 1
        while (j*j <= n) : 
            if (i*i+j*j == n) : 
                return True
            j = j+1
        i = i+1     
    return False
# ---
def genetic_modification_10(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'nucleic_acid_delivery_method': ['transduction'],
        'introduced_elements': 'gRNAs and CRISPR machinery',
    }
# ---
def stopRequested(self):
    return False
# ---
def loss_fn(model: LmHeadModel, example: LmExample, *, key=None):
        return model.compute_next_token_loss(example, key=key)
# ---
def testForElseBreakNotNested(self):
    self.assertRaisesRegexp(
        util.ParseError, "'continue' not in loop",
        _ParseAndVisit, 'for i in (1,):\n  pass\nelse:\n  continue')
# ---
def obj__formdata(self):
                js = Attr('./td/a[1]', 'onclick', default=None)(self)
                if js is None:
                    return
                args = re.search(r'\((.*)\)', js).group(1).split(',')

                form = args[0].strip().split('.')[1]
                idx = args[2].strip()
                idroot = args[4].strip().replace("'", "")
                return (form, idx, idroot)
# ---
def cookies(self):
        if self._cookies is None:
            cookie = self.headers.get("Cookie")
            if cookie is not None:
                cookies = SimpleCookie()
                cookies.load(cookie)
                self._cookies = {
                    name: cookie.value for name, cookie in cookies.items()
                }
            else:
                self._cookies = {}
        return self._cookies
# ---
def is_full(self):
        return self._queue.full()
# ---
def set_clone_source(self, slot_id: int, clone_source: jnp.ndarray | int) -> "SequenceTable":
        clone_sources = self.clone_sources.at["seq", slot_id].set(clone_source)
        return dataclasses.replace(self, clone_sources=clone_sources)
# ---
def run_check(runnable):
    sys.stderr.write('Test command: %s\n' % ' '.join(runnable))

    try:
        ret = subprocess.check_call(runnable)
    except subprocess.CalledProcessError as err:
        return err.returncode

    return ret
# ---
def test_create(self):
        resp = FakeResponse()
        self.type_action_controller.create(self.req, {}, resp)
        self.assertEqual({'id': fake.VOLUME_TYPE_ID,
                          'os-volume-type-access:is_public': True},
                         resp.obj['volume_type'])
# ---
def auth(self):
        return self._auth
# ---
def convert(messages):
            return [OpenAIChatMessage(role=msg[self.role_key], content=msg[self.content_key]) for msg in messages]
# ---
def trainables_only(model, filter):
    """
    Filters out non-trainable parameters from the model. This is used internally to
    for the optimizer state and to compute gradients, but you can also use it to filter out
    params for logging or something.
    """
    return _partition_trainable_params(model, filter)[0]
# ---
def clean_setup_provider(request, provider):
    BaseProvider.clear_providers()
    setup_or_skip(request, provider)
    yield
    BaseProvider.clear_providers()
# ---
def _global_router_name(self, hosting_device_id, logical=False):
        if logical is True:
            return cisco_constants.LOGICAL_ROUTER_ROLE_NAME
        else:
            return '%s-%s' % (cisco_constants.ROUTER_ROLE_NAME_PREFIX,
                              hosting_device_id[-cisco_constants.ROLE_ID_LEN:])
# ---
def pallas_tpu_impl_batched(x_batched: jax.Array) -> jax.Array:
    """TPU/Pallas implementation placeholder.

    Replace this body with a real `jax.experimental.pallas` kernel. Keep the signature stable and match the reference.
    """

    return reference_impl_batched(x_batched)
# ---
def convert_to_cache(self, value, record, validate=True):
        if not value:
            return False
        if isinstance(value, basestring):
            if validate:
                # force parsing for validation
                self.from_string(value)
            value = value[:DATETIME_LENGTH]
            if len(value) == DATE_LENGTH:
                value += " 00:00:00"
            return value
        return self.to_string(value)
# ---
def build(
        self, hist: int, area_weights: Grid, static_data: xr.Dataset | None
    ) -> nn.Module:
        # This prevents a circular import bug.
        from ocean_emulators.models.corrector import Correctors

        return Correctors(
            non_negative_corrector_names=self.non_negative_corrector_names,
            ocean_heat_corrector=self.ocean_heat_corrector,
            hist=hist,
            area_weights=area_weights,
            static_data=static_data,
        )
# ---
def test_transaction_engine_fn_commit(self):
        fn = self._trans_fn()
        testing.db.transaction(fn, 5, value=8)
        self._assert_fn(5, value=8)
# ---
def value_match(cls, pattern, value):
        return pattern == value
# ---
def start_recording(self):
        self.recording_enabled = True
        self.startrecordingAction.setEnabled(False)
        self.stoprecordingAction.setEnabled(True)
# ---
def _is_port_free(self, port: int) -> bool:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(("", port))
                return True
            except OSError:
                return False
# ---
def testZeros(self):
        hpcp = HPCP()([0]*10, [0]*10)
        self.assertEqualVector(hpcp, [0.]*12)
# ---
def modulate_fused(x: torch.Tensor,
                   shift: torch.Tensor,
                   scale: torch.Tensor) -> torch.Tensor:
  return modulate(x, shift, scale)
# ---
def __post_init__(self):
        if self.hidden_dim % self.num_heads:
            raise ValueError(f"hidden_dim {self.hidden_dim} must be divisible by num_heads {self.num_heads}")
        if self.max_seq_len % self.num_blocks:
            raise ValueError(f"seq_len {self.max_seq_len} must be divisible by num_blocks {self.num_blocks}")
# ---
def init(cls, Vocab: Axis, config: Gemma3Config, *, key):
        k_t, k_emb = jrandom.split(key, 2)
        transformer = Gemma2Transformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return Gemma3LMHeadModel(transformer, embeddings, lm_head)
# ---
def run(
        self, fn: Callable, *args, name: str | None = None
    ) -> _ImmediateFuture | Generator[_ImmediateFuture, None, None]:
        """Execute function immediately and wrap result."""
        result = fn(*args)
        return _ImmediateFuture(result)
# ---
def __init__(self):
        try:
            _check_option_support(['timing', 'single_bridge', 'dual_bridge'])
        except OSError:
            raise exception.DriverLoadError(
                driver=self.__class__.__name__,
                reason=_("Unable to locate usable ipmitool command in "
                         "the system path when checking ipmitool version"))
        _check_temp_dir()
# ---
def __init__(self, artist, title, year=None):
        super(Painting, self).__init__(artist, title, year)
# ---
def get_shard_dir(dir_name: str, subset_name: str | None, split: str) -> str:
    if (subset_name == "default") or (subset_name is None):
        return os.path.join(dir_name, split)

    logger.info(f"Getting shard dir for {dir_name} {subset_name} {split}")
    logger.info(f"shard dir (os.path.join(dir_name, subset_name, split)): {os.path.join(dir_name, subset_name, split)}")
    return os.path.join(dir_name, subset_name, split)
# ---
def test_axis_names_metadata():
    field = M.__dataclass_fields__["a"]
    assert field.metadata["axis_names"] == ("batch",)
# ---
def _nan_test_deep(ds_input: xr.Dataset):
    """Expensive tests that compute on the entire dataset"""
    ds_nan_test_2d, ds_nan_test_3d = split_2d_3d(ds_input)
    print("2D consistency check")
    ensure_nan_consistency(ds_nan_test_2d, "2D nan consistency check")

    print("3D consistency check")
    ensure_nan_consistency(ds_nan_test_3d, "3D nan consistency check")
# ---
def _run(self):
        """Thread target - runs the worker with error handling."""
        try:
            self._create_and_run_worker()
            self.result_queue.put(("success", None))
        except Exception as e:
            logger.error(f"{self.__class__.__name__} failed", exc_info=True)
            self.result_queue.put(("error", e))
# ---
def _async_checkpoint_remover(self):
        while True:
            checkpoint = self._async_checkpoint_remover_queue.get(block=True)
            self._checkpoint_being_removed = checkpoint
            self._do_rm_checkpoint(checkpoint)
            self._checkpoint_being_removed = None
# ---
def fake_authorize(context, target=None, action=None):
            raise exception.PolicyNotAuthorized(action='index')
# ---
def _getSSCC(self, cr, uid, context=None):
        cr.execute('select id from stock_tracking where create_uid=%s order by id desc limit 1', (uid,))
        res = cr.fetchone()
        return (res and res[0]) or False
# ---
def image_renderer(node: RenderTreeNode, context: RenderContext) -> str:
        return _render_inline_as_text(node, context)
# ---
def rint(a: A) -> A:
    return wrap_elemwise_unary(jnp.rint, a)
# ---
def gelu(a: A, approximate: bool = True) -> A:
    return wrap_elemwise_unary(jnn.gelu, a, approximate=approximate)
# ---
def delete_image(self, image_id):
        """Deletes the provided image."""
        resp, body = self.delete("images/%s" % str(image_id))
        self.validate_response(schema.delete, resp, body)
        return service_client.ResponseBody(resp, body)
# ---
def wrap_coeffs(coeffs):
        base = x if isinstance(x, NamedArray) else y
        axis = _poly_axis_from_input(base, coeffs.shape[0])
        return NamedArray(coeffs, (axis,))
# ---
def test_clone_volume_clear(self):
        drv = self._driver
        mox = self._prepare_clone_mock('fail')

        mox.ReplayAll()

        volume_name = 'volume_name'
        clone_name = 'clone_name'
        volume_id = volume_name + str(hash(volume_name))
        try:
            drv._clone_volume(volume_name, clone_name, volume_id)
        except Exception as e:
            if isinstance(e, api.NaApiError):
                pass
            else:
                raise

        mox.VerifyAll()
# ---
def switch_to(self):
        """Make this window the current OpenGL rendering context.

        Only one OpenGL context can be active at a time.  This method sets
        the current window's context to be current.  You should use this
        method in preference to `pyglet.gl.Context.set_current`, as it may
        perform additional initialisation functions.
        """
        raise NotImplementedError('abstract')
# ---
def loadlist(self, playlist, mode='replace'):
        self.command('loadlist', playlist.encode(fs_enc), mode)
# ---
def __init__(
        self,
        pdrop: float = 0.5,
        broadcast_axes: AxisSpec | None = None,
        inference: bool = False,
    ):
        self.pdrop = pdrop
        self.broadcast_axes = broadcast_axes
        self.inference = inference
# ---
def canonicalize(value):
        if isinstance(value, dict):
            return {k: canonicalize(v) for k, v in sorted(value.items())}
        if isinstance(value, list):
            return [canonicalize(x) for x in value]
        if callable(value):
            return f"{value.__module__}.{value.__qualname__}"
        return value
# ---
def check_chainlink_initialized():
    """Check if .chainlink directory exists."""
    cwd = os.getcwd()
    current = cwd

    while True:
        candidate = os.path.join(current, ".chainlink")
        if os.path.isdir(candidate):
            return True
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent

    return False
# ---
def __init__(self):
        try:
            _check_option_support(['single_bridge', 'dual_bridge'])
        except OSError:
            raise exception.DriverLoadError(
                driver=self.__class__.__name__,
                reason=_("Unable to locate usable ipmitool command in "
                         "the system path when checking ipmitool version"))
        _check_temp_dir()
# ---
def loss_fn_with_mask(
        pred: torch.Tensor, target: torch.Tensor, wet: PrognosticMask
    ) -> torch.Tensor:
        wet = wet.to(device=pred.device)
        pred = pred * wet
        target = target * wet
        return loss_fn(pred, target)
# ---
def test_hf_audio_loading():
    # Use the Real Librispeech Valudation. Testing one doesn't support streaming.
    ac = AudioDatasetSourceConfig(id="WillHeld/test_librispeech_parquet", text_key="text")
    audio_iterator = ac.doc_iterator("validation")
    for i in range(10):
        audio, sample, text = next(audio_iterator)
# ---
def reserve_slot(self, slot_id: int | jnp.ndarray | None = None) -> tuple["DecodeState", int]:
        sequences, slot = self.sequences.reserve_slot(slot_id)
        return dataclasses.replace(self, sequences=sequences), slot
# ---
def result(self, timeout: float | None = None) -> Any:
        return ray.get(self._object_ref, timeout=timeout)
# ---
def truncate_model_name(model_name: str, max_length: int = 62) -> str:
    """Truncate model name to max_length if it exceeds that length."""
    return model_name[:max_length] if len(model_name) > max_length else model_name
# ---
def selection_function(out_key):
        out_coords = out_key[1:]
        block_id = out_coords
        return get_item(read_chunks, block_id)
# ---
def concatenate_strings(test_tup1, test_tup2):
  res = tuple(ele1 + ele2 for ele1, ele2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def test_rate_limiter_throttles():
    """RateLimiter prevents running too frequently."""
    limiter = RateLimiter(interval_seconds=0.2)
    assert limiter.should_run()
    assert not limiter.should_run()
    time.sleep(0.25)
    assert limiter.should_run()
# ---
def to_state_dict(self, prefix: Optional[str] = None) -> StateDict:
        # weight can be None for certain filtering things like LoRA
        scaled = dataclasses.replace(
            self, weight=self.weight * self.reparam.active_scale if self.weight is not None else None
        )
        return default_eqx_module_to_state_dict(scaled, prefix)
# ---
def user_title(self):
        return self._get_title_and_company()['title']
# ---
def testForBreak(self):
    self.assertEqual((0, '1\n'), _GrumpRun(textwrap.dedent("""\
        for i in (1, 2, 3):
          print i
          break""")))
# ---
def baz(x: Float["b"]):  # type: ignore  # noqa: F722
        pass
# ---
def min_jumps(arr, n):
	jumps = [0 for i in range(n)]
	if (n == 0) or (arr[0] == 0):
		return float('inf')
	jumps[0] = 0
	for i in range(1, n):
		jumps[i] = float('inf')
		for j in range(i):
			if (i <= j + arr[j]) and (jumps[j] != float('inf')):
				jumps[i] = min(jumps[i], jumps[j] + 1)
				break
	return jumps[n-1]
# ---

def next_smallest(lst):
    """
    You are given a list of integers.
    Write a function next_smallest() that returns the 2nd smallest element of the list.
    Return None if there is no such element.
    
    next_smallest([1, 2, 3, 4, 5]) == 2
    next_smallest([5, 1, 4, 3, 2]) == 2
    next_smallest([]) == None
    next_smallest([1, 1]) == None
    """
    lst = sorted(set(lst))
    return None if len(lst) < 2 else lst[1]
# ---
def _create_lm_example(tokens):
                tokens = hax.named(tokens, self.Pos)
                example = LmExample.causal(
                    tokens=tokens,
                    eos_id=eos_id,
                    block_cross_document_attention=block_cross_document_attention,
                )
                example = jax.lax.with_sharding_constraint(example, sharding)
                return example
# ---
def start(self):
        self._start_time = time.time()
        self._n += 1
# ---
def model_type(cls) -> Type[LmT]:
        pass
# ---
def add_transactions_to_block(self, block, tx_list):
        [tx.rehash() for tx in tx_list]
        block.vtx.extend(tx_list)
# ---
def GetVariableValue( variable ):
  return vim.eval( variable )
# ---
def delete(self):
        self.fsm.requestFinalState()
        DistributedCCharBaseAI.DistributedCCharBaseAI.delete(self)
        self.lonelyDoneEvent = None
        self.lonely = None
        self.chattyDoneEvent = None
        self.chatty = None
        self.walkDoneEvent = None
        self.walk = None
        return
# ---
def run(self, config=None, run_prediction=False):
        self.distribute_tasks()
        self.aggregate_metrics()
# ---
def swap_List(newList): 
    size = len(newList) 
    temp = newList[0] 
    newList[0] = newList[size - 1] 
    newList[size - 1] = temp  
    return newList
# ---
def mock_load_datasets(config):
        return [DatasetWithMetaData(mock_dataset, "arithmetic", "test", "main")]
# ---
def get_environment() -> Environment:
    loader = FileSystemLoader(TEMPLATE_PATHS)
    environment = Environment(loader=loader)
    environment.filters.update(FILTERS)

    return environment
# ---
def __init__(self, sigma_min=0, sigma_max=10, dtype=torch.float32):
    super().__init__()
    self.sigma_min = torch.tensor(sigma_min, dtype=dtype)
    self.sigma_max = torch.tensor(sigma_max, dtype=dtype)
# ---
def maximum(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "maximum")
    if x1.dtype not in _real_numeric_dtypes or x2.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in maximum")
    return elemwise(nxp.maximum, x1, x2, dtype=result_type(x1, x2))
# ---
def log_summary(self, metrics: typing.Mapping[str, Any]):
        self.run.summary.update(_convert_value_to_loggable_rec(metrics))
# ---
def remove_nested(test_tup):
  res = tuple()
  for count, ele in enumerate(test_tup):
    if not isinstance(ele, tuple):
      res = res + (ele, )
  return (res)
# ---
def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()
# ---
def do_shutdown():
        logger.info("Shutting down ray...")
        ray.shutdown()
# ---
def validation_sets(self) -> Mapping[str, ProcessedAudioCache]:
        if self._has_validation_set:
            validation_set = self.validation_set()
            if validation_set is not None:
                return {"": validation_set}
        return {}
# ---
def min(x, /, *, axis=None, keepdims=False, split_every=None):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in min")
    return reduction(
        x,
        nxp.min,
        axis=axis,
        dtype=x.dtype,
        split_every=split_every,
        keepdims=keepdims,
    )
# ---
def shape(self) -> tuple[int, ...]:
        return self.array.shape
# ---
def attention_config(self) -> AttentionConfig:  # type: ignore[override]
        cfg = super().attention_config()
        return dataclasses.replace(cfg, qk_norm=self.norm_config)
# ---
def get_tracker(name: Literal["tensorboard"]) -> TensorboardTracker: ...
# ---
def check_consistent_with_graph(key_fn, graph):
    for k, v in graph.items():
        assert key_fn(k) == v
# ---
def _flatten(means_or_stds: xr.Dataset) -> torch.Tensor:
    if "lev" in means_or_stds.dims:
        array = conditional_rearrange(
            means_or_stds,
            "(variable lev)=var",
            concat_dim="var",
        ).rename({"var": "variable"})
    else:
        array = means_or_stds.to_dataarray()
    return torch.from_numpy(array.to_numpy().flatten())
# ---
def render(self, name, value, attrs=None, renderer=None):
        output = []
        output.append(super(AdminFileWidget, self).render(name, value, attrs)) # really for AdminFileWidget
        instance = getattr(value, 'instance', None)
        if instance is not None and value:
            output = ['<a target="_blank" href="%s"><img src="%s" alt="%s"/></a>' % \
                (instance.image.url, instance.thumb.url, instance.image)] + output
        return mark_safe(u''.join(output))
# ---
def include_constructor(loader: yaml.Loader, node: yaml.Node) -> Any:
        if hasattr(loader.stream, "name"):
            name = loader.stream.name  # type: ignore
        else:
            raise ValueError(
                "To support includes, you must load a file object, not a string"
            )
        filename = os.path.normpath(os.path.join(os.path.dirname(name), node.value))
        with open(filename) as f:
            return yaml.safe_load(f)
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetDepartmentCount(), 9)
# ---
def compute_xxh3_128_hex(text: str) -> str:
    val = hash_xxh3_128(text.encode("utf-8"))
    return f"{val:032x}"
# ---
def contractedBrailleToggled(self, checkbox):
        grid = self.get_widget('contractionTableGrid')
        grid.set_sensitive(checkbox.get_active())
        self.prefsDict["enableContractedBraille"] = checkbox.get_active()
# ---
def depth(self):
        return self.__depth
# ---
def __init__(self, host='localhost', port=8125, enabled=True, prefix=''):
        self.addr = None
        self.enabled = enabled
        if enabled:
            self.set_address(host, port)
        self.prefix = prefix
        self.udp_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
# ---
def _node_type_name(node: ast.AST) -> str:
    return type(node).__name__
# ---
def test_load_hf_model_streaming(model_id: str, config, min_params: int):
    """Test loading HF models via streaming."""
    converter = config.hf_checkpoint_converter()

    with use_test_mesh():
        state_dict = converter.load_state_dict(model_id)

    assert isinstance(state_dict, dict)
    assert len(state_dict) > 0

    param_count = _count_params(state_dict)
    assert param_count >= min_params, f"Expected at least {min_params} params, got {param_count}"
# ---
def _count_params(state_dict: dict) -> int:
    """Count total parameters in a state dict."""
    total = 0
    for value in state_dict.values():
        if hasattr(value, "size"):
            total += value.size
        elif hasattr(value, "shape"):
            total += int(np.prod(value.shape))
    return total
# ---
def send_bit(self, bit):
        """ Send out a single bit, and pulse clock."""
        if self._MOSI == 0:
            return
        #
        # The input is read on the rising edge of the clock.
        #
        GPIO.output(self._MOSI, bit)  # Set the bit.
        GPIO.output(self._CLK, 1)     # Rising edge sends data
        GPIO.output(self._CLK, 0)
# ---
def get_root_input_type_from_json(data):
    """Return the root input type from JSON formatted string."""
    return parse_format(json.loads(data))
# ---
def test_add_scalars():
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2))

    b = xp.add(a, 1)
    assert_array_equal(b.compute(), np.array([[2, 3, 4], [5, 6, 7], [8, 9, 10]]))

    c = xp.add(2, a)
    assert_array_equal(c.compute(), np.array([[3, 4, 5], [6, 7, 8], [9, 10, 11]]))

    with pytest.raises(TypeError):
        xp.add(1, 2)
# ---
def __init__(self, head):
        self.head = head
        self.lsize = 0
        while head.next:
            head = head.next
            self.lsize += 1

        self.m1_idx = None
        self.m2_idx = None
        if self.lsize > self._largesize:
            self.m1_idx = self.lsize / 3   # start from 1/3
            self.m1 = self._getN(self.m1_idx)
            self.m2_idx = self.m1_idx * 2  # start from 2/3
            self.m2 = self._getN(self.m2_idx)
# ---
def get_output_shape(input_shape):
      return 1 * input_shape
# ---
def out_qdq(compute_dtype, out, scale, amax_history):
    return out
# ---
def getRandomLine():
    return list(zip(np.random.uniform(-1,1.00,2),np.random.uniform(-1,1.00,2)))
# ---
def testDoubleBasic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.float64)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.float64)
    self._testCpuMatmul(x, y)
# ---
def object_list_uid(object_list):
  """Creates a single string from object ids."""
  object_list = nest.flatten(object_list)
  return ', '.join([str(abs(id(x))) for x in object_list])
# ---
def LineAndColumnAfterLastNonWhitespace():
  line, column = CurrentLineAndColumn()
  line_value = vim.current.line[ :column ].rstrip()
  while not line_value:
    line = line - 1
    if line == -1:
      return None
    line_value = vim.current.buffer[ line ].rstrip()
  return line, len( line_value )
# ---
def dot(
    axis: AxisSelection | None,
    *arrays: NamedArray,
    precision: PrecisionLike = None,
    preferred_element_type: DTypeLike | None = None,
    out_axes: PartialAxisSpec | None = ...,
    dot_general=jax.lax.dot_general,
) -> NamedArray: ...
# ---
def _get_username(self, section=None):
        try:
            names = env.config_object.get_list(section, env.config_object.USERNAME)
            username = names[0]
        except:
            print ('You must first set up a database server on this machine, '
                   'and create a database user')
            raise
        return username
# ---
def reload(self, weight_callback: WeightSource):
        """Reload the model weights using the provided callback.

        Args:
            weight_callback: Function that takes the current model and returns new model
        """
        self.inference_context.reload(weight_callback)
# ---
def addOperators(self, num, target):
        """
        Adapted from https://leetcode.com/discuss/58614/java-standard-backtrace-ac-solutoin-short-and-clear

        Algorithm:
        1. DFS
        2. Special handling for multiplication
        3. Detect invalid number with leading 0's
        :type num: str
        :type target: int
        :rtype: List[str]
        """
        ret = []
        self.dfs(num, target, 0, "", 0, 0, ret)
        return ret
# ---
def __float__(self, /):
        if self.ndim != 0:
            raise TypeError("float is only allowed on arrays with 0 dimensions")
        if self.dtype in _complex_floating_dtypes:
            raise TypeError("float is not allowed on complex floating-point arrays")
        return float(self.compute())
# ---
def height(self):
        return self.__height
# ---
def _qr2(a):
    Q, R = nxp.linalg.qr(a)
    U, S, Vh = nxp.linalg.svd(R)
    S = S[:, nxp.newaxis]  # add extra dim
    return Q, R, U, S, Vh
# ---
def has_type(self, node_type: str) -> bool:
        """Check whether the bank has entries for the given node type."""
        entries = self.entries.get(node_type)
        return entries is not None and len(entries) > 0
# ---
def test_iris_config_missing_file(tmp_path):
    """Test error on missing config file."""
    with pytest.raises(FileNotFoundError):
        IrisConfig.load(tmp_path / "nonexistent.yaml")
# ---
def _serialize_json_and_commit(path: str, obj):
    fs: AbstractFileSystem = fsspec.core.url_to_fs(path)[0]
    fs.mkdirs(os.path.dirname(path), exist_ok=True)
    if fs.exists(path):
        fs.copy(path, f"{path}.bak")

    for _ in range(10):
        try:
            with fsspec.open(path, "w") as file:
                file.write(obj.to_json())
            break
        except FileNotFoundError:
            logger.exception(f"Failed to write {path}")
# ---
def test_from_string(self):
        """Can convert from string to type id."""
        self.assertEquals(
            vm_utils.ImageType.from_string(vm_utils.ImageType.KERNEL_STR),
            vm_utils.ImageType.KERNEL)
# ---
def __init__(self, dag):
        self.dag = dag
# ---
def trainer_config():
    """Create a basic trainer config for tests."""
    return TrainerConfig(
        id="test-run",
        checkpointer=CheckpointerConfig(),
        ray=RayConfig(),
    )
# ---
def ratio_response(x):
    """How we display actual size ratios

    Common ratios in sizes span several orders of magnitude,
    which is hard for us to perceive.

    We keep ratios in the 1-3 range accurate, and then apply a logarithm to
    values up until about 100 or so, at which point we stop scaling.
    """
    if x < math.e:
        return x
    elif x <= 100:
        return math.log(x + 12.4)  # f(e) == e
    else:
        return math.log(100 + 12.4)
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.four.unique() == 3.0).sum()
# ---
def _read_stack_chunk(array, axis=None):
    return nxp.expand_dims(array, axis=axis)
# ---
def format_doc(hit, schema, dates):
    """Format given doc to match given schema."""
    doc = hit.get('_source', {})
    doc.setdefault(config.ID_FIELD, hit.get('_id'))
    doc.setdefault('_type', hit.get('_type'))

    for key in dates:
        if key in doc:
            doc[key] = parse_date(doc[key])

    return doc
# ---
def _clear_mesh(mesh):
        del mesh
# ---
def test_valid_absolute_path(self):
        """Test that absolute paths are accepted."""
        loc = UnresolvedLocation(path="/absolute/path/data.zarr")
        assert loc.path == "/absolute/path/data.zarr"
# ---
def __add_random_fuzzing_values(self):
        n = self.ui.spinBoxNumberRandom.value()
        minimum = self.ui.spinBoxRandomMinimum.value()
        maximum = self.ui.spinBoxRandomMaximum.value()
        self.fuzz_table_model.add_random(n, minimum, maximum)
# ---
def orcaModifierChanged(self, widget):
        """Signal handler for the changed signal for the orcaModifierComboBox
           Set the 'orcaModifierKeys' preference to the new value.

        Arguments:
        - widget: the component that generated the signal.
        """

        model = widget.get_model()
        myIter = widget.get_active_iter()
        orcaModifier = model[myIter][0]
        self.prefsDict["orcaModifierKeys"] = orcaModifier.split(', ')
# ---
def match(self, item):
        return True
# ---
def __init__(self, *args, **kwargs):
        super(FarmworkForm, self).__init__(*args, **kwargs)
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
        # parse the shard name to get the shard number
        shard_num = int(shard_name.split("_")[1])
        return ([shard_num * 10 + i] * 10 for i in range(row, self._rows_per_shard))
# ---
def test_bound_limit_offset(self):
        table = self.tables.some_table
        self._assert_result(
            select([table])
            .order_by(table.c.id)
            .limit(bindparam("l"))
            .offset(bindparam("o")),
            [(2, 2, 3), (3, 3, 4)],
            params={"l": 2, "o": 1},
        )
# ---
def __init__(self, lat: Lat, lon: Lon):
        super().__init__()
        lat_lon_grid = lat_lon_meshgrid(lat, lon)  # [2, H, W]
        lat_rad = torch.deg2rad(lat_lon_grid[0])  # [H, W]
        lon_rad = torch.deg2rad(lat_lon_grid[1])  # [H, W]

        x = torch.cos(lat_rad) * torch.cos(lon_rad)
        y = torch.cos(lat_rad) * torch.sin(lon_rad)
        z = torch.sin(lat_rad)

        grid = torch.stack([x, y, z], dim=0)  # [3, H, W]
        self.grid = grid.float().unsqueeze(0)
# ---
def _infer_out_sharding(out_axes: tuple[Axis, ...]):
    mapping = current_thread_local_mapping()
    mesh = _resolve_mesh()

    if mesh is None:
        return None

    pspec = pspec_for_axis(out_axes, mapping)
    if not all_mesh_axes_explicit(mesh, pspec):
        # JAX doesn't like it when you pass out_sharding for non-explicit axes
        return None
    return NamedSharding(mesh, pspec)
# ---
def start(self) -> str:
        """Start controller, return address. Idempotent - returns existing if healthy."""
        ...
# ---
def __init__(self, job_id: JobName, status: cluster_pb2.JobStatus):
        self.job_id = job_id
        self.status = status
        state_name = cluster_pb2.JobState.Name(status.state)
        msg = f"Job {job_id} {state_name}"
        if status.error:
            msg += f": {status.error}"
        super().__init__(msg)
# ---
def batch_axis_name(self) -> str | None:
        return self.mesh.batch_axis_name
# ---
def permute_sharded(x_flat_: Array, topk_idx_flat_: Array):
            sort_idx_ = jnp.argsort(topk_idx_flat_, axis=-1)
            x_repeat_sort_ = jnp.take(x_flat_, sort_idx_ // self.config.num_experts_per_tok, axis=0)
            group_sizes_ = jnp.bincount(topk_idx_flat_, length=self.config.n_routed_experts)

            return x_repeat_sort_, group_sizes_, sort_idx_
# ---
def _default_move_type(self, cr, uid, context=None):
        """ Gets default type of move
        @return: type
        """
        if context is None:
            context = {}
        picking_type = context.get('picking_type')
        type = 'internal'
        if picking_type == 'in':
            type = 'in'
        elif picking_type == 'out':
            type = 'out'
        return type
# ---
import re
def change_date_format(dt):
        return re.sub(r'(\d{4})-(\d{1,2})-(\d{1,2})', '\\3-\\2-\\1', dt)
# ---
def __init__(self, index):
        self.index = index
        self.events = []
        self.length = 0
# ---
def photoloop():
	count = 0
	while (count < 9):
		sleep(0.5)
		image1 =  newphoto()
		if lastfilter is not "none":
			dofilter(lastfilter,image1)
		count = count + 1
# ---
def testClassDef(self):
    self.assertEqual((0, "<type 'type'>\n"), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          pass
        print type(Foo)""")))
# ---
def test_ref_get_with_invalid_election_id_non_existent_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': '69' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified an ID for a non-existent election.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def _run() -> int:
        return len(dupekit.process_native(small_parquet_path))
# ---
import math
def sum_of_odd_Factors(n): 
    res = 1
    while n % 2 == 0: 
        n = n // 2 
    for i in range(3,int(math.sqrt(n) + 1)): 
        count = 0
        curr_sum = 1
        curr_term = 1
        while n % i == 0: 
            count+=1 
            n = n // i 
            curr_term *= i 
            curr_sum += curr_term    
        res *= curr_sum  
    if n >= 2: 
        res *= (1 + n) 
    return res
# ---
def testGradientInput1WithTranspose(self):
    self._VerifyInput1(transpose_a=True, transpose_b=False)
    self._VerifyInput1(transpose_a=False, transpose_b=True)
    self._VerifyInput1(transpose_a=True, transpose_b=True)
# ---
def __call__(
        self,
        x: NamedArray,
        xa: Optional[NamedArray] = None,
        attn_mask: Optional[AttentionMask] = None,
        *,
        key=None,
    ) -> NamedArray:
        keys = hax.jax_utils.maybe_rng_split(key, self.Layer.size) if key is not None else None
        x = self.layers.fold(x, xa, attn_mask, key=keys)
        x = self.layer_norm(x)

        return x
# ---
def all_ready(self) -> bool:
        """True if all VMs in the group are in READY state."""
        return all(v.state == vm_pb2.VM_STATE_READY for v in self.vms)
# ---
def _get_ip_address() -> str:
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            return s.getsockname()[0]
    except Exception:
        return "127.0.0.1"
# ---
def convert_batch_dict_to_output_rows(batch_dict: dict, output_column_names: list[str], batch_size: int) -> list[dict]:
    output_rows = []
    for i in range(batch_size):
        output_row = {}
        for col in output_column_names:
            if col in batch_dict:
                output_row[col] = batch_dict[col][i]
        output_rows.append(output_row)

    return output_rows
# ---
def list_tasks(self) -> list[TaskInfo]: ...
# ---
def test_impl_2(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.rolling(7).sum()
            return Ac.sum()
# ---
def device_username(self):
        if self._values['managed']:
            return None
        return self._values['device_username']
# ---
def causal(cls, *, sliding_window: int | None = None) -> "AttentionMask":
        return cls(is_causal=True, sliding_window=sliding_window)
# ---
def _block_disk_supported(conn, root):
    '''
    Currently we do not support importing VMs with block device from
    Xen on Rhel 5.x
    '''
    if conn.getType() == 'Xen':
        block_disks = root.findall('.//disk[@type="block"]')
        block_disks = [d for d in block_disks
                       if d.attrib.get('device', None) == "disk"]
        return len(block_disks) == 0

    return True
# ---
def encode(self, text, add_special_tokens=True):
        if add_special_tokens:
            text = f"{self.bos_token} {text} {self.eos_token}"

        tokens = []
        while text:
            for token in self.TOKENS:
                if text.startswith(token):
                    tokens.append(self.TOKENS.index(token))
                    text = text[len(token) :]
                    break
            else:
                raise ValueError(f"Unknown token in text: '{text}'")

        return tokens
# ---
def cancel_job_fetch(self):
        self.main_model.cancel_job_fetch()
# ---
def string_match(cls, pattern, value):
        """Determine whether the value matches the pattern. Both
        arguments are strings. Subclasses implement this method.
        """
        raise NotImplementedError()
# ---
def build_pot(srcdir, project_id, sources):
    # Must be relative paths
    sources = [os.path.join('C', source) for source in sources]
    outfile = os.path.join(srcdir, project_id + '.pot')
    subprocess.call(['itstool', '-o', outfile] + sources)
# ---
def do_block(carry: CarryT, block: M, *args, **kwargs) -> tuple[CarryT, OutputT_co]:
            carry, output = fn(block, carry, *args, **kwargs)
            return carry, output
# ---
def __init__(self, vocab_size, **kwargs):
        self._vocab = {i: i for i in range(vocab_size)}
        self._vocab_size = vocab_size
        super().__init__(**kwargs)
# ---
def _merge_lora_modules(module):
        if isinstance(module, LoraLinear):
            return module.merge()
        else:
            return module
# ---
def test_attributos_quarto_voo(self):
        p_voo = self.escala.escalas[25]
        self.assertFalse(p_voo.checkin)
        self.assertEqual(p_voo.checkin_time, None)
        self.assertEqual(p_voo.flight_no, '2872')
        self.assertEqual(p_voo.activity_info, 'AD2872')
# ---
def logprobs_from_choice(self, choice: Choice) -> np.ndarray:
        """Extract logprobs array."""
        if not choice.logprobs or not choice.logprobs.content:
            raise ValueError("Choice missing logprobs. Use logprobs=True in API call.")

        logprobs = np.array([t.logprob for t in choice.logprobs.content], dtype=np.float32)

        if np.all(logprobs == 0):
            logger.warning("All logprobs are zero - may cause NaN loss")

        return logprobs
# ---
def chart_title(self):
        """Returns a title for the chart."""
        if self.field:
            return 'Top filtered results for "{0:s}"'.format(self.field)
        return 'Top results for an unknown field after filtering'
# ---
def chaos_raise(key: str) -> None:
    """Convenience: raise an exception if chaos fires for this key.

    Handles delay_seconds before raising.
    """
    if rule := chaos(key):
        time.sleep(rule.delay_seconds)
        raise rule.error or RuntimeError(f"chaos: {key}")
# ---
def fsspec_size(file_path: str) -> int:
    """Get file size (in bytes) of a file on an `fsspec` filesystem."""
    fs = fsspec.core.url_to_fs(file_path)[0]

    return fs.size(file_path)
# ---
def __init__(self, somedict):
        self._dict = dict(somedict)   # make a copy
        self._hash = None
# ---
def __init__(
        self,
        loss: torch.Tensor,
        loss_per_channel: torch.Tensor,
        input_data: torch.Tensor,
        target_data: torch.Tensor,
        gen_data: torch.Tensor,
    ):
        super().__init__(loss, loss_per_channel)
        assert target_data.shape == gen_data.shape
        self.input_data = input_data
        self.target_data = target_data
        self.gen_data = gen_data
# ---
def count_Fac(n):  
    m = n 
    count = 0
    i = 2
    while((i * i) <= m): 
        total = 0
        while (n % i == 0): 
            n /= i 
            total += 1 
        temp = 0
        j = 1
        while((temp + j) <= total): 
            temp += j 
            count += 1
            j += 1 
        i += 1
    if (n != 1): 
        count += 1 
    return count
# ---
def __init__(self, value: int):
        self.value = value
        self.call_count = 0
# ---
def Split(list): 
    od_li = [] 
    for i in list: 
        if (i % 2 != 0): 
            od_li.append(i)  
    return od_li
# ---
def compute(model, input_ids):
            return model(input_ids, attn_mask=attn_mask)
# ---
def build_controller_image(
    tag: str,
    push: bool,
    dockerfile: str | None,
    context: str | None,
    platform: str,
    region: tuple[str, ...],
    project: str,
):
    """Build Docker image for Iris controller."""
    _build_image("controller", tag, push, dockerfile, context, platform, region, project)
# ---
def vm_ops(self) -> PlatformOps: ...
# ---
def display(self):
        """The display this window belongs to.  Read-only.

        :type: `Display`
        """
        return self._display
# ---
import math 
def sum_series(number):
 total = 0
 total = math.pow((number * (number + 1)) /2, 2)
 return total
# ---
def mesh_axis_specs(self) -> List[str]:
        """Materialized mesh axis names; validates mesh config."""
        ici, dcn = self.mesh.axis_shapes(jax.device_count(), self.num_slices)
        return list(ici.keys() | dcn.keys())
# ---
def __post_init__(self):
        assert self.num_heads % self.num_kv_heads == 0, "num_heads must be divisible by num_kv_heads"
        if self.head_dim is None:
            assert self.hidden_dim % self.num_heads == 0, "hidden_dim % num_heads must be 0 when head_dim=None"
# ---
def dependency_index_str(i: int) -> str:
    return f"DEP[{i}]"
# ---
def __len__(self):
        """Number of batches for this worker (same for all ranks)."""
        total_batches = len(self._inner)
        if self.drop_last:
            return total_batches // self.num_replicas
        else:
            return (total_batches + self.num_replicas - 1) // self.num_replicas
# ---
def test_read_dataset_streaming_with_columns(self, sample_data, tmpdir, ext, create_fn):
        """Test streaming reading with column selection for both formats"""
        input_file = os.path.join(tmpdir, f"test_input.{ext}")
        create_fn(sample_data, input_file)

        rows = list(read_dataset_streaming(input_file, columns=["id"]))

        assert len(rows) == len(sample_data)
        assert "id" in rows[0]
        assert "text" not in rows[0]
# ---
def stop(self, timeout: Duration = Duration.from_seconds(10.0)) -> None:
        pass
# ---
def downgrade(engine_name):
    globals()["downgrade_%s" % engine_name]()
# ---
def update_strs(slist):
      update_num(len(slist))
      for s in slist:
        update_str(s)
# ---
def refresh(self):
        self.metric.refresh()
        self.fill.refresh(self.metric.value)
# ---
def test_do_execute_no_params_wo_replace(self):
        self._test_do_execute_no_params(False)
# ---
def get_default_display(self):
        """Get the default display device.

        :deprecated: Use `pyglet.canvas.get_display`.

        :rtype: `Display`
        """
        return pyglet.canvas.get_display()
# ---
def build_pod_config(self) -> ResourceConfig:
        return ResourceConfig.with_tpu(self.tpu_type, slice_count=self.slice_count)
# ---
def get_block_header_hash_by_number(self, block_number) -> Optional[bytes]:
        with self.lock:
            return self._state.get_block_header_hash_by_number(block_number)
# ---
def to_option_list(self, ascommalist=''):
        '''return all options as a string
           if ascommalist is set to the name of a key, and
           the value of that key is a dict, format the dict
           as a list of comma delimited key=value pairs'''
        return self.stringify(ascommalist)
# ---
def sum_Pairs(arr,n): 
    sum = 0
    for i in range(n - 1,-1,-1): 
        sum += i*arr[i] - (n-1-i) * arr[i] 
    return sum
# ---
def __init__(self):
        # ast is CharClass or may be changed to PatternClass in one case
        self.ast = ast.CharClass()
# ---
def do_shard(x, y):
            x = hax.shard(x, resource_map)
            assert_inside_pjit(x, NamedSharding(mesh, PartitionSpec(None, ResourceAxis.DATA)))

            y = hax.shard(y, resource_map)
            assert_inside_pjit(y, NamedSharding(mesh, PartitionSpec(ResourceAxis.DATA, ResourceAxis.MODEL)))

            return x, y
# ---
def test_scan_not_0th_axis():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take(Width, 2)

    total, selected = hax.scan(scan_fun, Depth)(0.0, named1)

    assert jnp.all(jnp.isclose(total, jnp.sum(named1.array, axis=(0, 1, 2))))
    assert jnp.all(jnp.equal(selected.array, named1.take(Width, 2).rearrange(selected.axes).array))
# ---
def log10(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in log10")
    return elemwise(nxp.log10, x, dtype=x.dtype)
# ---
def get_perrin(n):
  if (n == 0):
    return 3
  if (n == 1):
    return 0
  if (n == 2):
    return 2 
  return get_perrin(n - 2) + get_perrin(n - 3)
# ---
def noprofit_noloss(actual_cost,sale_amount): 
  if(sale_amount == actual_cost):
    return True
  else:
    return False
# ---
def parse(expr, **kw):
    sm = PSM()
    sm.source = Source(expr)
    sm.starts_with(OpeningOfGroup(parent=None, initial=True))
    sm.pre_action = kw.get("pre_action", None)
    sm.post_action = kw.get("post_action", None)
    sm.parse()
    return sm.state.g.group
# ---
def _zeros_like(mapping, dtype, n):
    if isinstance(n, hax.NamedArray):
        return hax.shard(hax.zeros_like(n, dtype=dtype), mapping)
    elif is_jax_array_like(n):
        return jnp.zeros_like(n, dtype)
    else:
        assert jnp.isscalar(n)
        if dtype is None:
            # if it's a nan, we want to go to 0
            if n != n:
                return 0
            return n - n
        else:
            return jnp.zeros((), dtype=dtype)
# ---
def find_Min(arr,low,high): 
    while (low < high): 
        mid = low + (high - low) // 2;   
        if (arr[mid] == arr[high]): 
            high -= 1; 
        elif (arr[mid] > arr[high]): 
            low = mid + 1; 
        else: 
            high = mid; 
    return arr[high];
# ---
def sync_global_devices(name: str):
    """Creates a barrier across all hosts/devices."""
    h = np.uint32(zlib.crc32(name.encode()))
    assert_equal(h, f"sync_global_devices name mismatch ('{name}')")
# ---
def filter_resolved_atoms(structure: Structure) -> Structure:
    resolved_atom_indices = np.where(structure.atoms["is_present"])[0]
    return Structure.extract_atoms(structure, resolved_atom_indices)
# ---
def __getitem__(self, key):
        return self.array.__getitem__(key)
# ---
def __getattr__(self, method_name: str) -> ActorMethod: ...
# ---
def can_cast(from_, to, /):
    if isinstance(from_, CoreArray):
        from_ = from_.dtype
    return nxp.can_cast(from_, to)
# ---
def run_id(self) -> str:
        """Returns the run id"""
        assert self.config.id is not None
        return self.config.id
# ---
def __str__(self):
        return self.email
# ---
def coerce_depth_type(ndim, depth):
    for i in range(ndim):
        if isinstance(depth[i], tuple):
            depth[i] = tuple(int(d) for d in depth[i])
        else:
            depth[i] = int(depth[i])
    return depth
# ---
def register_preference_adapter(adapter: PreferenceTransformAdapter):
    preference_transform_templates[adapter.source] = adapter
# ---
def loss_function(model: LmHeadModel, example: LmExample, *, key=None):
        return model.compute_next_token_loss(example, key=key, logsumexp_weight=config.z_loss_weight)
# ---
def assertCalled(self, instance):
        disk_image_type = vm_utils.ImageType.DISK_VHD
        vm_ref = "blah"
        first_vdi_ref = "blah"
        vdis = ["blah"]

        self.called = False
        self.conn._vmops._attach_disks(instance, disk_image_type,
                                       vm_ref, first_vdi_ref, vdis)
        self.assertTrue(self.called)
# ---
def test_lambda_config_serialization(self):
    # Test serialization with output_shape and output_shape_type
    layer = keras.layers.Lambda(lambda x: x + 1, output_shape=(1, 1))
    layer(keras.backend.variable(np.ones((1, 1))))
    config = layer.get_config()
    layer = keras.layers.deserialize({
        'class_name': 'Lambda',
        'config': config
    })
    layer = keras.layers.Lambda.from_config(config)
# ---
def count_Rectangles(radius):  
    rectangles = 0 
    diameter = 2 * radius 
    diameterSquare = diameter * diameter 
    for a in range(1, 2 * radius):  
        for b in range(1, 2 * radius): 
            diagnalLengthSquare = (a * a +  b * b)  
            if (diagnalLengthSquare <= diameterSquare) : 
                rectangles += 1
    return rectangles
# ---
def test_evaluate_integer(self):
        expr = lit(42)
        assert expr.evaluate({}) == 42
# ---
def host(self):
        '''the hostname, but I like host better'''
        return self.hostname
# ---
def flashPersistenceToggled(self, checkbox):
        grid = self.get_widget('flashMessageDurationGrid')
        grid.set_sensitive(not checkbox.get_active())
        self.prefsDict["flashIsPersistent"] = checkbox.get_active()
# ---
def confirmPopup(title, msg, answerCallback):
    content = ConfirmPopup(text=msg)
    content.bind(on_answer=answerCallback)
    popup = Popup(title=title,
                    content=content,
                    size_hint=(None, None),
                    size=(dp(600),dp(200)),
                    auto_dismiss= False)
    popup.open()
    return popup
# ---
def shard_loss(x_in, w_in, y_in):
            loss = linear_softmax_cross_entropy_loss(
                x_in,
                y_in,
                w_in,
                reduction=None,
                implementation="mosaic_tpu",
            )
            local_sum = jnp.sum(loss)
            total_sum = jax.lax.psum(local_sum, "data")
            total_denom = jax.lax.psum(x_in.shape[0], "data")
            return total_sum / total_denom
# ---
def get_vm(self, vm_id: str) -> vm_pb2.VmInfo | None:
        """Get VM info by ID."""
        vm = self._vm_registry.get_vm(vm_id)
        return vm.info if vm else None
# ---
def RegisterWorker(self, request, context):
        """Worker management (workers send heartbeats to controller)"""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def folio(self):
        return self.__folio
# ---
def _get_current_step_save_interval(self, step):
        # binary search for the correct interval
        # we assume that the intervals are sorted by until
        current_policy = next(filter(lambda p: p.until is None or p.until >= step, self.step_policies), None)
        if current_policy is None:
            return None
        return current_policy.every
# ---
def observe_property(self, name, handler):
        self._property_handlers[name].append(handler)
        _mpv_observe_property(self._event_handle, hash(name)&0xffffffffffffffff, name.encode('utf-8'), MpvFormat.STRING)
# ---
def skip_if_checkpoint_not_accessible(path: str):
    def try_load_path(path):
        try:
            fs, path_to_open = _get_fs_and_plain_path(path)
            fs.open(path_to_open, "rb")
        except Exception:
            return False
        else:
            return True

    return pytest.mark.skipif(not try_load_path(path), reason="Checkpoint not accessible")
# ---
def test_virtual_offsets_indexing_fails():
    v_offsets = virtual_offsets((3,))
    with pytest.raises(NotImplementedError):
        v_offsets[0:2]

    v_offsets = virtual_offsets((3, 2))
    with pytest.raises(NotImplementedError):
        v_offsets[0:2, 1]
# ---
def client_with_autoscaler(service_with_autoscaler):
    """Dashboard test client with autoscaler enabled."""
    dashboard = ControllerDashboard(service_with_autoscaler)
    return TestClient(dashboard._app)
# ---
def __repr__(self):
        return f"WriteOp(type={self.writer_type}, pattern={self.output_pattern})"
# ---
def test_magic_index_props(self):
        """Index can look up properties on response object"""
        index = GlobalIndex.all("idx-name", DynamoKey("id"))
        index.response = {"FooBar": 2}
        self.assertEqual(index["FooBar"], 2)
        with self.assertRaises(KeyError):
            self.assertIsNotNone(index["Missing"])
# ---
def init(Vocab: Axis, config: LlamaConfig, *, key) -> "LlamaEmbedding":
        token_embeddings = hnn.Embedding.init(Vocab, config.Embed, key=key)
        norm = None
        if config.input_embedding_norm:
            norm = config.mk_LayerNorm(config.Embed)
        return LlamaEmbedding(token_embeddings, norm)
# ---
def test_start_not_inited(self):
        profiler.clean()
        profiler.start("name")
# ---
def return_sum(dict):
  sum = 0
  for i in dict.values():
    sum = sum + i
  return sum
# ---
def model_type(self):
        """Return our custom GMM-enabled model type"""
        return MixtralLMHeadModelGMM
# ---
def hello():
        print("Hello from validation job!")
        return 42
# ---
def shortname(self):
    # This is the only property provided from above
        return self.__shortname
# ---
def visitdir(self, dir):
        return False
# ---
def logaddexp(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.logaddexp](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.logaddexp.html)
    """
    return jnp.logaddexp(x1, x2)
# ---
def _bloom_hash(x: str) -> int:
    if isinstance(x, bytes):
        return int.from_bytes(hashlib.blake2b(x, digest_size=8).digest(), "big")
    return int.from_bytes(hashlib.blake2b(x.encode(), digest_size=8).digest(), "big")
# ---
def is_finite(self) -> bool:
        return True
# ---
def __call__(self, q: NamedArray, position_ids: NamedArray) -> NamedArray:
        raise NotImplementedError("This is an abstract base class for RotaryEmbeddings. Use a subclass instead.")
# ---
def div_of_nums(nums,m,n):
 result = list(filter(lambda x: (x % m == 0 or x % n == 0), nums)) 
 return result
# ---
def _chunk(x: NamedArray) -> NamedArray:
        return x.unflatten_axis(PosPad, (Chunks, C))
# ---
def test_float_array_padding():
    """Test padding behavior with float arrays."""
    ary = np.array([1.0, 2.0], dtype=np.float32)
    result = train_batch.trim_and_pad(ary, max_seq_len=4, pad_to=4, padding_value=999)

    expected = np.array([1.0, 2.0, 999.0, 999.0], dtype=np.float32)
    np.testing.assert_array_equal(result, expected)
# ---
def __post_init__(self):
        if self.update_rms_clipping is not None and self.update_rms_clipping <= 0:
            raise ValueError("update_rms_clipping must be a positive number or None.")

        if self.clip_update_norm is not None and self.update_rms_clipping is not None:
            raise ValueError("Cannot use both update_rms_clipping and clip_update_norm at the same time.")
# ---
def supports_fork(self) -> bool:
        return True
# ---
def multiply_list(items):
    tot = 1
    for x in items:
        tot *= x
    return tot
# ---
def visit_BinOp(self, node: ast.BinOp) -> ast.BinOp:
        node.op = self._maybe_swap(node.op, _ARITHMETIC_OPS)
        self.generic_visit(node)
        return node
# ---
def emphasis(self, text):
        """Rendering *emphasis* text.

        :param text: text content for emphasis.
        """
        return '<em>%s</em>' % text
# ---
def amax(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """
    Aliax for max. See max for details.
    """
    return wrap_reduction_call(jnp.amax, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def idle_count(self) -> int:
        return sum(1 for w in self._workers.values() if w.status == WorkerStatus.IDLE)
# ---
def close(self):
        """Close any files linked to this dataset
        """
        if self._file_obj is not None:
            self._file_obj.close()
        self._file_obj = None
# ---
def init(config: ApertusConfig, *, key) -> "ApertusTransformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, ApertusDecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return ApertusTransformer(config, layers, ln_f)
# ---
def __str__(self):
        return f"IdentityMap({list(str(x) for x in self._data.values())})"
# ---
def update(self, n=1, **kwargs):
        for k, v in kwargs.items():
            if v is None:
                continue
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v, n=n)
# ---
def metadata(self) -> Dict[str, Any]:
        """Any metadata that changes the behavior of this processor."""
        raise NotImplementedError
# ---
def _send_payload(self, payload):
        assert jax.process_index() == 0
        out = broadcast_shard(payload, hax.partitioning.infer_resource_partitions(payload))
        return out
# ---
def test_and_one_false(self):
        expr = (col("a") > 0) & (col("b") > 0)
        assert expr.evaluate({"a": 1, "b": -1}) is False
        assert expr.evaluate({"a": -1, "b": 1}) is False
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetWeekdayRange(0), (4, 11))
    self.assertEqual(self.schedule.GetWeekdayRange(1), (12, 19))
    self.assertEqual(self.schedule.GetWeekdayRange(2), (20, 27))
    self.assertEqual(self.schedule.GetWeekdayRange(3), (28, 37))
    self.assertEqual(self.schedule.GetWeekdayRange(4), (38, 47))
    self.assertEqual(self.schedule.GetWeekdayRange(5), (48, 57))
# ---
def __init__(self,
                 namespace,
                 kubeconfig='/etc/origin/master/admin.kubeconfig',
                 verbose=False,
                 all_namespaces=False):
        ''' Constructor for OpenshiftCLI '''
        self.namespace = namespace
        self.verbose = verbose
        self.kubeconfig = Utils.create_tmpfile_copy(kubeconfig)
        self.all_namespaces = all_namespaces
        self.oc_binary = locate_oc_binary()
# ---
def has_cupy() -> bool:
    return importlib.util.find_spec("cupy") is not None
# ---
def intermediate(self, x):
            return x + 2 * self.w
# ---
def strip_region_tags(sample_text):
    """Remove blank lines and region tags from sample text"""
    magic_lines = [
        line for line in sample_text.split("\n") if len(line) > 0 and "# [" not in line
    ]
    return "\n".join(magic_lines)
# ---
def get_stack(self, start_index, stack_size):
        end_index = len(self.examplers) - stack_size
        if end_index < 0:
            stack = list(self.examplers) + [self.examplers[-1] for _ in range(-end_index)]
        else:
            start_index = min(start_index, end_index)
            stack = [self.examplers[i + start_index] for i in range(stack_size)]
        return np.stack(stack, axis=-1)
# ---
def calculate_sharpe(self):
        """
        http://en.wikipedia.org/wiki/Sharpe_ratio
        """
        return sharpe_ratio(self.algorithm_volatility,
                            self.algorithm_period_returns,
                            self.treasury_period_return)
# ---
def fn(x):
        return x + 1
# ---
def __init__(self):
        self.vocab_size = len(self.TOKENS)
        self.TOKENS.sort(key=len, reverse=True)  # Sort by length for greedy matching
        self.pad_token_id = self.TOKENS.index("<pad>")
        self.eos_token = "</s>"
        self.bos_token = "<s>"
# ---
def height(self):
        with self.lock:
            if not self._last_block:
                return -1
            return self._last_block.block_number
# ---
def copy_page(self, src_page: int, dst_page: int) -> "ListCache[PageCacheT]":
        return ListCache(tuple(cache.copy_page(src_page, dst_page) for cache in self.caches))
# ---
def _setup_graph(self):
        NR_PROC = min(multiprocessing.cpu_count() // 2, 20)
        self.pred_funcs = [self.trainer.get_predictor(
            self.input_names, self.output_names)] * NR_PROC
# ---
def fake_add_to_aggregate(context, aggregate, host):
            fake_add_to_aggregate.called = True
# ---
def test_conf_inline_ratelimiting(self):
        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):
            app = memcache.MemcacheMiddleware(
                FakeApp(),
                {'error_suppression_limit': '5',
                 'error_suppression_interval': '2.5'})
        self.assertEqual(app.memcache._error_limit_count, 5)
        self.assertEqual(app.memcache._error_limit_time, 2.5)
        self.assertEqual(app.memcache._error_limit_duration, 2.5)
# ---
def __init__(self):

        player.__init__(self)
        self.experiences = deque()
# ---
def test_parse_to_value_exception(self):
        text = 'not important'

        with mock.patch.object(yaml, 'load') as yaml_loader:
            yaml_loader.side_effect = self.raised_exception

            self.assertRaises(ValueError,
                              template_format.parse, text)
# ---
def image_url(self, pixel_size=None):
        """
        Get the URL for the user icon in the desired pixel size, if it exists. If no
        size is supplied, give the URL for the full-size image.
        """
        if "profile" not in self._raw:
            return
        profile = self._raw["profile"]
        if (pixel_size):
            img_key = "image_%s" % pixel_size
            if img_key in profile:
                return profile[img_key]
        return profile[self._DEFAULT_IMAGE_KEY]
# ---
def wrap(*a, **kw):
                offset = int(a[-1])  # convert from 0-d array
                block_id = offset_to_block_id(offset, numblocks)
                return func(*a[:-1], block_id=block_id, **kw)
# ---
def release_slot(self, slot_id: int) -> "DecodeState":
        sequences = self.sequences.release_slot(slot_id)
        return dataclasses.replace(self, sequences=sequences)
# ---
def _serialize_and_commit(self, cache_dir):
        path = os.path.join(cache_dir, LEDGER_FILE_NAME)
        return _serialize_json_and_commit(path, self)
# ---
def __init__(self, design_dir: str, designfolding: bool = False) -> None:
        super().__init__(write_interval="batch")
        self.designfolding = designfolding
        if design_dir is not None:
            self.init_outdir(design_dir)
# ---
def upload(request):
	#
	return render(request, 'centres/upload.html')
# ---
def schedule(step):
            tokens_trained = step * self.batch_size * self.seq_length
            return jnp.minimum(ctx.learning_rate, self.batch_size * self.a * tokens_trained**self.b)
# ---
def __init__(self, crash_point: int):
            self.crash_point = crash_point
# ---
def __post_init__(self):
            if self.kwargs is None:
                self.kwargs = {}
# ---
def hypot(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.hypot](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.hypot.html)
    """
    return jnp.hypot(x1, x2)
# ---
def schemes(self):
        '''the scheme, split by plus signs'''
        return self.scheme.split('+')
# ---
def full_name(self):
        fn = self.first_name
        ln = self.last_name
        if fn and ln:
            name = u"%s %s" % (fn, ln)
        else:
            name = str(self.email)
        return name
# ---
def visitdir(self, dir):
        return "all"
# ---
def tiny_corpus_config(path):
    _write_tiny_corpus(path)
    component = DatasetComponent(
        source=UrlDatasetSourceConfig(
            train_urls=[f"file://{path}/train/docs.jsonl"],
            validation_urls=[f"file://{path}/validation/docs.jsonl"],
        ),
        cache_dir=f"{path}/cache",
    )
    return LmDataConfig(components={"tiny": component})
# ---
def loop():
    return asyncio.get_event_loop()
# ---
def reset(self) -> None:
        self._attempt = 0
# ---
def __init__(self):
                self.array = hax.ones((Dim1, Dim2))
# ---
def _workflow_trigger(self, cr, uid, ids, trigger, context=None):
        #override in order to trigger the workflow of stock.picking at the end of create, write and unlink operation
        #instead of it's own workflow (which is not existing)
        return self.pool.get('stock.picking')._workflow_trigger(cr, uid, ids, trigger, context=context)
# ---
def __repr__(self):
        return '{0.__class__.__name__}({0.slot!r})'.format(self)
# ---
def ready_slice_count(self) -> int:
        """Count of VM groups where all VMs are ready."""
        with self._vm_groups_lock:
            snapshot = list(self._vm_groups.values())
        return sum(1 for g in snapshot if g.status().all_ready)
# ---
def testReturn(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          return 'bar'
        print foo()""")))
# ---
def key_str_all(self):
        '''
        Return all managed key strings
        '''
        ret = {}
        for status, keys in six.iteritems(self.list_keys()):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                path = os.path.join(self.opts['pki_dir'], status, key)
                with salt.utils.fopen(path, 'r') as fp_:
                    ret[status][key] = fp_.read()
        return ret
# ---
def is_being_preempted(self) -> bool:
        """
        Check if the TPU slice is being preempted.
        This is a workaround for the fact that Ray doesn't expose this information directly.
        """
        return get_current_tpu_is_preempted()
# ---
def from_batch(items: Sequence[np.ndarray], item_rank: Optional[int] = None) -> "PreparedBatch":
        data, offsets, shapes = _prepare_batch(items, item_rank)
        return PreparedBatch(data, offsets, shapes)
# ---
def test_single_batched_selector():
    B, S, V = Axis("batch", 4), Axis("seq", 3), Axis("vocab", 7)
    x = hax.arange((B, S, V))
    idx = hax.arange((B, S), dtype=jnp.int32) % V.size
    out = x["vocab", idx]
    assert out.axes == (B, S)
    assert jnp.array_equal(out.array, _ref_gather(x, V, idx))
# ---
def get_metrics(self) -> WeightTransferServerMetrics:
        """Get transfer metrics."""
        return self.metrics
# ---
def model_type(cls) -> type["MixtralLMHeadModel"]:
        return MixtralLMHeadModel
# ---
def max(self, axis: AxisSelection | None = None, *, where=None) -> "NamedArray":  # pragma: no cover
        return haliax.max(self, axis=axis, where=where)
# ---
def test_decode_source_skips_specials(tok):
    ids = [tok.sos_token_id, tok.encode_char("h"), tok.encode_char("i"), tok.eos_token_id]
    assert tok.decode_source(ids) == "hi"
# ---
def chronos(monkeypatch):
    """Virtual time fixture - makes time.sleep() controllable for fast tests."""
    clock = VirtualClock()

    # Patch time module
    monkeypatch.setattr(time, "time", clock.time)
    monkeypatch.setattr(time, "monotonic", clock.time)
    monkeypatch.setattr(time, "sleep", clock.sleep)

    return clock
# ---
def test_patch_from__half_extent():
    # Half the extent should give half the patch size
    patch_h, patch_w = patch_from(
        patch_extent=(90.0, 180.0), input_height=4, input_width=8
    )
    assert patch_h == 2
    assert patch_w == 4
# ---
def error(status, message=""):
    if message:
        data = json.dumps(dict(message=message))
    else:
        data=""
    return make_response(data, status)
# ---
def chip_count(self) -> int:
        """CPU has no accelerator chips."""
        return 0
# ---
def large_product(nums1, nums2, N):
    result = sorted([x*y for x in nums1 for y in nums2], reverse=True)[:N]
    return result
# ---
def stop(self):
        self.running = False
# ---
def cached_fn_impl(leaves, treedef):
        user_fn_names, args, kwargs = jtu.tree_unflatten(treedef, leaves)
        return cacheable_fn(user_fn_names, *args, **kwargs)
# ---
def groups(self) -> dict[str, ScalingGroup]:
        """All scale groups."""
        return self._groups
# ---
def pre_jit(x):
        if is_source:
            inp = x
        else:
            inp = np.zeros_like(x)
        inp = np.expand_dims(inp, axis=0)
        return host_local_array_to_global_array(inp, global_mesh, pspec)
# ---
def test_impl(df):
            B = df.A.str.split(',')
            return B.str.get(1)
# ---
def mT(self):
        from cubed.array_api.linear_algebra_functions import matrix_transpose

        return matrix_transpose(self)
# ---
def delete_element():
    """Deletes a single element with given hid"""
    element = request.get_json()
    home_services.delete_element(element['hid'])
    return 'OK'
# ---
def __init__(self, *args, **kwargs):
		self.stateName = kwargs["stateName"]
		self.root = args[0]
		self.id = kwargs["id"]
		Frame.__init__(self, self.root.mainWindow)
		self.config(
			background="gold"
		)
		self.place(relwidth=1, relheight=1)
# ---
def run(self, command, *args):
        self.command('run', command, *args)
# ---
def gh_pages(c):
    """Publish to GitHub Pages"""
    preview(c)
    c.run('ghp-import -b {github_pages_branch} '
          '-m {commit_message} '
          '{deploy_path} -p'.format(**CONFIG))
# ---
def test_join_workers():
    producers = [Mock()]
    cworker = Mock()
    consumers = [Mock()]

    classify_documents._join_workers(cworker, producers, consumers)

    for p in producers:
        assert p.join.called
    assert cworker.set_producers_done.called
    for c in consumers:
        assert c.join.called
    assert cworker.clear_producers_done.called
# ---
def selected_target_account_index(self):
        return self._selected_target_index
# ---
def _stop_actor(actor: ActorHandle) -> None:
    try:
        ray.get(actor.teardown.remote(), timeout=TEARDOWN_ACTOR_TIMEOUT)
        ray.get(actor.__ray_terminate__.remote(), timeout=TERMINATE_ACTOR_TIMEOUT)
    except (ActorDiedError, ActorUnavailableError):
        pass
    except GetTimeoutError as e:
        logger.warning(
            f"Failed to gracefully shut down actor in {TERMINATE_ACTOR_TIMEOUT} seconds; killing it instead: {e}"
        )
    finally:
        ray.kill(actor)
# ---
def test_shard_cache_crashes_if_processor_throws():
    class ThrowingProcessor(SimpleProcessor):
        def __call__(self, batch: Sequence[Sequence[int]]):
            raise RuntimeError("exc")

    with tempfile.TemporaryDirectory(ignore_cleanup_errors=True) as tmpdir:
        with pytest.raises(RuntimeError):
            build_or_load_cache(tmpdir, SimpleShardSource(), ThrowingProcessor())
# ---
def compute(a, b):
            result = a + b
            print(f"{a} + {b} = {result}")
            return result
# ---
def __init__(self, dim=50):
        super().__init__()
        center = torch.randn((dim, 3))
        std = torch.rand((dim))
        self.dim = dim
        self.register_buffer("center", center)
        self.register_buffer("std", std)
# ---
def __str__(self):
        """Return a string representation of this check."""

        return "@"
# ---
def __init__(self, hidden_size, frequency_embedding_size=256):
    super().__init__()
    self.mlp = nn.Sequential(
      nn.Linear(frequency_embedding_size, hidden_size, bias=True),
      nn.SiLU(),
      nn.Linear(hidden_size, hidden_size, bias=True))
    self.frequency_embedding_size = frequency_embedding_size
# ---
def __init__(self):
        self._min_known_len = 0
# ---
def _to_job_id_str(self, job_or_id) -> str:
        """Convert Job object or string to job_id string."""
        if isinstance(job_or_id, str):
            return (
                JobName.from_string(job_or_id).to_wire()
                if job_or_id.startswith("/")
                else JobName.root(job_or_id).to_wire()
            )
        # Assume it's a Job object
        return str(job_or_id.job_id)
# ---
def mock_apply_blockwise(*args, **kwargs):
    # Raise an error on every 3rd call
    global mock_call_counter
    mock_call_counter += 1
    if mock_call_counter % 3 == 0:
        raise IOError("Test fault injection")
    return apply_blockwise(*args, **kwargs)
# ---
def test_ckpt_path_with_input_name():
    input_name = make_input_name("checkpoints/step-555000", "myrun")
    assert ckpt_path_to_step_name(input_name) == "myrun-555000"
# ---
def _init_wandb(config: DedupConfig):
    """Initialize wandb for deduplication tracking."""
    init_wandb(
        run_name=f"{config.mode}",
        tags=[str(config.mode)],
        config={
            "mode": str(config.mode),
            "input_path": config.input_paths,
            "processes": config.processes,
        },
    )
# ---
def display(self):
        image(self.im, self.x, self.y)
# ---
def sqrts(x):
        yield np.sqrt(x)
        yield -np.sqrt(x)
# ---
def setUp(self):
        super(CsrfTests, self).setUp()
        # The CSRF library uses the time, so we mock it out.
        self.time_mock = mock.Mock()
        csrf.time = self.time_mock
        self.time_mock.time = mock.Mock(return_value=MOCKED_TIME)
        # The handler tests need a WSGIApplication.
        app = webapp2.WSGIApplication([('/', self.TestHandler)])
        self.testapp = webtest.TestApp(app)
# ---
def __repr__(self) -> str:
        return f"Timer(elapsed={self.elapsed_seconds():.3f}s)"
# ---
def count_X(tup, x): 
    count = 0
    for ele in tup: 
        if (ele == x): 
            count = count + 1
    return count
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
            # parse the shard name to get the shard number
            shard_num = int(shard_name.split("_")[1])
            for i in range(10):
                if i == self.crash_point:
                    raise _CustomException(f"Crashing at {shard_num} {i} {self.crash_point}")
                if i >= row:
                    yield [shard_num * 10 + i] * 10
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {
            "transformer": "model",
            "embeddings": "model",
            "lm_head": "lm_head",
        }
# ---
def __call__(self, x, *, key):  # pragma: no cover - unused in this test
            return x + self.array + self.static
# ---
def __init__(
        self,
        mol_dir: Path | str,
    ) -> None:
        self.mol_dir = Path(mol_dir)
        self._struct_cache: dict[tuple[Path, bool], Structure] = {}
        self._once_keys: set[str] = set()
# ---
def test_is_local_path():
    assert is_local_path("relative_path/path")
    assert is_local_path("/absolute_path/path")
    assert is_local_path("file:relative_path/path")
    assert is_local_path("file://absolute_path/path")
    assert is_local_path("file:///absolute_path/path")
    assert not is_local_path("s3://host/path")
    assert not is_local_path("gs://host/path")
# ---
def after_insert(self):
		'''set opening stock and item price'''
		if self.standard_rate:
			for default in self.item_defaults:
				self.add_price(default.default_price_list)

		if self.opening_stock:
			self.set_opening_stock()
# ---
def set_input_embeddings(self, value):
        self.bimamba.backbone.embeddings.word_embeddings = value
# ---
def get_ls_l_desc_fields():
    return {
        'id': True,
        'class': True,
        'folder': True,
        'length': True,
        'modified': True,
        'name': True,
        'project': True,
        'size': True,
        'state': True
    }
# ---
def test_capture_early_option_parsing(testdir):
    testdir.makeconftest("""
        def pytest_runtest_setup():
            print ("hello19")
    """)
    testdir.makepyfile("def test_func(): pass")
    result = testdir.runpytest("-vs")
    assert result.ret == 0
    assert 'hello19' in result.stdout.str()
# ---
def logaddexp(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "logaddexp")
    if x1.dtype not in _real_floating_dtypes or x2.dtype not in _real_floating_dtypes:
        raise TypeError("Only real floating-point dtypes are allowed in logaddexp")
    return elemwise(nxp.logaddexp, x1, x2, dtype=result_type(x1, x2))
# ---
def binomial_Coeff(n,k): 
    C = [0] * (k + 1); 
    C[0] = 1; # nC0 is 1 
    for i in range(1,n + 1):  
        for j in range(min(i, k),0,-1): 
            C[j] = C[j] + C[j - 1]; 
    return C[k]; 
def sum_Of_product(n): 
    return binomial_Coeff(2 * n,n - 1);
# ---
def test_flip_multiple_axes(tmp_path, spec, executor):
    # Note 'a' has one fewer element in both axes to force chunking to cross array boundaries
    a = cubed.random.random(
        (9999, 9999), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.flip(a)
    run_operation(tmp_path, executor, "flip_multiple_axes", b)
# ---
def vm_manager(
        self,
        group_config: config_pb2.ScaleGroupConfig,
        vm_factory: TrackedVmFactory,
        *,
        dry_run: bool = False,
    ) -> VmManagerProtocol:
        raise NotImplementedError("Local platform uses LocalController autoscaler")
# ---
def decorator(fn):
            return pytest.mark.skipif("CI" in os.environ, reason=fn_or_msg)(fn)
# ---
def _register_service(
    name: str,
    full_name: str,
    client_class: type,
) -> None:
    """Register a service in the global registry."""
    methods = _discover_methods_from_client(client_class)
    SERVICES[name] = ServiceInfo(
        name=name,
        full_name=full_name,
        client_class=client_class,
        methods=methods,
    )
# ---
def _process_link(self, m, link, title=None):
        line = m.group(0)
        text = m.group(1)
        if line[0] == '!':
            return self.renderer.image(link, title, text)

        self._in_link = True
        text = self.output(text)
        self._in_link = False
        return self.renderer.link(link, title, text)
# ---
def vm_count(self) -> int:
        return get_tpu_topology(self.variant).vm_count
# ---
def get_logs(self, container_id: str, since: Timestamp | None = None) -> list[LogLine]:
        c = self._containers.get(container_id)
        if not c:
            return []
        if since:
            since_dt = datetime.fromtimestamp(since.epoch_seconds(), tz=timezone.utc)
            return [log for log in c._logs if log.timestamp > since_dt]
        return c._logs
# ---
def cli(local: bool, rounds: int, delay: float, rounds_positional: int | None):
    """Token-passing actor test."""
    # Positional arg takes precedence (for job submission compatibility)
    actual_rounds = rounds_positional if rounds_positional is not None else rounds

    if local:
        success = run_local(actual_rounds, delay)
        raise SystemExit(0 if success else 1)
    else:
        # When run as a job, main() will use iris_ctx() for context
        main(actual_rounds, delay)
# ---
def multi_worker_cluster(use_docker, docker_cleanup_scope):
    """Provide a cluster with multiple workers."""
    with E2ECluster(num_workers=3, use_docker=use_docker) as cluster:
        yield cluster
# ---
def scan(self, init: T, *extra_args, unroll: int | bool | None = None, **extra_kwargs): ...
# ---
def _terminate_process(process: subprocess.Popen) -> None:
    try:
        if sys.platform == "win32":
            process.kill()
        else:
            os.killpg(process.pid, signal.SIGKILL)
    except Exception as e:
        logger.info(f"Failed to terminate process {process.pid} -- already terminated? {e}")
# ---
def __init__(self, proc1, proc2):
        self._procs = (proc1, proc2)
        self._stdout = proc2.stdout
# ---
def delete_floatingip_postcommit(self, context, fip_context):
        pass
# ---
def body(i, ref_counts):
            def dec(rc):
                page = seq_pages["page", i].scalar()
                return rc.at["page", page].add(-1)

            return jax.lax.cond(is_valid_page["page", i].scalar(), dec, lambda x: x, ref_counts)
# ---
def should_skip(name):
        if name.startswith('.') and name not in ('.github', '.claude'):
            return True
        return name in SKIP_DIRS or name.endswith('.egg-info')
# ---
def __ge__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.greater_equal(self, other)
# ---
def test_area_weighted_mean_bias(self):
        """Verify bias is correctly computed as mean(gen - target)."""
        target = torch.zeros(1, 4, 4)
        gen = torch.ones(1, 4, 4) * 2  # gen - target = 2
        weights = torch.ones(4, 4) / 16

        bias = area_weighted_mean_bias(target, gen, weights)

        assert torch.allclose(bias, torch.tensor([2.0]))
# ---
def blocks(self):
        """An array-like interface to the blocks of an array."""
        from cubed.core.indexing import BlockView

        return BlockView(self)
# ---
def test_valid_position_mask_invalid_source(tok):
    mask = tok.valid_position_mask("not valid{{{")
    assert not any(mask)
# ---
def test_plain_desc(self):
        table = self.tables.some_table
        lx = table.c.x.label("lx")
        self._assert_result(
            select([lx]).order_by(lx.desc()), [(3,), (2,), (1,)]
        )
# ---
def flatten_dims(tensor: torch.Tensor) -> torch.Tensor:
            return rearrange(
                tensor, "batch time variable lat lon -> batch (time variable) lat lon"
            )
# ---
def unflatten(treedef: Any, leaves: Iterable[Any]) -> Any:
    """Alias for :func:`haliax.tree_util.tree_unflatten` matching :func:`jax.tree.unflatten`."""

    return tree_util.tree_unflatten(treedef, leaves)
# ---
def local_rope(self) -> RotaryEmbeddingsConfig:
        """Local RoPE config used for Gemma-3's alternating local attention."""
        return dataclasses.replace(self.rope, theta=self.rope_local_base_freq)
# ---
def _make(name: str = "test-job") -> cluster_pb2.Controller.LaunchJobRequest:
        return cluster_pb2.Controller.LaunchJobRequest(
            name=name,
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=1024**3),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )
# ---
def embed(self, input_ids, *args):
        input_embeds = self.token_embeddings(input_ids)
        if self.norm is not None:
            input_embeds = self.norm(input_embeds)
        return input_embeds
# ---
def broadcast_to(self, axes: AxisSpec) -> "NamedArray":  # pragma: no cover
        return haliax.broadcast_to(self, axes=axes)
# ---
def convert_issue_to_xml(issue):
    return f"""<issue>
        <number>{issue.number}</number>
        <title>{issue.title}</title>
        <state>{issue.state}</state>
        <created_at>{issue.created_at}</created_at>
        <updated_at>{issue.updated_at}</updated_at>
        <labels>{','.join([label.name for label in issue.labels])}</labels>
        <url>{issue.html_url}</url>
        <body>\n{issue.body}</body>
        <comments>{issue.comments}</comments>
    </issue>\n"""
# ---
def job_id(self) -> str: ...
# ---
def _quick():
    return 1
# ---

def extract_min_max(test_tup, K):
  res = []
  test_tup = list(test_tup)
  temp = sorted(test_tup)
  for idx, val in enumerate(temp):
    if idx < K or idx >= len(temp) - K:
      res.append(val)
  res = tuple(res)
  return (res)
# ---
def read_jsonl_to_list(url):
      response = requests.get(url, stream=True)
      data_list = []

      # Process each line in the response content
      for line in response.iter_lines(decode_unicode=True):
        if line:
          data = json.loads(line)
          data_list.append(data)

      return data_list
# ---
def __xor__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_xor(self, other)
# ---
def __eq__(self, other):
        """
        Returns true if both objects are equal
        """
        if not isinstance(other, ContributorOrcid):
            return False

        return self.__dict__ == other.__dict__
# ---
def _setup_digits(self, env):
        """ Setup the digits for ``self`` and its corresponding column """
        pass
# ---
def __init__(self):
        """
        Construct empty object.
        """
        self.num_lines = 0
        self.remaining_lines = MAX_NUM_LINES
        self.lines = []
# ---
def init_sanity_tqdm(self) -> None:
        """Initialize the sanity check progress bar."""
        bar = super().init_sanity_tqdm()
        return self._update_bar_description(bar)
# ---
def _seq_params_from_work(work: PrefillWork, idx: int) -> SeqDecodingParams:
    def select(x):
        if isinstance(x, NamedArray):
            return x["seq", idx]
        elif is_jax_array_like(x):
            return x[idx]
        else:
            raise TypeError(f"Unexpected type in seq_params: {type(x)}")

    return hax.tree_util.tree_map(select, work.seq_params)
# ---
def mk_LayerNorm(self, axis: AxisSpec) -> hnn.RmsNorm:
        return self.norm_config.build(axis)
# ---
def visit_FunctionDef(self, node: ast.FunctionDef) -> ast.FunctionDef:
        if node.name in self.mapping:
            node = ast.FunctionDef(
                name=self.mapping[node.name],
                args=node.args,
                body=node.body,
                decorator_list=node.decorator_list,
                returns=node.returns,
                type_comment=node.type_comment,
            )
            ast.copy_location(node, node)
        self.generic_visit(node)
        return node
# ---
def test_rearrange_with_ellipsis():
    assert einops_rearrange(z, "... w c -> ... c w").axes == (B, D, H, C, W)
    assert einops_rearrange(z, "b d ... -> d ... b").axes == (D, H, W, C, B)

    assert einops_rearrange(z, "b ... c -> b c ...").axes == (B, C, D, H, W)
    # make sure the values are right too
    z_t = z.array.transpose((0, 4, 1, 2, 3))
    assert (einops_rearrange(z, "b ... c -> b c ...").array == z_t).all()
# ---
def isfinite(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in isfinite")
    return elemwise(nxp.isfinite, x, dtype=nxp.bool)
# ---
def create_tmpfile(prefix='tmp'):
        ''' Generates and returns a temporary file name '''

        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False) as tmp:
            return tmp.name
# ---
def set_icon(self, *images):
        """Set the window icon.

        If multiple images are provided, one with an appropriate size 
        will be selected (if the correct size is not provided, the image
        will be scaled).

        Useful sizes to provide are 16x16, 32x32, 64x64 (Mac only) and
        128x128 (Mac only).

        :Parameters:
            `images` : sequence of `pyglet.image.AbstractImage`
                List of images to use for the window icon.

        """
        pass
# ---
def unschedule_router_precommit(self, context, router_context):
        pass
# ---
def test_forward_output_shape(params, tiny_cfg):
    batch_size, seq_len = 2, 16
    token_ids = jax.random.randint(jax.random.PRNGKey(1), (batch_size, seq_len), 1, 100)

    logits = forward(params, token_ids, tiny_cfg)
    assert logits.shape == (batch_size, seq_len, tiny_cfg.vocab_size)
# ---
def parse_typespec(thing):
    if isinstance(thing, basestring):
        return thing
    elif '$and' in thing:
        return '(' + ' AND '.join(map(parse_typespec, thing['$and'])) + ')'
    elif '$or' in thing:
        return '(' + ' OR '.join(map(parse_typespec, thing['$or'])) + ')'
    else:
        return 'Type spec could not be parsed'
# ---
def Image(self, data, *args, **kwargs):
        if isinstance(data, np.ndarray):
            data = scale_image(data)
        return wandb.Image(data, *args, **kwargs)
# ---
def enterOff(self):
        pass
# ---
def flatten_to_device(means_or_stds: xr.Dataset) -> torch.Tensor:
            if "lev" in means_or_stds.dims:
                array = conditional_rearrange(
                    means_or_stds,
                    "(variable lev)=var",
                    concat_dim="var",
                ).rename({"var": "variable"})
            else:
                array = means_or_stds.to_dataarray()
            return torch.from_numpy(array.to_numpy().flatten()).to(self.device)
# ---
def start_tls(self, read_server_info=True):
        self.start_tls_called = True
# ---
def test_make_choices_only_one():
    probs = pd.DataFrame(
        [[1, 0, 0], [0, 1, 0]], columns=['a', 'b', 'c'], index=['x', 'y'])
    choices = mnl.make_choices(probs)

    pdt.assert_series_equal(
        choices,
        pd.Series([0, 1], index=['x', 'y']))
# ---
def mod(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.mod(x1, x2)
# ---
def render_timestamp(timestamp):
    return datetime.datetime.fromtimestamp(timestamp//1000).ctime()
# ---
def loadPics(self):
        self.im = loadImage("block.png")
# ---
def __setstate__(self, state):
        self.__init__(**state)
# ---
def capabilities(self):
        return {
            "boolean indexing": False,  # not supported in Cubed (#73)
            "data-dependent shapes": False,  # not supported in Cubed
            "max dimensions": nxp.__array_namespace_info__().capabilities()[
                "max dimensions"
            ],
        }
# ---
def _rebuild_named_array(like, array):
        if is_named_array(array):
            return array

        if is_named_array(like):
            return hax.NamedArray(array, like.axes)
        else:
            return array
# ---
def on_line(line: str) -> None:
            logger.info("[%s] %s", host, line)
# ---
def __init__(self, verb, description):
        """ Initialize EventType.

        :verb: Scim verb
        :description: HR description text
        """
        self.verb = verb
        self.description = description
# ---
def func(self, *args, **kwargs):
            if self.stopped():
                raise RuntimeError("stopped!")
            return self._func(*args, **kwargs)
# ---
def __len__(self) -> int:
        """Get the length of the dataset.

        Returns
        -------
        int
            The length of the dataset.

        """
        return len(self.dataset.target_ids) * self.dataset.multiplicity
# ---
def assert_array_equal(a, b, **kwargs):
    a = to_numpy(a)
    b = to_numpy(b)
    npt.assert_array_equal(a, b, **kwargs)
# ---
def estimate_memory_bytes(self, candidate: CandidateConfig) -> int:
        """Estimate memory usage in bytes for training a candidate configuration."""
        ...
# ---
def digit_distance_nums(n1, n2):
         return sum(map(int,str(abs(n1-n2))))
# ---
def _unnormalize_fts_input(
        self, fts: Prognostic, fts_boundary: Boundary
    ) -> tuple[Prognostic, Boundary]:
        # Corrector is run in float64 to avoid precision loss
        fts = self._unnormalize_fts_prognostic(fts)
        fts_boundary = fts_boundary.to(torch.float64)
        fts_boundary = self.normalize.unnormalize_tensor_boundary(
            fts_boundary, fill_value=0.0
        )

        return fts, fts_boundary
# ---
def create_test_entrypoint():
    """Create a simple test entrypoint."""
    from dataclasses import dataclass

    @dataclass
    class TestEntrypoint:
        callable: object
        args: tuple = ()
        kwargs: dict | None = None

        def __post_init__(self):
            if self.kwargs is None:
                self.kwargs = {}

    def test_fn():
        print("Hello from test")

    return TestEntrypoint(callable=test_fn)
# ---
def expm1(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in expm1")
    return elemwise(nxp.expm1, x, dtype=x.dtype)
# ---
def increment_numerics(test_list, K):
  res = [str(int(ele) + K) if ele.isdigit() else ele for ele in test_list]
  return res
# ---
def test_context_wait(job_context):
    futures = [job_context.run(lambda x: x, i) for i in range(5)]
    ready, pending = job_context.wait(futures, num_returns=2)
    assert len(ready) == 2
    assert len(pending) == 3
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key=None, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids)
        return self.norm(x)
# ---
def test_equal(self):
        expr = col("score") == 100
        assert expr.evaluate({"score": 100}) is True
        assert expr.evaluate({"score": 50}) is False
# ---
def _trigger(self):
        t = time.time()
        mean, max = eval_with_funcs(
            self.pred_funcs, self.eval_episode, self.get_player_fn)
        t = time.time() - t
        if t > 10 * 60:  # eval takes too long
            self.eval_episode = int(self.eval_episode * 0.94)
        self.trainer.monitors.put_scalar('mean_score', mean)
        self.trainer.monitors.put_scalar('max_score', max)
# ---
def __init__(self,Occurrence):
        self.directory=Occurrence[0]
# ---
def __next__(self):
        return next(self.it)
# ---
def delete_previous_logs():
    cmd = 'rm -rf logs/*'
    cr.run_command(cmd)
# ---
def preemptible_constraint(preemptible: bool = True) -> Constraint:
    """Constraint requiring workers to be preemptible (or not)."""
    return Constraint(key=PREEMPTIBLE_ATTRIBUTE_KEY, op=ConstraintOp.EQ, value=str(preemptible).lower())
# ---
def forward(self, s: Tensor) -> Tensor:
        """Perform the forward pass.

        Parameters
        ----------
        s : Tensor
            The sequence embeddings

        Returns
        -------
        Tensor
            The predicted bfactor histogram.

        """
        return self.bfactor(s)
# ---
def contract_tuple(chunks, factor):
    """Return simple chunks tuple such that factor divides all elements

    Examples
    --------

    >>> contract_tuple((2, 2, 8, 4), 4)
    (4, 8, 4)
    """
    assert sum(chunks) % factor == 0

    out = []
    residual = 0
    for chunk in chunks:
        chunk += residual
        div = chunk // factor
        residual = chunk % factor
        good = factor * div
        if good:
            out.append(good)
    return tuple(out)
# ---
def num_rows(self):
        return len(self.offsets)
# ---
def get_job_status(self, job_id: JobName) -> cluster_pb2.JobStatus:
        request = cluster_pb2.Controller.GetJobStatusRequest(job_id=job_id.to_wire())
        response = self._client.get_job_status(request)
        return response.job
# ---
def _strip_sizes(axes: AxisSpec) -> AxisSelection:
    """Strip sizes from axes, returning only the names."""
    if isinstance(axes, hax.Axis):
        return axes.name
    return tuple(axis.name for axis in axes)
# ---
def open(self) -> zarr.Array:
        return open_if_lazy_zarr_array(self.array)
# ---
import re
def text_match_one(text):
        patterns = 'ab+?'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
import re
def text_starta_endb(text):
        patterns = 'a.*?b$'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def without_axes(axis_spec: AxisSelection, to_remove: AxisSelection, allow_mismatched_sizes=False) -> AxisSelection:  # type: ignore
    """As eliminate_axes, but does not raise if any axis in to_remove is not present in axis_spec"""
# ---
def __init__(
        self,
        pooling: int = 2,
    ):
        super().__init__()
        self.avgpool = torch.nn.AvgPool2d(pooling)
# ---
def is_inexact_arrayish(x):
    """
    Similar to [equinox.is_inexact_array][] but works on anything that has a shape and dtype
    and the dtype is inexact.

    Specifically, we want to work with [jax.ShapeDtypeStruct][]s, which are not arrays.
    """
    if hasattr(x, "shape") and hasattr(x, "dtype"):
        return jnp.issubdtype(x.dtype, jnp.inexact)
    else:
        return False
# ---
def initialize(config: TrainerConfig | AllConfig):
    """Initializes jax, logging, setting the run name/id in the process. Also initializes tracking and saves config
    as hyperparameters and an artifact"""
    if isinstance(config, TrainerConfig):
        trainer_config = config
    else:
        trainer_config = config.trainer

    trainer_config.initialize()
    levanter.tracker.log_configuration(config)
# ---
def match(self, item):
        return False
# ---
def main():
    """Main function to run the parallel Llama 75M speedrun."""
    steps = default_speedrun("parallel_llama_75m", speedrun_config)
    executor_main(steps=steps)
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Job Percentage", device_id)
# ---
def __init__(self, tree, path: str, mode: str):
        self.path = path
        self.mode = mode
        self.tree = tree
# ---
def test_create_snapshot(self):
        """Test snapshot can be created and deleted."""
        mox = self.mox
        drv = self._driver

        mox.StubOutWithMock(drv, '_clone_volume')
        drv._clone_volume(IgnoreArg(), IgnoreArg(), IgnoreArg())
        mox.ReplayAll()

        drv.create_snapshot(FakeSnapshot())

        mox.VerifyAll()
# ---
def __call__(self, batch: Sequence[Sequence[int]]):
            raise RuntimeError("exc")
# ---
def addExclusions(nonbondedforce, pairlist):
  """ add nonbonded exclusions between pairs """
  for i,j in pairlist:
    nonbondedforce.addExclusion(i,j)
# ---
import heapq
def cheap_items(items,n):
  cheap_items = heapq.nsmallest(n, items, key=lambda s: s['price'])
  return cheap_items
# ---
def make_tokenizer(tokenizer: str | PreTrainedTokenizer) -> PreTrainedTokenizer:
    if isinstance(tokenizer, str):
        return AutoTokenizer.from_pretrained(tokenizer)
    return tokenizer
# ---
def fray_cluster():
    set_current_cluster(create_cluster("local"))
    yield
    set_current_cluster(None)
# ---
def _populate_var_3d_idx(self):
        for kt in self.VAR_SET:
            self.VAR_3D_IDX[kt] = torch.tensor([])
            for i, k in enumerate(self.prognostic_var_names):
                if kt in k:
                    self.VAR_3D_IDX[kt] = torch.cat(
                        [self.VAR_3D_IDX[kt], torch.tensor([i])]
                    )
            self.VAR_3D_IDX[kt] = self.VAR_3D_IDX[kt].to(torch.int32)
# ---
def __init__(self, program, timeOfDay):
        super().__init__()
        self._program = program
        self._timeOfDay = timeOfDay
# ---
def is_being_preempted(self) -> bool:
        return get_current_tpu_is_preempted()
# ---
def max_Pos(self) -> Axis:
        return Axis(name="position", size=self.max_length)
# ---
def test_get_by_id(self):
        # Ensure id is correct for the current/logged in user.
        with self.client:
            self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="admin_user"),
                follow_redirects=True,
            )
            self.assertTrue(current_user.id == 1)
# ---
def print_screen(self):
        """
        Return MAX_NUM_LINES lines.
        """
        return self.lines[-MAX_NUM_LINES:]
# ---
def objective(params):
        a, b, c = params
        pred = a * L**2 + b * L + c
        residuals = y - pred
        return jnp.sum(huber(residuals))
# ---
def _get_cluster_spec(self) -> str:
        return f"local?use_isolated_env={self.config.use_isolated_env}"
# ---
def with_cpu(**kwargs: Any) -> ResourceConfig:
        return ResourceConfig(device=CpuConfig(), **kwargs)
# ---
def call_process(self, message, stream_route, stream_id):
        """
        Handles pre-processing of packet and process work
        """
        self.process(message)
# ---
def scan_for_old(ng):
    nodes = [n for n in ng.nodes if n.bl_idname in old_bl_idnames]
    for node in nodes:
        mark_old(node)
# ---
def setUp(self):
        self.conf = tecaconf.ConfigHandler(
            "tests/test_data/configuration.json",
            {"starting_path": "tests/test_data/images"}
        )
        self.files_list = [
          "foo.doc",
          "yukinon.jpg",
          "cuteflushadoingflushathings.webm"
        ]
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        dtype = x.dtype
        mean = x.mean(self.axis)
        var = x.var(self.axis)
        inv = hax.rsqrt(var + self.eps)
        out = (x - mean) * inv
        out = out.astype(dtype)

        if self.weight is not None:
            out = self.weight * out
        if self.bias is not None:
            out = out + self.bias
        return out
# ---
def parse_table(self, m):
        item = self._process_table(m)

        cells = re.sub(r'(?: *\| *)?\n$', '', m.group(3))
        cells = cells.split('\n')
        for i, v in enumerate(cells):
            v = re.sub(r'^ *\| *| *\| *$', '', v)
            cells[i] = re.split(r' *\| *', v)

        item['cells'] = cells
        self.tokens.append(item)
# ---
def test_alter_disconnect_to_false(self):
        self._test_alter_disconnect(True, False)
        self._test_alter_disconnect(False, False)
# ---
def full_like(
    x, /, fill_value, *, dtype=None, device=None, chunks=None, spec=None
) -> "Array":
    return full(fill_value=fill_value, **_like_args(x, dtype, device, chunks, spec))
# ---
def newFinal(self, name):
    pass
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        """Block until job completes."""
        ...
# ---
def __call__(self, indices: np.ndarray | jnp.ndarray) -> np.ndarray: ...
# ---
def log(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in log")
    return elemwise(nxp.log, x, dtype=x.dtype)
# ---
def transform_linear(x):
        if not isinstance(x, hax.nn.Linear):
            return x

        # do something that distinguishes doing weights jointly from independently
        new_weight = x.weight - hax.mean(x.weight)
        return dataclasses.replace(x, weight=new_weight)
# ---
def prefix(self):
        return self._prefix
# ---
def Receptionist_Hears_Dialtone (self):
        self.Step (Message = "Receptionist hears dial-tone...")

        self.Log (Message = "Receptionist agent waits for dial-tone...")
        self.Receptionist.sip_phone.Wait_For_Dialtone ()
# ---
def test_map_blocks_no_array_args(spec, executor):
    def func(block, block_id=None):
        return nxp.ones_like(block) * int(sum(block_id))

    a = cubed.map_blocks(func, dtype="int64", chunks=((5, 3),), spec=spec)
    assert a.chunks == ((5, 3),)

    assert_array_equal(
        a.compute(executor=executor),
        np.array([0, 0, 0, 0, 0, 1, 1, 1], dtype="int64"),
    )
# ---
def _createPreamble(self):
        """
        """
        ex = []
        ex.append('M48\n') # Beginning of a part program header
        ex.append('METRIC,TZ\n') # Metric, trailing zeros
        ex.append('G90\n') # Absolute mode
        ex.append('M71\n') # Metric measuring mode        
        return ex
# ---
def worker(mock_bundle_cache, mock_image_cache, mock_runtime):
    """Create Worker with mocked dependencies."""
    config = WorkerConfig(
        port=0,
        port_range=(50000, 50100),
        poll_interval=Duration.from_seconds(0.1),  # Fast polling for tests
    )
    return Worker(
        config,
        bundle_provider=mock_bundle_cache,
        image_provider=mock_image_cache,
        container_runtime=mock_runtime,
    )
# ---
def _arg_func(a, **kwargs):
    # pass through
    return {"i": a["i"], "v": a["v"]}
# ---
def test_get_root_on_create(self):
        root_on_create_val = Instance.get_root_on_create(
            'redis')
        self.assertFalse(root_on_create_val)
# ---
def closest_num(N):
  return (N - 1)
# ---
def visit_msqrt(self, element):
        content = self._visit_children(element)
        return BracedNode(f"\\sqrt{{{content}}}")
# ---
def test_scope_restoration():
    scope1 = MultitonScope()
    with scope1:
        _instance1 = DummyMultiton.init_instance(5)
        assert DummyMultiton.get_instance().value == 5

    with pytest.raises(ValueError):
        DummyMultiton.get_instance()

    with MultitonScope():
        _instance2 = DummyMultiton.init_instance(100)
        assert DummyMultiton.get_instance().value == 100

    with scope1:
        assert DummyMultiton.get_instance().value == 5
# ---
def _load_metadata(checkpoint_path, fs=None):
    if fs is None:
        fs, _, _ = fsspec.get_fs_token_paths(str(checkpoint_path))
    with fs.open(os.path.join(checkpoint_path, "metadata.json")) as metadata_in:
        metadata = json.load(metadata_in)
    return metadata
# ---
def test_collect_capturing(testdir):
    p = testdir.makepyfile("""
        print ("collect %s failure" % 13)
        import xyz42123
    """)
    result = testdir.runpytest(p)
    result.stdout.fnmatch_lines([
        "*Captured stdout*",
        "*collect 13 failure*",
    ])
# ---
def send_endorsement_notification(endorsement):
    subject = _("%s has endorsed you on Villages.cc") % endorsement.endorser
    send_notification(subject, endorsement.endorser, endorsement.recipient,
                      'endorsement_notification_email.txt',
                      {'endorsement': endorsement})
# ---
def _convert_to_hf_url(model_id: str, revision: Optional[str] = None) -> str:
    """Convert a HuggingFace model ID to an hf:// URL for fsspec streaming.

    Args:
        model_id: HuggingFace model ID like "meta-llama/Llama-2-7b"
        revision: Optional git revision (branch, tag, or commit hash)

    Returns:
        An hf:// URL like "hf://meta-llama/Llama-2-7b" or "hf://meta-llama/Llama-2-7b@main"
    """
    if revision:
        return f"hf://{model_id}@{revision}"
    return f"hf://{model_id}"
# ---
def add_dims_to_spec(_, qss, sds):
                if partition_grads_into_blocks:
                    qss = jax.tree.map(lambda qs: PartitionSpec(*((None,) + qs)), qss)
                if sds is not None:
                    qss = jax.tree.map(lambda qs: PartitionSpec(*(sds + qs)), qss)
                return qss
# ---
def __repr__(self):
        return "WindowOp"
# ---
def key_function(out_key):
        out_coords = out_key[1:]
        inc_coords = tuple(
            bi // split_every if i == axis else bi for i, bi in enumerate(out_coords)
        )
        return ((scanned.name,) + out_coords, (increment.name,) + inc_coords)
# ---
def test_without_args(self, mock_start, mock_stop):
        self.assertEqual((1, 2), trace_hide_args_func(1, i=2))
        expected_info = {
            "function": {
                "name": "osprofiler.tests.unit.test_profiler"
                        ".trace_hide_args_func"
            }
        }
        mock_start.assert_called_once_with("hide_args", info=expected_info)
        mock_stop.assert_called_once_with()
# ---
def test_corrupt_program_single_step(bank):
    source = CORPUS[3]  # add -- very short
    rng = random.Random(42)

    corrupted, mutations = corrupt_program(source, num_steps=1, bank=bank, rng=rng)

    assert len(mutations) <= 1
    if mutations:
        assert corrupted != source
# ---
def testFunctionDefLocal(self):
    self.assertEqual((0, 'baz\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          def bar():
            print 'baz'
          bar()
        foo()""")))
# ---
def test_empty_set_against_string_negation(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.z.notin_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [(1,), (2,), (3,), (4,)], params={"q": []})
# ---
def square(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in square")
    return elemwise(nxp.square, x, dtype=x.dtype)
# ---
def url(self) -> str: ...
# ---
def __len__(self):
        return len(self._variables)
# ---
def test_no_carry_over(self, testdir):
        p = testdir.makepyfile("""
            def test_func1():
                print ("in func1")
            def test_func2():
                print ("in func2")
                assert 0
        """)
        result = testdir.runpytest(p)
        s = result.stdout.str()
        assert "in func1" not in s
        assert "in func2" in s
# ---
def unregister_endpoint(self, endpoint_id: str) -> None:
        self._remote_client.unregister_endpoint(endpoint_id)
# ---
def path(self) -> str:
        """Returns the URL path to mount the application to when serving multiple applications."""
        return "/iris.cluster.ControllerService"
# ---
def test_bincount():
    X = Axis("X", 6)
    x = hax.named([0, 1, 1, 2, 3, 1], (X,))
    B = Axis("B", 5)

    out = hax.bincount(x, B)
    expected = jnp.bincount(x.array, length=B.size)
    assert out.axes == (B,)
    assert jnp.all(out.array == expected)

    w = hax.arange((X,), dtype=jnp.float32)
    out_w = hax.bincount(x, B, weights=w)
    expected_w = jnp.bincount(x.array, weights=w.array, length=B.size)
    assert jnp.allclose(out_w.array, expected_w)
# ---
def test_krackhardt_closeness(self):
        c = nx.closeness_centrality(self.K)
        d = {
            0: 0.529,
            1: 0.529,
            2: 0.500,
            3: 0.600,
            4: 0.500,
            5: 0.643,
            6: 0.643,
            7: 0.600,
            8: 0.429,
            9: 0.310,
        }
        for n in sorted(self.K):
            assert almost_equal(c[n], d[n], places=3)
# ---
def test_impl(df):
            return df.A.str.split(',')
# ---
def task_id(self) -> JobName:
        """Full task identifier (/job/.../index)."""
        return self._task_name
# ---
def _metric_unflatten(reduction: ReductionType, children):
    """Unflatten Metric for JAX."""
    value, count = children
    return Metric(_value=value, _count=count, reduction=reduction)
# ---
def record_batch(self, batch: TrainBatchOutput):
        if self._n_batches == 0:
            self._loss = batch.loss
            self._loss_per_channel = batch.loss_per_channel
        else:
            self._loss += batch.loss
            self._loss_per_channel += batch.loss_per_channel
        self._n_batches += 1
# ---
def test_index_step(tmp_path, spec, executor):
    # use 400MB chunks so that intermediate after indexing has 200MB chunks
    a = cubed.random.random(
        (20000, 10000), chunks=(10000, 5000), spec=spec
    )  # 400MB chunks
    b = a[::2, :]
    run_operation(tmp_path, executor, "index_step", b)
# ---
def stack_tree(batch_name, individual_datums):
    def _stack_leaves_unchecked(*leaves):
        if is_named_array(leaves[0]):
            return hax.stack(batch_name, leaves)
        else:
            return jnp.stack(leaves)

    return jax.tree.map(_stack_leaves_unchecked, *individual_datums, is_leaf=is_named_array)
# ---
def data_updater(self):
        while self.run:
            for irc_nick in self.data:
                self.update_data(irc_nick)
                time.sleep(30)

            time.sleep(600)
# ---
def step_impl(context):
    context.user_service.save(context.base_user)
# ---
def conj(a: A) -> A:
    return wrap_elemwise_unary(jnp.conj, a)
# ---
def name(self) -> str:
        return "threads"
# ---
def SearchInCurrentBuffer( pattern ):
  """ Returns the 1-indexed line on which the pattern matches
  (going UP from the current position) or 0 if not found """
  return GetIntValue( "search('{0}', 'Wcnb')".format( EscapeForVim( pattern )))
# ---
def get_all_address_state(self):
        with self.lock:
            return self._state.get_all_address_state()
# ---
def _Delete(self):
    """Delete the log group."""
    delete_cmd = util.AWS_PREFIX + [
        '--region', self.region,
        'logs', 'delete-log-group',
        '--log-group-name', self.name
    ]
    vm_util.IssueCommand(delete_cmd, raise_on_failure=False)
# ---
def test_wait_delete_table(self):
        """Delete table shall wait for the table to go offline."""
        tablename = "foobar_wait"
        hash_key = DynamoKey("id")
        self.dynamo.create_table(tablename, hash_key=hash_key, wait=True)
        result = self.dynamo.delete_table(tablename, wait=True)
        self.assertTrue(result)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.pct_change(1)
            return Ac.sum()
# ---
import datetime
def check_date(m, d, y):
    try:
        m, d, y = map(int, (m, d, y))
        datetime.date(y, m, d)
        return True
    except ValueError:
        return False
# ---
def the_object_name_is_in_the_collection_collection(name, collection):
    assert collection in [c.name for c in the_object_name_exists(name).users_collection]
# ---
def forward(self, x):
    with torch.cuda.amp.autocast(enabled=False):
      x = F.layer_norm(x.float(), [self.dim])
    return x * self.weight[None, None, :]
# ---
def delete_lines(self, min_row, max_row):
        if min_row == -1:
            self.current_label.fuzz_values = self.current_label.fuzz_values[:-1]
        else:
            self.current_label.fuzz_values = self.current_label.fuzz_values[:min_row] + self.current_label.fuzz_values[
                                                                                        max_row + 1:]

        _ = self.current_label  # if user deleted all, this will restore a fuzz value

        self.fuzz_table_model.update()
# ---
def named(self, name):
        if not hasattr(self, name):
            colors = ', '.join(self.__colors())
            raise Exception('Unknown color %s. Available colors are: %s' % (name, colors))
        return getattr(self, name)
# ---
def test_apply_gufunc_elemwise_loop(spec):
    def foo(x):
        assert x.shape in ((2,), (1,))
        return 2 * x

    a = cubed.from_array(np.array([1, 2, 3]), chunks=2, spec=spec)
    z = apply_gufunc(foo, "()->()", a, output_dtypes=int)
    assert z.chunks == ((2, 1),)
    assert_equal(z, np.array([2, 4, 6]))
# ---
def __str__(self):
        """String representation of the Check tree rooted at this node."""

        pass
# ---
def columnas():
        return ["Emisor","Fecha_CFDI","Tipo","RFC_Emisor","Folio_fiscal","Folio","Receptor",
                "RFC_Receptor", "Subtotal","IEPS","IVA","Ret IVA","Ret ISR","TC","Total"]
# ---
def current_tracker() -> "Tracker": ...
# ---
def filename(self):
        pass
# ---
def as_dict(self):
        return { 'args': [ self.args[i].decode('utf-8') for i in range(self.num_args) ] }
# ---
def _create_app(self) -> ActorServiceASGIApplication:
        return ActorServiceASGIApplication(service=self)
# ---
def test_add_different_chunks_fail(spec, executor):
    a = xp.ones((10,), chunks=(3,), spec=spec)
    b = xp.ones((10,), chunks=(5,), spec=spec)
    c = xp.add(a, b)
    assert_array_equal(c.compute(executor=executor), np.ones((10,)) + np.ones((10,)))
# ---
def test_dtypes():
    assert len(info.dtypes()) > 0
# ---
def test_get_job_status_returns_error_for_missing_job(client):
    """GetJobStatus RPC returns error for non-existent job."""
    resp = client.post(
        "/iris.cluster.ControllerService/GetJobStatus",
        json={"jobId": JobName.root("nonexistent").to_wire()},
        headers={"Content-Type": "application/json"},
    )
    # Connect RPC returns non-200 status for errors
    assert resp.status_code != 200
# ---
def _setPitchForVoiceType(self, voiceType, value):
        """Sets the pitch value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM
        - value: the pitch value to set.
        """

        voiceACSS = self._getACSSForVoiceType(voiceType)
        voiceACSS[acss.ACSS.AVERAGE_PITCH] = value
        voiceACSS['established'] = True
# ---
def _run_step(viz: Viz, step: VizStep):
    logger.info(f"Running step {step}")
    start = time.perf_counter()
    try:
        getattr(viz, f"step_{step}")()
        end = time.perf_counter()
        logger.info(f"Step {step} took {end - start:.2f} seconds")
    except Exception as e:
        logger.error(f"Step {step} failed: {e}")
        raise
# ---
def visit_BoolOp(self, node: ast.BoolOp) -> ast.BoolOp:
        node.op = self._maybe_swap(node.op, _BOOL_OPS)
        self.generic_visit(node)
        return node
# ---
def get_channel_loss_scale_dict(
    label: str, loss_scale_per_channel: torch.Tensor
) -> dict[str, torch.Tensor]:
    return get_channel_dict(label, "loss_scale", loss_scale_per_channel)
# ---
def test_expression_filter_pushdown(self, sync_backend, vortex_file):
        """Test filter pushdown with expression."""
        ds = Dataset.from_files(str(vortex_file)).load_vortex().filter(col("score") > 500)

        results = list(Backend.execute(ds, context=sync_backend))
        assert len(results) == 49  # scores 510, 520, ..., 990
        assert all(r["score"] > 500 for r in results)
# ---
def _wrapped_hist(arr):
        return _single_shard_histogram(arr, bin_edges=bins, reduce_mesh=flattened_spec)
# ---
def _pipeline() -> int:
        return len([dupekit.process_dicts_loop(d) for d in in_memory_table.to_pylist()])
# ---
def check_isosceles(x,y,z):
  if x==y or y==z or z==x:
	   return True
  else:
     return False
# ---
def start(self):
        """Start the event loop in a background thread."""
        if self._thread is None:
            raise RuntimeError("AsyncBridge already stopped.")
        self._thread.start()

        # Wait for loop to start
        self._started.wait()
# ---
def create_log():
    # Note that different steps cannot share variables
    with tempfile.NamedTemporaryFile(prefix="executor-log-") as f:
        return f.name
# ---
def __len__(self):
        return len(self.store)
# ---
def set_output_state(self, state):
        self._lo.set_output_state(state)
# ---
def fetch_startup_logs(self, tail_lines: int = 100) -> str | None:
        return "(local controller  no startup logs)"
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - stop worker and wait for completion."""
        self.stop()
        self.join(timeout=5)

        try:
            status, error = self.result_queue.get_nowait()
            if status == "error":
                raise RuntimeError(f"{self.__class__.__name__} failed") from error
        except queue.Empty:
            pass

        return False
# ---
def stop(self):
        """Stop the training worker."""
        self._should_stop = True
        self.transfer_server.cleanup()
# ---
def test_monotonic_fails_posix(apply_failing_clock_call,
                               errno_value,
                               strerror):
    """
    A failure in C{clock_gettime} results in an L{OSError} that
    presents the failure's errno.
    """
    calls = apply_failing_clock_call('_clock_gettime', errno_value)

    with pytest.raises(OSError) as exc:
        monotonic()

    assert len(calls) == 1
    assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC

    assert str(exc.value) == strerror
# ---
def load_json(self, loads=json_loads):
        try:
            self.parsed_json = loads(self.body)
        except Exception:
            if not self.body:
                return None
            raise InvalidUsage("Failed when parsing body as json")

        return self.parsed_json
# ---
def __init__(
        self,
        project: str,
        zone: str,
        api: GcsApi | None = None,
    ):
        self._project = project
        self._zone = zone
        self._api = api or RealGcsApi()
# ---
def get_actor_pool_name(self) -> str:
        return str(self)
# ---
def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4'), (2, 100, 200, 'ext4')]
# ---
def init(Vocab: Axis, config: Olmo2Config, *, key) -> "Olmo2Embedding":
        return Olmo2Embedding(Vocab, hnn.Embedding.init(Vocab, config.Embed, key=key))
# ---
def fold_via(
        self, fn: FoldFunction[M, P, CarryT], *, unroll: int | bool | None = None
    ) -> Callable[Concatenate[CarryT, P], CarryT]: ...
# ---
def __del__(self):
        # delete the work directories in the background, use the ls to make sure we don't
        # accidentally run this on our local machine
        subprocess.Popen(
            [
                "bash",
                "-c",
                "ls /dev/accel* && (rm -rf $HOME/marin/; rm -rf $HOME/.cache/; sudo rm -f /tmp/libtpu_lockfile)",
            ],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        _hacky_remove_tpu_lockfile()
# ---
def __init__(self, node, emailMessage, references=list(), children=dict(), slotted=bool("false")):
        self.node = node
        self.children = dict(children)
        self.references = references[:]
        self.slotted = slotted
        self.emailMessage = emailMessage
# ---
def is_Two_Alter(s):  
    for i in range (len( s) - 2) : 
        if (s[i] != s[i + 2]) : 
            return False
    if (s[0] == s[1]): 
        return False
    return True
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "embed_tokens", "position_embeddings": "embed_positions"}
# ---
def __copy__(self):
        return self.copy(deep=False)
# ---
def laplace(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.laplace(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def _tqdm_logging_one_time_setup():
    global _did_tqdm_logging_one_time_setup
    if _did_tqdm_logging_one_time_setup:
        return
    _did_tqdm_logging_one_time_setup = True
    tqdm_logging.set_log_rate(timedelta(seconds=60))
# ---
def _api_path(self):
        return "/product/license"
# ---
def _description_searchable(self):
        return bool(self.store or self.search or (self.column and self.column._fnct_search))
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Start Time", device_id)
# ---
def test_dialect_conn_options(self):
        engine = testing_engine("sqlite://", options=dict(_initialize=False))
        engine.dialect = Mock()
        conn = engine.connect()
        c2 = conn.execution_options(foo="bar")
        eq_(
            engine.dialect.set_connection_execution_options.mock_calls,
            [call(c2, {"foo": "bar"})]
        )
# ---
def _expose_cache_rows(cache_path: str, exemplar: T, num_rows: int) -> None:
    cache = TreeStore.open(exemplar, cache_path, mode="a", cache_metadata=False)
    futures = jax.tree.leaves(jax.tree.map(lambda x: x.offsets[0].write(num_rows), cache.tree))
    for future in futures:
        future.result()
# ---
def template_trim(filt_q, trim_q):
    return "filt_q: %d, trim_q: %d" % (filt_q, trim_q), ["--filter_qual", str(filt_q), "--trim_qual", str(trim_q)]
# ---
def mk_LayerNorm(self, axis: AxisSpec):
        return self.norm_config.build(axis)
# ---
def takephoto():
	camera.capture(imagefile)
	image1 = Image.open(imagefile)
	return image1
# ---
def heartbeat_deadline(self, timeout: Duration) -> Timestamp:
        """Compute the deadline when this worker's heartbeat will expire.

        Args:
            timeout: Heartbeat timeout duration

        Returns:
            Timestamp when the heartbeat will be considered expired
        """
        return self.last_heartbeat.add(timeout)
# ---
def discover(self) -> str | None:
        """Return address if controller is healthy via SSH check."""
        host = self._manual_config.host
        port = self._manual_config.port or DEFAULT_CONTROLLER_PORT
        conn = self._create_ssh_connection(host)
        if check_health(conn, port):
            return self.address
        return None
# ---
def render_children(
        node: RenderTreeNode,
        context: RenderContext,
    ) -> str:
        return separator.join(child.render(context) for child in node.children)
# ---
def setUp(self):
        self.parser = Parser()
# ---
def load_data(self):
        try:
            f = open('data/er_nick-data.csv', 'rt')
            reader = csv.reader(f)
            for nick_irc,id,nick_er,level,strength,rank_points,citizenship in reader:
                self.data[nick_irc] = {'id': int(id), 'nick': nick_er, 'level': int(level), 'strength': float(strength), 'rank_points': int(rank_points), 'citizenship': citizenship}
            f.close()
        except:
            pass
# ---
def name(self) -> str:
        return "beam"
# ---
def test_resolve_resolved_location(self):
        """Test resolving a ResolvedLocation returns it unchanged."""
        base = S3Location(bucket="test-bucket", path="base/path")
        other = LocalLocation(path=Path("/other/path"))

        resolved = base.resolve(other)

        assert resolved == other
# ---
def rights(self):
        'Copyright/licensing information. (optional)'
        try:
            return self.opt_meta['rights']
        except KeyError:
            return None
# ---
def __init__(self):
            self.tokenizer = create_test_tokenizer()
# ---
def test_dialect_engine_options(self):
        engine = testing_engine("sqlite://")
        engine.dialect = Mock()
        e2 = engine.execution_options(foo="bar")
        eq_(
            engine.dialect.set_engine_execution_options.mock_calls,
            [call(e2, {"foo": "bar"})]
        )
# ---
def min_length(list1):
   min_length = min(len(x) for x in  list1 )  
   min_list = min((x) for x in   list1)
   return(min_length, min_list)
# ---
def processTeamQueue():
	"""
	This should be called periodically to check and schedule jobs pending for the
	particular team
	"""
	pass
# ---
def test_valid_position_mask_has_true_entries(tok):
    source = "def f(x):\n    return x + 1\n"
    mask = tok.valid_position_mask(source)

    assert len(mask) == tok.num_position_tokens
    assert any(mask), "Should have at least one valid edit position"

    # Position 0 should be valid (start of FunctionDef).
    assert mask[0] is True
# ---
def exec_stage_func(input, func=None, config=None, name=None, compute_id=None):
    return func(input, config=config)
# ---
def clean_primeiro_numero(self):
        cleaned_data = self.cleaned_data

        telefone = Telefone()
        telefone.tipo = self.data['primeiro_tipo']
        telefone.ddd = self.data['primeiro_ddd']
        telefone.numero = self.data['primeiro_numero']
        telefone.principal = self.data['primeiro_principal']

        cleaned_data['primeiro_telefone'] = telefone
        return cleaned_data
# ---
def loss_fn(params, token_ids, loss_mask):
        return ar_loss(params, token_ids, loss_mask, config)
# ---
def __init__(self, **kwargs):
        self.options = kwargs
# ---
def test_count_add_capacity(self):
        """Count addition with consumed_capacity"""
        count = Count(4, 2, Capacity(3, 0))
        count2 = Count(5, 3, Capacity(2, 0))
        ret = count + count2
        self.assertEqual(ret, 9)
        self.assertEqual(ret.scanned_count, 5)
        self.assertEqual(ret.consumed_capacity.read, 5)
# ---
def lbs(arr): 
	n = len(arr) 
	lis = [1 for i in range(n+1)] 
	for i in range(1 , n): 
		for j in range(0 , i): 
			if ((arr[i] > arr[j]) and (lis[i] < lis[j] +1)): 
				lis[i] = lis[j] + 1
	lds = [1 for i in range(n+1)] 
	for i in reversed(range(n-1)): 
		for j in reversed(range(i-1 ,n)): 
			if(arr[i] > arr[j] and lds[i] < lds[j] + 1): 
				lds[i] = lds[j] + 1
	maximum = lis[0] + lds[0] - 1
	for i in range(1 , n): 
		maximum = max((lis[i] + lds[i]-1), maximum) 
	return maximum
# ---
def terminate(self, job_id: JobId) -> None:
        """Terminate a running job.

        Attempts graceful termination first, then forceful kill if needed.

        Args:
            job_id: Job identifier

        Raises:
            KeyError: If job_id is not found
        """
        ...
# ---
def test_register_route(self):
        # Ensure about route behaves correctly.
        response = self.client.get("/register", follow_redirects=True)
        self.assertIn(b"<h1>Register</h1>\n", response.data)
# ---
def get_value_max(self):
        """Return the maximum value possible for an ADC read"""
        return 2 ** self._BitLength - 1
# ---
def _reset_worker_state(self) -> None:
        """Reset worker state: wipe all containers and clear tracking."""
        logger.info("Resetting worker state")

        # Clear task tracking
        with self._lock:
            self._tasks.clear()

        # Wipe ALL iris containers (simple, no tracking needed)
        self._cleanup_all_iris_containers()

        logger.info("Worker state reset complete")
# ---
def _get_vm_status(vm_name: str, zone: str, project: str) -> dict | None:
    """Get detailed status of a VM."""
    result = _run_gcloud(
        [
            "compute",
            "instances",
            "describe",
            vm_name,
            f"--project={project}",
            f"--zone={zone}",
            "--format=json",
        ]
    )
    if result.returncode != 0:
        return None
    return json.loads(result.stdout)
# ---
def _prepare_bucket(self, task_name: str) -> list[dict[str, str]] | None:
        if not self.sample_logging_config.should_log():
            return None

        bucket = self.sample_outputs.setdefault(task_name, [])
        if self.sample_logging_config.allow_more(len(bucket)):
            return bucket

        return None
# ---
def width(self):
        """The width of the window, in pixels.  Read-write.

        :type: int
        """
        return self.get_size()[0]
# ---
def overlay_remove(self, overlay_id):
        self.command('overlay_remove', overlay_id)
# ---
def test_iris_config_empty_file(tmp_path):
    """Test error on empty config file."""
    bad_config = tmp_path / "bad.yaml"
    bad_config.write_text("")
    with pytest.raises(ValueError, match="Config file is empty"):
        IrisConfig.load(bad_config)
# ---


def how_many_times(string: str, substring: str) -> int:
    """ Find how many times a given substring can be found in the original string. Count overlaping cases.
    >>> how_many_times('', 'a')
    0
    >>> how_many_times('aaa', 'a')
    3
    >>> how_many_times('aaaa', 'aa')
    3
    """
    times = 0

    for i in range(len(string) - len(substring) + 1):
        if string[i:i+len(substring)] == substring:
            times += 1

    return times
# ---
def epoch_ms(self) -> int:
        """Get milliseconds since epoch."""
        return self._epoch_ms
# ---
def test_str_get(self):
        def test_impl(df):
            B = df.A.str.split(',')
            return B.str.get(1)

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def _get_client():
        return get_test_client()
# ---
def axis_spec_to_tuple(axis_spec: PartialShapeDict) -> tuple[AxisSelector, ...]: ...
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: Optional[str] = None
    ) -> HFCheckpointConverter["MistralConfig"]:  # type: ignore
        hf_model_path = "mistralai/Mistral-7B-v0.1" if ref_checkpoint is None else ref_checkpoint

        return HFCheckpointConverter(
            self,
            reference_checkpoint=hf_model_path,
            trust_remote_code=True,
            tokenizer=hf_model_path,
            HfConfigClass=HfMistralConfig,
        )
# ---
def __init__(self):
        self.transfer_server_address = None  # Actual JAX transfer server address
        self._requested_transfers = []
        self._lock = asyncio.Lock()
        self._latest_weight_id = None
        self._pending_completion = {}
        self._transfer_id = 0
# ---
def clean_wiki_html(html: str, remove_reference_section: bool = True) -> str:
    """
    Cleans the HTML by removing unwanted elements.
    """
    html = unwrap_eqn(html)
    html = remove_and_append_infobox(html)

    if remove_reference_section:
        html = remove_references_from_html(html)

    return html
# ---
def update_status(self, context, status):
        """Update local status.

            This method call updates status attribute of
            VPNServices.
        """
        cctxt = self.client.prepare()
        return cctxt.call(context, 'update_status', status=status)
# ---
def wait(self, timeout: float | None = None, *, raise_on_failure: bool = True) -> JobStatus:
        effective_timeout = timeout if timeout is not None else 86400.0
        try:
            self._job.wait(timeout=effective_timeout, raise_on_failure=raise_on_failure)
        except Exception:
            if raise_on_failure:
                raise
        return self.status()
# ---
def gather_lists(self, model: LightningModule, object):
        if torch.distributed.is_available() and torch.distributed.is_initialized():
            object_list = [None] * model.trainer.world_size
            torch.distributed.all_gather_object(object_list, object)
            return list(itertools.chain(*object_list))
        else:
            return object
# ---
def script_message(self, *args):
        self.command('script_message', *args)
# ---
def previous_palindrome(num):
    for x in range(num-1,0,-1):
        if str(x) == str(x)[::-1]:
            return x
# ---
def __init__(self, comodel_name=None, relation=None, column1=None, column2=None,
                 string=None, **kwargs):
        super(Many2many, self).__init__(
            comodel_name=comodel_name,
            relation=relation,
            column1=column1,
            column2=column2,
            string=string,
            **kwargs
        )
# ---
def inversion_elements(test_tup):
  res = tuple(list(map(lambda x: ~x, list(test_tup))))
  return (res)
# ---
def dict_to_querystring(objs):
    return "&".join(["%s=%s" % (k, v)
                     for k, v in objs.items()
                     if v is not None])
# ---
def check_element(test_tup, check_list):
  res = False
  for ele in check_list:
    if ele in test_tup:
      res = True
      break
  return (res)
# ---
def _get_ovn_controller(self, install_method="sandbox"):
        ovn_nbctl = self.controller_client("ovn-nbctl")
        ovn_nbctl.set_sandbox("controller-sandbox", install_method,
                              self.context['controller']['host_container'])
        ovn_nbctl.set_daemon_socket(self.context.get("daemon_socket", None))
        return ovn_nbctl
# ---
def test_concat(tmp_path, spec, executor):
    # Note 'a' has one fewer element in axis=0 to force chunking to cross array boundaries
    a = cubed.random.random(
        (9999, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    c = xp.concat((a, b), axis=0)
    run_operation(tmp_path, executor, "concat", c)
# ---
def init(weight):
            return FoldModule(weight=weight)
# ---
def remove_ar5iv_footer(html: BeautifulSoup):
    # This is the ar5iv footer generated on xyz date
    footer = html.findAll("footer")
    for fn in footer:
        fn.decompose()
# ---
def concatenate_nested(test_tup1, test_tup2):
  res = test_tup1 + test_tup2
  return (res)
# ---
def get_path():
    return addon.getAddonInfo('path').decode('utf-8')
# ---
def cursor_execute(
                self,
                execute,
                cursor,
                statement,
                parameters,
                context,
                executemany,
                ):
                cursor_stmts.append((str(statement), parameters, None))
                return execute(cursor, statement, parameters, context)
# ---
def validate(candidate):
            return hax.where(self.used_mask["seq", candidate], INVALID, candidate)
# ---
def parse_heading(self, m):
        self.tokens.append({
            'type': 'heading',
            'level': len(m.group(1)),
            'text': m.group(2),
        })
# ---
def loader_version(request: pytest.FixtureRequest) -> c.LoaderVersion:
    return request.param
# ---
def work_dir(self) -> Optional[str]:
        """The directory path (specified as an fsspec or obstore URL) used for storing intermediate data."""
        return self._work_dir
# ---
def update_router_precommit(self, context, router_context):
        pass
# ---
def _product_get_all_report(self, cr, uid, ids, product_ids=False, context=None):
        return self._product_get_report(cr, uid, ids, product_ids, context, recursive=True)
# ---
def compute_reward(generated_tokens: np.ndarray, expected_tokens: np.ndarray) -> float:
    """
    Compute reward based on exact token ID match.

    Args:
        generated_tokens: Array of generated token IDs
        expected_tokens: Array of expected token IDs (e.g., tokenized "Paris")

    Returns:
        1.0 if expected_tokens appear consecutively in generated_tokens, else 0.0
    """
    if np.array_equal(generated_tokens, expected_tokens):
        return 1.0
    else:
        return 0.0
# ---
def __truediv__(self, other: "Location") -> "ResolvedLocation":
        return self.resolve(other)
# ---
def __next__(self):
        """
        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.
        :return: The current object of the iterator.
        """
        if self.index >= self.list_length:
            raise StopIteration
        else:
            self.index += 1
            self.update(task_id=self.task_id,
                        progress=self.index)

            return self.list[self.index - 1]
# ---
def bank():
    return SubtreeBank.from_corpus(CORPUS)
# ---
def batch_size(self):
        return self._batch_size
# ---
def lower_ctr(str):
      lower_ctr= 0
      for i in range(len(str)):
          if str[i] >= 'a' and str[i] <= 'z': lower_ctr += 1     
      return  lower_ctr
# ---
def author(self):
        '''Name of the author. (optional)

        If there are multiple authors, pass a list of strings.

        To control the file-as and role attribute, use author objects instead
        of strings; file-as is an alternate form of the name used for sorting.
        For a description of the role attribute, see the docstring of the
        author class.'''
        if len(self._authors) == 1:
            return self._authors[0]
        return tuple([aut for aut in self._authors])
# ---
def _use_upload_table(self):
        """
            Set the resource and the table to being s3_import_upload
        """

        if self.upload_resource == None:
            from s3resource import S3Resource
            (prefix, name) = self.UPLOAD_TABLE_NAME.split("_",1)
            self.upload_resource = S3Resource(prefix, name)
        self.resource = self.upload_resource
        self.table = self.upload_table
        self.tablename = self.upload_tablename
# ---
from sys import maxsize 
def max_sub_array_sum(a,size): 
	max_so_far = -maxsize - 1
	max_ending_here = 0
	start = 0
	end = 0
	s = 0
	for i in range(0,size): 
		max_ending_here += a[i] 
		if max_so_far < max_ending_here: 
			max_so_far = max_ending_here 
			start = s 
			end = i 
		if max_ending_here < 0: 
			max_ending_here = 0
			s = i+1
	return (end - start + 1)
# ---
def calculate_damage(self, rank_points, strength, weapon_power, level, bonus):
        index = 0

        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:
            if(self.rank_to_pos.index(k) > index):
                index = self.rank_to_pos.index(k)

        return(math.trunc(((index / 20) + 0.3) * ((strength / 10) + 40) * (1 + (weapon_power / 100)) * (1.1 if level > 99 else 1) * bonus))
# ---
def find_Odd_Pair(A,N) : 
    oddPair = 0
    for i in range(0,N) :  
        for j in range(i+1,N) :  
            if ((A[i] ^ A[j]) % 2 != 0):  
                oddPair+=1  
    return oddPair
# ---
def test_df_input(self):
        def test_impl(df):
            return df.B.sum()

        n = 121
        df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))
# ---
def weights():
    return {"ds1": 0.5, "ds2": 0.3, "ds3": 0.2}
# ---
def fftfreq(axis: Axis, d: float = 1.0) -> NamedArray:
    """Named version of :func:`jax.numpy.fft.fftfreq`."""

    return NamedArray(jfft.fftfreq(axis.size, d), (axis,))
# ---
def lower(self, *args: Args.args, **kwargs: Args.kwargs) -> jax.stages.Lowered:
        raise NotImplementedError
# ---
def convert_to_cache(self, value, record, validate=True):
        # apply rounding here, otherwise value in cache may be wrong!
        value = float(value or 0.0)
        digits = self.digits
        return float_round(value, precision_digits=digits[1]) if digits else value
# ---
def execute(conn, clauseelement, multiparams, params):
            canary.append('execute')
            return clauseelement, multiparams, params
# ---
def compile_function(func, *, config=None):
        raise NotImplementedError(f"Cannot compile {func} with {config}")
# ---
def data_loader(self):
        self.load_data()

        self.data_saver_th = threading.Thread(target=self.data_saver)
        self.data_saver_th.daemon = True
        self.data_saver_th.start()

        self.data_updater_th = threading.Thread(target=self.data_updater)
        self.data_updater_th.daemon = True
        self.data_updater_th.start()
# ---
def render_short_timestamp(timestamp):
    return str(datetime.datetime.fromtimestamp(timestamp//1000))
# ---
def assumed_state(self):
        """State is assumed, if no template given."""
        return self._template is None
# ---
def flip(self):
        """Swap the OpenGL front and back buffers.

        Call this method on a double-buffered window to update the
        visible display with the back buffer.  The contents of the back buffer
        is undefined after this operation.

        Windows are double-buffered by default.  This method is called
        automatically by `EventLoop` after the `on_draw` event.
        """
        raise NotImplementedError('abstract')
# ---
def square_nums(nums):
 square_nums = list(map(lambda x: x ** 2, nums))
 return square_nums
# ---
def __init__(self, x=0.0, y=0.0):
        self.x = x
        self.y = y
# ---
def reset(self) -> None:
        """Reset timer to current time."""
        self._start = time.monotonic()
# ---
def set_caption(self, caption):
        """Set the window's caption.

        The caption appears in the titlebar of the window, if it has one,
        and in the taskbar on Windows and many X11 window managers.

        :Parameters:
            `caption` : str or unicode
                The caption to set.

        """
        raise NotImplementedError('abstract')
# ---
def circle_circumference(r):
  perimeter=2*3.1415*r
  return perimeter
# ---
def test_do_executemany_w_replace(self):
        self._test_do_executemany(True)
# ---
def test_worker_crash_mid_task(cluster):
    """Worker task monitor crashes mid-task. Task fails, controller detects
    via heartbeat reconciliation or report_task_state."""
    _url, client = cluster
    # task_monitor chaos kills the monitoring loop  task fails with error
    enable_chaos("worker.task_monitor", failure_rate=1.0)
    job = submit(client, _quick, "crash-mid-task")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_FAILED
# ---
def _ema_context(self):
        """
        A context where the stepper uses the EMA model.
        """
        self._ema.store(parameters=self.model.parameters())
        self._ema.copy_to(model=self.model)
        try:
            yield
        finally:
            self._ema.restore(parameters=self.model.parameters())
# ---
def from_periods(cls, start, end):
        """Create an interval with two Periods as the endpoints.
        """
        end_date = end.open_right_endpoint() if end is not None else None
        start_date = start.date if start is not None else None
        return cls(start_date, end_date)
# ---
def test_available(self):
        eq_(self.record.available, True)
# ---
def argmin(array: NamedArray, axis: AxisSelector | None) -> NamedArray:
    return wrap_reduction_call(jnp.argmin, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def _curriculum_checkpoint_hook(info: levanter.callbacks.StepInfo):
            checkpoint_dir = self.config.trainer.checkpointer.expanded_path(self.config.run_id)
            try:
                future = self._curriculum_actor.save_checkpoint.remote(checkpoint_dir)
                get_default_job_ctx().get(future)
            except Exception as e:
                logger.error(f"Failed to save curriculum checkpoint: {e}")
# ---
def remove_column(list1, n):
   for i in list1: 
    del i[n] 
   return list1
# ---
def loss_full_fn(x):
        loss, _ = _full_loss_and_logz(x, lm_head, labels, precision=jax.lax.Precision.HIGHEST)
        return jnp.mean(loss)
# ---
def __init__(self, dataset_id: "TorchTrainDataset.Id", label_mask: PrognosticMask):
        self.dataset_id: TorchTrainDataset.Id = dataset_id
        self.label_mask = label_mask
        self.raw_data: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []
        self.load_stats: LoadStats | None = None
# ---
def _validate_job_done(job):
    if job.status != STATUS.DONE:
        raise JobNotDone("Job %r is %s" % (job.id, job.status))
# ---
def main(config: EvaluationConfig) -> None:
    evaluate(config)
# ---
def _get_dashboard_address(self) -> str:
        if ray.is_initialized():
            try:
                ctx = ray.get_runtime_context()
                gcs_address = getattr(ctx, "gcs_address", None)
                if gcs_address is not None:
                    return gcs_address
            except Exception:
                pass
        return self._address
# ---
def bytearray_to_utf8(x):
    return x.decode('utf-8')
# ---
def pad_token_id(self) -> int:
        return self.the_tokenizer.pad_token_id
# ---
def infer_xla_v_block_size(
    b: int,
    h: int,
    v: int,
    *,
    dtype: Optional[jnp.dtype],
    device_kind: Optional[str] = None,
) -> int:
    """Heuristic v-block size for the XLA streaming path."""
    del b, h, dtype, device_kind  # currently unused
    target = min(v, 32768)
    if target <= 0:
        return 1
    # Keep the block size <= v to avoid excess padding work.
    if target == v:
        return target
    return max(128, 128 * (target // 128))
# ---
def updateTable(self):
        pass
# ---
def with_name_like(self, name):
        Util.validate_type(name, "str")
        return self._with_name_like(name)
# ---
def check_valid(test_tup):
  res = not any(map(lambda ele: not ele, test_tup))
  return (res)
# ---
def binary_cross_entropy_loss(
    logits: NamedArray,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: None = None,
) -> jnp.ndarray | NamedArray: ...
# ---
def init(Vocab: Axis, config: ParallelLlamaConfig, *, key) -> "ParallelLlamaEmbedding":
        token_embeddings = hnn.Embedding.init(Vocab, config.Embed, key=key)
        return ParallelLlamaEmbedding(token_embeddings)
# ---
def Find_Min(lst): 
    minList = min((x) for x in lst) 
    return minList
# ---
import heapq as hq
def heap_assending(nums):
  hq.heapify(nums)
  s_result = [hq.heappop(nums) for i in range(len(nums))]
  return s_result
# ---
def test_bpe_round_trip_various_texts(llama3_tokenizer):
    """Validate BPE round-trip for diverse text patterns."""
    for text in ["!!}", "Hello world", "  spaces  ", "123", "\n\n"]:
        for token_id in llama3_tokenizer.encode(text, add_special_tokens=False):
            token_str = llama3_tokenizer.convert_ids_to_tokens(token_id)
            assert llama3_tokenizer.convert_tokens_to_ids(token_str) == token_id
# ---
def dartmouth(domain, b, threshold=None):
    '''A flood detection method from the Dartmouth Flood Observatory.

        This method is a refinement of the simple b2-b1 detection method.
    '''
    if threshold == None:
        threshold = float(domain.algorithm_params['dartmouth_threshold'])
    return get_dartmouth(b).lte(threshold)
# ---
def test_diagnose_common_issues_repeated():
    class M(eqx.Module):
        a: jnp.ndarray = eqx.field()
        b: jnp.ndarray = eqx.field()

        def __init__(self):
            super().__init__()
            self.a = jnp.zeros(1)
            self.b = self.a

    try:
        hax.debug.diagnose_common_issues(M())
        pytest.fail("Should have raised an exception")
    except hax.debug.ModuleProblems as e:
        assert len(e.reused_arrays) == 1
        assert len(e.static_arrays) == 0
# ---
def __call__(
        self,
        lhs,
        rhs,
        dimension_numbers,
        precision,
        preferred_element_type=None,
        **kwargs,
    ):
        cfg = aqt_config.set_context(self.cfg, jrandom.PRNGKey(42), train_step=None)
        return cfg(lhs, rhs, dimension_numbers, precision, preferred_element_type)
# ---
def tunnel(
        self,
        controller_address: str,
        local_port: int | None = None,
        timeout: float | None = None,
        tunnel_logger: logging.Logger | None = None,
    ) -> AbstractContextManager[str]:
        """Return direct connection for manual platform (no tunnel needed)."""
        return nullcontext(controller_address)
# ---
def __repr__(self):
        return ('<{0.__class__.__name__}'
                ' event={0.event_type!r}'
                ' subject={0.subject!r}>').format(self)
# ---
def adjacent_num_product(list_nums):
    return max(a*b for a, b in zip(list_nums, list_nums[1:]))
# ---
def hash_func(obj: Any) -> int:
    """Convert arbitrary objects to i128 integers for hashing."""
    h = sha256(dumps(obj)).digest()
    return int.from_bytes(h[:16], "big", signed=True)
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> float | None:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=True,
        )
# ---
def root(self) -> Path:
        return self._root
# ---
def vpnservice_updated(self, context, **kwargs):
        """Vpnservice updated rpc handler

        VPN Service Driver will call this method
        when vpnservices updated.
        Then this method start sync with server.
        """
        router = kwargs.get('router', None)
        self.sync(context, [router] if router else [])
# ---
def wait_for_result(self, timeout=None):
        """Wait for worker completion, raise an error if it failed."""
        try:
            status, error = self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return WaitResult.TIMEOUT

        if status == "error":
            raise RuntimeError(f"{self.__class__.__name__} failed") from error

        return WaitResult.SUCCESS
# ---
def embed(self, input_ids, *args):
        return self.token_embeddings(input_ids)
# ---
def spawn_executor(self, max_workers: int, prefix: str) -> ThreadPoolExecutor:
        """Create a ThreadPoolExecutor that will be shut down when this container stops."""
        executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=prefix)
        with self._lock:
            self._executors.append(executor)
        return executor
# ---
def failing_generator(items):
        for item in items:
            if item == 3:
                raise ValueError("Test error")
            yield item
# ---
def max_length(list1):
    max_length = max(len(x) for x in  list1 )  
    max_list = max((x) for x in   list1)
    return(max_length, max_list)
# ---
def nanmin(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
) -> NamedArray:
    return wrap_reduction_call(jnp.nanmin, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def from_state_dict(cls, config: GatedDeltaNetConfig, state: dict[str, jnp.ndarray], *, key) -> "GatedDeltaNet":
        """
        Build a fresh layer from config + state dict.
        """
        layer = cls.init(config, key=key)
        return layer.load_state_dict(state)
# ---
def update(self, other):
		for k in other:
			self[k] = other[k]
# ---
def unique(
    array: NamedArray, Unique: Axis, *, axis: AxisSelector | None = None, fill_value: ArrayLike | None = None
) -> NamedArray: ...
# ---
def create(self, request, *args, **kwargs):
        raise MethodNotAllowed(self.action)
# ---
def config_get(self):
        return bottle.template('{{!ret}}',
                               ret=json.dumps(self.config))
# ---
def ready_count(self) -> int:
        """Number of VMs in READY state."""
        return sum(1 for v in self.vms if v.state == vm_pb2.VM_STATE_READY)
# ---
def test_string_set(self):
        """Store and retrieve a string set"""
        self.make_table()
        item = {
            "id": "a",
            "datas": set(["a", "b"]),
        }
        self.dynamo.put_item("foobar", item)
        ret = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(ret, item)
# ---
def endorsement(request, endorsement_id):
    endorsement = get_object_or_404(Endorsement, pk=endorsement_id)
    return locals()
# ---
def _prep_inputs(t):
        if not low_mem:
            if not sum(t.shape[:no_batch_dims]) == no_batch_dims:
                t = t.expand(orig_batch_dims + t.shape[no_batch_dims:])
            t = t.reshape(-1, *t.shape[no_batch_dims:])
        else:
            t = t.expand(orig_batch_dims + t.shape[no_batch_dims:])
        return t
# ---
def QLoraSize(self) -> Axis:
        return Axis("q_lora_rank", self.q_lora_rank)
# ---
def average_Even(n) : 
    if (n% 2!= 0) : 
        return ("Invalid Input") 
        return -1  
    sm = 0
    count = 0
    while (n>= 2) : 
        count = count+1
        sm = sm+n 
        n = n-2
    return sm // count
# ---
def test_basic_identity():
    assert einops_rearrange(z, "b d h w c -> b d h w c").axes == (B, D, H, W, C)
    assert (einops_rearrange(z, "b d h w c -> b d h w c").array == z.array).all()
# ---
def main(config: TokenizeConfig):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    tokenize(config)
# ---
def getContainedObjectInterface(self):
        return IPublication
# ---
def add_trigger(self, trigger):
        """ Add a recomputation trigger on ``self``. """
        if trigger not in self._triggers:
            self._triggers += (trigger,)
# ---
def _align_variables(variables, join='outer'):
    """Align all DataArrays in the provided dict, leaving other values alone.
    """
    alignable = [k for k, v in variables.items() if hasattr(v, 'indexes')]
    aligned = align(*[variables[a] for a in alignable],
                    join=join, copy=False)
    new_variables = OrderedDict(variables)
    new_variables.update(zip(alignable, aligned))
    return new_variables
# ---
def profile_memray(func):
    """Decorator to profile a function call with memray."""
    return partial(execute_with_memray, func)
# ---
def loss_weight(self, sigma):
        # note: in AF3 there is a + at denominator while in EDM a *, we think this is a mistake in the paper
        return (sigma**2 + self.sigma_data**2) / ((sigma * self.sigma_data) ** 2)
# ---
def fn(config: MyConfig | None):
        print(config.input_path, os.path.exists(config.input_path), flush=True)
        if os.path.exists(config.input_path):
            raise Exception("Failed")
        else:
            append_log(log, config)
# ---
def test_non_dict_parent(self):
        expr = col("meta")["score"]
        assert expr.evaluate({"meta": "not a dict"}) is None
# ---
def alt(self) -> ast.Alternative:
        assert self.is_alt
        return self.group.seq[0]
# ---
def __call__(self, x):
            return self.down_proj(self.up_proj(x))
# ---
def unflatten_from_export(self, template: "XIELUActivation") -> "XIELUActivation":
        """Squeeze [1] parameters back to scalars."""
        del template
        alpha_p = hax.named(jnp.squeeze(self.alpha_p.array), ())
        alpha_n = hax.named(jnp.squeeze(self.alpha_n.array), ())
        return XIELUActivation(alpha_p, alpha_n, self.beta, self.eps)
# ---
def fuse_all_optimize_dag(dag, array_names=None):
    """Force all operations to be fused."""
    dag = dag.copy()
    always_fuse = [op for op in dag.nodes() if op.startswith("op-")]
    return multiple_inputs_optimize_dag(
        dag, array_names=array_names, always_fuse=always_fuse
    )
# ---
def get_task(self, task_id: JobName) -> ControllerTask | None:
        with self._lock:
            return self._tasks.get(task_id)
# ---
def select_scaffold_binder(
        self,
        tokens: np.ndarray,
        random: np.random.Generator,
        fixed_crop: bool = False,
    ):
        tokens = self.select_scaffold(tokens, random, fixed_crop)
        return self.resect_and_reindex(tokens, random)
# ---
def test_rich_progress_bar(spec, executor):
    # test indirectly by checking it doesn't cause a failure
    progress = RichProgressBar()

    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]], chunks=(2, 2), spec=spec)
    c = xp.add(a, b)
    assert_array_equal(
        c.compute(executor=executor, callbacks=[progress]),
        np.array([[2, 3, 4], [5, 6, 7], [8, 9, 10]]),
    )
# ---
def get_max_balance():
        return AdvancedBankAccount.MAX_BALANCE
# ---
def script_BIP34_coinbase_height(height):
    if height <= 16:
        res = CScriptOp.encode_op_n(height)
        # Append dummy to increase scriptSig size above 2 (see bad-cb-length consensus rule)
        return CScript([res, OP_1])
    return CScript([CScriptNum(height)])
# ---
def model_type(self):  # noqa: D401
        return Qwen3LMHeadModel
# ---
def _delete(self):
        """
        Delete myself by my own id.
        As of 20170114 no other methods call me. You must do `foo._delete()`
        :return:
        """
        command = ['ec2', 'delete-security-group', '--region', self.region,
                   # '--dry-run',
                   '--group-id', self.id
                   ]
        bin_aws(command, decode_output=False)
        print('Deleted {0}'.format(command))  # TODO: Log(...)
        return True
# ---
def torch_loss(model, input_ids) -> torch.Tensor:
            return model(input_ids, labels=input_ids)[0]
# ---
def _choose_port(id):
    port = int(id) % 2**12 + (65535 - 2**12 + 1)
    return port
# ---
def __iter__(self):
        """
        Iterator for CmdText object.
        """
        for l in self.lines:
            yield l
# ---
def info(self):
        return {}
# ---
def test_capsys_results_accessible_by_attribute(capsys):
    sys.stdout.write("spam")
    sys.stderr.write("eggs")
    capture_result = capsys.readouterr()
    assert capture_result.out == "spam"
    assert capture_result.err == "eggs"
# ---
def __init__(self, radius):
        self.radius = radius
# ---
def stop(self) -> None:
        """No-op: FakeVm has no background threads to stop."""
# ---
def loadfile(self, filename, mode='replace', **options):
        self.command('loadfile', filename.encode(fs_enc), mode, MPV._encode_options(options))
# ---
def comma_main_mixture(*, tokenizer: str = llama3_tokenizer, permutation_type: PermType = "feistel"):
    """LmMixtureDatasetConfig for the main training stage."""
    tokenized = common_pile_tokenized(tokenizer=tokenizer)
    components = {f"common_pile/{dataset}": tokenized[f"common_pile/{dataset}"] for dataset in COMMON_PILE_DATASETS}
    return lm_mixture_data_config(
        components=components,
        weights=COMMA_MAIN_MIXTURE_WEIGHTS,
        permutation_type=permutation_type,
    )
# ---
def __len__(self) -> int:
        """Get the length of the dataset.

        Returns
        -------
        int
            The length of the dataaset.

        """
        if self.overfit is not None:
            length = sum(len(d.samples[: self.overfit]) for d in self.datasets)
        else:
            length = sum(len(d.samples) for d in self.datasets)

        return length
# ---
def get_channel_loss_dict(
    label: str, loss_per_channel: torch.Tensor, loss_name: str = "loss"
) -> dict[str, torch.Tensor]:
    return get_channel_dict(label, loss_name, loss_per_channel)
# ---
def run_pipeline(original: torch.Tensor, count) -> torch.Tensor:
        if count == 0:
            return original
        with torch.no_grad():
            x = downsample(original)
            x = torch.square(x) + 1  # a nonlinearity
            x = run_pipeline(x, count - 1)
            x = upsample(x)
            padded = _pad_like_unet(x, original)
            result = padded + original
            return result
# ---
def fold_metrics_jit(m1, m2):
        return fold(m1, m2)
# ---
def visit_mroot(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            index = self._visit(children[1])
            return BracedNode(f"\\sqrt[{index}]{{{base}}}")
        return TextNode("")
# ---
def char_frequency(str1):
    dict = {}
    for n in str1:
        keys = dict.keys()
        if n in keys:
            dict[n] += 1
        else:
            dict[n] = 1
    return dict
# ---
def preempted_until_n():
        return ray.get(actor.run.remote())
# ---
def to_python(self, value):
		if not value:
			return []

		if isinstance(value, list):
			return value

		return value.split(',')
# ---
def move_by_offset(self, xoffset, yoffset):
        """Moving the mouse to an offset from current mouse position.
        Args:
            xoffset: X offset to move to.
            yoffset: Y offset to move to.
        """
        self._actions.append(lambda:
            self._driver.execute(Command.MOVE_TO, {
                'xoffset': xoffset,
                'yoffset': yoffset}))
        return self
# ---
def _handle_interrupt(self, _signum: int, _frame: object):
        self.logger.log("Interrupted! Cleaning up...", level="WARN")
        signal.signal(signal.SIGINT, None)
        signal.signal(signal.SIGTERM, None)
        self._interrupted = True
# ---
def free(self):
        SummaryKeyMatcher.cNamespace().free(self)
# ---
def leaky_relu(a: A) -> A:
    return wrap_elemwise_unary(jnn.leaky_relu, a)
# ---
def test_parse_paren_groups():
    lhs, rhs = parse_rearrangement("a (b c) d -> b c a d")
    assert lhs.is_ordered
    assert _simplify_captures(lhs) == ["a", ("b", "c"), "d"]
    assert rhs.is_ordered
    assert _simplify_captures(rhs) == ["b", "c", "a", "d"]

    lhs, rhs = parse_rearrangement("a (b: c) (d: e f) -> b c a d")
    assert lhs.is_ordered
    assert _simplify_captures(lhs) == ["a", {"b": ("c",)}, {"d": ("e", "f")}]
# ---
def __init__(self, pool: ActorPool[T]):
        self._pool = pool
# ---
def format_tmps(tmps):
    return format_line(prefix='temps'.rjust(RJUST), values=tmps)
# ---
def test_get_typed_cols(self):
        print(get_typed_cols(go('Integer')))
        print(get_typed_cols(go('String')))
        print(get_typed_cols(go('Point')))
        print(get_typed_cols(go('Role')))
        print(get_typed_cols(go('RoleIO')))
        print(get_typed_cols(go('Log')))
        print(get_typed_cols(go('Meta')))
# ---
def accelerator_type_friendly(accel_type: int) -> str:
    """Return human-friendly accelerator type name.

    Examples:
        ACCELERATOR_TYPE_UNSPECIFIED (0) -> "unspecified"
        ACCELERATOR_TYPE_CPU (1) -> "cpu"
        ACCELERATOR_TYPE_GPU (2) -> "gpu"
        ACCELERATOR_TYPE_TPU (3) -> "tpu"
    """
    name = accelerator_type_name(accel_type)
    if name.startswith("ACCELERATOR_TYPE_"):
        return name.replace("ACCELERATOR_TYPE_", "").lower()
    return name.lower()
# ---
def UnregisterEndpoint(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __str__(self):
        'Returns the text.'
        return self.text
# ---
def _maybe_swap(self, op: ast.AST, group: tuple) -> ast.AST:
        if type(op) in group and self.rng.random() < self.swap_prob:
            alternatives = [cls for cls in group if cls is not type(op)]
            if alternatives:
                self._changed = True
                return self.rng.choice(alternatives)()
        return op
# ---
def sources_restart(self):
        node = bottle.request.body.getvalue().decode('utf-8')
        self.ndb.sources[node].start()
# ---
def enc8(x):
	if (x > 255):
		raise Exception("The integer %d cannot be encoded on 8 bits." % x)
	else:
		return x
# ---
def test_line_start_escaping():
    """Tests -, #, + at the beginning of lines."""
    test_cases = [
        ("- hyphen", "\\- hyphen"),
        ("+ plus", "\\+ plus"),
        ("# header", "\\# header"),
        ("Normal line - not escaped", "Normal line - not escaped"),
    ]
    for text, expected in test_cases:
        assert minimal_markdown_escape(text) == expected
# ---
def test_capture_not_started_but_reset():
    capsys = StdCapture()
    capsys.stop_capturing()
# ---
def _slugify(value: str) -> str:
    slug = SLUGIFY_PATTERN.sub("_", value.lower()).strip("_")
    return slug or "dataset"
# ---
def rust_minhash_pipeline(batch: pa.RecordBatch) -> int:
    pipeline = [
        Transformation.CleanText(input_col="text", output_col="clean"),
        Transformation.MinHash(input_col="clean", output_col="sig", num_perms=286, ngram_size=5, seed=42),
        Transformation.MinHashLSH(input_col="sig", output_col="buckets", num_bands=26),
    ]
    res = dupekit.transform(batch, pipeline)
    return len(res)
# ---
def test_no_date_range(self):
        """Test when no start or end date is provided"""
        self.assertTrue(check_create_time("2023-01-01 12:00:00 PDT"))
# ---
def from_hf_config(rope_theta, config: dict | None) -> "RotaryEmbeddingsConfig":
        if config is None:
            return DefaultRotaryEmbeddingsConfig(theta=rope_theta)
        tpe = config.get("rope_type") or config.get("type") or "default"
        return RotaryEmbeddingsConfig.get_choice_class(tpe).make_from_hf_config(rope_theta, config)
# ---
def test_binary_converts_unicode(self):
        """Binary will convert unicode to bytes"""
        b = Binary("a")
        self.assertTrue(isinstance(b.value, bytes))
# ---
def test_str_split_box_df(self):
        def test_impl(df):
            return pd.DataFrame({'B': df.A.str.split(',')})

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df).B, test_impl(df).B, check_names=False)
# ---
def clause(self):
        return self.clause_with_joiner('and')
# ---
def __init__(self, window):
        from time import time
        from pyglet.text import Label
        self.label = Label('', x=10, y=10, 
                           font_size=24, bold=True,
                           color=(127, 127, 127, 127))

        self.window = window
        self._window_flip = window.flip
        window.flip = self._hook_flip

        self.time = 0.0
        self.last_time = time()
        self.count = 0
# ---
def __getitem__(self, item):
		if isinstance(item, slice):
			keys = sorted(self._addresses.keys())[item]
			return [self[k] for k in keys]

		if item not in self._values:
			self._parse_row(item)

		return self._values[item]
# ---
def sort(self, items):
        """Sort the list of objects and return a list.
        """
        return sorted(items)
# ---
def exit_with_code(code: int):
        sys.exit(code)
# ---
def odd_values_string(str):
  result = "" 
  for i in range(len(str)):
    if i % 2 == 0:
      result = result + str[i]
  return result
# ---
def scan_checkpoint_policy_encode(policy: ScanCheckpointPolicy):
        return policy
# ---
def readlineCR(uart):
	line = b''
	while True:
		byte = uart.read()
		line += byte
		if byte == b'\r':
			return line
# ---
def ones_like(a: NamedArray, dtype=None) -> NamedArray:
    """Creates a NamedArray with all elements set to 1"""
    return NamedArray(jnp.ones_like(a.array, dtype=dtype), a.axes)
# ---
import re
def occurance_substring(text,pattern):
 for match in re.finditer(pattern, text):
    s = match.start()
    e = match.end()
    return (text[s:e], s, e)
# ---
def test_clone_volume(self):
        drv = self._driver
        mox = self._prepare_clone_mock('pass')

        mox.ReplayAll()

        volume_name = 'volume_name'
        clone_name = 'clone_name'
        volume_id = volume_name + str(hash(volume_name))
        share = 'ip:/share'

        drv._clone_volume(volume_name, clone_name, volume_id, share)

        mox.VerifyAll()
# ---
def fn(config: MyConfig | None):
        append_log(log, config)
# ---
def get_array_counts_from_metrics(result):
    filter = beam.metrics.MetricsFilter().with_name("completed_tasks")
    metrics = result.metrics().query(filter)["counters"]
    new_array_counts = {
        metric.key.metric.namespace: metric.result for metric in metrics
    }
    return new_array_counts
# ---
def all_reduce_mean(x):
    world_size = get_world_size()
    if world_size > 1:
        dist.all_reduce(x)
        x /= world_size

    return x
# ---
def recurPowerNew(base, exp):

    # Base case is when exp = 0
    if exp <= 0:
        return 1

    # Recursive Call
    elif exp % 2 == 0:
        return recurPowerNew(base*base, exp/2)

    return base * recurPowerNew(base, exp - 1)
# ---
def name(self) -> str:
        return "lithops"
# ---
def __init__(self, result, callbacks):
        self.result = result
        self.callbacks = callbacks
        self.array_counts = {}
        self.scheduler = sched.scheduler(time.time, time.sleep)
        poll(self, self.result)  # poll immediately
        self.scheduler.run()
# ---
def expanded_path(self, run_id) -> str:
        if self.append_run_id_to_base_path:
            return os.path.expanduser(os.path.join(self.base_path, run_id))
        return os.path.expanduser(self.base_path)
# ---
def unsize_axes(axis_spec: AxisSelection, to_unsize: AxisSelection) -> AxisSelection: ...
# ---
def broadcast_trick(func):
    """Apply Dask's broadcast trick to array API functions that produce arrays
    containing a single value to save space in memory.

    Note that this should only be used for arrays that never mutated.
    """
    inner = partial(_broadcast_trick_inner, func)
    inner.__doc__ = func.__doc__
    inner.__name__ = func.__name__
    return inner
# ---
def test_shard_map_multiple_args():
    mesh = Mesh(np.array(jax.devices()), (ResourceAxis.DATA,))

    def fn(a, b):
        return a + b

    sm = hax.shard_map(fn, mesh=mesh, check_rep=False)
    x = hax.ones(Dim)
    y = hax.arange(Dim)
    with axis_mapping({"dim": ResourceAxis.DATA}), mesh:
        out = sm(x, y)

    assert out.axes == (Dim,)
    assert jnp.allclose(out.array, x.array + y.array)
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.field == other.field and \
            self.ascending == other.ascending
# ---
def from_job_id(cls, job_id: JobName) -> "Namespace":
        """Derive namespace from hierarchical job ID.

        The namespace is the first component of the job ID hierarchy.
        For example:
            JobName.from_string("/abc123/worker-0") -> Namespace("abc123")

        Args:
            job_id: Hierarchical job ID

        Returns:
            Namespace derived from root job ID

        Raises:
            ValueError: If job_id is empty
        """
        return cls(job_id.namespace)
# ---
def test_fully_buffered_result_proxy(self):
        self._test_proxy(_result.FullyBufferedResultProxy)
# ---
def fake_is_vdi_pv(*args, **kwargs):
            return should_return
# ---
def should_display_status_to_user(self):
        """
        Whether or not the status should be displayed to the user.
        """
        return False
# ---
def create_weight_transfer_config():
    return WeightTransferConfig(
        mode=WeightTransferMode.ARROW_FLIGHT,
        sync_interval_steps=1,
        max_weight_transfer_wait_time=10.0,
    )
# ---
def __init__(self, ray_method: Any):
        self._ray_method = ray_method
# ---
def __create_input(self, main_root):
		content = self.__manager.load_content()
		input_dir = os.path.join(main_root,'input')
		try:
			content.get(self.__task.data['id'], 'input', os.path.join(main_root, 'input'))
		except:
			safe_mkdir(input_dir)
# ---
def load_state_dict(self, state: dict[str, torch.Tensor]) -> None:
        """Load state from ``state_dict``."""
        if "per_channel_scale" in state:
            self._per_channel_scale = state["per_channel_scale"].to(self._device)
# ---
def find_clusters(atimes):
    foo = Counter()
    bar = dict()
    for i in xrange(120, 3660, 10):
        clusters = get_clusters(atimes, i)
        cs = len(clusters)
        foo[cs] += 1

        # note first occurance of this cluster size.
        if cs not in bar:
            bar[cs] = i
        # print(len(atimes), i, cs)

    return bar[foo.most_common()[0][0]]
# ---
def __lt__(self, other: "Timestamp") -> bool:
        return self._epoch_ms < other._epoch_ms
# ---
def get_logs(self, task_id: str, start_line: int = 0) -> list[cluster_pb2.Worker.LogEntry]: ...
# ---
def test_lots_of_ellipsis():
    partial_order = ("apple", ..., "banana", ..., "cherry", ...)
    candidates = ("banana", "orange", "cherry", "apple", "grape")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("apple", "banana", "orange", "cherry", "grape")
# ---
def testRaiseTraceback(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        import sys
        try:
          try:
            raise Exception
          except:
            e, _, tb = sys.exc_info()
            raise e, None, tb
        except:
          e2, _, tb2 = sys.exc_info()
        assert e is e2
        assert tb is tb2""")))
# ---
def generate_hash_from_pair(chosen, rejected) -> str:
    """Generate a hash from chosen and rejected message lists."""
    return hashlib.sha256((str(chosen) + str(rejected)).encode()).hexdigest()
# ---
def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self._stop_tokens = None
        self.max_tokens = 512
# ---
def __iter__(self):
        for f in self._files:
            yield f
# ---
def get_block_by_number(self, block_number) -> Optional[Block]:
        with self.lock:
            return self._state.get_block_by_number(block_number)
# ---
def increment(self, stats, sample_rate=1):
        """
        Increments one or more stats counters
        """
        self.update_stats(stats, 1, sample_rate)
# ---
def get_next(next_link=None):
            request = prepare_request(next_link)

            pipeline_response = self._client._pipeline.run(request, stream=False, **kwargs)
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                raise HttpResponseError(response=response, error_format=ARMErrorFormat)

            return pipeline_response
# ---
def check(string): 
  if len(set(string).intersection("AEIOUaeiou"))>=5: 
    return ('accepted') 
  else: 
    return ("not accepted")
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        k_up, k_down = maybe_rng_split(key, 2)
        x = self.up_proj(x, key=k_up)
        x = self.act_fn(x)
        x = self.down_proj(x, key=k_down)
        return x
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "google/gemma-3-1b-pt"
    ) -> HFCheckpointConverter["Gemma3Config"]:  # type: ignore
        return HFCheckpointConverter(
            self,
            reference_checkpoint=ref_checkpoint,
            trust_remote_code=True,
            HfConfigClass=HfGemma3Config,
        )
# ---
def matchfn(self, f):
        # Some information is lost in the superclass's constructor, so we
        # can not accurately create the matching function for the subdirectory
        # from the inputs. Instead, we override matchfn() and visitdir() to
        # call the original matcher with the subdirectory path prepended.
        return self._matcher.matchfn(self._path + "/" + f)
# ---
def key_function(out_key):
        out_coords = out_key[1:]

        # return a tuple with a single item that is a list of input keys to be combined
        in_keys = [
            list(
                range(
                    bi * split_every,
                    min((bi + 1) * split_every, numblocks),
                )
            )
            for bi in out_coords
        ]
        return ([(name,) + tuple(p) for p in product(*in_keys)],)
# ---
def alive_threads(self) -> list[ManagedThread]:
        """Return threads that are still alive, including those in child containers."""
        with self._lock:
            threads = list(self._threads)
            children = list(self._children)

        alive = [t for t in threads if t.is_alive]
        for child in children:
            alive.extend(child.alive_threads())
        return alive
# ---
def max_len_per_seq(self) -> int:
        return self.page_size * self.pages_per_seq
# ---
def test_transaction_tlocal_engine_ctx_rollback(self):
        fn = self._trans_rollback_fn()
        engine = engines.testing_engine(options=dict(
                                strategy='threadlocal',
                                pool=testing.db.pool))
        ctx = engine.begin()
        assert_raises_message(
            Exception,
            "breakage",
            testing.run_as_contextmanager, ctx, fn, 5, value=8
        )
        self._assert_no_data()
# ---
def output_heading(self):
        return self.renderer.header(
            self.inline(self.token['text']),
            self.token['level'],
            self.token['text'],
        )
# ---
def loss_fn(model, completions=completions, advantages=advantages, key=key):
                return compute_policy_gradient_loss(model, prompt_named, completions, advantages, key)
# ---
def _compute_related(self, records):
        """ Compute the related field ``self`` on ``records``. """
        for record in records:
            value = record
            # traverse the intermediate fields, and keep at most one record
            for name in self.related[:-1]:
                value = value[name][:1]
            record[self.name] = value[self.related[-1]]
# ---
def _call_fn(self, indices: Sequence[int], items):
        if "key" in self._extra_kwargs:
            key = self._maybe_fold_in_key(self._extra_kwargs["key"], indices)
            kwargs = {**self._extra_kwargs, "key": key}
        else:
            kwargs = self._extra_kwargs
        return self.fn(items, *self._extra_args, **kwargs)
# ---
def _is_frac(expr: str) -> bool:
    return bool(re.search(r"^-?[0-9]+.?/0*[1-9][0-9]*.?$", expr))
# ---
def __next__(self):
        if self._iterator is None:
            if not inspect.isgenerator(self._result):
                raise StopIteration
            self._iterator = iter(self._result)
        return next(self._iterator)
# ---
def _jit_paged_decode(attn, x, pos_ids, cache: KvPageCache, binfo: PageBatchInfo) -> tuple[NamedArray, KvPageCache]:
    return attn.paged_decode(x, cache, binfo, pos_ids=pos_ids, key=jrandom.PRNGKey(2))
# ---
def test_url_generation_with_special_chars(self):
        """Test URL generation with special characters."""
        loc = S3Location(bucket="test-bucket", path="data/test file.zarr")
        assert loc.url() == "s3://test-bucket/data/test%20file.zarr"
# ---
def url(self) -> str:
        path = quote(self.path.lstrip("/"))
        bucket = quote(self.bucket, safe="")
        return f"s3://{bucket}/{path}"
# ---
def wait_for_image_status(self, image_id, status):
        """Waits for an image to reach a given status."""
        waiters.wait_for_image_status(self, image_id, status)
# ---
def test_host_shutdown(self):
        self._test_host_action(self.conn.host_power_action, 'shutdown')
# ---
def ptp(x, axis=0):
    return _Nptp(x, axis)
# ---
def _kindpatsalwaysmatch(kindpats):
    """ "Checks whether the kindspats match everything, as e.g.
    'relpath:.' does.
    """
    for kind, pat, source in kindpats:
        # TODO: update me?
        if pat != "" or kind not in ["relpath", "glob"]:
            return False
    return True
# ---
def binding_genetic_modification_2(testapp, lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'binding',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
# ---
def convert_to_export(self, value, env):
        if not value:
            return ''
        return self.from_string(value) if env.context.get('export_raw_data') else ustr(value)
# ---
def mock_data_source():
    """Create a minimal 8x8 DataSource for testing."""
    return _create_mock_data_source("test_8x8", grid_size=8)
# ---
def ndim(a: A) -> A:
    return wrap_elemwise_unary(jnp.ndim, a)
# ---
def volume_cylinder(r,h):
  volume=3.1415*r*r*h
  return volume
# ---
def __init__(self, source: Iterable[T], operations: list[LogicalOp] | None = None):
        """Create a dataset from a source and optional operations.

        Args:
            source: Source data iterable
            operations: List of operations to apply
        """
        self.source = source
        self.operations = operations or []
# ---
def optimized_func_der(self, r):
        p = Pool(processes=4)

        self_args = [self] * len(r)
        i_args = range(len(r))
        r_args = [r] * len(r)

        return np.array(p.map(optimized_func_i_der,
                              zip(self_args, r_args, i_args)))
# ---
def test_startswith_unescaped(self):
        col = self.tables.some_table.c.data
        self._test(col.startswith("ab%c"), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10})
# ---
def __init__(
        self,
        loss_fn: LossFnWithMask,
        *,
        limit: float | None,
        device: torch.device,
        num_channels: int,
    ):
        self.loss_fn = loss_fn
        self._device = device
        self._per_channel_scale: Float[torch.Tensor, " var"] = torch.ones(
            num_channels, device=self._device
        )
        self._limit = limit
# ---
def __call__(self, *, input_ids, attn_mask, pos_ids, key):
        return DummyModelOutput(self._logits)
# ---
def cached_resource_by_id(self, resource_id, name=None):
        return VOLUME_TYPES[resource_id]
# ---
def first_Factorial_Divisible_Number(x): 
    i = 1;
    fact = 1; 
    for i in range(1,x): 
        fact = fact * i 
        if (fact % x == 0): 
            break
    return i
# ---
def __enter__(self):
        """Start controller with autoscaler (or connect to remote)."""
        if self._remote_url:
            return self

        config = self._load_or_default_config()
        config = make_local_config(config)
        self._manager = ClusterManager(config)
        self._manager.start()
        return self
# ---
def vref(self, vr):
        self._Vref = vr
# ---
def test_replicas_on_job_request():
    """JobRequest.replicas is the gang-scheduling count (maps to num_slices for TPU)."""
    request = JobRequest(
        name="multi-slice",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=TpuConfig(variant="v4-8")),
        replicas=4,
    )
    assert request.replicas == 4
# ---
def pbar_logger(iterable=None, desc="train", **tqdm_mkwargs):
    kwargs = copy.copy(tqdm_mkwargs)
    if "desc" not in kwargs:
        kwargs["desc"] = desc
    if "iterable" not in kwargs:
        kwargs["iterable"] = iterable

    _tqdm_logging_one_time_setup()
    pbar = tqdm(**kwargs)

    def update_pbar(step: StepInfo):
        pbar.update(step.next_step - pbar.n)
        pbar.set_postfix(loss=jnp_to_python(step.loss))

    return update_pbar
# ---
def swapped_date(date, first, second):
    attrs = {DAY: date.day, MONTH: date.month, YEAR: last_two_digits(date.year)}
    newattrs = {first: attrs[second], second: attrs[first]}
    if YEAR in newattrs:
        newattrs[YEAR] += 2000
    return date.replace(**newattrs)
# ---
def mock_data_source_large():
    """Create a larger 16x16 DataSource for multi-scale testing."""
    return _create_mock_data_source("test_16x16", grid_size=16)
# ---
def test_moveaxis(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.moveaxis(a, [0, -1], [-1, 0])
    assert_array_equal(
        b.compute(),
        np.moveaxis(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), [0, -1], [-1, 0]),
    )
# ---
def test_multiple_empty_sets(self):
        # test that any anonymous aliasing used by the dialect
        # is fine with duplicates
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.in_(bindparam("q", expanding=True)))
            .where(table.c.y.in_(bindparam("p", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": [], "p": []})
# ---
def _serialize_config(self) -> str:
        """Serialize cluster config to YAML for the controller VM."""
        import yaml

        return yaml.dump(config_to_dict(self.config), default_flow_style=False)
# ---
def get_bucket_location(bucket_name_or_path):
    """Get the GCS bucket's location."""
    if bucket_name_or_path.startswith("gs://"):
        bucket_name = split_gcs_path(bucket_name_or_path)[0]
    else:
        bucket_name = bucket_name_or_path

    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    # this returns upper case regions, which isn't consistent with the rest of the codebase
    return bucket.location.lower()
# ---
def testDeleteClassLocal(self):
    self.assertEqual((0, 'False\n'), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          bar = 'baz'
          del bar
        print hasattr(Foo, 'bar')""")))
# ---
def oindex(self):
        return self.array.oindex
# ---
def _gcloud_delete_tpu(project_id: str, zone: str, name: str) -> bool:
    cmd = [
        "gcloud",
        "compute",
        "tpus",
        "tpu-vm",
        "delete",
        name,
        f"--project={project_id}",
        f"--zone={zone}",
        "--quiet",
    ]
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        logger.warning("Failed to delete TPU %s in zone %s: %s", name, zone, result.stderr.strip())
        return False
    return True
# ---
def intersects(self, bbox, srs):
        bbox = self._bbox_in_coverage_srs(bbox, srs)
        return bbox_intersects(self.bbox, bbox)
# ---
def port_job():
            info = get_job_info()
            if info is None:
                raise ValueError("JobInfo not available")
            # Verify ports are set
            if "http" not in info.ports or "grpc" not in info.ports:
                raise ValueError(f"Ports not set: {info.ports}")
            # Verify they're valid port numbers
            assert info.ports["http"] > 0
            assert info.ports["grpc"] > 0
# ---
def test_bound_in_heterogeneous_two_tuple(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.z).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(
            stmt,
            [(2,), (3,), (4,)],
            params={"q": [(2, "z2"), (3, "z3"), (4, "z4")]},
        )
# ---
def unembed(self, input_embeds: NamedArray):
        """
        Unembed the input embeddings back to the vocabulary space.

        Equivalent to `input_embeds.dot(self.weight, axis=self.Embed)`.
        """
        return input_embeds.dot(self.weight, axis=self.Embed) * self.reparam.unembed_active_scale
# ---
def transform_records():
        """Generator that yields transformed records."""
        for raw_row in shard_dataset:
            transformed_row = transform_row(raw_row, task.cfg, adapter)
            if transformed_row is not None:
                yield transformed_row.model_dump()
# ---
def __init__(self, client: ActorClient, method_name: str):
        self._client = client
        self._method_name = method_name
# ---
def test_after_end_date(self):
        """Test when create_time is after the end date"""
        self.assertFalse(check_create_time("2023-02-01 00:00:01 PST", "2023-01-01", "2023-01-31"))
# ---
def create_tpu_group(vms: list[MagicMock]) -> VmGroupProtocol:
        from iris.time_utils import Timestamp

        return TpuVmGroup(
            group_id="test-slice-001",
            scale_group="test-group",
            zone="us-central1-a",
            project_id="test-project",
            vms=vms,
            vm_registry=registry,
            created_at=Timestamp.from_ms(1234567890),
        )
# ---
def test_negative(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.negative(a)
    run_operation(tmp_path, executor, "negative", b)
# ---
def vmap_via(self, fn: VmapFunction[M, P, OutputT_co]) -> Callable[P, OutputT_co]: ...
# ---
def frame_back_step(self):
        self.command('frame_back_step')
# ---
def _default_stock_location(self, cr, uid, context=None):
        try:
            location_model, location_id = self.pool.get('ir.model.data').get_object_reference(cr, uid, 'stock', 'stock_location_stock')
            with tools.mute_logger('openerp.osv.orm'):
                self.pool.get('stock.location').check_access_rule(cr, uid, [location_id], 'read', context=context)
        except (orm.except_orm, ValueError):
            location_id = False
        return location_id
# ---
def copy(a: A) -> A:
    return wrap_elemwise_unary(jnp.copy, a)
# ---
def test_brackets_in_strings_ignored():
    assert brackets_balanced('"(not a bracket"')
    assert brackets_balanced("'[still balanced'")
# ---
def test_ports_list(self):
        self.assertEqual(self.mda.get_ports_list(), [0x03B0, 0x03B1, 0x03B2, 0x03B3,
                                                     0x03B4, 0x03B5, 0x03B6, 0x03B7,
                                                     0x03B8, 0x03B9, 0x03BA, 0x03BB])
# ---
def format_response(response: Message) -> str:
    """Format a protobuf response message as JSON."""
    return json_format.MessageToJson(
        response,
        preserving_proto_field_name=True,
        indent=2,
    )
# ---
def delete_image_metadata_item(self, image_id, key):
        """Deletes a single image metadata key/value pair."""
        resp, body = self.delete("images/%s/metadata/%s" %
                                 (str(image_id), key))
        self.validate_response(schema.delete, resp, body)
        return service_client.ResponseBody(resp, body)
# ---
def test_dense(self):
    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 2))

    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 2))

    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(None, None, 2))

    testing_utils.layer_test(
        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 5, 2))
# ---
def add_words(self, words, update=True):
        """Add a word or words to the list of words to auto-complete."""
        for word in words:
            if self.isusable(word):
                self._word_freq[word] += 1
        if update:
            self._update_word_list()
# ---
def sum_series(n):
  if n < 1:
    return 0
  else:
    return n + sum_series(n - 2)
# ---
def _update_selected_pane(self):
        self.import_table.refresh()
        self._refresh_swap_list_items()
        self.view.update_selected_pane()
        self.view.set_swap_button_enabled(self.can_perform_swap())
# ---
def test_distinct(data):
  if len(data) == len(set(data)):
    return True
  else:
    return False;
# ---
def embed(self, input_ids, *, key):
        input_embeds = self.token_embeddings(input_ids)
        input_Pos = input_ids.resolve_axis("position")
        position_embeds = self.position_embeddings.embed(hax.arange(input_Pos))
        x = input_embeds + position_embeds

        return x
# ---
def _is_special_module(module):
        return _is_lora_compatible_module(module) or isinstance(module, hnn.Stacked)
# ---
def f(x):
        x_ref = hax.new_ref(hax.zeros(X))

        def scan_fn(_, i):
            slice = x_ref.slice({"x": i})
            slice[...] = jnp.sin(x * i)
            return None, None

        hax.scan(scan_fn, X)(None, jnp.arange(X.size))
        return x_ref[...].sum().scalar()
# ---
def _add_default_env_variables(env: dict, default_env: dict | None):
    if default_env is not None:
        default_env = deepcopy(default_env)
        env = mergedeep.merge(default_env, env)

    # Ray gets mad if the values aren't all strings, but e.g. ints
    env = {str(k): str(v) for k, v in env.items()}
    return env
# ---
def test_map_shard_error_propagation(backend):
    """Test that exceptions in map_shard functions propagate correctly."""

    def failing_generator(items):
        for item in items:
            if item == 3:
                raise ValueError("Test error")
            yield item

    ds = Dataset.from_list([list(range(1, 6))]).flat_map(lambda x: x).map_shard(failing_generator)

    with pytest.raises(ValueError, match="Test error"):
        list(Backend.execute(ds, context=backend))
# ---
def compute_logits(model: LmHeadModel, example: LmExample):
            model = trainer.mp.cast_to_compute(model)
            activations = model.activations(example.tokens, key=None, attn_mask=example.attn_mask)
            head = model.get_lm_head()
            logits = hax.dot(activations, head, axis=model.Embed)
            return logits
# ---
def shutdown(self) -> None:
        """Terminate all actor jobs."""
        ...
# ---
def test_nunique(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df.A[2] = 0
            return df.A.nunique()

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        # test compile again for overload related issues
        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
# ---
def userdatabase(user) :
    db = database
    if not os.path.isfile(db):
        return None
    return db
# ---
def __init__(self):
        self.explore_rate = self.MAX_EXPLORATION_RATE
        self.brain = Brain()
        self.memory = Memory(self.MEMORY_CAPACITY)
        self.steps = 0
# ---
def join_path(output_path: str, name: str | None) -> str:
        return os.path.join(output_path, name) if name else output_path
# ---
def test_group_by_composed(self):
        table = self.tables.some_table
        expr = (table.c.x + table.c.y).label("lx")
        stmt = (
            select([func.count(table.c.id), expr])
            .group_by(expr)
            .order_by(expr)
        )
        self._assert_result(stmt, [(1, 3), (1, 5), (1, 7)])
# ---
def handler(self):
                pass
# ---
def mock_open(path, mode="rb"):
            if path in files:
                import io

                return io.BytesIO(files[path])
            raise FileNotFoundError(f"File not found: {path}")
# ---
def find_even_Pair(A,N): 
    evenPair = 0
    for i in range(0,N): 
        for j in range(i+1,N): 
            if ((A[i] ^ A[j]) % 2 == 0): 
                evenPair+=1
    return evenPair;
# ---
def inc_qps():
    pass
# ---
def test_method(self):
        self.assertEqual("POST", self.post.get_method())
        self.assertEqual("GET", self.get.get_method())
# ---
def vm_ops(self) -> PlatformOps:
        return _GcpPlatformOps(self._platform, self._label_prefix)
# ---
def num_channels(self):
    """Number of color channels."""
    return 3
# ---
def Embed(self) -> Axis:
        return self.config.Embed
# ---
def __init__(self, alpha = 1, beta = 1, lmbda = 0):
        d = 1 + ChiSquareDistr(2.0 * beta) / NoncentralChiSquareDistr(2 * alpha, lmbda)
        super(NoncentralBetaDistr, self).__init__(d)
        self.alpha = alpha
        self.beta = beta
        self.lmbda = lmbda
# ---
def __init__(self, kind, match):
        """Initiates Check instance.

        :param kind: The kind of the check, i.e., the field before the
                     ':'.
        :param match: The match of the check, i.e., the field after
                      the ':'.
        """

        self.kind = kind
        self.match = match
# ---
def intersection_array(array_nums1,array_nums2):
 result = list(filter(lambda x: x in array_nums1, array_nums2)) 
 return result
# ---
def __getstate__(self) -> dict:
        # Serialize only the discovery parameters - discovery state resets on deserialize
        return {
            "name": self._name,
            "count": self._count,
            "job_id": self._job_id,
        }
# ---
def __init__(self, config: RolloutTrackerConfig, run_id: str):
        self._run = wandb.init(
            entity=config.entity,
            project=config.project,
            name=config.name,
            tags=config.tags,
            id=run_id,
            resume="allow",
            mode=config.mode,
        )
# ---
def tearDown(self):
        super(SimpleInstanceTest, self).tearDown()
        CONF.network_label_regex = self.orig_conf
        CONF.ip_start = None
# ---
def _get_bias_dropout_scale(self):
    if self.training:
      return bias_dropout_add_scale_fused_train
    else:
      return  bias_dropout_add_scale_fused_inference
# ---
def power(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.power(x1, x2)
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        """Returns a dict mapping eqx.Module keys to torch keys that need to be renamed for serialization"""
        return {}
# ---
def rust_compute_paragraph_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> pa.RecordBatch:
    pipeline = [
        dupekit.Transformation.SplitParagraphs(text_col=text_col, id_col=id_col),
        dupekit.Transformation.Hash(input_col="paragraph_text", output_col="hash", algo=dupekit.HashAlgorithm.Xxh3_128),
    ]
    return dupekit.transform(batch, pipeline)
# ---
def distinct_values(schedule: Sequence[ScheduleStep[T]] | T) -> set[T]:
    if not isinstance(schedule, Sequence) or (schedule and not isinstance(schedule[0], ScheduleStep)):
        return {schedule}  # type: ignore
    return set(step.value for step in schedule)
# ---
def find_user_password(self, realm, authuri):
        self.target_realm = realm
        self.target_url = authuri
        return self.user, self.password
# ---
def _substitute_implementations():
        """Replaces implementation ids with input_types."""
        impls = {}
        for id_ in input_type['implementations']:
            type_ = input_types[id_]
            impls[type_['name']] = type_
        input_type['implementations'] = impls
# ---
def symmetric_percentile_norm(
    data: Any, percentile: float = 98.0, fallback: float = 1.0
) -> colors.Normalize:
    flat = _flatten_for_norm(data)
    flat = flat[~np.isnan(flat)]
    if flat.size == 0:
        max_abs = fallback
    else:
        max_abs = np.percentile(np.abs(flat), percentile)
        if not np.isfinite(max_abs) or max_abs == 0:
            max_abs = fallback
    return colors.Normalize(vmin=-max_abs, vmax=max_abs)
# ---
def isusable(self, word):
        """Returns a value indicating if the given word should be kept as a
        suggestion for autocomplete."""
        return len(word) > self.comp_len + 2
# ---
def resolve(self, host, port, *args, **kwargs):
        if (host, port) in self.mapping:
            host, port = self.mapping[(host, port)]
        elif host in self.mapping:
            host = self.mapping[host]
        return self.resolver.resolve(host, port, *args, **kwargs)
# ---
def with_prefix(prefix: str | None, leaf: str | None) -> str | None: ...
# ---
def test_equal(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    c = xp.equal(a, b)
    assert_array_equal(c.compute(), np.full((3, 3), True))
# ---
def LoadFromFile(self, file):
        self.fs, self.s = wav.read(file)
        self.sLength, self.nChans = self.s.shape
# ---
def __init__(self, min_value, max_value, instance=None,
        can_delete_vote=True, template='ratings/like_widget.html', attrs=None):
        super(LikeWidget, self).__init__(attrs)
        self.min_value = min_value
        self.max_value = max_value
        self.instance = instance
        self.can_delete_vote = can_delete_vote
        self.template = template
# ---
def tick(self, ts: int | None = None) -> None:
        """Advance all VM group state transitions.

        Call this to simulate time passing and VMs completing boot/init.
        """
        ts = ts or Timestamp.now().epoch_ms()
        with self._lock:
            for fake_vm_group in self._slices.values():
                fake_vm_group.tick(ts)
# ---
def kill_task(self, task_id: str, term_timeout_ms: int = 5000) -> bool: ...
# ---
def init(config: Olmo2Config, *, key) -> "Olmo2Transformer":
        S = Stacked
        if not config.scan_layers:
            from haliax.nn.scan import BlockSeq

            S = BlockSeq

        layers = S.init(config.Layers, Olmo2DecoderLayer, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = config.mk_LayerNorm(config.Embed)

        return Olmo2Transformer(config, layers, ln_f)
# ---
def show_text(self, string, duration='-', level=None):
        self.command('show_text', string, duration, level)
# ---
def create(self, model: M) -> ModelAveraging[M]:
        pass
# ---
def _getVolumeForVoiceType(self, voiceType):
        """Gets the volume (gain) value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM

        Returns the volume (gain) value for the given voice type, or
        None if not set.
        """

        return self._getKeyValueForVoiceType(voiceType, acss.ACSS.GAIN)
# ---
def _clip_fn(u):
            clip_denom = hax.maximum(1.0, hax.sqrt(hax.mean(u * u)) / threshold)
            return u / clip_denom
# ---
def _docker_container_running(container_name: str) -> bool:
    result = subprocess.run(
        ["docker", "inspect", "-f", "{{.State.Running}}", container_name],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        return False
    output = result.stdout.strip().lower()
    if output == "true":
        return True
    if output == "false":
        return False
    raise RuntimeError(f"Unexpected output from docker inspect: {output!r}")
# ---
def test_make_blockwise_key_function_contract_1d():
    func = lambda x: 0

    key_fn = make_blockwise_key_function(
        func, "z", "j", "x", "ij", numblocks={"x": (1, 2)}
    )

    graph = make_blockwise_graph(func, "z", "j", "x", "ij", numblocks={"x": (1, 2)})
    check_consistent_with_graph(key_fn, graph)
# ---
def minimum_Length(s) : 
    maxOcc = 0
    n = len(s) 
    arr = [0]*26
    for i in range(n) : 
        arr[ord(s[i]) -ord('a')] += 1
    for i in range(26) : 
        if arr[i] > maxOcc : 
            maxOcc = arr[i] 
    return n - maxOcc
# ---
def test_cold_migration(self):
        """Test cold migrating server and then confirm the migration"""
        self._test_cold_migrate_server(revert=False)
# ---
def output(self):
        return luigi.s3.S3Target(path='{}/{}/{}-{}.zip'.format(self.root_path,
                                                               self.raw_path,
                                                               str(self.month).zfill(2),
                                                               self.year))
# ---
def join_key(prefix, k):
    if k is None:
        return prefix
    return f"{prefix}.{k}" if prefix else k
# ---
def concat_axes(a1: AxisSelection, a2: AxisSelection) -> AxisSelection:
    pass
# ---
def saved_fd(fd):
    new_fd = os.dup(fd)
    try:
        yield
    finally:
        os.dup2(new_fd, fd)
        os.close(new_fd)
# ---
def compute_mean_std(predictions):
    stacked = np.stack(predictions, axis=0)
    return np.mean(stacked, axis=0), np.std(stacked, axis=0)
# ---
def cli():
    """TPU VM Manager - Manage preemptible TPU VMs for GitHub Actions CI."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[True] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[True] = ...,
) -> tuple[NamedArray, Array, Array, Array, Array]: ...
# ---
def list_pilots (self, ptype=ANY) :
        """
        List IDs of data and/or compute pilots
        """

        raise Exception ("%s.list_pilots() is not implemented" % self.__class__.__name__)
# ---
def do(src):
            # match positions where slot_ids == src; take first
            eq = (slot_ids == src).array
            idx = jnp.nonzero(eq, size=1, fill_value=INVALID)[0][0]
            return idx
# ---
def Find_Min_Length(lst):  
    minLength = min(len(x) for x in lst )
    return minLength
# ---
def create_actor(self) -> ActorHandle:
        assert self._slice_info
        slice_name = self._slice_info.slice_name
        return TPUHostActor.options(resources={slice_name: 1}, num_cpus=0.0).remote(self._slice_info)
# ---
def rebuild(c):
    """`build` with the delete switch"""
    c.run('pelican -d -s pelicanconf.py')
# ---
def close(self):
        self.close_request.set()
# ---
def generate_examples(self, n_examples: int, rng: np.random.Generator, tokenizer=None) -> list[dict[str, str]]:
        examples = []
        for _ in range(n_examples):
            a = rng.integers(self.min_val, self.max_val)
            b = rng.integers(self.min_val, self.max_val)
            result = a + b
            prompt = f"What is {a}+{b}? Output just the number:"
            answer = str(result)
            examples.append({"prompt": prompt, "answer": answer})
        return examples
# ---
def __post_init__(self):
        if isinstance(self.name, Axis):
            raise ValueError(f"Axis name cannot be an Axis object: {self.name}")
# ---
def test_evaluate_existing_column(self):
        expr = col("name")
        assert expr.evaluate({"name": "alice"}) == "alice"
# ---
def build(self, ctx: LrScheduleContext):
        return _inv_decay_schedule(ctx.learning_rate, ctx.min_lr, ctx.decay_steps)
# ---
def body(i, rc):
                page = src_pages["page", i]

                def inc(rc):
                    return rc.at["page", page].add(1)

                return jax.lax.cond(is_valid(page).scalar(), inc, lambda x: x, rc)
# ---
def euler(f, x0, y0, xf, n):
    pas = (xf - x0) / n
    courbe = XY()
    courbe.title = "Methode d Euler"
    courbe.add("Solution approchee", listeEuler(f, x0, y0, pas, n))
    courbe.render_to_file("courbeEulerPython.svg")
# ---
def _wait_ref(self, timeout: float | None, raise_on_failure: bool) -> JobStatus:
        try:
            ray.get(self._ref, timeout=timeout)
        except Exception:
            if raise_on_failure:
                raise
        return self.status()
# ---
def test_is_client_error(self):
        self.assertFalse(status.is_client_error(399))
        self.assertFalse(status.is_client_error(500))

        for i in range(400, 499):
            self.assertTrue(status.is_client_error(i))
# ---
def module_org_with_manifest():
    org = entities.Organization().create()
    manifests.upload_manifest_locked(org.id)
    return org
# ---
def captured_inputs(self):
    """Returns the list of implicitly captured inputs."""
    self._create_definition_if_needed()
    return self._extra_inputs
# ---
def first_even(nums):
    first_even = next((el for el in nums if el%2==0),-1)
    return first_even
# ---
def _populate_boundary_idx(self):
        """
        Populates the indices of the boundary variables in the input tensor.

        We assume the indices INPT_BOUNDARY_IDX will be used after the boundary
        condition is extracted from the input tensor
        """
        for i, k in enumerate(self.boundary_var_names):
            self.INPT_BOUNDARY_IDX[k] = torch.tensor([i])
# ---
def test___cmp__ge(self):
        self._test__cmp__(
            lambda left, right: left >= right,
            (
                True,
                True,
                True,
                False,
                True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
                TypeError if PY3 else True,
            ),
            '>='
        )
# ---
def preprocessing(text, tab=4):
    text = _newline_pattern.sub('\n', text)
    text = text.expandtabs(tab)
    text = text.replace('\u00a0', ' ')
    text = text.replace('\u2424', '\n')
    pattern = re.compile(r'^ +$', re.M)
    return pattern.sub('', text)
# ---
def soft_sign(a: A) -> A:
    return wrap_elemwise_unary(jnn.soft_sign, a)
# ---
def from_iterable(caches: Iterable[PageCacheT]) -> "ListCache[PageCacheT]":
        return ListCache(tuple(caches))
# ---
def test_actor_method_with_kwargs(client: LocalClient):
    actor = client.create_actor(Adder, name="adder")
    assert actor.add.remote(a=3, b=4).result() == 7
    assert actor.add(a=10, b=20) == 30
# ---
def run(self, fn: Callable, *args, name: str | None = None):
        if self.ray_options:
            remote_fn = ray.remote(**self.ray_options)(fn)
        else:
            remote_fn = ray.remote(max_retries=100)(fn)

        options: dict[str, Any] = {"scheduling_strategy": "SPREAD"}
        if name:
            options["name"] = name
        return remote_fn.options(**options).remote(*args)
# ---
def area(self):
        return math.pi * self.radius ** 2
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """Peak FLOP/s for a single GPU."""
        from fray.cluster.device_flops import device_flops

        flops = device_flops(self.variant, dtype)
        if flops is None:
            raise ValueError(f"Unknown device/dtype: {self.variant}/{dtype}")
        return flops
# ---
def from_minutes(cls, minutes: int) -> "Duration":
        """Create duration from minutes."""
        return cls(minutes * 60 * 1000)
# ---
def coll(sx, sy, dx, dy):
    m = 0
    for p in range(32):
        m2 = m + 2**(-p)
        if inside(sx + dx * m2, sy + dy * m2): m = m2
    return (sx + dx*m, sy + dy*m)
# ---
def reserve(c):
    """`build`, then `serve`"""
    build(c)
    serve(c)
# ---
def binary_discretization(z):
  z_hard = torch.sign(z)
  z_soft = z / torch.norm(z, dim=-1, keepdim=True)
  return z_soft + (z_hard - z_soft).detach()
# ---
def getMisMatchesQP(data, clf):
    #print(data)
    data_x = np.c_[data[:,0], data[:,1]]
    results = clf.predict(data_x)
    #print(np.sign(results))
    print("mismatch ", float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data))
    print("score ", clf.score(data_x, data[:,2]))

    return float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data)
# ---
def grpc_channel(self) -> aio.Channel:
        """Create the channel designed to connect to this service.

        This property caches on the instance; repeated calls return
        the same channel.
        """
        # Return the channel from cache.
        return self._grpc_channel
# ---
def device_mesh(self) -> Mesh:
        ici, dcn = self.mesh.axis_shapes(jax.device_count(), self.num_slices)
        axis_types = None
        if self.use_explicit_mesh_axes:
            axis_names = list(ici.keys()) + [k for k in dcn.keys() if k not in ici]
            axis_types = tuple(AxisType.Explicit for _ in axis_names)
        return create_mesh_from_axis_specs(ici_axes=ici, dcn_axes=dcn, axis_types=axis_types)
# ---
def __gt__(self, other: "Duration") -> bool:
        return self._ms > other._ms
# ---
def _is_int(x: float) -> bool:
    try:
        return abs(x - round(x)) <= 1e-7
    except BaseException:
        return False
# ---
def load_prime_intellect_env(self, env_id: str, env_args: dict) -> Any:
        """
        Get the Verifiers environment for the environment ID.
        """
        self._ensure_verifiers_installed()
        import verifiers as vf

        logger.debug(f"Loading Verifiers environment for {env_id} with arguments: {env_args}")

        if env_id not in self.ENVS:
            self.ENVS[env_id] = vf.load_environment(env_id=env_id, **env_args)

        return self.ENVS[env_id]
# ---
def replace(self, resource, id_, document):
        args = self._es_args(resource, refresh=True)
        return self.es.index(body=document, id=id_, **args)
# ---
def restart(self) -> str:
        self.stop()
        return self.start()
# ---
def count_Rotation(arr,n):   
    for i in range (1,n): 
        if (arr[i] < arr[i - 1]): 
            return i  
    return 0
# ---
def parse_lheading(self, m):
        """Parse setext heading."""
        self.tokens.append({
            'type': 'heading',
            'level': 1 if m.group(2) == '=' else 2,
            'text': m.group(1),
        })
# ---
def get_block(self, header_hash: bytes) -> Optional[Block]:
        with self.lock:
            return self._state.get_block(header_hash)
# ---
def addcommissioninfo(self, comminfo, name=None):
        self.comminfo[name] = comminfo
# ---
def _convert_token_to_id(self, token: str) -> int:
    return self._vocab_str_to_int.get(
      token, self._vocab_str_to_int['[UNK]'])
# ---
def test_perturb_operators_valid_python(rng):
    source = "def f(a, b):\n    if a > b:\n        return a + b\n    return a - b"
    for seed in range(20):
        r = random.Random(seed)
        result = perturb_operators(source, r, swap_prob=0.5)
        if result is not None:
            ast.parse(result)
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[False] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[False] = ...,
) -> NamedArray: ...
# ---
def test_alter_disconnect_to_true(self):
        self._test_alter_disconnect(False, True)
        self._test_alter_disconnect(True, True)
# ---
def check_Equality(str):
  if (str[0] == str[-1]):  
    return ("Equal") 
  else:  
    return ("Not Equal")
# ---
def get_available_ports(only_free=False):
    ports = _get_available_ports()

    if only_free:
        ports = list(set(ports) - set(DxlIO.get_used_ports()))

    return ports
# ---
def log_xla_to_wandb(step: StepInfo):
        nonlocal last_mtime
        save_xla_dumps_to_wandb(last_mtime)
        # update time to now
        last_mtime = time.time()
# ---
def _test_encryption(self, message):
        enc = self.alice.encrypt(message)
        self.assertFalse(enc.endswith('\n'))
        dec = self.bob.decrypt(enc)
        self.assertEquals(dec, message)
# ---
def _loss_fn(params: TinyMLP, x: NamedArray, y: NamedArray) -> jax.Array:
    preds = params(x)
    diff = preds - y
    return hax.mean(diff * diff).scalar()
# ---
def __returnOK( self, lfn ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    successful = {}
    for lfn in res['Value'].keys():
      successful[lfn] = True
    resDict = {'Successful':successful, 'Failed':{}}
    return S_OK( resDict )
# ---
def result(self):
        """Obtain the final result of the parse.

        Raises ValueError if the parse failed to reduce to a single result.
        """

        if len(self.values) != 1:
            raise ValueError("Could not parse rule")
        return self.values[0]
# ---
def cross_entropy(logits, y, ignore_index=-100):
    """Cross entropy loss."""
    logits = logits.view(-1, logits.shape[-1])
    y = y.view(-1)
    return F.cross_entropy(logits, y, ignore_index=ignore_index)
# ---
def test_is_stop_signal_partial_match_with_padding():
    # stop_sequence is shorter and left padded with INVALID
    tail_tokens = hax.named(jnp.array([5, 6, 7], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[INVALID, 6, 7]], dtype=jnp.int32), axis=("seq", "position"))
    assert is_stop_signal(tail_tokens, stop_sequences)
# ---
def do_append(first, second, underneath=False):
  sign = "-" if underneath else "+"
  background = "-background black" if PARAMS["DO_POLAROID"] else ""
  command = "convert -gravity center %s %sappend %s %s %s" % (background, sign, first, second, first)
  ret = subprocess.call(command, shell=True)

  if ret != 0:
    raise Exception("Command failed: ", command)
# ---
def __init__(self, instances: list[dict] | None = None):
        self._instances = instances or []
# ---
def list_endpoints(self, prefix: str) -> list[cluster_pb2.Controller.Endpoint]:
        request = cluster_pb2.Controller.ListEndpointsRequest(prefix=prefix)
        response = self._client.list_endpoints(request)
        return list(response.endpoints)
# ---
def testSubmediantPosition(self):
        # Make sure that the submediant of a key based on 440 is in the
        # correct location (submediant was randomly selected from all the
        # tones)
        tonic = 440
        submediant = tonic * 2**(9./12.)
        hpcp = HPCP()([submediant], [1])

        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,1.,0.,0.])
# ---
def message_unsubscribe(self, *args, **kwargs):
        """Send the unsubscribe action on stock.picking model to match with subscribe"""
        return self.pool.get('stock.picking').message_unsubscribe(*args, **kwargs)
# ---
def __init__(self,occurrence):
        self.occurrence=occurrence
        self.label=self.occurrence.arguments[0]
# ---
def deposit(amount):
    global balance
    balance += amount
    return balance
# ---
def wrap_elemwise_unary(f, a, *args, **kwargs):
    if isinstance(a, NamedArray):
        return NamedArray(f(a.array, *args, **kwargs), a.axes)
    else:
        return f(a, *args, **kwargs)
# ---
def cursor_execute(conn, *args, **kw):
            canary.append('cursor_execute')
# ---
def get_batch(self, indices: Sequence[int] | np.ndarray) -> Sequence[T_co]:
        return self._run_coroutine(self.dataset.get_batch(indices))
# ---
def get_data(key):
            name = key[0]
            index = key[1]  # 1d index
            return input_data[name][index]
# ---
def _get_value(self, name):
		if name not in self._values:
			raw_value = self[self.structure.index(name)]

			self._set_value(name, raw_value)

		return self._values[name]
# ---
def manipulate(text):
            for key in rules:
                pattern = getattr(self.rules, key)
                m = pattern.match(text)
                if not m:
                    continue
                self.line_match = m
                out = getattr(self, 'output_%s' % key)(m)
                if out is not None:
                    return m, out
            return False
# ---
def __iter__(self):
        return iter(self._dict)
# ---
def test_data(request):
    data, choosers, spec, probabilities = request.param
    return {
        'data': data,
        'choosers': choosers,
        'spec': spec,
        'probabilities': probabilities
    }
# ---
def indented(self, width: int) -> Generator[None, None, None]:
        self.env["indent_width"] += width
        try:
            yield
        finally:
            self.env["indent_width"] -= width
# ---
def first_non_repeating_character(str1):
  char_order = []
  ctr = {}
  for c in str1:
    if c in ctr:
      ctr[c] += 1
    else:
      ctr[c] = 1 
      char_order.append(c)
  for c in char_order:
    if ctr[c] == 1:
      return c
  return None
# ---
def _is_passive_array(arr):
    return isinstance(arr, _PassiveNamedArray)
# ---
def register_method_args(self):
        """
        That is the method where users should override in their
        modules according to be able to send their method arguments
        to the Overlord. If they dont have it nothing breaks
        just that one in the base class is called

        @return : empty {}
        """

        # to know they didnt implement it
        return {}
# ---
def index(self, request):
        queryset = Condition.objects.select_related('source', 'target_option')
        serializer = ConditionIndexSerializer(queryset, many=True)
        return Response(serializer.data)
# ---
def __init__(self, cache_dir: Path, max_bundles: int = 100):
        self._cache_dir = cache_dir
        self._bundles_dir = cache_dir / "bundles"
        self._extracts_dir = cache_dir / "extracts"
        self._max_bundles = max_bundles
        self._extract_locks: dict[str, threading.Lock] = defaultdict(threading.Lock)

        self._bundles_dir.mkdir(parents=True, exist_ok=True)
        self._extracts_dir.mkdir(parents=True, exist_ok=True)
# ---
def _load_stack(path):
    path = Path(path)
    suffix = path.suffix.lower()
    if suffix in {".cif", ".mmcif"}:
        cif_file = pdbx.CIFFile.read(str(path))
        stack = pdbx.get_structure(cif_file, use_author_fields=False)
    elif suffix in {".pdb", ".ent"}:
        pdb_file = pdbio.PDBFile.read(str(path))
        stack = pdbio.get_structure(pdb_file, model=None)
    else:
        raise ValueError(f"Unsupported structure file extension: {suffix}")

    return stack
# ---
def test_minhash_dimensions():
    """Test that MinHash output has correct dimensions."""
    texts = ["doc one", "doc two"]
    num_perms = 128
    batch = pa.RecordBatch.from_pydict({"text": texts})
    pipeline = [Transformation.MinHash(input_col="text", output_col="sig", num_perms=num_perms, ngram_size=3, seed=42)]
    sigs = transform(batch, pipeline)["sig"]
    for sig in sigs:
        assert len(sig.as_py()) == num_perms
        assert all(isinstance(x, int) for x in sig.as_py())
# ---
def body(i, buf):
        p = t_pages[i]
        s = t_slots[i]
        ins = kv_ev[i][None, None, :, :]
        return lax.dynamic_update_slice(buf, ins, (p, s, 0, 0))
# ---
def _render_path_elem(x):
    match x:
        case jtu.DictKey(key):
            return f"{key}"
        case jtu.GetAttrKey(key):
            return f"{key}"
        case jtu.SequenceKey(i):
            return f"{i}"
        case jtu.FlattenedIndexKey(i):
            return f"{i}"
        case _:
            return str(x)
# ---
def the_object_name1_has_a_boolean_difference_by_name2(name1, name2):
    obj = the_object_name_exists(name1)
    for modifier in obj.modifiers:
        if modifier.type == "BOOLEAN" and modifier.object and modifier.object.name == name2:
            return True
    assert False, "No boolean found"
# ---
def asGenKw(self):
        """ @rtype: GenKw """
        impl_type = EnkfNode.cNamespace().get_impl_type(self)
        assert impl_type == ErtImplType.GEN_KW

        return GenKw.createCReference(self.valuePointer(), self)
# ---
def encode(self, text, add_special_tokens=True):
            return [ord(c) for c in text]
# ---
def __init__(self, server, host: str, port: int):
        self.server = server
        self.host = host
        self.port = port
        self.thread = threading.Thread(target=self.server.run, daemon=True)
# ---
def load(cls: "JSONSerializable", path: Path) -> "JSONSerializable":
        """Load the object from a JSON file.

        Parameters
        ----------
        path : Path
            The path to the file.

        Returns
        -------
        Serializable
            The loaded object.

        """
        with path.open("r") as f:
            return cls.from_dict(json.load(f))
# ---
import re
def find_char_long(text):
  return (re.findall(r"\b\w{4,}\b", text))
# ---
def test_column_var(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.var()

        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(), test_impl())
# ---
def __len__(self) -> int:
        return self._run_coroutine(self.dataset.async_len())
# ---
def clone_sources(self) -> ht.i32[NamedArray, "seq"]:  # type: ignore[name-defined]
        """Mapping from clone targets to their parent sequences."""
        return self.sequences.clone_sources
# ---
def stop(self, handle: VllmServerHandle) -> None:
        if handle.process is not None:
            handle.process.kill()
# ---
def pairwise(iterable):
    # pairwise('ABCDEFG') --> AB BC CD DE EF FG
    a, b = tee(iterable)
    next(b, None)
    return zip(a, b)
# ---
def __str__(self):
        return '[' + ', '.join('{}'.format(el) for el in self._queue) + ']'
# ---
def sources_list(self, mode='short'):
        ret = {}
        mode = bottle.request.query.mode or mode
        for name, spec in self.ndb.sources.items():
            ret[name] = {'class': spec.nl.__class__.__name__,
                         'status': spec.status}
            if mode == 'full':
                ret[name]['config'] = spec.nl_kwarg
        return bottle.template('{{!ret}}', ret=json.dumps(ret))
# ---
def get_total_number_of_sequences(m,n): 
	T=[[0 for i in range(n+1)] for i in range(m+1)] 
	for i in range(m+1): 
		for j in range(n+1): 
			if i==0 or j==0: 
				T[i][j]=0
			elif i<j: 
				T[i][j]=0
			elif j==1: 
				T[i][j]=i 
			else: 
				T[i][j]=T[i-1][j]+T[i//2][j-1] 
	return T[m][n]
# ---
def test_extract_stack_summaries():
    frame = inspect.currentframe()
    stack_summaries = extract_stack_summaries(frame)
    assert stack_summaries[-1].name == "test_extract_stack_summaries"
    assert stack_summaries[-1].module == "cubed.tests.test_utils"
    assert not stack_summaries[-1].is_cubed()
# ---
def to_t(arr: jnp.ndarray):
            return torch.from_numpy(np.array(arr))
# ---
def round_flops_to_bucket(flops: float, base: float = 1.1) -> float:
    """Round FLOP count to the nearest power of base.

    Args:
        flops: FLOP count to round.
        base: Base for the power buckets (default 1.1 for ~10% buckets).
    """
    if flops <= 0:
        return flops

    k = math.log(flops) / math.log(base)
    return base ** round(k)
# ---
def _handle_key_binding_message(self, binding_name, key_state, key_name):
        self._key_binding_handlers[binding_name](key_state, key_name)
# ---
def num_arrays(self) -> int:
        """Return the number of arrays in this plan."""
        return sum(d.get("type") == "array" for _, d in self.dag.nodes(data=True))
# ---
def tree_structure(tree, is_leaf=None):
    """
    Version of [jax.tree_util.tree_structure][] that automatically treats NamedArrays as leaves.
    """
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: is_leaf(x) or is_named_array(x)

    return jax.tree_util.tree_structure(tree, is_leaf=is_leaf)
# ---
def test_job_name_to_safe_token_and_deep_nesting():
    job = JobName.from_string("/a/b/c/d/e/0")
    assert job.to_safe_token() == "job__a__b__c__d__e__0"
    assert job.require_task()[1] == 0
# ---
def value(self) -> NamedArray:
        """Materialize this reference view as a `NamedArray`."""
        _, axes_spec, index_tuple = self._prepare(Ellipsis)
        result = self._ref[tuple(index_tuple)]
        return named(result, axes_spec)
# ---
def __post_init__(self):
        if self.a <= 0 or self.b >= 0:
            raise ValueError("Power schedule expects a > 0 and b < 0")
# ---
def get_device_type(device: cluster_pb2.DeviceConfig) -> str:
    """Extract device type from config."""
    if device.HasField("cpu"):
        return "cpu"
    elif device.HasField("gpu"):
        return "gpu"
    elif device.HasField("tpu"):
        return "tpu"
    return "cpu"
# ---
def dims(self):
        """Mapping from dimension names to lengths.

        This dictionary cannot be modified directly, but is updated when adding
        new variables.
        """
        return Frozen(SortedKeysDict(self._dims))
# ---
def count(s,c) : 
    res = 0 
    for i in range(len(s)) : 
        if (s[i] == c): 
            res = res + 1
    return res
# ---
def extract_nth_element(list1, n):
    result = [x[n] for x in list1]
    return result
# ---
def compute_svm_cv(K, y, C=100.0, n_folds=5,
                   scoring=balanced_accuracy_scoring):
    """Compute cross-validated score of SVM with given precomputed kernel.
    """
    cv = StratifiedKFold(y, n_folds=n_folds)
    clf = SVC(C=C, kernel='precomputed', class_weight='auto')
    scores = cross_val_score(clf, K, y,
                             scoring=scoring, cv=cv)
    return scores.mean()
# ---
def _massage_env(env):
    # Ray pretends it's running in a TTY, which leads to a ton of log spam from tqdm.
    # Levanter uses tqdm_loggable, which tries to sniff out the TTY, but it doesn't work with Ray.
    # So we force it
    env = dict(env)
    if "TERM" not in env:
        env["TERM"] = "dumb"

    if "TF_CPP_MIN_LOG_LEVEL" not in env:
        # Suppress TensorFlow logs, which can be very verbose
        env["TF_CPP_MIN_LOG_LEVEL"] = "3"

    return env
# ---
def output_reparam(use_mup: bool = True) -> type[AbstractLinearReparam]:
        """Return the reparameterization class for an output linear layer."""

        return mup.OutputLinearMup if use_mup else mup.LinearStandardParam
# ---
def _fix_sqrt(string):
    """Convert \\sqrt{x} or \\sqrt x to \\sqrt{x}. (Legacy - use process_latex_sqrt)"""
    # Use new abstraction for converting to text format
    return process_latex_sqrt(string)
# ---
def open_file_like(self, file_like):
        '''Assign a file-like object, such as those provided by StringIO, as an open file object.
        >>> from io import StringIO
        >>> fileLikeOpen = StringIO()
        >>> mf = MidiFile()
        >>> mf.open_file_like(fileLikeOpen)
        >>> mf.close()
        '''
        self.file = file_like
# ---
def __getSynsets(self, word, wordNetCode):
        """
        It returns the synsets given both word and language code
        """
        from nltk.corpus import wordnet as wn

        synsets = wn.synsets(word, lang=wordNetCode)
        return synsets
# ---
def test_str_replace_noregex(self):
        def test_impl(df):
            return df.A.str.replace('AB', 'EE', regex=False)

        df = pd.DataFrame({'A': ['ABCC', 'CABBD']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def add(self, entry: SubtreeEntry) -> None:
        """Add a subtree entry to the bank."""
        if entry.node_type not in self.entries:
            self.entries[entry.node_type] = []
        self.entries[entry.node_type].append(entry)
# ---
def LineAndColumnNumbersClamped( line_num, column_num ):
  new_line_num = line_num
  new_column_num = column_num

  max_line = len( vim.current.buffer )
  if line_num and line_num > max_line:
    new_line_num = max_line

  max_column = len( vim.current.buffer[ new_line_num - 1 ] )
  if column_num and column_num > max_column:
    new_column_num = max_column

  return new_line_num, new_column_num
# ---
def test_check_health_returns_healthy_on_success():
    """check_health returns healthy result when curl succeeds."""
    conn = MagicMock()
    conn.run.return_value = MagicMock(returncode=0, stdout="OK")
    result = check_health(conn, port=10001)
    assert result.healthy is True
# ---
def terminate(self) -> None:
        """Mark VM group as terminated."""
        ts = Timestamp.now().epoch_ms()
        for vm in self._vms:
            vm.info.state = vm_pb2.VM_STATE_TERMINATED
            vm.info.state_changed_at.CopyFrom(time_pb2.Timestamp(epoch_ms=ts))
        self._terminated = True
# ---
def test_spawn_vhd_glance_windows(self):
        self._test_spawn(glance_stubs.FakeGlance.IMAGE_VHD, None, None,
                         os_type="windows", architecture="i386")
        self.check_vm_params_for_windows()
# ---
def hf_checkpoint_converter(self, ref_checkpoint: str | None = None) -> HFCheckpointConverter["ParallelLlamaConfig"]:
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfLlamaConfig,
        )
# ---
def test_filter(sample_data, backend):
    """Test filtering dataset."""
    ds = Dataset.from_list(sample_data).filter(lambda x: x % 2 == 0)
    assert list(Backend.execute(ds, context=backend)) == [2, 4, 6, 8, 10]
# ---
def test_impl(df):
            B = df.A.str.replace('AB*', 'EE', regex=True)
            return B
# ---
def on_context_lost():
            """The window's GL context was lost.

            When the context is lost no more GL methods can be called until it
            is recreated.  This is a rare event, triggered perhaps by the user
            switching to an incompatible video mode.  When it occurs, an
            application will need to reload all objects (display lists, texture
            objects, shaders) as well as restore the GL state.

            :event:
            """
# ---
import math 
def get_sum(n): 
	sum = 0
	i = 1
	while i <= (math.sqrt(n)): 
		if n%i == 0: 
			if n/i == i : 
				sum = sum + i 
			else: 
				sum = sum + i 
				sum = sum + (n / i ) 
		i = i + 1
	sum = sum - n 
	return sum
def check_abundant(n): 
	if (get_sum(n) > n): 
		return True
	else: 
		return False
# ---
def __init__(self, inputfiles):
        """
        :param inputfiles: list of pdb files needed for averaging
        """
        self.inputs = inputfiles
        self.size = []
        self.nbknots = None
        self.radius = None
        self.coordknots = []
# ---
def init_fn(params):
        return None
# ---
def __init__(self, model_name: str, attribute_name: str, model_type: str | None, *args, **kwargs):
        super().__init__(model_name, attribute_name, model_type, *args, **kwargs)
# ---
def uniform(
    key, shape: AxisSpec, dtype=float, minval: NamedOrNumeric = 0.0, maxval: NamedOrNumeric = 1.0
) -> NamedArray:
    shape = axis_spec_to_shape_dict(shape)
    minval = broadcast_to(minval, shape).array
    maxval = broadcast_to(maxval, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.uniform(key=key, shape=jax_shape, dtype=dtype, minval=minval, maxval=maxval)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def multiply(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.multiply(x1, x2)
# ---
def scopes_get(self):
        return self._scopes
# ---
def test_lazy_zarr_array(tmp_path):
    zarr_path = tmp_path / "lazy.zarr"
    arr = lazy_zarr_array(zarr_path, shape=(3, 3), dtype=int, chunks=(2, 2))

    assert not zarr_path.exists()
    with pytest.raises((FileNotFoundError, TypeError, ValueError)):
        arr.open()

    arr.create()
    assert zarr_path.exists()
    arr.open()
# ---
def print_ls_l_header():
    print(get_ls_l_header())
# ---
def __enter__(self):
        if len(self._cmanagers) > 0:
            raise RuntimeError("Trainer is already entered")

        self._cmanagers = [
            levanter.current_tracker(self.tracker),
            haliax.partitioning.set_mesh(self.device_mesh),
            hax.axis_mapping(self.parameter_axis_mapping),
        ]

        for cmanager in self._cmanagers:
            cmanager.__enter__()

        return self
# ---
def make_fake_popen(lines: list[str] | None = None):
    """Create a mock Popen-like object for streaming tests."""
    if lines is None:
        lines = ["[iris-init] Bootstrap starting", "[iris-init] Bootstrap complete"]
    mock = MagicMock()
    mock.stdout = iter(line + "\n" for line in lines)
    mock.returncode = 0
    mock.wait.return_value = 0
    mock.args = []
    return mock
# ---
def __init__(
        self,
        state: WorkerState,
        task_queue: "Queue[PendingTask]",
        resolver: Resolver,
        timeout: float,
    ):
        self.state = state
        self._task_queue = task_queue
        self._resolver = resolver
        self._timeout = timeout
        self._discover_backoff = ExponentialBackoff(initial=0.05, maximum=1.0)
        self._actor_client: ActorClient | None = None
        self._stop_event: threading.Event | None = None
# ---
def is_upper(string):
  return (string.upper())
# ---
def __rlshift__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer", "__rlshift__")
        if other is NotImplemented:
            return other
        return elemwise(
            nxp.bitwise_left_shift, other, self, dtype=result_type(self, other)
        )
# ---
def test_deduplicate_all_unique(backend):
    """Test deduplication when all items are unique."""
    data = [{"id": i, "val": f"item_{i}"} for i in range(10)]

    ds = Dataset.from_list(data).deduplicate(key=lambda x: x["id"])

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 10
# ---
def test_score_syntax_errors():
    assert score_syntax_errors(open('input/10.test').read().splitlines()) == (26397, 288957)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': ['aa', 'bb', 'aa', 'cc', 'cc']})
            return df.A.nunique()
# ---
def unregister_endpoint(self, endpoint_id: str) -> None:
        """Unregister an endpoint via RPC.

        This is a no-op for the RPC implementation. The controller automatically
        cleans up endpoints when jobs terminate, so explicit unregistration
        is not required.

        Args:
            endpoint_id: Endpoint ID (ignored)
        """
        # No-op: controller auto-cleans endpoints on job termination
        del endpoint_id
# ---
def _check_entered(self) -> None:
        if not self._entered:
            raise RuntimeError("JobGroup must be entered before calling run methods")
# ---
def _run(stop_event: threading.Event) -> None:
            logger.debug("Running server %s (%s)", name, server)
            server.run()
            logger.debug("Server %s exited", name)
# ---
def can_be_scheduled(self) -> bool:
        """Check if task is ready to be scheduled.

        A task can be scheduled if:
        - It has no attempts yet (fresh task), or
        - Its current attempt is terminal AND it should retry
        """
        if not self.attempts:
            return True
        return self.attempts[-1].is_terminal() and not self.is_finished()
# ---
def perform_create(self, serializer):
        instance = serializer.save()
        instance.open(applicant=self.request.user)
# ---
def test_sample_edit_with_validation_valid(tokenizer):
    source = "x = 1 + 2\n"
    replacement_tokens = tokenizer.encode_source("3 * 4")

    mutation = sample_edit_with_validation(
        source=source,
        edit_position=4,
        original_span_end=9,
        replacement_tokens=replacement_tokens,
        tokenizer=tokenizer,
    )
    assert mutation is not None
    assert mutation.apply(source) == "x = 3 * 4\n"
# ---
def create_vm_group_side_effect(_tags: dict[str, str] | None = None) -> MagicMock:
        manager._create_count += 1
        slice_id = f"new-slice-{manager._create_count}"
        return make_mock_slice(slice_id)
# ---
def test_nofiles(self):
        self.assertEqual(0, len(tecautils.filterImages([], self.conf)))
# ---
def test_is_none_1(self):
        self.assertIsNone(string_color('a'))
# ---
def _bias_dropout_add(x, bias, scale, residual, prob):
    return bias_dropout_add_scale(
      x, bias, scale, residual, prob, training)
# ---
def __divmod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.divmod(self, other)
# ---
def fake_eject_subordinate(id, compute_uuid, host_uuid):
            fake_eject_subordinate.called = True
# ---
def get_absolute_url(self):
        return reverse('software_edit', kwargs={'pk': self.pk})
# ---
def count_odd(array_nums):
   count_odd = len(list(filter(lambda x: (x%2 != 0) , array_nums)))
   return count_odd
# ---
def test_gemma_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    config_class = TrainLmConfig

    check_load_config(config_class, config_file)
# ---
def lateralsuface_cylinder(r,h):
  lateralsurface= 2*3.1415*r*h
  return lateralsurface
# ---
def deeplift_tensor_grad(grad):
    return_grad = complex_module_gradients[-1]
    del complex_module_gradients[-1]
    return return_grad
# ---
def rust_compute_document_hashes(batch: pa.RecordBatch, text_col: str, id_col: str) -> pa.RecordBatch:
    pipeline = [
        dupekit.Transformation.Hash(input_col=text_col, output_col="hash", algo=dupekit.HashAlgorithm.Xxh3_128),
    ]
    return dupekit.transform(batch, pipeline)
# ---
def set_time(self, match):
        self.time_text.SetLabel(format_time(match.game_time_so_far))
        self.stage_text.SetLabel(match.stage_name)
# ---
def fmin(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.fmin](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fmin.html)
    """
    return jnp.fmin(x1, x2)
# ---
def checkinfo2(content):
    content[1] = content[1].decode('gbk')
    key = content[1].encode('utf-8')
    if key == '':
        return ''


    result = animation(key)    #
    return result
# ---
def __init__(self, **kwds):
#        LOG.debug("extensions constructor: %s" % kwds)
        super(Extensions, self).__init__(**kwds)
# ---
def _do_block(carry, block, *extra_args, **extra_kwargs):
        return block(carry, *extra_args, **extra_kwargs)
# ---
def get_block_is_duplicate(self, block: Block) -> bool:
        with self.lock:
            return self._state.get_block(block.headerhash) is not None
# ---
def delete_ports(self, inc_ports):
        ''' remove a port from a service '''
        if not isinstance(inc_ports, list):
            inc_ports = [inc_ports]

        ports = self.get(Service.port_path) or []

        if not ports:
            return True

        removed = False
        for inc_port in inc_ports:
            port = self.find_ports(inc_port)
            if port:
                ports.remove(port)
                removed = True

        return removed
# ---
def __init__(self, remote, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs) 
        self.remote = remote
        self.InitUI()
# ---
def data_received(self, data):
        """Called with snippets received from the serial port"""
        raise NotImplementedError
# ---
def worker(worker_id: int):
        try:
            for i in range(num_operations):
                vm = MagicMock()
                vm_id = f"vm-{worker_id}-{i}"
                vm.info = vm_pb2.VmInfo(vm_id=vm_id)

                registry.register(vm)
                registry.unregister(vm_id)
        except Exception as e:
            errors.append(e)
# ---
def test_unify_chunks_blockwise_2d(chunks_a, chunks_b, expected_chunksize):
    a = xp.ones((10, 10), chunks=chunks_a)
    b = xp.ones((10, 10), chunks=chunks_b)

    _, arrays = unify_chunks(a, "ij", b, "ji")
    for arr in arrays:
        assert arr.chunksize == expected_chunksize

    c = xp.matmul(a, b)
    assert_array_equal(c.compute(), np.matmul(np.ones((10, 10)), np.ones((10, 10))))
# ---
def call_kernel(q_b, k_b, v_b, si, sink):
            if sink is None:
                return kernel(q_b, k_b, v_b, segment_ids=si)
            return kernel(q_b, k_b, v_b, segment_ids=si, sinks=sink)
# ---
def result(self) -> Any:
        return self._result
# ---
def click_save_button(self):
        """
        :rtype: BrowseMoviePage
        """
        self._click(AddMoviePageLocators.SAVE_BUTTON_LOCATOR)
        return BrowseMoviePage(self._driver)
# ---
def starting_job(method, notebook, data):
    job = notebook.current_job
    job.fetch_logs()
    if job.is_started:
        return views.render('started_job', notebook, {'job': job})
    return {'job': job, 'username': SAAGIE_USERNAME}
# ---
def record(self, info):
                self.recorded.append(info)
# ---
def load_config() -> CliConfig:
    if os.path.exists(".levanter.yaml"):
        d = yaml.load(open(".levanter.yaml", "r"), Loader=yaml.SafeLoader)
    elif os.path.exists(".config"):
        warnings.warn("Using deprecated .config file. Please rename to .levanter.yaml")
        d = yaml.load(open(".config", "r"), Loader=yaml.SafeLoader)
    else:
        d = {}

    return draccus.decode(CliConfig, d)
# ---
def fused_func(*args):
        return pipeline2.config.function(pipeline1.config.function(*args))
# ---
def reset(self):
        self.base_controller.reset()
        self.sim.reset()
        self.I = 0.0
# ---
def init_log(self, tail: int | None = None) -> str:
        """Return bootstrap log (empty if not yet initializing)."""
        lines = self._log_lines[-tail:] if tail else self._log_lines
        return "\n".join(lines)
# ---
def vmap_fun(x):
        return x.sum(Width)
# ---
def cbrt(a: A) -> A:
    return wrap_elemwise_unary(jnp.cbrt, a)
# ---
def setup(self):
        IfcStore.purge()
        bpy.ops.wm.read_homefile(app_template="")
        bpy.data.batch_remove(bpy.data.objects)
        bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)
        blenderbim.bim.handler.setDefaultProperties(None)
        bpy.ops.bim.create_project()
# ---
def vms(self) -> list[ManagedVm]:
        return self._managed_vms
# ---
def less_equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.less_equal](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.less_equal.html)
    """
    return jnp.less_equal(x1, x2)
# ---
def backwards(self, orm):
        # Removing unique constraint on 'Vendeur', fields ['code_permanent']
        db.delete_unique(u'encefal_vendeur', ['code_permanent'])
# ---
def bias_init_zero_(bias):
    with torch.no_grad():
        bias.fill_(0.0)
# ---
def createDirectory( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
from collections import Counter
def sort_counter(dict1):
 x = Counter(dict1)
 sort_counter=x.most_common()
 return sort_counter
# ---
def frequency(a,x): 
    count = 0  
    for i in a: 
        if i == x: count += 1
    return count
# ---
def post(self, request):
        data = request.data
        serializer = AuthenticateSerializer(data=data)
        if serializer.is_valid(raise_exception=True):
            new_date = serializer.data
            return response.Response(new_date,status=status.HTTP_200_OK)
        return response.Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
# ---
def __init__(self, urls):
        self.urls = urls
        # Force materialization early so duplicate shard names surface immediately.
        _ = self._shard_name_to_url_mapping
# ---
def read_log(path: str):
    with open(path) as f:
        return list(map(json.loads, f.readlines()))
# ---
def newImage(self, row=0, col=0, filename=""):
    print("%d.%d > %s" % (row, col, filename))
# ---
def __repr__(self) -> str:
        return f"{self.object_id} {self.allocation_type.name} {self.memory or ''} {self.address or ''} {self.call or ''}"
# ---
def on_hide():
            """The window was hidden.

            This event is triggered when a window is minimised or (on Mac OS X)
            hidden by the user.

            :event:
            """
# ---
def terminate_job(
        self,
        job_id: str,
    ) -> cluster_pb2.Empty:
        """Terminate a running job."""
        request = cluster_pb2.Controller.TerminateJobRequest(job_id=job_id)
        return self._service.terminate_job(request, None)
# ---
def key_helper(self):
        """ Generates a public key and a private key and stores them in the config. The public key will be applied to
        all the instances in the deployment later on when wait() is called.
        """
        if self.config['key_helper']:
            private_key, public_key = util.generate_rsa_keypair()
            self.config['ssh_private_key'] = private_key.decode()
            self.config['ssh_public_key'] = public_key.decode()
# ---
def ohc_anomaly_global(self, data: xr.Dataset) -> xr.DataArray:
        c_p = 3850  # J/(kg C)
        rho_0 = 1025  # kg/m^3

        # Use real areacello for physical calculations
        areacello = data["areacello_spherical"]

        OHC = ((data["thetao"] * c_p * rho_0) * areacello * data["dz"]).sum(
            ["x", "y", "lev"]
        ) / 1e21

        OHC = remove_climatology(OHC)

        OHC = OHC.rename("OHC Anomaly")
        OHC = OHC.assign_attrs(units="ZJ")
        return OHC
# ---
def compute_disto_loss(
        self,
        model: LightningModule,
        out: Dict[str, torch.Tensor],
        batch: Dict[str, torch.Tensor],
        idx_dataset: int,
    ) -> None:
        """Compute distogram loss."""
        # Compute validation disto loss
        val_disto_loss, _ = distogram_loss(
            out, batch, aggregate_distogram=model.aggregate_distogram
        )

        return val_disto_loss
# ---
def dependents(self):
        """ Return the computed fields that depend on ``self``. """
        return (field for field, path in self._triggers)
# ---
from collections import defaultdict 

def most_occurrences(test_list):
  temp = defaultdict(int)
  for sub in test_list:
    for wrd in sub.split():
      temp[wrd] += 1
  res = max(temp, key=temp.get)
  return (str(res))
# ---
def test_edit_experiment_view_started_experiment(self):
        """ Tests edit_experiment template renders when experiment has started """
        experiment = self.create_test_experiment()
        experiment.tracks_finalized = True
        experiment.save()
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(experiment.id,)))
        self.assertTemplateUsed(response, "ab_tool/edit_experiment.html")
# ---
def sanitize_filename(name):
            return re.sub(r"[^A-Za-z0-9._-]", "_", name)
# ---
def num_shards(self) -> int:
        """Total number of logical shards in the plan.

        This counts unique shard_idx values, not total SourceItems. When
        intra-shard parallelism is enabled, multiple SourceItems may belong
        to the same logical shard.
        """
        if not self.source_items:
            return 0
        return len({item.shard_idx for item in self.source_items})
# ---
def test_resolve_unresolved_location(self):
        """Test resolving an UnresolvedLocation against a LocalLocation."""
        base = LocalLocation(path=Path("/base/path"))
        unresolved = UnresolvedLocation(path="subdir/file.zarr")

        resolved = base.resolve(unresolved)

        assert isinstance(resolved, LocalLocation)
        assert resolved.path == Path("/base/path/subdir/file.zarr")
# ---
def _compare_eqx_and_haliax(hax_mod: eqx.Module, eqx_mod: eqx.Module):
    def f(x: NamedArray, *args, **kwargs):
        unnamed_x = x.array
        hax_out = hax_mod(x, *args, **kwargs)  # type: ignore
        eqx_out = eqx_mod(unnamed_x, *args, **kwargs)  # type: ignore

        assert jnp.allclose(hax_out.array, eqx_out)
        return hax_out

    return f
# ---
def create_tmpfile_copy(inc_file):
        '''create a temporary copy of a file'''
        tmpfile = Utils.create_tmpfile('lib_openshift-')
        Utils._write(tmpfile, open(inc_file).read())

        # Cleanup the tmpfile
        atexit.register(Utils.cleanup, [tmpfile])

        return tmpfile
# ---
def test_no_port_reuse_before_release(allocator):
    """Test that allocated ports are not reused before release."""
    ports1 = allocator.allocate(count=5)
    ports2 = allocator.allocate(count=5)

    assert len(set(ports1) & set(ports2)) == 0
# ---
def permutation_coefficient(n, k): 
	P = [[0 for i in range(k + 1)] 
			for j in range(n + 1)] 
	for i in range(n + 1): 
		for j in range(min(i, k) + 1): 
			if (j == 0): 
				P[i][j] = 1
			else: 
				P[i][j] = P[i - 1][j] + ( 
						j * P[i - 1][j - 1]) 
			if (j < k): 
				P[i][j + 1] = 0
	return P[n][k]
# ---
def view(self, name):
        ret = []
        obj = getattr(self.ndb, name)
        for line in obj.dump():
            ret.append(line)
        return bottle.template('{{!ret}}', ret=json.dumps(ret))
# ---
def executor(request):
    return request.param
# ---
def __init__(self, token_s: int, num_bins: int) -> None:
        """Initialize the bfactor module.

        Parameters
        ----------
        token_s : int
            The token embedding size.

        """
        super().__init__()
        self.bfactor = nn.Linear(token_s, num_bins)
        self.num_bins = num_bins
# ---
def delete_file(file_path):
    compss_delete_file(file_path)
# ---
def norm_config(self) -> LayerNormConfigBase:
        """Get the normalization configuration for Gemma."""
        return GemmaNormConfig(
            eps=self.layer_norm_epsilon,
            use_weight=True,  # GemmaRMSNorm requires use_weight=True
            use_bias=False,  # GemmaRMSNorm doesn't support bias
        )
# ---
def resolved_param_mapping(self) -> ResourceMapping:
        # Parameter mapping should inherit shared defaults so parameters on those logical axes shard the
        # same way as compute unless explicitly overridden.
        mapping = self._resolved_shared_axis_mapping()
        for logical, physical in self.param_mapping.items():
            mapping[logical] = _norm(physical)
        return mapping
# ---
def default_hf_config(self) -> HfConfig:
        return self.hf_config_from_hf_checkpoint(None)
# ---
def test_encrypt_with_leading_newlines(self):
        self._test_encryption('\n\nMessage with leading newlines.')
# ---
def relu(a: A) -> A:
    return wrap_elemwise_unary(jnn.relu, a)
# ---
def get_stats(self, container_id: str) -> ContainerStats: ...
# ---
def test_context_run(job_context):
    future = job_context.run(lambda x: x * 2, 5)
    assert job_context.get(future) == 10
# ---
def visit_mphantom(self, element):
        content = self._visit_children(element)
        return BracedNode(f"\\phantom{{{content}}}")
# ---
def include_constructor(loader, node):
        filepath = loader.construct_scalar(node)
        # Resolve relative to the current YAML file's directory
        base_dir = os.path.dirname(loader.name) if hasattr(loader, "name") else "."
        full_path = os.path.join(base_dir, filepath)

        with open(full_path, "r") as f:
            return yaml.safe_load(f)
# ---
def round_up_to_multiple(x, multiple=10):
    """Round up to the nearest multiple"""
    return math.ceil(x / multiple) * multiple
# ---
def search(self, query):
        raise NotImplementedError
# ---
def reverse_words(s):
        return ' '.join(reversed(s.split()))
# ---
def __gt__(self, other: "Timestamp") -> bool:
        return self._epoch_ms > other._epoch_ms
# ---
def fake_init_pool(id, name):
            fake_init_pool.called = True
# ---
def load_state_dict(self, state_dict):
    self.generator.set_state(state_dict.get('random_state'))
    self.counter = state_dict['counter']
    # self.start_counter = self.counter
    self.restarting = True
# ---
def parse_query(query):
    q = {'mode': 'main'}
    if query.startswith('?'): query = query[1:]
    queries = urlparse.parse_qs(query)
    for key in queries:
        if len(queries[key]) == 1:
            q[key] = queries[key][0]
        else:
            q[key] = queries[key]
    return q
# ---
def __exit__(self, exc_type, exc, tb) -> None:
        self.close()
# ---
def test_raw_metric_vm_disk(metrics_collection, appliance, provider):
    vm_name = provider.data['cap_and_util']['capandu_vm']
    query = query_metric_db(appliance, provider, 'disk_usage_rate_average',
        vm_name)

    for record in query:
        if record.disk_usage_rate_average is not None:
            assert record.disk_usage_rate_average > 0, 'Zero VM Disk IO'
            break
# ---
def active():
    proc = multiprocessing.active_children()
    arr = []
    for p in proc:
        print(p.pid)
        arr.append(p.pid)

    return str(arr)
# ---
def test_save_config_success(self):
        resp = self.xe.save_config()
        self.assertEqual(204, resp.status_code)
# ---
def main():
    """Entry point that runs the async main loop."""
    asyncio.run(main_async())
# ---
def replace_axis(axis_spec: AxisSelection, old: AxisSelector, new: AxisSelection) -> AxisSelection: ...
# ---
def health_check(
        self,
        request: cluster_pb2.Empty,
    ) -> cluster_pb2.Worker.HealthResponse: ...
# ---
def convert_eval_to_dolma(cfg: ConvertEvalToDolmaConfig):
    pipeline = (
        Dataset.from_files(f"{cfg.input_path}/**/*.jsonl.gz")
        .flat_map(load_jsonl)
        .map(map_row)
        .write_jsonl(f"{cfg.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )
    Backend.execute(pipeline)
# ---
def onload(self):
		super(Item, self).onload()

		self.set_onload('stock_exists', self.stock_ledger_created())
		self.set_asset_naming_series()
# ---
def load_llama3_tokenizer() -> PreTrainedTokenizer:
    """
    Load the base llama3 tokenizer.

    Returns:
        The llama3 tokenizer instance

    Raises:
        OSError, GatedRepoError, HTTPError: If access to the tokenizer is not available
    """
    return AutoTokenizer.from_pretrained(llama3_tokenizer_hf_path)
# ---
def zeros_like(total: Arrayish, per_tag: Arrayish) -> "_EvalRunningMeans":
        z = RunningMean.zeros_like(total)
        per_tag = RunningMean.zeros_like(per_tag)
        return _EvalRunningMeans(z, per_tag, z, per_tag)
# ---
def load_data(self):
        print(self.datafolder)
        self.samplefile = glob.glob(os.path.join(self.datafolder, "*_SAMPLES.csv"))[0]
        if os.path.isfile(self.samplefile):
            self.samplesdf = pd.read_csv(self.samplefile, encoding='ISO-8859-1')
        else:
            print("File not found: ", self.samplefile)
            self.samplesdf = None

        self.combodefaults = {'cuvette': ['600', '2000', '4000']}
# ---
def get_instance(cls) -> Self:
        """
        Get the instance of the Multiton class for the current scope.
        """
        instance = cls._current_scope.get(cls)
        if instance is None:
            raise ValueError(f"{cls} not initialized")
        return instance
# ---
def _getRateForVoiceType(self, voiceType):
        """Gets the speaking rate value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM

        Returns the rate value for the given voice type, or None if
        not set.
        """

        return self._getKeyValueForVoiceType(voiceType, acss.ACSS.RATE)
# ---
def __init__(self, loss_weights: Tensor, dist_sync_on_step: bool = False, ignore_index: int = -100):
        super().__init__(dist_sync_on_step, ignore_index)
        self.name = "weighted_cross_entropy"
        self.loss_weights = loss_weights
        # Note: this differs from the inherited implementation, whose `reduction='sum'`.
        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
# ---
def negative(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in negative")
    return elemwise(nxp.negative, x, dtype=x.dtype)
# ---
def substract_elements(test_tup1, test_tup2):
  res = tuple(tuple(a - b for a, b in zip(tup1, tup2))
   for tup1, tup2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def test_unique_parallel(self):
        # TODO: test without file
        def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.four.unique() == 3.0).sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
# ---
def put(self, obj: Any) -> Any:
        """Store an object and return a reference to it.

        Args:
            obj: Object to store

        Returns:
            Reference to the stored object (type depends on context)
        """
        ...
# ---
def string_to_tuple(str1):
    result = tuple(x for x in str1 if not x.isspace()) 
    return result
# ---
def _normalize_url(url: str) -> str:
    url = url.strip()
    if url.startswith("<") and url.endswith(">"):
        url = url[1:-1]
    if "#" in url:
        url = url.split("#", 1)[0]
    if "?" in url:
        url = url.split("?", 1)[0]
    return url
# ---
def decrease(rank):
    return dispatch('user', mutation_types.DECREASE, rank)
# ---
def host(self, host):
        """
        Sets the host of this ContributorOrcid.

        :param host: The host of this ContributorOrcid.
        :type: str
        """

        self._host = host
# ---
def __getitem__(self, key):
        return self._fallback(key)
# ---
def __wrapped__(self):
        return self._fn
# ---


def sort_third(l: list):
    """This function takes a list l and returns a list l' such that
    l' is identical to l in the indicies that are not divisible by three, while its values at the indicies that are divisible by three are equal
    to the values of the corresponding indicies of l, but sorted.
    >>> sort_third([1, 2, 3])
    [1, 2, 3]
    >>> sort_third([5, 6, 3, 4, 8, 9, 2])
    [2, 6, 3, 4, 8, 9, 5]
    """
    l = list(l)
    l[::3] = sorted(l[::3])
    return l
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"stacked": None}
# ---
def setSyntax(self, syntax):
        """Choose smart indentation algorithm according to syntax"""
        self._smartIndenter = self._chooseSmartIndenter(syntax)
# ---
def test_take_with_filter_and_map(backend):
    """Test take fuses with other operations."""
    ds = (
        Dataset.from_list([list(range(20))])
        .flat_map(lambda x: x)
        .filter(lambda x: x % 2 == 0)  # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
        .take_per_shard(5)  # [0, 2, 4, 6, 8]
        .map(lambda x: x * 2)  # [0, 4, 8, 12, 16]
    )
    result = list(Backend.execute(ds, context=backend))
    assert result == [0, 4, 8, 12, 16]
# ---
def __init__(self, code, msg, headers, data, url=None):
        io.StringIO.__init__(self, data)
        self.code, self.msg, self.headers, self.url = code, msg, headers, url
# ---
def nanvar(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    ddof: int = 0,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nanvar, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype, ddof=ddof
    )
# ---
def wait_until_finish_with_callbacks(result, callbacks):
    MetricCallbackPoller(result, callbacks)
# ---
def inference_loader_pair(trainer_pair: TrainPair) -> tuple[TrainConfig, DataLoader]:
    cfg, trainer = trainer_pair
    return cfg, trainer.inference_loader
# ---
def _init_weight(key: PRNGKeyArray, shape: tuple[int, ...], std: float) -> Float[Array, "..."]:
    """Initialize weights with truncated normal."""
    return std * random.truncated_normal(key, -3, 3, shape)
# ---
def _resolve_axis_in_tree(tree, axis):
    """
    Resolves an axis in a tree of NamedArrays. This is useful for finding the batch axis in a batch of data.
    """
    for leaf in haliax.tree_util.tree_leaves(tree):
        if isinstance(leaf, haliax.NamedArray):
            try:
                return leaf.resolve_axis(axis)
            except ValueError:
                pass

    raise ValueError(f"Could not find axis {axis} in tree {tree}")
# ---
def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        raise NotImplementedError
# ---
def _searchsorted(x, y, side):
    res = nxp.searchsorted(x, y, side=side)
    # 0 is only correct for the first block of a, but blockwise doesn't have a way
    # of telling which block is being operated on (unlike map_blocks),
    # so set all 0 values to a special value and set back at the end of searchsorted
    res = nxp.where(res == 0, -1, res)
    return res[nxp.newaxis, :]
# ---
def update_floatingip_postcommit(self, context, fip_context):
        pass
# ---
def test_as_remote_kwargs_with_env_vars():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=CpuConfig())
    kwargs = as_remote_kwargs(config, env_vars={"FOO": "bar"})
    assert kwargs["runtime_env"] == {"env_vars": {"FOO": "bar"}}
# ---
def __init__(self, nr_eval, input_names, output_names, get_player_fn):
        self.eval_episode = nr_eval
        self.input_names = input_names
        self.output_names = output_names
        self.get_player_fn = get_player_fn
# ---
def decode_complex_value(
        self, field_name: str, field: FieldInfo, value: Any
    ) -> Any:
        if isinstance(value, str) and value.startswith("@"):
            with open(value[1:]) as f:
                return yaml.safe_load(f)
        return super().decode_complex_value(field_name, field, value)
# ---
def check_identical(test_list1, test_list2):
  res = test_list1 == test_list2
  return (res)
# ---
def _record_dry_run(self, step: ExecutorStep, action: str, reason: str, output_path: str) -> None:
        """Track dry-run decisions for summary output."""
        self._dry_run_plan.append((step.name, action, reason, output_path))
# ---
def output_path_affinity(input_path):
                output_dir = (
                    design_dir / const.affinity_dirname
                    if self.output_dir is None
                    else self.output_dir
                )
                return [
                    output_dir / f"{input_path.stem}.npz",
                ]
# ---
def namespace(self) -> str:
        """Get the namespace (root component) for actor isolation."""
        return self._parts[0]
# ---
def metadata(self) -> dict[str, Any]:
        return {
            "tokenizer": self.tokenizer.name_or_path,
            "vocab_size": len(self.tokenizer),
            "chat_template": self.chat_template,
            "messages_field": self.messages_field,
            "system_prompt_field": self.system_prompt_field,
            "chat_template_kwargs_field": self.chat_template_kwargs_field,
        }
# ---
def __post_init__(self):
        self._rng = np.random.default_rng(seed=self.seed)
# ---
def _add_tensor_and_parents(self, tensor):
    op = self._add_op_and_parents(tensor.op)
    return op.outputs[tensor.value_index]
# ---
def __rmul__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__rmul__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.multiply, other, self, dtype=result_type(self, other))
# ---
def test_get_has_csrf_token(self):
        self.login()
        response = self.testapp.get('/', status=200).body
        self.assertIn('CSRF Token:', response)
        self.assertEqual(response.split(':')[-1], csrf.make_token())
# ---
def test_sentinel_file_timeout(tmp_path: Path) -> None:
    """Test SentinelFile.wait raises TimeoutError when file doesn't appear."""
    sentinel = SentinelFile(str(tmp_path / "nonexistent.txt"))

    import pytest

    start = time.monotonic()
    with pytest.raises(TimeoutError, match="not signalled within"):
        sentinel.wait(timeout=Duration.from_seconds(0.1))
    elapsed = time.monotonic() - start

    # Should wait approximately the timeout duration
    assert 0.09 < elapsed < 0.2
# ---
def __init__(self, data): self.data = data
# ---
def embed(self, input_ids: NamedArray):
        x = self.token_embeddings(input_ids)
        return self.norm(x) if self.norm is not None else x
# ---
def chip_count(self) -> int:
        return self.device.chip_count()
# ---
def test_profiler_get_shorten_id(self):
        uuid_id = "4e3e0ec6-2938-40b1-8504-09eb1d4b0dee"
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        result = prof.get_shorten_id(uuid_id)
        expected = "850409eb1d4b0dee"
        self.assertEqual(expected, result)
# ---
def get_group(self, name: str) -> ScalingGroup | None:
        """Get a scale group by name."""
        return self._groups.get(name)
# ---
def test_class_method_skip(self, mock_start, mock_stop):
        self.assertEqual("foo", FakeTraceClassMethodSkip.class_method("foo"))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def slice(
        self, axis: AxisSelector, new_axis: AxisSelector | None = None, start: int = 0, length: int | None = None
    ) -> "NamedArray": ...
# ---
def make_config(src: DataSource):
        return TrainConfig.from_yaml_and_cli(
            [
                default_config,
                "--experiment.data_root",
                str(cache / src.name),
            ]
        )
# ---
def discover(self) -> str | None:
        return self._controller.url if self._controller else None
# ---
def corofunc1():
            called[0] += 1
            return event.ReturnValue(insert_events=[evt2], append_events=[evt])
# ---
def hf_checkpoint_converter(self, ref_checkpoint: Optional[str] = None) -> HFCheckpointConverter["LlamaConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfLlamaConfig,
        )
# ---
def autolink(self, link, is_email=False):
        """Rendering a given link or email address.

        :param link: link content or email address.
        :param is_email: whether this is an email or not.
        """
        text = link = escape(link)
        if is_email:
            link = 'mailto:%s' % link
        return '<a href="%s">%s</a>' % (link, text)
# ---
def _is_jit_tracer(x) -> bool:
    if isinstance(x, NamedArray):
        x = x.array
    return isinstance(x, jax.core.Tracer)
# ---
def remove_endpoint(self, endpoint_id: str) -> ControllerEndpoint | None:
        with self._lock:
            endpoint = self._endpoints.pop(endpoint_id, None)
            if endpoint:
                # Remove from task tracking
                for task_endpoints in self._endpoints_by_task.values():
                    task_endpoints.discard(endpoint_id)
            return endpoint
# ---
def test_impl(n):
            df1 = pd.DataFrame({'key1': np.arange(n), 'A': np.arange(n) + 1.0})
            df2 = pd.DataFrame({'key2': n - np.arange(n), 'A': n + np.arange(n) + 1.0})
            df3 = pd.concat([df1, df2])
            return df3.A.sum() + df3.key2.sum()
# ---
def get_step_times_from_wandb(run_id: str, entity: str | None = None, project: str = WANDB_PROJECT) -> list[float]:
    if entity is None:
        entity = _default_wandb_entity()
    run = wandb.Api().run(f"{entity}/{project}/{run_id}")
    return [
        row["throughput/duration"]
        for row in run.scan_history(keys=["throughput/duration"])
        if "throughput/duration" in row
    ]
# ---
def on_load(self):
        if 'Authentication' in self.response.headers:
            self.browser.token = self.response.headers['Authentication'].split(' ')[-1]
# ---
def __init__(self, rules):
        """Initialize the 'or' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules
# ---
def surfacearea_cube(l):
  surfacearea= 6*l*l
  return surfacearea
# ---
def are_Equal(arr1,arr2,n,m):
    if (n != m):
        return False
    arr1.sort()
    arr2.sort()
    for i in range(0,n - 1):
        if (arr1[i] != arr2[i]):
            return False
    return True
# ---
def get_references(self):
        return self.references
# ---
def replacer(match):
        old_link = match.group(0)
        new_link = f"https://wandb.ai/marin-community{match.group(1)}"
        replacements.append((old_link, new_link))
        return new_link
# ---
def build_or_load(
        cache_dir: str,
        shard_source: ShardedDataSource[T],
        processor: BatchProcessor[T, U],
        options: Optional["CacheOptions"] = None,
    ) -> "TreeCache[U]":
        if options is None:
            options = CacheOptions.default()
        return build_or_load_cache(cache_dir, shard_source, processor, options=options)
# ---
def save(self, fs, node_id):
        assert isinstance(fs, EnkfFs)
        assert isinstance(node_id, NodeId)

        EnkfNode.cNamespace().store(self, fs, True, node_id)
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            df.A.fillna(5.0, inplace=True)
            DF = df.A.fillna(5.0)
            s = DF.sum()
            m = df.A.mean()
            v = df.A.var()
            t = df.A.std()
            Ac = df.A.cumsum()
            return Ac.sum() + s + m + v + t
# ---
def should_exclude(file_path: pathlib.Path) -> bool:
    return matches_pattern(file_path, EXCLUDE_PATTERNS)
# ---
def test_contains_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.contains("b##cde", escape="#"), {7})
# ---
def is_axis_compatible(ax1: AxisSelector, ax2: AxisSelector):
    """
    Returns true if the two axes are compatible, meaning they have the same name and, if both are Axis, the same size
    """
    if isinstance(ax1, str):
        if isinstance(ax2, str):
            return ax1 == ax2
        return ax1 == ax2.name
    if isinstance(ax2, str):
        return ax1.name == ax2
    return ax1.name == ax2.name
# ---
def main(pegasus_share_dir):
    if len(sys.argv) != 2:
        usage()
        exit(1)

    if "-h" in sys.argv:
        usage()
        exit(1)

    workflowdir = sys.argv[1]
    if os.path.exists(workflowdir):
        print("ERROR: WORKFLOW_DIR '%s' already exists" % workflowdir)
        exit(1)

    workflowdir = os.path.abspath(workflowdir)
    sharedir = os.path.join(pegasus_share_dir, "init")
    w = Workflow(workflowdir, sharedir)
    w.configure()
    w.generate()
# ---
def test_visualize_shardings_plain_array(capsys):
    x = jnp.ones((4, 4))
    visualize_shardings(x)
    out = capsys.readouterr().out
    assert out.strip() != ""
# ---
def test_magic_table_props(self):
        """Table can look up properties on response object"""
        hash_key = DynamoKey("id")
        self.dynamo.create_table("foobar", hash_key=hash_key)
        ret = self.dynamo.describe_table("foobar")
        assert ret is not None
        self.assertEqual(ret.item_count, ret["ItemCount"])
        with self.assertRaises(KeyError):
            self.assertIsNotNone(ret["Missing"])
# ---
def __call__(self, x: NamedArray, group_sizes: NamedArray, *, key=None) -> NamedArray:
        k1, k2, k3 = maybe_rng_split(key, 3)

        w1_output = self.w1(x, group_sizes, key=k1)

        activated = self.act(w1_output)

        w3_output = self.w3(x, group_sizes, key=k3)

        gated = activated * w3_output

        final_output = self.w2(gated, group_sizes, key=k2)

        return final_output
# ---
def test_deadline_raise_if_expired():
    """Expired deadline raises TimeoutError."""
    deadline = Deadline.from_seconds(0.05)
    time.sleep(0.1)
    with pytest.raises(TimeoutError, match="Test timeout"):
        deadline.raise_if_expired("Test timeout")
# ---
def make_stop_wrapper(vm_id, orig_stop):
                    def stop_wrapper():
                        stop_called.append(vm_id)
                        return orig_stop()

                    return stop_wrapper
# ---
def get_network_id(self, runner):
        # FIXME: We can save on some steps if we only do this once
        obj = runner.get_plan(self.adapts).describe_object()
        return obj.get("VpcId", None)
# ---
def test_sum(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.sum(a)
    assert_array_equal(
        b.compute(executor=executor), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).sum()
    )
# ---
def output_list_item(self):
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'list_item_end':
            if self.token['type'] == 'text':
                body += self.tok_text()
            else:
                body += self.tok()

        return self.renderer.list_item(body)
# ---
def __init__(
        self,
        config: config_pb2.IrisClusterConfig,
        threads: ThreadContainer | None = None,
    ):
        self._config = config
        self._threads = threads
        self._controller: _InProcessController | None = None
        self._temp_dir: tempfile.TemporaryDirectory | None = None
        self._autoscaler: Autoscaler | None = None
# ---
def read(self):
        ''' read from file '''
        # check if it exists
        if self.filename is None or not self.file_exists():
            return None

        contents = None
        with open(self.filename) as yfd:
            contents = yfd.read()

        return contents
# ---
def gpt2_tokenizer():
    """GPT-2 tokenizer without chat template (for fallback testing)."""
    return AutoTokenizer.from_pretrained("gpt2")
# ---
def size(self) -> int:
        return len(self._resolve().endpoints)
# ---
def remove_words(self, words):
        """Remove a word or words from the list of words to auto-complete."""
        if isinstance(words, basestring):
            del self._word_freq[words]
            self._word_list.remove(words)
        else:
            for w in words:
                try:
                    del self._word_freq[w]
                    self._word_list.remove(w)
                except KeyError:
                    pass
# ---
def dict(self):
		"""
		Return a dict of the row as colname: value
		"""
		return dict(zip(self.structure.column_names, self))
# ---
def max_sub_array_sum_repeated(a, n, k): 
	max_so_far = -2147483648
	max_ending_here = 0
	for i in range(n*k): 
		max_ending_here = max_ending_here + a[i%n] 
		if (max_so_far < max_ending_here): 
			max_so_far = max_ending_here 
		if (max_ending_here < 0): 
			max_ending_here = 0
	return max_so_far
# ---
def normalizerootdir(dir, funcname):
    if dir == ".":
        util.nouideprecwarn(
            "match.%s() no longer accepts '.', use '' instead." % funcname, "20190805"
        )
        return ""
    return dir
# ---
def __exit__(self, exc_type, exc_value, traceback):
        assert Multiton._current_scope is self.scope, (
            "MultitonScope.__exit__ called without matching __enter__"
        )
        Multiton._current_scope = self.previous_scope
# ---
def job_id(self) -> str:
        return self._job_id
# ---
def test_impl(df):
            A = df.A.str.split(',')
            B = A.str.get(1)
            return B
# ---
def node_value(self, decode_str=False):
        return MpvNode.node_cast_value(byref(c_void_p(self.val)), self.format.value, decode_str)
# ---
def pos_key(p):
        p = p.replace("H", "")
        num = ""
        ins = ""
        for c in p:
            if c.isdigit():
                num += c
            else:
                ins += c
        return (int(num), ins or "")
# ---
def assert_allclose(a, b, **kwargs):
    a = to_numpy(a)
    b = to_numpy(b)
    npt.assert_allclose(a, b, **kwargs)
# ---
def gcd(x, y):
    gcd = 1
    if x % y == 0:
        return y
    for k in range(int(y / 2), 0, -1):
        if x % k == 0 and y % k == 0:
            gcd = k
            break  
    return gcd
# ---
def list_all_endpoints(self) -> list[ControllerEndpoint]:
        """Return all registered endpoints."""
        with self._lock:
            return list(self._endpoints.values())
# ---
def __init__(self, urls):
        super().__init__(urls)
# ---
def explain(self, f):
        return self._matcher.explain(f, True)
# ---
from typing import List


def all_prefixes(string: str) -> List[str]:
    """ Return list of all prefixes from shortest to longest of the input string
    >>> all_prefixes('abc')
    ['a', 'ab', 'abc']
    """
    result = []

    for i in range(len(string)):
        result.append(string[:i+1])
    return result
# ---
def load_apartment_by_mailbox_id(mailbox_id):
        # type: (int) -> Optional[ApartmentDTO]
        apartment_orm = Apartment.select().where(Apartment.mailbox_rebus_id == mailbox_id).first()
        if apartment_orm is None:
            return None
        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
        return apartment_dto
# ---
def test_zero_deadline_expires_immediately():
    """Deadline with zero timeout expires immediately."""
    deadline = Deadline.from_ms(0)
    assert deadline.expired()
    assert deadline.remaining_ms() == 0
    assert deadline.remaining_seconds() == 0.0
# ---
def __init__(self, req = None):
        if req is not None:
            self.req = req
            etalage_env = req.environ.get('etalage', {})
            for key in object.__getattribute__(self, 'env_keys'):
                value = etalage_env.get(key)
                if value is not None:
                    setattr(self, key, value)
# ---
def __init__(self, sigma_min=1e-3, sigma_max=1):
    super().__init__()
    self.sigmas = 1.0 * torch.tensor([sigma_min, sigma_max])
# ---
def __init__(self):
            super().__init__()
            self.a = jnp.zeros(1)
            self.b = hax.named(jnp.zeros(3), "a")
# ---
def _component_cache_dir(name: str, component: DatasetComponent, default_root: str | None) -> str:
    base = component.cache_dir if component.cache_dir is not None else default_root
    if base is None:
        raise ValueError(f"No cache_dir provided for component {name}")
    if component.cache_dir is None:
        return os.path.join(base, name)
    return base
# ---
def init(In: hax.Axis, Out: hax.Axis, Mid: hax.Axis, *, key):
        w_in = hax.random.normal(key, hax.concat_axis_specs(In, Mid)) * 0.02
        w_out = hax.random.normal(key, hax.concat_axis_specs(Mid, Out)) * 0.02
        return Mlp(w_in, w_out, In, Out, Mid)
# ---
def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1.two, df2.two])
            return (A3 == 'foo').sum()
# ---
def is_heartbeat_expired(self, timeout: Duration) -> bool:
        """Check if this worker's heartbeat has expired.

        Args:
            timeout: Heartbeat timeout duration

        Returns:
            True if the worker has not sent a heartbeat within the timeout period
        """
        return self.last_heartbeat.age_ms() > timeout.to_ms()
# ---
def testClassDefWithVar(self):
    self.assertEqual((0, 'abc\n'), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          bar = 'abc'
        print Foo.bar""")))
# ---
def characterization_insertion_CRISPR(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'characterization',
        'method': 'CRISPR',
        'modified_site_nonspecific': 'random',
        'introduced_elements': 'synthesized DNA'
    }
# ---
def remote(self, *args: Any, **kwargs: Any) -> ActorFuture:
        """Submit method call to thread pool, returning a future."""
        return self._executor.submit(self._method, *args, **kwargs)
# ---
def cancel_or_abort_call(self, ar):
        """
        Either cancels a future pending call, or aborts the current processing if the given AR is unset.

        The pending call is keyed by the AsyncResult returned by _routing_call.
        """
        if not self._cancel_pending_call(ar) and not ar.ready():
            self._interrupt_control_thread()
# ---
def visitdir(self, dir):
        if self(dir):
            return "all"
        return self._matcher.visitdir(dir)
# ---
def _decode_tokens_pretty(tok, ids):
    # we want to make sure we don't have any weird characters in the output
    # so we'll decode the tokens and then escape them
    if hasattr(tok, "convert_ids_to_tokens"):
        return [str(t) for t in tok.convert_ids_to_tokens(ids)]
    else:
        return [str(t) for t in tok.decode(ids)]
# ---
def make_state(key):
        model = MLP(in_size=2, out_size=1, width_size=2, depth=3, key=key)
        optim = optax.adam(1e-4)
        opt_state = optim.init(arrays_only(model))

        return model, opt_state, key
# ---
def loss_ref_batched(v):
        return jnp.sum(reference_impl_batched(v))
# ---
def filter(self, record: Record) -> bool:
        """Filter structures based on their resolution.

        Parameters
        ----------
        record : Record
            The record to filter.

        Returns
        -------
        bool
            Whether the record should be filtered.

        """
        num_chains = record.structure.num_chains
        num_valid = sum(1 for chain in record.chains if chain.valid)
        return num_chains <= self.max_chains and num_valid >= self.min_chains
# ---
def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return df.two.nunique()
# ---
def try_load_path(path):
        try:
            fs, path_to_open = _get_fs_and_plain_path(path)
            fs.open(path_to_open, "rb")
        except Exception:
            return False
        else:
            return True
# ---
def foo(x: f32[NamedArray, "batch embed"]):  # type: ignore  # noqa: F722
        pass
# ---
def switch_func(txn):
            txn.date = swapped_date(txn.date, first, second)
# ---
import re
def find_character(string):
  uppercase_characters = re.findall(r"[A-Z]", string) 
  lowercase_characters = re.findall(r"[a-z]", string) 
  numerical_characters = re.findall(r"[0-9]", string) 
  special_characters = re.findall(r"[, .!?]", string) 
  return uppercase_characters, lowercase_characters, numerical_characters, special_characters
# ---
def test_squeeze_2d(spec, executor):
    a = xp.asarray([[[1], [2], [3]]], chunks=(1, 2, 1), spec=spec)
    b = xp.squeeze(a, (0, 2))
    assert_array_equal(
        b.compute(executor=executor), np.squeeze([[[1], [2], [3]]], (0, 2))
    )
# ---
def __init__(self, exc_info):
        self.exc_type = exc_info[0]
        self.exc = exc_info[1]
        self.tb = tblib.Traceback(exc_info[2])
# ---
def __init__(self, 
                row_dict_I,
                ):
        self.analysis_id=row_dict_I['analysis_id'];
        self.experiment_id=row_dict_I['experiment_id'];
        self.sample_name_abbreviation=row_dict_I['sample_name_abbreviation'];
        self.sample_name=row_dict_I['sample_name'];
        self.time_point=row_dict_I['time_point'];
        self.analysis_type=row_dict_I['analysis_type'];
        self.used_=row_dict_I['used_'];
        self.comment_=row_dict_I['comment_'];
# ---
def _insert_vm_to_db(self, uuid, name, state):
        vm = models.VM(uuid=uuid, name=name, state=state)
        vm.save()
# ---
def set_parameters(self, parameters_dict):

        if "power" in parameters_dict:
            self.set_power(parameters_dict["power"])

        if "freq" in parameters_dict:
            self.set_frequency(parameters_dict["freq"])

        if "dac_overridden" in parameters_dict:
            self._dac_overridden = parameters_dict["dac_overridden"]
        else:
            self._dac_overridden = False
# ---
def _combine_ffmpeg_command(
            sourcefolder, moviename, framerate, frame_pattern, ffmpeg_options
        ):
            # we need `-y` because i can not properly diagnose the errors here...
            command = (
                f'ffmpeg -r {framerate} -i "{os.path.join(sourcefolder, frame_pattern)}"'
                f" -y {ffmpeg_options} -r {framerate}"
                f' "{os.path.join(sourcefolder, moviename)}"'
            )
            return command
# ---
def update_demand(self, demand: int) -> None:
        """Update current demand."""
        self._current_demand = demand
        self._peak_demand = max(self._peak_demand, demand)
# ---
def init(HeadDim, config):
        return Llama3RotaryEmbeddings(HeadDim, config)
# ---
def royal_order(last_id=-1):
    '''Retrive royal orders
    if last_id not defiend it will return the max
    return list of royal orders tuples up to MAX_PAGES_TO_SEARCH (page=10)
    [(id, title, url, text)...]
    '''
    orders = []
    _news = retrieve_news(royal=1, last_id=last_id)
    for item in _news:
        _detail = retrieve_detail(item)
        orders.append(_detail)
    return orders
# ---
def get_numbers_as_list(midi_str):
    '''
    Translate each char into a number, return in a list.
    Used for reading data messages where each byte encodes
    a different discrete value.

    >>> get_numbers_as_list('\\x00\\x00\\x00\\x03')
    [0, 0, 0, 3]
    '''
    post = []
    for item in midi_str:
        if is_num(item):
            post.append(item)
        else:
            post.append(ord(item))
    return post
# ---
def test_stmt_exception_pickleable_no_dbapi(self):
        self._test_stmt_exception_pickleable(Exception("hello world"))
# ---
def process_latex_matrices(text: str) -> str:
    """Convert matrix environments to simple bracket notation."""

    def matrix_replacer(content: str) -> str:
        return f"[{content}]"

    for env in ["matrix", "pmatrix", "bmatrix", "vmatrix", "Vmatrix"]:
        text = replace_latex_environment(text, env, matrix_replacer)

    return text
# ---
def sum_of_digits(nums):
    return sum(int(el) for n in nums for el in str(n) if el.isdigit())
# ---
def step_impl(context):
    context.execute_steps(u'''
        given I open History dialog
    ''')
    # Click on import
    context.browser.find_element_by_id('ImportHistory').click()
    WebDriverWait(context.browser, 10).until(
        expected_conditions.visibility_of_element_located(
            (By.ID, 'ImportRequestForm')))
# ---
def max_sum_rectangular_grid(grid, n) : 
	incl = max(grid[0][0], grid[1][0]) 
	excl = 0
	for i in range(1, n) : 
		excl_new = max(excl, incl) 
		incl = excl + max(grid[0][i], grid[1][i]) 
		excl = excl_new 
	return max(excl, incl)
# ---
def test_no_flexibility():
    partial_order = ("apple", "banana")
    candidates = ("banana", "apple", "cherry")
    with pytest.raises(ValueError):
        rearrange_for_partial_order(partial_order, candidates)
# ---
def main():
    """Create calibration coefficient files for AVHRR."""
    out_dir = sys.argv[1]
    coeffs = get_all_coeffs()
    save_coeffs(coeffs, out_dir=out_dir)
# ---
def create(self,problem,**kwargs):
    path = self.pathbase
    payload=json.dumps(problem)
    params = self.params.copy()
    params.update(kwargs)
    req = self.session.post(path,
                        params=params, 
                        headers=self.headers,
                        data=payload)    
    self.validateReply(req)
    return req.text
# ---
def __init__(self, execute = False, ip = '127.0.0.1', port = 5000, npc = False):
		self.ip = ip
		self.port = port
		self.npc = npc
		if (execute):
			self.iniciaBatalha()
# ---
def _make_and_expr(self, check1, _and, check2):
        """Create an 'and_expr'.

        Join two checks by the 'and' operator.
        """

        return [('and_expr', AndCheck([check1, check2]))]
# ---
def tearDown(self):
        super(BaseSystemTest, self).tearDown()
        for tablename in self.dynamo.list_tables():
            self.dynamo.delete_table(tablename)
        self.dynamo.clear_hooks()
# ---
def TextAfterCursor():
  """Returns the text after CurrentColumn."""
  return ToUnicode( vim.current.line[ CurrentColumn(): ] )
# ---
def test_prefix(self):
        import functools
        other_event_deco = functools.partial(event.event, _prefix="__test_")

        @other_event_deco
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.event_name == '__test_test'
# ---
def __init__(self, what, expected, detail=None):
        message = f"'{what}' is not {expected}"
        if detail:
            message = f"{message}: {detail}"
        super().__init__(message)
# ---
def locale_from_currency_code(dx_code):
    """
    This is a (temporary) hardcoded mapping between currency_list.json in nucleus and standard
    locale string useful for further formatting

    :param dx_code: An id of nucleus/commons/pricing_models/currency_list.json collection
    :return: standardised locale, eg 'en_US'; None when no mapping found
    """
    currency_locale_map = {0: 'en_US', 1: 'en_GB'}
    return currency_locale_map[dx_code] if dx_code in currency_locale_map else None
# ---
def the_object_name_is_voided_by_void(name, void):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    for rel in element.HasOpenings:
        if rel.RelatedOpeningElement.Name == void:
            return True
    assert False, "No void found"
# ---
def __len__(self) -> int:
        """Get the length of the dataset.

        Returns
        -------
        int
            The length of the dataset.

        """
        total = len(self.yaml_paths) * (self.dataset.multiplicity - self.skip_offset)
        return max(total, 0)
# ---
def test_groupby_reduction_axis1():
    a = xp.full((5, 4 * 6), 7.0, chunks=(2, 4))
    b = xp.asarray([0, 1, 0, 1] * 6, chunks=(4,))
    c = mean_groupby_reduction(a, b, axis=1, num_groups=2)
    assert_array_equal(c.compute(), np.full((5, 2), 7))
# ---
def _create(self):
        raise NotImplementedError('abstract')
# ---
def reduceCongr(a, b, m):
        gcdAB = gcd(a, b)
        a /= gcdAB
        b /= gcdAB
        m /= gcd(gcdAB, m)
        modinv = modInverse(a, m)
        b *= modinv
        return (1, b, m)
# ---
def unembed_active_scale(self):
        """Scaling factor applied when unembedding embeddings."""
        raise NotImplementedError
# ---
def rights(self, val):
        self.opt_meta['rights'] = _EpubMeta('dc:rights', '' + val)
# ---
def testSComplexBasic(self):
    x = np.arange(1., 5.).reshape([4, 1]).astype(np.complex64)
    y = np.arange(1., 3.).reshape([1, 2]).astype(np.complex64)
    self._testCpuMatmul(x, y)
# ---
def __repr__(self):
        return "<exactmatcher files=%r>" % self._files
# ---
def residual_linear(x, W, x_skip, residual_scale):
  """x_skip + residual_scale * W @ x"""
  dim_out, dim_in = W.shape[0], W.shape[1]
  return torch.addmm(
    x_skip.view(-1, dim_out),
    x.view(-1, dim_in),
    W.T,
    alpha=residual_scale).view(*x.shape[:-1], dim_out)
# ---
def test_check_health_passes_duration_to_run():
    """check_health passes Duration to conn.run without TypeError."""
    conn = FakeSshConnection()
    result = check_health(conn, port=10001)
    assert result.healthy is True
    assert conn.last_timeout == Duration.from_seconds(10)
# ---
def test_three_equal(x,y,z):
  result= set([x,y,z])
  if len(result)==3:
    return 0
  else:
    return (4-len(result))
# ---
def __next__(self) -> T:
        start = time.perf_counter()
        item = next(self.items)
        self.this_load_time = time.perf_counter() - start
        self.total_time += self.this_load_time
        return item
# ---
def pronunciationFocusChange(self, widget, event, isFocused):
        """Callback for the pronunciation tree's focus-{in,out}-event signal."""

        _settingsManager.setSetting('usePronunciationDictionary', not isFocused)
# ---
def vm_count(self) -> int:
        """Return the number of registered VMs."""
        with self._lock:
            return len(self._vms)
# ---
def chip_count(self) -> int:
        """Total accelerator chips across all replicas/slices."""
        return self.device.chip_count() * self.replicas
# ---
def run(self) -> dict[str, Any]:
        """Main entry point for replaying completions."""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            return loop.run_until_complete(self.run_async())
        finally:
            loop.close()
# ---
def worker_id(self) -> WorkerId | None:
        """Worker from the most recent attempt, if any.

        Returns the worker ID from the last attempt regardless of whether
        the attempt is terminal. This is used for reporting which worker
        ran (or is running) the task.
        """
        if self.attempts:
            return self.attempts[-1].worker_id
        return None
# ---
def test_expr_in_set(self):
        expr1 = col("score") > 0.5
        expr2 = col("score") > 0.5
        s = {expr1}
        assert expr2 in s
# ---
def create_multi_layer_neural_network(input_vars, out_dims, num_hidden_layers):

        num_hidden_neurons = 128

        hidden_layer = lambda: Dense(num_hidden_neurons, activation=cntk.ops.relu)
        output_layer = Dense(out_dims, activation=None)

        model = Sequential([LayerStack(num_hidden_layers, hidden_layer),
                            output_layer])(input_vars)
        return model
# ---
def _sampling_noise(self):
    noise = self.sampler.sample()
    beta = self.k / torch.arange(1, self.num_betas + 1, 1,
                                 dtype=torch.float32)
    beta = beta[:, None, None]
    assert beta.ndim == noise.ndim
    s = noise / beta
    s = torch.sum(s, axis=0)
    s = s - math.log(10.0)
    s = self.gamma_tau * (s / self.k)
    return s
# ---
def go(t):
    o = option.Option(**{'handle': t, 'type': t})
    o.validate()
    return o
# ---
def __rshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.right_shift(self, other)
# ---
def get_train_aggregator() -> TrainAggregator:
        return TrainAggregator()
# ---
def is_root(self) -> bool:
        """True if this is a root job (no parent)."""
        return len(self._parts) == 1
# ---
def __init__(self, repeat: MinimumOfRepetition):
        self.repeat = repeat
        self.max = []
# ---
def list_tuple(listx):
  tuplex = tuple(listx)
  return tuplex
# ---
def on_task_end(self, event):
        self.peak_measured_mem = event.peak_measured_mem_end
# ---
def __init__(self, prev):
        self.prev = prev  # ContentOfGroup or CharClass
        self.pattern = ast.PatternChar()
        self.pattern.type = ast.PatternChar.Unicode

        self.prev.add(self.pattern)
# ---
def _getPitchForVoiceType(self, voiceType):
        """Gets the pitch value for the given voice type.

        Arguments:
        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM

        Returns the pitch value for the given voice type, or None if
        not set.
        """

        return self._getKeyValueForVoiceType(voiceType,
                                             acss.ACSS.AVERAGE_PITCH)
# ---
def softmax_no_cast(t: torch.Tensor, dim: int = -1) -> torch.Tensor:
    """
    Softmax, but without automatic casting to fp32 when the input is of
    type bfloat16
    """
    d = t.dtype
    if d is torch.bfloat16:
        with torch.autocast("cuda", enabled=False):
            s = torch.nn.functional.softmax(t, dim=dim)
    else:
        s = torch.nn.functional.softmax(t, dim=dim)

    return s
# ---
def _read_jsonl_gz(path: Path) -> list[dict]:
    """Read records from a gzipped JSONL file."""
    records = []
    with gzip.open(path, "rt", encoding="utf-8") as handle:
        for line in handle:
            if line.strip():
                records.append(json.loads(line))
    return records
# ---
def unique_sublists(list1):
    result ={}
    for l in  list1: 
        result.setdefault(tuple(l), list()).append(1) 
    for a, b in result.items(): 
        result[a] = sum(b)
    return result
# ---
def list_tasks(self, request: cluster__pb2.Worker.ListTasksRequest, ctx: RequestContext) -> cluster__pb2.Worker.ListTasksResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def check_key_function(key_function, out_coords, expected_str):
    res = key_function(("out",) + out_coords)

    assert str(tuple(iter_repr_nested(r) for r in res)) == expected_str
# ---
def convert_tensor_out_to_dict(tensor_out: torch.Tensor) -> DictSingleChannelVar:
    tensor_map = TensorMap.get_instance()
    assert tensor_out.ndim == 5
    assert tensor_out.shape[2] == len(tensor_map.prognostic_var_names)
    out_dict = {}
    for i, var in enumerate(tensor_map.prognostic_var_names):
        out_dict[var] = tensor_out[:, :, i]
    out_dict.update(add_derived_variables(tensor_out))
    return out_dict
# ---
def _rechunk_r1(R1, split_every=4):
    # expand R1's chunk size in axis 0 so that new R1 will be smaller by factor of split_every
    if R1.numblocks[0] == 1:
        raise ValueError(
            "Can't expand R1 chunk size further. Try increasing allowed_mem"
        )
    chunks = (R1.chunksize[0] * split_every, R1.chunksize[1])
    return merge_chunks(R1, chunks=chunks)
# ---
def num_slices(self):
        """number of nodes"""
        return max(getattr(device, "slice_index", 0) for device in jax.devices()) + 1
# ---
def cm4(self):
        """Process the CM4 oceans dataset (a coupled ocean model from CMIP)."""
        raise NotImplementedError("Not yet implemented, please come back later!")
# ---
def test_normal():
    check_gen_is_equal(jax.random.normal, hax.random.normal)
# ---
def enroll_student(self, email, password, course):
        """
        Student login and enroll for the course
        """
        self.login(email, password)
        self.enroll(course, verify=True)
# ---
def TerminateJob(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __init__(self,*args,**kwargs):
            super(ImportFrom,self).__init__(*args,**kwargs)
            HOST_TYPE=((1,"001"),(2,"002"))  #

            self.fields['host_type'].widget.choices = models.userInfo.objects.all().values_list("id","name")
            models.userInfo.objects.get()
            models.userInfo.objects.filter()
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        client = self._handle._resolve()
        return getattr(client, self._method)(*args, **kwargs)
# ---
def test_unify_chunks_broadcast_scalar():
    a = xp.ones((10,), chunks=(3,))
    b = a + 1
    assert_array_equal(b.compute(), np.ones((10,)) + 1)
# ---
def load_checkpoint(self, ckpt_path: str):
        checkpoint = torch.load(ckpt_path, map_location=torch.device(self.device))
        model_state_dict = checkpoint["model"]
        new_state_dict = OrderedDict()
        for k, v in model_state_dict.items():
            name = k.removeprefix("module.")
            new_state_dict[name] = v
        self.model.load_state_dict(new_state_dict)
# ---
def get_mod_ndwi(b):
    return b['b6'].subtract(b['b4']).divide(b['b4'].add(b['b6'])).select(['sur_refl_b06'], ['b1'])
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.sorts == other.sorts
# ---
def testFormatInode(self):
    """Tests the _FormatInode function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    inode_string = test_helper._FormatInode(
        event, event_data, event_data_stream)
    self.assertEqual(inode_string, '-')
# ---
def get_queryset(self):
        queryset = Ticket.get_user_related_tickets(self.request.user)
        return queryset
# ---
def c_out(self, sigma):
        return sigma * self.sigma_data / torch.sqrt(self.sigma_data**2 + sigma**2)
# ---
def inference_server(trainer_config, baby_llama_config, loaded_model):
    """Create an InferenceServer instance."""
    model, tokenizer = loaded_model
    with trainer_config.use_device_mesh(), hax.axis_mapping(trainer_config.compute_axis_mapping):
        return InferenceServer.create(baby_llama_config, model, tokenizer)
# ---
def test_retrieval(self, expected, actual):
        assert expected == actual
# ---
def crispr_deletion(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'deletion',
        'purpose': 'repression',
        'method': 'CRISPR'
    }
# ---
def count_Unset_Bits(n) :  
    cnt = 0;  
    for i in range(1,n + 1) : 
        temp = i;  
        while (temp) :  
            if (temp % 2 == 0) : 
                cnt += 1;  
            temp = temp // 2;  
    return cnt;
# ---
def deploymentconfig(self, config):
        ''' setter for deploymentconfig property '''
        self.dconfig = config
# ---
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.counter = 0
    self.restarting = False
# ---
def unassign_task(self, task_id: JobName, resources: cluster_pb2.ResourceSpecProto) -> None:
        """Unassign a task from this worker, updating committed resources."""
        self.running_tasks.discard(task_id)
        self.committed_cpu -= resources.cpu
        self.committed_mem -= resources.memory_bytes
        self.committed_gpu -= get_gpu_count(resources.device)
        self.committed_tpu -= get_tpu_chip_count(resources.device)
# ---
def render(self, view_name, notebook, data=None, method='GET', **kwargs):
        if data is None:
            data = {}
        try:
            view = views[view_name]
        except KeyError:
            raise ResponseError(404)
        template_data = view(method, notebook, data, **kwargs)
        if isinstance(template_data, tuple):
            template_name, template_data = template_data
        else:
            template_name = view.__name__ + '.html'
        return template_name, template_data
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids)
        x = self.norm(x)
        return x
# ---
def test_block_id_to_offset():
    numblocks = (5, 3)
    for block_id in itertools.product(*[list(range(n)) for n in numblocks]):
        offset = block_id_to_offset(block_id, numblocks)
        assert offset_to_block_id(offset, numblocks) == block_id

    with pytest.raises(ValueError, match="invalid entry in coordinates array"):
        block_id_to_offset((6, 12), numblocks)

    with pytest.raises(ValueError, match="index 100 is out of bounds for array"):
        offset_to_block_id(100, numblocks)
# ---
def test_write_bytes_to_buffer(self):
        """In python3, stdout / stderr are text io wrappers (exposing a buffer
        property of the underlying bytestream).  See issue #1407
        """
        f = capture.CaptureIO()
        f.buffer.write(b'foo\r\n')
        assert f.getvalue() == 'foo\r\n'
# ---
def __float__(self) -> float:  # pragma: no cover
        return float(self.array)
# ---
def fetch_server(self) -> ServerInfo:
        return self._server_info
# ---
def _loop(self):
        while True:
            try:
                self.lc.handle()
            except Exception as ex:
                print('Got exception while handling lcm message', ex)
# ---
def items(self):
		return [(k, self[k]) for k in self]
# ---
def mod_ndwi(domain, b, threshold=None):
    if threshold == None:
        threshold = float(domain.algorithm_params['mod_ndwi_threshold'])
    return get_mod_ndwi(b).lte(threshold)
# ---
def _find_base_path(input_path: str | list[str], input_files: list[str]) -> str:
    # Determine base path for rebasing
    base_path = input_path[0] if isinstance(input_path, list) else input_path
    if base_path in input_files:
        # NOTE: if the base_path is in the input_files, means it's a specific file, so rebase to its directory
        base_path = os.path.dirname(base_path)
    return base_path
# ---
def testOneValue(self):
        gen = VectorInput([1])
        tcToTotal = sTCToTotal()
        p = Pool()

        gen.data >> tcToTotal.envelope
        tcToTotal.TCToTotal >> (p, 'lowlevel.tctototal')

        self.assertRaises(RuntimeError, lambda: run(gen))
# ---
def new_func(func):
        def wrap(*a, block_id=None, **kw):
            arrays = kw.pop("arrays")
            args = a + arrays
            return func(*args, block_id=block_id, **kw)

        return wrap
# ---
def max_queued_tokens(self) -> int:
        """Maximum number of tokens that can be buffered in the queue."""
        return self.queued_tokens.axis_size("position")
# ---
def test_basic_actor_call():
    """Test basic actor method calls work correctly."""
    server = ActorServer(host="127.0.0.1")
    server.register("calc", Calculator())
    port = server.serve_background()

    try:
        resolver = FixedResolver({"calc": f"http://127.0.0.1:{port}"})
        client = ActorClient(resolver, "calc")
        assert client.add(2, 3) == 5
        assert client.multiply(4, 5) == 20
    finally:
        server.stop()
# ---
def _ensure_width(fig, target_w=8.5):
            w, h = fig.get_size_inches()
            if abs(w - target_w) > 0.01:
                scale = target_w / w
                fig.set_size_inches(target_w, h * scale, forward=True)
# ---
def contains(self, bbox, srs):
        bbox = self._geom_in_coverage_srs(bbox, srs)
        with self._prep_lock:
            return self.prepared_geom.contains(bbox)
# ---
def config(self) -> MConfig:
        pass
# ---
def div_even_odd(list1):
    first_even = next((el for el in list1 if el%2==0),-1)
    first_odd = next((el for el in list1 if el%2!=0),-1)
    return (first_even/first_odd)
# ---
def map(self, fn: Callable[[T_co], U]) -> "ShardedDataSource[U]":
        return _MappedShardedDataSource(self, fn)
# ---
def raise_for_ec(kls, ec, func, *args):
        ec = 0 if ec > 0 else ec
        ex = kls.EXCEPTION_DICT.get(ec , kls.default_error_handler)
        if ex:
            raise ex(ec, *args)
# ---
def run(self):
        self.call_count += 1
        print(f"Running {self.fn_id}, call count: {self.call_count}")
        if self.call_count < self.preempt_until_n_calls:
            raise TimeoutError("Simulated preemption via TimeoutError")

        return np.zeros(1)
# ---
def init(Layers, *, key):
            stack = hax.nn.Stacked.init(Layers, Module)(
                layer_idx=hax.arange(Layers), key=jax.random.split(key, Layers.size)
            )
            return Model(layers=stack)
# ---
def open_pager(self):
        "Open the selected item with the system's pager"
        data = self.get_selected_item()
        if data['type'] == 'Submission':
            text = '\n\n'.join((data['permalink'], data['text']))
            self.term.open_pager(text)
        elif data['type'] == 'Comment':
            text = '\n\n'.join((data['permalink'], data['body']))
            self.term.open_pager(text)
        else:
            self.term.flash()
# ---
def test_filter_passing_none_pass():
    candidates = [
        _make_candidate("def f(x):\n    return 0\n"),
        _make_candidate("def f(x):\n    return -1\n"),
    ]
    tests = ["assert f(1) == 1"]
    passing = filter_passing(candidates, tests)
    assert passing == []
# ---
def _check_for_unused_aliases(axis_aliases, used_aliases, equation):
    if any(alias not in used_aliases for alias in axis_aliases):
        unused_aliases_str = ", ".join([alias for alias in axis_aliases if alias not in used_aliases])
        raise_parse_error(f"Unused aliases from kwargs: {unused_aliases_str}", equation, None)
# ---
def num_cpus(self) -> int:
        """The number of CPUs this processor needs to run."""
        return self.bt.num_cpus
# ---
def __str__(self):
        return "NoncentralFDistr(df1={0},df2={1},lambda={2})#{3}".format(self.df1, self.df2, self.lmbda, self.id())
# ---
def test_endswith_autoescape_escape(self):
        col = self.tables.some_table.c.data
        self._test(col.endswith("e%fg", autoescape=True, escape="#"), {6})
        self._test(col.endswith("e#fg", autoescape=True, escape="#"), {9})
# ---
def to_proto(self) -> "time_pb2.Timestamp":
        """Convert to proto Timestamp message."""
        return time_pb2.Timestamp(epoch_ms=self._epoch_ms)
# ---
def sample(self, node_type: str, rng: random.Random) -> SubtreeEntry | None:
        """Sample a random subtree of the given AST node type.

        Returns None if no subtrees of that type are in the bank.
        """
        candidates = self.entries.get(node_type)
        if not candidates:
            return None
        return rng.choice(candidates)
# ---
def loc(self):
        """Attribute for location based indexing. Only supports __getitem__,
        and only when the key is a dict of the form {dim: labels}.
        """
        return _LocIndexer(self)
# ---
def path_to_step_name(path):
    # we want llama-8b-tootsie-phase2-730000
    components = path.split("/")
    step = components[-2].split("-")[-1]
    name = components[-4].split("/")[-1]
    return f"analysis/viz/{name}-{step}"
# ---
def on_close(self):
        """Default on_close handler."""
        self.has_exit = True
        from pyglet import app
        if app.event_loop.is_running:
            self.close()
# ---
def target(stop_event: threading.Event) -> None:
            ctx.run(self._run, stop_event)
# ---
def tanh(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in tanh")
    return elemwise(nxp.tanh, x, dtype=x.dtype)
# ---
def test_pack_empty_tree():
    tree = {}
    offsets, packed = pack_pytree(tree, dtype=jnp.float32)
    assert packed.size == 0
    rebuilt = unpack_pytree(offsets, packed)
    assert rebuilt == tree
# ---
def _assign_linear_weight(named_linear: hnn.Linear, np_weight: jnp.ndarray, out_axis: Axis, in_axis: Axis):
            w_named = hax.named(jnp.asarray(np_weight, dtype=jnp.float32), (out_axis.name, in_axis.name))
            return dataclasses.replace(named_linear, weight=w_named)
# ---
def binomial_coeff(n, k): 
	C = [[0 for j in range(k + 1)] 
			for i in range(n + 1)] 
	for i in range(0, n + 1): 
		for j in range(0, min(i, k) + 1): 
			if (j == 0 or j == i): 
				C[i][j] = 1
			else: 
				C[i][j] = (C[i - 1][j - 1] 
							+ C[i - 1][j]) 
	return C[n][k] 
def lobb_num(n, m): 
	return (((2 * m + 1) *
		binomial_coeff(2 * n, m + n)) 
					/ (m + n + 1))
# ---
def __init__(self, doc_cache: TreeCache[dict], seq_len: int):
        super().__init__()
        self.doc_cache = doc_cache
        self.seq_len = seq_len
        self._store: TreeStore | None = doc_cache.store
        self._cached_len: int | None = None
# ---
def extract_id(row: dict, corpus_type: str) -> str:
    """Extract ID from row based on corpus type.

    Recursively navigates nested structures as defined in CORPUS_TYPE_TO_ID_GUIDE."""
    guide = CORPUS_TYPE_TO_ID_GUIDE[corpus_type]

    # grab the key, then navigate nested if needed
    val = row[guide["key"]]

    while "nested" in guide:
        nested = guide["nested"]
        assert isinstance(nested, dict)
        val = val[nested["key"]]
        guide = nested

    return val
# ---
def showProfileGUI(self, widget):
        """Show profile Dialog to add a new one"""

        orca_gui_profile.showProfileUI(self)
# ---
def validation_sets(self) -> Mapping[str, AsyncDataset[np.ndarray]]:
        doc_caches = self.build_caches("validation")
        return doc_caches
# ---
def is_finite(self) -> bool:
        return all(dataset.is_finite() for dataset, _ in self.datasets)
# ---
def KeyPos(self) -> Axis:
        return self.Pos.alias("key_position")
# ---
def test_axis_shapes_inherit_defaults_and_absorb():
    cfg = MeshConfig(axes={"model": 2})
    ici, dcn = cfg.axis_shapes(num_devices=8, num_slices=1)
    # data should absorb remaining ICI after replica=1, model=2 -> data = 4
    assert ici == {"data": 4, "replica": 1, "model": 2}
    # replica_dcn should absorb all slices by default
    assert dcn == {"replica_dcn": 1}
# ---
def test_response_tokens_from_choice(inference_ctx, llama3_tokenizer):
    """Test extracting token IDs from Choice using BPE round-trip."""
    response_text = "The answer is 42"
    choice = create_choice_with_logprobs(llama3_tokenizer, response_text)

    tokens = inference_ctx.response_tokens_from_choice(choice)

    # Should match tokenizer's encoding
    expected_tokens = llama3_tokenizer.encode(response_text, add_special_tokens=False)
    np.testing.assert_array_equal(tokens, expected_tokens)
# ---
def _set_tunnel(self, host, port=None, headers=None):
        self._tunnel_host = host
        self._tunnel_port = port
        if headers:
            self._tunnel_headers = headers
        else:
            self._tunnel_headers.clear()
# ---
def bind(self, read_server_info=True):
        return self.bound
# ---
def outer(x1, x2, /):
    return blockwise(
        nxp.linalg.outer, "ij", x1, "i", x2, "j", dtype=result_type(x1, x2)
    )
# ---
def __add__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.add(self, other)
# ---
def get_stop_tokens(tokenizer_name: str):
    """Get stop tokens from tokenizer."""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    return [tokenizer.eos_token_id]
# ---
def attrs(self):
        """Dictionary of global attributes on this dataset
        """
        if self._attrs is None:
            self._attrs = OrderedDict()
        return self._attrs
# ---
def __init__(self, dim):
        super().__init__()
        self.proj = nn.Module()
        self.proj.register_buffer("weight", torch.randn(dim, 1))
        self.proj.register_buffer("bias", torch.randn(dim))
# ---
def _get_view_id(self, cr, uid, type):
        """Get the view id suiting the given type

        @param type: the picking type as a string
        @return: view i, or False if no view found
        """
        res = self.pool.get('ir.model.data').get_object_reference(cr, uid, 
            'stock', self._VIEW_LIST.get(type, 'view_picking_form'))            
        return res and res[1] or False
# ---
def __del__(self):
        print("Updated " + self.tableName + " table.")
# ---
def test_rademacher():
    check_gen_is_equal(lambda k, s: jax.random.rademacher(k, s), lambda k, s: hax.random.rademacher(k, s))
# ---
def scan_via(
        self, fn: Callable[..., tuple[CarryT, OutputT_co]], *, unroll: int | bool | None = None
    ) -> Callable[P, tuple[CarryT, OutputT_co]]: ...
# ---
def __repr__(self):
        return "Grid with %i knots"%self.nbknots
# ---
def get_user_id(user: UserProfile) -> int:
    return user.id
# ---
def gensym(name: str) -> str:
    global sym_counter
    sym_counter += 1
    return f"{name}-{sym_counter:03}"
# ---
def loadPlugin(plugin_name): 
    """
    @type plugin_name: str
    """
    pass
# ---
def __init__(self, items: Iterable[T]):
        self.total_time = 0.0
        start = time.perf_counter()
        self.items = iter(items)
        self.total_time += time.perf_counter() - start
        self.this_load_time = 0.0
# ---
def test_cluster(use_docker, docker_cleanup_scope):
    """Provide a running test cluster for E2E tests."""
    with E2ECluster(use_docker=use_docker) as cluster:
        yield cluster
# ---
def on_lower_bound_changed(self):
        self.ui.spinBoxUpperBound.setMinimum(self.ui.spinBoxLowerBound.value())
        self.ui.spinBoxBoundaryNumber.setMaximum(math.ceil((self.ui.spinBoxUpperBound.value()
                                                            - self.ui.spinBoxLowerBound.value()) / 2))
# ---
from typing import List, Tuple


def sum_product(numbers: List[int]) -> Tuple[int, int]:
    """ For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    >>> sum_product([])
    (0, 1)
    >>> sum_product([1, 2, 3, 4])
    (10, 24)
    """
    sum_value = 0
    prod_value = 1

    for n in numbers:
        sum_value += n
        prod_value *= n
    return sum_value, prod_value
# ---
def Log (self,
             Message,
             Delay_In_Seconds = 0.0):
        if self.Next_Step is None:
            self.Next_Step = 1
        if self.Start_Time is None:
            self.Start_Time = clock ()

        logging.info ("     " + str (self.Next_Step - 1) + ": " + Message)
        sleep (Delay_In_Seconds)
# ---
def init(cls, Vocab: Axis, config: ToyLmConfig, *, key: PRNGKeyArray) -> "ToyLmHeadModel":
        k_embed, k_head = jax.random.split(key, 2)
        embed_weight = hax.random.normal(k_embed, (Vocab, config.Embed), dtype=jnp.float32)
        lm_head = hax.random.normal(k_head, (config.Embed, Vocab), dtype=jnp.float32)
        return cls(config, Vocab, embed_weight, lm_head, jnp.array(0.0, dtype=jnp.float32))
# ---
def __define_tables(self):
        """
            Define the database tables for jobs and items
        """

        self.job_table = self.define_job_table()
        self.item_table = self.define_item_table()
# ---
def test_device_flops_for_invalid():
    assert device_flops_for_jax_device("Unknown Device XYZ") is None
    assert device_flops("invalid", "bf16") is None
    assert device_flops("v4", "invalid_dtype") is None
# ---
def init(Vocab: Axis, config: HackableTransformerConfig, *, key):
        emb = hnn.Embedding.init(Vocab, config.Embed, key=key)
        ln = config.mk_LayerNorm(config.Embed) if config.input_embedding_norm else None
        return HackableEmbedding(emb, ln)
# ---
def test_fn(model_weights):
    return evaluate_fn(model_weights, [cifar_test])
# ---
def to_jax_shape(shape: AxisSpec):
    if isinstance(shape, Axis):
        return shape.size
    elif isinstance(shape, Sequence):
        return tuple(s.size for s in shape)
    return tuple(shape[a] for a in shape)
# ---
def vector_of(max_vec_size: int, min_vec_size=1):
    """A hypothesis helper: generates vector array shapes."""
    return st.lists(
        st.integers(min_value=min_vec_size, max_value=max_vec_size),
        min_size=1,
        max_size=1,
    ).map(tuple)
# ---
def act(self, state):

        return self.calculate(state)
# ---
def save(self):
        pass
# ---
def compute_rloo_advantages(rollouts: list[Rollout]) -> np.ndarray:
    """Compute RLOO (Reward Leave-One-Out) advantages for a group of rollouts."""
    rewards = np.array([r.episode_reward for r in rollouts])

    n = len(rewards)
    if n <= 1:
        return np.zeros_like(rewards)

    total = rewards.sum()
    leave_one_out_baselines = (total - rewards) / (n - 1)
    advantages = rewards - leave_one_out_baselines
    return advantages
# ---
def set_broadcast_tx(self, broadcast_tx):
        with self.lock:
            self.tx_pool.set_broadcast_tx(broadcast_tx)
# ---
def test_quantile_parallel(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def forward(self, fts_input: Input, fts: Prognostic) -> Prognostic:
        """Applies all corrections sequentially to the input features.

        Args:
            fts_input: Input tensor
            fts: Output tensor to correct

        Returns:
            Corrected output tensor after applying all corrections
        """
        for corrector in self.correctors:
            fts = corrector(fts_input, fts)
        return fts
# ---
def elu(a: A) -> A:
    return wrap_elemwise_unary(jnn.elu, a)
# ---
def isexact(self):
        return True
# ---
def v5p16_scale_group() -> config_pb2.ScaleGroupConfig:
    """Multi-host TPU scale group (v5p-16, 2 VMs per slice)."""
    return config_pb2.ScaleGroupConfig(
        name="tpu-v5p-16",
        min_slices=0,
        max_slices=5,
        accelerator_type=config_pb2.ACCELERATOR_TYPE_TPU,
        accelerator_variant="v5p-16",
        runtime_version="v2-alpha-tpuv5",
        zones=["us-central1-a"],
    )
# ---
def test_append_single_rank(cache_metadata):
    with tempfile.TemporaryDirectory() as tmpdir:
        builder = JaggedArrayStore.open(tmpdir, item_rank=1, dtype=jnp.float32, cache_metadata=cache_metadata)

        data = jnp.array([1.0, 2.0, 3.0])
        builder.append(data)

        assert len(builder) == 1

        result = builder[0]
        assert jnp.all(result == data)
# ---
def __init__(self, root, cwd, badfn=None, rules=[]):
        super(treematcher, self).__init__(root, cwd, badfn)
        rules = list(rules)
        self._matcher = pathmatcher.treematcher(rules)
        self._rules = rules
# ---
def cluster(ctx):
    """Cluster management commands."""
    parent_obj = ctx.obj or {}
    ctx.ensure_object(dict)
    ctx.obj.update(parent_obj)
# ---


def change_base(x: int, base: int):
    """Change numerical base of input number x to base.
    return string representation after the conversion.
    base numbers are less than 10.
    >>> change_base(8, 3)
    '22'
    >>> change_base(8, 2)
    '1000'
    >>> change_base(7, 2)
    '111'
    """
    ret = ""
    while x > 0:
        ret = str(x % base) + ret
        x //= base
    return ret
# ---
def test_fold():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    acc = hax.zeros((Height, Width))

    total = hax.fold(lambda x, y: x + y, Depth)(acc, named1)

    assert jnp.all(jnp.isclose(total.rearrange(acc.axes).array, jnp.sum(named1.array, axis=2)))
# ---
def odd_bit_set_number(n):
    count = 0;res = 0;temp = n
    while temp > 0:
        if count % 2 == 0:
            res |= (1 << count)
        count += 1
        temp >>= 1
    return (n | res)
# ---
def whatis(self, value):
        post = self.reverse_lookup[value]
        return post
# ---
def num_queued_tokens(self) -> jax.Array:
        """Expose current queued token count from ``TokenQueue``."""
        return self.tqueue.num_queued_tokens
# ---
def test_multi_buffer(self):
        grid = Grid((3, 3))
        f = TimeFunction(name="f", grid=grid)
        g = TimeFunction(name="g", grid=grid, save=Buffer(7))

        op = Operator([Eq(f.forward, 1), Eq(g, f.forward)])
        op(time_M=3)
        # f looped all time_order buffer and is 1 everywhere
        assert np.allclose(f.data, 1)
        # g looped indices 0 to 3, rest is still 0
        assert np.allclose(g.data[0:4], 1)
        assert np.allclose(g.data[4:], 0)
# ---
def Pos(self) -> Axis:
        return Axis("position", self.grug_config.max_seq_len)
# ---
def list_instances(self, project: str, zone: str) -> list[dict]: ...
# ---
def on_trash(self):
		super(Item, self).on_trash()
		frappe.db.sql("""delete from tabBin where item_code=%s""", self.name)
		frappe.db.sql("delete from `tabItem Price` where item_code=%s", self.name)
		for variant_of in frappe.get_all("Item", filters={"variant_of": self.name}):
			frappe.delete_doc("Item", variant_of.name)
# ---
def __init__(self, s: str):
        datetime = cftime.datetime.strptime(s, "%Y-%m-%d", calendar="julian")
        datetime = datetime.replace(hour=12)
        self.datetime = datetime
# ---
def get_image_metadata_item(self, image_id, key):
        """Returns the value for a specific image metadata key."""
        resp, body = self.get("images/%s/metadata/%s" % (str(image_id), key))
        body = json.loads(body)
        self.validate_response(schema.image_meta_item, resp, body)
        return service_client.ResponseBody(resp, body['meta'])
# ---
def test_edit_experiment_view_wrong_course(self):
        """ Tests edit_experiment when attempting to access a experiment from a different course """
        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(experiment.id,)))
        self.assertError(response, UNAUTHORIZED_ACCESS)
# ---
def init_validation_tqdm(self) -> None:
        """Initialize the validation progress bar."""
        bar = super().init_validation_tqdm()
        return self._update_bar_description(bar)
# ---
def count_num_lines(path):
    with open(path) as path_inf:
        return sum(1 for line in path_inf)
# ---
def as_async_dataset(self) -> "AsyncDataset[T_co]":
        return AsyncifiedDataset(self)
# ---
def jnp_to_python(a: jnp.ndarray):
    if isinstance(a, (float, int)):
        return float(a)
    elif a.shape == () or a.shape == (1,):
        return a.item()
    else:
        return a.tolist()
# ---
def is_Monotonic(A): 
    return (all(A[i] <= A[i + 1] for i in range(len(A) - 1)) or
            all(A[i] >= A[i + 1] for i in range(len(A) - 1)))
# ---
def testWhile(self):
    self.assertEqual((0, '2\n1\n'), _GrumpRun(textwrap.dedent("""\
        i = 2
        while i:
          print i
          i -= 1""")))
# ---
def describe_array(arr):
    if isinstance(arr, NamedArray):
        return f"NamedArray(axes={arr.axes}, dtype={arr.dtype})"
    else:
        return f"ndarray(shape={arr.shape}, dtype={arr.dtype})"
# ---
def get_children(self):
        return self.children
# ---
def _in_block(block_name: str, node: RenderTreeNode) -> bool:
    while node.parent:
        if node.parent.type == block_name:
            return True
        node = node.parent
    return False
# ---
def ready_count(self) -> int:
        """Number of actors that are available for RPC."""
        ...
# ---
def iter_buildable_deps(name):
            """
            instanciates a builder for each image dependency
            does nothing when the image cannot be build
            """
            for dep_name, _ in self.getImage(name).imageDeps():
                try:
                    self.getImage(dep_name)
                    yield dep_name
                except KeyError:
                    continue
# ---
def func(x):
        return nxp.sum(x, axis=2)
# ---
def test_equal_5(self):
        self.assertEqual(string_color('Mathew Smith'), '8B00F1')
# ---
def get_es(url):
    o = urlparse(url)
    es = elasticsearch.Elasticsearch(hosts=[{'host': o.hostname, 'port': o.port}])
    es.transport.serializer = ElasticJSONSerializer()
    return es
# ---
def template_output(output):
    # output, output_cmd
    output = os.path.abspath(output)
    return "output\t{}".format(output), ["--output", output]
# ---
def test_host_maintenance_off(self):
        self._test_host_action(self.conn.host_maintenance_mode,
                               False, 'off_maintenance')
# ---
def KHeads(self) -> Axis:
        return Axis("k_heads", self.num_k_heads)
# ---
def num_cpus(self) -> int:
        if self.override_resources is not None:
            cpus = self.override_resources.get("num_cpus", None)
            if cpus is not None:
                return cpus
        return num_cpus_used_by_tokenizer(self.tokenizer)
# ---
def write(self, data: bytes) -> int:
            self.write_calls.append(len(data))
            return super().write(data)
# ---
def triggerCharacters(self):
        """Trigger characters for smart indentation"""
        return self._smartIndenter.TRIGGER_CHARACTERS
# ---
def __init__(self):
                self.finish_revert_migration_called = False
# ---
def retranslateUi(self, Dialog):
        Dialog.setWindowTitle(_translate("Dialog", "Samples Manager", None))
# ---
def counting_map(self, x):
        self.map_count += 1
        self.processed_ids.append(x["id"])
        return {**x, "processed": True}
# ---
def test_edit_experiment_view(self):
        """ Tests edit_experiment template renders when authenticated """
        experiment = self.create_test_experiment()
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(experiment.id,)))
        self.assertTemplateUsed(response, "ab_tool/edit_experiment.html")
# ---
def forward(self, indices, sigma):
    x = self.vocab_embed(indices)
    c = F.silu(self.sigma_map(sigma))

    rotary_cos_sin = self.rotary_emb(x)

    with torch.cuda.amp.autocast(dtype=torch.bfloat16):
      for i in range(len(self.blocks)):
        x = self.blocks[i](x, rotary_cos_sin, c, seqlens=None)
      x = self.output_layer(x, c)

    return x
# ---
def max_slices(self) -> int:
        """Maximum number of VM groups allowed."""
        return self._config.max_slices
# ---
def get_slice(self, slice_id: str) -> FakeVmGroup | None:
        """Get a specific VM group by ID."""
        with self._lock:
            return self._slices.get(slice_id)
# ---
def test_profiler_get_base_id_unset_case(self, mock_generate_uuid):
        mock_generate_uuid.return_value = "42"
        prof = profiler._Profiler("secret")
        self.assertEqual(prof.get_base_id(), "42")
        self.assertEqual(prof.get_parent_id(), "42")
# ---
def with_output(self, x, y, z, *, static1, static2):
            assert static1 is True
            assert static2 is False
            out = x + self.w + y + z
            return out, 2 * self.w + y
# ---
def count_pairs(arr, n, k):
  count=0;
  for i in range(0,n):
    for j in range(i+1, n):
      if arr[i] - arr[j] == k or arr[j] - arr[i] == k:
        count += 1
  return count
# ---
def generate_data(self, data_dir, tmp_dir, task_id=-1):
    generator_utils.generate_dataset_and_shuffle(
        self.generator(data_dir, tmp_dir, True),
        self.training_filepaths(data_dir, self.train_shards, shuffled=False),
        self.generator(data_dir, tmp_dir, False),
        self.dev_filepaths(data_dir, self.dev_shards, shuffled=False))
# ---
def __init__(self, key):
            self.weight = jnp.ones((2, 2))
            self.bias = jnp.ones(2)
# ---
def area_weighted_mean_bias(
    target: torch.Tensor, gen: torch.Tensor, area_weights: torch.Tensor
) -> torch.Tensor:
    area_weights = area_weights.to(target.device)
    return area_weighted_mean(gen - target, area_weights)
# ---
def copy_read_to_write(chunk_key: Tuple[slice], *, config: CubedCopySpec) -> None:
    # workaround limitation of lithops.utils.verify_args
    if isinstance(chunk_key, list):
        chunk_key = tuple(chunk_key)
    data = np.asarray(config.read.open()[chunk_key])
    config.write.open()[chunk_key] = data
# ---
def test_without_private(self, mock_start, mock_stop):
        fake_cls = FakeTraceWithMetaclassHideArgs()
        self.assertEqual(10, fake_cls._method(10))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def t_error(self, t):
        raise SyntaxError(
            "Illegal character: '%s' at Line %d" % (t.value[0], t.lineno)
        )
# ---
def __init__(self, host: str, port: int, queue_name: str):
        self.host = host
        self.port = port
        self.queue_name = queue_name
# ---
def test_tokens_are_equal(self):
        # It should fail if the tokens aren't equal length.
        self.assertFalse(csrf._tokens_are_equal('a', 'ab'))
        # It should fail if the tokens are different.
        self.assertFalse(csrf._tokens_are_equal('abcde', 'abcdf'))
        # It should succeed if the tokens are the same.
        self.assertTrue(csrf._tokens_are_equal('abcde', 'abcde'))
# ---
def vocab_size(self) -> int:
        """Vocabulary size derived from the tokenizer."""
        return get_vocab_size_for_tokenizer(self.tokenizer)
# ---
def _get_fs_and_plain_path(path, fs=None):
    if fs is None:
        fs, _, (path_to_open,) = fsspec.get_fs_token_paths(str(path))
    else:
        path_to_open = path
    return fs, path_to_open
# ---
def test_retry_budget_exact(cluster):
    """Task fails exactly N-1 times, succeeds on last attempt."""
    _url, client = cluster
    enable_chaos("worker.create_container", failure_rate=1.0, max_failures=2, error=RuntimeError("chaos: transient"))
    job = submit(client, _quick, "exact-retry", max_retries_failure=2)
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def test_pad():
    Height = Axis("Height", 3)
    Width = Axis("Width", 2)

    arr = hax.arange((Height, Width))
    padded = hax.pad(arr, {Height: (1, 2), Width: (0, 1)}, mode="constant", constant_values=0)

    expected = jnp.pad(arr.array, [(1, 2), (0, 1)], mode="constant", constant_values=0)
    assert padded.axes[0].size == Height.size + 3
    assert padded.axes[1].size == Width.size + 1
    assert jnp.all(expected == padded.array)
# ---
def remove(self, thread: ManagedThread) -> None:
        """Remove a thread from this container.

        Called automatically when threads complete. Thread-safe.
        """
        with self._lock:
            try:
                self._threads.remove(thread)
            except ValueError:
                # Already removed, that's fine
                pass
# ---
def saveWindowState(self):
        """
        Saves the main window state (position, size, toolbar positions)
        """
        mwState = self.saveState().toBase64() 
        mwGeom  = self.saveGeometry().toBase64() 
        cbpos.config['mainwindow', 'state'] = unicode(mwState)
        cbpos.config['mainwindow', 'geometry'] = unicode(mwGeom)
        cbpos.config.save()
# ---
def clause(self):
        return '1', ()
# ---
def executor(request):
    if request.param.name == "processes" and "cupy" in nxp.__name__:
        pytest.skip(reason="CuPy is not supported with 'processes' executor")
    return request.param
# ---
def init(In, Out, key):
            up_proj = hax.nn.Linear.init(In, Out, key=key)
            down_proj = hax.nn.Linear.init(Out, In, key=key)
            return Block(up_proj, down_proj)
# ---
def test_capture_conftest_runtest_setup(testdir):
    testdir.makeconftest("""
        def pytest_runtest_setup():
            print ("hello19")
    """)
    testdir.makepyfile("def test_func(): pass")
    result = testdir.runpytest()
    assert result.ret == 0
    assert 'hello19' not in result.stdout.str()
# ---
def is_abundant(n):
    fctrsum = sum([fctr for fctr in range(1, n) if n % fctr == 0])
    return fctrsum > n
# ---
def contribute_to_class(self, cls, name):
		super(JSONField, self).contribute_to_class(cls, name)
		setattr(cls, name, JSONDescriptor(self))
		models.signals.pre_init.connect(self.fix_init_kwarg, sender=cls)
# ---
def __or__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer or boolean", "__or__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.bitwise_or, self, other, dtype=result_type(self, other))
# ---
def _forward(x: jax.Array, labels: jax.Array, w: jax.Array):
        return linear_softmax_cross_entropy_loss_fwd_pallas_mosaic_tpu(
            x,
            labels,
            w,
            block_sizes=block_sizes,
            dtype=dtype,
            logit_soft_cap=logit_soft_cap,
            precision=precision,
        )
# ---
def test_make_blockwise_key_function_contract_0d():
    func = lambda x: 0

    key_fn = make_blockwise_key_function(
        func, "z", "", "x", "ij", numblocks={"x": (1, 1)}
    )

    graph = make_blockwise_graph(func, "z", "", "x", "ij", numblocks={"x": (1, 1)})
    check_consistent_with_graph(key_fn, graph)
# ---
def backwards(self, orm):
        # Removing unique constraint on 'Package', fields ['name', 'url']
        db.delete_unique(u'api_package', ['name', 'url'])

        # Deleting model 'Package'
        db.delete_table(u'api_package')
# ---
def unshard(x: jax.Array) -> jax.Array:
    return reshard(x, P((None,)))
# ---
def load_jsonl(self) -> Dataset[dict]:
        """Load records from JSONL files."""
        return Dataset(self.source, [*self.operations, LoadFileOp("jsonl", None)])
# ---
def _start_heartbeat(self) -> None:
        """Start background thread that periodically refreshes the lease."""

        def heartbeat_loop():
            while not self._stop_event.wait(HEARTBEAT_INTERVAL):
                self._status_file.refresh_lock()

        self._heartbeat_thread = Thread(target=heartbeat_loop, daemon=True)
        self._heartbeat_thread.start()
# ---
def _soft_sample(self, logits):
    return 0
# ---
def params(tiny_cfg):
    key = jax.random.PRNGKey(0)
    return init_edit_params(tiny_cfg, key=key)
# ---
def reset(self) -> Self:
        """Return a reset version of this cache."""
        raise NotImplementedError
# ---
def transform_dclm_hq(html: str) -> str:
    """Transform DCLM HQ HTML to markdown."""
    output = convert_page(
        html,
        extract_method="resiliparse",
        config=ResiliparseConfig(
            links=False,
            skip_elements=[],
            markdownify_config=HtmlToMarkdownConfig(
                include_images=False,
                include_links=False,
            ),
        ),
    )
    return output["content"]
# ---
def __getattr__(self, attr):
		if attr in self.structure:
			return self._get_value(attr)

		if attr in self.structure._abstractions: # Union abstractions etc
			field, func = self.structure._abstractions[attr]
			return func(field, self)

		if "__" in attr:
			return self._query(attr)

		return super(DBRow, self).__getattribute__(attr)
# ---
def is_alive(self) -> bool:
        """True if any thread in this container or its children is still running."""
        with self._lock:
            threads = list(self._threads)
            children = list(self._children)
        return any(t.is_alive for t in threads) or any(c.is_alive for c in children)
# ---
def _initialize_jax_config(self):
        for key, value in self.jax_config.items():
            jax.config.update(key, value)

        if self.jax_compilation_cache_dir is not None:
            jax.config.update("jax_compilation_cache_dir", self.jax_compilation_cache_dir)
# ---
def _get_value(self, original, split):
        if original:
            value = round(original * split) / split
            return Decimal(str(value))
# ---
def format_fans(fans):
    return format_line(prefix='fans'.rjust(RJUST), values=fans)
# ---
def compute_advantages(self, rollout_group: list[Rollout]) -> list[float]:
        """Compute advantages for a group of rollouts."""
        return compute_rloo_advantages(rollout_group)
# ---
def get_prep_value(self, value):
		return ','.join(value)
# ---
def __init__(self, epoch_ms: int):
        self._epoch_ms = epoch_ms
# ---
def save_yaml(self, save_path: Path) -> None:
        """Save config to YAML file."""
        with open(save_path, "w") as f:
            yaml.dump(self.model_dump(), f)
# ---
def __unicode__(self):
        return 'ManualIDVerification for {name}, status: {status}'.format(
            name=self.name,
            status=self.status,
        )
# ---
def testTryBareExcept(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        try:
          raise AssertionError
        except:
          pass""")))
# ---
def test_corrupt_program_zero_steps(bank):
    source = CORPUS[0]
    corrupted, mutations = corrupt_program(source, num_steps=0, bank=bank)
    assert corrupted == source
    assert mutations == []
# ---
def testGradientInput1(self):
    with self.test_session(use_gpu=False):
      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2],
                   dtype=tf.float64, name="x")
      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],
                   shape=[2, 4], dtype=tf.float64, name="y")
      m = tf.matmul(x, y, name="matmul")
      err = gc.ComputeGradientError(y, [2, 4], m, [3, 4])
    print("matmul input1 gradient err = ", err)
    self.assertLess(err, 1e-10)
# ---
def setUp(self):
        super(XenAPIDetermineDiskImageTestCase, self).setUp()
        glance_stubs.stubout_glance_client(self.stubs)

        class FakeInstance(object):
            pass

        self.fake_instance = FakeInstance()
        self.fake_instance.id = 42
        self.fake_instance.os_type = 'linux'
        self.fake_instance.architecture = 'x86-64'
# ---
def test_qr(tmp_path, spec, executor):
    a = cubed.random.random(
        (40000, 1000), chunks=(5000, 1000), spec=spec
    )  # 40MB chunks
    q, r = xp.linalg.qr(a)
    # don't optimize graph so we use as much memory as possible (reading from Zarr)
    run_operation(tmp_path, executor, "qr", q, r, optimize_graph=False)
# ---
def test_str_contains_regex(self):
        def test_impl():
            A = StringArray(['ABC', 'BB', 'ADEF'])
            df = pd.DataFrame({'A': A})
            B = df.A.str.contains('AB*', regex=True)
            return B.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), 2)
# ---
def fn(config: MyConfig):
        append_log(log, config)
        time.sleep(run_time)
# ---
def floor_Min(A,B,N):
    x = max(B - 1,N)
    return (A*x) // B
# ---
def check_for_active_boms(self):
		if self.default_bom:
			bom_item = frappe.db.get_value("BOM", self.default_bom, "item")
			if bom_item not in (self.name, self.variant_of):
				frappe.throw(
					_("Default BOM ({0}) must be active for this item or its template").format(bom_item))
# ---
def is_leaf(x):
        return isinstance(x, list)
# ---
def test_expand_dims(spec, executor):
    a = xp.asarray([1, 2, 3], chunks=(2,), spec=spec)
    b = xp.expand_dims(a, axis=0)
    assert_array_equal(b.compute(executor=executor), np.expand_dims([1, 2, 3], 0))
# ---
def test_permutation_is_bijective_over_full_range(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    indices = jnp.arange(length)
    permuted = permutation(indices)
    # Check if all elements are unique, which is a necessary condition for a bijective function
    assert len(jnp.unique(permuted)) == length
# ---
def interaction_alts():
    return pd.DataFrame({
        'prop': [10, 20, 30, 40]},
        index=[1, 2, 3, 4])
# ---
def slice_dataset(self, start_index: Optional[int] = None, end_index: Optional[int] = None):
        """
        Slices the dataset from `start_index` to `end_index`.
        """
        return SlicedAsyncDataset(self, start_index, end_index)
# ---
def test_large_number_of_tasks(tmp_path, timing_map, n_tasks, retries, use_backups):
    path = f"{BASE_PATH}/{tmp_path.name}"
    outputs = asyncio.run(
        run_test(
            app_function=deterministic_failure_modal,
            input=range(n_tasks),
            use_backups=use_backups,
            path=path,
            timing_map=timing_map
        )
    )

    assert outputs == set(range(n_tasks))
    check_invocation_counts(path, timing_map, n_tasks, retries)
# ---
def axis_indices(self, axis: Sequence[AxisSelector]) -> tuple[int | None, ...]: ...
# ---
def get_id_from_row(row: dict, id_path: tuple[str, ...]) -> str | None:
    """Traverse a tuple path in a row to extract the ID, or return None if missing."""
    obj = row
    for key in id_path:
        obj = obj.get(key)
        if obj is None:
            raise ValueError(f"ID path {id_path} not found in row: {row}")
    return obj
# ---
def with_effect(self, synthdef, release_time=0.25, **settings):
        import supriya.patterns

        return supriya.patterns.Pfx(
            self, synthdef=synthdef, release_time=release_time, **settings
        )
# ---
def client():
    """Create a local cluster client for testing."""
    client = LocalClusterClient.create()
    yield client
    client.shutdown()
# ---
def status(self) -> cluster_pb2.TaskStatus:
        """Get current task status.

        Returns:
            TaskStatus proto containing state, worker assignment, and metrics
        """
        return self._client._cluster_client.get_task_status(self.task_id)
# ---
def test_dupfile_on_bytesio():
    io = py.io.BytesIO()
    f = capture.safe_text_dupfile(io, "wb")
    f.write("hello")
    assert io.getvalue() == b"hello"
    assert 'BytesIO object' in f.name
# ---
def exact(root, cwd, files, badfn=None):
    return exactmatcher(root, cwd, files, badfn=badfn)
# ---
def as_sync_dataset(self):
        return SyncifiedDataset(self)
# ---
def main():
    print("Hello from oa-ucsf-brain!")
# ---
def regular_update(updates, nu_hat):
            # For subsequent steps, compute updates normally
            return jax.tree_util.tree_map(
                lambda m, v: None if m is None else m / (jnp.sqrt(v + eps_root) + eps),
                updates,
                nu_hat,
                is_leaf=lambda x: x is None,
            )
# ---
def run_server():
        uvicorn_server.run()
# ---
def _parse_progress(self, chunk):
        m = self.DISK_PROGRESS_RE.match(chunk)
        if m is None:
            return None
        try:
            return int(m.group(1))
        except ValueError:
            raise OutputParserError('error parsing progress regex: %r'
                                    % m.groups)
# ---
def reset_daily_weights(self):
        for day in ["M", "T", "W", "R", "F", "S"]:
            self.daily_weights[day] = 0
            self.daily_totals[day]  = 0
# ---
def __int__(self):
		return self.id
# ---
def evt():
    return event.build_event("event")
# ---
def load_store(cls, store, decoder=None):
        """Create a new dataset from the contents of a backends.*DataStore
        object
        """
        variables, attributes = store.load()
        if decoder:
            variables, attributes = decoder(variables, attributes)
        obj = cls(variables, attrs=attributes)
        obj._file_obj = store
        return obj
# ---
def escape_link(url):
    """Remove dangerous URL schemes like javascript: and escape afterwards."""
    lower_url = url.lower().strip('\x00\x1a \n\r\t')
    for scheme in _scheme_blacklist:
        if lower_url.startswith(scheme):
            return ''
    return escape(url, quote=True, smart_amp=False)
# ---
def index():
    """Landing page for SciNet"""
    return render_template("index.html")
# ---
def log_summary(metrics: dict[str, Any]):
    """
     Log summary metrics to the global tracker.

    Args:
         metrics: Metrics to log
    """
    global _global_tracker
    if _global_tracker is None:
        warnings.warn("No global tracker set")
        return

    _global_tracker.log_summary(metrics)
# ---
def descobreAtaqueUsado(self, atksXML, pkmn):
		for i in range(0, len(atksXML)):
			id = int(atksXML[i].find('id').text) - 1 
			ppXML = int(atksXML[i].find('power_points').text)
			pp = pkmn.getAtks(id).getPpAtual()

			if (pp != ppXML):
				pkmn.getAtks(id).decreasePp()
				return id

		return id
# ---
def logistic(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.logistic(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def visitdir(self, dir):
        """Decides whether a directory should be visited based on whether it
        has potential matches in it or one of its subdirectories. This is
        based on the match's primary, included, and excluded patterns.

        Returns the string 'all' if the given directory and all subdirectories
        should be visited. Otherwise returns True or False indicating whether
        the given directory should be visited.
        """
        return True
# ---
def _normalize_split_every(split_every, axis):
    split_every = split_every or 4
    if isinstance(split_every, dict):
        split_every = {k: split_every.get(k, 2) for k in axis}
    elif isinstance(split_every, Integral):
        n = builtins.max(int(split_every ** (1 / (len(axis) or 1))), 2)
        split_every = dict.fromkeys(axis, n)
    else:
        raise ValueError("split_every must be a int or a dict")
    return split_every
# ---
def ray_auth_secret(secret_override: str | None = None) -> str:
    """Return the Secret Manager secret name to use for Ray auth token retrieval."""
    return secret_override or DEFAULT_RAY_AUTH_TOKEN_SECRET
# ---
def sign(x):
    return 1 if x > 0 else -1
# ---
def bias(self):
        return self.gen - self.target
# ---
def get_message(self):
        return self.emailMessage
# ---
def init(self, **kwargs):
        """Initialize wandb run."""
        if self._enabled and not self._initialized:
            try:
                self.run = wandb.init(**kwargs)
                self._initialized = True
            except Exception as e:
                logger.error(f"Failed to initialize wandb: {e}")
                self._enabled = False
# ---
def on_upper_bound_changed(self):
        self.ui.spinBoxLowerBound.setMaximum(self.ui.spinBoxUpperBound.value() - 1)
        self.ui.spinBoxBoundaryNumber.setMaximum(math.ceil((self.ui.spinBoxUpperBound.value()
                                                            - self.ui.spinBoxLowerBound.value()) / 2))
# ---
import math
def perimeter_pentagon(a):
  perimeter=(5*a)
  return perimeter
# ---
def task_index(self) -> int:
        """0-indexed task number within the job."""
        return self.task_id.require_task()[1]
# ---
def _environment(self):
        env = super(XenCommand, self)._environment()
        env.update(self._ssh_agent.auth)
        return env
# ---
def test_pjit_class_init():
    with axis_mapping(resource_map):
        devices = jax.devices()
        with Mesh(np.array(devices).reshape(-1, 2), (ResourceAxis.DATA, ResourceAxis.MODEL)) as mesh, set_mesh(mesh):
            mod = named_jit(MyModuleInit)()

        assert mod.named.array.shape == (Dim2.size, Dim3.size)

        assert mod.unnamed1.shape == ()
        assert mod.named2.array.shape == (Dim3.size,)
# ---
def test_filter_expression_logical_and(backend):
    """Test filter with logical AND expression."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"a": 1, "b": 2},
            {"a": -1, "b": 3},
            {"a": 2, "b": -1},
            {"a": -1, "b": -1},
        ]
    ).filter((col("a") > 0) & (col("b") > 0))

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == {"a": 1, "b": 2}
# ---
def test_listdir (self):
    import os
    fs_version = list (fs.listdir (utils.TEST_ROOT))
    os_version = os.listdir (utils.TEST_ROOT)
    self.assertEquals (fs_version, os_version, "%s differs from %s" % (fs_version, os_version))
# ---
def divmod(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.divmod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.divmod.html)
    """
    return jnp.divmod(x1, x2)
# ---
def convert_tokens_to_ids(self, token):
        """Convert token string to token ID."""
        if isinstance(token, list):
            return [self.TOKENS.index(t) for t in token]
        return self.TOKENS.index(token)
# ---
def __init__(self, exemplar):
        self.exemplar = exemplar
# ---
def set_location(self, x, y):
        """Set the position of the window.

        :Parameters:
            `x` : int
                Distance of the left edge of the window from the left edge
                of the virtual desktop, in pixels.
            `y` : int
                Distance of the top edge of the window from the top edge of
                the virtual desktop, in pixels.

        """
        raise NotImplementedError('abstract')
# ---
def area_weighted_sum(
    data: torch.Tensor,
    area_weights: torch.Tensor,
    dim: tuple[int, ...] = (-2, -1),
    keepdim: bool = False,
) -> torch.Tensor:
    return weighted_sum(data, area_weights, dim=dim, keepdim=keepdim)
# ---
def test_count_repr(self):
        """Count repr"""
        count = Count(0, 0)
        self.assertEqual(repr(count), "Count(0)")
# ---
def platform_type(s):
    if s not in _VALID_PLATFORMS:
      raise argparse.ArgumentTypeError(f'Invalid Platform specified: "{s}".')
    return s
# ---
def __init__(self):
        columns = ['mean_height', 'min_height', 'max_height', 'mean_width', 'min_width', 'max_width', 'time', 'girth','id']
        self.data = DataFrame(columns=columns)
        self.event = []
# ---
def sink_attention_vanilla(
    query,
    key,
    value,
    sinks,
    sm_scale: float = 0.125,
    sliding_window: int | None = None,
    start_q=0,
):
    return sink_attention(
        query,
        key,
        value,
        sinks,
        sm_scale,
        sliding_window,
        start_q,
        attn_backend=None,
        block_size=None,
        inference=True,
    )
# ---
def actual_head_size(self):
        """Returns the actual head size based on the head_dim or calculated from hidden_dim and num_heads."""
        if self.head_dim is not None:
            return self.head_dim
        return self.hidden_dim // self.num_heads
# ---
def nansum(
    array: NamedArray,
    axis: AxisSelection | None = None,
    *,
    where: NamedArray | None = None,
    dtype: DTypeLike | None = None,
) -> NamedArray:
    return wrap_reduction_call(
        jnp.nansum, array, axis, where, single_axis_only=False, supports_where=True, dtype=dtype
    )
# ---
def createLink( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def __add__(self, other):
        lst = []
        for k in self.lookup:
            lst.append((k, self.lookup[k]))
        for k in other.lookup:
            lst.append((k, other.lookup[k]))
        return Enumeration(lst)
# ---
def test_stack(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    c = xp.stack((a, b), axis=0)
    run_operation(tmp_path, executor, "stack", c)
# ---
def Out(self) -> AxisSpec:
        return self.layers[-1].Out
# ---
def updated_slice(
        self, start: Mapping[AxisSelector, int | "NamedArray"], update: "NamedArray"
    ) -> "NamedArray":  # pragma: no cover
        return haliax.updated_slice(self, start=start, update=update)
# ---
def test_with_trace_etype(self, mock_start, mock_stop):

        def foo():
            with profiler.Trace("foo"):
                raise ValueError("bar")

        self.assertRaises(ValueError, foo)
        mock_start.assert_called_once_with("foo", info=None)
        mock_stop.assert_called_once_with(info={
            "etype": "ValueError",
            "message": "bar"
        })
# ---
def key_str(self, match):
        '''
        Return the specified public key or keys based on a glob
        '''
        ret = {}
        for status, keys in six.iteritems(self.name_match(match)):
            ret[status] = {}
            for key in salt.utils.isorted(keys):
                path = os.path.join(self.opts['pki_dir'], status, key)
                with salt.utils.fopen(path, 'r') as fp_:
                    ret[status][key] = fp_.read()
        return ret
# ---
def close_pane(self, index):
        was_selected = index == self.selected_pane_index
        del self.panes[index]
        if not self.panes:
            self.view.close()
            return
        self._selected_pane_index = min(self._selected_pane_index, len(self.panes) - 1)
        if was_selected:
            self._update_selected_pane()
# ---
def test_context_put_get(job_context):
    obj = {"key": "value"}
    ref = job_context.put(obj)
    assert job_context.get(ref) == obj
# ---
def _copy(_):
        last_idx = (src_len + page_size - 1) // page_size - 1
        src_page = decode_state.sequences.page_indices["seq", parent_local_id, "page", last_idx].scalar()
        dst_page = decode_state.sequences.page_indices["seq", child_local_id, "page", last_idx].scalar()
        return state.cache.copy_page(src_page, dst_page)
# ---
def on_validation_epoch_start(self):
    if self.ema:
      self.ema.store(itertools.chain(
        self.backbone.parameters(),
        self.noise.parameters()))
      self.ema.copy_to(itertools.chain(
        self.backbone.parameters(),
        self.noise.parameters()))
    self.backbone.eval()
    self.noise.eval()
    assert self.valid_metrics.nll.mean_value == 0
    assert self.valid_metrics.nll.weight == 0
# ---
def convert_to_export(self, value, env):
        if env.context.get('export_raw_data'):
            return value
        return ustr(value)
# ---
def _write(filename, contents):
        ''' Actually write the file contents to disk. This helps with mocking. '''

        with open(filename, 'w') as sfd:
            sfd.write(contents)
# ---
def to_dataframe(self, message, dtypes=None):
        record_batch = self._parse_arrow_message(message)

        if dtypes is None:
            dtypes = {}

        df = record_batch.to_pandas()

        for column in dtypes:
            df[column] = pandas.Series(df[column], dtype=dtypes[column])

        return df
# ---
def test_horizontal_regrid():
    pass
# ---
def create(self, model: M) -> EmaModelAveraging[M]:
        return EmaModelAveraging(model=model, beta=self.beta)
# ---
def set_cookie(
        self,
        domain,
        name,
        value,
        path="/",
        exp=time.time() + timedelta(hours=744).total_seconds(),  #: 31 days retention
    ):
        self.cookies[
            name
        ] = f".{domain}\tTRUE\t{path}\tFALSE\t{exp}\t{name}\t{value}"
# ---
def isRequired(self, key):
        """ @rtype: bool """
        return SummaryKeyMatcher.cNamespace().is_required(self, key)
# ---
def due_date(self):
        """ Returns a date object of the todo's due date. """
        return self.get_date(config().tag_due())
# ---
def load_vortex(self, columns: list[str] | None = None) -> Dataset[dict]:
        """Load records from Vortex files."""
        return Dataset(self.source, [*self.operations, LoadFileOp("vortex", columns)])
# ---
def add_parent(self, parent):
        self.parent = parent
        self.parent.calls = []
# ---
def _list_vdis(self):
        url = FLAGS.xenapi_connection_url
        username = FLAGS.xenapi_connection_username
        password = FLAGS.xenapi_connection_password
        session = xenapi_conn.XenAPISession(url, username, password)
        return session.call_xenapi('VDI.get_all')
# ---
def test_char_offset_to_token_index(tok):
    source = "hello"
    assert tok.char_offset_to_token_index(source, 0) == 0
    assert tok.char_offset_to_token_index(source, 3) == 3
    assert tok.char_offset_to_token_index(source, 10) == 4
# ---
def matchfn(self, f):
        return self._matcher.matches(f)
# ---
def feature_encoders(self, data_dir):
    if self.is_character_level:
      encoder = text_encoder.ByteTextEncoder()
    else:
      vocab_filename = os.path.join(
          data_dir, self.vocab_problem.vocab_filename)
      encoder = text_encoder.SubwordTextEncoder(vocab_filename)
    input_encoder = text_encoder.ImageEncoder(channels=self.num_channels)
    return {"inputs": input_encoder, "targets": encoder}
# ---
def print(self, *args, **kwargs):
        """Override print method to intercept and modify all progress updates."""
        # Check if this is updating a progress bar description and modify it
        if args:
            # Try to update any active progress bars before printing
            self._update_all_progress_bars()

        return super().print(*args, **kwargs)
# ---
def get_power(self):
        return self._power
# ---
def all_Characters_Same(s) :
    n = len(s)
    for i in range(1,n) :
        if s[i] != s[0] :
            return False
    return True
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.quantile(.25)
# ---
def config_with_special_chars() -> config_pb2.BootstrapConfig:
    """Config with values containing braces and special characters."""
    return config_pb2.BootstrapConfig(
        docker_image="gcr.io/test/iris:v1.0-{tag}",
        cache_dir="/cache/{project}/iris",
        worker_port=10001,
        env_vars={
            "MESSAGE": "Hello {world}!",
            "JSON": '{"key": "value"}',
        },
    )
# ---
def run(self):
        try:
            with fray_default_job_ctx(SyncContext()):
                super().run()
        except BaseException as e:
            self._exception = e
            raise
# ---
def click_remove_button(self):
        """
        :rtype: HomePage
        """
        self._click(BrowseMoviePageLocators.REMOVE_BUTTON_LOCATOR)
        self.alert_accept()
        from .home import HomePage
        return HomePage(self._driver)
# ---
def is_key_present(d,x):
  if x in d:
    return True
  else:
     return False
# ---
def from_proto(cls, proto: "time_pb2.Timestamp") -> "Timestamp":
        """Create from proto Timestamp message."""
        return cls(proto.epoch_ms)
# ---
def nets_dir(self) -> Path:
        return self.output_dir / "saved_nets"
# ---
def __init__(self, gan=None, config=None, trainer=None):
      super().__init__(config=config, gan=gan, trainer=trainer)
      self.d_grads = None
      self.g_grads = None
# ---
def action_confirm(self, cr, uid, ids, context=None):
        """ Confirms stock move.
        @return: List of ids.
        """
        moves = self.browse(cr, uid, ids, context=context)
        self.write(cr, uid, ids, {'state': 'confirmed'})
        self.create_chained_picking(cr, uid, moves, context)
        return []
# ---
def ones_like(x, /, *, dtype=None, device=None, chunks=None, spec=None) -> "Array":
    return ones(**_like_args(x, dtype, device, chunks, spec))
# ---
def delete_local(self):
        '''
        Delete the local private key file
        '''
        path = os.path.join(self.opts['pki_dir'], 'local.key')
        if os.path.isfile(path):
            os.remove(path)
# ---
def __enter__(self):
        """Start batch generation thread."""
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        return self
# ---
def isinf(a: A) -> A:
    return wrap_elemwise_unary(jnp.isinf, a)
# ---
def get_ts_diff(utc_str1: str, utc_str2: str) -> float:
    # Parse the UTC string to a datetime object
    utc_dt1 = datetime.strptime(utc_str1, "%Y-%m-%dT%H:%M:%S")
    utc_dt2 = datetime.strptime(utc_str2, "%Y-%m-%dT%H:%M:%S")
    diff = utc_dt2 - utc_dt1
    diff_seconds = diff.total_seconds()
    diff_hrs = diff_seconds / 3600.0
    return diff_hrs
# ---
def __init__(self, df1 = 1, df2 = 1, lmbda = 0):
        d1 = NoncentralChiSquareDistr(df1, lmbda) / df1
        d2 = ChiSquareDistr(df2) / df2
        super(NoncentralFDistr, self).__init__(d1, d2)
        self.df1 = df1
        self.df2 = df2
        self.lmbda = lmbda
# ---
def testAllSemitones(self):
        # Tests whether a spectral peak output of 12 consecutive semitones
        # yields a HPCP of all 1's
        tonic = 440
        freqs = [(tonic * 2**(x/12.)) for x in range(12)]
        mags = [1] * 12
        hpcp = HPCP()(freqs, mags)
        self.assertEqualVector(hpcp, [1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.])
# ---
def __del__(self):
        if hasattr(self, "_batches") and hasattr(self._batches, "stop"):
            self._batches.stop()
# ---
def _max_params_for_budget(self, budget: float) -> float:
        """Compute max_params as a function of budget.

        Returns base_max_params for budgets <= base_max_params_budget,
        then scales with sqrt(budget) for larger budgets, capped at global_max_params.
        """
        scaling = self.base_max_params * math.sqrt(budget / self.base_max_params_budget)
        return min(max(self.base_max_params, scaling), self.global_max_params)
# ---
def create(zone: str):
    """Create a TPU VM with a random fun name in the specified zone."""
    name = create_tpu_vm(zone)
    click.echo(f"Created TPU VM: {name}")
# ---
def compute_rmsd(atom_coords: torch.Tensor, pred_atom_coords: torch.Tensor):
    rmsd, _ = compute_subset_rmsd(
        atom_coords,
        pred_atom_coords,
        atom_mask=torch.ones_like(atom_coords[..., 0]),
        align_weights=torch.ones_like(atom_coords[..., 0]),
        subset_mask=torch.ones_like(atom_coords[..., 0]),
        multiplicity=1,
    )
    return rmsd
# ---
def log10(a: A) -> A:
    return wrap_elemwise_unary(jnp.log10, a)
# ---
def release(queue_name: str, lease_id: str = Body(...), timestamp: float = Body(...)):
            if queue_name not in self.queues:
                return Response(status_code=404)
            lease = Lease(item=None, lease_id=lease_id, timestamp=timestamp)
            self.queues[queue_name].release(lease)
            return {"status": "ok"}
# ---
def update_num(n):
      hasher.update(compat.as_bytes("%x" % n))
# ---
def all_unique(test_list):
    if len(test_list) > len(set(test_list)):
        return False
    return True
# ---
def testMatMul_Inputs_Empty(self):
    n, k, m = 3, 0, 4
    x = self._randMatrix(n, k, np.float32)
    y = self._randMatrix(k, m, np.float32)
    self._testCpuMatmul(x, y)
    self._testGpuMatmul(x, y)
# ---
def test_load_file_auto_detects_vortex(self, sync_backend, vortex_file, tmp_path):
        """Test that load_file() auto-detects vortex format."""
        output_pattern = str(tmp_path / "output-{shard:05d}.jsonl.gz")

        ds = Dataset.from_files(str(vortex_file)).load_file().filter(lambda r: r["id"] < 10).write_jsonl(output_pattern)

        results = list(Backend.execute(ds, context=sync_backend))
        assert len(results) == 1
# ---
def test_score_candidate_empty_tests():
    candidate = _make_candidate("x = 1\n")
    result = score_candidate(candidate, [])
    assert result.tests_passed == 0
    assert result.tests_total == 0
    assert result.test_pass_rate == 0.0
# ---
def replace_latex_environment(text: str, env_name: str, replacer: Callable[[str], str]) -> str:
    """Replace \\begin{env}...\\end{env} with processed content."""
    pattern = f"\\\\begin{{{env_name}}}(.*?)\\\\end{{{env_name}}}"
    return re.sub(pattern, lambda m: replacer(m.group(1)), text, flags=re.DOTALL)
# ---
def delete(self):
        """ Deletes all the resources associated with the deployment (instance template, network, firewall, instance
        group manager and all its instances.
        """
        self.deployment.delete()
# ---
def usage():
    print("Usage: %s WORKFLOW_DIR" % sys.argv[0])
# ---
def tuple_str_int(test_str):
  res = tuple(int(num) for num in test_str.replace('(', '').replace(')', '').replace('...', '').split(', '))
  return (res)
# ---
def clear(self):
        self.exc = None
        self.tb = None
# ---
def main(*args):
    _args = ()
    for arg in args:
        if isinstance(arg, Path):
            _args += (str(arg),)
        else:
            _args += (arg,)
    _main(_args)
# ---
def delete_job(job_id):
    try:
        job = _get_job(job_id)
        _validate_job_finished(job)
        _remove_job(job_id)
    except ClientError as e:
        logging.info('Cannot delete job, error: %s', e)
        return errCode[e.err_name]
    return {'status': doneCode}
# ---
def __rfloordiv__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__rfloordiv__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.floor_divide, other, self, dtype=result_type(self, other))
# ---
def square_Sum(n):  
    return int(n*(4*n*n-1)/3)
# ---
def __enter__(self):
        return self
# ---
def test_weighted_mean_with_nan_as_zero():
    tensor = torch.tensor([[1.0, float("nan")], [3.0, 4.0]])
    tensor_masked = torch.where(torch.isnan(tensor), 0.0, tensor)
    weights = torch.tensor([[0.1, 0.2], [0.3, 0.4]])
    wrong_result = weighted_mean(tensor_masked, weights)
    correct_result = weighted_mean(tensor, weights)
    assert correct_result > wrong_result
# ---
def forward(self, a, s):
        a = self.a_norm(a)
        s = self.s_norm(s)
        a = sigmoid(self.s_scale(s)) * a + self.s_bias(s)
        return a
# ---
def _format_kvpairs(**kwargs):
	return ', '.join('{0!s}="{1}"'.format(k, _filter_token(v)) for (k, v) in kwargs.items())
# ---
def load_data(self):  # pragma: no cover
        warnings.warn('the Dataset method `load_data` has been deprecated; '
                      'use `load` instead',
                      FutureWarning, stacklevel=2)
        return self.load()
# ---
def set_if_frequency(self, if_frequency):
        self._if_frequency = if_frequency
        self._if_period = 1 / if_frequency * 1e9
# ---
def do(c):
                return c.at["seq", tid].set(INVALID)
# ---
def _is_integer_like_scalar_index(value: Any) -> bool:
        if isinstance(value, (int, np.integer)):
            return True
        if not is_jax_array_like(value):
            return False
        shape = getattr(value, "shape", None)
        if shape != ():
            return False
        dtype = getattr(value, "dtype", None)
        if dtype is None:
            return False
        return jnp.issubdtype(dtype, jnp.integer)
# ---
def test_tree_reduce(spec):
    a = xp.asarray(np.arange(242).reshape((11, 22)), chunks=(3, 4), spec=spec)
    b = tree_reduce(a, np.sum, axis=0, dtype=np.int64, split_every={0: 2})
    assert_array_equal(
        b.compute(), np.arange(242).reshape((11, 22)).sum(axis=0, keepdims=True)
    )
# ---
def __init__(self, interval_seconds: float):
        self._interval = interval_seconds
        self._last_run: float | None = None
# ---
def _sample_array():
    Height, Width = hax.make_axes(Height=2, Width=3)
    data = jnp.array([[1.0, jnp.nan, 3.0], [jnp.nan, 5.0, 6.0]])
    arr = hax.named(data, (Height, Width))
    return Height, Width, data, arr
# ---
def _compute_multiplier(limit: int, dtype, largest_block: int, result):
    """
    Utility function for auto_chunk, to fin how much larger or smaller the ideal
    chunk size is relative to what we have now.
    """
    return (
        limit
        / dtype.itemsize
        / largest_block
        / math.prod(r for r in result.values() if r)
    )
# ---
def __setitem__(self, key, val):
        self.__dict__[key] = val
# ---
def _make_dataset(key: jax.Array, *, n_points: int = 2048) -> tuple[NamedArray, NamedArray]:
    data_axis = Axis("data", n_points)
    feature_axis = Axis("in", 2)
    out_axis = Axis("out", 1)

    xy = jrandom.uniform(key, (n_points, 2), minval=-1.0, maxval=1.0)
    inputs = hax.named(xy, (data_axis, feature_axis))
    targets = hax.named(xy[:, :1], (data_axis, out_axis))
    return inputs, targets
# ---
def restore_original_weights(self, pl_module: LightningModule) -> None:
        """Restore model weights to original weights.

        Parameters
        ----------
        pl_module: LightningModule
            The LightningModule instance.

        """
        pl_module.load_state_dict(self._weights_buffer, strict=False)
        del self._weights_buffer
# ---
def flush_stdout():
        sys.stdout.flush()
        sys.stderr.flush()
        time.sleep(5)
        while not event.is_set():
            print("Waiting...", flush=True)
            print("\n", file=sys.stderr, flush=True)
            time.sleep(5)
# ---
def countdown(n):
            for i in range(n, 0, -1):
                print(f"Countdown: {i}")
                time.sleep(0.3)
            print("Liftoff!")
            return "Done!"
# ---
def find_src(i):
        src = gen_state.decode_state.clone_sources["seq", i].scalar()

        def do(src):
            # match positions where slot_ids == src; take first
            eq = (slot_ids == src).array
            idx = jnp.nonzero(eq, size=1, fill_value=INVALID)[0][0]
            return idx

        return jax.lax.cond(is_valid(src), do, lambda x: x, src)
# ---
def release(self, on_element):
        """Releasing a held mouse button.
        Args:
            on_element: The element to mouse up.
        """
        if on_element: self.move_to_element(on_element)
        self._actions.append(lambda:
            self._driver.execute(Command.MOUSE_UP, {}))
        return self
# ---
def __enter__(self) -> "WorkerPool":
        self._launch_workers()
        self.wait_for_workers()
        return self
# ---
def get_right_shard(self, op_index: int) -> Any:
        """Get right shard for join at given op index.

        Raises:
            ValueError: If no right shard is provided for the join
        """
        shards = self.aux_shards.get(op_index, [])
        if len(shards) != 1:
            raise ValueError(f"Expected exactly 1 right shard for join at op index {op_index}, got {len(shards)}")
        return shards[0]
# ---
def size_in_bytes(self) -> int:
        """How many bytes does this field take up?"""
        return self.dtype.itemsize
# ---
def add_str(test_tup, K):
  res = [ele for sub in test_tup for ele in (sub, K)]
  return (res)
# ---
from operator import eq
def count_same_pair(nums1, nums2):
    result = sum(map(eq, nums1, nums2))
    return result
# ---
def detail_export(self, request, pk=None):
        serializer = ConditionExportSerializer(self.get_object())
        xml = ConditionRenderer().render([serializer.data])
        return XMLResponse(xml, name=self.get_object().key)
# ---
def test_synthetic_subtrees_deterministic():
    r1 = random.Random(99)
    r2 = random.Random(99)
    e1 = generate_synthetic_subtrees(r1, count_per_category=10)
    e2 = generate_synthetic_subtrees(r2, count_per_category=10)
    assert [e.source for e in e1] == [e.source for e in e2]
# ---
def test_wf_improved(self):
        G = nx.union(self.P4, nx.path_graph([4, 5, 6]))
        c = nx.closeness_centrality(G)
        cwf = nx.closeness_centrality(G, wf_improved=False)
        res = {0: 0.25, 1: 0.375, 2: 0.375, 3: 0.25, 4: 0.222, 5: 0.333, 6: 0.222}
        wf_res = {0: 0.5, 1: 0.75, 2: 0.75, 3: 0.5, 4: 0.667, 5: 1.0, 6: 0.667}
        for n in G:
            assert almost_equal(c[n], res[n], places=3)
            assert almost_equal(cwf[n], wf_res[n], places=3)
# ---
def _power_on(driver_info):
    """Turn the power ON for this node.

    :param driver_info: the ipmitool parameters for accessing a node.
    :returns: one of ironic.common.states POWER_ON or ERROR.
    :raises: IPMIFailure on an error from ipmitool (from _power_status call).

    """
    return _set_and_wait(states.POWER_ON, driver_info)
# ---
def specify_none(
        self,
        tokens: np.ndarray,
        contact_mask: np.ndarray,
        design_mask: np.ndarray,
        random: np.random.Generator,
    ):
        pass
# ---
def handle_operation_start_callbacks(callbacks, name):
    if callbacks is not None:
        event = OperationStartEvent(name)
        [callback.on_operation_start(event) for callback in callbacks]
# ---
def post_validate(self, last: List[int]) -> int:
        """Propagate step number for downstream stage validation."""
        in_current_step = all((it == self.step for it in last))
        assert in_current_step, (
            f"stages are executing out of order! On step {self.step!r}."
        )

        return self.step
# ---
def _fix_a_slash_b(string):
    if len(string.split("/")) != 2:
        return string
    a = string.split("/")[0]
    b = string.split("/")[1]
    try:
        a = int(a)
        b = int(b)
        assert string == f"{a}/{b}"
        new_string = "\\frac{" + str(a) + "}{" + str(b) + "}"
        return new_string
    except BaseException:
        return string
# ---
def min_product_tuple(list1):
    result_min = min([abs(x * y) for x, y in list1] )
    return result_min
# ---
def push(self, item):
        """Push an element to the end of the queue.

        Parameters
        ----------
        item :
            The element to append.

        """
        self._queue.append(item)
# ---
def inference_ctx(llama3_tokenizer, dummy_server):
    return LevanterInferenceContext(
        LevanterInferenceContextConfig(
            inference_server_config=None,
            tokenizer=llama3_tokenizer,
            stop_tokens=None,
            max_tokens=100,
            mesh=None,
            axis_mapping={},
        )
    )
# ---
def testHarmonics(self):
        # Regression test for the 'harmonics' parameter
        tone = 100. # arbitrary frequency [Hz]
        freqs = [tone, tone*2, tone*3, tone*4]
        mags = [1]*4

        hpcpAlg = HPCP(minFrequency=50, maxFrequency=500, bandPreset=False, harmonics=3)
        hpcp = hpcpAlg(freqs, mags)
        expected = [0., 0., 0., 0.1340538263, 0., 0.2476127148, 0., 0., 0., 0., 1., 0.]
        self.assertAlmostEqualVector(hpcp, expected, 1e-4)
# ---
def test_get_names(self):
        self.assertEqual(self.config.get_names(), ['', ''])
# ---
def logs_tail(self, handle: VllmServerHandle, *, max_lines: int = 200) -> str:
        return _native_logs_tail(handle.log_dir, max_lines=max_lines)
# ---
def _shards_for_pspec(pspec):
        if mesh is None or pspec is None:
            return 1

        count = 1
        for axis in pspec:
            if axis is None:
                continue
            if isinstance(axis, tuple):
                for sub_axis in axis:
                    count *= _mesh_axis_size(sub_axis)
            else:
                count *= _mesh_axis_size(axis)
        return count
# ---
def compare_to(self, other: "CacheMetadata") -> deepdiff.DeepDiff:
        if other.preprocessor_metadata is None:
            sorta_self = dataclasses.replace(self, preprocessor_metadata=None)
        else:
            sorta_self = self
        return deepdiff.DeepDiff(sorta_self, other)
# ---
def flatten(axes):
            if axes is None:
                return axes
            result = []
            for ax in axes:
                if isinstance(ax, tuple):
                    result += list(ax)
                else:
                    result.append(ax)
            return tuple(result)
# ---

def is_bored(S):
    """
    You'll be given a string of words, and your task is to count the number
    of boredoms. A boredom is a sentence that starts with the word "I".
    Sentences are delimited by '.', '?' or '!'.
   
    For example:
    >>> is_bored("Hello world")
    0
    >>> is_bored("The sky is blue. The sun is shining. I love this weather")
    1
    """
    import re
    sentences = re.split(r'[.?!]\s*', S)
    return sum(sentence[0:2] == 'I ' for sentence in sentences)
# ---
def _set_velocity(self, value):
        self._parameter2 = value
# ---
def keep_first(k, items: Iterator[T]) -> T:
            """Reducer that keeps the first item."""
            return next(iter(items))
# ---
def get_actor_name_from_actor_info(self, actor_info: SliceInfo) -> str:
        return str(actor_info.slice_name)
# ---
def gamma(key, shape: AxisSpec, a: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    a = broadcast_to(a, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.gamma(key=key, a=a, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def authors():
    return [make_author() for _ in range(3)]
# ---
def test_map_overlap_1d_change_dtype():
    x = np.arange(6)
    a = xp.asarray(x, chunks=(3,))

    b = cubed.map_overlap(
        lambda x: x.astype(np.float64),
        a,
        dtype=np.float64,
        chunks=((5, 5),),
        depth=1,
        boundary=0,
        trim=False,
    )

    assert b.dtype == np.float64
    assert_array_equal(b.compute(), np.array([0, 0, 1, 2, 3, 2, 3, 4, 5, 0]))
# ---
def obj_code(self):
                if Field('label')(self) == "LIQUIDITES":
                    return 'XX-liquidity'
                code = CleanText(TableCell('code'))(self)
                return code if is_isin_valid(code) else NotAvailable
# ---
def last_occurence_char(string,char):
 flag = -1
 for i in range(len(string)):
     if(string[i] == char):
         flag = i
 if(flag == -1):
    return None
 else:
    return flag + 1
# ---
def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        """Load the state of the callback.

        Parameters
        ----------
        state_dict: Dict[str, Any]
            The state of the callback to load.

        """
        self._cur_step = state_dict["cur_step"]
        self._ema_weights = state_dict["ema_weights"]
# ---
def test_mask_roundtrip(data_source):
    data = data_source.data

    unflattened = unflatten_masks(data.copy())
    flattened = flatten_masks(unflattened.copy())

    assert flattened == data, "Assume a safe roundtrip"
# ---
def resistance(self):
        return self._resistance
# ---
def main(config: DeconConfig):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

    result = decontaminate(config)
    print(f"Decontamination completed: {result}")
# ---
def reverse_string_list(stringlist):
    result = [x[::-1] for x in stringlist]
    return result
# ---
def get_code_cells(self):
        return [cell['source'] for cell in self.json['cells']
                if cell['cell_type'] == 'code']
# ---
def add_external_ips(self, inc_external_ips):
        ''' add an external_ip to the external_ips list '''
        if not isinstance(inc_external_ips, list):
            inc_external_ips = [inc_external_ips]

        external_ips = self.get_external_ips()
        if not external_ips:
            self.put(Service.external_ips, inc_external_ips)
        else:
            external_ips.extend(inc_external_ips)

        return True
# ---
def __init__(self, path, json_data):
        if path is None:
            path = 'Untitled.ipynb'
        if json_data is None:
            json_data = json.dumps({
                'cells': [],
                'metadata': {'kernelspec': {'name': 'python3'}}})
        self.path = path
        self.json = json.loads(json_data)
        # In cached instances, current_job is already defined.
        if not hasattr(self, 'current_job'):
            self.current_job = None
# ---
def code_inline(node: RenderTreeNode, context: RenderContext) -> str:
    code = node.content
    all_chars_are_whitespace = not code.strip()
    longest_backtick_seq = longest_consecutive_sequence(code, "`")
    if longest_backtick_seq:
        separator = "`" * (longest_backtick_seq + 1)
        return f"{separator} {code} {separator}"
    if code.startswith(" ") and code.endswith(" ") and not all_chars_are_whitespace:
        return f"` {code} `"
    return f"`{code}`"
# ---
def named_shape(self) -> Mapping[str, int]:
        return self.shape
# ---
def state_dict(self) -> Dict[str, Any]:
        """Return the current state of the callback.

        Returns
        -------
        Dict[str, Any]
            The current state of the callback.

        """
        return {
            "cur_step": self._cur_step,
            "ema_weights": self._ema_weights,
        }
# ---
def _sum_wo_cat(a, axis=None, dtype=None, split_every=None):
    if a.shape[axis] == 1:
        return squeeze(a, axis)

    extra_func_kwargs = dict(dtype=dtype)
    return reduction(
        a,
        _chunk_sum,
        axis=axis,
        dtype=dtype,
        split_every=split_every,
        extra_func_kwargs=extra_func_kwargs,
    )
# ---
def getInstance():
        if not ObjectBackendRegistry.instance:
            ObjectBackendRegistry.instance = ObjectBackendRegistry()

        return ObjectBackendRegistry.instance
# ---
def unsize_axes(axis_spec: PartialShapeDict, to_unsize: AxisSelection) -> PartialShapeDict: ...
# ---
def do_fold(init, *args, **kwargs):
            carry = init
            for i, block in enumerate(self.blocks):
                (block_args, block_kwargs) = haliax.tree_util.tree_map(
                    functools.partial(BlockSeq._slice_out, self.Block, i),
                    (args, kwargs),
                )
                carry = block(carry, *block_args, **block_kwargs)
                carry = tree_checkpoint_name(carry, self._carry_ckpt_name)
            return carry
# ---
def copy_page(self, src_page: int, dst_page: int) -> Self:
        """Return a copy of this cache with ``src_page`` cloned into ``dst_page``."""
        raise NotImplementedError
# ---
def __repr__(self):
        return '<{}: {}{}>'.format(
            type(self).__name__,
            self.field,
            '+' if self.ascending else '-',
        )
# ---
def test_expired_deadline_remaining_is_zero():
    """Expired deadline returns zero for remaining time."""
    deadline = Deadline.from_ms(10)
    time.sleep(0.02)  # 20ms - definitely expired

    assert deadline.expired()
    assert deadline.remaining_ms() == 0
    assert deadline.remaining_seconds() == 0.0
# ---
def test_empty_homogeneous_tuples(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.y).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def dot(self, *args, **kwargs) -> "NamedArray":
        if "axis" in kwargs or len(args) == 0:
            return haliax.dot(self, *args, **kwargs)
        else:
            axis = args[0]
            args = args[1:]
            # We want to get the deprecation warning for this style
            return haliax.dot(axis, self, *args, **kwargs)
# ---
def TryJumpLocationInOpenedTab( filename, line, column ):
  filepath = os.path.realpath( filename )

  for tab in vim.tabpages:
    for win in tab.windows:
      if win.buffer.name == filepath:
        vim.current.tabpage = tab
        vim.current.window = win
        vim.current.window.cursor = ( line, column - 1 )

        # Center the screen on the jumped-to location
        vim.command( 'normal! zz' )
        return True
  # 'filename' is not opened in any tab pages
  return False
# ---
def _presentMessage(self, text, interrupt=False):
        """If the text field is not None, presents the given text, optionally
        interrupting anything currently being spoken.

        Arguments:
        - text: the text to present
        - interrupt: if True, interrupt any speech currently being spoken
        """

        self.script.speakMessage(text, interrupt=interrupt)
        try:
            self.script.displayBrailleMessage(text, flashTime=-1)
        except:
            pass
# ---
def test_task_queue_fifo_order(job_request):
    """Tasks are returned in FIFO order."""
    state = ControllerState()

    req1 = job_request("job1")
    req2 = job_request("job2")
    submit_job(state, "j1", req1)
    submit_job(state, "j2", req2)

    pending = state.peek_pending_tasks()
    assert len(pending) == 2
    assert pending[0].job_id == JobName.root("j1")
    assert pending[1].job_id == JobName.root("j2")
# ---
def test_is_stop_signal_multiple_stop_sequences_one_matches():
    # Multiple stop_sequences, only one matches
    tail_tokens = hax.named(jnp.array([8, 9, 10], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(
        jnp.array([[1, 2, 3], [8, 9, 10], [4, 5, 6]], dtype=jnp.int32), axis=("seq", "position")
    )
    assert is_stop_signal(tail_tokens, stop_sequences)
# ---
def __init__(self, rollout_worker_config):
        super().__init__(rollout_worker_config)
        self.rollout_worker_config = rollout_worker_config

        # Metrics
        self.rollouts_generated = 0
        self.weight_transfers = 0
# ---
def transform_web(html: str) -> str:
    """Transform web HTML to markdown using resiliparse."""
    output = convert_page(html, extract_method="resiliparse", config=ResiliparseConfig())
    return output["content"]
# ---
def _get_policy_path(self):
        """Locate the policy json data file.

        :param policy_file: Custom policy file to locate.

        :returns: The policy path

        :raises: ConfigFilesNotFoundError if the file couldn't
                 be located.
        """
        policy_file = CONF.find_file(self.policy_file)

        if policy_file:
            return policy_file

        raise cfg.ConfigFilesNotFoundError(path=CONF.policy_file)
# ---
def test_cumulative_sum_2d_recursive(executor):
    a = xp.ones((10, 100), chunks=(10, 10))
    b = xp.cumulative_sum(a, axis=1)
    assert_array_equal(
        b.compute(executor=executor),
        np.cumulative_sum(np.ones((10, 100)), axis=1),
    )
# ---
def active_scale(self):
        """Return the scaling applied to activations."""
        raise NotImplementedError
# ---
def is_too_many_lines(offset: int, count: int) -> bool:
    return server.config.max_lines is not None and offset + count > server.config.max_lines
# ---
def forward(
        self,
        a,  # Float['... d']
        s,
    ):  # -> Float['... d']:
        a = self.adaln(a, s)
        b = self.swish_gate(a) * self.a_to_b(a)
        a = self.output_projection(s) * self.b_to_a(b)

        return a
# ---
def test_capturing_error_recursive(self):
        with self.getcapture() as cap1:
            print("cap1")
            with self.getcapture() as cap2:
                print("cap2")
                out2, err2 = cap2.readouterr()
                out1, err1 = cap1.readouterr()
        assert out1 == "cap1\n"
        assert out2 == "cap2\n"
# ---
def __post_init__(self):
        # Normalize legacy single-array segment_ids to a tuple for consistency
        if self.segment_ids is not None and not isinstance(self.segment_ids, tuple):
            warnings.warn("Storing segment_ids as a single NamedArray is deprecated. Use a tuple instead.")
            object.__setattr__(self, "segment_ids", (self.segment_ids, self.segment_ids))
# ---
def get_bytes(self):
        midi_str = put_variable_length_number(self.time)
        return midi_str
# ---
def log_hyperparameters(self, hparams: typing.Mapping[str, Any]):
        self.writer.add_hparams(hparams, {"dummy": 0})
# ---
def init(config: Gpt2Config, *, key):
        # vectorize the blocks
        blocks = Stacked.init(config.Layers, Gpt2Block, gradient_checkpointing=config.gradient_checkpointing)(
            config,
            key=shaped_rng_split(key, config.num_layers),
        )
        ln_f = hnn.LayerNorm.init(config.Embed, eps=config.layer_norm_epsilon, use_bias=config.use_bias)

        return Gpt2Transformer(config, blocks, ln_f)
# ---
def _summary_block() -> dict:
        if config_name:
            summary = size_info.get("config")
            if summary is not None:
                return summary
        summary = size_info.get("dataset")
        if summary is not None:
            return summary
        return {}
# ---
def __str__(self) -> str:
        """Canonical wire format: '/root/child/grandchild'."""
        return "/" + "/".join(self._parts)
# ---
def eval_func_der(self, m, k, r, i):
        """
        Find the derivative of the evaluation function with respect
        to the ith component of the vector r
        """
        return self.data[m][k][i]
# ---
def _is_url_like(path):
        return urllib.parse.urlparse(path).scheme != ""
# ---
def loadPics(self):
        self.standing = loadImage("gripe_stand.png")
        self.falling = loadImage("grfalling.png")
        for i in range(8):
            imageName = "gr" + str(i) + ".png"
            self.walkR.append(loadImage(imageName))
        for i in range(8):
            imageName = "gl" + str(i) + ".png"
            self.walkL.append(loadImage(imageName))
# ---
def _end_message_token(self) -> int:
        tokens = self.tokenizer.encode("<|im_end|>", add_special_tokens=False)
        assert len(tokens) == 1, f"Expected single token for <|im_end|>, got {len(tokens)}"
        return tokens[0]
# ---
def __init__(self, client):
        super(Model_LicenseInfo, self).__init__(client)
        Util.validate_type(client, "saklient.cloud.client.Client")
# ---
def test_cumulative_sum_1d():
    a = xp.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], chunks=(4,))
    b = xp.cumulative_sum(a, axis=0)
    assert_array_equal(
        b.compute(),
        np.cumulative_sum(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), axis=0),
    )
# ---
def worker(idx: int):
        barrier.wait()
        results[idx] = actor.increment.remote(1).result()
# ---
def test_actor_group_wait_ready_partial(client: LocalClient):
    group = client.create_actor_group(Counter, name="counters", count=5)
    handles = group.wait_ready(count=2)
    assert len(handles) == 2
# ---
def list_workers(ctx):
    """List Ray workers."""
    with ray_dashboard(DashboardConfig.from_cluster(ctx.obj.config_file)):
        result = subprocess.check_output(
            ["ray", "list", "workers", "--format=json", f"--limit={1000}"],
            text=True,
            timeout=60,
        )
        print(json.dumps(json.loads(result), indent=2))
# ---
def path(self) -> str:
        return self._path
# ---
def _get_fn_name(fn: Any) -> str:
    """Safely get a function name, handling partials and callables."""
    # Unwrap partials to find the underlying function name
    while hasattr(fn, "func"):
        fn = fn.func
    return getattr(fn, "__qualname__", getattr(fn, "__name__", str(fn)))
# ---
def _end_message_token(self) -> int:
        (token,) = self.tokenizer.encode("<|eot_id|>", add_special_tokens=False)
        return token
# ---
def test_build_runtime_env_tpu_clears_jax_platforms():
    from fray.v2.ray_backend.backend import build_runtime_env

    request = JobRequest(
        name="tpu-test",
        entrypoint=Entrypoint.from_callable(lambda: None),
        resources=ResourceConfig(device=TpuConfig(variant="v4-8")),
    )
    env = build_runtime_env(request)
    assert env["env_vars"]["JAX_PLATFORMS"] == ""
# ---
def test_axis_shapes_overlap_error():
    cfg = MeshConfig(axes={"data": 1}, dcn_axes={"data": 1})
    with pytest.raises(ValueError):
        cfg.axis_shapes(num_devices=4, num_slices=1)
# ---
def codespan(self, text):
        """Rendering inline `code` text.

        :param text: text content for inline code.
        """
        text = escape(text.rstrip(), smart_amp=False)
        return '<code>%s</code>' % text
# ---
from collections import defaultdict 
def freq_element(test_tup):
  res = defaultdict(int)
  for ele in test_tup:
    res[ele] += 1
  return (str(dict(res)))
# ---
def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="boltzgen",
        description="Boltzgen command line interface",
    )
    subparsers = parser.add_subparsers(dest="command", required=True)
    build_run_parser(subparsers)
    build_configure_parser(subparsers)
    build_execute_parser(subparsers)
    build_download_parser(subparsers)
    build_check_parser(subparsers)
    return parser
# ---
def compute_reward(self, correct_answer: str, actual_response: str, tokenizer=None) -> float:
        """Compute reward for a response."""
        ...
# ---
def init_w_grad():
        w_grad_tile_ref[...] = jax.lax.dot_general(
            x_ref[...],
            xw_scratch_ref[...],
            (((0,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
        w_write_future.start()
# ---
def get_apartment_count():
        # type: () -> int
        return Apartment.select().count()
# ---
def __init__(self, dim):
    super().__init__()
    self.weight = nn.Parameter(torch.ones([dim]))
    self.dim = dim
# ---
def init(cls, Vocab: Axis, config: GemmaConfig, *, key) -> "GemmaLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = GemmaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return GemmaLMHeadModel(transformer, embeddings, lm_head)
# ---
def dispatcher(self):
        return event.EventDispatcher()
# ---
def _run_dashboard(host: str, port: int):
    """Run the FastAPI dashboard server."""
    uvicorn.run(app, host=host, port=port, log_level="info")
# ---
def test_parse_json_document(self):
        tmpl_str = '["foo" , "bar"]'
        msg = 'The template is not a JSON object or YAML mapping.'
        self._parse_template(tmpl_str, msg)
# ---
def teardown(self) -> None:
        if self._awaitable:
            _cancel_tasks_and_wait([self._awaitable])
        self._awaitable = None
        self._host_info = None
# ---
def step_impl(context, username, password, email, first_name, last_name):
    context.base_user = User(username=username, email=email, password=password, first_name=first_name,
                        last_name=last_name)
# ---
def power_base_sum(base, power):
    return sum([int(i) for i in str(pow(base, power))])
# ---
def gensym(name="array"):
    """Generate a name with an incrementing counter"""
    global sym_counter
    sym_counter += 1
    return f"{name}-{sym_counter:03}"
# ---
def get_legacy_sigopcount_block(block, accurate=True):
    count = 0
    for tx in block.vtx:
        count += get_legacy_sigopcount_tx(tx, accurate)
    return count
# ---
def fn(x):
        return {
            "expanded": hax.broadcast_axis(x, D),
            "twice": x + x,
        }
# ---
def save_settings(self):
        settings = QSettings()
        settings.setValue(WINDOWSTATE_SETTING, self.saveState())
        settings.setValue(GEOMETRY_SETTING, self.saveGeometry())
        settings.setValue(FILENAME_SETTING, self.filename)
# ---
def use_flash_attention(self) -> bool:
        """Whether to use flash attention based on the backend."""
        if self.attn_backend is None:
            return default_attention_type() != AttentionBackend.VANILLA
        return self.attn_backend != AttentionBackend.VANILLA
# ---
def test_order_by_selectable_in_unions(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2).order_by(table.c.id)
        s2 = select([table]).where(table.c.id == 3).order_by(table.c.id)

        u1 = union(s1, s2).limit(2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def hard_wrap(self):
        """Grammar for hard wrap linebreak. You don't need to add two
        spaces at the end of a line.
        """
        self.linebreak = re.compile(r'^ *\n(?!\s*$)')
        self.text = re.compile(
            r'^[\s\S]+?(?=[\\<!\[_*`~]|https?://| *\n|$)'
        )
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"token_embeddings": "model.embed_tokens"}
# ---
def test_composed_multiple(self):
        table = self.tables.some_table
        lx = (table.c.x + table.c.y).label("lx")
        ly = (func.lower(table.c.q) + table.c.p).label("ly")
        self._assert_result(
            select([lx, ly]).order_by(lx, ly.desc()),
            [(3, util.u("q1p3")), (5, util.u("q2p2")), (7, util.u("q3p1"))],
        )
# ---
def test___cmp__ne(self):
        self._test__cmp__(
            lambda left, right: left != right,
            (
                True,
                False,
                False,
                True,
                False,
                True,
                PY3,
                True,
                True,
                True,
            ),
            '!='
        )
# ---
def test_filter_ips_black_list(self):
        CONF.network_label_regex = '.*'
        CONF.ip_regex = '.*'
        CONF.black_list_regex = '^10.123.123.*'
        ip = self.instance.get_visible_ip_addresses()
        ip = filter_ips(
            ip, CONF.ip_regex, CONF.black_list_regex)
        self.assertEqual(2, len(ip))
        self.assertTrue('10.123.123.123' not in ip)
# ---
def __repr__(self):
        return ', '.join('{}'.format(el) for el in self._queue)
# ---
def get_pairs_count(arr, n, sum):
    count = 0 
    for i in range(0, n):
        for j in range(i + 1, n):
            if arr[i] + arr[j] == sum:
                count += 1
    return count
# ---
def is_ancestor_of(self, other: "JobName", *, include_self: bool = True) -> bool:
        """True if this job name is an ancestor of another job name."""
        if include_self and self == other:
            return True
        if len(self._parts) >= len(other._parts):
            return False
        return other._parts[: len(self._parts)] == self._parts
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        self.metrics["hparams"] = hparams
# ---
def __init__(self, func, queue):
            super(Worker, self).__init__()
            self._func = func
            self.q = queue
# ---
def test_parse_no_version_format(self):
        yaml = ''
        self._parse_template(yaml, 'Template format version not found')
        yaml2 = '''Parameters: {}
Mappings: {}
Resources: {}
Outputs: {}
'''
        self._parse_template(yaml2, 'Template format version not found')
# ---
def _inv_sqrt_decay_schedule(lr: float, min_lr: float, warmup_steps: int, timescale: float = 10000):
    def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / jnp.sqrt(jnp.maximum(count + warmup_steps, 1) / timescale))
        return jnp.maximum(lr * decay, min_lr)

    return schedule
# ---
def __init__(self, config, vocab_size, mask_index):
    super().__init__(config, vocab_size)
    self.mask_index = mask_index
    self.neg_infinity = -1000.0
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> float:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def poke(self, context):
        hook = BaseHook.get_connection(self.conn_id).get_hook()

        logging.info('Poking: ' + self.sql)
        records = hook.get_records(self.sql)
        if not records:
            return False
        else:
            if str(records[0][0]) in ('0', '',):
                return False
            else:
                return True
            print(records[0][0])
# ---
def _apply_detokenizer(detokenizer):
    def detok(text):
      for i, t in enumerate(text, 0):
        text[i] = detokenizer(t)
      return text
    return detok
# ---
def __len__(self):
        return len(self._queue)
# ---
def requires(self):
        return RTask()
# ---
def _watch_stop() -> None:
                    self._stop_event.wait()
                    assert on_stop is not None
                    try:
                        on_stop()
                    except Exception:
                        logger.exception("on_stop callback failed for %s", name or "<unnamed>")
# ---
def decimal_to_Octal(deciNum):
    octalNum = 0
    countval = 1;
    dNo = deciNum;
    while (deciNum!= 0):
        remainder= deciNum % 8;
        octalNum+= remainder*countval;
        countval= countval*10;
        deciNum //= 8; 
    return (octalNum)
# ---
def __lt__(self, other):
        if not hasattr(other, "handler_order"):
            # No handler_order, leave in original order.  Yuck.
            return True
        return self.handler_order < other.handler_order
# ---
def on_fuzzing_range_end_changed(self, value: int):
        self.ui.sBAddRangeStart.setMaximum(value - 1)
        self.ui.sBAddRangeStep.setMaximum(value - self.ui.sBAddRangeStart.value())
# ---
def _get_lb_name(self, msg):
        # TODO(wenjianhn): utf-8 support, base64
        ##return "%s_%s" % (msg['project_id'],
        return "%s" % msg['uuid']
# ---
def __send(self, data, flags=0):
        try:
            return self.__iowait(self._connection.send, data, flags)
        except OpenSSL.SSL.SysCallError as e:
            if e[0] == -1 and not data:
                # errors when writing empty strings are expected and can be ignored
                return 0
            raise
# ---

def unique_digits(x):
    """Given a list of positive integers x. return a sorted list of all 
    elements that hasn't any even digit.

    Note: Returned list should be sorted in increasing order.
    
    For example:
    >>> unique_digits([15, 33, 1422, 1])
    [1, 15, 33]
    >>> unique_digits([152, 323, 1422, 10])
    []
    """
    odd_digit_elements = []
    for i in x:
        if all (int(c) % 2 == 1 for c in str(i)):
            odd_digit_elements.append(i)
    return sorted(odd_digit_elements)
# ---
def __call__(self, text, rules=None):
        return self.output(text, rules)
# ---
def volume(vol) :
    global player
    if player == 'omxplayer':
        return volume_omxplayer(vol)
    else:
        return volume_alsa(vol)
# ---
def sign(a: A) -> A:
    return wrap_elemwise_unary(jnp.sign, a)
# ---
def _format_timestamp(ms: int) -> str:
    if ms == 0:
        return "-"
    return Timestamp.from_ms(ms).as_formatted_date()
# ---
def list_images_with_detail(self, params=None):
        """Returns a detailed list of images filtered by any parameters."""
        url = 'images/detail'
        if params:
            url += '?%s' % urllib.urlencode(params)

        resp, body = self.get(url)
        body = json.loads(body)
        self.validate_response(schema.list_images_details, resp, body)
        return service_client.ResponseBodyList(resp, body['images'])
# ---
def copy_template(self, template, dest, mode=0o644):
        "Copy template to dest in workflowdir with mode"
        path = os.path.join(self.workflowdir, dest)
        t = self.jinja.get_template(template)
        t.stream(**self.__dict__).dump(path)
        os.chmod(path, mode)
# ---
def __getitem__(self, idx):
        return (self.datasets[idx], self.lengths[idx])
# ---
def test_full_flexibility():
    partial_order = (...,)
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
# ---
def do_time_ctrl(self, command):
        msg = Forseti.TimeControl()
        msg.command_name = command
        self.lc.publish('Timer/Control', msg.encode())
# ---
def all_Bits_Set_In_The_Given_Range(n,l,r): 
    num = ((1 << r) - 1) ^ ((1 << (l - 1)) - 1) 
    new_num = n & num 
    if (num == new_num): 
        return True
    return False
# ---
def is_finite(self) -> bool:
        return self.cache.is_finite()
# ---
def get_init_log(self, vm_id: str, tail: int | None = None) -> str:
        """Get initialization log for a VM.

        Returns empty string if VM is not found.
        """
        vm = self.get_vm(vm_id)
        return vm.init_log(tail) if vm else ""
# ---
def test_load_env_vars_single_key():
    """Test env var with no value (empty string)."""
    result = load_env_vars([["KEY_ONLY"]])
    assert result["KEY_ONLY"] == ""
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        self.metrics["artifact"] = {"path": artifact_path, "name": name, "type": type}
# ---
def test_static_method_skip(self, mock_start, mock_stop):
        self.assertEqual(25, FakeTraceStaticMethodSkip.static_method(25))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def create_child(self, name: str) -> "ThreadContainer":
        """Create a child container whose lifecycle is bound to this parent."""
        child = ThreadContainer(name=name)
        with self._lock:
            self._children.append(child)
        return child
# ---
def __init__(self, backend):
        import matplotlib.pyplot as plt
        from IPython.core.interactiveshell import InteractiveShell
        from IPython.core.pylabtools import backend2gui

        self.shell = InteractiveShell.instance()
        self.old_backend = backend2gui[str(plt.get_backend())]
        self.new_backend = backend
# ---
def is_finished(self) -> bool:
        return self.state in (
            cluster_pb2.JOB_STATE_SUCCEEDED,
            cluster_pb2.JOB_STATE_FAILED,
            cluster_pb2.JOB_STATE_KILLED,
            cluster_pb2.JOB_STATE_UNSCHEDULABLE,
        )
# ---
def managed(self):
        return None
# ---
def test_filter_ips_white_list(self):
        CONF.network_label_regex = '.*'
        CONF.ip_regex = '^(15.|123.)'
        CONF.black_list_regex = '^10.123.123.*'
        ip = self.instance.get_visible_ip_addresses()
        ip = filter_ips(
            ip, CONF.ip_regex, CONF.black_list_regex)
        self.assertEqual(2, len(ip))
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)
# ---
def test_write_string_table(datadir):
    from openpyxl.writer.strings import write_string_table

    datadir.join("reader").chdir()
    table = ['This is cell A1 in Sheet 1', 'This is cell G5']
    content = write_string_table(table)
    with open('sharedStrings.xml') as expected:
        diff = compare_xml(content, expected.read())
        assert diff is None, diff
# ---
def test_format_shard_path_basename_placeholder():
    """Test that {basename} placeholder works."""
    pattern = "output/{basename}-{shard:05d}.jsonl"
    result = format_shard_path(pattern, 3, 10)
    assert result == "output/shard_3-00003.jsonl"
# ---
def reciprocal(a: A) -> A:
    return wrap_elemwise_unary(jnp.reciprocal, a)
# ---
def __init__(
        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, "Current State", device_id)
# ---
def _default_tpu_name_from_config(config_data: dict | None, username: str) -> str | None:
    if not config_data:
        return None
    cluster_name = config_data.get("cluster_name")
    if not cluster_name:
        return None
    return f"dev-{cluster_name}-{username}"
# ---
def extract_results(result: Any) -> tuple[list[str], list[str]]:
    if isinstance(result, (pa.RecordBatch, pa.Table)):
        id_col = "doc_id" if "doc_id" in result.schema.names else "id"
        return result["hash"].to_pylist(), result[id_col].to_pylist()
    return [x["hash"] for x in result], [x["id"] for x in result]
# ---
def subtract(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.subtract(x1, x2)
# ---
def test_capture_badoutput_issue412(testdir):
    testdir.makepyfile("""
        import os

        def test_func():
            omg = bytearray([1,129,1])
            os.write(1, omg)
            assert 0
        """)
    result = testdir.runpytest('--cap=fd')
    result.stdout.fnmatch_lines('''
        *def test_func*
        *assert 0*
        *Captured*
        *1 failed*
    ''')
# ---
def ring_buffer():
    return LogRingBuffer(maxlen=10)
# ---
def test_k5_closeness(self):
        c = nx.closeness_centrality(self.K5)
        d = {0: 1.000, 1: 1.000, 2: 1.000, 3: 1.000, 4: 1.000}
        for n in sorted(self.K5):
            assert almost_equal(c[n], d[n], places=3)
# ---
def slice_id(self) -> str:
        """Alias for group_id - the primary ID used in SliceInfo protos."""
        ...
# ---
def test_read_dataset_streaming(self, sample_data, tmpdir, ext, create_fn):
        """Test streaming reading for both JSONL.GZ and Parquet files"""
        input_file = os.path.join(tmpdir, f"test_input.{ext}")
        create_fn(sample_data, input_file)

        rows = list(read_dataset_streaming(input_file))

        assert len(rows) == len(sample_data)
        assert rows[0]["id"] == "doc1"
        assert rows[0]["text"] == sample_data[0]["text"]
# ---
def name(self):
        """Return the name of the switch."""
        return self._name
# ---
def check_xsrf_cookie(self):
        return
# ---
def test_weird_inline_newlines():
    html = """<I>Murray
Montgomery</I><BR>"""
    expected = "*Murray Montgomery*  \n"
    assert to_markdown(html) == expected
# ---
def has_len(self):
        return self.data_store.is_finite()
# ---
def getReplicas( self, lfn, rpc = '', url = '', timeout = None ):
    res = self.__checkArgumentFormat( lfn )
    if not res['OK']:
      return res
    lfns = res['Value'].keys()
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.getReplicas( lfns )
# ---
def convert_to_export(self, value, env):
        return ','.join(name for id, name in value.name_get()) if value else ''
# ---
def traced_func(i):
    return i
# ---
def on_random_range_max_changed(self):
        self.ui.spinBoxRandomMinimum.setMaximum(self.ui.spinBoxRandomMaximum.value() - 1)
# ---
def is_terminal(self) -> bool:
        """Check if this attempt is in a terminal state."""
        return self.state in TERMINAL_TASK_STATES
# ---
def width(self, new_width):
        self.set_size(new_width, self.height)
# ---
def dict_value(self, decode_str=False):
        return { self.keys[i].decode('utf-8'): self.values[i].node_value(decode_str) for i in range(self.num) }
# ---
def create_completeness_minus_contamination_column(pd_tool_bins):
    pd_tool_bins['newcolumn'] = pd_tool_bins['recall_bp'] + pd_tool_bins['precision_bp'] - 1
# ---
def test_special_tokens_injection(marin_tokenizer: PreTrainedTokenizer):
    """Test that special tokens are correctly replaced."""
    special_tokens_injection_check(marin_tokenizer)
# ---
def hook(mod):
    if sys.version[0] > '1':
        for i in range(len(mod.imports)-1, -1, -1):
            if mod.imports[i][0] == 'strop':
                del mod.imports[i]
    return mod
# ---
def cumsum(self, axis: AxisSelector, *, dtype=None) -> "NamedArray":  # pragma: no cover
        return haliax.cumsum(self, axis=axis, dtype=dtype)
# ---
def subfunction(self):
        run_debug(function(self))
# ---
def _pspec_for(self, shape_spec: ShapeSpec | NamedShapeSpec) -> PartitionSpec:
        if isinstance(shape_spec, ShapeSpec):  # type: ignore
            batch_name = hax.partitioning.physical_axis_name(self.dl.batch_axis_name, self.dl.axis_resources)
            return PartitionSpec(batch_name, *((None,) * (len(shape_spec.shape) - 1)))
        else:
            return hax.partitioning.pspec_for_axis(shape_spec.shape, self.dl.axis_resources)
# ---
def test_list(self):
        """Store and retrieve a list"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "abc", "l": ["a", 1, False]})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["l"], ["a", 1, False])
# ---
def is_subset_sum(set, n, sum):
	if (sum == 0):
		return True
	if (n == 0):
		return False
	if (set[n - 1] > sum):
		return is_subset_sum(set, n - 1, sum)
	return is_subset_sum(set, n-1, sum) or is_subset_sum(set, n-1, sum-set[n-1])
# ---
def init_logits():
        xw_scratch_ref[...] = jax.lax.dot_general(
            x_ref[...],
            w_ref[...],
            (((1,), (0,)), ((), ())),
            preferred_element_type=jnp.float32,
            precision=precision,
        )
# ---
def to_grug_model_config(self) -> GrugModelConfig:
        return GrugModelConfig(
            vocab_size=llama3_tokenizer_vocab_size,
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            max_seq_len=self.max_seq_len,
        )
# ---
import sys 
def min_coins(coins, m, V): 
    if (V == 0): 
        return 0
    res = sys.maxsize 
    for i in range(0, m): 
        if (coins[i] <= V): 
            sub_res = min_coins(coins, m, V-coins[i]) 
            if (sub_res != sys.maxsize and sub_res + 1 < res): 
                res = sub_res + 1  
    return res
# ---
def fake_pool_set_name_label(self, session, pool_ref, name):
            fake_pool_set_name_label.called = True
# ---
def __init__(self, model_name: str, attribute_name: str, *args, **kwargs):
        super().__init__(model_name, attribute_name)
# ---
def foo(x):
            return x
# ---
def get_vm(self, vm_id: str) -> ManagedVm | None:
        """Get a specific VM by ID."""
        with self._lock:
            return self._vms.get(vm_id)
# ---
def indentBlock(block):
            cursor = cursorAtSpaceEnd(block)
            cursor.insertText(' ' if withSpace else self.text())
# ---
def cli():
    """TPU CI Infrastructure Management - Manage preemptible TPU VMs for GitHub Actions CI."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        stream=sys.stderr,
    )
# ---
def test_to_numeric(self):
        def test_impl(df):
            B = pd.to_numeric(df.A, errors='coerce')
            return B

        df = pd.DataFrame({'A': ['123.1', '331.2']})
        hpat_func = self.jit(locals={'B': types.float64[:]})(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def outer_product(x, y):
        return np.einsum("i,j->ij", x, y)
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        return ScaleByAdamHState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)
# ---
def test_actor_thread_safety(job_context):
    actor = job_context.create_actor(SimpleActor, 0)

    futures = [actor.increment.remote(1) for _ in range(100)]
    [job_context.get(f) for f in futures]

    final_value = actor.get_value.remote()
    assert job_context.get(final_value) == 100
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        """Total peak FLOP/s across all devices."""
        if isinstance(self.device, CpuConfig):
            # just use some reasonable number
            return 100e9
        return self.device_flops(dtype) * self.chip_count()
# ---
def test_truediv_operator(self):
        """Test the / operator for path joining."""
        base = S3Location(bucket="test-bucket", path="base/path")
        unresolved = UnresolvedLocation(path="subdir/file.zarr")

        resolved = base / unresolved

        assert isinstance(resolved, S3Location)
        assert resolved.path == "base/path/subdir/file.zarr"
# ---
def flatten_axes(axis: AxisSpec, old_axes: AxisSelection, new_axis: AxisSelector) -> AxisSpec:
    pass
# ---
def count_folder(count: int, item: T) -> tuple[bool, int]:
            return (count < size, count + 1)
# ---
def test_bound_in_scalar(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.x.in_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [(2,), (3,), (4,)], params={"q": [2, 3, 4]})
# ---
import re 
regex = '[a-zA-z0-9]$'
def check_alphanumeric(string): 
	if(re.search(regex, string)): 
		return ("Accept") 
	else: 
		return ("Discard")
# ---
def sub_add(self, filename):
        self.command('sub_add', filename.encode(fs_enc))
# ---
def _sampling_noise(self):
    pass
# ---
def __rmod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.mod(other, self)
# ---
def evaluate(self, record: dict) -> Any:
        return _ARITHMETIC_OPS[self.op](self.left.evaluate(record), self.right.evaluate(record))
# ---
def stop(self, timeout: Duration = Duration.from_seconds(10.0)) -> None:
        """Signal VM thread to stop and wait for it to exit."""
        self._threads.stop(timeout=timeout)
# ---
def unprotect(self, tag: str) -> None:
        """No-op for local provider (no eviction)."""
        del tag
# ---
def __init__(self, directory=None):
        if directory is None:
                directory = []
        import copy
        self.directory = copy.deepcopy(directory)
        self.bound = False
        self.start_tls_called = False
        self.extend = self.Extend(self)

        self.operation = {
                    "!" : self._search_not,
                    "&" : self._search_and,
                    "|" : self._search_or,
            }
# ---
def ongoing_process():
        while True:
            for item in range(1, 101):
                yield item
# ---
import re
def split_lowerstring(text):
 return (re.findall('[a-z][^a-z]*', text))
# ---
def test_logical_or(self):
        expr = (col("a") > 0) | (col("b") > 0)
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def test_updated_on(self):
        eq_(self.record.updated_on, None)
# ---
def visit_munderover(self, element):
        children = self._get_clean_children(element)
        if len(children) == 3:
            base = self._visit(children[0])
            under = self._visit(children[1])
            over = self._visit(children[2])
            return BracedNode(f"{{{base}}}_{{{under}}}^{{{over}}}")
        return TextNode("")
# ---
def start(self):
        """Start background polling thread."""
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._poll_loop, daemon=True)
        self._thread.start()
# ---
def title_field_is_required_present(self):
        """
        :rtype: bool
        """
        return self._is_element_present(AddMoviePageLocators.TITLE_INPUT_ERROR_LOCATOR)
# ---
def testAssignTuple(self):
    self.assertEqual((0, 'a b\n'), _GrumpRun(textwrap.dedent("""\
        baz = ('a', 'b')
        foo, bar = baz
        print foo, bar""")))
# ---
def em(node: RenderTreeNode, context: RenderContext) -> str:
    text = make_render_children(separator="")(node, context)
    indicator = node.markup
    return indicator + text + indicator
# ---
def _get_mixtral_config(use_flash=False, num_kv_heads=4, seq_len=128) -> MixtralConfig:
    return MixtralConfig(
        max_seq_len=seq_len,
        hidden_dim=16,
        intermediate_dim=32,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,  # disable for tests so debugging is easier
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
    )
# ---
def as_short_time(self) -> str:
        """Format as HH:MM:SS for log lines."""
        dt = datetime.fromtimestamp(self.epoch_seconds(), tz=timezone.utc)
        return dt.strftime("%H:%M:%S")
# ---
def test_lookup(self):
        assert event.Priority.lookup(event.Priority.CORE) is event.Priority.CORE
        assert event.Priority.lookup(event.Priority.CORE.value) is event.Priority.CORE
        assert event.Priority.lookup(-12312412) == -12312412
# ---
def translate_path(path):
    return xbmc.translatePath(path).decode('utf-8')
# ---
def from_data_source(
        cls,
        data: Float[torch.Tensor, "batch time variable lat lon"],
        mask: Float[torch.Tensor, " variable"],
        src: DataSource,
    ) -> Self:
        means_torch = _flatten(src.means)
        stds_torch = _flatten(src.stds)
        return cls(data, means_torch, stds_torch, mask)
# ---
def fresh_marin_tokenizer():
    try:
        base = load_llama3_tokenizer()
    except Exception as exc:
        pytest.skip(f"Could not load llama3 tokenizer: {exc}", allow_module_level=True)
    return create_marin_tokenizer(base)
# ---
def fused_key_func(out_key):
        args = key_function(out_key)
        # split all args to the fused function into groups, one for each predecessor function
        func_args = tuple(
            item
            for pkf, a in zip(predecessor_key_functions, args, strict=True)
            for item in apply_blockwise_key_func(pkf, a)
        )
        return split_into(func_args, predecessor_funcs_nargs)
# ---
def init(cls, Vocab: Axis, config: LmConfigT, *, key: PRNGKeyArray) -> "LmHeadModel[LmConfigT]":
        pass
# ---
def _mean_func(a, **kwargs):
    dtype = dict(kwargs.pop("dtype"))
    n = _numel(a, dtype=dtype["n"], **kwargs)
    total = nxp.sum(a, dtype=dtype["total"], **kwargs)
    return {"n": n, "total": total}
# ---
def _map(example: dict) -> LmExample:
                loss_weight = example[loss_weights_key]
                loss_weight = self.loss_weight_transform(loss_weight)
                return _create_lm_example(example[input_ids_key], loss_weight)
# ---
def lucky_num(n):
 List=range(-1,n*n+9,2)
 i=2
 while List[i:]:List=sorted(set(List)-set(List[List[i]::List[i]]));i+=1
 return List[1:n+1]
# ---
def get_fai(b):
    '''Just the internals of the FAI method'''
    return b['b2'].subtract(b['b1'].add(b['b5'].subtract(b['b1']).multiply((859.0 - 645) / (1240 - 645)))).select(['sur_refl_b02'], ['b1'])
# ---
def all_different_after_swap(l):
    final = [s[-1] for s in l]
    return len(final) == len(set(final))
# ---
def count_Primes_nums(n):
    ctr = 0
    for num in range(n):
        if num <= 1:
            continue
        for i in range(2,num):
            if (num % i) == 0:
                break
        else:
            ctr += 1
    return ctr
# ---
def read_batch(self, timeout: float | None = None) -> RolloutBatch | None:
        """Read a single batch with optional timeout."""
        return self._queue.pop(timeout)
# ---
def mock_render_to_string(template_name, context):
    """Return a string that encodes template_name and context"""
    return str((template_name, context))
# ---
def on_close():
            """The user attempted to close the window.

            This event can be triggered by clicking on the "X" control box in
            the window title bar, or by some other platform-dependent manner.

            The default handler sets `has_exit` to ``True``.  In pyglet 1.1, if
            `pyglet.app.event_loop` is being used, `close` is also called,
            closing the window immediately.

            :event:
            """
# ---
def get(self) -> int:
        return self._value
# ---
def test_with_args(self, mock_start, mock_stop):
        self.assertEqual(1, traced_func(1))
        expected_info = {
            "info": "some_info",
            "function": {
                "name": "osprofiler.tests.unit.test_profiler.traced_func",
                "args": str((1,)),
                "kwargs": str({})
            }
        }
        mock_start.assert_called_once_with("function", info=expected_info)
        mock_stop.assert_called_once_with()
# ---
def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LmDatasetSourceConfigBase:
        humanfriendly_tokens = humanfriendly.format_size(self.num_tokens)[0:-1].replace(" ", "").replace("byte", "")
        out = _patch_source_config(
            self.input_config, self.cache_path, extra_tags=["subsampled", f"subsampled-{humanfriendly_tokens}"]
        )

        return out
# ---
def is_finite(self) -> bool:
        return self.dataset.is_finite() and self.end_index is not None
# ---
def read_all_available(self) -> list[RolloutBatch]:
        """Read all currently available batches without blocking."""
        return self._queue.pop_all()
# ---
def test_deadline_from_now_with_duration():
    """Deadline.from_now works with Duration."""
    deadline = Deadline.from_now(Duration.from_ms(100))
    assert not deadline.expired()
    time.sleep(0.15)
    assert deadline.expired()
# ---
def fn(config: MyConfig | None):
        pass
# ---
def test_works_with_a_single_feature_in_fc(self):
    from_geom = ee.FeatureCollection(ee.Geometry.Point(16.37, 48.225))

    df = dask_ee.read_ee(from_geom)

    self.assertEqual(list(df.columns), ['geo'])
    self.assertEqual(df.compute().shape, (1, 1))
# ---
def forget(self, request):
		return self.match(request).forget(request)
# ---
def __call__(self, t, x):
        """Apply exponential modulation to input.

        Args:
            t: Time values
            x: Input tensor to modulate

        Returns:
            Modulated tensor
        """
        if self.modulate:
            decay = hax.exp(-t * self.deltas_abs.broadcast_axis(self.PosPerBlock))
            x = x * (decay + self.shift)

        return x
# ---
def default_validation_sets(tokenizer: str, base_path: str = "tokenized/") -> dict[str, TokenizerStep]:
    # Avoid circular dependencies
    # TODO: Will - break apart defaults a bit
    from experiments.evals.exp1600_uncheatable_evals import uncheatable_eval_tokenized

    validation_sets = dict(paloma_tokenized(base_path=base_path, tokenizer=tokenizer))
    validation_sets.update(uncheatable_eval_tokenized(base_path=base_path, tokenizer=tokenizer))
    return validation_sets
# ---
class Node: 
	def __init__(self, data): 
		self.data = data 
		self.left = None
		self.right = None
def get_height(root): 
	if root is None: 
		return 0
	return max(get_height(root.left), get_height(root.right)) + 1
def is_tree_balanced(root): 
	if root is None: 
		return True
	lh = get_height(root.left) 
	rh = get_height(root.right) 
	if (abs(lh - rh) <= 1) and is_tree_balanced( 
	root.left) is True and is_tree_balanced( root.right) is True: 
		return True
	return False
# ---
def __init__(self, upsampling: int = 2, **kwargs):
        super().__init__()
        self.upsampler = torch.nn.Upsample(scale_factor=upsampling, mode="bilinear")
# ---
def content_only(args: list[str]) -> str:
        return args[0] if args else ""
# ---
def test_registry_unregister_removes_vm(registry: VmRegistry):
    """Unregistering a VM removes it from the registry."""
    vm = MagicMock()
    vm.info = vm_pb2.VmInfo(vm_id="test-vm-001")

    registry.register(vm)
    registry.unregister("test-vm-001")

    assert registry.vm_count() == 0
    assert registry.get_vm("test-vm-001") is None
# ---
def test_find_image_in_cache_no_shares(self):
        drv = self._driver
        drv._mounted_shares = []
        result = drv._find_image_in_cache('image_id')
        if not result:
            pass
        else:
            self.fail('Return result is unexpected')
# ---
def plot(files, fac=1.0):
    for f in files:
        if f.split('.')[-1] == 'xy':
            td = np.loadtxt(f)
            plt.plot(td[:, 0], np.log(1. / td[:, 1]) * fac, label=f)
        elif f.split('.')[-1] == 'spc':
            td = SPC(f)
            plt.plot(td.xdata, np.log(1. / np.array(td.ydata)), label=f)
    plt.legend()
    plt.show()
# ---
def do_blank_image(height, width, filename, color="black"):
  command = "convert -size %dx%d xc:%s %s" % (width, height, color, filename)

  ret = subprocess.call(command, shell=True)

  if ret != 0:
    raise Exception("Command failed: "+ command)
# ---
def split(self, axis: AxisSelector, new_axes: Sequence[Axis]) -> Sequence["NamedArray"]:  # pragma: no cover
        return haliax.split(self, axis=axis, new_axes=new_axes)
# ---
def concat_axes(a1: Sequence[Axis], a2: AxisSpec) -> tuple[Axis, ...]:
    pass
# ---
def sum_Odd(n): 
    terms = (n + 1)//2
    sum1 = terms * terms 
    return sum1  
def sum_in_Range(l,r): 
    return sum_Odd(r) - sum_Odd(l - 1)
# ---
def tryLoad(self, fs, node_id):
        """
        @type fs: EnkfFS
        @type node_id: NodeId
        @rtype: bool
        """
        assert isinstance(fs, EnkfFs)
        assert isinstance(node_id, NodeId)

        return EnkfNode.cNamespace().try_load(self, fs, node_id)
# ---
def calculate_rank_name(self, rank_points):
        index = 0

        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:
            if(self.rank_to_pos.index(k) > index):
                index = self.rank_to_pos.index(k)

        return self.rank_to_pos[index]
# ---
def size(self):
		if hasattr(self.file, "size"):
			return self.file.size()
		elif isinstance(self.file, file):
			from os.path import getsize
			return getsize(self.file.name)
		raise NotImplementedError
# ---
def genetic_modification_9(lab, award, human_donor_1):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'donor': human_donor_1['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'method': 'transient transfection'
    }
# ---
def year(self):
        program = self.program()
        return program.start_date.year if program else None
# ---
def test_setup_failure_does_not_kill_capturing(testdir):
    sub1 = testdir.mkpydir("sub1")
    sub1.join("conftest.py").write(_pytest._code.Source("""
        def pytest_runtest_setup(item):
            raise ValueError(42)
    """))
    sub1.join("test_mod.py").write("def test_func1(): pass")
    result = testdir.runpytest(testdir.tmpdir, '--traceconfig')
    result.stdout.fnmatch_lines([
        "*ValueError(42)*",
        "*1 error*"
    ])
# ---
def test_make_choices_real_probs(random_seed, utilities):
    probs = mnl.utils_to_probs(utilities)
    choices = mnl.make_choices(probs)

    pdt.assert_series_equal(
        choices,
        pd.Series([1, 2], index=[0, 1]))
# ---
def eval_batch_size(self):
        return self.per_device_eval_parallelism * self.data_axis_size
# ---
def visitdir(self, dir):
        r = False
        for m in self._matchers:
            v = m.visitdir(dir)
            if v == "all":
                return v
            r |= v
        return r
# ---
def to(self, device: torch.device) -> None:
        for step in self:
            self.example_by_step[step] = (
                self[step][0].to(device, non_blocking=True),
                self[step][1].to(device, non_blocking=True),
            )
# ---
def __init__(self, mapping: dict[str, str]):
        self.mapping = mapping
# ---
def __eq__(self, other: object) -> CompareExpr:  # type: ignore[override]
        return CompareExpr(self, _to_expr(other), "eq")
# ---
def flops_per_token(self, vocab_size: int, seq_len: int) -> float:
        """Return FLOPs per token for this model configuration."""
        ...
# ---
def check_vm_params_for_linux(self):
        self.assertEquals(self.vm['platform']['nx'], 'false')
        self.assertEquals(self.vm['PV_args'], '')
        self.assertEquals(self.vm['PV_bootloader'], 'pygrub')

        # check that these are not set
        self.assertEquals(self.vm['PV_kernel'], '')
        self.assertEquals(self.vm['PV_ramdisk'], '')
        self.assertEquals(self.vm['HVM_boot_params'], {})
        self.assertEquals(self.vm['HVM_boot_policy'], '')
# ---
def dict_to_str(d):
    """
    Given a dictionary d, return a string with 
    each entry in the form 'key: value' and entries
    separated by newlines.
    """
    vals = []
    for k in d.keys():
        vals.append('{}: {}'.format(k, d[k]))
    v = '\n'.join(vals)
    return v
# ---
def get_chunk(in_key, config):
    """Read a chunk from the named array"""
    name = in_key[0]
    in_coords = in_key[1:]
    arr = config.reads_map[name].open()
    selection = key_to_slices(in_coords, arr)
    arg = arr[selection]
    arg = numpy_array_to_backend_array(arg)
    return arg
# ---
def dev_shards(self):
    raise NotImplementedError()
# ---
def num_bytes(model: PyTree):
    # especially with jax.vjp, we get duplicate arrays and want to uniq them
    # NB we need to use object identity here, mostly because of ShapedDtypeStruct
    leaves = {id(x): x for x in jax.tree_util.tree_leaves(model) if is_jax_array_like(x)}
    return sum(x.nbytes for x in leaves.values())
# ---
def __init__(self, parent: OpeningOfGroup, initial: bool=False):
        self.parent = parent
        self.is_initial = initial
        self.limited_prev = parent if initial else self
        self.quantified = ContentOfGroup.NotQuantified

        # forward of function
        self.add = self.parent.add
# ---
def build_eval_harness_config(self) -> LmEvalHarnessConfig:
        if self.eval_harness_tasks is None:
            return None
        return LmEvalHarnessConfig(task_spec=convert_to_levanter_task_config(self.eval_harness_tasks))
# ---
def get_lock_file_hash(lock_path):
    """Get a hash of the lock file for cache invalidation."""
    try:
        mtime = os.path.getmtime(lock_path)
        return hashlib.md5(f"{lock_path}:{mtime}".encode()).hexdigest()[:12]
    except OSError:
        return None
# ---
def can_access_delivery_email(user_profile: UserProfile) -> bool:
    realm = user_profile.realm
    if realm.email_address_visibility == Realm.EMAIL_ADDRESS_VISIBILITY_ADMINS:
        return user_profile.is_realm_admin

    if realm.email_address_visibility == Realm.EMAIL_ADDRESS_VISIBILITY_MODERATORS:
        return user_profile.is_realm_admin or user_profile.is_moderator

    return False
# ---
def device_is_address(self):
        if is_valid_ip(self.device):
            return True
        return False
# ---
def footnotes(self, text):
        """Wrapper for all footnotes.

        :param text: contents of all footnotes.
        """
        html = '<div class="footnotes">\n%s<ol>%s</ol>\n</div>\n'
        return html % (self.hrule(), text)
# ---
def block_id_to_offset(block_id: Tuple[int, ...], numblocks: Tuple[int, ...]) -> int:
    """Convert a block ID (chunk coordinates) to an index offset."""
    return int(np.ravel_multi_index(block_id, numblocks))
# ---
def save_file(self):
        if self.filename is not None:
            config_string = self.rootnode.dump()
            with open(self.filename, 'w') as f:
                f.write(config_string)
            self.dirty = False
        else:
            self.show_savecfg_dlg()
# ---
def _str_hash_legacy(s: str) -> str:
    return hashlib.blake2b(s.encode(), digest_size=8).hexdigest()
# ---
def execute(conn, clauseelement, multiparams,
                                                    params ):
            stmts.append((str(clauseelement), params, multiparams))
# ---
def author():
    return make_author()
# ---
def _inventory_line_hook(self, cr, uid, inventory_line, move_vals):
        """ Creates a stock move from an inventory line
        @param inventory_line:
        @param move_vals:
        @return:
        """
        return self.pool.get('stock.move').create(cr, uid, move_vals)
# ---
def __init__(self, name, description, pluginClass,
				 pluginStartForm, pluginStartMethod,
				 pluginEditForm=None, pluginEditMethod=None):
		self.name=name #No Spaces please...
		self.description=description
		self.plugin=pluginClass
		self.manage_addForm=pluginStartForm
		self.manage_addMethod=pluginStartMethod
		self.manage_editForm=pluginEditForm
		self.manage_editMethod=pluginEditMethod
# ---
def max_sum_increasing_subseq(a, n, index, k):
	dp = [[0 for i in range(n)] 
			for i in range(n)]
	for i in range(n):
		if a[i] > a[0]:
			dp[0][i] = a[i] + a[0]
		else:
			dp[0][i] = a[i]
	for i in range(1, n):
		for j in range(n):
			if a[j] > a[i] and j > i:
				if dp[i - 1][i] + a[j] > dp[i - 1][j]:
					dp[i][j] = dp[i - 1][i] + a[j]
				else:
					dp[i][j] = dp[i - 1][j]
			else:
				dp[i][j] = dp[i - 1][j]
	return dp[index][k]
# ---
def translate_inverse(unity):
    for key, value in UNITIES.items():
        if unity == value:
            return key
    else:
        return u"NONE"
# ---
def remove_negs(num_list): 
    for item in num_list: 
        if item < 0: 
           num_list.remove(item) 
    return num_list
# ---
def real(x, /):
    if x.dtype == complex64:
        dtype = float32
    elif x.dtype == complex128:
        dtype = float64
    else:
        raise TypeError("Only complex floating-point dtypes are allowed in real")
    return elemwise(nxp.real, x, dtype=dtype)
# ---
def maybe_untuple(x: Sequence[T] | T) -> T | Sequence[T]:
    """
    If x is a tuple with one element, return that element. Otherwise return x.
    """
    if isinstance(x, tuple) and len(x) == 1:
        return x[0]
    return x
# ---
def test_dense_constraints(self):
    k_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = keras.layers.Dense(
        3, kernel_constraint=k_constraint, bias_constraint=b_constraint)
    layer(keras.backend.variable(np.ones((2, 4))))
    self.assertEqual(layer.kernel.constraint, k_constraint)
    self.assertEqual(layer.bias.constraint, b_constraint)
# ---
def create_integration_entrypoint():
    """Create a simple test entrypoint for integration tests."""

    def test_fn():
        print("Hello from test task!")
        return 42

    return Entrypoint.from_callable(test_fn)
# ---
def policy_encode(policy: jmp.Policy):
        def name(dtype):
            if hasattr(dtype, "name"):
                return dtype.name
            elif hasattr(dtype, "dtype"):
                return name(dtype.dtype)

        out = f"compute={name(policy.compute_dtype)},params={name(policy.param_dtype)},output={name(policy.output_dtype)}"
        assert jmp.get_policy(out) == policy
        return out
# ---
def main(config: Config):
        assert "wikitext" in config.data.components
        comp = config.data.components["wikitext"]
        assert isinstance(comp.source, UrlDatasetSourceConfig)
        assert comp.cache_dir == "gs://levanter-data/tokenized/wikitext"
# ---
def process():
    """ Process submitted form data. """
    format = request.form['format']

    try:
        node = {
            'png': 'map_png',
            'svg': 'map_svg',
            'jpg': 'map_jpg',
        }[format]
    except KeyError:
        flash("The output format you selected is not supported.")
        return redirect(url_for('error'))
    else:
        return redirect(url_for(node, _method='POST'), code=307)
# ---
def ssh_head(ctx, extra_args):
    """SSH to cluster head node using ray attach."""
    cmd_args = _maybe_add_ray_verbose(ctx.obj, ["ray", "attach", ctx.obj.config_file])
    if extra_args:
        cmd_args.extend(["--", *extra_args])
    subprocess.run(cmd_args, check=True)
# ---
def test_async_handler(self):
        @event.event(enable=False)
        async def on_async_test(self):
            pass

        assert hasattr(on_async_test, '_h_info')
        h_info = on_async_test._h_info
        assert h_info.event_name == 'async_test'
        assert h_info.handler is on_async_test
        assert h_info.priority is event.Priority.DEFAULT
        assert not h_info.should_enable
        assert h_info.is_async
# ---
def embed(self, input_ids: NamedArray):
        """
        Args:
            input_ids: token IDs with shape > {Vocab}
        """
        input_embeds = self.weight.take(self.Vocab, input_ids)
        return input_embeds * self.reparam.active_scale
# ---
def dofilter (theimage,thefilter):
   lastfilter = thefilter
   global image1
   image1 =  image1.filter(thefilter)
   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def __call__(
        self,
        params: PyTree,
        tokens: jax.Array,
        cfg: GrugConfigLike,
        *,
        mask: AttentionMask | jax.Array | None = None,
    ) -> jax.Array: ...
# ---
import re
def remove_extra_char(text1):
  pattern = re.compile('[\W_]+')
  return (pattern.sub('', text1))
# ---
def test_as_remote_kwargs_tpu():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=TpuConfig(variant="v4-32"))
    kwargs = as_remote_kwargs(config)
    assert kwargs["num_cpus"] == 8
# ---
def armstrong_number(number):
 sum = 0
 times = 0
 temp = number
 while temp > 0:
           times = times + 1
           temp = temp // 10
 temp = number
 while temp > 0:
           reminder = temp % 10
           sum = sum + (reminder ** times)
           temp //= 10
 if number == sum:
           return True
 else:
           return False
# ---
def validation_step(self, batch, batch_idx):
    return self._compute_loss(batch, prefix='val')
# ---
def test_list_type_access_private(self):
        expected = {'volume_type_access': [
            {'volume_type_id': fake.VOLUME_TYPE3_ID,
             'project_id': PROJ2_UUID},
            {'volume_type_id': fake.VOLUME_TYPE3_ID,
             'project_id': PROJ3_UUID}]}
        result = self.type_access_controller.index(self.req,
                                                   fake.VOLUME_TYPE3_ID)
        self.assertEqual(expected, result)
# ---
def _slice_has_active_workers(self, slice_obj: VmGroupProtocol, vm_status_map: VmWorkerStatusMap) -> bool:
        """Check if any worker in a slice has running tasks (lookup by VM address)."""
        for vm in slice_obj.vms():
            vm_address = vm.info.address
            if not vm_address:
                continue
            status = vm_status_map.get(vm_address)
            if status is not None and not status.is_idle:
                return True
        return False
# ---
def get_y_from_x(self, x):
        if self.b == 0:
            return 0.0

        return 1.0 * (-self.c - self.a * x) / self.b
# ---
def test_bound_in_two_tuple(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.y).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(
            stmt, [(2,), (3,), (4,)], params={"q": [(2, 3), (3, 4), (4, 5)]}
        )
# ---
def format_accelerator_display(accel_type: int, variant: str = "") -> str:
    """Format accelerator type and variant for display.

    Examples:
        format_accelerator_display(3, "v5litepod-16") -> "tpu (v5litepod-16)"
        format_accelerator_display(2, "A100") -> "gpu (A100)"
        format_accelerator_display(1, "") -> "cpu"
    """
    friendly = accelerator_type_friendly(accel_type)
    if variant:
        return f"{friendly} ({variant})"
    return friendly
# ---
def define(self, dimensions):
                return {d: ('middle', 2, 2) for d in dimensions}
# ---
def hidden_dim(self) -> int:
        return self.core.hidden_dim
# ---
def fn(x):
        x = hax.shard(x)
        visualize_shardings(x)
        return x
# ---
def mean(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.mean(self, axis=axis, dtype=dtype, where=where)
# ---
def get_index(self, obj: T) -> int:
        return self._obj_to_index[obj]
# ---
def resolve_axis(axis_spec: AxisSpec, axis_selection: AxisSelection) -> AxisSpec: ...
# ---
def predict(self, documents: list[str]):
        return self.model.predict(documents, k=self.k)
# ---
def custom_fn(pred):
        pred_embeddings, pred_lm_head = pred
        loss = fused_cross_entropy_loss_and_logsumexp_penalty(
            pred_embeddings,
            pred_lm_head,
            Contract=Embed,
            Label=Vocab,
            target_y=true_ids,
            reduction=None,
            logsumexp_weight=0.5,
            block_size=block_size,
            dtype=pred_embeddings.dtype,
        )

        return loss.mean().scalar()
# ---
def __hash__(self):
        return hash(('not', hash(self.subquery)))
# ---
def remove_words(list1, removewords):
    for word in list(list1):
        if word in removewords:
            list1.remove(word)
    return list1
# ---
def __call__(self, params: PyTree) -> jax.Array: ...
# ---
def add(self, other):
        assert isinstance(self.ast, ast.CharClass)
        self.ast.elems = self.ast.elems + (other,)
# ---
def assemblyStatus(self, percent):
        self.status.assembly = True
        self.status.assembly_percent = percent
# ---
def reload(self) -> str:
        return self.restart()
# ---
def __ne__(self, other):
        if not isinstance(other, MultiCoverage):
            return NotImplemented
        return not self.__eq__(other)
# ---
import sys 
def solve(a,n):   
    mx = -sys.maxsize - 1
    for j in range(1,n):  
        if (mx > a[j]):  
            return False  
        mx = max(mx,a[j - 1])    
    return True
# ---
def list_instances(self, project: str, zone: str) -> list[dict]:
        return self._instances
# ---
def translate_unity(unity):
    return UNITIES.get(unity, UNITIES["NONE"])
# ---
def _fetch_all_from_queue(q: queue.Queue, timeout: float) -> List:
    """Fetch all items from `q` which arrive within `timeout` seconds."""
    deadline = time.time() + timeout
    items = []
    while time.time() < deadline:
        try:
            item = q.get(timeout=max(0, deadline - time.time()))
            items.append(item)
        except queue.Empty:
            break
    return items
# ---
def _start_process(name: str, cmd: list[str], env: dict[str, str]) -> ManagedProcess:
    printable = " ".join(cmd)
    print(f"Starting {name}: {printable}")
    proc = subprocess.Popen(cmd, cwd=ROOT, env=env)
    return ManagedProcess(name, proc)
# ---
def __eq__(self, other):
        """Equality."""
        return isinstance(other, EventType) and other.verb == self.verb
# ---
def test_get_type(self):
        self.assertEqual("http", self.get.get_type())
# ---
def __init__(self, cluster: Cluster, status_file: StatusFile):
        self.cluster = cluster
        self._status_file = status_file
        self._job_id: JobId | None = None
        self._heartbeat_thread: Thread | None = None
        self._stop_event = Event()
# ---
def sort_sublists(list1):
      list1.sort()  
      list1.sort(key=len)
      return  list1
# ---
def convert_td(self, el, text, convert_as_inline):
        if convert_as_inline:
            return text + " "
        colspan = 1
        if "colspan" in el.attrs:
            colspan = _try_convert_int(el["colspan"], 1)

        return " " + text.strip().replace("\n", " ") + " |" * colspan
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n)})
            df['B'] = df.A.map(lambda a: 2 * a)
            return df.B.sum()
# ---
def _get_step(path: str) -> str:
        # make sure it looks like step-{train_step_number}
        g = re.match(r"step-(\d+)/?$", path)
        if g is None:
            raise ValueError(f"Invalid path: {path}")

        return g.group(1)
# ---
def __rpow__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.power(other, self)
# ---
def _check_plotfunc_output(func, da, framedim="time", **kwargs):
    timestep = 0
    timestep_value = da[framedim].data[timestep]
    fig = plt.figure()
    oargs = func(da, fig, timestep, timestep_value, framedim, **kwargs)
    # I just want the number of output args, delete plot
    plt.close(fig)
    if oargs is None:
        return 0
    else:
        return len(oargs)
# ---
def find_free(_: jnp.ndarray):
            free_flags = ~self.used_mask
            maybe = hax.argmax(free_flags, "seq").scalar()
            available = (~self.used_mask["seq", maybe]).scalar()
            return hax.where(available, maybe, INVALID)
# ---
def check_plateau_status(trajectory: np.ndarray, window: int = 100) -> bool:
    """Check if trajectory is plateaued using curriculum logic."""
    stats = LessonStats(
        training_stats=PerformanceStats(
            total_samples=len(trajectory), reward_history=trajectory, last_update_step=len(trajectory)
        )
    )
    return is_plateaued(stats, window=window, threshold=0.01)
# ---
def num_position_tokens(self) -> int:
        return self.max_seq_len
# ---
def nth_items(list,n):
 return list[::n]
# ---
def vm(ctx):
    """VM management commands (via controller RPC)."""
    pass
# ---
def test_select_columns(self):
        with config.db.connect() as conn:
            res = conn.execute(
                select(
                    [self.tables.square.c.area, self.tables.square.c.perimeter]
                )
                .select_from(self.tables.square)
                .order_by(self.tables.square.c.id)
            ).fetchall()
            eq_(res, [(100, 40), (1764, 168)])
# ---
def __init__(self, field):
		self.field = field
# ---
def read_all_remote(self):
        '''
        Return a dict of all remote key data
        '''
        data = {}
        for status, mids in six.iteritems(self.list_keys()):
            for mid in mids:
                keydata = self.read_remote(mid, status)
                if keydata:
                    keydata['acceptance'] = status
                    data[mid] = keydata

        return data
# ---
def _fully_replicated_sharding(mesh):
    return hax.partitioning.sharding_for_axis((), {}, mesh)
# ---
def test_relu_squared_scalar(use_jit):
    f = hax.nn.relu_squared
    if use_jit:
        f = jax.jit(f)

    x = 5.0
    expected = 25.0
    actual = f(x)
    assert jnp.allclose(actual, expected)

    x_neg = -5.0
    expected_neg = 0.0
    actual_neg = f(x_neg)
    assert jnp.allclose(actual_neg, expected_neg)
# ---
def __init__(self, default_factory=lambda: None):
            self.__factory = default_factory
# ---
def on_fuzzing_range_start_changed(self, value: int):
        self.ui.sBAddRangeEnd.setMinimum(value)
        self.ui.sBAddRangeStep.setMaximum(self.ui.sBAddRangeEnd.value() - value)
# ---
def get_words_on_cluster(self, cluster):
        return self.vocab[self.clusters == cluster]
# ---
def test_select_partial_columns(backend):
    """Test select with columns that don't exist in all records."""
    ds = Dataset.from_list(
        [
            {"id": 1, "name": "alice"},
            {"id": 2, "name": "bob", "score": 60},
        ]
    ).select("id", "score")

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert results[0] == {"id": 1}
    assert results[1] == {"id": 2, "score": 60}
# ---
def is_leaf(x):
                return eqx.is_array(x) or isinstance(x, eqx.Module) or haliax.is_named_array(x)
# ---
def merge(lst):  
    return [list(ele) for ele in list(zip(*lst))]
# ---
def _stop_server() -> None:
            logger.debug("Signaling server %s to exit", name)
            server.should_exit = True
# ---
def test_map_overlap_1d_single_chunk():
    x = np.arange(6)
    a = xp.asarray(x, chunks=(6,))

    b = cubed.map_overlap(
        lambda x: x,
        a,
        dtype=a.dtype,
        chunks=((8,),),
        depth=1,
        boundary=0,
        trim=False,
    )

    assert_array_equal(b.compute(), np.array([0, 0, 1, 2, 3, 4, 5, 0]))
# ---
def abs(self, f):
        return self._matcher.abs(self._path + "/" + f)
# ---


def unique(l: list):
    """Return sorted unique elements in a list
    >>> unique([5, 3, 5, 2, 3, 3, 9, 0, 123])
    [0, 2, 3, 5, 9, 123]
    """
    return sorted(list(set(l)))
# ---
def testFunctionDefGenerator(self):
    self.assertEqual((0, "['foo', 'bar']\n"), _GrumpRun(textwrap.dedent("""\
        def gen():
          yield 'foo'
          yield 'bar'
        print list(gen())""")))
# ---
def port(self):
        """Get the port the server is running on."""
        if self.config.port > 0:
            return self.config.port

        # query the uvicorn server socket list for the port
        for server in self._server.servers:
            for sock in server.sockets:
                addr = sock.getsockname()
                return addr[1]

        return None
# ---
def copy_page(self, src_page: int, dst_page: int) -> "KvPageCache":
        """Copy the entire contents of page ``src_page`` into ``dst_page``.

        This is used when creating clones that should have an identical last partial page, but mapped to a fresh page.
        """
        new_k = self.kv_pages.at["page", dst_page].set(self.kv_pages["page", src_page])
        return dataclasses.replace(self, kv_pages=new_k)
# ---
def _maybe_add_ray_verbose(ctx: Context, cmd_args: list[str]) -> list[str]:
    """Add `-v` to Ray CLI commands when cluster.py verbose mode is enabled.

    Most Ray CLI invocations here are of the form `["ray", "<subcommand>", ...]`.
    We insert `-v` after the subcommand (e.g. `ray up -v ...`) since Ray exposes per-subcommand verbose flags.
    """
    if ctx.verbose:
        if len(cmd_args) < 2:
            return cmd_args
        return [*cmd_args[:2], "-v", *cmd_args[2:]]
    return cmd_args
# ---
def get_init_log(self, vm_id: str, tail: int | None = None) -> str:
        """Get initialization log for a VM."""
        ...
# ---
def absolute(a: A) -> A:
    return wrap_elemwise_unary(jnp.absolute, a)
# ---
def wake(self) -> None: ...
# ---
def __init__(self, shortname, loader):
        # Not preloaded
        # loaders must produce dictionaries (or an appropriate iterable)
        # with the required keys.
        # The reason for this is that code for certain servers need not be loaded
        # if it's not going to be used at all
        # It also prevents import loop collisions.
        global __ServerImplementationDict
        self.__data = ServerImplementationDict(loader)
        self.__shortname = shortname
# ---
def available(self) -> bool:
        """Return if entity is available."""
        return self.coordinator.last_update_success and self.coordinator.data["printer"]
# ---
def test_full_end_to_end_cache_with_groups():
    td = tempfile.TemporaryDirectory()
    with td as tmpdir:
        cache = build_or_load_cache(
            tmpdir,
            SimpleShardSource(num_shards=5),
            TestProcessor(),
        )

        expected = simple_process(TestProcessor(), SimpleShardSource(num_shards=5))

        all_data = cache[:]

        check_datasets_equal(all_data, expected)
# ---
def test_registered(self):
        eq_(self.record.registered, False)
# ---
def find_chainlink_dir() -> Path | None:
    """Find the .chainlink directory by walking up from cwd."""
    current = Path.cwd()
    for _ in range(10):
        candidate = current / '.chainlink'
        if candidate.is_dir():
            return candidate
        parent = current.parent
        if parent == current:
            break
        current = parent
    return None
# ---
def logical_or(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.logical_or(x1, x2)
# ---
def effective_principals(self, request):
		creds = [Everyone]
		params = _parse_authorization(request, self.secret, self.realm)
		if params is None:
			return creds
		if not _is_valid_nonce(params['nonce'], self.secret):
			_add_www_authenticate(request, self.secret, self.realm)
			return creds
		groups = self.callback(params, request)
		if groups is None:
			return creds
		creds.append(Authenticated)
		creds.append('u:%s' % params['username'])
		creds.extend(groups)
		return creds
# ---
def create(self, rsc, **kw):
        if isinstance(rsc, string_types):
            cls = getattr(resource, rsc.capitalize())
            rsc = cls(kw, self)
        return rsc.save()
# ---
def gradient_magnitude(
    tensor: torch.Tensor, dim: tuple[int, ...] = (-2, -1)
) -> torch.Tensor:
    """Compute the magnitude of gradient across the specified dimensions."""
    gradients = torch.gradient(tensor, dim=dim)
    return torch.sqrt(sum([g**2 for g in gradients]))
# ---
def is_gentarget(self, target):
    return isinstance(target, GoThriftLibrary)
# ---
def get_directory_friendly_name(name: str) -> str:
    """Convert a huggingface repo name to a directory friendly name."""
    return name.replace("/", "--").replace(".", "-").replace("#", "-")
# ---
def test_delete_existing_snapshot(self):
        drv = self._driver
        mox = self._prepare_delete_snapshot_mock(True)

        drv.delete_snapshot(FakeSnapshot())

        mox.VerifyAll()
# ---
def test_reset_on_high_resolution_enable(self):
        self.assertEqual(self.reset_count, 0)

        self.mda.io_write_byte(0x3B8, 0x01)
        self.assertEqual(self.reset_count, 1)

        # Second write shouldn't call reset again.
        self.mda.io_write_byte(0x3B8, 0x01)
        self.assertEqual(self.reset_count, 1)
# ---
def prepared_registry(self):
        ''' prepared_registry property '''
        if not self.__prepared_registry:
            results = self.prepare_registry()
            if not results or ('returncode' in results and results['returncode'] != 0):
                raise RegistryException('Could not perform registry preparation. {}'.format(results))
            self.__prepared_registry = results

        return self.__prepared_registry
# ---
def get_version():
    main_ns = {}
    version_path = convert_path('gpyfft/version.py')
    with open(version_path) as version_file:
        exec(version_file.read(), main_ns)
    version = main_ns['__version__']
    return version
# ---
def __init__(self, config: ReplayConfig):
        self.config = config
# ---
def save_design_only_structure_to_cif(atom_design_mask, structure, output_path: Path):
    design_atom_indices = torch.where(atom_design_mask)[0].cpu().numpy()
    design_only_str = Structure.extract_atoms(
        structure, design_atom_indices, res_reindex=True
    )
    cif_text = to_mmcif(design_only_str)
    output_path.write_text(cif_text)
    return cif_text
# ---
def validate_has_variants(self):
		if not self.has_variants and frappe.db.get_value("Item", self.name, "has_variants"):
			if frappe.db.exists("Item", {"variant_of": self.name}):
				frappe.throw(_("Item has variants."))
# ---
def _batch_sizes() -> dict[str, int]:
    return {"130m": 128, "300m": 128, "520m": 128, "1_2b": 256}
# ---
def tree_flatten(self) -> Any:
        return ((self.array,), self.main_axes)
# ---
def test_init_capturing(self):
        capouter = StdCaptureFD()
        try:
            capman = CaptureManager("fd")
            capman.start_global_capturing()
            pytest.raises(AssertionError, "capman.start_global_capturing()")
            capman.stop_global_capturing()
        finally:
            capouter.stop_capturing()
# ---
def inherit_docs(cls):
    for name, func in vars(cls).items():
        if not func.__doc__:
            for parent in cls.__bases__:
                try:
                    parfunc = getattr(parent, name)
                except AttributeError: # parent doesn't have function
                    break
                if parfunc and getattr(parfunc, '__doc__', None):
                    func.__doc__ = parfunc.__doc__
                    break
    return cls
# ---
def cmd_modules(self, argument):
        """List active modules"""
        index = self.bot.help_index
        output = "active modules   -- %s" % " ".join(index['modules'].keys())
        self.send(self.target, output)
# ---
def area_trapezium(base1,base2,height):
 area = 0.5 * (base1 + base2) * height
 return area
# ---
def __truediv__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "truediv")
# ---
def _cleanup_all_iris_containers(self) -> None:
        """Remove all iris-managed containers at startup.

        This handles crash recovery cleanly without tracking complexity.
        """
        removed = self._runtime.remove_all_iris_containers()
        if removed > 0:
            logger.info("Startup cleanup: removed %d iris containers", removed)
# ---
def filter_steps(self, enabled_steps: List[str]):
        self.steps = [s for s in self.steps if s.name in enabled_steps]
# ---
def f(s):
        spc = player.get_action_space()
        act = func([[s]])[0][0].argmax()
        if random.random() < 0.001:
            act = spc.sample()
        if verbose:
            print(act)
        return act
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})
            Ac = df.A.cumsum()
            return Ac.sum()
# ---
def build(self, axis: AxisSpec) -> LayerNorm:
        return LayerNorm.init(axis, eps=self.eps, use_weight=self.use_weight, use_bias=self.use_bias)
# ---
def _roll_and_update(amax_h, update):
        return jnp.roll(amax_h, shift=-1, axis=0).at[0].set(update)
# ---
from itertools import combinations 
def find_combinations(test_list):
  res = [(b1 + a1, b2 + a2) for (a1, a2), (b1, b2) in combinations(test_list, 2)]
  return (res)
# ---
def _get_ha_binding(self, context, router_id):
        with context.session.begin(subtransactions=True):
            query = context.session.query(ha_db.RouterHASetting)
            query = query.filter(
                ha_db.RouterHASetting.router_id == router_id)
            return query.first()
# ---
def test_trainer__mini_2step(trainer_pair: TrainPair, caplog):
    caplog.set_level(logging.INFO)
    _, trainer = trainer_pair

    trainer.run()
# ---
def forward(
        self,
        z: Tensor,
        pair_mask: Tensor,
        chunk_size_tri_attn: int = None,
        use_kernels: bool = False,
    ) -> Tensor:
        # Compute pairwise stack
        dropout = get_dropout_mask_rowise(self.dropout, z, self.training)
        z = z + dropout * self.triangular(z, mask=pair_mask)
        z = z + self.transition_z(z)

        # Post-LN
        z = self.z_post_norm(z)
        return z
# ---
def visit_UnaryOp(self, node: ast.UnaryOp) -> ast.UnaryOp:
        node.op = self._maybe_swap(node.op, _UNARY_OPS)
        self.generic_visit(node)
        return node
# ---
def update_linear(layer: Linear, grad_layer: Linear) -> Linear:
        lr_scale = layer.reparam.lr_scale
        new_weight = layer.weight - (base_lr * lr_scale) * grad_layer.weight
        if layer.bias is None or grad_layer.bias is None:
            new_bias = layer.bias
        else:
            new_bias = layer.bias - base_lr * grad_layer.bias
        return dataclasses.replace(layer, weight=new_weight, bias=new_bias)
# ---
def barrier():  # Wait
    compss_barrier()
# ---
def _raw_indices_for_at(array, indexes):
    sliced_axes, ordered_slices = _compute_new_axes_and_slices_for_index(array, indexes)
    _sliced = index(array, indexes)
    ordered_slices = [s.array if isinstance(s, NamedArray) else s for s in ordered_slices]
    return ordered_slices, _sliced.axes
# ---
def convert_to_cache(self, value, record, validate=True):
        if value is None or value is False:
            return False
        return ustr(value)[:self.size]
# ---
def _batchify_ctor(ctor, batch_dims):
        # this is gross but it basically just vmaps the ctor over each batch dimension
        return functools.reduce(lambda ctor, batch_axis: vmap(ctor, batch_axis), reversed(batch_dims), ctor)
# ---
def volts(self):
        """ADC voltages presented as a list"""
        return self._volts
# ---
def empty_page_cache(self, spec: PageTableSpec, *, dtype) -> "KvPageCache":
        return KvPageCache.init(spec, self.config.KVHeads, self.config.HeadSize, dtype=dtype)
# ---
def on_compute_end(self, event):
        events_df = pd.DataFrame(self.events)
        plan_df = pd.DataFrame(self.plan)
        fig = generate_mem_usage(events_df, plan_df)

        self.dst = Path(f"history/{event.compute_id}")
        self.dst.mkdir(parents=True, exist_ok=True)
        self.dst = self.dst / f"memory.{self.format}"

        fig.savefig(self.dst)
# ---
def getUserIdsWorker(login, pwd, qin, qout):
	while True:
		url = qin.get()
		if url == None:
			break
		qin.task_done()
		try:
			dom = getAtomFeed(url, login, pwd)
		except:
			continue
		userIds = dom.getElementsByTagName('snx:userid')
		for index, item, in enumerate(userIds):
			qout.put(item.firstChild.data)
# ---
def init_fn(key):
        return hax.nn.MLP.init(In, Out, 2, 3, key=key)
# ---
def __call__(self, L):
        """Get positional embeddings for the first L positions.

        Args:
            L: Length to get embeddings for

        Returns:
            Tuple of (z, t) embeddings limited to length L
        """
        if L > self.PosPerBlock.size:
            raise ValueError(f"Requested length {L} > max size {self.PosPerBlock.size}")

        return self.z.slice(self.PosPerBlock, length=L), self.t.slice(self.PosPerBlock, length=L)
# ---
def test_reads_dask_dataframe(self):
    fc = ee.FeatureCollection('WRI/GPPD/power_plants')
    df = dask_ee.read_ee(fc)

    head = df.head()
    columns = df.columns

    self.assertIsNotNone(df)
    self.assertIsNotNone(head)
    self.assertIsInstance(df, dd.DataFrame)
    self.assertEqual(df.compute().shape, (28_664, 23))

    print(columns)
    print(head)
# ---
def test_str_replace_regex(self):
        def test_impl(df):
            return df.A.str.replace('AB*', 'EE', regex=True)

        df = pd.DataFrame({'A': ['ABCC', 'CABBD']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def _read_ovf_from_zip_ova(ova_path):
    with open(ova_path, 'rb') as fh:
        zf = zipfile.ZipFile(fh)
        name = _find_ovf(zf.namelist())
        if name is not None:
            return zf.read(name)
    raise ClientError('OVA does not contains file with .ovf suffix')
# ---
def address(self) -> str:
        return self.vm_name
# ---
def DATA_STATES(state):
    if state == 'open':
        return YELLOW() + state + ENDC()
    elif state == 'closing':
        return YELLOW() + state + ENDC()
    elif state == 'closed':
        return GREEN() + state + ENDC()
    else:
        return state
# ---
def replace_axis(axis_spec: AxisSpec, old: AxisSelector, new: AxisSpec) -> AxisSpec: ...
# ---
def test_edit_experiment_view_nonexistent(self):
        """Tests edit_experiment when experiment does not exist"""
        e_id = NONEXISTENT_EXPERIMENT_ID
        response = self.client.get(reverse("ab_testing_tool_edit_experiment", args=(e_id,)))
        self.assertTemplateNotUsed(response, "ab_tool/edit_experiment.html")
        self.assertEquals(response.status_code, 404)
# ---
def __init__(self, location: str, config: WeightTransferConfig):
        super().__init__(location)
        self.config = config
        self._weights_store = {}
        self._latest_weight_id = None
        self._lock = threading.Lock()
        self._location = location
# ---
def shutdown(self):
        self.server.should_exit = True
# ---
def ngcd(x,y):
    i=1
    while(i<=x and i<=y):
        if(x%i==0 and y%i == 0):
            gcd=i;
        i+=1
    return gcd;
def num_comm_div(x,y):
  n = ngcd(x,y)
  result = 0
  z = int(n**0.5)
  i = 1
  while(i <= z):
    if(n % i == 0):
      result += 2 
      if(i == n/i):
        result-=1
    i+=1
  return result
# ---
def receive_tally():
    '''
    This is a test route to be able to test that callbacks are correctly sent
    '''
    print("ATTENTION received tally callback: ")
    print(request.get_json(force=True, silent=True))
    return make_response("", 202)
# ---
def step(state: TrainingState, batch: dict[str, jax.Array]):
        (_loss, metrics), grads = jax.value_and_grad(loss_and_metrics, has_aux=True)(state.params, batch)
        updates, new_opt_state = optimizer.update(grads, state.opt_state, state.params)
        new_params = optax.apply_updates(state.params, updates)
        new_state = replace(state, step=state.step + 1, params=new_params, opt_state=new_opt_state)
        return new_state, metrics
# ---
def _soft_sample(self, logits):
    soft_top_k = logits - torch.mean(logits, dim=-1,
                                     keepdim=True)
    return soft_top_k / torch.norm(soft_top_k, dim=-1,
                                   keepdim=True)
# ---
def local_reducer(rows: Iterator[dict], attr_name: str = attr_name, attr_label: str | None = attr_label) -> DDSketch:
        """Build DDSketch from rows in a single shard."""
        sketch = DDSketch()
        for row in rows:
            attributes = row["attributes"]
            value = attributes[attr_name][attr_label] if attr_label else attributes[attr_name]
            sketch.add(value)
        return sketch
# ---
def _poly_axis_from_input(p: NamedArray | ArrayLike, size: int) -> Axis:
    if isinstance(p, NamedArray):
        if p.ndim != 1:
            raise ValueError("Polynomial coefficient arrays must be 1D")
        return p.axes[0].resize(size)
    else:
        return Axis(DEFAULT_POLY_AXIS_NAME, size)
# ---
def get_version():
    return addon.getAddonInfo('version')
# ---
def tearDown(self):
        removef(self.dbname)
        removef(self.bakname)
        super(SQLiteDBChecker, self).tearDown()
# ---
def remove_footnotes(html: BeautifulSoup):
    # Remove footnotes since they are plopped in the middle of the text
    footnotes = html.findAll("div", {"class": "ltx_role_footnote"})
    for fn in footnotes:
        fn.decompose()
# ---
def testFormatTimeZone(self):
    """Tests the _FormatTimeZone function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    zone_string = test_helper._FormatTimeZone(
        event, event_data, event_data_stream)
    self.assertEqual(zone_string, 'UTC')
# ---
def has_soundlibs():
    try:
        import librosa  # noqa F401
        import soundfile  # noqa F401

        return True
    except ImportError:
        return False
# ---
def rectangle_perimeter(l,b):
  perimeter=2*(l+b)
  return perimeter
# ---
def compute_single_distogram_loss(pred, target, mask):
    # Compute the distogram loss
    errors = -1 * torch.sum(
        target * torch.nn.functional.log_softmax(pred, dim=-1),
        dim=-1,
    )
    denom = 1e-5 + torch.sum(mask, dim=(-1, -2))
    mean = errors * mask
    mean = torch.sum(mean, dim=-1)
    mean = mean / denom[..., None]
    batch_loss = torch.sum(mean, dim=-1)
    global_loss = torch.mean(batch_loss)
    return global_loss
# ---
def test_index_chunk_aligned(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = a[0:5000, :]
    run_operation(tmp_path, executor, "index_chunk_aligned", b)
# ---
def safe_name(self) -> str:
        return self.name.replace("/", "-").lower()
# ---
def comment(self, params=None):
        if params is None:
            params = dict()
        params['sale_id'] = self.sale_id
        return Sale(Api.call('sales/create_comment', params))
# ---
def __rrshift__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer", "__rrshift__")
        if other is NotImplemented:
            return other
        return elemwise(
            nxp.bitwise_right_shift, other, self, dtype=result_type(self, other)
        )
# ---
def encode_text(text: str):
    tokens = current_tokenizer.encode(text)
    for token in tokens:
        console.print(f"{token} ", end="")
    console.print()
# ---
def round(a: A, decimals: int = 0) -> A:
    return wrap_elemwise_unary(jnp.round, a, decimals=decimals)
# ---
def candidates_for_budget(
        self,
        budget: float,
        seq_len: int = DEFAULT_SEQ_LEN,
    ) -> Iterator[CandidateConfig]:
        """Yield valid candidate training configs for the given FLOP budget.

        This is the main entry point for generating training configurations.
        Implementations should iterate over model architectures and yield
        complete CandidateConfig objects with model, optimizer, batch size,
        and training steps all configured.
        """
        ...
# ---
def create_process(self, process_id, vpnservice, namespace):
        return ipsec.OpenSwanProcess(
            self.conf,
            process_id,
            vpnservice,
            namespace)
# ---
def named(a, axis: AxisSelection) -> NamedArray:
    """Creates a NamedArray from a numpy array and a list of axes."""
    a = jnp.asarray(a)
    axes = check_shape(a.shape, axis)
    return NamedArray(a, axes)
# ---
def remove(self, item):
        """Remove an element from the queue.

        Parameters
        ----------
        item :
            The element to remove.

        """
        super(PriorityQueue, self).remove(item)
        heapq.heapify(self._queue)
# ---
def Vocab(self):
        return self.model.Vocab
# ---
def revert_seek(self):
        self.command('revert_seek');
# ---
def expired(self) -> bool:
        """Check if deadline has passed."""
        return time.monotonic() >= self._deadline
# ---
def model_type(cls) -> Type["MixtralLMHeadModel"]:
        return MixtralLMHeadModel
# ---
def __invert__(self, /):
        if self.dtype not in _integer_or_boolean_dtypes:
            raise TypeError("Only integer or boolean dtypes are allowed in __invert__")
        return elemwise(nxp.bitwise_invert, self, dtype=self.dtype)
# ---
def rademacher(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.rademacher(key, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def transition_task(state: ControllerState, task: ControllerTask, new_state: int, *, error: str | None = None) -> None:
    """Transition a task to a new state via handle_event."""
    state.handle_event(
        TaskStateChangedEvent(
            task_id=task.task_id,
            new_state=new_state,
            attempt_id=task.current_attempt_id,
            error=error,
        )
    )
# ---
def execute(self):
        with self._volumes(), self._ssh_agent:
            yield self._start_helper()
# ---
def copy_clfftdll_to_package():
    import shutil
    shutil.copy(
        os.path.join(CLFFT_DIR, 'bin', 'clFFT.dll'),
        'gpyfft')

    shutil.copy(
        os.path.join(CLFFT_DIR, 'bin', 'StatTimer.dll'),
        'gpyfft')
    print("copied clFFT.dll, StatTimer.dll")
# ---
def test_param_usage(self):
        @event.event('evt_test', priority=-12, enable=False)
        def on_test(self):
            pass

        assert hasattr(on_test, '_h_info')
        h_info = on_test._h_info
        assert h_info.event_name == 'evt_test'
        assert h_info.handler is on_test
        assert h_info.priority == -12
        assert not h_info.should_enable
        assert not h_info.is_async
# ---
def validation_sets(self) -> Mapping[str, AsyncDataset[np.ndarray]]:
        pass
# ---
def create_router_precommit(self, context, router_context):
        pass
# ---
def state_dict(self):
    return dict(decay=self.decay,
                num_updates=self.num_updates,
                shadow_params=self.shadow_params)
# ---
def peirce_skill_score(self):
        """
        Multiclass Peirce Skill Score (also Hanssen and Kuipers score, True Skill Score)
        """
        n = float(self.table.sum())
        nf = self.table.sum(axis=1)
        no = self.table.sum(axis=0)
        correct = float(self.table.trace())
        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (no * no).sum() / n ** 2)
# ---
def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func
# ---
def get_obj(self, index: int) -> T:
        return self._index_to_obj[index]
# ---
def sinh(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in sinh")
    return elemwise(nxp.sinh, x, dtype=x.dtype)
# ---
def run_func_threads(input, func=None, config=None, name=None, compute_id=None):
    return func(input, config=config)
# ---
def _diag_mask(Ci: Axis, Cj: Axis) -> NamedArray:
    ii = hax.arange(Ci)
    jj = hax.arange(Cj)
    I = ii.broadcast_axis(Cj)
    J = jj.broadcast_axis(Ci)
    return cast(NamedArray, I == J)
# ---
import math  
def next_Perfect_Square(N): 
    nextN = math.floor(math.sqrt(N)) + 1
    return nextN * nextN
# ---
def onCurrentTabChanged(self, index, tabs=None):
        if tabs is None:
            tabs = self.tabs
        widget = tabs.widget(index)
        try:
            signal = widget.shown
        except AttributeError:
            pass
        else:
            signal.emit()
# ---
def test_run_blocking():
    with TemporaryVenv() as venv:
        result = venv.run([venv.python_path, "--version"], capture_output=True, text=True)
        assert result.returncode == 0
        assert "Python" in result.stdout
# ---
def size(self) -> int:
        return len(self.queue)
# ---
def __getitem__(self: T_ChunkedArray, key, /) -> T_ChunkedArray:
        from cubed.core.indexing import index

        return index(self, key)
# ---
def first(self):
        """Get first doc."""
        return self.docs[0] if self.docs else None
# ---
def output_open_html(self):
        text = self.token['text']
        tag = self.token['tag']
        if self._parse_block_html and tag not in _pre_tags:
            text = self.inline(text, rules=self.inline.inline_html_rules)
        extra = self.token.get('extra') or ''
        html = '<%s%s>%s</%s>' % (tag, extra, text, tag)
        return self.renderer.block_html(html)
# ---
def empty(self):
        """Check if the queue is empty.

        Returns
        -------
        bool :
            Whether the queue is empty.

        """
        return len(self._queue) <= 0
# ---
def is_found(hit):
            if 'exists' in hit:
                hit['found'] = hit['exists']
            return hit.get('found', False)
# ---
def forward(
        self, fts: Float[torch.Tensor, "batch channel height width"]
    ) -> Float[torch.Tensor, "batch channel+3 height width"]:
        grid = self.grid.to(fts.device).expand(fts.shape[0], -1, -1, -1)
        return torch.cat((fts, grid), dim=1)
# ---
def sell(self, owner, data,
             size, price=None, plimit=None,
             exectype=None, valid=None):

        order = SellOrder(owner=owner, data=data,
                          size=size, price=price, pricelimit=plimit,
                          exectype=exectype, valid=valid)

        return self.submit(order)
# ---
def grid_points(chunks, sizes):
    cumchunks = [np.cumsum((0,) + c) for c in chunks]
    points = [x * size / x[-1] for x, size in zip(cumchunks, sizes)]
    return points
# ---
def port(self) -> int:
        return self._port
# ---
def get_coords_dict(self):
        return {
            co: self._prognostic_src.data[co] for co in self._prognostic_src.data.coords
        }
# ---
def generator(self, data_dir, tmp_dir, is_training):
    raise NotImplementedError()
# ---
def count_integer(list1):
    ctr = 0
    for i in list1:
        if isinstance(i, int):
            ctr = ctr + 1
    return ctr
# ---
def get(self):
        self.redirect('/posts/last')
# ---
def get_channels(self):
        '''Get all channels used in this Track.
        '''
        post = []
        for event in self.events:
            if event.channel not in post:
                post.append(event.channel)
        return post
# ---
def is_overwrite_with_gradient(v):
        return isinstance(v, OverwriteWithGradient)
# ---
def compute_amax_history(x, amax_history):
    amax_update = jnp.max(jnp.abs(x)).astype(amax_history.dtype)
    new_history = jnp.roll(amax_history, shift=-1, axis=0).at[0].set(amax_update)
    return new_history
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})
            Ac = df.A.shift(1)
            return Ac.sum()
# ---
def get_branches(git_path, module, dest):
    branches = []
    cmd = '%s branch --no-color -a' % (git_path,)
    (rc, out, err) = module.run_command(cmd, cwd=dest)
    if rc != 0:
        module.fail_json(msg="Could not determine branch data - received %s" % out, stdout=out, stderr=err)
    for line in out.split('\n'):
        if line.strip():
            branches.append(line.strip())
    return branches
# ---
def print_ls_desc(desc, **kwargs):
    print(get_ls_desc(desc, **kwargs))
# ---
def configurable_default(cls):
        return BlockingResolver
# ---
def remote(self, *args, **kwargs) -> Any:
        """Call method asynchronously, returning a future compatible with ctx.get()."""
        raise NotImplementedError
# ---
def __call__(self, batch: Sequence[Sequence[int]]) -> Sequence[dict[str, Sequence[int]]]:
        return [{"data": x} for x in batch]
# ---
def __init__(self, size):
        self._data = np.zeros((size,))
        self._capacity = size
        self._size = 0
# ---
def difference(n) :  
    S = (n*(n + 1))//2;  
    res = S*(S-1);  
    return res;
# ---
def __setitem__(self, key, value):
        self._data[id(key)] = [key, value]
# ---
def flatten(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> tuple[Sequence[Any], Any]:
    """Alias for :func:`haliax.tree_util.tree_flatten` matching :func:`jax.tree.flatten`."""

    return tree_util.tree_flatten(tree, is_leaf=is_leaf)
# ---
def _GetNumNonVisibleFiles( file_list ):
  """Returns the number of file in the iterable list of files |file_list| which
  are not curerntly open in visible windows"""
  return len(
      [ f for f in file_list
        if not BufferIsVisible( GetBufferNumberForFilename( f, False ) ) ] )
# ---
def polyval(p: NamedArray | ArrayLike, x: NamedOrNumeric) -> NamedOrNumeric:
    """Named version of [jax.numpy.polyval][].

    When ``x`` is a [haliax.NamedArray][], the returned array reuses ``x``'s axes.
    Otherwise a regular :mod:`jax.numpy` array is returned.
    """

    arr_p, arr_x = unwrap_namedarrays(p, x)
    result = jnp.polyval(arr_p, arr_x)
    if isinstance(x, NamedArray):
        return NamedArray(result, x.axes)
    else:
        return result
# ---
def explicit(mask: NamedArray) -> "AttentionMask":
        return AttentionMask(is_causal=False, causal_offset=None, explicit_mask=mask)
# ---
def clean_text(text: str) -> str:
    """Clean text for display."""
    return text.replace("\n", "\\n").replace("\t", "\\t")
# ---
def _has_type_access(type_id, project_id):
    for access in ACCESS_LIST:
        if access['volume_type_id'] == type_id and \
           access['project_id'] == project_id:
            return True
    return False
# ---
def test_re_raise_passthrough(self):
        """DynamoDBError can re-raise itself if missing original exception"""
        err = DynamoDBError(400, Code="ErrCode", Message="Ouch", args={})
        caught = False
        try:
            err.re_raise()
        except DynamoDBError as e:
            caught = True
            self.assertEqual(err, e)
        self.assertTrue(caught)
# ---
def to_proto(self) -> cluster_pb2.Worker.LogEntry:
        proto = cluster_pb2.Worker.LogEntry(
            source=self.source,
            data=self.data,
        )
        proto.timestamp.CopyFrom(Timestamp.from_seconds(self.timestamp.timestamp()).to_proto())
        return proto
# ---
def newExec(self):
    pass
# ---
def run_streaming(self, command: str) -> MagicMock:
        return make_fake_popen()
# ---
def getFirstGrid(self):
        """
        Implements function to get the first grid.

        :return: the grid.
        """
        li = []
        with open(self.fichier, 'r') as fi:
            for line in fi.readlines():
                li.append(line)
        return li
# ---
def default(v, d):
    return v if exists(v) else d
# ---
def identical(self, other):
        """Like equals, but also checks all dataset attributes and the
        attributes on all variables and coordinates.

        See Also
        --------
        Dataset.broadcast_equals
        Dataset.equals
        """
        try:
            return (utils.dict_equiv(self.attrs, other.attrs)
                    and self._all_compat(other, 'identical'))
        except (TypeError, AttributeError):
            return False
# ---
def last_scale_up_ms(self) -> int:
        """Timestamp of last scale-up operation."""
        return self._last_scale_up.epoch_ms()
# ---
def __lt__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "lt")
# ---
def teardown_class(cls):
        metadata.drop_all()
# ---
def record_logs(logs):
        nonlocal step
        for j, log in enumerate(logs):
            if len(log) > 0:
                if label != "":
                    log = {f"{label}/{k}": v for k, v in log.items()}
                wandb.log(log, step=step + j)
        step += len(logs)
# ---
def get_tracker(name: Literal["wandb"]) -> WandbTracker: ...
# ---
def exit_submission(self):
        "Close the submission and return to the subreddit page"

        self.active = False
# ---
def init_fn(params):
        mu = otu.tree_zeros_like(params, dtype=mu_dtype)  # First moment
        nu = otu.tree_zeros_like(params)  # Second moment
        mog = otu.tree_zeros_like(params, dtype=mu_dtype)  # gradient from
        return ScaleByMarsState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu, mog=mog)
# ---
def _debug(m):
        print >> sys.stderr, m
# ---
def watch(self, model, **kwargs):
        """Watch model parameters and gradients."""
        if self._enabled:
            wandb.watch(model, **kwargs)
# ---
def main():
    cfg = EvalConfig.from_yaml_and_cli()
    cfg.prepare_output_dirs()  # we do this first so logging can use them

    handle_logging(cfg.debug, cfg.experiment.output_dir)
    handle_warnings()

    Evaluator = Eval(cfg)

    try:
        Evaluator.run()
    except Exception as e:
        # Log the exception with traceback
        logger.exception("Evaluation failed with an exception")
        raise e
# ---
def binary_cross_entropy_loss(
    logits: NamedArray,
    targets: NamedArray,
    reduction: ReductionFunction | None | Unspecified = UNSPECIFIED,
    where: NamedArray | None = None,
    weight: NamedArray | None = None,
    reduction_axis: AxisSelection = ...,
) -> NamedArray: ...
# ---
def emptyLayout(layout):
    for i in reversed(range(layout.count())):
        layout.itemAt(i).widget().setParent(None)
# ---
def __init__(self, xPos, yPos):
        super(Actor, self).__init__(xPos, yPos)
        self.speed = 5
        self.dy = 0
        self.d = 3
        self.dir = "right"
        # self.newdir = "right"
        self.state = "standing"
        self.walkR = []
        self.walkL = []
# ---
def _base_plot(
    ax, base_data, timestamp, timestep_value, framedim, plotmethod=None, **kwargs
):
    data = base_data.isel({framedim: timestamp})
    p = _core_plot(ax, data, plotmethod=plotmethod, **kwargs)
    return p
# ---
def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(f"{name}: {str(meter)}")
        return self.delimiter.join(loss_str)
# ---
def loss_amount(actual_cost,sale_amount): 
  if(sale_amount > actual_cost):
    amount = sale_amount - actual_cost
    return amount
  else:
    return None
# ---
def native_value(self) -> datetime | None:
        """Return sensor state."""
        job: OctoprintJobInfo = self.coordinator.data["job"]
        if (
            not job
            or not job.progress.print_time_left
            or not _is_printer_printing(self.coordinator.data["printer"])
        ):
            return None

        read_time = self.coordinator.data["last_read_time"]

        return read_time + timedelta(seconds=job.progress.print_time_left)
# ---
def add_placeholder_op(dag, inputs, outputs):
    add_op(dag, placeholder_func, [a.name for a in inputs], [b.name for b in outputs])
# ---
def __init__(self):
        self.transport = None
        self.connected = threading.Event()
        self.disconnected = threading.Event()
        self.port = None
# ---
def join_tuples(test_list):
  res = []
  for sub in test_list:
    if res and res[-1][0] == sub[0]:
      res[-1].extend(sub[1:])
    else:
      res.append([ele for ele in sub])
  res = list(map(tuple, res))
  return (res)
# ---
def test_do_execute_no_params_w_replace(self):
        self._test_do_execute_no_params(True)
# ---
def __init__(self, job_id: str, future: Future[None]):
        self._job_id = job_id
        self._future = future
        self._terminated = threading.Event()
# ---
def inference_target(self, step: int | slice):
        x_index = self._get_x_index(step)
        label = self._get_label(x_index)
        return label
# ---
def on_terminate(hosts: list[str]) -> None:
        callback_hosts.extend(hosts)
# ---
def get_custom_objects():
  """Retrieves a live reference to the global dictionary of custom objects.

  Updating and clearing custom objects using `custom_object_scope`
  is preferred, but `get_custom_objects` can
  be used to directly access `_GLOBAL_CUSTOM_OBJECTS`.

  Example:

  ```python
      get_custom_objects().clear()
      get_custom_objects()['MyObject'] = MyObject
  ```

  Returns:
      Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).
  """
  return _GLOBAL_CUSTOM_OBJECTS
# ---
def test_pass_different_length_seq(num_kv_heads):
    config = MistralConfig(
        max_seq_len=64,
        hidden_dim=32,
        intermediate_dim=32,
        num_heads=2,
        num_kv_heads=num_kv_heads,
        use_flash_attention=True,
    )
    check_model_works_with_seqlen(MistralLMHeadModel, config, 16)
# ---
def GetCurrentBufferNumber():
  return vim.current.buffer.number
# ---
def __call__(
        self, module: M_contra, carry: CarryT, *args: P.args, **kwargs: P.kwargs
    ) -> tuple[CarryT, OutputT_co]: ...
# ---
def raw_ohc(ds):
    c_p = 3850  # J/(kg C)
    rho_0 = 1025  # kg/m^3
    ohc = ds.thetao * c_p * rho_0  # C*J/(kg C)*kg/m^3 = J/m^3
    return ohc
# ---
def expand(self, pcoll):
        return (
            pcoll
            | "Prepare" >> beam.FlatMap(self.prepare_stage)
            | beam.Reshuffle()
            | "Execute" >> beam.MapTuple(self.exec_stage)
            | beam.combiners.ToList()
            | "Validate" >> beam.Map(self.post_validate)
        )
# ---
def sin(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in sin")
    return elemwise(nxp.sin, x, dtype=x.dtype)
# ---
def tie_weights(self):
        """Tie weights."""
        super().tie_weights()
# ---
def play(self, filename):
        self.loadfile(filename)
# ---
def test_unstack(spec, executor, chunks):
    a = xp.full((4, 6), 1, chunks=(2, 3), spec=spec)
    b = xp.full((4, 6), 2, chunks=(2, 3), spec=spec)
    c = xp.full((4, 6), 3, chunks=(2, 3), spec=spec)
    d = xp.stack([a, b, c], axis=0)

    d = d.rechunk(chunks)

    au, bu, cu = cubed.compute(*xp.unstack(d), executor=executor, optimize_graph=False)

    assert_array_equal(au, np.full((4, 6), 1))
    assert_array_equal(bu, np.full((4, 6), 2))
    assert_array_equal(cu, np.full((4, 6), 3))
# ---
def resolve(self, host, port, family=socket.AF_UNSPEC):
        addrinfo = socket.getaddrinfo(host, port, family)
        results = []
        for family, socktype, proto, canonname, address in addrinfo:
            results.append((family, address))
        return results
# ---
def get_stats(self, container_id: str) -> ContainerStats:
        del container_id
        return ContainerStats(memory_mb=100, cpu_percent=10, process_count=1, available=True)
# ---
def Extract(lst): 
    return [item[0] for item in lst]
# ---
def step_impl(context, username, field, value):
    user = context.user_service.exists(username)

    if user is not None:
        user[field] = value
        context.user_service.update(user.to_json())
    else:
        raise UserNotFound(username, "User was not found")
# ---
def load_tokenizer_cmd(model_name: str):
    """Load a tokenizer and start REPL."""
    if load_tokenizer(model_name):
        click.Context(cli).invoke(repl_cmd)
# ---
def from_command(cls, *argv: str) -> "Entrypoint":
        """Create a command-line entrypoint.

        Args:
            *argv: Command and arguments (e.g., "python", "train.py", "--epochs", "10")

        Returns:
            Entrypoint configured for command execution
        """
        if not argv:
            raise ValueError("Command must have at least one argument")
        return cls(command=list(argv))
# ---
def __init__(self):
        self._loop = None
        self._thread = threading.Thread(target=self._run_event_loop, daemon=True)
        self._started = threading.Event()
# ---
def to_safe_token(self) -> str:
        """Return a filesystem/tag-safe token derived from this name."""
        return "job__" + "__".join(self._parts)
# ---
import heapq
def expensive_items(items,n):
  expensive_items = heapq.nlargest(n, items, key=lambda s: s['price'])
  return expensive_items
# ---
def concatenate_elements(list):
  ans = ' '
  for i in list:
    ans = ans+ ' '+i
  return (ans)
# ---
def delete(self,solverId):
    path = "{base}/{solverId}".format(base=self.pathbase,
                                      solverId=str(solverId))
    req = self.session.delete(path,
                          params=self.params,
                          headers=self.headers)
    self.validateReply(req)
    return True
# ---
def add_hook(self, fn: Callback, *, every: int = 1): ...
# ---
def diagnostics(self, handle: VllmServerHandle, *, max_lines: int = 200) -> dict[str, str]:
        diagnostics: dict[str, str] = {}
        if handle.docker_run_cmd:
            diagnostics["vLLM Docker run command (redacted)"] = handle.docker_run_cmd
        diagnostics["vLLM Docker logs (tail)"] = self.logs_tail(handle, max_lines=max_lines)
        diagnostics["vLLM Docker inspect"] = _docker_inspect(handle.docker_container_name)
        return diagnostics
# ---
def modInverse(a, m):
    x, y = egcd(a, m)
    if gcd(a, m) == 1:
        return x % m
# ---
def add_file(self, arcname, str_or_bytes, in_spine = False,
      guide_title = None, guide_type = None):
        '''Add the string or bytes instance str_or_bytes to the archive
        under the name arcname.'''
        self.opf.filelist.append(_Fileinfo(arcname, in_spine, guide_title,
                                 guide_type))
        self.epub_f.writestr(arcname, str_or_bytes)
# ---
def test_task_config():
    task_spec = [
        TaskConfig(
            task="hellaswag",
            task_alias="hellaswag_10shot",
            num_fewshot=10,
        ),
        TaskConfig(
            task="hellaswag",
            task_alias="hellaswag_5shot",
            num_fewshot=5,
        ),
        "lambada_openai",
    ]

    config = LmEvalHarnessConfig(
        task_spec=task_spec,
    )

    q = config.to_task_dict()

    assert len(q) == 3
# ---
def reset(self):
        return self._reset()
# ---
def __post_init__(self):
        assert not (self.fp8 and self.int8), "Cannot use FP8 and INT8 quantization at the same time."
# ---
def load_tokenizer(tokenizer_path: Path):
    return AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)
# ---
def __str__(self):
        return self.text
# ---
def debug(ctx):
    """Cluster debugging and validation commands.

    These commands discover the controller VM via GCP, establish SSH tunnels
    transparently, and provide operational tooling.
    """
    pass
# ---
def test_task_detail_page_loads(client):
    """Test /task/{task_id} page loads successfully."""
    response = client.get("/task/test-task-123")
    assert response.status_code == 200
    assert response.headers["content-type"] == "text/html; charset=utf-8"
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> ListCache[KvPageCache]:
        """
        Creates an initial cache for this model. Note that in order to create a decoder state, you
        need to couple the KvPageCache to the PageTable's state with a BatchInfo object.
        """
        return hax.auto_sharded(self.transformer.initial_cache(spec, dtype=dtype))
# ---
def test_rust_native(benchmark: Any, small_parquet_path: str) -> None:
    """
    Baseline: Rust reads file from disk, parses Parquet, transforms, returns RecordBatch.
    """

    def _run() -> int:
        return len(dupekit.process_native(small_parquet_path))

    assert benchmark(_run) > 0
# ---
def act(self, s):
        if random.random() < self.explore_rate:
            return random.randint(0, NUM_ACTIONS - 1)
        else:

            return np.argmax(self.brain.predict(s))
# ---
def test_raw_metric_host_disk(metrics_collection, appliance, provider):
    host_name = get_host_name(provider)
    query = query_metric_db(appliance, provider, 'disk_usage_rate_average',
        host_name)

    for record in query:
        if record.disk_usage_rate_average is not None:
            assert record.disk_usage_rate_average > 0, 'Zero Host Disk IO'
            break
# ---
def test_copy_experiment_inavlid_id(self):
        """ Tests that copy_experiment fails with bad experiment_id """
        url = reverse("ab_testing_tool_copy_experiment", args=(12345,))
        response = self.client.post(url, follow=True)
        self.assertEquals(response.status_code, 404)
# ---
def output_code(self):
        return self.renderer.block_code(
            self.token['text'], self.token['lang']
        )
# ---
def get_device() -> torch.device:
    """The primary device for inference.

    We avoid setting this as torch.device so we can choose when
    to shuttle tensors to the device.
    """
    if _CHOSEN_DEVICE is not None:
        return _CHOSEN_DEVICE

    if torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")
# ---
def resource_spec():
    """Create a ResourceSpec for testing with enough capacity for multiple jobs."""

    def _make(cpu: int = 10, memory_bytes: int = 10 * 1024**3) -> cluster_pb2.ResourceSpecProto:
        return cluster_pb2.ResourceSpecProto(cpu=cpu, memory_bytes=memory_bytes, disk_bytes=10 * 1024**3)

    return _make
# ---
def __post_init__(self):
        if " " in self.name:
            raise ValueError("Job name must not contain spaces")
        if self.replicas is None:
            # Pick up replicas from ResourceConfig (set by e.g. with_tpu slice_count)
            self.replicas = self.resources.replicas
# ---
def index():
    return render_template('index.html'), 200
# ---
def execution_stats(func):
    """Decorator to measure timing information and peak memory usage of a function call."""

    return partial(execute_with_stats, func)
# ---
def vmap(self, *extra_args, **extra_kwargs):
        """Apply each block independently using :func:`haliax.vmap`.

        Returns the stacked outputs of each block.
        """

        return haliax.vmap(type(self.stacked).__call__, self.Block)(self.stacked, *extra_args, **extra_kwargs)
# ---
def global_data_indices_by_device_for_step(self, step: int) -> dict[jax.Device, range]:
        local_indices = self.local_data_indices_by_device_for_step(step)
        offset = self.scheduler.global_data_offset_by_step(step)

        return {device: range(offset + r.start, offset + r.stop, r.step) for device, r in local_indices.items()}
# ---
def test_poly():
    R = Axis("R", 3)
    roots = hax.named([1.0, 2.0, 3.0], (R,))
    coeffs = hax.poly(roots)
    assert jnp.allclose(coeffs.array, jnp.poly(roots.array))
    assert coeffs.axes[0] == R.resize(coeffs.array.shape[0])
# ---
def job_id(self) -> JobName:
        """Parent job identifier."""
        return self._task_name.parent or self._task_name
# ---
def byte_size(self):
        return self.data.nbytes + self.offsets.nbytes + (self.shapes.nbytes if self.shapes is not None else 0)
# ---
def hash_binary(bin_path):
    blocksize = 65536
    hasher = hashlib.sha1()
    with open(bin_path, 'rb') as bin_file:
        buf = bin_file.read(blocksize)
        while len(buf) > 0:
            hasher.update(buf)
            buf = bin_file.read(blocksize)
    return hasher.hexdigest()
# ---
def get_mol(ccd: str, mols: dict, moldir: str) -> Mol:
    """Get mol from CCD code.

    Return mol with ccd from mols if it is in mols. Otherwise load it from moldir,
    add it to mols, and return the mol.
    """
    mol = mols.get(ccd)

    if mol is None:
        mol = load_molecules(moldir, [ccd])[ccd]
        mols[ccd] = mol  # cache for future calls

    return mol
# ---
def sequence(n): 
	if n == 1 or n == 2: 
		return 1
	else: 
		return sequence(sequence(n-1)) + sequence(n-sequence(n-1))
# ---
def write_xml(self, uid, title, authors):
        'Write the xml code for the table of contents.'
        xml = self._head.format(uid, self.max_depth, title)
        for aut in authors:
            xml += self._doc_author.format(aut)
        xml += '  <navMap>\n'
        for entry in self.entries:
            xml += self._navp_xml(entry, 2)
        xml += '  </navMap>\n</ncx>'
        return xml
# ---
def test_capsyscapfdbinary(self, testdir):
        p = testdir.makepyfile("""
            def test_one(capsys, capfdbinary):
                pass
        """)
        result = testdir.runpytest(p)
        result.stdout.fnmatch_lines([
            "*ERROR*setup*test_one*",
            "E*capfdbinary*capsys*same*time*",
            "*1 error*"])
# ---
def foo(x: f32["batch embed"]):  # type: ignore  # noqa: F722
        pass
# ---
def __call__(self, text):
        return self.parse(text)
# ---
def name_get(self, cr, uid, ids, context=None):
        # always return the full hierarchical name
        res = self._complete_name(cr, uid, ids, 'complete_name', None, context=context)
        return res.items()
# ---
def test_no_ellipsis():
    partial_order = ("apple", "banana", "cherry")
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)
    assert actual_output == ("apple", "banana", "cherry")
# ---
def __iter__(self):
        """
        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.
        :return: The logger instance.
        """
        return self
# ---
import collections
def freq_count(list1):
  freq_count= collections.Counter(list1)
  return freq_count
# ---
def abs(a: A) -> A:
    return wrap_elemwise_unary(jnp.abs, a)
# ---
def _psum(xs: Any) -> Any:
    return jax.tree.map(lambda x: jnp.sum(x, dtype=x.dtype, axis=0), xs)
# ---
def __init__(self, exemplar: T):
        self.exemplar = exemplar
# ---
def _get_address(self):
        self._socket = self.transport.get_extra_info("peername") or (
            None,
            None,
        )
        self._ip = self._socket[0]
        self._port = self._socket[1]
# ---
def update_fn(updates, state, params=None):
        new_state = {"count": state["count"] + 1}
        # simple transformation: negate gradients
        transformed_updates = jax.tree_util.tree_map(lambda g: -g, updates)
        return transformed_updates, new_state
# ---
def increase(rank):
    pass
# ---
def task_status(self, task_name: JobName) -> cluster_pb2.TaskStatus:
        """Get status of a specific task.

        Args:
            task_name: Full task name (/job/.../index)

        Returns:
            TaskStatus proto containing state, worker assignment, and metrics
        """
        return self._cluster_client.get_task_status(task_name)
# ---
def getPlugins(): pass
# ---
def direct_fn(pred):
        pred_embeddings, pred_lm_head = pred
        logits = hax.dot(pred_embeddings, pred_lm_head, axis="embed", preferred_element_type=pred_embeddings.dtype)
        target_y = hax.nn.one_hot(true_ids, Vocab, dtype=pred_embeddings.dtype)
        loss, _ = cross_entropy_loss_and_log_normalizers(logits, Vocab, target_y)
        return loss.mean().scalar()
# ---
def dict_depth(d):
    if isinstance(d, dict):
        return 1 + (max(map(dict_depth, d.values())) if d else 0)
    return 0
# ---
def test_ckpt_path_with_input_name_trailing_slash():
    input_name = make_input_name("checkpoints/step-555000/", "myrun")
    assert ckpt_path_to_step_name(input_name) == "myrun-555000"
# ---
def list_iris_containers(self, all_states: bool = True) -> list[str]: ...
# ---
def test_dense_regularization(self):
    layer = keras.layers.Dense(
        3,
        kernel_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l1',
        activity_regularizer='l2',
        name='dense_reg')
    layer(keras.backend.variable(np.ones((2, 4))))
    self.assertEqual(3, len(layer.losses))
# ---
def list2cols(cols, objs):
    return cols, [tuple([o[k] for k in cols])
                  for o in objs]
# ---
def prng_key_for(self, slot_id: int, pos_id: int) -> jaxtyping.PRNGKeyArray:
        """
        Get the PRNG key for the given slot ID and position.
        This is used to sample new tokens for the given slot ID and position.
        """
        per_pos_key = self.prng_keys[ensure_scalar(slot_id)]
        return jax.random.fold_in(per_pos_key, ensure_scalar(pos_id))
# ---
def _write_metadata(cfg: UncheatableEvalDownloadConfig, records: list[dict[str, Any]]) -> None:
    if not records:
        return
    metadata_path = posixpath.join(str(cfg.output_path), cfg.metadata_filename)
    with fsspec.open(metadata_path, "w", encoding="utf-8") as meta_file:
        json.dump(records, meta_file, indent=2, ensure_ascii=False)
    logger.info("Wrote metadata to %s", metadata_path)
# ---
def test_cluster_job_success(cluster):
    request = JobRequest(
        name="success-test-job",
        entrypoint=Entrypoint.from_callable(lambda: None),
        environment=EnvironmentConfig.create(),
    )

    job_id = cluster.launch(request)
    info = cluster.wait(job_id)

    assert info.status == "succeeded"
    assert info.error_message is None
# ---
def maximum(a,b):   
    if a >= b: 
        return a 
    else: 
        return b
# ---
def _get_current_tpu_pod_type() -> str:
    """Return the TPU pod type for the current node across Ray versions."""

    if hasattr(TPUAcceleratorManager, "_get_current_node_tpu_pod_type"):
        return TPUAcceleratorManager._get_current_node_tpu_pod_type()
    if hasattr(TPUAcceleratorManager, "get_current_node_tpu_pod_type"):
        return TPUAcceleratorManager.get_current_node_tpu_pod_type()
    raise AttributeError("TPUAcceleratorManager is missing TPU pod type helpers")
# ---
def try_parse(value):
    try:    return int(value)
    except: return { 'true': True, 'false': False }.get(value.lower(), value)
# ---
def get_msg(self):
        return self._saved_msg
# ---
def _compute_hash(self, path: Path) -> str:
        h = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(65536), b""):
                h.update(chunk)
        return h.hexdigest()
# ---
def less(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "less")
    return elemwise(nxp.less, x1, x2, dtype=nxp.bool)
# ---
def fuse_only_optimize_dag(dag, *, array_names=None, only_fuse=None):
    """Force only specified operations to be fused, all others will be left even if they are suitable for fusion."""
    dag = dag.copy()
    always_fuse = only_fuse
    never_fuse = set(op for op in dag.nodes() if op.startswith("op-")) - set(only_fuse)
    return multiple_inputs_optimize_dag(
        dag, array_names=array_names, always_fuse=always_fuse, never_fuse=never_fuse
    )
# ---
def dispatch_event(self, *args):
        if not self._enable_event_queue or self._allow_dispatch_event:
            if EventDispatcher.dispatch_event(self, *args) != False:
                self._legacy_invalid = True
        else:
            self._event_queue.append(args)
# ---
def opendb(self):
        return smadata2.db.mock.MockDatabase()
# ---
def __new__(cls, name, bases, members):
        cls.members = [v for k, v in members.items() if not k.startswith("__") and not callable(v)]
        return super().__new__(cls, name, bases, members)
# ---
def _as_tuple(si):
            if si is None:
                return None
            if isinstance(si, tuple):
                return si
            else:
                return (si, si)
# ---
def test_different_specs(tmp_path):
    spec1 = cubed.Spec(tmp_path, allowed_mem=100000)
    spec2 = cubed.Spec(tmp_path, allowed_mem=200000)
    a = xp.ones((3, 3), chunks=(2, 2), spec=spec1)
    b = xp.ones((3, 3), chunks=(2, 2), spec=spec2)
    with pytest.raises(
        ValueError, match="Arrays must have same spec in single computation"
    ):
        xp.add(a, b)
# ---
def remove(self, container_id: str) -> None:
        result = subprocess.run(
            ["docker", "rm", "-f", container_id],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode != 0:
            raise RuntimeError(f"Failed to remove container: {result.stderr}")
# ---
def linear_1d(module, grad_input, grad_output):
    """No change made to gradients."""
    return None
# ---
def dict_map(fn, dic, leaf_type):
    new_dict = {}
    for k, v in dic.items():
        if type(v) is dict:
            new_dict[k] = dict_map(fn, v, leaf_type)
        else:
            new_dict[k] = tree_map(fn, v, leaf_type)

    return new_dict
# ---
def test_named_ref_scalar_update():
    X = hax.Axis("x", 5)
    ref = hax.new_ref(hax.zeros(X))
    ref[{"x": 2}] = 3.14
    assert pytest.approx(ref.value()[{"x": 2}].array.item()) == 3.14
# ---
def palindrome_lambda(texts):
  result = list(filter(lambda x: (x == "".join(reversed(x))), texts))
  return result
# ---
def virtual_offsets(shape: T_Shape) -> VirtualOffsetsArray:
    return VirtualOffsetsArray(shape)
# ---
def PlaceSign( sign_id, line_num, buffer_num, is_error = True ):
  # libclang can give us diagnostics that point "outside" the file; Vim borks
  # on these.
  if line_num < 1:
    line_num = 1

  sign_name = 'YcmError' if is_error else 'YcmWarning'
  vim.command( 'sign place {0} name={1} line={2} buffer={3}'.format(
    sign_id, sign_name, line_num, buffer_num ) )
# ---
def __init__(self, client: "IrisClient", job_id: JobName):
        self._client = client
        self._job_id = job_id
# ---
def _save_metadata(checkpoint_path, fs, step, is_temporary):
    metadata = {"step": step, "timestamp": datetime.datetime.now().isoformat(), "is_temporary": is_temporary}
    if jax.process_index() == 0:
        with fs.open(os.path.join(checkpoint_path, "metadata.json"), "w") as json_out:
            json.dump(metadata, json_out)
# ---
def testFormatMACB(self):
    """Tests the _FormatMACB function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    macb_string = test_helper._FormatMACB(event, event_data, event_data_stream)
    self.assertEqual(macb_string, '..C.')
# ---
def right_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    return jnp.right_shift(x1, x2)
# ---
def test_inference_ctx():
    return create_test_inference_context()
# ---
def status_represent(index):
            if index == None:
                return "Unknown" # @todo: use messages (internationalize)
            else:
                return import_upload_status[index]
# ---
def genetic_modification_7_addgene_source(testapp):
    item = {
        'name': 'addgene',
        'title': 'Addgene',
        'status': 'released'
    }
    return testapp.post_json('/source', item).json['@graph'][0]
# ---
def set_asset_naming_series(self):
		if not hasattr(self, '_asset_naming_series'):
			from erpnext.assets.doctype.asset.asset import get_asset_naming_series
			self._asset_naming_series = get_asset_naming_series()

		self.set_onload('asset_naming_series', self._asset_naming_series)
# ---
def fold(self, init: T, *args, unroll: int | bool | None = None, **kwargs) -> T: ...
# ---
def generator(self):
        """Generate pages from specified page interval."""
        for page_number in self.page_number_gen():
            title = '{page_ns}:{prefix}/{number}'.format(
                page_ns=self._page_ns,
                prefix=self._prefix,
                number=page_number)
            page = ProofreadPage(self._index.site, title)
            page.page_number = page_number  # remember page number in djvu file
            yield page
# ---
def test_concat(spec, executor):
    # note: middle chunk of output reads from three input chunks
    a = xp.full((4, 5), 1, chunks=(3, 2), spec=spec)
    b = xp.full((1, 5), 2, chunks=(3, 2), spec=spec)
    c = xp.full((3, 5), 3, chunks=(3, 2), spec=spec)
    d = xp.concat([a, b, c], axis=0)
    assert_array_equal(
        d.compute(executor=executor),
        np.concatenate(
            [np.full((4, 5), 1), np.full((1, 5), 2), np.full((3, 5), 3)], axis=0
        ),
    )
# ---
def decimal_To_Binary(N): 
    B_Number = 0
    cnt = 0
    while (N != 0): 
        rem = N % 2
        c = pow(10,cnt)  
        B_Number += rem*c  
        N //= 2 
        cnt += 1
    return B_Number
# ---
def service(state, mock_scheduler):
    """Create a ControllerServiceImpl for testing."""
    return ControllerServiceImpl(state, mock_scheduler, bundle_prefix="file:///tmp/iris-test-bundles")
# ---
def shutdown(self):
        """Signal shutdown and wait for threads to finish"""
        logger.info("Shutting down inference context.")
        self.shutdown_event.set()
        self.inference_thread.join(timeout=1)
        self.batch_thread.join(timeout=1)
# ---
def run_on_pod_multislice(
    remote_fn: RemoteFunction | Callable, tpu_type: str, num_slices: Sequence[int]
) -> list[ray.ObjectRef]:
    """Run a remote function on multiple TPU slices."""
    return ray.get(
        run_on_pod(
            remote_fn,
            tpu_type,
            num_slices=num_slices,
            max_retries_failure=0,
            max_retries_preemption=0,
        )
    )
# ---
def test_sliding_window_mask():
    Pos = hax.Axis("pos", 16)
    KeyPos = Pos.alias("key_pos")
    window = 4
    mask = AttentionMask.causal(sliding_window=window)
    mat = mask.materialize(Pos, KeyPos)
    q_pos = hax.arange(Pos)
    k_pos = hax.arange(KeyPos)
    diff = q_pos.broadcast_axis(KeyPos) - k_pos.broadcast_axis(Pos)
    expected = (diff >= 0) & (diff < window)
    assert hax.all(mat == expected)
# ---
def get(self):
        self.handle_request('GET')
# ---
def exists(self):
        if self.want.member_id is None:
            return False
        uri = 'https://{0}:{1}/mgmt/cm/device/licensing/pool/regkey/licenses/{2}/offerings/{3}/members/{4}'.format(
            self.client.provider['server'],
            self.client.provider['server_port'],
            self.want.pool_id,
            self.want.key,
            self.want.member_id
        )
        resp = self.client.api.get(uri)
        if resp.status == 200:
            return True
        return False
# ---
def kill(self, container_id: str, force: bool = False) -> None:
        del force  # Local containers don't distinguish force vs graceful
        if container_id in self._containers:
            self._containers[container_id].kill()
# ---
def __str__(self):
        return f"{self.name}({self.size})"
# ---
def total_trainable_params(self, vocab_size: int) -> Optional[float]:
        return None
# ---
def do_save(self, match):
        self.lc.publish('Match/Save', match.encode())
# ---
def ensure_first(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.rearrange((axis, ...))
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            return leaf
# ---
def test_xxh3_64_batch():
    # Batch vs. rowwise xxh3_64 parity
    inputs = [b"one", b"two", b"three", b"four"]
    expected = [hash_xxh3_64(x) for x in inputs]
    actual = hash_xxh3_64_batch(inputs)
    assert actual == expected
# ---
def shuffle_ds(ds, k):
            if self.shuffle is True:
                ds = ds.shuffle(k, perm_type=perm_type)
            elif isinstance(self.shuffle, int) and self.shuffle > 0:
                ds = ds.era_shuffle(self.shuffle, key=k, perm_type=perm_type)
            return ds
# ---
def updateGrid(self):
        """
        Implements function to update the grid to alter n-1
        round values

        """
        with open(self.fichier, 'r') as fi:
            for line in fi.readlines():
                i = 0
                for car in line:
                    j = 0
                    if car != '\n':
                        self.grille[i][j] = car
                        j += 1
                    i += 1
# ---
def arctanh(a: A) -> A:
    return wrap_elemwise_unary(jnp.arctanh, a)
# ---
def _str_is_int(x: str) -> bool:
    try:
        x = _strip_properly_formatted_commas(x)
        x = float(x)
        return abs(x - round(x)) <= 1e-7
    except BaseException:
        return False
# ---
def __init__(self, **kwargs):
        self.kwargs = kwargs

        # Tell NumPy to use a single thread
        # from https://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy
        os.environ["MKL_NUM_THREADS"] = "1"
        os.environ["NUMEXPR_NUM_THREADS"] = "1"
        os.environ["OMP_NUM_THREADS"] = "1"
        os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
# ---

def string_to_md5(text):
    """
    Given a string 'text', return its md5 hash equivalent string.
    If 'text' is an empty string, return None.

    >>> string_to_md5('Hello world') == '3e25960a79dbc69b674cd4ec67a72c62'
    """
    import hashlib
    return hashlib.md5(text.encode('ascii')).hexdigest() if text else None
# ---
def genetic_modification_source(testapp, lab, award, source, gene):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'introduced_gene': gene['@id'],
        'purpose': 'expression',
        'method': 'CRISPR',
        'reagents': [
            {
                'source': source['@id'],
                'identifier': 'sigma:ABC123'
            }
        ]
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def _reduce_gen(shard: Any, key_fn: Callable, reducer_fn: Callable) -> Iterator:
    for key, items_iter in _merge_sorted_chunks(shard, key_fn):
        yield reducer_fn(key, items_iter)
# ---
def list_all_workers(self) -> list[ControllerWorker]:
        with self._lock:
            return list(self._workers.values())
# ---
def test_select_from_plain_union(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2)
        s2 = select([table]).where(table.c.id == 3)

        u1 = union(s1, s2).alias().select()
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def get_block_depth(cls, block_structure, block_key):
        """
        Return the precalculated depth of a block within the block_structure:

        Arguments:
            block_structure: a BlockStructure instance
            block_key: the key of the block whose depth we want to know

        Returns:
            int
        """
        return block_structure.get_transformer_block_field(
            block_key,
            cls,
            cls.BLOCK_DEPTH,
        )
# ---
def decorate_as_label(self, label_type, labels):
        allure_label_marker = '{prefix}.{label_type}'.format(prefix=ALLURE_LABEL_PREFIX, label_type=label_type)
        allure_label = getattr(pytest.mark, allure_label_marker)
        return allure_label(*labels, label_type=label_type)
# ---
def build(self, ctx: LrScheduleContext):
        return optax.cosine_decay_schedule(ctx.learning_rate, ctx.decay_steps, ctx.min_lr_ratio, self.exponent)
# ---
def _health_check() -> dict:
    """Health check endpoint."""
    return {"status": "healthy", "service": "levanter-inference"}
# ---
def _state_dict_key_map(self) -> dict[str, str | None]:
        return {"blocks": None}
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        if isinstance(self.device, CpuConfig):
            return 100e9
        return self.device_flops(dtype) * self.chip_count()
# ---
def clear(self):
        """Clears Enforcer rules, policy's cache and policy's path."""
        self.set_rules({})
        self.policy_path = None
# ---
def search(self, cr, user, args, offset=0, limit=None, order=None, context=None, count=False):
        return self.pool.get('stock.picking').search(cr, user, args, offset, limit, order, context, count)
# ---
def select(x):
        if isinstance(x, NamedArray):
            return x["seq", idx]
        elif is_jax_array_like(x):
            return x[idx]
        else:
            raise TypeError(f"Unexpected type in seq_params: {type(x)}")
# ---
def test_axis_shapes_force_replica_dcn_when_other_absorber():
    cfg = MeshConfig(dcn_axes={"other_dcn": -1})
    _, dcn = cfg.axis_shapes(num_devices=8, num_slices=2)
    # replica_dcn forced to 1 to leave only one absorber
    assert dcn["replica_dcn"] == 1
    assert dcn["other_dcn"] == 2
# ---
def reload(self) -> "JaggedArrayStore":
        offsets = ts.open(_unshaped_spec(self.offsets))
        data = ts.open(_unshaped_spec(self.data))
        shapes = None if self.shapes is None else ts.open(_unshaped_spec(self.shapes.spec())).result()

        offsets = offsets.result()
        data = data.result()

        return JaggedArrayStore(offsets, data, shapes, self.item_rank)
# ---
def cursorAtSpaceEnd(block):
            cursor = QTextCursor(block)
            cursor.setPosition(block.position() + len(blockIndentation(block)))
            return cursor
# ---
def __hash__(self):
        return hash((self.field, self.ascending))
# ---
def _map(example: dict) -> LmExample:
                return _create_lm_example(example[input_ids_key])
# ---
def clause(self):
        return '0', ()
# ---
def test_metric_pytree():
    """Metrics are JAX pytrees."""
    m = Metric(_value=30.0, _count=2.0, reduction=ReductionType.MEAN)
    flat, treedef = jax.tree_util.tree_flatten(m)
    reconstructed = jax.tree_util.tree_unflatten(treedef, flat)

    assert jnp.allclose(reconstructed.value(), 15.0)  # 30 / 2 = 15
    assert reconstructed.reduction == ReductionType.MEAN
# ---
def __init__(self, writer: "SummaryWriter"):
        self.writer = writer
# ---
def setup_class(cls):
        global users, metadata
        metadata = MetaData(testing.db)
        users = Table('users', metadata,
            Column('user_id', INT, primary_key=True,
                            test_needs_autoincrement=True),
            Column('user_name', VARCHAR(20)),
        )
        metadata.create_all()
# ---
def decode_cmd(tokens: str, tokenizer: str | None):
    """Decode tokens to text."""
    if tokenizer:
        load_tokenizer(tokenizer)
    try:
        token_list = ast.literal_eval(tokens)
        decode_tokens(token_list)
    except Exception as e:
        console.print(f"[red]Error parsing tokens: {e}[/red]")
# ---
def test_add(spec, any_executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]], chunks=(2, 2), spec=spec)
    c = xp.add(a, b)
    assert_array_equal(
        c.compute(executor=any_executor), np.array([[2, 3, 4], [5, 6, 7], [8, 9, 10]])
    )
# ---
def the_object_name1_and_name2_are_different_elements(name1, name2):
    ifc = an_ifc_file_exists()
    element1 = ifc.by_id(the_object_name_exists(name1).BIMObjectProperties.ifc_definition_id)
    element2 = ifc.by_id(the_object_name_exists(name2).BIMObjectProperties.ifc_definition_id)
    assert element1 != element2, f"Objects {name1} and {name2} have same elements {element1} and {element2}"
# ---
def from_string(value):
        """ Convert an ORM ``value`` into a :class:`datetime` value. """
        if not value:
            return None
        value = value[:DATETIME_LENGTH]
        if len(value) == DATE_LENGTH:
            value += " 00:00:00"
        return datetime.strptime(value, DATETIME_FORMAT)
# ---
def simple_loss_fn(model, batch, key=None):
    """Loss function returning scalar only."""
    return hax.sum(batch * model.weight)
# ---
def count_Char(str,x): 
    count = 0
    for i in range(len(str)):  
        if (str[i] == x) : 
            count += 1
    n = 10
    repititions = n // len(str)  
    count = count * repititions  
    l = n % len(str)  
    for i in range(l): 
        if (str[i] == x):  
            count += 1
    return count
# ---
def _to_pallas_dslice(value):
    if is_pallas_dslice(value):
        return value
    if isinstance(value, HaliaxDSlice):
        return dslice(value.start, value.size)
    raise TypeError("Expected a haliax.dslice or pallas.dslice")
# ---
def key_to_slices(
    key: Tuple[int, ...], arr: T_ZarrArray, chunks: Optional[T_Chunks] = None
) -> Tuple[slice, ...]:
    """Convert a chunk index key to a tuple of slices"""
    chunks = normalize_chunks(chunks or arr.chunks, shape=arr.shape, dtype=arr.dtype)
    return get_item(chunks, key)
# ---
def test_olmo2_attention(use_flash, num_kv_heads):
    config = _get_olmo2_config(use_flash=use_flash, num_kv_heads=num_kv_heads)
    key = random.PRNGKey(0)

    attention = Olmo2Attention.init(config=config, key=key)

    x, mask = _get_random_inputs(config)
    out = attention(x, mask)

    # Check output has correct shape
    assert out.array.shape == x.array.shape
    assert out.axes == x.axes
# ---
def __checkArgumentFormat( self, path ):
    if type( path ) in types.StringTypes:
      urls = {path:False}
    elif type( path ) == types.ListType:
      urls = {}
      for url in path:
        urls[url] = False
    elif type( path ) == types.DictType:
      urls = path
    else:
      return S_ERROR( "TransformationClient.__checkArgumentFormat: Supplied path is not of the correct format." )
    return S_OK( urls )
# ---
def onShortcutUnindentWithBackspace(self):
        """Backspace pressed, unindent
        """
        assert self._qpart.textBeforeCursor().endswith(self.text())

        charsToRemove = len(self._qpart.textBeforeCursor()) % len(self.text())
        if charsToRemove == 0:
            charsToRemove = len(self.text())

        cursor = self._qpart.textCursor()
        cursor.setPosition(cursor.position() - charsToRemove, QTextCursor.KeepAnchor)
        cursor.removeSelectedText()
# ---
def check_K(test_tup, K):
  res = False
  for ele in test_tup:
    if ele == K:
      res = True
      break
  return (res)
# ---
def get_input_embeddings(self):
        return self.bimamba.backbone.embeddings.word_embeddings
# ---
def test_select_exists(self, connection):
        stuff = self.tables.stuff
        eq_(
            connection.execute(
                select([literal(1)]).where(
                    exists().where(stuff.c.data == "some data")
                )
            ).fetchall(),
            [(1,)],
        )
# ---
def run_cmd(cmd: list[str], check: bool = False) -> subprocess.CompletedProcess:
    click.echo(f"  $ {' '.join(cmd)[:200]}")
    return subprocess.run(cmd, cwd=ROOT_DIR, check=check)
# ---
def _mean_aggregate(a, **kwargs):
    return nxp.divide(a["total"], a["n"])
# ---
def test_set_current_client_restores_on_exception():
    """Context manager should restore previous client even on exception."""
    explicit = LocalClient(max_threads=2)
    with pytest.raises(RuntimeError):
        with set_current_client(explicit):
            raise RuntimeError("boom")
    assert current_client() is not explicit
# ---
def test_reinitialize_some_tokens_empty_list(local_gpt2_tokenizer):
    tokenizer = local_gpt2_tokenizer
    model = None

    with pytest.raises(ValueError, match="No tokens to reinitialize"):
        reinitialize_some_tokens(model, tokenizer, [], jax.random.PRNGKey(0))
# ---
def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{attr}'"
        )
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        return dir in self._dirs
# ---
def __ne__(self, other):
        if not isinstance(other, BBOXCoverage):
            return NotImplemented
        return not self.__eq__(other)
# ---
def __init__(self, dim, base=10_000):
    super().__init__()
    inv_freq = 1.0 / (
      base ** (torch.arange(0, dim, 2).float() / dim)
    )
    self.register_buffer('inv_freq', inv_freq)
    self.seq_len_cached = None
    self.cos_cached = None
    self.sin_cached = None
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        k_gate, k_up, k_down = maybe_rng_split(key, 3)
        hidden_states = self.gate_proj(x, key=k_gate)
        hidden_states = self.act(hidden_states)
        hidden_states = hidden_states * self.up_proj(x, key=k_up)
        outputs = self.down_proj(hidden_states, key=k_down)
        return outputs
# ---
def handle_heartbeat(self, request: cluster_pb2.HeartbeatRequest) -> cluster_pb2.HeartbeatResponse: ...
# ---
def schedule(count):
        decay = jnp.minimum(1.0, 1.0 / jnp.sqrt(jnp.maximum(count + warmup_steps, 1) / timescale))
        return jnp.maximum(lr * decay, min_lr)
# ---
def test_caching_behavior(temp_cache_dir, test_bundle):
    """Test that bundles are cached and not re-downloaded."""
    cache = BundleCache(temp_cache_dir)

    file_url = f"file://{test_bundle}"

    # First download
    extract_path1 = cache.get_bundle(file_url)

    # Second request - should use cache and return same path
    extract_path2 = cache.get_bundle(file_url)

    assert extract_path1 == extract_path2
# ---
def train_worker_task():
            with remove_tpu_lockfile_on_exit():
                logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", force=True)
                worker = TrainWorker(config=train_worker_config)
                worker.train()
# ---
def surface_Area(b,s): 
    return 2 * b * s + pow(b,2)
# ---
def loss_ref(x_raw, w_raw, y_raw):
        loss_val, lse_val = linear_softmax_cross_entropy_loss_reference(
            x_raw,
            y_raw,
            w_raw,
            dtype=jnp.float32,
            logit_soft_cap=logit_soft_cap,
        )
        loss_val = loss_val + logsumexp_weight * (lse_val**2)
        return loss_val.mean()
# ---
def done(self, lease: Lease[T_co]) -> None:
        if lease.lease_id in self.leases:
            del self.leases[lease.lease_id]
# ---
def _apply_update(tree, update, overwrite):
        if overwrite is not None:
            return overwrite

        return eqx.apply_updates(tree, update)
# ---
def update_header(file_path, year, ci):
    config = _get_config(file_path, year)
    ext = file_path.rsplit('.', 1)[-1]
    if ext not in SUPPORTED_FILES or not os.path.isfile(file_path):
        return False
    if os.path.basename(file_path)[0] == '.':
        return False
    return _update_header(file_path, config, SUBSTRING, SUPPORTED_FILES[ext]['regex'],
                          SUPPORTED_FILES[ext]['format'], ci)
# ---
def test_evaluate_float(self):
        expr = lit(3.14)
        assert expr.evaluate({}) == 3.14
# ---
def the_object_name_has_a_body_of_value(name, value):
    assert the_object_name_exists(name).data.body == value
# ---
import math 
def max_Prime_Factors (n): 
    maxPrime = -1 
    while n%2 == 0: 
        maxPrime = 2
        n >>= 1    
    for i in range(3,int(math.sqrt(n))+1,2): 
        while n % i == 0: 
            maxPrime = i 
            n = n / i 
    if n > 2: 
        maxPrime = n  
    return int(maxPrime)
# ---
def _find_acl_templates(conn, acl_templates):
    acl_template_ids = []
    for acl_template in acl_templates:
        acl_template_id = _find_acl_template(conn, acl_template)
        if acl_template_id:
            acl_template_ids.append(acl_template_id)
    return acl_template_ids
# ---
def _normalize(
        self,
        data: Float[torch.Tensor, "batch time var lat lon"],
        fill_nan: bool = True,
        fill_value: float = 0.0,
    ) -> Float[torch.Tensor, "batch time var lat lon"]:
        """Normalize input data treated as torch Tensors."""
        norm = (data - self.means.view(1, 1, -1, 1, 1)) / self.stds.view(1, 1, -1, 1, 1)
        if fill_nan:
            norm = norm.nan_to_num(nan=fill_value)
        norm = norm.to(data.dtype)
        return norm
# ---
def test_invalid_http_url(self):
        """Test that HTTP URLs are rejected."""
        with pytest.raises(ValidationError, match="Absolute urls are not supported"):
            UnresolvedLocation(path="http://example.com/data")
# ---
def __init__(
        self,
        controller_address: str,
        workspace: Path | None = None,
        bundle_gcs_path: str | None = None,
    ):
        logger.info(
            "FrayIrisClient connecting to %s (workspace=%s, bundle_gcs_path=%s)",
            controller_address,
            workspace,
            bundle_gcs_path,
        )
        self._iris = IrisClientLib.remote(controller_address, workspace=workspace, bundle_gcs_path=bundle_gcs_path)
# ---
def run_vllm_inference(model_path, **model_init_kwargs):
    llm = LLM(model=model_path, **model_init_kwargs)

    sampling_params = SamplingParams(
        max_tokens=100,
        temperature=0.7,
    )

    generated_texts = llm.generate(
        "Hello, how are you?",
        sampling_params=sampling_params,
    )

    return generated_texts
# ---
def test_total_consumed_capacity(self):
        """ConsumedCapacity can parse results with only Total"""
        response = {
            "TableName": "foobar",
            "ReadCapacityUnits": 4,
            "WriteCapacityUnits": 5,
        }
        cap = ConsumedCapacity.from_response(response)
        self.assertEqual(cap.total, (4, 5))
        self.assertIsNone(cap.table_capacity)
# ---
def prod(
        self, axis: AxisSelection | None = None, *, dtype=None, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.prod(self, axis=axis, dtype=dtype, where=where)
# ---
def device_reference(self):
        if not self._values['managed']:
            return None
        return self._values['device_reference']
# ---
def _return_hosts(self, hosts: list[str]) -> None:
        """Return hosts to the available pool (callback for ManualVmGroup.terminate)."""
        for host in hosts:
            if host in self._hosts:
                self._available_hosts.add(host)
                logger.debug("Host %s returned to pool", host)
# ---
def loss_fn(p):
        loss, _ = ar_loss(p, token_ids, loss_mask, tiny_cfg)
        return loss
# ---
def _get_batch_path(self, timestamp: float, counter: int) -> str:
        """Get path for batch with timestamp and hostname."""
        timestamp_int = int(timestamp * 1000000)  # microseconds for ordering
        return f"{self.path}/{timestamp_int:020d}_{self.hostname}_{counter:06d}.pkl"
# ---
def from_job(cls, job):
        """Create runner from RLJob."""

        train_config, _ = job.to_worker_configs()
        return cls(train_config)
# ---
def test_dense_with_policy(self):
    inputs = ops.convert_to_tensor(
        np.random.randint(low=0, high=7, size=(2, 2)), dtype='float16')
    layer = keras.layers.Dense(5, dtype=policy.Policy('infer_float32_vars'))
    outputs = layer(inputs)
    self.assertEqual(outputs.dtype, 'float16')
    self.assertEqual(layer.kernel.dtype, 'float32')
# ---
def __init__(
        self,
        shape: Optional[T_Shape] = None,
        dtype: Optional[T_DType] = None,
        chunks: Optional[T_RegularChunks] = None,
    ):
        dict.__init__(self)
        self.shape = shape
        self.dtype = dtype
        self.chunks = chunks
# ---
def intersect_axes(ax1: tuple[AxisSelector, ...], ax2: AxisSelection) -> tuple[AxisSelector, ...]:  # type: ignore
    ...
# ---
def auto_sharded(x: T, mesh: Optional[Mesh] = None) -> T:
    """
    Shard a PyTree using the global axis mapping. NamedArrays in the PyTree are sharded using the axis mapping
     and the names in the tree.

    If there is no axis mapping, the global axis mapping, this function is a no-op.
    """
    mapping = current_thread_local_mapping()

    if mapping is None:
        return x

    return shard(x, mapping=mapping, mesh=mesh)
# ---
import heapq
def nth_super_ugly_number(n, primes):
    uglies = [1]
    def gen(prime):
        for ugly in uglies:
            yield ugly * prime
    merged = heapq.merge(*map(gen, primes))
    while len(uglies) < n:
        ugly = next(merged)
        if ugly != uglies[-1]:
            uglies.append(ugly)
    return uglies[-1]
# ---
def imag(a: A) -> A:
    return wrap_elemwise_unary(jnp.imag, a)
# ---
def union_axes(a1: AxisSelection, a2: AxisSelection) -> AxisSelection: ...
# ---
def test_count_with_filter(backend):
    """Test count with filter operation."""
    ds = Dataset.from_list(range(100)).filter(lambda x: x % 2 == 0).count()
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == 50
# ---
def openai_client(self):
            """Return a mock AsyncOpenAI client that returns proper ChatCompletion objects."""
            mock_client = AsyncMock()
            # Configure the mock to return a proper ChatCompletion
            mock_client.chat.completions.create = AsyncMock(return_value=create_mock_chat_completion())
            return mock_client
# ---
def reset_backoff(self) -> None:
        """Reset backoff state (typically after successful operation)."""
        self._consecutive_failures = 0
        self._backoff_until = Timestamp.from_ms(0)
# ---
def combine_masks_or(mask1: NamedArray | None, mask2: NamedArray | None) -> NamedArray | None:
    if mask1 is None:
        return mask2
    if mask2 is None:
        return mask1
    return mask1 | mask2.broadcast_axis(mask1.axes)
# ---
def load_lua():
    """ Use this function if you intend to use mpv's built-in lua interpreter. This is e.g. needed for playback of
    youtube urls. """
    CDLL('liblua.so', mode=RTLD_GLOBAL)
# ---
def bias_dropout_add_scale(
  x: torch.Tensor,
  bias: typing.Optional[torch.Tensor],
  scale: torch.Tensor,
  residual: typing.Optional[torch.Tensor],
  prob: float,
  training: bool,
) -> torch.Tensor:
  if bias is not None:
    out = scale * F.dropout(
      x + bias, p=prob, training=training
    )
  else:
    out = scale * F.dropout(x, p=prob, training=training)

  if residual is not None:
    out = residual + out
  return out
# ---
def prod_Square(n):
    for i in range(2,(n) + 1):
        if (i*i < (n+1)):
            for j in range(2,n + 1):
                if ((i*i*j*j) == n):
                    return True;
    return False;
# ---
def _broadcast_like(x, template):
    return nxp.broadcast_to(x, template.shape)
# ---
def create_dag():
    return nx.MultiDiGraph()
# ---
def get_tx_metadata(self, transaction_hash) -> list:
        with self.lock:
            return self._state.get_tx_metadata(transaction_hash)
# ---
def test_make_blockwise_key_function_contract():
    func = lambda x: 0

    key_fn = make_blockwise_key_function(
        func, "z", "ik", "x", "ij", "y", "jk", numblocks={"x": (2, 1), "y": (1, 2)}
    )

    graph = make_blockwise_graph(
        func, "z", "ik", "x", "ij", "y", "jk", numblocks={"x": (2, 1), "y": (1, 2)}
    )
    check_consistent_with_graph(key_fn, graph)
# ---
def get_item_defaults(item_code, company):
	item = frappe.get_cached_doc('Item', item_code)

	out = item.as_dict()

	for d in item.item_defaults:
		if d.company == company:
			row = copy.deepcopy(d.as_dict())
			row.pop("name")
			out.update(row)
	return out
# ---
def count_and_extract(hash_key, items):
        """Reducer that counts items and extracts content from first item."""
        items_list = list(items)
        return {
            "hash": hash_key,
            "count": len(items_list),
            "content": items_list[0]["content"],
        }
# ---
def test_ref_get_with_invalid_election_id_non_integer_election_id(self):
        response = self.client.get(
            reverse('results-export'),
            { 'election': 'hey' },
            HTTP_REFERER=reverse('results'),
            follow=True
        )

        messages = list(response.context['messages'])
        self.assertEqual(
            messages[0].message,
            'You specified a non-integer election ID.'
        )
        self.assertRedirects(response, reverse('results'))
# ---
def log2(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in log2")
    return elemwise(nxp.log2, x, dtype=x.dtype)
# ---
def test_is_server_error(self):
        self.assertFalse(status.is_server_error(499))
        self.assertFalse(status.is_server_error(600))

        for i in range(500, 599):
            self.assertTrue(status.is_server_error(i))
# ---
def __init__(self):
        super().__init__()
        self._failed = False
        self._slice_info: SliceInfo | None = None
# ---
def test_len_with_drop_last(self):
        """Length should only count complete batches when drop_last=True."""
        # Group sizes: 3, 5, 2 -> batches: 3//2=1, 5//2=2, 2//2=1 = 4
        sampler = EquivalenceGroupBatchSampler.from_dataset_sizes(
            dataset_sizes=[3, 5, 2],
            batch_size=2,
            shuffle=False,
            drop_last=True,
        )
        assert len(sampler) == 4
# ---
def _loglikelihood_tokens(self, requests, disable_tqdm: bool = False):
        raise NotImplementedError("_loglikelihood_tokens is not yet supported")
# ---
def __rxor__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.bitwise_xor(other, self)
# ---
def spherical_area_weights(data: xr.Dataset) -> Grid:
    num_lon = data.lon.size
    lats = torch.from_numpy(data.lat.to_numpy())
    weights = torch.cos(torch.deg2rad(lats)).repeat(num_lon, 1).t()
    weights /= weights.sum()
    return weights
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[T_co]:
        raise NotImplementedError
# ---
def multiples_of_num(m,n): 
    multiples_of_num= list(range(n,(m+1)*n, n)) 
    return list(multiples_of_num)
# ---
def permutation_subjects_ktst(y):
    """Permute class labels of Contextual Disorder dataset for KTST.
    """
    yp = np.random.randint(0, 2, len(y)/2)
    yp = np.concatenate((yp, np.logical_not(yp).astype(int)))
    y_perm = np.arange(len(y))
    for i in range(len(y)/2):
        if yp[i] == 1:
            y_perm[i] = len(y)/2+i
            y_perm[len(y)/2+i] = i
    return y_perm
# ---
def test_concat_different_chunks(spec):
    a = xp.asarray([[1], [5]], chunks=(2, 2), spec=spec)
    b = xp.asarray([[2, 3, 4], [6, 7, 8]], chunks=(2, 3), spec=spec)
    c = xp.concat([a, b], axis=1)
    assert_array_equal(
        c.compute(),
        np.concatenate(
            [
                np.array([[1], [5]]),
                np.array([[2, 3, 4], [6, 7, 8]]),
            ],
            axis=1,
        ),
    )
# ---
def large_company(self):
        """
        :example: 'SOCAR'
        """
        return self.random_element(self.large_companies)
# ---
def test_to_obj():
    msg = '{"aa": 1, "bb": ["hoge", "hogi"], "cc": {"cc1" : 50}}'
    converted = jps.utils.to_obj(msg)
    assert converted.aa == 1
    assert converted.bb[0] == 'hoge'
    assert converted.bb[1] == 'hogi'
    assert len(converted.bb) == 2
    assert converted.cc.cc1 == 50
# ---
def _compute_output_axes(inputs, batch_dims, In, Out):
    """
    Does two things:
    1. Replace In with Out
    2. turn spatial dims (non-batch, non-In, non-Out) into raw names b/c they change size in convolutions
    """
    unchanging_dims = [Out, *batch_dims]
    return [ax.name if ax not in unchanging_dims else ax for ax in replace_axis(inputs.axes, In, Out)]
# ---
def test_capture_and_logging(self, testdir):
        p = testdir.makepyfile("""
            import logging
            def test_log(capsys):
                logging.error('x')
            """)
        result = testdir.runpytest_subprocess(p)
        assert 'closed' not in result.stderr.str()
# ---
def convert_ids_to_tokens(self, token_id):
        """Convert token ID to token string (BPE format)."""
        if isinstance(token_id, list):
            return [self.TOKENS[tid] for tid in token_id]
        return self.TOKENS[token_id]
# ---
def max_of_two( x, y ):
    if x > y:
        return x
    return y
# ---
def test_object_transpose(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)

    assert_array_equal(a.mT.compute(), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).T)

    assert_array_equal(a.T.compute(), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).T)
# ---
def count_list(input_list): 
    return len(input_list)
# ---
def oswritebytes(fd, obj):
    os.write(fd, tobytes(obj))
# ---
def local_keys(self):
        '''
        Return a dict of local keys
        '''
        ret = {'local': []}
        for fn_ in salt.utils.isorted(os.listdir(self.opts['pki_dir'])):
            if fn_.endswith('.pub') or fn_.endswith('.pem'):
                path = os.path.join(self.opts['pki_dir'], fn_)
                if os.path.isfile(path):
                    ret['local'].append(fn_)
        return ret
# ---
def add_pairwise(test_tup):
  res = tuple(i + j for i, j in zip(test_tup, test_tup[1:]))
  return (res)
# ---
def validate(self, task):
        """Check that 'driver_info' contains IPMI credentials.

        Validates whether the 'driver_info' property of the supplied
        task's node contains the required credentials information.

        :param task: a task from TaskManager.
        :raises: InvalidParameterValue if required IPMI parameters
            are missing.
        :raises: MissingParameterValue if a required parameter is missing.

        """
        _parse_driver_info(task.node)
# ---
def job_context(request, ray_cluster):
    if request.param == "sync":
        return SyncContext()
    elif request.param == "threadpool":
        return ThreadContext(max_workers=2)

    return RayContext()
# ---
def asdict_optional(obj):
            return asdict(obj) if obj else None
# ---
def as_dataset(obj):
    """Cast the given object to a Dataset.

    Handles DataArrays, Datasets and dictionaries of variables. A new Dataset
    object is only created in the last case.
    """
    obj = getattr(obj, '_dataset', obj)
    if not isinstance(obj, Dataset):
        obj = Dataset(obj)
    return obj
# ---
def __init__(self, filename="Default.log"):
        self.terminal = sys.stdout
        self.log = open(filename, "a")
# ---
def expiration_datetime(self):
        """Datetime that the verification will expire. """
        days_good_for = settings.VERIFY_STUDENT["DAYS_GOOD_FOR"]
        return self.created_at + timedelta(days=days_good_for)
# ---
def testInt32Random(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.int32)
      y = self._randMatrix(k, m, np.int32)
      self._testCpuMatmul(x, y)
# ---
def discover_vm_groups(self) -> list[FakeVmGroup]:
        """No-op for fake - returns empty list."""
        return []
# ---
def device_password(self):
        if self._values['device_password'] is None:
            return None
        return self._values['device_password']
# ---
def __hash__(self):
        return 0
# ---
def done(self) -> bool:
        return self._future.done()
# ---
def testTryMultipleExcept(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        try:
          raise AssertionError
        except RuntimeError:
          print 'foo'
        except AssertionError:
          print 'bar'
        except:
          print 'baz'""")))
# ---
def test_validate_success_login_form(self):
        # Ensure correct data validates.
        form = LoginForm(email="ad@min.com", password="admin_user")
        self.assertTrue(form.validate())
# ---
def test_prod_sum_bool():
    a = xp.ones((2,), dtype=xp.bool)
    assert_array_equal(xp.prod(a).compute(), xp.asarray([1], dtype=xp.int64))
    assert_array_equal(xp.sum(a).compute(), xp.asarray([2], dtype=xp.int64))
# ---
def build(self, Vocab: Axis, *, key: PRNGKeyArray) -> GrugWrapper:
        grug_cfg = GrugModelConfig(
            vocab_size=Vocab.size,
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            max_seq_len=self.max_seq_len,
        )
        return GrugWrapper.init(Vocab, grug_cfg, key=key)
# ---
def set_test_params(self):
        self.num_nodes = 1
        self.setup_clean_chain = True
        self.extra_args = [[]]
# ---
def __rmatmul__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__rmatmul__")
        if other is NotImplemented:
            return other
        return matmul(other, self)
# ---
def setUp(self):
        self.app = memcache.MemcacheMiddleware(FakeApp(), {})
# ---
def test_add_dicts_base_case(self):
        """add_dict where one argument is None returns the other"""
        f = object()
        self.assertEqual(add_dicts(f, None), f)
        self.assertEqual(add_dicts(None, f), f)
# ---
def output_newline(self):
        return self.renderer.newline()
# ---
def test_invalid_user_pass_returns_auth_error(self):

        self.assertRaises(AuthError, IOSXE, node=node, username='stuff', password='things',
                          disable_warnings=True)
# ---
def get_jobs(self):
        if not self.is_supported:
            return []

        jobs_data = requests.get(JOBS_URL_PATTERN % self.id, auth=SAAGIE_BASIC_AUTH_TOKEN).json()
        return [SaagieJob(self.notebook, job_data) for job_data in jobs_data
                if job_data['category'] == 'processing' and
                job_data['capsule_code'] in self.SUPPORTED_CAPSULE_TYPES]
# ---
def get_plugin_url(queries):
    try:
        query = urllib.urlencode(queries)
    except UnicodeEncodeError:
        for k in queries:
            if isinstance(queries[k], unicode):
                queries[k] = queries[k].encode('utf-8')
        query = urllib.urlencode(queries)

    return sys.argv[0] + '?' + query
# ---
def make_inputs(batch):
        key = jax.random.PRNGKey(batch)
        key_x, key_w, key_y = jax.random.split(key, 3)
        x = jax.random.normal(key_x, (batch, hidden), dtype=jnp.float32)
        w = jax.random.normal(key_w, (hidden, vocab), dtype=jnp.float32)
        y = jax.random.randint(key_y, (batch,), 0, vocab, dtype=jnp.int32)
        return x, w, y
# ---
def healthy(self) -> bool:
        return not self.is_being_preempted()
# ---
def compare(self, param):
        try:
            result = getattr(self, param)
            return result
        except AttributeError:
            return self.__default(param)
# ---
def next_Power_Of_2(n): 
    count = 0; 
    if (n and not(n & (n - 1))): 
        return n   
    while( n != 0): 
        n >>= 1
        count += 1
    return 1 << count;
# ---
def softmax(data, axis):
    r"""Computes softmax.

    .. math:: \text{softmax}(x)_i = \frac{exp(x_i)}{\sum_j exp(x_j)}

    .. note::
        This operator can be optimized away for inference.

    Parameters
    ----------
    data: relay.Expr
        The input data to the operator.

    axis: int
        The axis to sum over when computing softmax
    """

    return _make.softmax(data, axis)
# ---
def flatten_for_export(self: Mod) -> Mod:
        if isinstance(self.axis, hax.Axis):
            return self

        if self.weight is not None:
            weight = self.weight.flatten("__OUT")
        else:
            weight = None

        if self.bias is not None:
            bias = self.bias.flatten("__OUT")
        else:
            bias = None

        return dataclasses.replace(self, weight=weight, bias=bias, axis=hax.flatten_axes(self.axis, "__OUT"))
# ---
def axis_spec_to_shape_dict(axis_spec: AxisSpec) -> dict[str, int]:  # type: ignore
    ...
# ---
def setUp(self):
        self.prepare_sqlite()
# ---
def test_replica_with_invalid_slave_of_id(self, mock_logging):
        self.assertRaises(exception.NotFound,
                          Instance.create,
                          None, 'name', 1, "UUID", [], [], None,
                          self.datastore_version, 1,
                          None, slave_of_id=str(uuid.uuid4()))
# ---
def sleep_and_handle_sigterm():
        def handler(signum, frame):
            sys.exit(0)

        signal.signal(signal.SIGTERM, handler)
        while True:
            time.sleep(1)
# ---
def odd_Days(N): 
    hund1 = N // 100
    hund4 = N // 400
    leap = N >> 2
    ordd = N - leap 
    if (hund1): 
        ordd += hund1 
        leap -= hund1 
    if (hund4): 
        ordd -= hund4 
        leap += hund4 
    days = ordd + leap * 2
    odd = days % 7
    return odd
# ---
def _create_test_curriculum_config() -> CurriculumConfig:
    """Minimal curriculum config with 3 independent lessons."""
    lessons = {
        name: LessonConfig(lesson_id=name, env_config=EnvConfig(env_class="test.FakeEnv", env_args={}))
        for name in ("easy", "medium", "hard")
    }
    return CurriculumConfig(lessons=lessons, max_seq_len=42)
# ---
def shutdown(self, wait: bool = True) -> None:
        self._executor.shutdown(wait=wait)
# ---
def total_flops(self, dtype: str = "bf16") -> float:
        return self.device_flops(dtype) * self.chip_count()
# ---
def __repr__(self):
        return "<patternmatcher patterns=%r>" % self._pats
# ---
def test_einsum_ordered_ellipsis_preserves_axis_order():
    Batch = Axis("batch", 4)
    SeqQ = Axis("seq_q", 4)
    SeqK = Axis("seq_k", 4)
    Head = Axis("head", 2)
    DHead = Axis("dhead", 8)

    q = hax.ones((Batch, SeqQ, Head, DHead))
    k = hax.ones((Batch, SeqK, Head, DHead))

    out = hax.einsum("... s h d, ... t h d -> ... s t h d", q, k)

    assert out.axes == (Batch, SeqQ, SeqK, Head, DHead)
    assert out.array.shape == (Batch.size, SeqQ.size, SeqK.size, Head.size, DHead.size)
# ---
def tri(N, M=None, k=0, typecode=None, dtype=None):
    """ returns a N-by-M array where all the diagonals starting from
        lower left corner up to the k-th are all ones.
    """
    dtype = convtypecode(typecode, dtype)
    if M is None: M = N
    m = np.greater_equal(np.subtract.outer(np.arange(N), np.arange(M)),-k)
    if m.dtype != dtype:
        return m.astype(dtype)
# ---
def test_ckpt_path_invalid_string_path():
    path = "checkpoints/llama/checkpoints/not-a-step-name"
    with pytest.raises(ValueError, match="Invalid path"):
        ckpt_path_to_step_name(path)
# ---
def has_len(self) -> bool:
        return self.dataset.is_finite()
# ---
def test_empty_insert(self):
        """test that execute() interprets [] as a list with no params"""

        testing.db.execute(users_autoinc.insert().
                    values(user_name=bindparam('name', None)), [])
        eq_(testing.db.execute(users_autoinc.select()).fetchall(), [(1, None)])
# ---
def in_qdq(compute_dtype, inp, scale, amax_history):
    qin, _, _ = qdq_and_return(inp, jnp.float8_e4m3fn, scale, amax_history, compute_dtype)
    return qin
# ---
def next(self, psm: PSM):
        if psm.char.isalpha() or psm.char == "_":
            self.parent.g.group.name += psm.char
            return self
        elif psm.char == ">":
            return self.parent
        else:
            psm.error = 'expected a letter, "_" or ">"'
# ---
def execute(self, cmd_name: str, *args, **kwargs):
        """Execute a command by name."""
        if cmd_name in self.commands:
            return self.commands[cmd_name](*args, **kwargs)
        else:
            console.print(f"[red]Unknown command: {cmd_name}[/red]")
# ---
import re
def multiple_split(text):
  return (re.split('; |, |\*|\n',text))
# ---
def test_init_params_no_timestep_embed(params):
    """EditModelParams should NOT have a timestep_embed field."""
    assert not hasattr(params, "timestep_embed")
# ---
def test_apply_gufunc_elemwise_core(spec):
    def foo(x):
        assert x.shape == (3,)
        return 2 * x

    a = cubed.from_array(np.array([1, 2, 3]), chunks=3, spec=spec)
    z = apply_gufunc(foo, "(i)->(i)", a, output_dtypes=int)
    assert z.chunks == ((3,),)
    assert_equal(z, np.array([2, 4, 6]))
# ---
def sub_seek(self, skip):
        self.command('sub_seek', skip)
# ---
def test_other_dtype_annotation():
    def bar(x: i32["batch"]):  # type: ignore  # noqa: F722
        pass

    spec = typing.get_args(typing.get_type_hints(bar, include_extras=True)["x"])[1]
    assert spec.dtype == jnp.int32
    assert spec.before == ("batch",)
# ---
def _shape_bucket(b: int, h: int, v: int) -> Optional[str]:
    for bucket in SHAPE_BUCKETS:
        if bucket.matches(b, h, v):
            return bucket.name
    return None
# ---
def main():
    # Load config from YAML
    cfg = TrainConfig.from_yaml_and_cli()
    cfg.prepare_output_dirs()  # we do this first so logging can use them

    handle_logging(cfg.debug, cfg.experiment.output_dir)
    handle_warnings()

    trainer = Trainer(cfg)

    try:
        trainer.run()
    except Exception as e:
        logger.exception("Training failed with an exception")
        raise e
# ---
def clause(self):
        if self.fast:
            return self.col_clause()
        else:
            # Matching a flexattr. This is a slow query.
            return None, ()
# ---
def setUp(self):
        with mock.patch(
            'airflow.providers.google.cloud.hooks.vision.CloudVisionHook.__init__',
            new=mock_base_gcp_hook_default_project_id,
        ):
            self.hook = CloudVisionHook(gcp_conn_id='test')
# ---
def reset(self) -> None:
        """Free all local sequence slots and reset to the initial `DecodeState`.

        Keeps the KV cache memory allocated. Reuses current `PageTable` object with pages freed.
        """
        self.gen_state = eqx.filter_jit(self.gen_state.reset, donate="all")()
        self.free_slots = list(range(int(self.gen_state.decode_state.max_seqs)))
        self.local_map.clear()
        self.sequences.clear()
        self.results = {}
# ---
def new(self, **kwargs):
        ctx = Ctx()
        ctx._parent = self
        for name, value in kwargs.iteritems():
            setattr(ctx, name, value)
        return ctx
# ---
def test_quantile_parallel_float_nan(self):
        def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float32)})
            df.A[0:100] = np.nan
            df.A[200:331] = np.nan
            return df.A.quantile(.25)

        hpat_func = self.jit(test_impl)
        n = 1001
        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def string_length(str1):
    count = 0
    for char in str1:
        count += 1
    return count
# ---
def _get_gemma3_config(use_flash=False, num_kv_heads=4, seq_len=128) -> Gemma3Config:
    from levanter.models.gemma import Gemma3Config

    return Gemma3Config(
        max_seq_len=seq_len,
        hidden_dim=16,
        num_heads=4,
        num_kv_heads=num_kv_heads,
        gradient_checkpointing=False,
        use_flash_attention=use_flash,
        flash_attention_block_size=8 if use_flash else None,
        head_dim=4,
        query_pre_attn_scalar=4,
        sliding_window=seq_len,
    )
# ---
def list_image_metadata(self, image_id):
        """Lists all metadata items for an image."""
        resp, body = self.get("images/%s/metadata" % str(image_id))
        body = json.loads(body)
        self.validate_response(schema.image_metadata, resp, body)
        return service_client.ResponseBody(resp, body['metadata'])
# ---
def _bbox_in_coverage_srs(self, bbox, srs):
        if srs != self.srs:
            bbox = srs.transform_bbox_to(self.srs, bbox)
        return bbox
# ---
def _combine_ffmpeg_command(
    sourcefolder, moviename, framerate, frame_pattern, ffmpeg_options
):
    # we need `-y` because i can not properly diagnose the errors here...
    command = 'ffmpeg -r %i -i "%s" -y %s -r %i "%s"' % (
        framerate,
        os.path.join(sourcefolder, frame_pattern),
        ffmpeg_options,
        framerate,
        os.path.join(sourcefolder, moviename),
    )
    return command
# ---
def _make_grug_mesh() -> Mesh:
    devices = jax.devices()
    if not devices:
        raise RuntimeError("No JAX devices available")
    mesh_devices = np.array(devices).reshape(len(devices), 1)
    return Mesh(
        mesh_devices,
        axis_names=("data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit),
    )
# ---
def _hf_auth_headers() -> dict[str, str]:
    for env_var in HF_TOKEN_ENV_VARS:
        token = os.environ.get(env_var)
        if token:
            return {"Authorization": f"Bearer {token}"}
    return {}
# ---
def tricky():
        try:
            print('Tricky called')
            return 1
        finally:
            print('Tricky finally called')
            return 42
        return 0
# ---
def test_iris_run_cli_job_failure(local_cluster_and_config, tmp_path):
    """Test iris_run.py returns non-zero on job failure."""
    _test_config, url, _client = local_cluster_and_config

    test_script = tmp_path / "fail.py"
    test_script.write_text("exit(1)")

    exit_code = run_iris_job(
        controller_url=url,
        command=[sys.executable, str(test_script)],
        env_vars={},
        wait=True,
    )

    assert exit_code == 1
# ---
def init_log(self, tail: int | None = None) -> str:
        return ""
# ---
def __init__(
        self,
        start: float = 0.0,
        stop: float = 5.0,
        num_gaussians: int = 50,
    ) -> None:
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        self.num_gaussians = num_gaussians
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer("offset", offset)
# ---
def to_unbatched_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            if selects_axis(leaf.main_axes, axis_to_strip):
                return leaf.strip_axis(axis_to_strip)
            else:
                return leaf.to_named_array()
        else:
            return leaf
# ---
def get_metrics(self, trainer, pl_module):
        """Override get_metrics to update progress bars on each metric update."""
        metrics = super().get_metrics(trainer, pl_module)
        self._update_all_progress_bars()
        return metrics
# ---
def test_raise(self):
        # raising URLError stops processing of request
        o = OpenerDirector()
        meth_spec = [
            [("http_open", "raise")],
            [("http_open", "return self")],
            ]
        handlers = add_ordered_mock_handlers(o, meth_spec)

        req = Request("http://example.com/")
        self.assertRaises(urllib.error.URLError, o.open, req)
        self.assertEqual(o.calls, [(handlers[0], "http_open", (req,), {})])
# ---
def speech_sequence(self, n):
        speech_acts_seq = []
        next_speech_id = 0
        for i in range(n):
            next_speech_id = searchsorted(cumsum(self._acts_transitions), rand() * sum(self._acts_transitions))
            speech_acts_seq.append(self._speech_acts[next_speech_id])
        return speech_acts_seq
# ---
def _merge_dataset(self, other, overwrite_vars, compat, join):
    aligned_self, other = partial_align(self, other, join=join, copy=False)

    replace_vars, new_vars, new_coord_names = _merge_expand(
        aligned_self, other._variables, overwrite_vars, compat)
    new_coord_names.update(other._coord_names)

    return replace_vars, new_vars, new_coord_names
# ---
def in_qdq_fwd(compute_dtype, inp, scale, amax_history):
    qin, new_scale, new_history = qdq_and_return(inp, jnp.float8_e4m3fn, scale, amax_history, compute_dtype)
    return qin, (new_scale, new_history)
# ---
def volume(self):
        raise NotImplemented
# ---
def list_endpoints_by_prefix(self, prefix: str) -> list[ControllerEndpoint]:
        """List endpoints matching a name prefix for non-terminal jobs."""
        with self._lock:
            return self._visible_endpoints(lambda ep: ep.name.startswith(prefix))
# ---
def _parse_rows(self):
        """Parse rows from the message only once."""
        if self._iter_rows is not None:
            return

        rows = self._stream_parser.to_rows(self._message)
        self._iter_rows = iter(rows)
# ---
def __init__(self, *a, **k):
        super(SessionRecordingComponent, self).__init__(*a, **a)
        self.set_trigger_recording_on_release(not (self._record_button.is_pressed))
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {"token_embeddings": "model.embed_tokens"}
# ---
def mode(self) -> Literal["native", "docker"]:
        if self.docker_container_name:
            return "docker"
        if self.process is not None or self.log_dir is not None:
            return "native"
        raise RuntimeError("Unable to infer vLLM server mode from handle state.")
# ---
def record_batch(
        self,
        target_data: dict[str, torch.Tensor],
        gen_data: dict[str, torch.Tensor],
        target_data_norm: dict[str, torch.Tensor],
        gen_data_norm: dict[str, torch.Tensor],
        i_time_start: int = 0,
    ):
        if self._target == "norm":
            target_data = target_data_norm
            gen_data = gen_data_norm
        self._target_agg.record_batch(target_data, i_time_start)
        self._gen_agg.record_batch(gen_data, i_time_start)
# ---
def stop(self) -> None:
        """Stop the controller and clean up resources.

        Shutdown ordering:
        1. Stop the controller (which stops its threads and autoscaler)
        2. Wait on the root ThreadContainer to verify all threads have exited
        """
        if self._controller:
            self._controller.stop()
            self._controller = None
            logger.info("Controller stopped")

        self._threads.wait()
# ---
def add_hook(self, fn: JitCallback, *, every: int = 1): ...
# ---
def tuple_to_float(test_tup):
  res = float('.'.join(str(ele) for ele in test_tup))
  return (res)
# ---
def __init__(self):
        c_ptr = SummaryKeyMatcher.cNamespace().alloc()

        super(SummaryKeyMatcher, self).__init__(c_ptr)
# ---
def visit_msup(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            sup = self._visit(children[1])
            return BracedNode(f"{{{base}}}^{{{sup}}}")
        return TextNode("")
# ---
def norm_config(self) -> LayerNormConfigBase:
        return RmsNormConfig(
            use_weight=self.use_layer_norm_weight,
            use_bias=self.use_bias,
            eps=self.layer_norm_epsilon,
        )
# ---
def list_jobs(self) -> list[cluster_pb2.JobStatus]:
        request = cluster_pb2.Controller.ListJobsRequest()
        response = self._client.list_jobs(request)
        return list(response.jobs)
# ---
def inferred_head_dim(self) -> int:
        if self.head_dim is not None:
            return self.head_dim
        if self.hidden_dim % self.num_heads != 0:
            raise ValueError(
                f"hidden_dim={self.hidden_dim} is not divisible by num_heads={self.num_heads}; set head_dim explicitly"
            )
        return self.hidden_dim // self.num_heads
# ---
def accept_all(self, include_rejected=False):
        self._call_all('accept_all', include_rejected)
# ---
def getAtomFeed(url, login, pwd):
	# var
	MAX_TRY = 10
	essai = 0

	# get atom document
	while essai < MAX_TRY:
		try:
			r = requests.get('http://' + url, auth=(login,pwd), timeout=10)
		except:
			essai += 1
			continue
		break
	else:
		raise ('Erreur lors de la requte')

	# parse atom document
	try:
		dom = xml.dom.minidom.parseString(r.text)
	except:
		raise ('Erreur lors du parsing du document Atom')

	return dom
# ---
def test_opposites_task_reward():
    task = OppositesTask()
    examples = task.generate_examples(10, np.random.default_rng(42))
    assert len(examples) == 10

    assert task.compute_reward("cold", "cold") == pytest.approx(1.0)
    assert task.compute_reward("cold", "warm") == pytest.approx(0.0)
# ---


def same_chars(s0: str, s1: str):
    """
    Check if two words have the same characters.
    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddeddabc')
    True
    >>> same_chars('abcd', 'dddddddabc')
    True
    >>> same_chars('dddddddabc', 'abcd')
    True
    >>> same_chars('eabcd', 'dddddddabc')
    False
    >>> same_chars('abcd', 'dddddddabce')
    False
    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddddabc')
    False
    """
    return set(s0) == set(s1)
# ---
def __init__(self, fichier, player):
        self.fichier = fichier
        self.grille = self.getFirstGrid()
        self.best_hit = 0
        self.players = player
# ---
def receive_serialdata(self, time, data):
        self.loggingWidget.log_input(data)

        try:
            self.rootnode.from_json(data)
        except ValueError as e:
            logger.error(str(e))

        # refresh widgets
        self.objectexplorer.refresh()
        self.plot.refresh(time)
        if self.recording_enabled:
            self.recordWidget.add_data(time, self.rootnode)
# ---
def count_elim(num):
  count_elim = 0
  for n in num:
    if isinstance(n, tuple):
        break
    count_elim += 1
  return count_elim
# ---
def _run_async_task(self, coro):
        if not self.loop.is_running() or not self.thread.is_alive():
            raise StopIteration
        try:
            future = asyncio.run_coroutine_threadsafe(coro, self.loop)
            return future.result()
        except (RuntimeError, asyncio.CancelledError):
            raise StopIteration
# ---
def _stop_heartbeat(self) -> None:
        """Stop the heartbeat thread."""
        self._stop_event.set()
        if self._heartbeat_thread is not None:
            self._heartbeat_thread.join(timeout=5)
# ---
def position_from_token(self, token_id: int) -> int:
        """Extract the position index from a position token ID.

        Raises ValueError if the token is not a position token.
        """
        if not self.is_position_token(token_id):
            raise ValueError(f"Token {token_id} is not a position token")
        return token_id - self.position_token_offset
# ---
def set_mouse_cursor(self, cursor=None):
        """Change the appearance of the mouse cursor.

        The appearance of the mouse cursor is only changed while it is
        within this window.

        :Parameters:
            `cursor` : `MouseCursor`
                The cursor to set, or None to restore the default cursor.

        """
        if cursor is None:
            cursor = DefaultMouseCursor()
        self._mouse_cursor = cursor
        self.set_mouse_platform_visible()
# ---
def _on_record_button_released(self):
        if self._should_trigger_recording:
            self._trigger_recording()

        self._should_trigger_recording = True
# ---
def asr_model_type(cls) -> Type["ASRMixin"]:
        pass
# ---
def get_target_time(self, start_step: int, num_steps: int):
        x_index = self._get_x_index(start_step)
        batch_index = x_index.values[0]
        steps_predicted = len(batch_index) // 2
        start_target_index = batch_index[steps_predicted]

        return self._times.isel(
            time=slice(
                start_target_index, start_target_index + num_steps * steps_predicted
            )
        )
# ---
def normalize_shape(shape: Union[int, Tuple[int, ...], None]) -> Tuple[int, ...]:
    """Normalize a `shape` argument to a tuple of ints."""

    if shape is None:
        raise TypeError("shape is None")

    if isinstance(shape, numbers.Integral):
        shape = (int(shape),)

    shape = cast(Tuple[int, ...], shape)
    shape = tuple(int(s) for s in shape)
    return shape
# ---
def should_display_status_to_user(self):
        """Whether or not the status from this attempt should be displayed to the user."""
        raise NotImplementedError
# ---
def _stop(self) -> threading.Event:
        return self._managed_thread.stop_event
# ---

def special_factorial(n):
    """The Brazilian factorial is defined as:
    brazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!
    where n > 0

    For example:
    >>> special_factorial(4)
    288

    The function will receive an integer as input and should return the special
    factorial of this integer.
    """
    fact_i = 1
    special_fact = 1
    for i in range(1, n+1):
        fact_i *= i
        special_fact *= fact_i
    return special_fact
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[True] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[False] = ...,
) -> tuple[NamedArray, Array, Array, Array, Array]: ...
# ---
def __repr__(self) -> str:
        return (
            f"cubed.Spec(work_dir={self._work_dir}, allowed_mem={self._allowed_mem}, "
            f"reserved_mem={self._reserved_mem}, executor={self._executor}, storage_options={self._storage_options}, zarr_compressor={self._zarr_compressor})"
        )
# ---
def _validate_vm_types(config: config_pb2.IrisClusterConfig) -> None:
    """Validate that scale groups have explicit vm_type."""
    for name, sg_config in config.scale_groups.items():
        if sg_config.vm_type == config_pb2.VM_TYPE_UNSPECIFIED:
            raise ValueError(f"Scale group '{name}' must set vm_type to tpu_vm, gce_vm, manual_vm, or local_vm.")
# ---
def _pipeline() -> int:
        docs = pq.read_table(small_parquet_path).to_pylist()
        return len([dupekit.process_dicts_loop(doc) for doc in docs])
# ---
def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "boing"}
# ---
def _broadcast_trick_inner(func, shape, *args, **kwargs):
    # cupy-specific hack. numpy is happy with hardcoded shape=().
    null_shape = () if shape == () else 1

    return nxp.broadcast_to(func(*args, shape=null_shape, **kwargs), shape)
# ---
def get(self):
		return ord(self.read(1))
# ---
def get_properties(self):
        return COMMON_PROPERTIES
# ---
def test_mods_to_tei(datadir):
    main("--inplace", datadir / "mods_to_tei.py", datadir / "mods_to_tei.xml")
    assert equal_documents(datadir / "mods_to_tei.xml", datadir / "mods_to_tei_exp.xml")
# ---
def vendi_from_sim(mat):
    mat = mat + mat.T
    np.fill_diagonal(mat, 1.0)
    eigvals, _ = np.linalg.eigh(mat / len(mat))
    eigvals = np.clip(eigvals, 0.0, None)
    return np.exp(np.nansum(-(eigvals * np.log(eigvals))))
# ---
def test_searchsorted_sorter_not_implemented():
    with pytest.raises(NotImplementedError):
        xp.searchsorted(xp.asarray([1, 0]), xp.asarray([1]), sorter=xp.asarray([1, 0]))
# ---
def init(self, run_id: Optional[str]) -> JsonLoggerTracker:
        del run_id
        log = logging.getLogger(self.logger_name)
        log.setLevel(self.level)
        return JsonLoggerTracker(log)
# ---
def setUp(self):
        self.get = Request("http://www.python.org/~jeremy/")
        self.post = Request("http://www.python.org/~jeremy/",
                            "data",
                            headers={"X-Test": "test"})
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step, commit=None):
        del commit
        to_log = {}
        for k, v in metrics.items():
            to_log[k] = _convert_value_to_loggable_rec(v)

        import trackio

        trackio.log(to_log, step=step)
# ---
def format_exception_with_traceback(exc: Exception) -> str:
    """Format an exception with its full traceback as a string.

    Suitable for embedding in JobStatus.error field.
    """
    tb = "".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
    return f"{type(exc).__name__}: {exc}\n\nTraceback:\n{tb}"
# ---
def dict_filter(dict,n):
 result = {key:value for (key, value) in dict.items() if value >=n}
 return result
# ---
def lps(str): 
	n = len(str) 
	L = [[0 for x in range(n)] for x in range(n)] 
	for i in range(n): 
		L[i][i] = 1
	for cl in range(2, n+1): 
		for i in range(n-cl+1): 
			j = i+cl-1
			if str[i] == str[j] and cl == 2: 
				L[i][j] = 2
			elif str[i] == str[j]: 
				L[i][j] = L[i+1][j-1] + 2
			else: 
				L[i][j] = max(L[i][j-1], L[i+1][j]); 
	return L[0][n-1]
# ---
def _zero_if_array_else_none(x: Any) -> ResolvedUnnamedAxisSpec:
    return 0 if is_jax_array_like(x) else None
# ---
def __init__(self, start: int = 0):
        self._value = start
# ---
def calculate_volatility(self, daily_returns):
        return np.std(daily_returns, ddof=1) * math.sqrt(self.num_trading_days)
# ---
def test_trainer__mini_benchmark(trainer_pair: TrainPair, caplog, benchmark):
    caplog.set_level(logging.INFO)
    _, trainer = trainer_pair

    @benchmark
    def run():
        trainer.run()
# ---
def test_on_end_date(self):
        """Test when create_time is exactly on the end date"""
        self.assertTrue(check_create_time("2023-01-31 23:59:59 PST", "2023-01-01", "2023-01-31"))
# ---
def get_env_var(self, key):
        '''return a environment variables '''
        results = self.get(DeploymentConfig.env_path) or []
        if not results:
            return None

        for env_var in results:
            if env_var['name'] == key:
                return env_var

        return None
# ---
def remove_tuple(test_tup):
  res = tuple(set(test_tup))
  return (res)
# ---
def visit_mn(self, element):
        return TextNode(element.get_text().strip())
# ---
def unstacked(self) -> Sequence[M]:
        """
        Returns the unstacked version of this module. This is useful for logging or saving checkpoints.

        """
        ...
# ---
def filter(self, record: Record) -> bool:
        """Filter a data record.

        Parameters
        ----------
        record : Record
            The object to consider filtering in / out.

        Returns
        -------
        bool
            True if the data passes the filter, False otherwise.

        """
        raise NotImplementedError
# ---
def func(block, block_id=None):
        return nxp.ones_like(block) * int(sum(block_id))
# ---
def test_str_split(self):
        def test_impl(df):
            return df.A.str.split(',')

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D', 'G', '', 'g,f']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def __init__(self, remote, match_control, *args, **kwargs):
        self.remote = remote
        super(ScheduleControl, self).__init__(*args, **kwargs)
        self.InitUI()
        self.remote.match_list_box = self.match_list
        self.match_control = match_control
# ---
def avatarExitNextState(self):
        if len(self.nearbyAvatars) == 0:
            if self.fsm.getCurrentState().getName() != 'Walk':
                self.fsm.request('Lonely')
# ---
def test_to_zarr(tmp_path, spec, executor, path):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    store = tmp_path / "output.zarr"
    cubed.to_zarr(a, store, path=path, executor=executor)
    res = open_backend_array(store, mode="r", path=path)
    assert_array_equal(res[:], np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
# ---
def direct_fn(pred):
        pred_embeddings, pred_lm_head = pred
        logits = hax.dot(pred_embeddings, pred_lm_head, axis="embed")
        target_y = hax.nn.one_hot(true_ids, Vocab, dtype=pred_embeddings.dtype)
        loss, logz = cross_entropy_loss_and_log_normalizers(logits, Vocab, target_y)
        return (loss + 0.5 * (logz**2)).mean().scalar()
# ---
def get_diff(b):
    '''Just the internals of the difference method'''
    return b['b2'].subtract(b['b1']).select(['sur_refl_b02'], ['b1'])
# ---
def __init__(self, ova_path, vminfo, vmid, irs):
        super(OvaCommand, self).__init__(vminfo, vmid, irs)
        self._ova_path = ova_path
# ---
def __repr__(self):
        return "<intersectionmatcher m1=%r, m2=%r>" % (self._m1, self._m2)
# ---
def format_rare_fraction(self) -> str:
        if self.rare_fraction >= 0.01:
            return f"{self.rare_fraction:.2f}"
        else:
            return f"{self.rare_fraction:.3f}"
# ---
def test_before_start_date(self):
        """Test when create_time is before the start date"""
        self.assertFalse(check_create_time("2022-12-31 23:59:59 PST", "2023-01-01", "2023-01-31"))
# ---
def mix_qkvz_axis(self) -> Axis:
        # [Q | K | V | Z]; the layer projects all at once
        return Axis("qkvz", self.key_dim * 2 + self.value_dim * 2)
# ---
def d(self, id=""):
        html = "<h2>Delete</h2>"
        if id == "" or id == None :
            html += "Error"
            return html

        if id == "0" :
          html += "0 is reserved, sorry"
          return html

        #if delete(id) == False:
        if nonexist(id) == False:
            html += "Delete error in id" % id
            html += getfooter()
            return html

        html += "Item %s set as non existent" % id
        return html
# ---
def init(axis: Axis, eps: float = 1e-6) -> "GatedRmsNorm":
        return GatedRmsNorm(axis=axis, weight=hax.ones(axis), eps=eps)
# ---
def static_method(arg):
        return arg
# ---
def _get_task(self) -> PendingTask | None:
        """Try to get a task from the queue."""
        try:
            return self._task_queue.get(timeout=0.5)
        except Empty:
            return None
# ---
def print_list_field(label, values):
    print_field(label, ('-' if len(values) == 0 else DELIMITER(', ').join(values)))
# ---
def find_Element(arr,ranges,rotations,index) :  
    for i in range(rotations - 1,-1,-1 ) : 
        left = ranges[i][0] 
        right = ranges[i][1] 
        if (left <= index and right >= index) : 
            if (index == left) : 
                index = right 
            else : 
                index = index - 1 
    return arr[index]
# ---
def kill_ray():
                            # silence spam from ray stop
                            os.system("bash -c 'ray stop -g 10 --force &> /dev/null'")
# ---
def tuple_to_dict(test_tup):
  res = dict(test_tup[idx : idx + 2] for idx in range(0, len(test_tup), 2))
  return (res)
# ---
def compute_grpo_loss(
    loss_objective: jax.Array,
    loss_masks: jax.Array,
    max_output_tokens: int,
) -> jax.Array:
    """Compute GRPO loss (token-level loss)."""
    return -1 * jnp.mean(jnp.sum(loss_objective * loss_masks, axis=1) / max_output_tokens)
# ---
def virtual_full(
    shape: T_Shape,
    fill_value: Any,
    *,
    dtype: T_DType,
    chunks: T_RegularChunks,
    **kwargs,
) -> VirtualFullArray:
    return VirtualFullArray(shape, dtype, chunks, fill_value, **kwargs)
# ---
def blank_req(self, path, environ = None, base_url = None, headers = None, POST = None, **kw):
        env = environ.copy() if environ else {}
        etalage_env = env.setdefault('etalage', {})
        for key in self.env_keys:
            value = getattr(self, key)
            if value is not None:
                etalage_env[key] = value
        return webob.Request.blank(path, environ = env, base_url = base_url, headers = headers, POST = POST, **kw)
# ---
def ndim(self):
        """Number of array dimensions (axes)."""
        return len(self.shape)
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        alpha_p = hnn.softplus(self.alpha_p)
        alpha_n = hnn.softplus(self.alpha_n)
        beta = self.beta
        eps = self.eps
        alpha_n = beta + alpha_n

        positive = alpha_p * x * x + beta * x
        negative = (hax.expm1(hax.minimum(x, eps)) - x) * alpha_n + beta * x
        return hax.where(x > 0, positive, negative)
# ---
def testFormatDisplayName(self):
    """Tests the _FormatDisplayName function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    display_name_string = test_helper._FormatDisplayName(
        event, event_data, event_data_stream)
    self.assertEqual(display_name_string, 'FAKE:log/syslog.1')
# ---
def method3(self, g=10, h=20):
        return g * h
# ---
def get_location(self):
        return self.location
# ---
def init(cls):
        cfg = aqt_config.config_v3()
        return cls(cfg)
# ---
def new(self):
        self.objectexplorer.model().beginResetModel()
        self.rootnode.clear()
        self.objectexplorer.model().endResetModel()
# ---
def col_clause(self):
        return self.field + " IS NULL", ()
# ---
def Caller_Hears_Dialtone (self):
        self.Step (Message = "Caller hears dial-tone...")

        self.Log (Message = "Caller agent waits for dial-tone...")
        self.Caller.sip_phone.Wait_For_Dialtone ()
# ---
def __enter__(self):
        import levanter.tracker.tracker_fns as tracker_fns

        if hasattr(self, "_tracker_cm"):
            raise RuntimeError("This tracker is already set as the global tracker")
        setattr(self, "_tracker_cm", tracker_fns.current_tracker(self))
        self._tracker_cm.__enter__()
# ---
def log_step_info(total_steps: Optional[int]):
    def log_step_info_inner(step: StepInfo):
        metrics = {"train/loss": step.loss, "global_step": step.step}
        if total_steps:
            metrics["run_progress"] = step.step / total_steps
        log_optimizer_hyperparams(step.opt_state, step=step.step, prefix="optim")
        levanter.tracker.log(metrics, step=step.step)

    return log_step_info_inner
# ---
def stack_converter(st):
        return [lookup[element] for element in st[::-1]]
# ---
def compute_reward(self, correct_answer: str, actual_response: str, tokenizer=None) -> float:
        return compute_soft_reward(correct_answer, actual_response)
# ---
from itertools import groupby
def modified_encode(alist):
        def ctr_ele(el):
            if len(el)>1: return [len(el), el[0]]
            else: return el[0]
        return [ctr_ele(list(group)) for key, group in groupby(alist)]
# ---
def before_insert(self):
		if not self.description:
			self.description = self.item_name
# ---
def tuple_intersection(test_list1, test_list2):
  res = set([tuple(sorted(ele)) for ele in test_list1]) & set([tuple(sorted(ele)) for ele in test_list2])
  return (res)
# ---
def __init__(self):
        self._count = 0
# ---
def capture_time():
    start = time.perf_counter()
    done = False

    def fn():
        if done:
            return end - start
        else:
            return time.perf_counter() - start

    yield fn
    end = time.perf_counter()
    done = True
# ---
def __call__(
        self, x: NamedArray, attn_mask: NamedArray | AttentionMask | None, *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)

        return x
# ---
def is_ray_initialized() -> bool:
    """Check if Ray is initialized without requiring callers to import Ray."""
    return ray.is_initialized()
# ---
def resize_vocab(self, new_size: int, key: Optional[PRNGKeyArray] = None) -> "ASRMixin":
        """
        Resizes the vocabulary of the ASR Output space. Key may be provided to use random initialization, otherwise,
        there should be some deterministic initialization of any new parameters.
        """
        pass
# ---
def __init__(self):
        self.lc = lcm.LCM('udpm://239.255.76.67:7667?ttl=1')
        self.lc.subscribe('Schedule/Schedule', self.handle_schedule)
        self.lc.subscribe('Timer/Time', self.handle_time)
        self.match_list_box = None
        self.match_control = None
        self.thread = threading.Thread(target=self._loop)
        self.thread.daemon = True
# ---
def pass_through_fn(rngkey, qs, grads_in, bal_counter):
            if have_qs_sharding:
                qs = _safe_sharding_constraint(qs, Qs_sharding)
            return qs, bal_counter
# ---
def init_fn(key, use_b):
        k_a, k_b = jax.random.split(key)
        return MyModule(a=hax.random.normal(k_a, (In, Out)), b=hax.random.normal(k_b, (In, Out)) if use_b else None)
# ---
def test_str_flatten(self):
        def test_impl(df):
            A = df.A.str.split(',')
            return pd.Series(list(itertools.chain(*A)))

        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})
        hpat_func = self.jit(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def add_string(list,string):
 add_string=[string.format(i) for i in  list]
 return add_string
# ---
def print_json_field(label, json_value):
    print_field(label, json.dumps(json_value, ensure_ascii=False))
# ---
def _is_url_like(path):
    return urllib.parse.urlparse(path).scheme != ""
# ---
def read_files(self, tfile, members):
        '''
        array with txt data from tarfile object
        '''
        self.data = [tfile.extractfile(member).read() for member in members if
                     tfile.extractfile(member) is not None]
# ---
def __ne__(self, other):
        """
        Returns true if both objects are not equal
        """
        return not self == other
# ---
def process_dataset(config: ConversationToDolmaConfig):
    """Convert conversation format to Dolma format."""
    pipeline = (
        Dataset.from_files(f"{config.input_path}/**/*.jsonl.gz", empty_glob_ok=False)
        .flat_map(load_jsonl)
        .map(transform_conversation_to_dolma)
        .write_jsonl(f"{config.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )
    Backend.execute(pipeline)
# ---
def _load_puzzle_file(self):
        filePath = "{dir}/{f}".format(dir=os.getcwd(), f=self._file_name)
        try:
            with open(filePath, mode='r') as puzzle_file:
                self._puzzle_input = puzzle_file.readlines()
        except IOError as err:
            errorMsg = (
                "ERROR: Failed to read the puzzle input from file '{file}'\n"
                "{error}"
            )
            print(errorMsg.format(file=self._file_name, error=err))
            exit(1)
# ---
def sum_digits(n):
  if n == 0:
    return 0
  else:
    return n % 10 + sum_digits(int(n / 10))
# ---
def finished_task_count(self) -> int:
        """Count of tasks in terminal states."""
        return sum(self.task_state_counts[s] for s in TERMINAL_TASK_STATES)
# ---
def __init__(self, dataset: AsyncDataset[T_co], key: PRNGKeyArray, perm_type: PermType = "feistel"):
        super().__init__()
        self.dataset = dataset
        self.key = key
        self._permutation: Optional[Permutation] = None
        self._perm_type = perm_type
# ---
def as_axis(ax_name: str) -> Ax:
        if spec[ax_name] is None:
            return ax_name  # type: ignore
        else:
            return Axis(ax_name, spec[ax_name])
# ---
def runTest(self):
    self.assertEqual(self.schedule.GetHoursRanges(0), [(4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11)])
    self.assertEqual(self.schedule.GetHoursRanges(3), [(28, 30), (30, 31), (31, 32), (32, 34), (34, 35), (35, 36), (36, 37)])
    self.assertEqual(self.schedule.GetHoursRanges(5), [(48, 49), (49, 50), (50, 52), (52, 53), (53, 54), (54, 56), (56, 57)])
# ---
def find(n,m):  
    q = n//m 
    return (q)
# ---
def testDeleteLocal(self):
    self.assertEqual((0, 'ok\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          bar = 123
          del bar
          try:
            print bar
            raise AssertionError
          except UnboundLocalError:
            print 'ok'
        foo()""")))
# ---
def newphoto():
   global image1
   image1 =  takephoto()

   tkimage1 = ImageTk.PhotoImage(image1)
   panel1.configure(image=tkimage1)
   panel1.image = tkimage1
# ---
def increment(self) -> None:
        self._count += 1
# ---
def location(self):
        program = self.program()
        return program.program_family.name if program else None
# ---
def heartbeat(self) -> str:
        return f"TPU allocation '{self.tpu_name}' active for {self.username}"
# ---
def get_index_from_jbor(thing):
    '''
    :returns: Array index of the JBOR if applicable; None otherwise

    Assumes :func:`is_job_ref` evaluates to True
    '''
    if '$dnanexus_link' in thing:
        return thing['$dnanexus_link'].get('index')
    else:
        return None
# ---
from math import radians, sin, cos, acos
def distance_lat_long(slat,slon,elat,elon):
 dist = 6371.01 * acos(sin(slat)*sin(elat) + cos(slat)*cos(elat)*cos(slon - elon))
 return dist
# ---
def badmatch(match, badfn):
    """Make a copy of the given matcher, replacing its bad method with the given
    one.
    """
    m = copy.copy(match)
    m.bad = badfn
    return m
# ---
def testFloatRandomTransposeBoth(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(k, n, np.float32)
      y = self._randMatrix(m, k, np.float32)
      self._testCpuMatmul(x, y, True, True)
      self._testGpuMatmul(x, y, True, True)
# ---
def quick_gelu(x):
    return x * sigmoid(1.702 * x)
# ---
def unregister(self, endpoint_id: str) -> None:
        """Unregister a previously registered endpoint.

        Args:
            endpoint_id: ID returned from register()
        """
        ...
# ---
def scan_via(
        self, fn: ScanFunction[M, CarryT, P, OutputT_co], *, unroll: int | bool | None = None
    ) -> Callable[Concatenate[CarryT, P], tuple[CarryT, OutputT_co]]: ...
# ---
def test_creates_correct_controller_type(self, config_fixture: str, expected_type: type, request):
        """create_controller returns correct controller type based on config."""
        config = request.getfixturevalue(config_fixture)
        controller = create_controller_vm(config)
        assert isinstance(controller, expected_type)
# ---
def test_hashing(benchmark: Any, sample_batch: pa.RecordBatch, granularity: str, backend: str) -> None:
    """Benchmark the hash generation step."""
    func = PROCESS_FUNCS[(granularity, backend)]
    benchmark.group = f"{granularity.title()}: Hash Generation"
    benchmark(func, sample_batch, "text", "id")
# ---
def synthetic_target_dir(self, target, target_workdir):
    all_sources = list(target.sources_relative_to_buildroot())
    source = all_sources[0]
    namespace = self._get_go_namespace(source)
    return os.path.join(target_workdir, 'src', 'go', namespace.replace(".", os.path.sep))
# ---
def alternate_elements(list1):
    result=[]
    for item in list1[::2]:
        result.append(item)
    return result
# ---
def __init__(
        self,
        loss_fn: LossFnWithContext,
        *,
        limit: float | None,
        device: torch.device,
        num_channels: int,
    ):
        self.loss_fn = loss_fn
        self._device = device
        self._per_channel_scale: Float[torch.Tensor, " var"] = torch.ones(
            num_channels, device=self._device
        )
        self._limit = limit
# ---
def remove(self, tag: str) -> None:
        subprocess.run(
            ["docker", "rmi", tag],
            capture_output=True,
            check=False,
        )
# ---
def run_aggregate_total(config: AggregateConfig) -> str:
    logger.info(f"Starting train-test overlap aggregation with config: {config}")
    aggregate_total(config)
    logger.info(f"Aggregation completed! Results written to {config.output_path}")
    return config.output_path
# ---
def match(self, item):
        """Check whether this query matches a given Item. Can be used to
        perform queries on arbitrary sets of Items.
        """
        raise NotImplementedError
# ---
def exec_module(old, module):
        old(module)
        if module.__name__ in _DISTUTILS_PATCH:
            patch_dist(module)
# ---
def test_exceed_max_pack_size():
    Pos = hax.Axis("pos", size=10)
    packer = SequencePacker(Pos=Pos, max_pack_size=2, pad_token=0)

    packer.add_example([1, 2, 3], [1, 1, 1])
    packer.add_example([4, 5, 6], [1, 1, 1])

    with pytest.raises(ValueError, match="Too many segments"):
        packer.add_example([7, 8], [1, 1])
# ---
def stop_wrapper():
                        stop_called.append(vm_id)
                        return orig_stop()
# ---
def stop(self):
        self._patcher.stop()
        self._patcher2.stop()
        self._server_mock = None
# ---
def get_console(self, task):
        """Get the type and connection information about the console."""
        driver_info = _parse_driver_info(task.node)
        url = console_utils.get_shellinabox_console_url(driver_info['port'])
        return {'type': 'shellinabox', 'url': url}
# ---
def output_reflink(self, m):
        key = _keyify(m.group(2) or m.group(1))
        if key not in self.links:
            return None
        ret = self.links[key]
        return self._process_link(m, ret['link'], ret['title'])
# ---
def update_stats(self, stats, delta=1, sampleRate=1):
        """
        Updates one or more stats counters by arbitrary amounts
        """
        if not self.enabled or self.addr is None:
            return

        if type(stats) is not list:
            stats = [stats]
        data = {}
        for stat in stats:
            data["%s%s" % (self.prefix, stat)] = "%s|c" % delta

        self.send(data, sampleRate)
# ---
def task_schedule_status(self, task: ControllerTask, context: SchedulingContext) -> TaskScheduleResult:
        """Get the current scheduling status of a task (for dashboard display).

        Delegates to the internal scheduler.
        """
        return self._scheduler.task_schedule_status(task, context)
# ---
def rotate_left(list1,m,n):
  result =  list1[m:]+list1[:n]
  return result
# ---
def check_master(self):
        '''
        Log if the master is not running
        NOT YET IMPLEMENTED
        '''
        return True
# ---
def valid_key(key, sep='.'):
        '''validate the incoming key'''
        common_separators = list(Yedit.com_sep - set([sep]))
        if not re.match(Yedit.re_valid_key.format(''.join(common_separators)), key):
            return False

        return True
# ---
def next(self):
        """Get the next row in the page."""
        self._parse_rows()
        if self._remaining > 0:
            self._remaining -= 1
        return next(self._iter_rows)
# ---
def order_clause(self):
        order = "ASC" if self.ascending else "DESC"
        if self.case_insensitive:
            field = '(CASE ' \
                    'WHEN TYPEOF({0})="text" THEN LOWER({0}) ' \
                    'WHEN TYPEOF({0})="blob" THEN LOWER({0}) ' \
                    'ELSE {0} END)'.format(self.field)
        else:
            field = self.field
        return f"{field} {order}"
# ---
def preprocess_image(screen_image):

    # crop the top and bottom
    screen_image = screen_image[35:195]

    # down sample by a factor of 2
    screen_image = screen_image[::2, ::2]

    # convert to grey scale
    grey_image = np.zeros(screen_image.shape[0:2])
    for i in range(len(screen_image)):
        for j in range(len(screen_image[i])):
            grey_image[i][j] = np.mean(screen_image[i][j])

    return np.array([grey_image.astype(np.float)])
# ---
def hf_config_from_config(self, config: LevConfig, vocab_size: Optional[int] = None) -> HfConfig:
        if vocab_size is None:
            vocab_size = self.Vocab.size
        return config.to_hf_config(vocab_size=vocab_size)
# ---
def re_arrange_array(arr, n):
  j=0
  for i in range(0, n):
    if (arr[i] < 0):
      temp = arr[i]
      arr[i] = arr[j]
      arr[j] = temp
      j = j + 1
  return arr
# ---
def test_mem_read_byte_off_screen(self):
        self.assertEqual(self.mda.mem_read_byte(4000), 0x00)
# ---
def test_utils_to_probs(utilities, test_data):
    probs = mnl.utils_to_probs(utilities)
    pdt.assert_frame_equal(probs, test_data['probabilities'])
# ---
def output_emphasis(self, m):
        text = m.group(2) or m.group(1)
        text = self.output(text)
        return self.renderer.emphasis(text)
# ---
def is_finite(self) -> bool:
        if self.stop_strategy == StopStrategy.RESTART_STRATEGY:
            return False

        return True
# ---
def test_raw_metric_vm_network(metrics_collection, appliance, provider):
    vm_name = provider.data['cap_and_util']['capandu_vm']
    query = query_metric_db(appliance, provider, 'net_usage_rate_average',
        vm_name)

    for record in query:
        if record.net_usage_rate_average is not None:
            assert record.net_usage_rate_average > 0, 'Zero VM Network IO'
            break
# ---
def test_xxh3_128_vector():
    # Catch un-intentional regressions
    assert hash_xxh3_128(b"hello") == 241804000618833338782870102822322583576
# ---
def __eq__(self, other):
        if not isinstance(other, TwoStageConfig):
            return False
        return hash(self) == hash(other)
# ---
def _maybe_override_auto_build_caches(config: TrainLmConfig, auto_build: bool) -> TrainLmConfig:
    data = config.data
    if data.auto_build_caches != auto_build:
        logger.info("Overriding auto_build_caches to %s", auto_build)
        data = dataclasses.replace(data, auto_build_caches=auto_build)
        config = replace(config, data=data)
    return config
# ---
def test_fold_doesnt_reduce_scalars():
    Height = Axis("Height", 10)
    named1 = hax.random.uniform(PRNGKey(0), (Height,))

    acc = hax.zeros((Height,))

    total = hax.fold(lambda x, z, y: x + z * y, Height)(acc, 4.0, named1)

    assert jnp.all(jnp.isclose(total.rearrange(acc.axes).array, jnp.sum(named1.array * 4.0)))
# ---
def configure(self, enabled: bool, is_main_process: bool):
        """Configure whether wandb should be enabled."""
        assert self._enabled is False, "WandB is already initialized"
        self._enabled = enabled and is_main_process
# ---
def volume_cube(l):
  volume = l * l * l
  return volume
# ---
def Embed(self) -> Axis:
        pass
# ---
def validate_hidden_field(form, field):
        raise ValidationError('Always wrong')
# ---
def sort_tuple(tup): 
	lst = len(tup) 
	for i in range(0, lst): 
		for j in range(0, lst-i-1): 
			if (tup[j][-1] > tup[j + 1][-1]): 
				temp = tup[j] 
				tup[j]= tup[j + 1] 
				tup[j + 1]= temp 
	return tup
# ---
def checkWall(self, wall):
        if wall.state == "hidden":
            if (self.x >= wall.x - self.d and
                    (self.x + 32 <= wall.x + 32 + self.d)):
                return False
# ---
def _install_package(self, db_version):
        sudo("pkg_add postgresql%s-server" %db_version)
        sudo("pkg_add postgresql%s-replicationtools" %db_version)
        sudo("svcadm enable postgresql")
# ---
def err1(context):
            stmt = context.statement

            if "ERROR ONE" in str(stmt) or "ERROR TWO" in str(stmt) \
                    or "ERROR THREE" in str(stmt):
                return MyException1("my exception")
            elif "ERROR FOUR" in str(stmt):
                raise MyException3("my exception short circuit")
# ---
def testExprCall(self):
    self.assertEqual((0, 'bar\n'), _GrumpRun(textwrap.dedent("""\
        def foo():
          print 'bar'
        foo()""")))
# ---
def test_select_all(self):
        with config.db.connect() as conn:
            res = conn.execute(
                select([text("*")])
                .select_from(self.tables.square)
                .order_by(self.tables.square.c.id)
            ).fetchall()
            eq_(res, [(1, 10, 100, 40), (10, 42, 1764, 168)])
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: Optional[str] = None
    ) -> HFCheckpointConverter["MixtralConfig"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfMixtralConfig,
        )
# ---
def __repr__(self):
        return "<unionmatcher matchers=%r>" % self._matchers
# ---
def min(
        self, axis: AxisSelection | None = None, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.min(self, axis=axis, where=where)
# ---
def delete_router_postcommit(self, context, router_context):
        pass
# ---
def _has_nonzero_weight(self, name: str) -> bool:
        weights = self.train_weights
        if weights is None:
            return True
        if isinstance(weights, dict):
            return weights.get(name, 0) > 0
        return any(w.get(name, 0) > 0 for _, w in weights)
# ---
def always(self):
        """Matcher will match everything and .files() will be empty --
        optimization might be possible."""
        return False
# ---
def __str__(self) -> str:
        if self.total == 0:
            return f"{self.level} total: 0"
        return (
            f"{self.method.capitalize()} {self.level.lower()} total: {self.total:,}, "
            f"dups: {self.dups:,} ({self.dups / self.total:.2%}), unique: {self.unique:,}, "
            f"dup_clusters: {self.dup_clusters:,}"
        )
# ---
def __init__(self, minimum_balance):
        BankAccount.__init__(self)
        self.minimum_balance = minimum_balance
# ---
def hr(node: RenderTreeNode, context: RenderContext) -> str:
    thematic_break_width = 70
    return "_" * thematic_break_width
# ---
def execute_codegen(self, target, target_workdir):
    self._generate_thrift(target, target_workdir)
# ---
def address(self) -> str:
        return "fake-host"
# ---
def check_isosceles(x,y,z):
  if x!=y & y!=z & z!=x:
	   return True
  else:
     return False
# ---
def __init__(self, exc, handler):
        self.exc = exc
        self.hndl = handler
        self.cls = type(exc)
        self.tb = None
# ---
def decode_list(alist):
    def aux(g):
        if isinstance(g, list):
            return [(g[1], range(g[0]))]
        else:
            return [(g, [0])]
    return [x for g in alist for x, R in aux(g) for i in R]
# ---
def unregister_message_handler(self, target):
        del self._message_handlers[target]
# ---
def convert_to_local_time(utc_str: str) -> str:
    # Parse the UTC string to a datetime object
    utc_dt = datetime.strptime(utc_str, "%Y-%m-%dT%H:%M:%S")

    # Set the timezone to UTC
    utc_dt = utc_dt.replace(tzinfo=pytz.UTC)

    # Convert to Pacific Time
    pacific_tz = pytz.timezone("America/Los_Angeles")
    pacific_dt = utc_dt.astimezone(pacific_tz)

    # Format the result as a string (optional)
    pacific_str = pacific_dt.strftime("%Y-%m-%d %H:%M:%S %Z")
    return pacific_str
# ---
def num_tasks(self):
        """Return the number of tasks needed to execute this plan."""
        tasks = 0
        for _, node in visit_nodes(self.dag):
            tasks += node["primitive_op"].num_tasks
        return tasks
# ---
def _merge_dict(self, other, overwrite_vars, compat, join):
    other = _align_variables(other, join='outer')

    alignable = [k for k, v in other.items() if hasattr(v, 'indexes')]
    aligned = partial_align(self, *[other[a] for a in alignable],
                            join=join, copy=False, exclude=overwrite_vars)

    aligned_self = aligned[0]

    other = OrderedDict(other)
    other.update(zip(alignable, aligned[1:]))

    return _merge_expand(aligned_self, other, overwrite_vars, compat)
# ---
def get_id(self, nick):
        return self.data[nick.lower()]['id']
# ---
def add(self,accum,item):
        return accum + item
# ---
def cb(sh):
            if axes is not None:
                visualize_named_sharding(axes, sh)
            else:
                try:
                    jax.debug.visualize_sharding(arr.shape, sh)
                except Exception:
                    pass
# ---
def p_translation_task(self, p):
        """
        translate_task : ID DONE TASK
                       | ID TASK
        """
        if len(p) == 4:
            done = True
            content = p[3]
        elif len(p) == 3:
            done = False
            content = p[2]
        task = Task(p[1], content, done)
        self.todo.append(task)
# ---
def test_selector(self):
        self.assertEqual("/~jeremy/", self.get.get_selector())
        req = Request("http://www.python.org/")
        self.assertEqual("/", req.get_selector())
# ---
def HealthCheck(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def state():
    """Create a fresh ControllerState for each test."""
    return ControllerState()
# ---
def __init__(self):
        self.named = hax.ones((Dim2, Dim3))
        self.unnamed1 = jnp.ones(())
        self.named2 = hax.ones(Dim3)
        self.static_field = 1
# ---
def binding_genetic_modification(testapp, lab, award):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'CRISPR dCas',
        'purpose': 'characterization',
        'method': 'CRISPR'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def model_name_or_path(model: ModelConfig) -> str:
        """Return a reference Levanter can read without staging to local disk."""
        if model.path is None:
            return model.name
        return model.path
# ---
def leaves(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> Sequence[Any]:
    """Alias for :func:`haliax.tree_util.tree_leaves` matching :func:`jax.tree.leaves`."""

    return tree_util.tree_leaves(tree, is_leaf=is_leaf)
# ---
def __eq__(self, other):  # pragma: no cover
        # special case because Jax sometimes call == on
        # types when they're in PyTrees
        if self.array is None:  # pragma: no cover
            return other.array is None

        if hasattr(other, "array") and other.array is None:  # pragma: no cover
            return False

        return haliax.equal(self, other)
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.std()
# ---
def test_clean_wiki_html():
    cleaned = clean_wiki_html(SAMPLE_WIKIPEDIA_HTML, remove_reference_section=True)
    soup = BeautifulSoup(cleaned, "html.parser")

    # References should be removed
    assert soup.find("div", {"class": "reflist"}) is None
    assert soup.find("h2", {"id": "References"}) is None

    # Infobox should be moved
    assert soup.find("h2", string="InfoBox") is not None

    # Equations should be unwrapped
    assert "$E = mc^2$" in str(cleaned)
# ---
def test_linecol_to_offset_second_line():
    source = "hello world\nsecond line\n"
    assert _linecol_to_offset(source, 2, 0) == 12
    assert _linecol_to_offset(source, 2, 7) == 19
# ---
def _rematcher(regex):
    """compile the regexp with the best available regexp engine and return a
    matcher function"""
    m = util.re.compile(regex)
    try:
        # slightly faster, provided by facebook's re2 bindings
        return m.test_match
    except AttributeError:
        return m.match
# ---
def __str__(self):
        return '{}: {}'.format(self.name, self.main_figure)
# ---
def _reinit_embed_vectors(Embed, new_vocab, embeddings_matrix, ids_to_reinit: Iterable[int], key):
    # Match the existing embedding statistics to avoid abrupt scale shifts.
    mu = hax.mean(embeddings_matrix, axis="vocab")
    std = hax.std(embeddings_matrix, axis="vocab")
    reinited = hax.random.truncated_normal(key, (new_vocab, Embed), -3, 3) * std + mu
    new_weight = embeddings_matrix.at["vocab", ids_to_reinit].set(reinited)
    return new_weight
# ---
def test___cmp__le(self):
        self._test__cmp__(
            lambda left, right: left <= right,
            (
                False,
                True,
                True,
                True,
                True,
                TypeError if PY3 else False,
                TypeError if PY3 else True,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
            ),
            '<='
        )
# ---
def fibonacci(n):
  if n == 1 or n == 2:
    return 1
  else:
    return (fibonacci(n - 1) + (fibonacci(n - 2)))
# ---
def test_dialect_engine_construction_options(self):
        dialect = Mock()
        engine = Engine(Mock(), dialect, Mock(),
                                execution_options={"foo": "bar"})
        eq_(
            dialect.set_engine_execution_options.mock_calls,
            [call(engine, {"foo": "bar"})]
        )
# ---
def add(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.add](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.add.html)
    """
    return jnp.add(x1, x2)
# ---
def onlySpeakDisplayedTextToggled(self, widget):
        """Signal handler for the "toggled" signal for the GtkCheckButton
        onlySpeakDisplayedText. In addition to updating the preferences,
        set the sensitivity of the contextOptionsGrid.

        Arguments:
        - widget: the component that generated the signal.
        """

        enable = widget.get_active()
        self.prefsDict["onlySpeakDisplayedText"] = enable
        self.get_widget("contextOptionsGrid").set_sensitive(not enable)
# ---
def test_check_share_in_use_incorrect_host(self):
        drv = self._driver
        mox = self.mox
        mox.StubOutWithMock(utils, 'resolve_hostname')
        utils.resolve_hostname(IgnoreArg()).AndRaise(Exception())
        mox.ReplayAll()
        share = drv._check_share_in_use('incorrect:8989', '/dir')
        mox.VerifyAll()
        if share:
            self.fail('Unexpected share detected.')
# ---
def __getitem__(self, key):
        """Access variables or coordinates this dataset as a
        :py:class:`~xray.DataArray`.

        Indexing with a list of names will return a new ``Dataset`` object.
        """
        from .dataarray import DataArray

        if utils.is_dict_like(key):
            return self.isel(**key)

        key = np.asarray(key)
        if key.ndim == 0:
            return DataArray._new_from_dataset(self, key.item())
        else:
            return self._copy_listed(key)
# ---
def wrapped_fold(*args, **kwargs):
        fold_calls.append(kwargs.get("unroll"))
        return original_fold(*args, **kwargs)
# ---
def to_kebab_case(name: str) -> str:
    """Convert PascalCase to kebab-case."""
    return re.sub(r"(?<!^)(?=[A-Z])", "-", name).lower()
# ---
def error_page_404(status, message, traceback, version):
    html = header
    html += "%s<br>" % (status)
    html += "%s" % (traceback)
    html += getfooter()
    return html
# ---
def handle(self):
		run_root = self.__backend_mgr.get_run_root(self.__task.data['backend'], self.__task.data['id'])

		main_root = os.path.join(run_root, 'main')

		safe_rmdir(main_root)
		safe_mkdir(main_root)

		self.__create_input(main_root)
		self.__create_context(main_root)
		self.__create_action(main_root)
		self.__create_navigator(main_root)
		self.__create_bootstrap(main_root)

		launcher_param = self.__create_launcher(run_root)

		self.__submit(launcher_param)
# ---
def test_rechunk_same_chunks(spec, use_new_impl):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 1), spec=spec)
    b = a.rechunk((2, 1), use_new_impl=use_new_impl)
    assert b is a
    task_counter = TaskCounter()
    res = b.compute(callbacks=[task_counter])
    # no tasks should have run since chunks are same
    assert task_counter.value == 0

    assert_array_equal(res, np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
# ---
def unique_counts(
    array: NamedArray,
    Unique: Axis,
    *,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]:
    """Shortcut for :func:`unique` that also returns counts."""

    values, counts = typing.cast(
        tuple[NamedArray, NamedArray],
        unique(
            array,
            Unique,
            return_counts=True,
            axis=axis,
            fill_value=fill_value,
        ),
    )
    return values, counts
# ---
def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
    ) -> None:
        self.unregister()
# ---
def assert_inside_pjit(arr, expected: NamedSharding):
            def assert_eq(x, y):
                assert x == y

            jax.debug.inspect_array_sharding(arr.array, callback=lambda x: assert_eq(x, expected))
# ---
def unsafe_buffer_pointer(self):  # pragma: no cover
        return self._ref.unsafe_buffer_pointer()
# ---
def format_type(s):
    if s not in _VALID_FORMATS:
      raise argparse.ArgumentTypeError(f'Invalid Format specified: "{s}".')
    return s
# ---
def coords(self):
        """Dictionary of xray.DataArray objects corresponding to coordinate
        variables
        """
        return DatasetCoordinates(self)
# ---
def create_test_logprobs(text: str):
    """Create logprobs content for a response text."""
    from openai.types.chat.chat_completion_chunk import ChoiceLogprobsLogprob

    logprobs_content = []
    for c in text:
        logprobs_content.append(
            ChoiceLogprobsLogprob(
                token=c,
                logprob=-1.0,
                bytes=[ord(c)],
                top_logprobs=[],
            )
        )
    return ChoiceLogprobs(content=logprobs_content)
# ---
def __mul__(self, other: object) -> ArithmeticExpr:
        return ArithmeticExpr(self, _to_expr(other), "mul")
# ---
def char_offset_to_token_index(
        self,
        source: str,
        char_offset: int,
    ) -> int:
        """Convert a character offset in source to a token index.

        For this byte-level tokenizer, the mapping is 1:1 -- character i
        maps to token i. For subword tokenizers, this would need a mapping
        table.
        """
        return min(char_offset, len(source) - 1)
# ---
def overlaps(self, other: Self) -> bool:
        """Check if this time range overlaps with another time range.

        Args:
            other: Another TimeConfig to check for overlap

        Returns:
            True if the time ranges overlap, False otherwise
        """
        return (
            self.start.datetime < other.end.datetime
            and self.end.datetime > other.start.datetime
        )
# ---
def device_username(self):
        if self._values['device_username'] is None:
            return None
        return self._values['device_username']
# ---
def highlight_value(key, value):
        if key in highlight_fields:
            return YELLOW() + value + ENDC()
        else:
            return value
# ---
from math import tan, pi
def area_polygon(s,l):
  area = s * (l ** 2) / (4 * tan(pi / s))
  return area
# ---
def test_current_pixel_wraps_bottom(self):
        self.mda.current_pixel = [719, 349]
        self.mda.io_read_byte(0x3BA)
        self.assertEqual(self.mda.current_pixel, [0, 0])
# ---

def choose_num(x, y):
    """This function takes two positive numbers x and y and returns the
    biggest even integer number that is in the range [x, y] inclusive. If 
    there's no such number, then the function should return -1.

    For example:
    choose_num(12, 15) = 14
    choose_num(13, 12) = -1
    """
    if x > y:
        return -1
    if y % 2 == 0:
        return y
    if x == y:
        return -1
    return y - 1
# ---
def fold(self, *args, **kwargs): ...
# ---
def the_object_name_is_selected(name):
    i_deselect_all_objects()
    additionally_the_object_name_is_selected(name)
# ---
def update(self, new_array_counts):
        for name, new_count in new_array_counts.items():
            old_count = self.array_counts.get(name, 0)
            # it's possible that new_count < old_count
            event = TaskEndEvent(name, num_tasks=(new_count - old_count))
            if self.callbacks is not None:
                [callback.on_task_end(event) for callback in self.callbacks]
            self.array_counts[name] = new_count
# ---
def check_answer(self, sample_str: str) -> bool:
        try:
            answer = extract_boxed(sample_str)
        except ValueError:
            return False
        return safe_grade(answer, self.answer, self.grader, self.timeout)
# ---
def getOpenVGDBToPhoenixMap(self):
        return OrderedDict(sorted(self.openVGDBToPhoenixMap.items(), key=lambda t: t[0]))
# ---
def _create_chained_picking(self, cr, uid, picking_name, picking, picking_type, moves_todo, context=None):
        picking_obj = self.pool.get('stock.picking')
        return picking_obj.create(cr, uid, self._prepare_chained_picking(cr, uid, picking_name, picking, picking_type, moves_todo, context=context))
# ---
def test_tracker_plugin_stuff_works():
    assert TrackerConfig.get_choice_class("wandb") is not None
    with pytest.raises(KeyError):
        TrackerConfig.get_choice_class("foo")
# ---
def mkdir(self, path):
        "Make relative directory in workflowdir"
        path = os.path.join(self.workflowdir, path)
        if not os.path.exists(path):
            os.makedirs(path)
# ---
def test_do_execute_w_replace(self):
        self._test_do_execute(True)
# ---
def real(self) -> "NamedArray":  # pragma: no cover
        return NamedArray(self.array.real, self.axes)
# ---
def _hydropathy_class(res):
    if res in HYDROPHOBIC:
        return "hydrophobic"
    elif res in HYDROPHILIC:
        return "hydrophilic"
    else:
        return "neutral"
# ---
def __init__(self, loss: torch.Tensor, loss_per_channel: torch.Tensor):
        self.loss = loss
        self.loss_per_channel = loss_per_channel
# ---
def manifest_entry(self):
        'Write the XML element for the manifest.'
        return _make_xml_elem('item', '',
          [
            ('href', self.name),
            ('id', self.ident),
            ('media-type', self.media_type)
          ])
# ---
def __init__(self, id, *, split, streaming: bool = True, **kwargs):
        self.id = id
        self.split = split
        self.streaming = streaming
        self.kwargs = kwargs
        self._shard_names = self._compute_shard_names()
# ---
def date(self):
        del self._date
# ---
def frequency_lists(list1):
    list1 = [item for sublist in list1 for item in sublist]
    dic_data = {}
    for num in list1:
        if num in dic_data.keys():
            dic_data[num] += 1
        else:
            key = num
            value = 1
            dic_data[key] = value
    return dic_data
# ---
def test_simple_only_fd(self, testdir):
        testdir.makepyfile("""
            import os
            def test_x():
                os.write(1, "hello\\n".encode("ascii"))
                assert 0
        """)
        result = testdir.runpytest_subprocess()
        result.stdout.fnmatch_lines("""
            *test_x*
            *assert 0*
            *Captured stdout*
        """)
# ---
def title(self, val):
        # If val is not a string, raise TypeError now rather than later.
        self._title = _EpubMeta('dc:title', '' + val)
# ---
def __init__(self, root: Path):
        self._root = root
        self._root.mkdir(parents=True, exist_ok=True)
        self._artifacts: list[LogArtifact] = []
# ---
def remove(self, container_id: str) -> None: ...
# ---
def _make_padding_example(ex: Ex) -> Ex:
    with local_cpu_mesh():
        return tree_zeros_like(ex)
# ---
def is_slow(self):
        return True
# ---
def _merge_bloom(bloom_files: Iterator[str]):
        merged_bloom = Bloom(config.estimated_doc_count, config.false_positive_rate)
        for bloom_file_path in bloom_files:
            fs, path = fsspec.url_to_fs(bloom_file_path)
            with fs.open(path, "rb") as f:
                bloom_bytes = f.read()
            shard_bloom = Bloom.load_bytes(bloom_bytes)
            merged_bloom.update(shard_bloom)
        yield merged_bloom.save_bytes()
# ---
def write_artifacts(self, log_tree: LogTree):
        """Write the artifacts section from the log tree."""
        self._file.write("\n---\n\n")
        for line in log_tree.summary_lines():
            self._file.write(line + "\n")
        self._file.flush()
# ---
def _make_abstract_mesh(*, data: int, model: int) -> AbstractMesh:
    return AbstractMesh(
        axis_sizes=(data, model),
        axis_names=("data", "model"),
        axis_types=(AxisType.Explicit, AxisType.Explicit),
    )
# ---
def _run() -> list[Any]:
        if mode == "batch":
            return func(text_samples)
        return [func(x) for x in text_samples]
# ---
def __init__(self, m1, m2):
        super(intersectionmatcher, self).__init__(m1._root, m1._cwd)
        self._m1 = m1
        self._m2 = m2
        self.bad = m1.bad
        self.traversedir = m1.traversedir
# ---
def sinc(a: A) -> A:
    return wrap_elemwise_unary(jnp.sinc, a)
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self._layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)
        return x
# ---
def test_from_zarr(tmp_path, spec, executor, path):
    store = store = tmp_path / "source.zarr"
    create_zarr(
        [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
        chunks=(2, 2),
        store=store,
        path=path,
    )
    a = cubed.from_zarr(store, path=path, spec=spec)
    assert_array_equal(
        a.compute(executor=executor), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    )
# ---
def forward(self, coords):
        B, N, _ = coords.shape
        dist2 = (
            (coords.view(B, N, 1, 3) - self.center.view(1, 1, self.dim, 3)) ** 2
        ).sum(dim=-1)
        emb = torch.exp(-dist2 / (2 * self.std.view(1, 1, self.dim)))
        return emb
# ---
def create_after_delay():
        time.sleep(0.05)
        sentinel.signal()
# ---
def test_lambda_output_shape_list_multiple_outputs(self):

    def lambda_fn(x):
      return x

    l = keras.layers.Lambda(lambda_fn, output_shape=[(10,), (20,)])
    output_shape = l.compute_output_shape([(10, 10), (10, 20)])
    self.assertAllEqual([(10, 10), (10, 20)], output_shape)
# ---
from collections import Counter 
def anagram_lambda(texts,str):
  result = list(filter(lambda x: (Counter(str) == Counter(x)), texts)) 
  return result
# ---
def _compatible_block(shard_len: int, max_block: int) -> int:
        """Pick largest block <= max_block that divides shard_len; prefer multiples of 128."""
        if shard_len <= 0:
            return max_block
        cap = min(max_block, shard_len)
        for step in (128, 1):
            candidate = cap - (cap % step)
            while candidate >= step:
                if shard_len % candidate == 0:
                    return candidate
                candidate -= step
        return 1
# ---
def test_position_token_id_out_of_range(tok):
    with pytest.raises(ValueError):
        tok.position_token_id(-1)
    with pytest.raises(ValueError):
        tok.position_token_id(512)
# ---
def find_substring(str1, sub_str):
   if any(sub_str in s for s in str1):
       return True
   return False
# ---
def create_datasets():
    ds1 = ListAsyncDataset([1, 2, 3, 4, 5])
    ds2 = ListAsyncDataset([10, 20, 30, 40, 50])
    ds3 = ListAsyncDataset([100, 200, 300, 400, 500])
    ds1.finalize()
    ds2.finalize()
    ds3.finalize()
    return {"ds1": ds1, "ds2": ds2, "ds3": ds3}
# ---
def default_choice_name(cls) -> str | None:
        return "text"
# ---
def __init__(self, rng: random.Random, swap_prob: float = 0.3):
        self.rng = rng
        self.swap_prob = swap_prob
        self._changed = False
# ---
def _view_updated(self):
        if self.document.can_restore_from_prefs():
            self.restore_view()
        # XXX Logically, we should call _update_selected_pane() but doing so
        # make tests fail. to investigate.
        self._refresh_target_selection()
        self.view.update_selected_pane()
        self._refresh_swap_list_items()
        self.import_table.refresh()
# ---
def main():
#    utils.drop_privileges()
    mapper = NoiseMapper()
    mapper.run()
# ---
def the_file_name_should_contain_value(name, value):
    with open(name, "r") as f:
        assert value in f.read()
# ---
def test_missing_logprobs_raises(inference_ctx):
    """Test that missing logprobs raises ValueError."""
    choice = Choice(
        finish_reason="stop",
        index=0,
        message=ChatCompletionMessage(role="assistant", content="test"),
        logprobs=None,
    )

    with pytest.raises(ValueError, match="missing logprobs"):
        inference_ctx.response_tokens_from_choice(choice)

    with pytest.raises(ValueError, match="missing logprobs"):
        inference_ctx.logprobs_from_choice(choice)
# ---
def run_debug(scenario, blend_filepath=None):
    try:
        result = run(scenario)
    except Exception as e:
        if blend_filepath:
            bpy.ops.wm.save_as_mainfile(filepath=blend_filepath)
        assert False, e
    if blend_filepath:
        bpy.ops.wm.save_as_mainfile(filepath=blend_filepath)
    return result
# ---
def _load_model():
            return load_model_from_checkpoint(
                checkpoint=config.initial_checkpoint,
                model_config=config.model,
                trainer_config=config.trainer,
                vocab_axis=Vocab,
                tokenizer=self.tokenizer,
                mesh=config.trainer.device_mesh,
                axis_mapping=self.config.trainer.parameter_axis_mapping,
                key=model_key,
            )
# ---
def metadata(self) -> dict[str, Any]:
        return {
            "input_ids_key": self.input_ids_key,
            "loss_weights_key": self.loss_weights_key,
        }
# ---
def _batchified_shape(Batch, leaf: hax.NamedArray | Array) -> ShapeSpec | NamedShapeSpec:
    if is_named_array(leaf):
        return NamedShapeSpec((Batch,) + leaf.axes, leaf.dtype)
    else:
        return ShapeSpec((Batch.size,) + tuple(leaf.shape), leaf.dtype)
# ---
def test_entrypoint_command():
    ep = Entrypoint.from_command("echo", "hello")
    assert ep.is_command
    assert not ep.is_callable
    assert ep.command == ["echo", "hello"]
# ---
def test_host_maintenance_on(self):
        self._test_host_action(self.conn.host_maintenance_mode,
                               True, 'on_maintenance')
# ---
def __init__(self, opts):
        self.opts = opts
        if self.opts['transport'] in ('zeromq', 'tcp'):
            self.key = Key(opts)
        else:
            self.key = RaetKey(opts)
# ---
def unregister(self, endpoint_id: str) -> None:
        """Unregister an endpoint.

        Args:
            endpoint_id: Endpoint ID to remove
        """
        self._cluster.unregister_endpoint(endpoint_id)
# ---
def find_Max_Num(arr,n) : 
    arr.sort(reverse = True) 
    num = arr[0] 
    for i in range(1,n) : 
        num = num * 10 + arr[i] 
    return num
# ---
def get_volume_mounts(self):
        '''return volume mount information '''
        return self.get_volumes(mounts=True)
# ---
def softmax(a: A, axis: AxisSelection | None = None) -> A:
    return wrap_axiswise_call(jnn.softmax, a, axis=axis, single_axis_only=False)
# ---
def check_greater(test_tup1, test_tup2):
  res = all(x < y for x, y in zip(test_tup1, test_tup2))
  return (res)
# ---
def increment(self) -> int:
        """Increment and return new value."""
        self._value += 1
        return self._value
# ---
def test_all_zero_dimension(spec, executor):
    a = xp.ones((0,), spec=spec)
    b = xp.all(a)
    assert b.ndim == 0
    assert b.size == 1
    assert b.compute(executor=executor)
# ---
def log1p(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in log1p")
    return elemwise(nxp.log1p, x, dtype=x.dtype)
# ---
def tearDown(self):
        self.db_info.delete()
        self.backup.delete()
        self.datastore.delete()
        self.datastore_version.delete()
        models.create_nova_client = self.orig_client
        task_api.API(self.context).create_instance = self.orig_api
        models.run_with_quotas = self.run_with_quotas
        backup_models.DBBackup.check_swift_object_exist = self.check
        self.backup.delete()
        self.db_info.delete()
        super(CreateInstanceTest, self).tearDown()
# ---
def mk_LayerNorm(self, axis: AxisSpec):
        """Create a layer normalization module using the config."""
        return self.norm_config.build(axis)
# ---
def _fake_compile_metrics(cls, start_time, stop_time=None):
        raise exception.CouldNotFetchMetrics()
# ---
def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1, df2])
            return (A3.two == 'foo').sum()
# ---
def float_sort(price):
  float_sort=sorted(price, key=lambda x: float(x[1]), reverse=True)
  return float_sort
# ---
def __init__(
        self,
        coordinator: OctoprintDataUpdateCoordinator,
        tool: str,
        temp_type: str,
        device_id: str,
    ) -> None:
        """Initialize a new OctoPrint sensor."""
        super().__init__(coordinator, f"{temp_type} {tool} temp", device_id)
        self._temp_type = temp_type
        self._api_tool = tool
# ---
def update(self, resource, id_, updates):
        args = self._es_args(resource, refresh=True)
        return self.es.update(id=id_, body={'doc': updates}, **args)
# ---
def Embed(self) -> Axis:
        return Axis(name="embed", size=self.hidden_dim)
# ---
def test_detect_text_with_error_response(self, annotator_client_mock):
        # Given
        detect_text_method = annotator_client_mock.text_detection
        detect_text_method.return_value = AnnotateImageResponse(
            error={"code": 3, "message": "test error message"}
        )

        # When
        with pytest.raises(AirflowException) as ctx:
            self.hook.text_detection(image=DETECT_TEST_IMAGE)

        err = ctx.value
        assert "test error message" in str(err)
# ---
def animation(name):
    url = bilibili(name)
    try:
        result = 'bilibili:'+url[-1][0]+''+url[-1][1]
        return result
    except IndexError:
        return ''
# ---
def __enter__(self):
        'Return self for use in with ... as ... statement.'
        return self
# ---
def lambada_detokenizer(text):
  text = text.replace("", '"')
  text = text.replace("", '"')
  return '\n'+text.strip()
# ---
def run(self):
        cmd = '''
              docker run --rm -v rita_store:/rita/data  rita/test-r 
        '''

        logger.debug(cmd)

        out = subprocess.check_output(cmd, shell=True)

        logger.debug(out)
# ---
def keyModifiedToggle(self, cell, path, model, col):
        """When the user changes a checkbox field (boolean field)"""

        model[path][col] = not model[path][col]
        return
# ---
def baseline_func_phase(self,z_data,f_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		returns the baseline as a function
		the points in between the datapoints are computed by cubic interpolation
		'''
		return interp1d(f_data, self._baseline_als(np.angle(z_data),lam,p,niter=niter), kind='cubic')
# ---
def __init__(
        self,
        plotly_name="showexponent",
        parent_name="scatterpolar.marker.colorbar",
        **kwargs
    ):
        super(ShowexponentValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            edit_type=kwargs.pop("edit_type", "colorbars"),
            values=kwargs.pop("values", ["all", "first", "last", "none"]),
            **kwargs
        )
# ---
def test_failed(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_FAILED) == JobStatus.FAILED
# ---
def setUp(self):
        self.oFile = vhdlFile.vhdlFile(lFile)
        self.assertIsNone(eError)
        self.oFile.set_indent_map(dIndentMap)
# ---
def __repr__(self) -> str:
        return f"RemoteRev({self.model_name_or_path!r}, {self.revision!r})"
# ---
def is_slow(self):
        """Indicate whether this query is *slow*, meaning that it cannot
        be executed in SQL and must be executed in Python.
        """
        return False
# ---
def count(self, **kwargs):
        """Get hits count."""
        return int(self.hits['hits']['total'])
# ---
def with_status_check(obj, *args, **kwargs):
            if obj.status not in valid_start_statuses:
                exception_msg = (
                    u"Error calling {} {}: status is '{}', must be one of: {}"
                ).format(func, obj, obj.status, valid_start_statuses)
                raise VerificationException(exception_msg)
            return func(obj, *args, **kwargs)
# ---
def format_match(match):
    print(match.match_number)
    print(match.team_names)
    print(match.team_numbers)
    return '{}: {} ({}) & {} ({}) vs. {} ({}) & {} ({})'.format(
        match.match_number, 
        match.team_names[0], match.team_numbers[0],
        match.team_names[1], match.team_numbers[1],
        match.team_names[2], match.team_numbers[2],
        match.team_names[3], match.team_numbers[3],
        )
# ---
def total_noise(self, t):
    return self.sigmas[0] ** (1 - t) * self.sigmas[1] ** t
# ---
def __init__(self, array: "Array"):
        self.array = array
# ---
def __unicode__(self):
        return "%s: %s" %(self.TIPOS[self.tipo][1],self.msg)
# ---
def stop(self, handle: VllmServerHandle) -> None:
        container_name = handle.docker_container_name
        subprocess.run(["docker", "rm", "-f", container_name], check=False, capture_output=True, text=True)
# ---
def opendb(self):
        self.prepare_sqlite()
        return smadata2.db.sqlite.create_or_update(self.dbname)
# ---
def handler():
            pass
# ---
def __init__(self, bundle_prefix: str):
        self._prefix = bundle_prefix.rstrip("/")
# ---
def create_integration_run_task_request(bundle_path: str, task_id: str):
    """Create a RunTaskRequest for integration testing."""
    entrypoint = create_integration_entrypoint()

    return cluster_pb2.Worker.RunTaskRequest(
        task_id=task_id,
        num_tasks=1,
        entrypoint=entrypoint.to_proto(),
        bundle_gcs_path=bundle_path,
        environment=cluster_pb2.EnvironmentConfig(),
        resources=cluster_pb2.ResourceSpecProto(cpu=1, memory_bytes=512 * 1024**2),
    )
# ---
def __dir__(self):
		result = self.__dict__.keys()
		result.extend(self.structure.column_names)
		return result
# ---
def test_distinct_selectable_in_unions(self):
        table = self.tables.some_table
        s1 = select([table]).where(table.c.id == 2).distinct()
        s2 = select([table]).where(table.c.id == 3).distinct()

        u1 = union(s1, s2).limit(2)
        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])
# ---
def is_idle(self) -> bool:
        return len(self.running_task_ids) == 0
# ---
def list_all(self):
        '''
        Print out all keys
        '''
        salt.output.display_output(
                self.key.list_keys(),
                'key',
                self.opts)
# ---
def test_generative_engine_event_dispatch_hasevents(self):
        def l1(*arg, **kw):
            pass
        eng = create_engine(testing.db.url)
        assert not eng._has_events
        event.listen(eng, "before_execute", l1)
        eng2 = eng.execution_options(foo='bar')
        assert eng2._has_events
# ---
def test_encrypt_really_long_message(self):
        self._test_encryption(''.join(['abcd' for i in xrange(1024)]))
# ---
def flush(self):
        """
        Flushes the remaining messages and progress bars state by forcing redraw. Can be useful if you want to be sure
        that a message or progress has been updated in display at a given moment in code, like when you are exiting an
        application or doing some kind of synchronized operations.
        """
        self.queue.put(dill.dumps(FlushCommand()))
# ---
def __repr__(self):
        return "<treematcher rules=%r>" % self._rules
# ---
def as_dict(self):
        pass
# ---
def test_impl(n):
            S1 = pd.Series(np.ones(n))
            S2 = pd.Series(np.random.ranf(n))
            df = pd.DataFrame({'A': S1, 'B': S2})
            return df.A.sum()
# ---
def test_sample_with_size_respects_limit(bank):
    rng = random.Random(42)
    entry = bank.sample_with_size("If", max_stmts=2, rng=rng)
    if entry is not None:
        assert entry.stmt_count <= 2
# ---
def build(self) -> "WatchCallback":
        return WatchCallback(
            watch_targets=self.watch_targets,
            include_norms=self.include_norms,
            include_per_parameter_norms=self.include_per_parameter_norms,
            include_histogram=self.include_histograms,
            split_scan_layers=self.split_scan_layers,
        )
# ---
def render_template(template_name, context={}):
    template = env.get_template(template_name)
    context.update(
        SETTINGS=settings,
        now=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        version='.'.join(map(str, __VERSION__)))
    return template.render(**context)
# ---
def CanComplete():
  """Returns whether it's appropriate to provide any completion at the current
     line and column."""
  try:
    line, column = LineAndColumnAfterLastNonWhitespace()
  except TypeError:
    return False
  if ( line, column ) == CurrentLineAndColumn():
    return True
  return ( ToBytes( vim.current.buffer[ line ][ column - 1 ] )
           in potential_hint_triggers )
# ---
def compute(a: int, b: int) -> int:
        result = a + b
        print(f"{a} + {b} = {result}")
        return result
# ---
def parse_gcs_url(gcs_url):
    """Parse the GCS URL and return the bucket name and prefix path."""
    match = re.match(r"gs://([^/]+)/(.+)", gcs_url)
    if match:
        bucket_name, path_prefix = match.groups()
        return bucket_name, path_prefix
    else:
        raise ValueError(f"Invalid GCS URL format: {gcs_url}")
# ---
def backend_storage_name():
    # get storage name from top-level config
    # e.g. set globally with CUBED_STORAGE_NAME=tensorstore
    storage_name = config.get("storage_name", None)

    if storage_name is None:
        import zarr

        if zarr.__version__[0] == "3":
            storage_name = "zarr-python-v3"
        else:
            storage_name = "zarr-python"

    return storage_name
# ---
def zeros_like(x: Arrayish) -> "RunningMean":
        return RunningMean(x * 0.0, x * 0.0)
# ---
def _get_startup(self):
        if not self.startup:
            self._get_member()
            if self.team_member:
                self.startup = self.team_member.startup
        return self.startup
# ---
def __init__(self, config: RLJobConfig):
        self.config = config
# ---
def get_default_job_ctx() -> JobContext:
    """Get the current default job context, creating one if unset."""
    ctx = _job_context.get()
    if ctx is None:
        ctx = create_job_ctx(context_type="auto")
    return ctx
# ---
def is_finished(self):
        return True
# ---
def chaos(key: str) -> ChaosRule | None:
    """Check if chaos should fire for this key.

    Returns the fired rule (with delay_seconds, error, etc.), or None.
    Call sites must explicitly handle delay_seconds and error.
    No side effects - use walrus operator pattern:

        if rule := chaos("worker.building_delay"):
            time.sleep(rule.delay_seconds)
    """
    rule = _rules.get(key)
    if rule is None:
        return None
    if rule.try_fire():
        return rule
    return None
# ---
def get_namespace(self, router_id):
        """Get namespace of router.

        :router_id: router_id
        :returns: namespace string.
        """
        return 'vpn-' + router_id
# ---
def test_is_cloneable_share_goodformat3(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://com.netapp:8080/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def noop():
        pass
# ---
def user_facebook_url(self):
        return self._get_profile().facebook_url
# ---
def test_synthetic_subtrees_no_duplicates(rng):
    entries = generate_synthetic_subtrees(rng, count_per_category=20)
    sources = [e.source for e in entries]
    assert len(sources) == len(set(sources))
# ---
def on_task_end(self, event):
        if self.check_timestamps and event.task_create_tstamp is not None:
            assert (
                event.task_result_tstamp
                >= event.function_end_tstamp
                >= event.function_start_tstamp
                >= event.task_create_tstamp
                > 0
            )
        self.value += event.num_tasks
# ---
def validate_kwargs(kwargs, allowed_kwargs,
                    error_message='Keyword argument not understood:'):
  """Checks that all keyword arguments are in the set of allowed keys."""
  for kwarg in kwargs:
    if kwarg not in allowed_kwargs:
      raise TypeError(error_message, kwarg)
# ---
def test_makes_patches__more_variables():
    x = torch.randn(1, 20, 4, 8)

    patch_embed = PerceiverEncoder(
        in_channels=20,
        out_channels=4,
        patch_size=4,
        perceiver=make_perceiver(20, 4),
        lat=torch.linspace(start=-90, end=90, steps=x.shape[-2]),
        lon=torch.linspace(start=0, end=360, steps=x.shape[-1]),
    )

    patches = patch_embed(x)

    assert patches.shape == (1, 4, 1, 2)
# ---
def __iter__(self):
        return iter(self.examples)
# ---
def get_init_log(self, vm_id: str, tail: int | None = None) -> str:
        """Get initialization log for a VM."""
        return self._vm_registry.get_init_log(vm_id, tail)
# ---
def progress(self):
        '''
        progress is part of multiple disk_progress its
        flat and not 100% accurate - each disk take its
        portion ie if we have 2 disks the first will take
        0-50 and the second 50-100
        '''
        completed = (self._disk_count - 1) * 100
        return (completed + self._disk_progress) / self._disk_count
# ---
def test_url_base(self):
        self.assertEqual(self.xe.url_base, 'https://{0}:{1}/api/v1'.format(node, port))
# ---
def body_init(self):
        self.body = []
# ---
def mock_open_for_remote(path, mode="r", **kwargs):
        if "data.commoncrawl.org" in path and "data-jsonl.paths.gz" in path:
            return paths_file.open("rb")
        return original_open(path, mode, **kwargs)
# ---
def _inject_implicit_mixed_number(step: str):
    """
    Automatically make a mixed number evalable
    e.g. 7 3/4 => 7+3/4
    """
    p1 = re.compile("([0-9]) +([0-9])")
    step = p1.sub("\\1+\\2", step)  # implicit mults
    return step
# ---
def is_show_uncategorized(self, request):
        """Return the result of the "?show_uncategorized" query string param"""

        show_uncategorized = request.GET.get('show_uncategorized', False)
        if show_uncategorized is True or show_uncategorized == 'true':
            return True
        return False
# ---
def zip_tuples(test_tup1, test_tup2):
  res = []
  for i, j in enumerate(test_tup1):
    res.append((j, test_tup2[i % len(test_tup2)])) 
  return (res)
# ---
def min_Num(arr,n):  
    odd = 0
    for i in range(n): 
        if (arr[i] % 2): 
            odd += 1 
    if (odd % 2): 
        return 1
    return 2
# ---
def __repr__(self):
        return ['NONE', 'SHUTDOWN', 'LOG_MESSAGE', 'GET_PROPERTY_REPLY', 'SET_PROPERTY_REPLY', 'COMMAND_REPLY',
                'START_FILE', 'END_FILE', 'FILE_LOADED', 'TRACKS_CHANGED', 'TRACK_SWITCHED', 'IDLE', 'PAUSE', 'UNPAUSE',
                'TICK', 'SCRIPT_INPUT_DISPATCH', 'CLIENT_MESSAGE', 'VIDEO_RECONFIG', 'AUDIO_RECONFIG',
                'METADATA_UPDATE', 'SEEK', 'PLAYBACK_RESTART', 'PROPERTY_CHANGE', 'CHAPTER_CHANGE'][self.value]
# ---
def _fields(self, resource):
        """Get projection fields for given resource."""
        datasource = self._datasource(resource)
        keys = datasource[2].keys()
        return ','.join(keys) + ','.join([config.LAST_UPDATED, config.DATE_CREATED])
# ---
def _tokenize(self, text, **kwargs):
        tokens = np.fromstring(text, dtype=int, sep=" ")
        return tokens
# ---
def test_max_entries_per_type():
    """Cap on entries per type should be respected."""
    # Generate a lot of unique programs.
    programs = [f"def f{i}(x):\n    return x + {i}\n" for i in range(200)]
    bank = SubtreeBank.from_corpus(programs, max_entries_per_type=50)
    for entries in bank.entries.values():
        assert len(entries) <= 50
# ---
def not_equal(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "not_equal")
    return elemwise(nxp.not_equal, x1, x2, dtype=nxp.bool)
# ---
def __init__(self, pattern, fields, cls):
        self.pattern = pattern
        self.fields = fields
        self.query_class = cls

        subqueries = []
        for field in self.fields:
            subqueries.append(cls(field, pattern, True))
        super().__init__(subqueries)
# ---
def softplus(a: A) -> A:
    return wrap_elemwise_unary(jnn.softplus, a)
# ---
def iter_inherited(self, name):
        for ctx in self.iter_containing(name):
            yield object.__getattribute__(ctx, name)
# ---
def SearchRecord(x):
	print(x)
	if (x in student_phoneNumber_name) :
		return student_phoneNumber_name[x]

	return False
# ---
def flatten_list(list1):
    result_list = []
    if not list1: return result_list
    stack = [list(list1)]
    while stack:
        c_num = stack.pop()
        next = c_num.pop()
        if c_num: stack.append(c_num)
        if isinstance(next, list):
            if next: stack.append(list(next))
        else: result_list.append(next)
    result_list.reverse()
    return result_list
# ---
def the_object_name_is_an_ifc_class(name, ifc_class):
    ifc = an_ifc_file_exists()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    assert element.is_a(ifc_class), f'Object "{name}" is an {element.is_a()}'
# ---
def fake_vm_start(*args, **kwargs):
            self.fake_vm_start_called = True
# ---
def test_memory_modeller():
    modeller = MemoryModeller()
    assert modeller.current_mem == 0
    assert modeller.peak_mem == 0

    modeller.allocate(100)
    assert modeller.current_mem == 100
    assert modeller.peak_mem == 100

    modeller.free(50)
    assert modeller.current_mem == 50
    assert modeller.peak_mem == 100
# ---
def _get_pipeline_info(self) -> str:
        """Get pipeline step information from environment variables."""
        pipeline_step = os.environ.get("BOLTZGEN_PIPELINE_STEP", "")
        pipeline_progress = os.environ.get("BOLTZGEN_PIPELINE_PROGRESS", "")

        if pipeline_step:
            if pipeline_progress:
                return f"[{pipeline_progress}] {pipeline_step}"
            else:
                return f"[Pipeline] {pipeline_step}"
        return ""
# ---
def init(ui):
    global _usetreematcher
    _usetreematcher = ui.configbool("experimental", "treematcher")
# ---
def _bump_params(params):
        return jax.tree.map(lambda x: x + jnp.ones_like(x) * 0.001, params)
# ---
def _assert_empty(args, msg='%s'):
    if args:
        raise ValueError(msg % args)
# ---
def initSequences(controller):
    mysqlsteps = [
             {'title': 'Adding MySQL manifest entries',
              'functions':[createmanifest]}
    ]
    controller.addSequence("Installing MySQL", [], [], mysqlsteps)
# ---
def scan_fun(acc, x, static1, *, static2):
        assert static1 is True
        assert static2 is False
        return acc + jnp.sum(x.array), x.take(Width, 2)
# ---
def _parse_periods(pattern):
    """Parse a string containing two dates separated by two dots (..).
    Return a pair of `Period` objects.
    """
    parts = pattern.split('..', 1)
    if len(parts) == 1:
        instant = Period.parse(parts[0])
        return (instant, instant)
    else:
        start = Period.parse(parts[0])
        end = Period.parse(parts[1])
        return (start, end)
# ---
def test_random_centroid_dimensions():
    """ensure the correct number of dimensions"""
    dimensions = random.randrange(1, 100)
    k = random.randrange(1, 100)
    centroids = kmeans.random_centroids(k, dimensions)
    for centroid in centroids:
        assert len(centroid) == dimensions
# ---
def __init__(self, content=None):
        ''' Constructor for deploymentconfig '''
        if not content:
            content = DeploymentConfig.default_deployment_config

        super(DeploymentConfig, self).__init__(content=content)
# ---
def name(cls):
        return "blocks_api:block_depth"
# ---


def monotonic(l: list):
    """Return True is list elements are monotonically increasing or decreasing.
    >>> monotonic([1, 2, 4, 20])
    True
    >>> monotonic([1, 20, 4, 10])
    False
    >>> monotonic([4, 1, 0, -10])
    True
    """
    if l == sorted(l) or l == sorted(l, reverse=True):
        return True
    return False
# ---
def schedule_router_precommit(self, context, router_context):
        pass
# ---
def stop_cluster(ctx):
    """Stop cluster."""
    config_obj, config_path = ctx.obj.config_obj, ctx.obj.config_file
    if not config_obj or not config_path:
        print("Error: --config required for cluster commands", file=sys.stderr)
        sys.exit(1)

    _stop_cluster_internal(ctx.obj, config_obj, config_path)
    print("Cluster stopped successfully!")
# ---
def success(self) -> bool:
        return self.exception is None
# ---
def _wrap_check(self, _p1, check, _p2):
        """Turn parenthesized expressions into a 'check' token."""

        return [('check', check)]
# ---
def details_template_name(self):
        return 'include/python_job_details.html'
# ---
def t_newline(self, t):
        r'\n+'
        t.lexer.lineno += len(t.value)
# ---
def test_index_with_tracer():
    H, W, D = hax.make_axes(H=20, W=30, D=40)
    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    @jax.jit
    def f(idx):
        return named1["H", idx]

    idx = jnp.array([1, 2, 3])
    assert jnp.all(jnp.equal(f(idx).array, named1.array[1:4, :, :]))

    idx = jnp.array(0)
    assert jnp.all(jnp.equal(f(idx).array, named1.array[0, :, :]))
# ---
def available_tpus(self) -> int:
        """Available TPU chip count after subtracting committed resources."""
        return get_tpu_chip_count(self.metadata.device) - self.committed_tpu
# ---
def JumpToPreviousWindow():
  """ Jump the vim cursor to its previous window position """
  vim.command( 'silent! wincmd p' )
# ---
def time(self):
        """Returns current virtual time."""
        with self._lock:
            return self._current_time
# ---
def build(self, Vocab: Axis, *, key: PRNGKeyArray) -> "LmT":
        return self.model_type.init(Vocab, self, key=key)
# ---
def activations(
        self,
        input_ids: NamedArray,
        attn_mask: Optional[AttentionMask | NamedArray] = None,
        *,
        key=None,
        pos_ids: NamedArray | None = None,
    ) -> NamedArray:
        x = self.embeddings.embed(input_ids)
        x = self.transformer(x, attn_mask=attn_mask, key=key, pos_ids=pos_ids)
        return x
# ---
def test_create_coinbase(self):
        height = 20
        coinbase_tx = create_coinbase(height=height)
        assert_equal(CScriptNum.decode(coinbase_tx.vin[0].scriptSig), height)
# ---
def test_typeerror(self):
        for case in (None, object(), 123, 'foobar'):
            self.assertRaises(TypeError, ctds.Parameter, case, b'123')

        self.assertRaises(TypeError, ctds.Parameter)
        self.assertRaises(TypeError, ctds.Parameter, output=False)

        for case in (None, object(), 123, 'foobar'):
            self.assertRaises(TypeError, ctds.Parameter, b'123', output=case)
# ---
def apply_blockwise(input_data, out_coords, bw_spec):
    args = []
    out_key = ("out",) + tuple(out_coords)  # array name is ignored by key_function
    in_keys = bw_spec.key_function(out_key)
    for in_key in in_keys:
        # just return the (1D) coord as a value
        def get_data(key):
            name = key[0]
            index = key[1]  # 1d index
            return input_data[name][index]

        arg = map_nested(get_data, in_key)
        args.append(arg)
    return bw_spec.function(*args)
# ---
def create_writer(self) -> "RolloutWriter":
        if self.storage_type == StorageType.FILE:
            if self.path is None:
                raise ValueError("path must be specified for FILE storage type")
            return FileRolloutWriter(self.path, self.max_rollout_files)
        else:
            if self.queue_name is None:
                raise ValueError("queue_name must be specified for IN_MEMORY storage type")
            return _get_or_create_queue(self.queue_name, self.queue_maxlen).writer()
# ---
def metadata(self) -> dict[str, Any]:
        return {
            "tokenizer": self.tokenizer.name_or_path,
            "vocab_size": len(self.tokenizer),
            "return_attention_mask": self.return_attention_mask,
            "padding": self.padding,
            "max_length": self.max_length,
            "append_bos": self._need_to_add_bos,
            "append_eos": self._need_to_add_eos,
        }
# ---
def load_record(record_id: str, record_dir: Path) -> Record:
    """Load the given record.

    Parameters
    ----------
    record_id : str
        The record id to load.
    record_dir : Path
        The path to the record directory.

    Returns
    -------
    Record
        The loaded record.
    """
    return Record.load(record_dir / f"{record_id}.json")
# ---
import bisect
def right_insertion(a, x):
    i = bisect.bisect_right(a, x)
    return i
# ---
def current_attempt(self) -> ControllerTaskAttempt | None:
        """The most recent attempt, or None if no attempts yet."""
        return self.attempts[-1] if self.attempts else None
# ---
def smallest_multiple(n):
    if (n<=2):
      return n
    i = n * 2
    factors = [number  for number in range(n, 1, -1) if number * 2 > n]
    while True:
        for a in factors:
            if i % a != 0:
                i += n
                break
            if (a == factors[-1] and i % a == 0):
                return i
# ---
def main():
    """Script entry point."""
    if os.name != 'nt':
        return 1
    return print_banner(**vars(parse()))
# ---
def __getitem__(self, word):
        return self.get_cluster(word)
# ---
def test_is_position_token_false_for_specials(tok):
    assert not tok.is_position_token(tok.pad_token_id)
    assert not tok.is_position_token(tok.sos_token_id)
    assert not tok.is_position_token(tok.eos_token_id)
# ---
def _get_client(self, url: str) -> ActorServiceClientSync:
        if self._client is None or self._client_url != url:
            self._client = ActorServiceClientSync(
                address=url,
                timeout_ms=int(self._timeout * 1000),
            )
            self._client_url = url
        return self._client
# ---
def no_of_subsequences(arr, k): 
	n = len(arr) 
	dp = [[0 for i in range(n + 1)] 
			for j in range(k + 1)] 
	for i in range(1, k + 1): 
		for j in range(1, n + 1): 
			dp[i][j] = dp[i][j - 1] 
			if arr[j - 1] <= i and arr[j - 1] > 0: 
				dp[i][j] += dp[i // arr[j - 1]][j - 1] + 1
	return dp[k][n]
# ---
def _flattened_spec(spec):
    out = []
    for s in spec:
        if isinstance(s, tuple):
            out.extend(s)
        elif s is None:
            pass
        else:
            out.append(s)

    return tuple(out)
# ---
def _explicitfiles(kindpats):
    """Returns the potential explicit filenames from the patterns.

    >>> _explicitfiles([(b'path', b'foo/bar', b'')])
    ['foo/bar']
    >>> _explicitfiles([(b'rootfilesin', b'foo/bar', b'')])
    []
    """
    # Keep only the pattern kinds where one can specify filenames (vs only
    # directory names).
    filable = [kp for kp in kindpats if kp[0] not in ("rootfilesin",)]
    return _roots(filable)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """Peak FLOP/s for a single TPU chip."""
        from fray.cluster.device_flops import device_flops

        flops = device_flops(self.variant, dtype)
        if flops is None:
            raise ValueError(f"Unknown device/dtype: {self.variant}/{dtype}")
        return flops
# ---
def max_sum_list(lists):
 return max(lists, key=sum)
# ---
def merge(self) -> hax.NamedArray:
        return hax.dot(self.lora_A.weight, self.lora_B.weight, axis=LORA_R) * self.scale
# ---
def _apply_logit_soft_cap(logits: Float[Array, "B V"], logit_soft_cap: Optional[float]) -> Float[Array, "B V"]:
    if logit_soft_cap is None:
        return logits
    return jnp.tanh(logits / logit_soft_cap) * logit_soft_cap
# ---
def test_weird_list_item():
    html = r"""<html><body><ol start="'3..'"><li>Item 3</li></ol></body></html>"""
    expected = "1. Item 3\n"  # back off to no start
    assert to_markdown(html) == expected
# ---
def parent(self):
        return object.__getattribute__(self, '_parent')
# ---
def floor_divide(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.floor_divide](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.floor_divide.html)
    """
    return jnp.floor_divide(x1, x2)
# ---
def user_phone(self):
        return self._get_profile().phone
# ---
def publisher(self, val):
        self.opt_meta['publisher'] = _EpubMeta('dc:publisher', '' + val)
# ---
def is_local(self) -> bool:
        return self._config.controller.WhichOneof("controller") == "local"
# ---
def ohc_map(ohc_intz):
            ohc_intz = remove_climatology(ohc_intz)
            # return last 1 year - first 1 year
            return ohc_intz.isel(time=slice(-73, None)).mean("time") - ohc_intz.isel(
                time=slice(0, 73)
            ).mean("time")
# ---
def create(self, context):
        # To ensure the creating type is PF
        if self.type != 'pf':
            raise exception.InvalidDeployType()
        super(PhysicalFunction, self).create(context)
# ---
def test_is_stop_signal_invalid_tokens_in_stop_sequences():
    # stop_sequence contains only INVALID tokens (should not match)
    tail_tokens = hax.named(jnp.array([1, 2, 3], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.array([[INVALID, INVALID, INVALID]], dtype=jnp.int32), axis=("seq", "position"))
    assert not is_stop_signal(tail_tokens, stop_sequences)
# ---
def _tree_random_like(rng_key: chex.PRNGKey, target_tree: chex.ArrayTree, dtype=None) -> chex.ArrayTree:
    # adopted from optax
    tree_def = jax.tree.structure(target_tree)
    keys = jax.random.split(rng_key, tree_def.num_leaves)
    keys_tree = jax.tree.unflatten(tree_def, keys)
    return jax.tree.map(
        lambda target_array, key: jax.random.normal(
            key, target_array.shape, dtype if dtype is not None else target_array.dtype
        ),
        target_tree,
        keys_tree,
    )
# ---
def message(self):
        return self.protocol.messages[int(self.ui.spinBoxFuzzMessage.value() - 1)]
# ---
def the_feature_extractor(self) -> SequenceFeatureExtractor:
        return self.the_processor.feature_extractor
# ---
def truncated_normal(key, shape: AxisSpec, lower: NamedOrNumeric, upper: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    lower = broadcast_to(lower, shape).array
    upper = broadcast_to(upper, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.truncated_normal(key=key, lower=lower, upper=upper, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def fn(x):
        return x + 5
# ---
def date_represent(date_obj):
        """
            Represent a datetime object as string

            @param date_obj: the datetime object

            @todo: replace by S3DateTime method?
        """
        return date_obj.strftime("%d %B %Y, %I:%M%p")
# ---
def build_env_string(env_dict: dict[str, str]) -> str:
    """Build properly escaped environment variable string for shell."""
    if not env_dict:
        return ""

    env_parts = []
    for key, value in env_dict.items():
        # Escape both key and value for shell safety
        escaped_key = shlex.quote(key)
        escaped_value = shlex.quote(value)
        env_parts.append(f"{escaped_key}={escaped_value}")

    return " ".join(env_parts)
# ---
def string_match(cls, pattern, value):
        return pattern.lower() in value.lower()
# ---
def __matmul__(self, other, /):
        other = self._check_allowed_dtypes(other, "numeric", "__matmul__")
        if other is NotImplemented:
            return other
        return matmul(self, other)
# ---
def pair_wise(l1):
    temp = []
    for i in range(len(l1) - 1):
        current_element, next_element = l1[i], l1[i + 1]
        x = (current_element, next_element)
        temp.append(x)
    return temp
# ---
def test_uniform_with_bounds_scalar():
    check_gen_is_equal(jax.random.uniform, hax.random.uniform)

    key = jax.random.PRNGKey(0)
    u = hax.random.uniform(key, shape=(Height, Width), minval=-3.0, maxval=0.5)

    assert u.axes == (Height, Width)

    assert hax.all(u >= -3.0)
    assert hax.all(u <= 0.5)
# ---
def __init__(self, a, b):
        self.a = a
        self.b = b
# ---
def check_monthnumb(monthname2):
  if(monthname2=="January" or monthname2=="March"or monthname2=="May" or monthname2=="July" or monthname2=="Augest" or monthname2=="October" or monthname2=="December"):
    return True
  else:
    return False
# ---
def handler(signum, frame):
            sys.exit(0)
# ---
def prepopulate(self):
        pass
# ---
def _get_bias_dropout_scale(self):
    if self.training:
      return bias_dropout_add_scale_fused_train
    else:
      return bias_dropout_add_scale_fused_inference
# ---
def _all_dbs_loaded(self):
        if self.client and self._core and self.lc:
            return True
        else:
            print("Load connection to Mongo and eQuilibrator local cache.")
            return False
# ---
def isneginf(a: A) -> A:
    return wrap_elemwise_unary(jnp.isneginf, a)
# ---
def test_axis_names_static_exclusive():
    with pytest.raises(ValueError):
        hax.field(static=True, axis_names=("x",))
# ---
def assert_eq(x, y):
                assert x == y
# ---
def alive(self):
        return not self.closed.is_set()
# ---
def __init__(self, seq_len):
        super().__init__()
        self.seq_len = seq_len
        self.begin = 0
        self.end = 256
        self.stride = 1
# ---
def _mean_combine(a, **kwargs):
    dtype = dict(kwargs.pop("dtype"))
    n = nxp.sum(a["n"], dtype=dtype["n"], **kwargs)
    total = nxp.sum(a["total"], dtype=dtype["total"], **kwargs)
    return {"n": n, "total": total}
# ---
def test_fn_exc():
    raise ValueError()
# ---
def forward(self, x):
    with torch.cuda.amp.autocast(enabled=False):
      x = F.layer_norm(x.float(), [self.dim])
    return x * self.weight[None,None,:]
# ---
def maybe_chunk(name, var, chunks):
            chunks = selkeys(chunks, var.dims)
            if not chunks:
                chunks = None
            if var.ndim > 0:
                return var.chunk(chunks, name=name, lock=lock)
            else:
                return var
# ---
def add_widget(self, widget):
        """Add a widget to the list of widgets to do auto-completion for."""
        if widget in self.widgets:
            return # Widget already added

        if isinstance(widget, TextBox):
            self._add_text_box(widget)
            return

        raise ValueError("Widget type %s not supported." % (type(widget)))
# ---
def test_submit_and_complete(self, test_cluster):
        """Job completes successfully."""

        def hello():
            return 42

        job_id = test_cluster.submit(hello, name=unique_name("test-job"))
        status = test_cluster.wait(job_id, timeout=30)
        assert status["state"] == "JOB_STATE_SUCCEEDED"
# ---
def cancelButtonClicked(self, widget):
        """Signal handler for the "clicked" signal for the cancelButton
           GtkButton widget. The user has clicked the Cancel button.
           Don't write out the preferences. Destroy the configuration window.

        Arguments:
        - widget: the component that generated the signal.
        """

        self.windowClosed(widget)
        self.get_widget("orcaSetupWindow").destroy()
# ---
def finger_all(self):
        self._call_all('finger_all')
# ---
def test_capsysbinary_forbidden_in_python2(self, testdir):
        testdir.makepyfile("""
            def test_hello(capsysbinary):
                pass
        """)
        result = testdir.runpytest()
        result.stdout.fnmatch_lines([
            "*test_hello*",
            "*capsysbinary is only supported on python 3*",
            "*1 error in*",
        ])
# ---
def __call__(self, carry: hax.NamedArray) -> hax.NamedArray:
            return carry + self.weight
# ---
def __len__(self):
        return len(self._data)
# ---
def LineTextInCurrentBuffer( line_number ):
  """ Returns the text on the 1-indexed line (NOT 0-indexed) """
  return vim.current.buffer[ line_number - 1 ]
# ---
def get_mapping(self):
        raise NotImplementedError
# ---
def test_take(spec, axis):
    a = xp.asarray(
        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]],
        chunks=(2, 2),
        spec=spec,
    )
    b = xp.asarray([1, 2], spec=spec)
    c = xp.take(a, b, axis=axis)
    x = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])
    assert_array_equal(c.compute(), x.take([1, 2], axis=axis))
# ---
def i_am_on_frame_number(number):
    bpy.context.scene.frame_set(int(number))
# ---
def init_fn(params):
        momentum_buffer = otu.tree_zeros_like(params)  # First moment
        return ScaleByScionState(momentum_buffer=momentum_buffer)
# ---
def min_Jumps(a, b, d): 
    temp = a 
    a = min(a, b) 
    b = max(temp, b) 
    if (d >= b): 
        return (d + b - 1) / b 
    if (d == 0): 
        return 0
    if (d == a): 
        return 1
    else:
        return 2
# ---
def init(self):
        if None not in self.comminfo.keys():
            self.comminfo = dict({None: self.p.commission})

        self.startingcash = self.cash = self.p.cash

        self.orders = list()  # will only be appending
        self.pending = collections.deque()  # popleft and append(right)

        self.positions = collections.defaultdict(Position)
        self.notifs = collections.deque()
# ---
def _get_hendrycks_math_test() -> Dataset:
    test_dataset = load_dataset("HuggingFaceH4/MATH-500", name="default", split="test")
    return cast(Dataset, test_dataset)
# ---
def reset(self):
        self._calls.reset()
# ---
def count_duplic(lists):
    element = []
    frequency = []
    if not lists:
        return element
    running_count = 1
    for i in range(len(lists)-1):
        if lists[i] == lists[i+1]:
            running_count += 1
        else:
            frequency.append(running_count)
            element.append(lists[i])
            running_count = 1
    frequency.append(running_count)
    element.append(lists[i+1])
    return element,frequency
# ---
def reload_model(self, model: LmHeadModel | None, state_dict: dict) -> LmHeadModel | None:
        assert model is not None or state_dict is not None, "Either model or state_dict must be provided"
        if model is None and state_dict is not None:
            with hax.set_mesh(self.mesh), hax.axis_mapping(self.axis_mapping):
                model = update_model(model, state_dict)

        self._inference_server.reload(lambda _: model)
        return model
# ---
import math 
def is_polite(n): 
	n = n + 1
	return (int)(n+(math.log((n + math.log(n, 2)), 2)))
# ---
def __init__(self, name: str = "root") -> None:
        self._name = name
        self._threads: list[ManagedThread] = []
        self._children: list[ThreadContainer] = []
        self._executors: list[ThreadPoolExecutor] = []
        self._lock = threading.Lock()
# ---
def test_column(self):
        expr = col("score")
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def maximize(self):
        """Maximize the window.

        The behaviour of this method is somewhat dependent on the user's
        display setup.  On a multi-monitor system, the window may maximize
        to either a single screen or the entire virtual desktop.
        """
        raise NotImplementedError('abstract')
# ---
def __init__(self, rule):
        """Initialize the 'not' check.

        :param rule: The rule to negate.  Must be a Check.
        """

        self.rule = rule
# ---
def _coerce_to_repo_ref(checkpoint: Union[str, RepoRef]) -> RepoRef:
    """Convert string or RepoRef to RepoRef."""
    if isinstance(checkpoint, str):
        return RepoRef.from_string(checkpoint)
    return checkpoint
# ---
def init(Order: Axis, w: float = 10, *, key=None):
        return Sin(w * hax.ones((Order,)))
# ---
def __call__(self, *args):
        return self.handle(self.meth_name, self.action, *args)
# ---
def __add__(self, other: "DupCounters") -> "DupCounters":
        assert isinstance(other, DupCounters)

        return DupCounters(
            method=self.method,
            level=self.level,
            total=self.total + other.total,
            dups=self.dups + other.dups,
            unique=self.unique + other.unique,
            dup_clusters=self.dup_clusters + other.dup_clusters,
        )
# ---
def __init__(
        self,
        store: T_Store,
        shape: T_Shape,
        dtype: T_DType,
        chunks: T_RegularChunks,
        path: Optional[str] = None,
        **kwargs,
    ):
        """Create a Zarr array lazily in memory."""
        super().__init__(shape, dtype, chunks)
        self.store = store
        self.path = path
        self.kwargs = kwargs
# ---
def __delattr__(self, name):
        """ Remove non-slot field attribute. """
        try:
            del self._attrs[name]
        except KeyError:
            raise AttributeError(name)
# ---
def extend(self, batch: Sequence[T]):
        """
        Append a batch of data to the store.
        """
        jtu.tree_map(
            lambda writer, *xs: writer.extend([np.asarray(x) for x in xs]),
            self.tree,
            *batch,
            is_leaf=heuristic_is_leaf,
        )
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        return {
            "mlp": None,
            "mlp_ln": "final_layer_norm",
            "attn_ln": "self_attn_layer_norm",
            "encoder_attn_ln": "encoder_attn_layer_norm",
        }
# ---
def is_point_on_right(self, point):
        return self.a * point[0] + self.b * point[1] + self.c < 0
# ---
def _apply_manual_overrides(ssh_config: SshConfig, manual: config_pb2.ManualProvider) -> SshConfig:
    return SshConfig(
        user=manual.ssh_user or ssh_config.user,
        key_file=manual.ssh_key_file or ssh_config.key_file,
        connect_timeout=ssh_config.connect_timeout,
    )
# ---
def activate(self):
        """Attempt to restore keyboard focus to the window.

        Depending on the window manager or operating system, this may not
        be successful.  For example, on Windows XP an application is not
        allowed to "steal" focus from another application.  Instead, the
        window's taskbar icon will flash, indicating it requires attention.
        """
        raise NotImplementedError('abstract')
# ---
def unbound_on_Server(host, port,
                              use_ssl,
                              connect_timeout, *a, **kwargs):
            return self._on_Server(host, port,
                              use_ssl,
                              connect_timeout, *a, **kwargs)
# ---
def resize_by_area(img, size):
  """image resize function used by quite a few image problems."""
  return tf.to_int64(
      tf.image.resize_images(img, [size, size], tf.image.ResizeMethod.AREA))
# ---
def test_greater_equal(self):
        expr = col("score") >= 100
        assert expr.evaluate({"score": 50}) is False
        assert expr.evaluate({"score": 100}) is True
        assert expr.evaluate({"score": 150}) is True
# ---
def svg_1d(chunks, sizes=None, **kwargs):
    return svg_2d(((1,),) + chunks, **kwargs)
# ---
def _compute_entropy_on_device(logit_fn, model, batch: B, Vocab) -> jnp.ndarray:
    with jax.named_scope("logits"):
        logits = logit_fn(model, batch)
    entropies = entropy_from_logits(logits, axis=Vocab)
    return entropies.flatten("token").array
# ---
def calc():
	h, l = input().split(' ')

	mapa = []

	for i_row in range(int(h)):
		mapa.append(input().split(' '))

	maior_num = 0



	for row in mapa:
		for col in row:
			n = int(col)
			if (n > maior_num):
				maior_num = n



	qtd = [0 for i in range(maior_num + 1)]

	for row in mapa:
		for col in row:
			n = int(col)	

			qtd[n] = qtd[n] + 1

	menor = 1
	for i in range(1, len(qtd)):
		if (qtd[i] <= qtd[menor]):	
			menor = i	



	print(menor)
# ---
def remove(k):
            del self._variables[k]
            self._coord_names.discard(k)
# ---
def __init__(self, tag, text, *args):
        '''The metadata entry is an XML element. *args is used for
        supplying the XML element's attributes as (key, value) pairs.'''
        self.tag = tag
        self.text = text
        self.attr = args
# ---
def fix_init_kwarg(self, sender, args, kwargs, **signal_kwargs):
		# Anything passed in as self.name is assumed to come from a serializer and
		# will be treated as a json string.
		if self.name in kwargs:
			value = kwargs.pop(self.name)

			# Hack to handle the xml serializer's handling of "null"
			if value is None:
				value = 'null'

			kwargs[self.attname] = value
# ---
def __init__(self) -> None:
        self._vms: dict[str, ManagedVm] = {}
        self._lock = threading.Lock()
# ---
def _is_checkpoint_dir(path):
        for checkpoint_dir_pattern in is_checkpoint_dir_pattern:
            if not fsspec_exists(os.path.join(path, checkpoint_dir_pattern)):
                return False
        return True
# ---
def bitwise_right_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_right_shift](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_right_shift.html)
    """
    return jnp.bitwise_right_shift(x1, x2)
# ---
def count_even(array_nums):
   count_even = len(list(filter(lambda x: (x%2 == 0) , array_nums)))
   return count_even
# ---
def allclose(a: NamedArray, b: NamedArray, rtol=1e-05, atol=1e-08, equal_nan=False) -> bool:
    """Returns True if two arrays are element-wise equal within a tolerance."""
    a, b = broadcast_arrays(a, b)
    return bool(jnp.allclose(a.array, b.array, rtol=rtol, atol=atol, equal_nan=equal_nan))
# ---
def matrix_to_list(test_list):
  temp = [ele for sub in test_list for ele in sub]
  res = list(zip(*temp))
  return (str(res))
# ---
def test_scan_str_arg():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))

    def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take("Width", 2)

    total, selected = hax.scan(scan_fun, "Height")(0.0, named1)

    assert jnp.all(jnp.isclose(total, jnp.sum(named1.array, axis=(0, 1, 2))))
    assert jnp.all(jnp.equal(selected.array, named1.take(Width, 2).array))
# ---
def bank():
    return SubtreeBank.from_corpus(SAMPLE_PROGRAMS)
# ---
def _step(key: jax.Array) -> dict[str, jax.Array]:
        tokens = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)
        return {"tokens": tokens[:, :-1], "labels": tokens[:, 1:]}
# ---
def markdownify_kwargs(self) -> dict:
        exclude = {"include_images", "include_links"}
        return {f.name: getattr(self, f.name) for f in fields(self) if f.name not in exclude}
# ---
def validate_conversion_factor(self):
		check_list = []
		for d in self.get('uoms'):
			if cstr(d.uom) in check_list:
				frappe.throw(
					_("Unit of Measure {0} has been entered more than once in Conversion Factor Table").format(d.uom))
			else:
				check_list.append(cstr(d.uom))

			if d.uom and cstr(d.uom) == cstr(self.stock_uom) and flt(d.conversion_factor) != 1:
				frappe.throw(
					_("Conversion factor for default Unit of Measure must be 1 in row {0}").format(d.idx))
# ---
def name(self) -> str:
        return "spark"
# ---
def device_flops(self) -> float:
        """Get the peak FLOPs/s for the device type."""
        device_flops = self.train_config.resources.device_flops()
        if device_flops is None:
            raise ValueError("Resources must provide device_flops() for speedrun calculations.")
        return device_flops
# ---
def log_hyperparameters(self, hparams: dict[str, Any]):
        self.run.config.update(_convert_value_to_loggable_rec(hparams), allow_val_change=True)
# ---
def protect(self, tag: str) -> None:
        """Increment refcount for an image (job using it)."""
        self._image_refcounts[tag] = self._image_refcounts.get(tag, 0) + 1
# ---
def removeDirectory( self, lfn, rpc = '', url = '', timeout = None ):
    return self.__returnOK( lfn )
# ---
def chunk_memory(arr) -> int:
    """Calculate the amount of memory in bytes that a single chunk uses."""
    if hasattr(arr, "chunkmem"):
        return arr.chunkmem
    return array_memory(
        arr.dtype,
        to_chunksize(normalize_chunks(arr.chunks, shape=arr.shape, dtype=arr.dtype)),
    )
# ---
def make_harness_lm(self):
        if jax.process_index() == 0:
            return LevanterHarnessLM(self)
        else:
            raise ValueError("Only process 0 can create the harness")
# ---
import re
def is_allowed_specific_char(string):
    get_char = re.compile(r'[^a-zA-Z0-9.]')
    string = get_char.search(string)
    return not bool(string)
# ---
def show_info(self):
        QMessageBox.about(
            self, QApplication.applicationName(),
            "%s %s\n"
            "Copyright (c) by %s" %
            (
                QCoreApplication.applicationName(),
                QCoreApplication.applicationVersion(),
                QCoreApplication.organizationName(),
            )
        )
# ---
def permutation(key, x: NamedArray, axis: AxisSelector, independent: bool = False):
    axis_index = x.axis_indices(axis)
    jax_array = jrandom.permutation(key, x.array, axis_index, independent=independent)
    return haliax.auto_sharded(NamedArray(jax_array, x.axes))
# ---
def make_dialogue(self, seeds):
        """Returns a list of sentences, each being a list of tokens."""
        acts = self.make_speech_bits(seeds)
        seq_map = self.simplify(acts)
        sents = self.report_seq(seq_map)
        return(sents)
# ---
def map_batches(self, fn: MapFunction[Sequence[U]], *extra_args, **extra_kwargs) -> "BatchMappedAsyncDataset[U]":
        return BatchMappedAsyncDataset(self, fn, *extra_args, **extra_kwargs)
# ---
def output_exemplar(self) -> Any:
        """
        An exemplar of what this processor returns. This is used to determine the output schema of a dataset.
        """
        raise NotImplementedError
# ---
def chunksize(self):
        """A tuple indicating the chunk size of each corresponding array dimension."""
        return tuple(max(c) for c in self.chunks)
# ---
def vm_state_name(state: int) -> str:
    """Return enum name like 'VM_STATE_READY'."""
    try:
        return _VM_STATE.values_by_number[state].name
    except KeyError:
        return f"UNKNOWN({state})"
# ---
def __init__(self, array: T_ZarrArray, chunks: T_RegularChunks):
        self.array = array
        self.chunks = chunks
# ---
def call(self) -> "_PoolCallProxy[T]":
        return _PoolCallProxy(self)
# ---
def asr_model_type(self) -> Type["WhisperASRModel"]:
        return WhisperASRModel
# ---
def join_path(dir_url: PathType, child_path: str) -> str:
    """Combine a URL for a directory with a child path"""
    parts = urlsplit(str(dir_url))
    new_path = quote(join(unquote(parts.path), child_path))
    split_parts = (parts.scheme, parts.netloc, new_path, parts.query, parts.fragment)
    return urlunsplit(split_parts)
# ---
def remove_datatype(test_tuple, data_type):
  res = []
  for ele in test_tuple:
    if not isinstance(ele, data_type):
      res.append(ele)
  return (res)
# ---
def test_nansum(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, xp.nan]], chunks=(2, 2), spec=spec)
    b = cubed.nansum(a)
    assert_array_equal(
        b.compute(), np.nansum(np.array([[1, 2, 3], [4, 5, 6], [7, 8, np.nan]]))
    )
# ---
def get_choice_logprobs(self, choice):
            return np.full(len(choice.message.content), -1.0, dtype=np.float32)
# ---
def evaluation_interval_seconds(self) -> float:
        """Configured evaluation interval in seconds."""
        return self._evaluation_interval.to_seconds()
# ---
def output_list(self):
        ordered = self.token['ordered']
        body = self.renderer.placeholder()
        while self.pop()['type'] != 'list_end':
            body += self.tok()
        return self.renderer.list(body, ordered)
# ---
def sum_div(number):
    divisors = [1]
    for i in range(2, number):
        if (number % i)==0:
            divisors.append(i)
    return sum(divisors)
# ---
def __init__(self):
        """Initialize the ParseState."""

        self.tokens = []
        self.values = []
# ---
def cifar_image_augmentation(images):
  """Image augmentation suitable for CIFAR-10/100.

  As described in https://arxiv.org/pdf/1608.06993v3.pdf (page 5).

  Args:
    images: a Tensor.
  Returns:
    Tensor of the same shape as images.
  """
  images = tf.image.resize_image_with_crop_or_pad(images, 40, 40)
  images = tf.random_crop(images, [32, 32, 3])
  images = tf.image.random_flip_left_right(images)
  return images
# ---
def __new__(cls, df, lmbda = 0):
        assert df >= 1
        d1 = NormalDistr(sqrt(lmbda))**2
        if df == 1:
            return d1
        d2 = ChiSquareDistr(df - 1)
        ncc2 = super(NoncentralChiSquareDistr, cls).__new__(cls, d1, d2)
        super(NoncentralChiSquareDistr, ncc2).__init__(d1, d2)
        ncc2.df = df
        ncc2.lmbda = lmbda
        return ncc2
# ---
def figure(self):
        rand = randint(0, FIG_LEN)
        return self.__figures[rand % FIG_LEN]
# ---
def host(self):
        """
        Gets the host of this ContributorOrcid.

        :return: The host of this ContributorOrcid.
        :rtype: str
        """
        return self._host
# ---
def _vm_exists_in_db(self, uuid):
        try:
            models.VM.objects.get_one(filters={'uuid': uuid})
            return True
        except exceptions.RecordNotFound:
            return False
# ---
def is_fuse_candidate(primitive_op: PrimitiveOperation) -> bool:
    """
    Return True if a primitive operation is a candidate for blockwise fusion.
    """
    return primitive_op.pipeline.function == apply_blockwise and (
        primitive_op.fusable_with_predecessors or primitive_op.fusable_with_successors
    )
# ---
def step_tracking_hook(info):
                current_step = int(info.step)
                self.all_steps_seen.append(current_step)
                self._track_training_step()
                current_loss = float(info.loss)
                self.losses.append(current_loss)
# ---
def summarize_document(doc: dict) -> DocumentSummary:
    """Validate that a document dict is in valid Dolma-format, and return summary (e.g., footprint in bytes, etc.)."""
    validated_doc = DolmaDocument.model_validate(doc)
    json_blob = json.dumps(doc)
    return DocumentSummary(document_bytes=get_size_bytes(json_blob), text_bytes=get_size_bytes(validated_doc.text))
# ---
def __init__(self, resistance, voltage):
        self._resistance = resistance
        self.voltage = voltage
# ---
def actual_head_size(self) -> int:
        return self.head_dim or (self.hidden_dim // self.num_heads)
# ---
def send_keys_to_element(self, element, *keys_to_send):
        """Sends keys to an element.
        Args:
            element: The element to send keys.
            keys_to_send: The keys to send.
        """
        self._actions.append(lambda:
            element.send_keys(*keys_to_send))
        return self
# ---
def configurable_base(cls):
        return Resolver
# ---
def sink_attention_jax_flash(
    query,
    key,
    value,
    sinks,
    sm_scale: float = 0.125,
    sliding_window: int | None = None,
    start_q: int = 0,
    *,
    block_size: int = 64,
):
    return sink_attention(
        query,
        key,
        value,
        sinks,
        sm_scale,
        sliding_window,
        start_q,
        attn_backend=AttentionBackend.JAX_FLASH,
        block_size=block_size,
        inference=True,
    )
# ---
def remove_matching_braces(s, brace_open="{", brace_close="}"):
    """Remove content up to and including the first matching brace pair."""
    end_pos = find_matching_brace(s, 0, brace_open, brace_close)
    return s[: end_pos + 1] if end_pos is not None else s
# ---
def _var_aggregate(a, correction=None, **kwargs):
    return nxp.divide(a["M2"], a["n"] - correction)
# ---
def num_chips(self) -> int:
        """Get the number of accelerator chips."""

        return _num_accelerator_chips(self.train_config.resources)
# ---
def scale_image(image_data):
    """
    Given an array of scalar data, rescale the data to the range [0, 255].
    """
    data_min = np.nanmin(image_data)
    data_max = np.nanmax(image_data)

    image_data = 255 * (image_data - data_min) / (data_max - data_min)
    image_data = np.minimum(image_data, 255)
    image_data = np.maximum(image_data, 0)
    image_data[np.isnan(image_data)] = 0

    return image_data
# ---
def process_loglikelihood(self, packed_request):
        out = self._jit_loglikelihood(self.model, packed_request)
        return out
# ---
def __repr__(self) -> str:  # pragma: no cover - trivial
        return self.name
# ---
def equal(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.equal](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.equal.html)
    """
    return jnp.equal(x1, x2)
# ---
def BufferIsVisible( buffer_number ):
  if buffer_number < 0:
    return False
  window_number = GetIntValue( "bufwinnr({0})".format( buffer_number ) )
  return window_number != -1
# ---
def selection_function(out_key):
        out_coords = out_key[1:]
        block_id = out_coords
        return get_item_with_depth(x.chunks, block_id, depth[0])
# ---
def parabola_vertex(a, b, c): 
  vertex=(((-b / (2 * a)),(((4 * a * c) - (b * b)) / (4 * a))))
  return vertex
# ---
def __tree_pp__(self, **kwargs):
        # For Equinox's tree pretty printer
        import jax._src.pretty_printer as pp

        if kwargs.get("short_arrays", True) and is_jax_array_like(self.array):
            return pp.text(f"Named({self.dtype}{self.shape})")
        else:
            return pp.text(str(self))
# ---
def from_id(cls, notebook, platform_id, job_id):
        return SaagieJob(
            notebook,
            requests.get(JOB_URL_PATTERN % (platform_id, job_id), auth=SAAGIE_BASIC_AUTH_TOKEN).json())
# ---
def pathJoin(parent, base):
    if parent.endswith('/'):
        return parent + base
    return parent + '/' + base
# ---
def process_bind_param(self, value, dialect):
                raise MyException("nope")
# ---
def array_value(self, decode_str=False):
        return [ self.values[i].node_value(decode_str) for i in range(self.num) ]
# ---
def filename(self):
        return self._filename
# ---
def test_connect_to_region(self):
        """Can connect to a dynamo region"""
        conn = DynamoDBConnection.connect("us-west-1")
        self.assertIsNotNone(conn.host)
# ---
def argmin(x, /, *, axis=None, keepdims=False, split_every=None):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in argmin")
    if axis is None:
        x = reshape(x, (-1,))
        axis = 0
        keepdims = False
    return arg_reduction(
        x,
        nxp.argmin,
        axis=axis,
        keepdims=keepdims,
        split_every=split_every,
    )
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        h = activations.relu(self.first(x))
        h = activations.relu(self.second(h))
        return self.third(h)
# ---
def format_limits(limits):
    return format_line(prefix='limits'.rjust(RJUST), values=limits)
# ---
def testAssertMsg(self):
    want = (0, "AssertionError('foo',)\n")
    self.assertEqual(want, _GrumpRun(textwrap.dedent("""\
        try:
          assert False, 'foo'
        except AssertionError as e:
          print repr(e)""")))
# ---
def decodeFailed(self, seg):
        if ( seg == None ): return
        mt.log.debug("Segment failed to decode: " + seg.msgid)
        self.segFailed(seg)
# ---
def __init__(self, registry: VmRegistry) -> None:
        self._registry = registry
# ---
def compute_blake2_hex(text: str) -> str:
    return bytes(hash_blake2(text.encode("utf-8"))).hex()
# ---
def __create_launcher(self, run_root):
		launcher = self.__task.data['backend']['launcher']
		return self.__launcher_mgr.create_launcher(launcher, run_root)
# ---
def cast_floats(x):
        if eqx.is_array(x) and jnp.issubdtype(x.dtype, jnp.floating):
            return x.astype(expected_float_dtype)
        return x
# ---
def check_format(self, sample_str: str) -> bool:
        try:
            _ = extract_boxed(sample_str)
            return True
        except ValueError:
            return False
# ---
def __init__(self, root, cwd, files, badfn=None):
        super(exactmatcher, self).__init__(root, cwd, badfn)

        if isinstance(files, list):
            self._files = files
        else:
            self._files = list(files)
# ---
def test_mixed_schedule__has_consistent_collated_batches(
    train_config: TrainConfig, schedule: TrainSchedule
):
    # Exposes underling consistency issue
    train_config.batch_size = 4

    with make_loader(train_config, schedule=schedule) as loader:
        for _ in itertools.islice(loader, 2):
            pass
# ---
def map(self, fn: Callable[[T], R]) -> Dataset[R]:
        """Map a function over the dataset.

        Args:
            fn: Function to apply to each element

        Returns:
            New dataset with map operation appended

        Example:
            >>> from zephyr import Backend
            >>> ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)
            >>> Backend.execute(ds)
            [2, 4, 6]
        """
        return Dataset(self.source, [*self.operations, MapOp(fn)])
# ---
def _default_lm_head_fn(params: PyTree) -> jax.Array:
    return params.output_proj
# ---
def test_entrypoint_proto_roundtrip_preserves_bytes():
    """Bytes survive to_proto -> from_proto without deserialization."""
    ep = Entrypoint.from_callable(_add, 1, 2)
    original_bytes = ep.callable_bytes

    proto = ep.to_proto()
    ep2 = Entrypoint.from_proto(proto)

    assert ep2.callable_bytes == original_bytes
    fn, args, kwargs = ep2.resolve()
    assert fn(*args, **kwargs) == 3
# ---
def _write_yaml_to_memory(yaml: str, path: str = "memory://test.yaml"):
    with fsspec.open(path, "w") as f:
        f.write(yaml)
    return path
# ---
def match(self, item):
        for subq in self.subqueries:
            if subq.match(item):
                return True
        return False
# ---
def seq_to_names(self, sequence):
        return([self.speakers[id] for id in sequence])
# ---
def output_footnote(self):
        self.inline._in_footnote = True
        body = self.renderer.placeholder()
        key = self.token['key']
        while self.pop()['type'] != 'footnote_end':
            body += self.tok()
        self.footnotes.append({'key': key, 'text': body})
        self.inline._in_footnote = False
        return self.renderer.placeholder()
# ---
def enabled(self):
        return self._enabled
# ---
def comment_urlview(self):
        data = self.get_selected_item()
        comment = data.get('body') or data.get('text') or data.get('url_full')
        if comment:
            self.term.open_urlview(comment)
        else:
            self.term.flash()
# ---
def test_mem_write_byte_attribute_before_char(self):
        self.mda.mem_write_byte(3999, MDA_ATTR_INTENSITY)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x00, MDA_BRIGHT_GREEN, MDA_BLACK))
        self.mda.mem_write_byte(3998, 0xFF)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def add_ten(self, item):
        return item + 10
# ---
def name_search(self, cr, user, name, args=None, operator='ilike', context=None, limit=100):
        if not args:
            args = []
        ids = self.search(cr, user, [('serial', '=', name)]+ args, limit=limit, context=context)
        ids += self.search(cr, user, [('name', operator, name)]+ args, limit=limit, context=context)
        return self.name_get(cr, user, ids, context)
# ---
def test_random_add(spec, executor):
    a = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    b = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)

    c = xp.add(a, b)

    x = nxp.unique_values(c.compute(executor=executor))
    assert len(x) > 90
# ---
def max_sum_increasing_subsequence(arr, n): 
	max = 0
	msis = [0 for x in range(n)] 
	for i in range(n): 
		msis[i] = arr[i] 
	for i in range(1, n): 
		for j in range(i): 
			if (arr[i] > arr[j] and
				msis[i] < msis[j] + arr[i]): 
				msis[i] = msis[j] + arr[i] 
	for i in range(n): 
		if max < msis[i]: 
			max = msis[i] 
	return max
# ---
def sample_plugin():
    class TestPlugin(BasePlugin):
        @event.event
        def on_test(self):
            pass

    return TestPlugin
# ---
def __init__(self):
        self._saved_msg = []
# ---
def copy(self, cr, uid, id, default=None, context=None):
        if default is None:
            default = {}
        default = default.copy()
        default.update({'move_history_ids2': [], 'move_history_ids': []})
        return super(stock_move, self).copy(cr, uid, id, default, context=context)
# ---
def test_permutation_creates_valid_instance():
    length = 100
    prng_key = jrandom.PRNGKey(0)
    permutation = LcgPermutation(length, prng_key)
    assert permutation.length == length
    assert 0 < permutation.a < length
    assert 0 <= permutation.b < length
# ---
def __init__(self, **kwargs):
        self._callbacks = None
        super().__init__(**kwargs)
# ---
def test_beam_search_no_duplicate_sources(params, model_cfg, tokenizer):
    """Beam should deduplicate candidates with the same source."""
    results = beam_search(
        params=params,
        initial_programs=["x = 1 + 2\n"],
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(3),
        beam_size=8,
        expansions_per_beam=3,
        max_depth=3,
    )
    sources = [c.source for c in results]
    assert len(sources) == len(set(sources))
# ---
def serve(self):
        try:
            logger.info(f"Starting Levanter inference server on {self.config.host}:{self.config.port}")
            self._server = uvicorn.Server(uvicorn.Config(self.app, host=self.config.host, port=self.config.port))
            self._server.run()
        finally:
            self.shutdown()
# ---
def test_gpt2_configs(config_file):
    from levanter.main.train_lm import TrainLmConfig

    check_load_config(TrainLmConfig, config_file)
# ---
def is_administrator_role(role: int) -> bool:
    return role in {UserProfile.ROLE_REALM_ADMINISTRATOR, UserProfile.ROLE_REALM_OWNER}
# ---
def vocab_size(self) -> int:
        return self.Vocab.size
# ---
def large_document_dataset():
    """Generate 500 documents: 100 unique content values, each appears 5 times."""
    docs = []
    for content_id in range(100):
        for copy_id in range(5):
            docs.append(
                {
                    "id": content_id * 5 + copy_id,
                    "content": f"document_{content_id}",
                    "value": content_id * 1000 + copy_id,
                }
            )
    return docs
# ---
def test_dict(self):
        """Store and retrieve a dict"""
        self.make_table()
        data = {
            "i": 1,
            "s": "abc",
            "n": None,
            "l": ["a", 1, True],
            "b": False,
        }
        self.dynamo.put_item("foobar", {"id": "abc", "d": data})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["d"], data)
# ---
def modis_diff(domain, b, threshold=None):
    '''Compute (b2-b1) < threshold, a simple water detection index.

       This method may be all that is needed in cases where the threshold can be hand tuned.
    '''
    if threshold == None: # If no threshold value passed in, load it based on the data set.
        threshold = float(domain.algorithm_params['modis_diff_threshold'])
    return get_diff(b).lte(threshold)
# ---
def __getitem__(self, ind):
        return self.lines[ind]
# ---
def status(self) -> VmGroupStatus:
        """Compute status from current VM states."""
        snapshots = [
            VmSnapshot(
                vm_id=vm.info.vm_id,
                state=vm.info.state,
                address=vm.info.address,
                init_phase=vm.info.init_phase,
                init_error=vm.info.init_error,
            )
            for vm in self._vms
        ]
        return VmGroupStatus(vms=snapshots)
# ---
def list_jobs(self) -> list[JobInfo]:
        """List all jobs managed by this cluster.

        Returns:
            List of job information for all jobs (running, completed, and failed)
        """
        ...
# ---
def sub_remove(self, sub_id=None):
        self.command('sub_remove', sub_id)
# ---
def loss_scale_per_channel(self) -> Float[torch.Tensor, " var"]:
        return self._per_channel_scale
# ---
def __getitem__(self, key):
        idx = ndindex[key]
        newshape = idx.newshape(self.shape)
        # use broadcast trick so array chunks only occupy a single value in memory
        return broadcast_trick(nxp.empty)(newshape, dtype=self.dtype)
# ---
def test_trainer_overlapping_time_ranges_raises_error(train_config, caplog):
    """Creating a trainer with overlapping train + val times should error."""

    with MultitonScope():
        with pytest.raises(ValueError, match="Training time range.*"):
            Trainer(train_config)
# ---
def __repr__(self):
        return "<differencematcher m1=%r, m2=%r>" % (self._m1, self._m2)
# ---
def get_Number(n, k): 
    arr = [0] * n; 
    i = 0; 
    odd = 1; 
    while (odd <= n):   
        arr[i] = odd; 
        i += 1; 
        odd += 2;
    even = 2; 
    while (even <= n): 
        arr[i] = even; 
        i += 1;
        even += 2; 
    return arr[k - 1];
# ---
def cal_electbill(units):
 if(units < 50):
    amount = units * 2.60
    surcharge = 25
 elif(units <= 100):
    amount = 130 + ((units - 50) * 3.25)
    surcharge = 35
 elif(units <= 200):
    amount = 130 + 162.50 + ((units - 100) * 5.26)
    surcharge = 45
 else:
    amount = 130 + 162.50 + 526 + ((units - 200) * 8.45)
    surcharge = 75
 total = amount + surcharge
 return total
# ---
def _check_cluster_head_running(config_path: str) -> bool:
    """Check if a Ray cluster head is already running.

    Returns True if a cluster head is detected, False otherwise.
    """
    try:
        with ray_dashboard(DashboardConfig.from_cluster(config_path)):
            return True
    except Exception:
        return False
# ---
def octal_To_Decimal(n):  
    num = n; 
    dec_value = 0; 
    base = 1; 
    temp = num; 
    while (temp): 
        last_digit = temp % 10; 
        temp = int(temp / 10); 
        dec_value += last_digit*base; 
        base = base * 8; 
    return dec_value;
# ---
def test_instance_not_auto_disk_config(self):
        """Should not partition unless instance is marked as
        auto_disk_config.
        """
        self.instance_values['auto_disk_config'] = False
        self.assertIsPartitionCalled(False)
# ---
def gensym(name="op"):
    global sym_counter
    sym_counter += 1
    return f"{name}-{sym_counter:03}"
# ---
def run_logged(cmd: list[str] | str, **kwargs) -> subprocess.CompletedProcess:
    """Run a subprocess command with logging.

    Args:
        cmd: Command to run (list or string)
        **kwargs: Arguments passed to subprocess.run

    Returns:
        CompletedProcess result from subprocess.run
    """
    if isinstance(cmd, list):
        cmd_str = shlex.join(cmd)
    else:
        cmd_str = cmd

    logger.info(f"Running command: {cmd_str}")
    return subprocess.run(cmd, **kwargs)
# ---
def test_scheduler_returns_empty_when_no_workers(scheduler, state, job_request):
    """Verify scheduler returns empty result when no workers available."""
    submit_job(state, "j1", job_request())

    pending_tasks = state.peek_pending_tasks()
    workers = state.get_available_workers()  # Empty

    result = scheduler.find_assignments(pending_tasks, workers)

    assert len(result.assignments) == 0
    assert len(result.timed_out_tasks) == 0
# ---
def with_output_path(self, output_path: str) -> "ExecutorStep":
        """Return a copy of the step with the given output_path."""
        return replace(self, override_output_path=output_path)
# ---
def on_fuzzing_start_changed(self, value: int):
        self.ui.spinBoxFuzzingEnd.setMinimum(self.ui.spinBoxFuzzingStart.value())
        new_start = self.message.convert_index(value - 1, self.proto_view, 0, False)[0]
        self.current_label.start = new_start
        self.current_label.fuzz_values[:] = []
        self.update_message_data_string()
        self.fuzz_table_model.update()
        self.ui.tblFuzzingValues.resize_me()
# ---
def compute_collinear_mask(v1, v2):
    norm1 = np.linalg.norm(v1, axis=1, keepdims=True)
    norm2 = np.linalg.norm(v2, axis=1, keepdims=True)
    v1 = v1 / (norm1 + 1e-6)
    v2 = v2 / (norm2 + 1e-6)
    mask_angle = np.abs(np.sum(v1 * v2, axis=1)) < 0.9063
    mask_overlap1 = norm1.reshape(-1) > 1e-2
    mask_overlap2 = norm2.reshape(-1) > 1e-2
    return mask_angle & mask_overlap1 & mask_overlap2
# ---
def _create_lm_example(tokens):
            tokens = hax.named(tokens, self.Pos)
            example = LmExample.causal(
                tokens=tokens,
                eos_id=eos_id,
                block_cross_document_attention=block_cross_document_attention,
            )

            example = jax.lax.with_sharding_constraint(example, sharding)

            return example
# ---
def importance_sampling_transformation(self, t):
    f_T = torch.log1p(- torch.exp(- self.sigma_max))
    f_0 = torch.log1p(- torch.exp(- self.sigma_min))
    sigma_t = - torch.log1p(- torch.exp(t * f_T + (1 - t) * f_0))
    return (sigma_t - self.sigma_min) / (
      self.sigma_max - self.sigma_min)
# ---
def __init__(self, object_ref: ray.ObjectRef):
        self._object_ref = object_ref
# ---
def without_axes(axis_spec: AxisSpec, to_remove: AxisSelection, allow_mismatched_sizes=False) -> AxisSpec:  # type: ignore
    ...
# ---
def render(self, task: "Task") -> RenderableType:
        text = (
            self.finished_text
            if not task.started or task.finished
            else self.spinner.render(task.get_time())
        )
        return text
# ---
def nancumsum(a: NamedArray, axis: AxisSelector, *, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.nancumsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nancumsum.html)
    """
    return wrap_axiswise_call(jnp.nancumsum, a, axis, dtype=dtype, single_axis_only=True)
# ---
def test_hf_audio_loading_source():
    # Use the Real Librispeech Valudation. Testing one doesn't support streaming.
    ac = AudioDatasetSourceConfig(id="WillHeld/test_librispeech_parquet", text_key="text")
    audio_iterator = iter(ac.get_shard_source("validation"))
    for i in range(10):
        audio, sample, text = next(audio_iterator)
# ---
def _start_daemon(self):
        ovn_nbctl = self._get_ovn_controller(self.install_method)
        return ovn_nbctl.start_daemon()
# ---
def test_equal_4(self):
        self.assertEqual(string_color('Hayden Smith'), '7E00EE')
# ---
def __init__(self, array: NamedArray):
        self.array = array
# ---
def test_int_array_index_2d(spec, ind):
    a = xp.asarray(
        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]],
        chunks=(3, 3),
        spec=spec,
    )
    b = a.rechunk((2, 2))  # force materialization to test indexing against zarr
    x = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])
    assert_array_equal(b[ind].compute(), x[ind])
# ---
def test_str_get_to_numeric(self):
        def test_impl(df):
            B = df.A.str.split(',')
            C = pd.to_numeric(B.str.get(1), errors='coerce')
            return C

        df = pd.DataFrame({'A': ['AB,12', 'C,321,D']})
        hpat_func = self.jit(locals={'C': types.int64[:]})(test_impl)
        pd.testing.assert_series_equal(
            hpat_func(df), test_impl(df), check_names=False)
# ---
def _process_sigma(self, sigma):
    if sigma is None:
      assert self.parameterization == 'ar'
      return sigma
    if sigma.ndim > 1:
      sigma = sigma.squeeze(-1)
    if not self.time_conditioning:
      sigma = torch.zeros_like(sigma)
    assert sigma.ndim == 1, sigma.shape
    return sigma
# ---
def training_data(self) -> Iterator[DataExample]:
        train_dataset = _get_hendrycks_math_train()

        for idx, item in enumerate(train_dataset):
            raw_prompt = item["problem"]
            raw_answer = item["solution"]
            example_id = f"train_{idx}"

            yield self.clean_example(raw_prompt, raw_answer, example_id)
# ---
def sum_num(numbers):
    total = 0
    for x in numbers:
        total += x
    return total/len(numbers)
# ---
def __setstate__(self, state: dict) -> None:
        self._endpoint_name = state["endpoint_name"]
        self._client = None
# ---
def tm_function(d, Nres):
    d0 = 1.24 * (torch.clip(Nres, min=19) - 15) ** (1 / 3) - 1.8
    return 1 / (1 + (d / d0) ** 2)
# ---
def find_Points(l1,r1,l2,r2): 
    x = min(l1,l2) if (l1 != l2) else -1
    y = max(r1,r2) if (r1 != r2) else -1
    return (x,y)
# ---
def test_shared(self):
        alice_pub = self.alice.get_public()
        bob_pub = self.bob.get_public()
        alice_shared = self.alice.compute_shared(bob_pub)
        bob_shared = self.bob.compute_shared(alice_pub)
        self.assertEquals(alice_shared, bob_shared)
# ---
def test_text(self):
        f = capture.CaptureIO()
        f.write("hello")
        s = f.getvalue()
        assert s == "hello"
        f.close()
# ---
def find_Min_Swaps(arr,n) : 
    noOfZeroes = [0] * n 
    count = 0 
    noOfZeroes[n - 1] = 1 - arr[n - 1] 
    for i in range(n-2,-1,-1) : 
        noOfZeroes[i] = noOfZeroes[i + 1] 
        if (arr[i] == 0) : 
            noOfZeroes[i] = noOfZeroes[i] + 1
    for i in range(0,n) : 
        if (arr[i] == 1) : 
            count = count + noOfZeroes[i] 
    return count
# ---
def __post_init__(self) -> None:
        _ = self.inferred_head_dim
        if self.num_heads % self.num_kv_heads != 0:
            raise ValueError("num_heads must be divisible by num_kv_heads for grouped-query attention")
        if self.vocab_size <= 0:
            raise ValueError("vocab_size must be positive")
        if self.max_seq_len <= 0:
            raise ValueError("max_seq_len must be positive")
# ---
def label_prefix(self) -> str:
        return self.prefix or "iris"
# ---
def ds_processed_validate(ds_processed: xr.Dataset, deep=False):
    """Validation function for the preprocessing stage"""
    ds_processed_schema.validate(ds_processed)
    ds_processed_coords_schema.validate(
        ds_processed.coords
    )  # this should be part of the dataset validation (maybe raise an issue/pr?)
    if deep:
        _nan_test_deep(ds_processed)
# ---
def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger("levanter.json_logger")
        self._last_metrics: dict[str, Any] = {}
        self._summary_metrics: dict[str, Any] = {}
# ---
def test_count_statements_function():
    tree = ast.parse("def f():\n    return 1\n")
    # FunctionDef + Return = 2.
    assert count_statements(tree) == 2
# ---
def softbreak(node: RenderTreeNode, context: RenderContext) -> str:
    if context.do_wrap and _in_block("paragraph", node):
        return WRAP_POINT
    return "\n"
# ---
def _create_from_content(self, rname, content):
        '''create a temporary file and then call oc create on it'''
        fname = Utils.create_tmpfile(rname + '-')
        yed = Yedit(fname, content=content)
        yed.write()

        atexit.register(Utils.cleanup, [fname])

        return self._create(fname)
# ---
def test_vertices_equals_lat_lon(self):
        """Asserts that the "vertices" property is identical to the return
        value of to_lat_lon()."""
        assert_equal(self.polycircle.vertices, self.polycircle.to_lat_lon())
# ---
def test_freeze_returns_named_array():
    X = hax.Axis("x", 3)
    ref = hax.new_ref(hax.arange(X))
    frozen = hax.freeze(ref)
    assert isinstance(frozen, hax.NamedArray)
    assert frozen.axes == ref.axes
    assert jnp.allclose(frozen.array, ref.value().array)
# ---
import re
def find_char(text):
  return (re.findall(r"\b\w{3,5}\b", text))
# ---
def default(self, value):
        """Convert mongo.ObjectId."""
        if isinstance(value, ObjectId):
            return str(value)
        return super(ElasticJSONSerializer, self).default(value)
# ---
def quad_means(tmp_path, t_length):
    # based on sizes from https://gist.github.com/TomNicholas/c6a28f7c22c6981f75bce280d3e28283
    spec = cubed.Spec(tmp_path, allowed_mem="2GB", reserved_mem="100MB")
    u = cubed.random.random((t_length, 1, 987, 1920), chunks=(10, 1, -1, -1), spec=spec)
    v = cubed.random.random((t_length, 1, 987, 1920), chunks=(10, 1, -1, -1), spec=spec)
    uv = u * v
    m = xp.mean(uv, axis=0)
    return m
# ---
def dummy_weight_callback(model):
        time.sleep(0.1)  # Simulate some work
        return model
# ---
def get_ls_desc(desc, print_id=False):
    addendum = ' : ' + desc['id'] if print_id is True else ''
    if desc['class'] in ['applet', 'workflow']:
        return BOLD() + GREEN() + desc['name'] + ENDC() + addendum
    else:
        return desc['name'] + addendum
# ---
def saveConfig(plugin_name, data): 
    """
    @type plugin_name: str
    @type data: object
    """
    pass
# ---
def accelerator_type_name(accel_type: int) -> str:
    """Return enum name like 'ACCELERATOR_TYPE_TPU'."""
    try:
        return _ACCELERATOR_TYPE.values_by_number[accel_type].name
    except KeyError:
        return f"UNKNOWN({accel_type})"
# ---
def reset_chaos() -> None:
    _rules.clear()
# ---
def rust_mark_paragraph_duplicates(
    batch: pa.RecordBatch, text_col: str, id_col: str, dup_map: dict, attribute_name: str
) -> pa.RecordBatch:
    # The mark function only needs the batch, map, and attr name.
    # text_col and id_col are ignored but kept for signature compatibility with the benchmark.
    return dupekit.mark_paragraph_duplicates(batch, dup_map, attribute_name)
# ---
def test_binary_equal(self):
        """Binary should eq other Binaries and also raw bytestrings"""
        self.assertEqual(Binary("a"), Binary("a"))
        self.assertEqual(Binary("a"), b"a")
        self.assertFalse(Binary("a") != Binary("a"))
# ---
def _trans_rollback_fn(self, is_transaction=False):
        def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
            raise Exception("breakage")
        return go
# ---
def deg2rad(a: A) -> A:
    return wrap_elemwise_unary(jnp.deg2rad, a)
# ---
def __iter__(self):
        return iter(self._variables)
# ---
def ix(self, word):
        """
        Returns the index on self.vocab and self.clusters for 'word'
        """
        temp = np.where(self.vocab == word)[0]
        if temp.size == 0:
            raise KeyError("Word not in vocabulary")
        else:
            return temp[0]
# ---
def product_Equal(n): 
    if n < 10: 
        return False
    prodOdd = 1; prodEven = 1
    while n > 0: 
        digit = n % 10
        prodOdd *= digit 
        n = n//10
        if n == 0: 
            break; 
        digit = n % 10
        prodEven *= digit 
        n = n//10
    if prodOdd == prodEven: 
        return True
    return False
# ---
def __create_context(self, main_root):
		context_dir = os.path.join(main_root, 'context')
		safe_mkdir(context_dir)

		action_default = {}
		for unit, param in self.__task.data['workflow'].items():
			action_default[unit] = self.__action_mgr.default_config(param['type'])

		navigators = self.__config_mgr.navigator()
		context_format = self.__navigator_mgr.context_format(navigators)

		self.__context_mgr.create_context_file(self.__task.data, action_default, context_format, context_dir)
# ---
def test_roll_scalar_named_shift():
    H = Axis("H", 4)
    W = Axis("W", 3)

    arr = hax.arange((H, W))
    shift = hax.named(jnp.array(1), ())

    rolled = hax.roll(arr, shift, H)
    expected = jnp.roll(arr.array, shift.array, axis=0)

    assert rolled.axes == arr.axes
    assert jnp.all(rolled.array == expected)
# ---
def build_worker_image(
    tag: str,
    push: bool,
    dockerfile: str | None,
    context: str | None,
    platform: str,
    region: tuple[str, ...],
    project: str,
):
    """Build Docker image for Iris worker."""
    _build_image("worker", tag, push, dockerfile, context, platform, region, project)
# ---
def hexagonal_num(n): 
	return n*(2*n - 1)
# ---
def __init__(
        self,
        source: ShardedDataSource[T_co],
        fn: Callable[[list[T_co]], Iterable[U]],
        batch_size,
        num_cpus=1,
        num_gpus=0,
        output_exemplar=None,
        **resources,
    ):
        self.source = source
        self._transform = _BatchMapTransform(
            fn, batch_size, num_cpus, num_gpus, resources, output_exemplar=output_exemplar
        )
# ---
def mock_optimizer_transform() -> optax.GradientTransformation:
    def init_fn(params):
        return {"count": jnp.array(0, dtype=jnp.int32)}

    def update_fn(updates, state, params=None):
        new_state = {"count": state["count"] + 1}
        # simple transformation: negate gradients
        transformed_updates = jax.tree_util.tree_map(lambda g: -g, updates)
        return transformed_updates, new_state

    return optax.GradientTransformation(init_fn, update_fn)
# ---
def uniquePairs(index):
  """ list of unique, internal pairs """
  return list(combinations( range(index[0],index[-1]+1),2 ) )
# ---
def test_metric_fold():
    """fold() combines metrics correctly."""
    m1 = Metric.from_value(10.0, ReductionType.MEAN)
    m2 = Metric.from_value(20.0, ReductionType.MEAN)
    result = fold(m1, m2)

    assert result.reduction == ReductionType.MEAN
    assert jnp.allclose(result.value(), 15.0)
# ---
def is_triangleexists(a,b,c): 
    if(a != 0 and b != 0 and c != 0 and (a + b + c)== 180): 
        if((a + b)>= c or (b + c)>= a or (a + c)>= b): 
            return True 
        else:
            return False
    else:
        return False
# ---
def check_raise(self):
        if self._exception:
            raise self._exception
# ---
def setWidth(self, width):
        self.__width = width
# ---
def odd_Num_Sum(n) : 
    j = 0
    sm = 0
    for i in range(1,n + 1) : 
        j = (2*i-1) 
        sm = sm + (j*j*j*j)   
    return sm
# ---
def to_t(x, requires_grad=True):
        t = torch.from_numpy(np.array(x))
        t.requires_grad_(requires_grad)
        return t
# ---
def intermediate(self, x, static1, *, static2):
            assert static1 is True
            assert static2 is False
            return x + 2 * self.w + static1
# ---
def hidden_reparam(use_mup: bool = True) -> type[AbstractLinearReparam]:
        """Return the reparameterization class for a hidden linear layer."""

        return mup.HiddenLinearMup if use_mup else mup.LinearStandardParam
# ---
def test_profiler_get_shorten_id_int(self):
        short_id_int = 42
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        result = prof.get_shorten_id(short_id_int)
        expected = "2a"
        self.assertEqual(expected, result)
# ---
import sys 
def find_max_val(n, x, y): 
	ans = -sys.maxsize 
	for k in range(n + 1): 
		if (k % x == y): 
			ans = max(ans, k) 
	return (ans if (ans >= 0 and
					ans <= n) else -1)
# ---
def should_run(self) -> bool:
        """Check if enough time has passed; updates last run time if True."""
        now = time.monotonic()
        if self._last_run is None or (now - self._last_run >= self._interval):
            self._last_run = now
            return True
        return False
# ---
def corofunc():
            called[0] = True
# ---
def delete_slice(self, group_config: config_pb2.ScaleGroupConfig, slice_id: str) -> None:
        zones = _resolve_zones(group_config, self._platform)
        for zone in zones:
            if _gcloud_delete_tpu(self._platform.project_id, zone, slice_id):
                return
        raise RuntimeError(f"Failed to delete TPU slice {slice_id} in any zone")
# ---
def from_seconds(cls, timeout_seconds: float) -> "Deadline":
        """Create deadline from seconds in the future."""
        return cls(time.monotonic() + timeout_seconds)
# ---
def cache_dir(tmp_path):
    """Create a temporary cache directory."""
    cache = tmp_path / "cache"
    cache.mkdir()
    return cache
# ---
def block(idx: int, size: int) -> "dslice":
        """
        Returns a dslice that selects a single block of size `size` starting at `idx`
        """
        return dslice(idx * size, size)
# ---
def test_mem_read_word_just_past_the_end(self):
        self.mda.video_ram[3998] = 0x12
        self.mda.video_ram[3999] = 0x34
        self.assertEqual(self.mda.mem_read_word(3999), 0x0034)
# ---
def create_nano_llama_config() -> LlamaConfig:
    """Create a tiny LlamaConfig for fast testing."""
    return LlamaConfig(
        max_seq_len=64,
        hidden_dim=64,
        intermediate_dim=128,
        num_heads=8,
        num_kv_heads=8,
        num_layers=4,
        tokenizer=DummyTokenizer(),
    )
# ---
def is_note_on(self):
        '''
        return a boolean if this is a NOTE_ON message and velocity is not zero_

        >>> mt = MidiTrack(1)
        >>> me1 = MidiEvent(mt)
        >>> me1.type_ = "NOTE_ON"
        >>> me1.velocity = 120
        >>> me1.is_note_on()
        True
        >>> me1.is_note_off()
        False
        '''
        return self.type_ == "NOTE_ON" and self.velocity != 0
# ---
def _detect_tpu_environment() -> bool:
    """Detects whether the TPU environment variable TPU_NAME is set and non-empty."""
    return bool(os.environ.get("TPU_NAME"))
# ---
def action_done(self, cr, uid, ids, context=None):
        """Changes picking state to done.

        This method is called at the end of the workflow by the activity "done".
        @return: True
        """
        self.write(cr, uid, ids, {'state': 'done', 'date_done': time.strftime('%Y-%m-%d %H:%M:%S')})
        return True
# ---
def to_numpy(a):
    return np.asarray(a)
# ---
def test_default_return_capacity(self):
        """When default_return_capacity=True, always return capacity"""
        self.dynamo.default_return_capacity = True
        with patch.object(self.dynamo, "call") as call:
            call().get.return_value = None
            rs = self.dynamo.scan("foobar")
            list(rs)
        call.assert_called_with(
            "scan",
            TableName="foobar",
            ReturnConsumedCapacity="INDEXES",
            ConsistentRead=False,
        )
# ---
def __init__(self, pool_size=2, strides=None, padding='valid'):
        super(LW_MaxPooling1D, self).__init__(pool_size, strides, padding)
# ---
def BatchCreateSessions(self, request, context):
        """Creates multiple new sessions.

    This API can be used to initialize a session cache on the clients.
    See https://goo.gl/TgSFN2 for best practices on session cache management.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def check_ruff(files: list[pathlib.Path], fix: bool) -> int:
    if not files:
        return 0

    click.echo("\nRuff linter:")
    args = ["uvx", "ruff@0.14.3", "check"]
    if fix:
        args.extend(["--fix", "--exit-non-zero-on-fix"])

    file_args = [str(f.relative_to(ROOT_DIR)) for f in files]
    args.extend(file_args)

    return run_cmd(args).returncode
# ---
def model_type(self) -> Type["QwenLMHeadModel"]:
        return QwenLMHeadModel
# ---
def __lshift__(self, other, /):
        other = self._check_allowed_dtypes(other, "integer", "__lshift__")
        if other is NotImplemented:
            return other
        return elemwise(
            nxp.bitwise_left_shift, self, other, dtype=result_type(self, other)
        )
# ---
def check_docker_available():
    """Check if Docker is available and running."""
    try:
        result = subprocess.run(
            ["docker", "info"],
            check=True,
            capture_output=True,
            timeout=5,
        )
        return result.returncode == 0
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False
# ---
def add_arguments(parser: argparse.ArgumentParser):
    parser.add_argument("--model_path", type=lambda s: Path(s), required=True)
# ---
def __init__(self):
        self._actors: dict[str, Any] = {}
        self._actor_locks: dict[str, threading.Lock] = {}
# ---
def add(self, source: str, data: str, timestamp: Timestamp | None = None) -> None:
        if timestamp:
            self.lines.append(LogLine.at(timestamp, source, data))
        else:
            self.lines.append(LogLine.now(source, data))
# ---
def filter_oddnumbers(nums):
 odd_nums = list(filter(lambda x: x%2 != 0, nums))
 return odd_nums
# ---
def lavita_allprocessed_to_dolma(row):
    try:
        return {
            "id": hashlib.sha256((row["instruction"] + row["input"] + row["output"]).encode("utf-8")).hexdigest(),
            "text": row["instruction"] + "\n\n" + "Context: \n" + row["input"] + "\n\n" + "Answer: \n" + row["output"],
            "source": "lavita/medical-qa-datasets/all-processed",
        }
    except Exception as e:
        print(e)
        return None
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        """CPU FLOPS not tracked."""
        raise NotImplementedError("CPU FLOPS not available")
# ---
def _extract_node_name(resource_name: str) -> str:
    if "/" in resource_name:
        return resource_name.split("/")[-1]
    return resource_name
# ---
def shard_names(self) -> Sequence[str]:
        return self.base_ds.shard_names
# ---
def inner(x):
        # x.axes == (Hidden,) but x.array is a batched tracer with leading batch_dim
        return hax.auto_sharded(x)
# ---
def open(self, chunks: dict[str, int] | None = None) -> xr.Dataset:
        engine = "netcdf4" if self.path.suffix == ".nc" else "zarr"
        return xr.open_dataset(self.path, engine=engine, chunks=chunks)
# ---
def __init__(self):
        self._calls = []
# ---
def __init__(self, *args, **kwargs):
        """Product

        :param int integer
        :param str string
        """
        self.integer = None
        self.string = None

        super(Product, self).__init__(*args, **kwargs)
# ---
def test_deduplicate_all_duplicates(backend):
    """Test deduplication when all items have same key."""
    data = [{"id": 1, "val": f"item_{i}"} for i in range(10)]

    ds = Dataset.from_list(data).deduplicate(key=lambda x: x["id"])

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0]["id"] == 1
# ---
def on_content(self, instance, value):
        Clock.schedule_once(lambda dt: self.ids.content.add_widget(value))
# ---
def __init__( self, **kwargs ):

    Client.__init__( self, **kwargs )
    opsH = Operations()
    self.maxResetCounter = opsH.getValue( 'Productions/ProductionFilesMaxResetCounter', 10 )

    self.setServer( 'Transformation/TransformationManager' )
# ---
def _on_eval_end(self, pl_module: LightningModule) -> None:
        """Restore original weights after evaluation.

        Parameters
        ----------
        pl_module: LightningModule
            The LightningModule instance.

        """
        if self.ema_initialized and self.eval_with_ema:
            self.restore_original_weights(pl_module)
# ---
def test_delete_missing(self):
        """Deleting a missing table returns False"""
        ret = self.dynamo.delete_table("foobar")
        self.assertTrue(not ret)
# ---
def stub_remove_volume_type_access(context, type_id, project_id):
            raise exception.VolumeTypeAccessNotFound(volume_type_id=type_id,
                                                     project_id=project_id)
# ---
def _write(path: Path, records: list[dict]) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        with gzip.open(path, "wt", encoding="utf-8") as handle:
            for record in records:
                handle.write(json.dumps(record))
                handle.write("\n")
# ---
def _load_tokenizer(tokenizer_name: str) -> transformers.PreTrainedTokenizer:
    """Load and cache a tokenizer by name"""
    return load_tokenizer_with_backoff(tokenizer_name)
# ---
def addSummaryKey(self, key):
        assert isinstance(key, str)
        return SummaryKeyMatcher.cNamespace().add_key(self, key)
# ---
def __init__(self, start: int, length: int | Axis):
        """
        As a convenience, if length is an Axis, it will be converted to `length.size`
        Args:
            start:
            length:
        """
        self.start = start
        if isinstance(length, Axis):
            self.size = length.size
        else:
            self.size = length
# ---
def __call__(self, target, cred):
        """Check the policy.

        Requires that at least one rule accept in order to return True.
        """

        for rule in self.rules:
            if rule(target, cred):
                return True

        return False
# ---
def _fold_in_key_vmap(key, indices):
    return jax.vmap(lambda i: jax.random.fold_in(key, i))(indices)
# ---
def sanitize(content: str, patterns: list[tuple[str, str]]) -> tuple[str, int]:
    """
    Apply sanitization patterns to content.
    Returns (sanitized_content, num_replacements).
    """
    total_replacements = 0
    for pattern, replacement in patterns:
        try:
            content, count = re.subn(pattern, replacement, content)
            total_replacements += count
        except re.error as e:
            log(f"Invalid regex pattern '{pattern}': {e}")
    return content, total_replacements
# ---
def setUp(self):
        super(NetappDirectCmodeNfsDriverOnlyTestCase, self).setUp()
        self._custom_setup()
# ---
def _read_parquet(path: Path) -> list[dict]:
    """Read records from a parquet file."""
    import pyarrow.parquet as pq

    table = pq.read_table(path)
    return table.to_pylist()
# ---
def any(
        self, axis: AxisSelection | None = None, *, where: "NamedArray" | None = None
    ) -> "NamedArray":  # pragma: no cover
        return haliax.any(self, axis=axis, where=where)
# ---
def test_add_non_handler(self):
        class NonHandler(object):
            pass
        self.assertRaises(TypeError,
                          OpenerDirector().add_handler, NonHandler())
# ---
def test_get_rrd_server(self):
        self.flags(xenapi_connection_url='myscheme://myaddress/')
        server_info = vm_utils.get_rrd_server()
        self.assertEqual(server_info[0], 'myscheme')
        self.assertEqual(server_info[1], 'myaddress')
# ---
def take(self, n: int):
        """
        Alias for `slice_dataset(end_index=n)`.
        """
        return self.slice_dataset(end_index=n)
# ---
def load_environment_from_spec(config: EnvConfig) -> MarinEnv:
    """Load an environment from the given configuration."""
    env_class = config.env_class
    env_args = config.env_args
    # Dynamically import the environment class
    module_name, class_name = env_class.rsplit(".", 1)
    env_module = __import__(module_name, fromlist=[class_name])
    env_class = getattr(env_module, class_name)

    # TODO(power) - thread random seed from the rollout worker.
    return env_class(**env_args)
# ---
def test_ring_buffer_fifo_eviction(ring_buffer):
    """Oldest records are evicted when buffer is full."""
    for i in range(15):
        ring_buffer.append(BufferedLogRecord(timestamp=float(i), level="INFO", logger_name="test", message=f"msg-{i}"))
    results = ring_buffer.query()
    assert len(results) == 10
    assert results[0].message == "msg-5"
    assert results[-1].message == "msg-14"
# ---
def foo_open(self): pass
# ---
def __call__(self, environ, start_response):
        if environ['PATH_INFO'].startswith('/socket.io'):
            socketio_manage(environ, {'': QueueStatusHandler})
# ---
def on_operation_start(self, event):
        self.progress.start_task(self.progress_tasks[event.name])
# ---
def extract_version_from_path(gcs_path: str) -> str:
    """Extract the version name from a GCS path."""
    # Extract model name from path like "gs://marin-eu-west4/checkpoints/llama-8b-tootsie-0.001-19ad63/hf/"
    parts = gcs_path.strip("/").split("/")
    return parts[-3]
# ---
def bootstrap_config() -> config_pb2.BootstrapConfig:
    """Standard bootstrap configuration for tests."""
    return config_pb2.BootstrapConfig(
        controller_address="10.0.0.1:10000",
        worker_id="test-worker",
        worker_port=10001,
        docker_image="gcr.io/test/iris-worker:latest",
        cache_dir="/var/cache/iris",
    )
# ---
def set_mouse_visible(self, visible=True):
        """Show or hide the mouse cursor.

        The mouse cursor will only be hidden while it is positioned within
        this window.  Mouse events will still be processed as usual.

        :Parameters:
            `visible` : bool
                If True, the mouse cursor will be visible, otherwise it
                will be hidden.

        """
        self._mouse_visible = visible
        self.set_mouse_platform_visible()
# ---
def validate_stock_for_has_batch_and_has_serial(self):
		if self.stock_ledger_created():
			for value in ["has_batch_no", "has_serial_no"]:
				if frappe.db.get_value("Item", self.name, value) != self.get_value(value):
					frappe.throw(_("Cannot change {0} as Stock Transaction for Item {1} exist.".format(value, self.name)))
# ---
def _ensure_int_is_array(x):
    # who tf decided that bools are ints
    if isinstance(x, int) and not isinstance(x, bool):
        return jnp.array(x)
    else:
        return x
# ---
def __add__(self, other: "RunningMean"):
        return self.add(other.mean, other.total)
# ---
def bitwise_or(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.bitwise_or](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.bitwise_or.html)
    """
    return jnp.bitwise_or(x1, x2)
# ---
def serve(c):
    """Serve site at http://localhost:8000/"""

    class AddressReuseTCPServer(RootedHTTPServer):
        allow_reuse_address = True

    server = AddressReuseTCPServer(
        CONFIG['deploy_path'],
        ('', CONFIG['port']),
        ComplexHTTPRequestHandler)

    sys.stderr.write('Serving on port {port} ...\n'.format(**CONFIG))
    server.serve_forever()
# ---
def CRISPR_introduction(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'expression',
        'nucleic_acid_delivery_method': ['transient transfection']
    }
# ---
def forward(
        self,
        times,
        acc_a,
        next_a,
    ):
        next_a = self.norm_next(next_a)
        fourier_embed = self.fourier_embed(times)
        normed_fourier = (
            self.norm_fourier(fourier_embed)
            .unsqueeze(1)
            .expand(-1, next_a.shape[1], -1)
        )
        cond_a = torch.cat((acc_a, normed_fourier), dim=-1)

        acc_a = acc_a + self.transition_block(next_a, cond_a)

        return acc_a
# ---
def convert_th(self, el, text, convert_as_inline):
        if convert_as_inline:
            return text + " "
        colspan = 1
        if "colspan" in el.attrs:
            colspan = _try_convert_int(el["colspan"], 1)
        return " " + text.strip().replace("\n", " ") + " |" * colspan
# ---
def deterministic_failure_modal_no_retries(i, path=None, timing_map=None, *, name=None):
    return deterministic_failure(path, timing_map, i, name=name)
# ---
def _is_relative_path(url_or_path):
    # if it's a url, it's not a relative path
    parsed_url = urlparse(url_or_path)

    if parsed_url.scheme:
        return False

    # otherwise if it starts with a slash, it's not a relative path
    return not url_or_path.startswith("/")
# ---
def __unicode__(self):
		return nom_estado
# ---
def footnote_ref(self, key, index):
        """Rendering the ref anchor of a footnote.

        :param key: identity key for the footnote.
        :param index: the index count of current footnote.
        """
        html = (
            '<sup class="footnote-ref" id="fnref-%s">'
            '<a href="#fn-%s" rel="footnote">%d</a></sup>'
        ) % (escape(key), escape(key), index)
        return html
# ---
def __array__(self, dtype=None):
        return np.asarray(self.x, dtype=dtype)
# ---
def blockIndentation(block):
            text = block.text()
            return text[:len(text) - len(text.lstrip())]
# ---
def detect_paired_end(path_fastqs):
    path_fastqs = [f for f in path_fastqs if f.endswith('.fastq') or f.endswith('.fq') or f.endswith('.fastq.gz') or f.endswith('.fq.gz')]
    if len(path_fastqs) % 2 == 1: return False, [path_fastqs, None, None, None]
    pair_obj = match_pairs(path_fastqs, True)
    path_fastqs = pair_obj[0]
    if pair_obj[1]==None: return False, pair_obj
    return True, pair_obj
# ---
def has_hf_files(path: str) -> bool:
        return fsspec_exists(os.path.join(path, "config.json"))
# ---
def combine_masks_and(mask1: NamedArray | None, mask2: NamedArray | None) -> NamedArray | None:
    if mask1 is None:
        return mask2
    if mask2 is None:
        return mask1
    return mask1 & mask2.broadcast_axis(mask1.axes)
# ---
def left_shift(x1: NamedOrNumeric, x2: NamedOrNumeric, /) -> NamedOrNumeric:
    """
    Named version of [jax.numpy.left_shift](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.left_shift.html)
    """
    return jnp.left_shift(x1, x2)
# ---
def test_no_name_exception(self):
        def define_class_with_no_name():
            @six.add_metaclass(profiler.TracedMeta)
            class FakeTraceWithMetaclassNoName(FakeTracedCls):
                pass
        self.assertRaises(TypeError, define_class_with_no_name, 1)
# ---
def test_pareto():
    b = hax.arange(Width, start=0.1)

    check_gen_is_equal(
        lambda k, s: jax.random.pareto(k, b.array.reshape(1, -1), shape=s), lambda k, s: hax.random.pareto(k, s, b)
    )
# ---
def __call__(self, carry):
            return carry + self.w, 2 * self.w
# ---
def validate_seed_results(self, results: list[tuple[str, str]]) -> bool:
        """Validate that seed jobs completed as expected."""
        expected = ["JOB_STATE_SUCCEEDED"] * len(results)
        actual = [r[1] for r in results]
        return actual == expected
# ---
def title(self):
        return self.__title
# ---
def _failing_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            monkeypatch.setattr(_api.ffi, "errno", errno.EINVAL)
            return -1
# ---
def timing(self, stats, time, sample_rate=1):
        """
        Log timing information
        """
        unit = 'ms'
        log.debug('%r took %s %s' % (stats, time, unit))
        self.update_stats(stats, "%s|%s" % (time, unit), sample_rate)
# ---
def last_Two_Digits(N): 
    if (N >= 10): 
        return
    fac = 1
    for i in range(1,N + 1): 
        fac = (fac * i) % 100
    return (fac)
# ---
def _custom_setup(self):
        kwargs = {}
        kwargs['netapp_mode'] = 'proxy'
        kwargs['configuration'] = create_configuration()
        self._driver = netapp_nfs.NetAppDirectCmodeNfsDriver(**kwargs)
# ---
def loss_fast(v):
        return jnp.sum(template_op(v))
# ---
def predecessor_ops(dag, name):
    """Return an op node's op predecessors in the same order as the input source arrays for the op.

    Note that each input source array is produced by a single predecessor op.
    """
    nodes = dict(dag.nodes(data=True))
    for input in nodes[name]["primitive_op"].source_array_names:
        pre_list = list(predecessors_unordered(dag, input))
        assert len(pre_list) == 1  # each array is produced by a single op
        yield pre_list[0]
# ---
def is_worker_failure(self) -> bool:
        """Whether this attempt failed due to worker death (derived from state)."""
        return self.state == cluster_pb2.TASK_STATE_WORKER_FAILED
# ---
def getCounters( self, table, attrList, condDict, older = None, newer = None, timeStamp = None,
                   rpc = '', url = '' ):
    rpcClient = self._getRPC( rpc = rpc, url = url )
    return rpcClient. getCounters( table, attrList, condDict, older, newer, timeStamp )
# ---
def test_buffered_row_result_proxy(self):
        self._test_proxy(_result.BufferedRowResultProxy)
# ---
def test_processed_data(processed_data):
    ds_processed_validate(processed_data)
# ---
def stop(self):
        """Stop the inference worker loop and server."""
        with self._shutdown_condition:
            self._running = False
            self._transfer_client.cleanup()
            self._shutdown_condition.notify()

        # Wait for the main loop to finish
        self._shutdown_complete.wait()

        # Now shutdown the inference server
        self._policy_ctx.shutdown()

        if self.weight_transfer_thread:
            self.weight_transfer_thread.join()
# ---
def norm_config(self) -> LayerNormConfigBase:
        return RmsNormConfig(use_weight=self.use_layer_norm_weight, use_bias=self.use_bias, eps=self.layer_norm_epsilon)
# ---
def fetch_users_by_id(user_ids: List[int]) -> List[UserProfile]:
        return list(UserProfile.objects.filter(id__in=user_ids).select_related())
# ---
def spec(tmp_path):
    return cubed.Spec(tmp_path, allowed_mem=1000000)
# ---
def _create_volume(self, size='0'):
        """Create a volume object."""
        vol = {}
        vol['size'] = size
        vol['user_id'] = 'fake'
        vol['project_id'] = 'fake'
        vol['host'] = 'localhost'
        vol['availability_zone'] = FLAGS.storage_availability_zone
        vol['status'] = "creating"
        vol['attach_status'] = "detached"
        return db.volume_create(self.context, vol)
# ---
def not_found(error):
    return make_response(jsonify( { 'error': 'Page Not Found' } ), 404)
# ---
def babylonian_squareroot(number):
    if(number == 0):
        return 0;
    g = number/2.0;
    g2 = g + 1;
    while(g != g2):
        n = number/ g;
        g2 = g;
        g = (g + n)/2;
    return g;
# ---
def prepare_sqlite(self):
        self.dbname = "__testdb__smadata2_%s_.sqlite" % self.__class__.__name__
        self.bakname = self.dbname + ".bak"

        # Start with a blank slate
        removef(self.dbname)
        removef(self.bakname)

        self.prepopulate()

        if os.path.exists(self.dbname):
            self.original = open(self.dbname).read()
        else:
            self.original = None
# ---
def get_batch_sync(self, indices) -> List[T]:
        # TODO: would be better to batch these up
        grouped = jtu.tree_map(lambda reader: reader.get_batch_sync(indices), self.tree, is_leaf=heuristic_is_leaf)

        out = [jtu.tree_map(lambda _, leaf: leaf[i], self.tree, grouped) for i in range(len(indices))]

        return out
# ---
def get_task(self, task_id: str) -> TaskInfo | None: ...
# ---
def __init__(self,occurrence):
        self.occurrence = occurrence
        self.number_of_arguments = 0
        if self.occurrence[1][1] == "[]":
            self.number_of_arguments = self.occurrence[1][0]
        self.name = self.occurrence[0][0]#[0]
        self.definition = self.occurrence[-1][0]
# ---
def __repr__dict__(self):
        return {'id':self.id,
                'analysis_id':self.analysis_id,
            'experiment_id':self.experiment_id,
            'sample_name_abbreviation':self.sample_name_abbreviation,
            'sample_name':self.sample_name,
            'time_point':self.time_point,
            'analysis_type':self.analysis_type,
            'used_':self.used_,
            'comment_':self.comment_}
# ---
def find_longest_conseq_subseq(arr, n): 
	ans = 0
	count = 0
	arr.sort() 
	v = [] 
	v.append(arr[0]) 
	for i in range(1, n): 
		if (arr[i] != arr[i - 1]): 
			v.append(arr[i]) 
	for i in range(len(v)): 
		if (i > 0 and v[i] == v[i - 1] + 1): 
			count += 1
		else: 
			count = 1
		ans = max(ans, count) 
	return ans
# ---
def state(self):
        """Return the state of the sensor."""
        return self._state
# ---
def md5_for_str(content):
    hash_md5 = hashlib.md5()
    hash_md5.update(content.encode())
    return str(hash_md5.hexdigest())
# ---
def parse_row(self, data, reclen=0):
		"""
		Assign data to a DBRow instance
		"""
		return DBRow(self, data=data, reclen=reclen)
# ---
import re  
def remove(list): 
    pattern = '[0-9]'
    list = [re.sub(pattern, '', i) for i in list] 
    return list
# ---
def trunc(a: A) -> A:
    return wrap_elemwise_unary(jnp.trunc, a)
# ---
def tracker(name):
            def go(conn, cursor, statement, parameters, context, executemany):
                canary.append((statement, context))
            return go
# ---
def define_tables(cls, metadata):
        Table(
            "is_distinct_test",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("col_a", Integer, nullable=True),
            Column("col_b", Integer, nullable=True),
        )
# ---
def __init__(self, m1, m2):
        super(differencematcher, self).__init__(m1._root, m1._cwd)
        self._m1 = m1
        self._m2 = m2
        self.bad = m1.bad
        self.traversedir = m1.traversedir
# ---
def find_location(self):
        for course, score in self.courses:
            location = str( course.rec["BUILDING"] )+ " " + str( course.rec["ROOM"] )
            # just need to find the first one, so break after this happens
            break   
        return location
# ---
def test_ics(self):
        """
        Check ICS output
        """
        escala = Escala('fixtures/escala_ics.xml')
        f_result = open(self.dir.get_data_dir() + 'fixtures/escala.ics')
        self.assertEqual(escala.ics(), f_result.read())
        f_result.close()
# ---
def __mod__(self, other, /):
        other = self._check_allowed_dtypes(other, "real numeric", "__mod__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.remainder, self, other, dtype=result_type(self, other))
# ---
def __exit__(self, exc_type, exc_val, exc_tb):
        global _global_tracker
        _global_tracker = self.old_tracker
# ---
def test_vmap_multiple_axes():
    Batch1 = Axis("Batch1", 4)
    Batch2 = Axis("Batch2", 3)
    Width = Axis("Width", 2)
    Depth = Axis("Depth", 5)

    named = hax.random.uniform(PRNGKey(0), (Batch1, Batch2, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, (Batch1, Batch2))(named)

    expected = jnp.sum(named.array, axis=2)

    assert jnp.allclose(selected.array, expected)
    assert selected.axes == (Batch1, Batch2, Depth)
# ---
def test_index_1d(spec, ind):
    a = xp.arange(12, chunks=(4,), spec=spec)
    assert_array_equal(a[ind].compute(), np.arange(12)[ind])
# ---
def is_numeric_str(s: str) -> bool:
        try:
            float(s)
            return True
        except ValueError:
            return False
# ---
def check(arr,n): 
    g = 0 
    for i in range(1,n): 
        if (arr[i] - arr[i - 1] > 0 and g == 1): 
            return False
        if (arr[i] - arr[i] < 0): 
            g = 1
    return True
# ---
def is_Even(n) : 
    if (n^1 == n+1) :
        return True; 
    else :
        return False;
# ---
def test_encrypt_message_with_newlines_at_end(self):
        self._test_encryption('This message has a newline at the end.\n')
# ---
def sign(x, /):
    if x.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in sign")
    return elemwise(nxp.sign, x, dtype=x.dtype)
# ---
def err2(context):
            stmt = context.statement
            if ("ERROR ONE" in str(stmt) or "ERROR FOUR" in str(stmt)) \
                    and isinstance(context.chained_exception, MyException1):
                raise MyException2("my exception chained")
            elif "ERROR TWO" in str(stmt):
                return context.chained_exception
            else:
                return None
# ---
def generate(self, prompts: list[str], sampling_params: SamplingParams) -> str:
        """
        Synchronous generate method - runs async code under the hood.

        Args:
            prompt: Input prompt
            max_tokens: Max tokens to generate

        Returns:
            Generated text
        """
        return self.bridge.run(self._generate_batch_async(prompts, sampling_params))
# ---
def test_ones(spec, executor):
    a = xp.ones((3, 3), chunks=(2, 2), spec=spec)
    assert_array_equal(a.compute(executor=executor), np.ones((3, 3)))
# ---
def test_bound_limit(self):
        table = self.tables.some_table
        self._assert_result(
            select([table]).order_by(table.c.id).limit(bindparam("l")),
            [(1, 1, 2), (2, 2, 3)],
            params={"l": 2},
        )
# ---
def mask_fn(param, path):
            path_str = ".".join(path) if isinstance(path, (list, tuple)) else str(path)
            if "Embedding" in path_str or "lm_head" in path_str:
                return "signum"
            elif isinstance(param, Linear):
                # scion for linear layers
                return dataclasses.replace(param, weight="scion", bias="signum" if param.bias is not None else None)
            else:
                return "signum"
# ---
def release_slot(self, slot_id: int) -> "SequenceTable":
        mask = hax.zeros_like(self.used_mask)
        mask = mask.at["seq", slot_id].set(True)
        return self.clear_slots(mask)
# ---
def tok_text(self):
        text = self.token['text']
        while self.peek()['type'] == 'text':
            text += '\n' + self.pop()['text']
        return self.inline(text)
# ---
def data_saver(self):
        while self.run:
            self.save_data()

            time.sleep(60)
# ---
def sum_negativenum(nums):
  sum_negativenum = list(filter(lambda nums:nums<0,nums))
  return sum(sum_negativenum)
# ---
def _linspace(
    x, size, start, step, endpoint, linspace_dtype, device=None, block_id=None
):
    bs = x.shape[0]
    i = block_id[0]
    adjusted_bs = bs - 1 if endpoint else bs

    blockstart = start + (i * size * step)
    blockstop = blockstart + adjusted_bs * step
    return nxp.linspace(
        blockstart, blockstop, bs, endpoint=endpoint, dtype=linspace_dtype
    )
# ---
def sort_dict_item(test_dict):
  res = {key: test_dict[key] for key in sorted(test_dict.keys(), key = lambda ele: ele[1] * ele[0])}
  return  (res)
# ---
def create_vm_with_labels(**kwargs):
        vm = MagicMock()
        vm.info = vm_pb2.VmInfo(
            vm_id=kwargs["vm_id"],
            slice_id=kwargs["slice_id"],
            scale_group=kwargs["scale_group"],
            zone=kwargs["zone"],
            address=kwargs.get("address", ""),
            state=vm_pb2.VM_STATE_BOOTING,
            labels=kwargs["labels"],
        )
        return vm
# ---
def num_cpus(self):
        return self._num_cpus
# ---
def testFormatSource(self):
    """Tests the _FormatSource function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))

    source_string = test_helper._FormatSource(
        event, event_data, event_data_stream)

    self.assertEqual(source_string, 'Test log file')
# ---
def process_shard(job: dict):
        return _build_single_shard_cache(
            shard_name=job["shard_name"],
            shard_index=job["index"],
            temp_root=temp_root,
            source=source,
            processor=processor,
            options=options,
            metadata=metadata,
        )
# ---
def func():
            pass
# ---
def CurrentLineContentsAndCodepointColumn():
  """Returns the line contents as a unicode string and the 0-based current
  column as a codepoint offset. If the current column is outside the line,
  returns the column position at the end of the line."""
  line = CurrentLineContents()
  byte_column = CurrentColumn()
  # ByteOffsetToCodepointOffset expects 1-based offset.
  column = ByteOffsetToCodepointOffset( line, byte_column + 1 ) - 1
  return line, column
# ---
def getName(self):
        return "NoncChi2({0},{1})".format(self.df, self.lmbda)
# ---
def save_spendable_output(self):
        self.log.debug("saving spendable output %s" % self.tip.vtx[0])
        self.spendable_outputs.append(self.tip)
# ---
def save_apartments(self, apartments_dto):
        apartments_dtos = []
        for apartment in apartments_dto:
            apartment_saved = self.save_apartment(apartment, send_event=False)
            apartments_dtos.append(apartment_saved)
        self.send_config_change_event('save')
        return apartments_dtos
# ---
def Sum(N): 
    SumOfPrimeDivisors = [0]*(N + 1)   
    for i in range(2,N + 1) : 
        if (SumOfPrimeDivisors[i] == 0) : 
            for j in range(i,N + 1,i) : 
                SumOfPrimeDivisors[j] += i           
    return SumOfPrimeDivisors[N]
# ---
def __init__(self, file=None, build=None, environment=None):
		self._addresses = {}
		self._values = {}
		self.file = file
		self.build = build
		self.environment = environment
# ---
def __init__(self, tracker, time, direction, opcode, data):
        print(str(type(self)).split('.')[3]+'('+str(len(data))+'): '+ str(data.get_array_hex(1))[1:-1])
# ---
def specified_element(nums, N):
    result = [i[N] for i in nums]
    return result
# ---


def common(l1: list, l2: list):
    """Return sorted unique common elements for two lists.
    >>> common([1, 4, 3, 34, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121])
    [1, 5, 653]
    >>> common([5, 3, 2, 8], [3, 2])
    [2, 3]

    """
    ret = set()
    for e1 in l1:
        for e2 in l2:
            if e1 == e2:
                ret.add(e1)
    return sorted(list(ret))
# ---
def parse_list_block(self, m):
        bull = m.group(2)
        self.tokens.append({
            'type': 'list_start',
            'ordered': '.' in bull,
        })
        cap = m.group(0)
        self._process_list_item(cap, bull)
        self.tokens.append({'type': 'list_end'})
# ---
def reload(self) -> str:
        """Re-run bootstrap on existing host.

        For ManualController this is the same as start() since there's no VM
        to preserve - we just re-run the bootstrap script.
        """
        return self.start()
# ---
def key_function(out_key):
        # Q1 is a simple 1:1 mapping, Q2_single has a single chunk
        return ((Q1.name,) + out_key[1:], (Q2_single.name,) + (0, 0))
# ---
def make_route(self):
		if not self.route:
			return cstr(frappe.db.get_value('Item Group', self.item_group,
					'route')) + '/' + self.scrub((self.item_name if self.item_name else self.item_code) + '-' + random_string(5))
# ---
def _stop_daemon(self):
        ovn_nbctl = self._get_ovn_controller(self.install_method)
        ovn_nbctl.stop_daemon()
# ---
def get_location(self):
        """Return the current position of the window.

        :rtype: (int, int)
        :return: The distances of the left and top edges from their respective
            edges on the virtual desktop, in pixels.
        """
        raise NotImplementedError('abstract')
# ---
def is_coscheduled(self) -> bool:
        """Whether this job uses coscheduling (all tasks assigned atomically)."""
        return self.request.HasField("coscheduling")
# ---
def list_endpoints(self, request: cluster__pb2.Controller.ListEndpointsRequest, ctx: RequestContext) -> cluster__pb2.Controller.ListEndpointsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def _setup_logger():
    logging.basicConfig(level=logging.INFO)
# ---
def _make(
        name: str = "test-job",
        cpu: int = 1,
        memory_bytes: int = 1024**3,
    ) -> cluster_pb2.Controller.LaunchJobRequest:
        return cluster_pb2.Controller.LaunchJobRequest(
            name=name,
            entrypoint=_make_test_entrypoint(),
            resources=cluster_pb2.ResourceSpecProto(cpu=cpu, memory_bytes=memory_bytes),
            environment=cluster_pb2.EnvironmentConfig(),
            replicas=1,
        )
# ---
def create_floatingip_postcommit(self, context, fip_context):
        pass
# ---
def compute(mdl, inp):
            return mdl(inp, attn_mask=attn_mask).array
# ---
def slow_create(tags=None):
            time.sleep(0.2)
            result = original_create(tags)
            create_completed.append(True)
            return result
# ---
def signal_handler(signal, frame):
	print('You pressed Ctrl+C!')
	sys.exit(0)
# ---
def _infer_transpose_keys(model_name: str) -> dict:
    """Infer the transpose keys for a model name, falling back to substring matching."""
    if model_name in _MODEL_TRANSPOSE_KEYS:
        return _MODEL_TRANSPOSE_KEYS[model_name]
    if "Qwen2.5" in model_name:
        return llama_transpose_keys
    raise KeyError(f"No MODEL_TRANSPOSE_KEYS registered for model: {model_name}")
# ---
def add_instruction(self, math_problem: str) -> str:
        """Add the standard instruction to a math problem."""
        return f"{math_problem}{self.question_suffix()}"
# ---
def get_autoscaler_status(self) -> cluster_pb2.Controller.GetAutoscalerStatusResponse:
        return self._remote_client.get_autoscaler_status()
# ---
def install_package(self):
        sudo('pkg_add libevent')
        with cd('/tmp'):
            run('wget %s' %self.pgbouncer_src)
            sudo('pkg_add %s' %self.pkg_name)
# ---
def exitLonely(self):
        self.ignore(self.lonelyDoneEvent)
        self.lonely.exit()
# ---
def build(self, Vocab: Axis, *, key: PRNGKeyArray) -> GrugWrapper:
        core_cfg = self.to_grug_model_config()
        cfg = GrugformerAttnSinkModelConfig(core=core_cfg, sink=self.sink)
        params = _init_grugformer_with_sinks(cfg, key=key)
        return GrugWrapper(
            params=params,
            grug_config=cfg,
            init_fn=_init_grugformer_with_sinks,
            forward_fn=_grug_activations_with_sinks,
            lm_head_fn=_lm_head_from_sink_params,
        )
# ---
def mean(x, axis=0):
    return _Nmean(x, axis)
# ---
def StdCapture(out=True, err=True, in_=True):
    return capture.MultiCapture(out, err, in_, Capture=capture.SysCapture)
# ---
def __pos__(self, /):
        if self.dtype not in _numeric_dtypes:
            raise TypeError("Only numeric dtypes are allowed in __pos__")
        return elemwise(nxp.positive, self, dtype=self.dtype)
# ---
def is_task(self) -> bool:
        """True if this is a task (last component is numeric)."""
        return self.task_index is not None
# ---
def date_range(self) -> str:
        return f"{self.start_date}to{self.end_date}"
# ---
def service(self):
        ''' service property '''
        return self.svc
# ---
def launch(
        model: ModelConfig,
        evals: list[EvalTaskConfig],
        output_path: str,
        max_eval_instances: int | None = None,
        wandb_tags: list[str] | None = None,
    ) -> None:
        if configure_logging:
            import logging

            logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", force=True)
        evaluator.evaluate(model, evals, output_path, max_eval_instances, wandb_tags)
# ---
def shuffle(self, key: PRNGKeyArray, *, perm_type: PermType = "feistel"):
        import levanter.data.permutation as permutation

        return permutation.PermutationDataset(self, key, perm_type=perm_type)
# ---
def get_create_serializer(self):
        return serializers.Resource(MaxCount=1, MinCount=1)
# ---
def __complex__(self, /):
        if self.ndim != 0:
            raise TypeError("complex is only allowed on arrays with 0 dimensions")
        return complex(self.compute())
# ---
def set_address(self, host, port=8125):
        try:
            self.addr = (socket.gethostbyname(host), port)
        except socket.gaierror:
            self.addr = None
            self.enabled = False
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_counts: typing.Literal[True],
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def setUp(self):
        import time
        self.repo_path = tempfile.mkdtemp(prefix='selftest-buildhistory',
            dir=get_bb_var('TOPDIR'))

        self.repo = Repo.init(self.repo_path)
        self.test_file = "test"
        self.var_map = {}
# ---
def adjac(ele, sub = []): 
  if not ele: 
     yield sub 
  else: 
     yield from [idx for j in range(ele[0] - 1, ele[0] + 2) 
                for idx in adjac(ele[1:], sub + [j])] 
def get_coordinates(test_tup):
  res = list(adjac(test_tup))
  return (res)
# ---
def __init__(self, rname, namespace, kubeconfig, registry_options):
        super(RegistryConfig, self).__init__(rname, namespace, kubeconfig, registry_options)
# ---
def _read_files():
            for path in paths:
                path = Path(path)
                if not path.exists() or not path.suffix == ".py":
                    continue
                try:
                    yield path.read_text(encoding="utf-8")
                except (OSError, UnicodeDecodeError):
                    continue
# ---
import re
def split_upperstring(text):
 return (re.findall('[A-Z][^A-Z]*', text))
# ---
def on_predict_epoch_end(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,  # noqa: ARG002
    ) -> None:
        print(f"Number of failed affinity predictions: {self.failed}")
# ---
def test_update_overflow_error():
    # Overflow: scalar start + update too large for axis  ValueError
    Seq = hax.Axis("seq", 4)
    arr = hax.zeros((Seq,), dtype=int)
    # update of length 3 starting at pos=2 would run off the end (2+3 > 4)
    upd = hax.arange((hax.Axis("seq", 3),), dtype=int)

    with pytest.raises(ValueError):
        updated_slice(arr, {"seq": 2}, upd)
# ---
def unique_product(list_data):
    temp = list(set(list_data))
    p = 1
    for i in temp:
        p *= i
    return p
# ---
def __init__(self, track, time=None, channel=None):
        MidiEvent.__init__(self, track, time=time, channel=channel)
        self.type_ = "DeltaTime"
# ---
def success(self) -> bool:
        return self.worker is not None
# ---
def get_template_path(config_name: str, infra_path: str = "infra") -> str:
    """Get the template path for a given config."""
    template_name = CONFIGS[config_name].get("TEMPLATE", "cluster")
    template_filename = f"marin-{template_name}-template.yaml"
    template_path = os.path.join(infra_path, template_filename)

    if not os.path.exists(template_path):
        raise FileNotFoundError(f"Template {template_filename} not found in {infra_path}")

    return template_path
# ---
def __init__(
        self,
        output_dir: Path,
        cuda_snapshot_frequency: int | None,
    ) -> None:
        self.output_dir = output_dir
        self.cuda_snapshot_frequency = cuda_snapshot_frequency
# ---
def quantize_dequantize(x, q_dtype, scale, compute_dtype):
    qx = quantize(x, q_dtype, scale, compute_dtype)
    return dequantize(qx, x.dtype, scale)
# ---
def test_marin_tokenizer_integration_checks(fresh_marin_tokenizer):
    tokenizer = fresh_marin_tokenizer
    run_all_tests(tokenizer)
# ---
def finish(self):
        self.writer.close()
# ---
def test_dicts_loop_io(benchmark: Any, small_parquet_path: str) -> None:
    """
    Python End-to-End: Read File -> List[dict] -> Loop calling Rust per item -> List[dict].
    Slowest Python approach (Baseline for worst case).
    """

    def _pipeline() -> int:
        docs = pq.read_table(small_parquet_path).to_pylist()
        return len([dupekit.process_dicts_loop(doc) for doc in docs])

    assert benchmark(_pipeline) > 0
# ---
def _buffer_response(status_headers, iterator):
        out = SpooledTemporaryFile(ProxyRouter.BUFF_RESPONSE_MEM_SIZE)
        size = 0

        for buff in iterator:
            size += len(buff)
            out.write(buff)

        content_length_str = str(size)
        # remove existing content length
        status_headers.replace_header('Content-Length',
                                      content_length_str)

        out.seek(0)
        return RewriteContent.stream_to_gen(out)
# ---
def to_state_dict(self, prefix: str | None = None) -> StateDict:
        return default_eqx_module_to_state_dict(self, prefix)
# ---
def __init__(self, field, fast=True):
        super().__init__(field, None, fast)
# ---
def load_parquet(self, columns: list[str] | None = None) -> Dataset[dict]:
        """Load records from parquet files."""
        return Dataset(self.source, [*self.operations, LoadFileOp("parquet", columns)])
# ---
def setup_clients(cls):
        super(MigrationsAdminTest, cls).setup_clients()
        cls.client = cls.os_admin.migrations_client
# ---
def __iter__(self):
        return self.iter_from_step(None)
# ---
def __rtruediv__(self, other, /):
        other = self._check_allowed_dtypes(other, "floating-point", "__rtruediv__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.divide, other, self, dtype=result_type(self, other))
# ---
def testPrintFunction(self):
    want = "abc\n123\nabc 123\nabcx123\nabc 123 "
    self.assertEqual((0, want), _GrumpRun(textwrap.dedent("""\
        "module docstring is ok to proceed __future__"
        from __future__ import print_function
        print('abc')
        print(123)
        print('abc', 123)
        print('abc', 123, sep='x')
        print('abc', 123, end=' ')""")))
# ---
def decomposed_mae_gradient_weighted(
    pred: torch.Tensor,
    target: torch.Tensor,
    gradient_weight: float,
    pad_mode: str = "constant",
) -> torch.Tensor:
    """MAE loss with spatial gradient matching penalty."""
    mae_per_channel = decomposed_mae(pred, target)
    grad_loss = gradient_l1_loss(pred, target, pad_mode)
    return mae_per_channel + gradient_weight * grad_loss
# ---
def sample_nam(sample_names):
  sample_names=list(filter(lambda el:el[0].isupper() and el[1:].islower(),sample_names))
  return len(''.join(sample_names))
# ---
def size(self):
        """Number of elements in the array."""
        return reduce(mul, self.shape, 1)
# ---
def polyfit(
    x: NamedArray | ArrayLike,
    y: NamedArray | ArrayLike,
    deg: int,
    rcond: ArrayLike | None = ...,
    full: Literal[False] = ...,
    w: NamedArray | ArrayLike | None = ...,
    cov: Literal[True] = ...,
) -> tuple[NamedArray, NamedArray]: ...
# ---
def exp(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in exp")
    return elemwise(nxp.exp, x, dtype=x.dtype)
# ---
def llama_small_optimizer_config() -> AdamConfig:
    return AdamConfig(
        learning_rate=1e-6,
        # don't overwhelm the learning signal
        weight_decay=1e-5,
        warmup=10,
        lr_schedule="cosine",
    )
# ---
def setcash(self, cash):
        self.startingcash = self.cash = self.p.cash = cash
# ---
def permutation_subjects(y):
    """Permute class labels of Contextual Disorder dataset.
    """
    y_perm = np.random.randint(0, 2, len(y)/2)
    y_perm = np.concatenate((y_perm, np.logical_not(y_perm).astype(int)))
    return y_perm
# ---
def test_use_pass_for_iterations_with_no_body(self):
        for num in range(1,5):
            pass

        self.assertEqual(4, num)
# ---
def tearDown(self):
        self.session.execute("DROP TABLE IF EXISTS ports;", None)
        super(TestNestedResourceTestCase, self).tearDown()
# ---
def display(self):
        if self.state == "visible":
            image(self.im, self.x, self.y)
# ---
def zeros_like(a: NamedArray, dtype=None) -> NamedArray:
    """Creates a NamedArray with all elements set to 0"""
    return NamedArray(jnp.zeros_like(a.array, dtype=dtype), a.axes)
# ---
def mock_load_datasets(config):
        return [DatasetWithMetaData(mock_dataset, "subset1", "train", "main")]
# ---
def remove_endpoints_for_job(self, job_id: JobName) -> list[ControllerEndpoint]:
        """Remove all endpoints for a job by removing endpoints for all its tasks."""
        with self._lock:
            all_removed = []
            for task_id in self._tasks_by_job.get(job_id, []):
                removed = self._remove_endpoints_for_task(task_id)
                all_removed.extend(removed)
            return all_removed
# ---
def __le__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "le")
# ---
def spine_entry(self):
        '''Write the XML element for the spine.
        (Empty string if in_spine is False.)'''
        if self.in_spine:
            return _make_xml_elem('itemref', '', [('idref', self.ident)])
        else:
            return ''
# ---
def __init__(self):
        pass
# ---
def test_validate_edit_invalid():
    source = "x = 1 + 2\n"
    mutation = Mutation(start=4, end=9, replacement="if :", node_type="BinOp", original="1 + 2")
    assert not validate_edit(source, mutation)
# ---
def add_to_dup_map(record: dict):
        shard_dup_map[record["hash"]] = {"canonical": record["canonical"]}
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        log_path = self.writer.logdir
        # sync the artifact to the logdir via fsspec
        try:
            fs, fs_path = fsspec.core.url_to_fs(log_path)
            fs.put(artifact_path, os.path.join(fs_path, name or os.path.basename(artifact_path)), recursive=True)
        except Exception:
            pylogger.exception(f"Error logging artifact {artifact_path} to {log_path}")
            return
# ---
def init_and_merge(state, *args, **kwargs):
        init_state = init_fn(*args, **kwargs)
        # remove all ShapeDTypeStructs from the state
        state = equinox.filter(state, lambda x: not isinstance(x, jax.ShapeDtypeStruct))
        return equinox.combine(state, init_state)
# ---
def test_within_range(self):
        """Test when create_time is within the given range"""
        self.assertTrue(check_create_time("2023-01-15 12:00:00 PDT", "2023-01-01", "2023-01-31"))
# ---
def test_pickle_drops_client(self):
        """Client is transient state  pickle should not carry it."""
        handle = IrisActorHandle("my-actor")
        # Manually set client to simulate resolved state
        handle._client = "fake-client"
        data = pickle.dumps(handle)
        restored = pickle.loads(data)
        assert restored._client is None
# ---
def _run_prefill(
    gen_state: GenState,
    model: LmHeadModel,
    sampler: Sampler,
    work: PrefillWork,
    max_seqs_in_prefill: int,
) -> tuple[GenState, _DecodeOutputs]:
    gen_state = _apply_prefill_work(gen_state, work)
    return _prefill_kernel(gen_state, model, sampler, work.queue, max_seqs_in_prefill)
# ---
def terminate(self, job_id: JobName) -> None:
        """Terminate a running job.

        Args:
            job_id: Job ID to terminate
        """
        self._cluster_client.terminate_job(job_id)
# ---
def get_wet_mask(inputs, device="cpu"):
    wet = xr.zeros_like(inputs[0][0])
    # inputs[0][0,12,12] = np.nan
    for data in inputs:
        wet += np.isnan(data[0])

    wet_nan = xr.where(wet != 0, np.nan, 1).to_numpy()
    wet = np.isnan(xr.where(wet == 0, np.nan, 0))
    wet = np.nan_to_num(wet.to_numpy())
    wet = torch.from_numpy(wet).type(torch.float32).to(device=device)
    return wet, wet_nan
# ---
def test_cumulative_sum(tmp_path, spec, executor):
    a = cubed.random.random(
        (50000, 5000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.cumulative_sum(a, axis=0)
    run_operation(tmp_path, executor, "cumulative_sum", b)
# ---
def __send_memoryview(self, data, flags=0):
        if hasattr(data, 'tobytes'):
            data = data.tobytes()
        return self.__send(data, flags)
# ---
def __radd__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.add(other, self)
# ---
def device_flops(self, dtype: str = "bf16") -> float:
        raise NotImplementedError("CPU FLOPS not available")
# ---
def on_compute_start(self, event):
        """Called when the computation is about to start.

        Parameters
        ----------
        event : ComputeStartEvent
            Information about the computation.
        """
        pass
# ---
def _shard_name_to_url_mapping(self):
        return _mk_shard_name_mapping(self.urls)
# ---
def forwards(self, orm):
        # Adding unique constraint on 'Vendeur', fields ['code_permanent']
        db.create_unique(u'encefal_vendeur', ['code_permanent'])
# ---
def test_int(self):
        """Store and retrieve an int"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "a", "num": 1})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["num"], 1)
# ---
def common_element(list1, list2):
     result = False
     for x in list1:
         for y in list2:
             if x == y:
                 result = True
                 return result
# ---
def test_endswith_sqlexpr(self):
        col = self.tables.some_table.c.data
        self._test(
            col.endswith(literal_column("'e%fg'")), {1, 2, 3, 4, 5, 6, 7, 8, 9}
        )
# ---
def preview(c):
    """Build production version of site"""
    c.run('pelican -s publishconf.py')
# ---
def build(c):
    """Build local version of site"""
    c.run('pelican -s pelicanconf.py')
# ---
def match_like(templ_leaf, tree_leaf):
        if templ_leaf is None:
            return None
        else:
            if tree_leaf is None:
                warnings.warn(f"Template has a non-None value where tree is None. Template value: {templ_leaf}")
            return tree_leaf
# ---
def attention_config(self) -> AttentionConfig:
        """Convert this MixtralConfig to an AttentionConfig for use with Attention."""
        return AttentionConfig(
            Embed=self.Embed,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            use_bias=self.use_bias,
            upcast_attn=self.upcast_attn,
            attn_backend=self.attn_backend,
            flash_attention_block_size=self.flash_attention_block_size,
            rope=self.rope,
        )
# ---
def test_set_enable_host_enable(self):
        self._test_host_action(self.conn.set_host_enabled, True, 'enabled')
# ---
def max_output_tokens(self) -> int:
        """Maximum output tokens across all lessons in the curriculum."""
        return max(lesson.sampling_params.max_output_tokens for lesson in self.lessons.values())
# ---
def max_seq_len(self) -> int:
        return self.core.max_seq_len
# ---
def test_getmethod_default_no_fd(self, monkeypatch):
        from _pytest.capture import pytest_addoption
        from _pytest.config import Parser
        parser = Parser()
        pytest_addoption(parser)
        default = parser._groups[0].options[0].default
        assert default == "fd" if hasattr(os, "dup") else "sys"
        parser = Parser()
        monkeypatch.delattr(os, 'dup', raising=False)
        pytest_addoption(parser)
        assert parser._groups[0].options[0].default == "sys"
# ---
def poll(self, job_id: JobId) -> JobInfo:
        """Get current status of a job without blocking.

        Args:
            job_id: Job identifier

        Returns:
            Current job information including status

        Raises:
            KeyError: If job_id is not found
        """
        ...
# ---
def main() -> None:
    args = parse_args()
    cfg = build_training_config(args)
    run_training(cfg, cache_dir=args.cache_dir)
# ---
def from_state_dict(self: M, state_dict: StateDict, prefix: str | None = None) -> M:
        # this method needs to "vectorize" the blocks, so that we have a single block h.FOO
        # first just do the normal thing with our own dict, which we'll post-process
        stacked = _stack_state_dict(state_dict, prefix=prefix)
        out = super().from_state_dict(stacked, prefix=prefix)  # type: ignore
        return out
# ---
def start_response(*args):
    pass
# ---
def test_delete_vm_resource_successful(self):
        RESOURCE_ID = pyuuid.UUID("00000000-0000-0000-0000-000000000001")
        self._insert_vm_to_db(uuid=RESOURCE_ID, name="test", state="off")

        VM_RES_ENDPOINT = self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT,
                                            RESOURCE_ID)

        response = requests.delete(VM_RES_ENDPOINT)

        self.assertEqual(response.status_code, 204)
        self.assertFalse(self._vm_exists_in_db(RESOURCE_ID))
# ---
def max_Product(arr): 
    arr_len = len(arr) 
    if (arr_len < 2): 
        return ("No pairs exists")           
    x = arr[0]; y = arr[1]      
    for i in range(0,arr_len): 
        for j in range(i + 1,arr_len): 
            if (arr[i] * arr[j] > x * y): 
                x = arr[i]; y = arr[j] 
    return x,y
# ---
def test_evaluate_string(self):
        expr = lit("hello")
        assert expr.evaluate({"anything": "ignored"}) == "hello"
# ---
def https_open(self, req):
        return self.do_open(self.httpconn, req)
# ---
def jacobsthal_num(n): 
	dp = [0] * (n + 1) 
	dp[0] = 0
	dp[1] = 1
	for i in range(2, n+1): 
		dp[i] = dp[i - 1] + 2 * dp[i - 2] 
	return dp[n]
# ---
def attention(
    q: Float[Array, "B Q Hq D"],
    k: Float[Array, "B K Hkv D"],
    v: Float[Array, "B K Hkv D"],
    mask: AttentionMask | Bool[Array, "B Q K"] | Float[Array, "B Q K"] | None,
) -> Float[Array, "B Q Hq D"]:
    if jax.default_backend() == "tpu":
        if isinstance(mask, jax.Array):
            return reference_attention(q, k, v, mask, logits_dtype=jnp.float32)
        return _tpu_splash_attention(q, k, v, mask)
    return reference_attention(q, k, v, mask, logits_dtype=jnp.float32)
# ---
def on_task_end(self, event):
        now = time.time()
        refresh = now - self.last_updated > (1.0 / REFRESH_PER_SECOND)
        self.last_updated = now
        self.progress.update(
            self.progress_tasks[event.name], advance=event.num_tasks, refresh=refresh
        )
# ---
def determine_inverse(self, records):
        """ Given the value of ``self`` on ``records``, inverse the computation. """
        if self.inverse:
            self.inverse(records)
# ---
def test_from_zarr_icechunk(icechunk_storage, executor):
    create_icechunk(
        [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
        icechunk_storage,
        chunks=(2, 2),
    )

    repo = Repository.open(icechunk_storage)
    session = repo.readonly_session(branch="main")
    store = session.store

    a = cubed.from_zarr(store, path="a")
    assert_array_equal(
        a.compute(executor=executor), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    )
# ---
def _get_velocity(self):
        return self._parameter2
# ---
def is_all_none(structure):
  iterable = nest.flatten(structure)
  # We cannot use Python's `any` because the iterable may return Tensors.
  for element in iterable:
    if element is not None:
      return False
  return True
# ---
def patched_start(self):
        captured_manager.append(self)
        return original_start(self)
# ---
def func(x, y):
        return x
# ---
def load_rule_file(rules_dir, filename):
    """Load a rule file and return its content, or empty string if not found."""
    if not rules_dir:
        return ""
    path = os.path.join(rules_dir, filename)
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except (OSError, IOError):
        return ""
# ---
def reset_testable(self):
        self.reset_count += 1
# ---
def char_to_bytes(char):
    return bin(ord(char))
# ---
def example_reading_spec(self):
    label_key = "image/class/label"
    data_fields, data_items_to_decoders = (
        super(Image2ClassProblem, self).example_reading_spec())
    data_fields[label_key] = tf.FixedLenFeature((1,), tf.int64)

    data_items_to_decoders["targets"] = contrib.slim().tfexample_decoder.Tensor(
        label_key)
    return data_fields, data_items_to_decoders
# ---
def finalize(self):
        self.is_complete = True
        if self.complete_promise is not None:
            self.complete_promise.set_result(None)
            if not asyncio.get_event_loop().is_running():
                _executor.submit(lambda: asyncio.run(self.notify_length_update()))
            else:
                asyncio.create_task(self.notify_length_update())
# ---
def test_two_contiguous_selectors():
    B, X, Y = Axis("batch", 3), Axis("x", 5), Axis("y", 7)
    a = hax.arange((B, X, Y))
    ix = hax.arange((B,), dtype=jnp.int32) % X.size
    iy = hax.arange((B,), dtype=jnp.int32) % Y.size
    out = a["x", ix, "y", iy]
    assert out.axes == (B,)
    ref = a.array[jnp.arange(3), ix.array, iy.array]
    assert jnp.array_equal(out.array, ref)
# ---
def test_dispatch_delayed(cluster):
    """Dispatch delayed by chaos (via heartbeat), but eventually goes through."""
    _url, client = cluster
    enable_chaos("controller.heartbeat", delay_seconds=3.0, failure_rate=1.0, max_failures=2)
    job = submit(client, _quick, "delayed-dispatch")
    status = wait(client, job, timeout=60)
    assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def remove_before_section(html: BeautifulSoup):
    # We only care about information after the first section
    section = html.find("section")
    if section:
        section = section.previous_sibling
        while section:
            new_section = section.previous_sibling
            section.extract()
            section = new_section
# ---
def subfunction(self):
        run(function(self))
# ---
def test_empty_dataset(backend):
    """Test operations on empty dataset."""
    ds = Dataset.from_list([])
    assert list(Backend.execute(ds, context=backend)) == []
    assert list(Backend.execute(ds.filter(lambda x: True), backend)) == []
    assert list(Backend.execute(ds.map(lambda x: x * 2), backend)) == []
    assert list(Backend.execute(ds.window(10), backend)) == []
# ---
def _check_size_consistency(
    spec1: AxisSelection, spec2: AxisSelection, name: str, size1: int | None, size2: int | None
):
    if size1 is not None and size2 is not None and size1 != size2:
        raise ValueError(f"Axis {name} has different sizes in {spec1} and {spec2}: {size1} != {size2}")
# ---
def create(*args, **kw):
        return create_environment(*args, **kw)
# ---
def exitOff(self):
        DistributedCCharBaseAI.DistributedCCharBaseAI.exitOff(self)
# ---
def __init__(
        self,
        start: float = 0.0,
        stop: float = 5.0,
        num_gaussians: int = 50,
    ):
        super().__init__()
        offset = torch.linspace(start, stop, num_gaussians)
        self.num_gaussians = num_gaussians
        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2
        self.register_buffer("offset", offset)
# ---
def convert_tokens_to_string(self, tokens):
    return ''.join(tokens)
# ---
def std(x, axis=0):
    N = asarray(x).shape[axis]
    return _Nstd(x, axis)*sqrt(N/(N-1.))
# ---
def test_deadline_uses_monotonic_time():
    """Deadline uses monotonic time (immune to clock changes)."""
    # Create a deadline 100ms in the future
    deadline = Deadline.from_ms(100)

    # Should not be expired immediately
    assert not deadline.expired()

    # Sleep for 50ms - still not expired
    time.sleep(0.05)
    assert not deadline.expired()

    # Sleep for another 60ms - should be expired now (total 110ms)
    time.sleep(0.06)
    assert deadline.expired()
# ---
def _unstack_chunk(*arrs, axis=0):
    # unstack each array in arrs and yield all in turn
    for arr in arrs:
        for a in nxp.unstack(arr, axis=axis):
            yield a
# ---
def _index_of_batch_axis(array, default):
        if isinstance(array, NamedArray):
            return array.axis_indices(axis)
        elif callable(default):
            return default(array)
        else:
            return default
# ---
def initialize(self, io_loop=None):
        super(BlockingResolver, self).initialize(io_loop=io_loop)
# ---
def __init__(self, func=lambda x: x):
        super(PriorityQueue, self).__init__()

        self.func = func
# ---
def dummy_entrypoint():
    """A simple entrypoint for testing."""
    pass
# ---
def tunnel(
        self,
        controller_address: str,
        local_port: int | None = None,
        timeout: float | None = None,
        tunnel_logger: logging.Logger | None = None,
    ) -> AbstractContextManager[str]:
        """Return direct connection for local platform (no tunnel needed)."""
        return nullcontext(controller_address)
# ---
def get_dataset(slice_time=True):
    if dataset_name == "OM4":
        ds_groundtruth = get_om4_gt(slice_time)
    elif dataset_name == "CM4":
        ds_groundtruth = get_cm4_gt(slice_time)
    else:
        raise ValueError("Incorrect dataset name")

    ds_groundtruth = ds_groundtruth.astype("float32")
    return ds_groundtruth
# ---
def _infer_mapping(model_name: str) -> dict:
    """Infer the vLLM mapping for a model name, falling back to substring matching."""
    if model_name in _MODEL_MAPPINGS:
        return _MODEL_MAPPINGS[model_name]
    if "Qwen2.5" in model_name:
        return levanter_qwen_to_vllm_mapping()
    raise KeyError(f"No MODEL_MAPPING registered for model: {model_name}")
# ---
def make_client() -> JobSubmissionClient:
    """Create a JobSubmissionClient based on environment variables."""
    address = os.environ.get("RAY_ADDRESS", REMOTE_DASHBOARD_URL)
    # Always pass an explicit HTTP dashboard URL. If Ray has to infer the Jobs
    # API endpoint (e.g. from a `ray://...` address), it can resolve to the head
    # node's internal `webui_url`, which isn't reachable from a developer laptop
    # when using SSH port forwarding.
    return JobSubmissionClient(address)
# ---
def __init__(self, urls, text_key="sentence", audio_key="audio", sampling_rate=16000):
        super().__init__(urls)
        self.text_key = text_key
        self.audio_key = audio_key
        self.sampling_rate = sampling_rate
# ---
def loss_and_metrics(params: GrugModelParameters, batch: dict[str, jax.Array]):
        logits = forward(params, batch["tokens"], model_cfg, mask=None)
        loss = cross_entropy_loss(logits, batch["labels"])
        metrics = {"loss": loss, "ppl": jnp.exp(loss)}
        return loss, metrics
# ---
def _direct_ssh(ssh_config: SshConfig, host: str) -> DirectSshConnection:
    return DirectSshConnection(
        host=host,
        user=ssh_config.user,
        port=ssh_config.port,
        key_file=ssh_config.key_file or None,
        connect_timeout=ssh_config.connect_timeout,
    )
# ---
def is_character_level(self):
    raise NotImplementedError()
# ---
def with_sliding_window(self, sliding_window: int | None) -> "AttentionMask":
        """Return a copy of this mask with ``sliding_window`` applied."""
        return AttentionMask(
            is_causal=self.is_causal,
            causal_offset=self.causal_offset,
            explicit_mask=self.explicit_mask,
            segment_ids=self.segment_ids,
            sliding_window=sliding_window,
        )
# ---
def _sort_matches(self):
        self.matches.sort(key=lambda t: t[0].date if t[0] is not None else t[1].date)
# ---
def _connect_networks_to_routers(self, lnetworks, lrouters, networks_per_router):
        for lrouter in lrouters:
            LOG.info("Connect %s networks to router %s" % (networks_per_router, lrouter["name"]))
            for lnetwork in lnetworks[:networks_per_router]:
                LOG.info("connect networks %s cidr %s" % (lnetwork["name"], lnetwork["cidr"]))
                self._connect_network_to_router(lrouter, lnetwork)

            lnetworks = lnetworks[networks_per_router:]
# ---
def compute(self) -> Tensor:
    """Computes the bits per dimension.

    Returns:
      bpd
    """
    return self.mean_value / self.weight / LOG2
# ---
def introduced_elements(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'episome',
        'purpose': 'characterization',
        'nucleic_acid_delivery_method': ['transient transfection'],
        'introduced_elements': 'genomic DNA regions'
    }
# ---
def __str__(self):
        return "NoncentralTDistr(df={0},mu={1})#{2}".format(self.df, self.mu, self.id())
# ---
def convert_to_cache(self, value, record, validate=True):
        if value is None or value is False:
            return False
        return ustr(value)
# ---
def __repr__(self):
        return "<alwaysmatcher>"
# ---
def load(self, fs, node_id):
        if not self.tryLoad(fs, node_id):
            raise Exception("Could not load node: %s iens: %d report: %d" % (self.name(), node_id.iens, node_id.report_step))
# ---
def insert(radio, genre, url) :
    db = cherrypy.session['database']
    sql =  "INSERT INTO Radio (radio, genre, url, exist) VALUES('%s', '%s', '%s', 1)" % (radio, genre, url)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def from_list(items: list[T]) -> Dataset[T]:
        """Create a dataset from a list."""
        return Dataset(items)
# ---
def __init__(self, result: Any):
        self._result = result
        self._iterator: Iterator[Any] | None = None
# ---
def string_match(cls, pattern, value):
        return pattern.search(cls._normalize(value)) is not None
# ---
def __init__(self, reader, read_session=None):
        self._reader = reader
        if read_session is not None:
            self._stream_parser = _StreamParser.from_read_session(read_session)
        else:
            self._stream_parser = None
# ---
def get_shard_source(self, split) -> ShardedDataSource[dict] | None:
        raise NotImplementedError
# ---
def test_add_scalar():
    Height = Axis("Height", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)

    named1 = hax.random.uniform(PRNGKey(0), (Height, Width, Depth))
    named2 = named1 + 1.0
    assert jnp.all(jnp.isclose(named2.array, named1.array + 1.0))

    named3 = 1.0 + named1
    assert jnp.all(jnp.isclose(named3.array, named1.array + 1.0))
# ---
def recomb_tag(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'tagging',
        'method': 'site-specific recombination'
    }
# ---
def shutdown(self) -> None:
        pass
# ---
def get_directory_friendly_name(model_name: str) -> str:
    return model_name.replace("/", "--").replace(".", "-")
# ---
def __getattr__(self, method_name: str) -> RayActorMethod:
        if method_name.startswith("_"):
            raise AttributeError(method_name)
        ray_method = getattr(self._actor_ref, method_name)
        return RayActorMethod(ray_method)
# ---
def moddiv_list(nums1,nums2):
  result = map(lambda x, y: x % y, nums1, nums2)
  return list(result)
# ---
def __init__(self):
        super().__init__()
# ---
def inspect_side_effect(container_id):
        inspect_call_count[0] += 1
        if inspect_call_count[0] == 1:
            return ContainerStatus(running=True)
        return ContainerStatus(running=False, exit_code=0)
# ---
def terminate(ctx, job_id):
    """Terminate a job."""
    cluster = ctx.obj["cluster"]
    cluster.terminate(job_id)
    click.echo(f"Terminated: {job_id}")
# ---
def evt(ctx):
            ctx.is_disconnect = evt_value
# ---
def __eq__(self, other):
        return type(self) == type(other) or other is None
# ---
def vars(self):  # pragma: no cover
        warnings.warn('the Dataset property `vars` has been deprecated; '
                      'use `data_vars` instead',
                      FutureWarning, stacklevel=2)
        return self.data_vars
# ---
def shutdown(self) -> None:
        self._inference_server.shutdown()
# ---
def make_loss_weight(id, prompt_length):
        loss_weight = (np.arange(len(id)) >= prompt_length - 1).astype(np.float32)
        loss_weight[-1] = 0
        return loss_weight
# ---
def __init__(self, username, password, httpsession):
        """Initialize the data object."""
        from pyebox import EboxClient

        self.client = EboxClient(username, password, REQUESTS_TIMEOUT, httpsession)
        self.data = {}
# ---
def set_db_records():
    affiemp = set_uw_account("affiemp")

    javerage = set_uw_account("javerage")

    ellen = set_uw_account("ellen")

    staff = set_uw_account("staff")
    staff.set_disable()

    retiree = set_uw_account("retiree")

    tyler = set_uw_account("faculty")

    leftuw = set_uw_account("leftuw")
    leftuw.set_terminate_date()

    testid = set_uw_account("testid")
# ---
def test_default_spec_config_override():
    # override default spec to increase allowed_mem
    from cubed import config

    with config.set(
        {"spec.allowed_mem": "4GB", "spec.executor_name": "single-threaded"}
    ):
        a = xp.ones((20000, 10000), chunks=(10000, 10000))
        b = xp.negative(a)
        assert_array_equal(b.compute(), -np.ones((20000, 10000)))
# ---
def test_format_shard_path_multiple_consecutive_slashes():
    """Test normalization of multiple consecutive slashes."""
    pattern = "gs://bucket///path////data-{shard:05d}.jsonl"
    result = format_shard_path(pattern, 0, 1)
    assert result == "gs://bucket/path/data-00000.jsonl"
# ---
def case(context):
        """Add to the context."""
        assert context == {"squee": "kapow"}
        context.boing = "thunk"
# ---
def _strip_properly_formatted_commas(expr: str):
    # We want to be careful because we don't want to strip tuple commas
    p1 = re.compile(r"(\d)(,)(\d\d\d)($|\D)")
    while True:
        next_expr = p1.sub("\\1\\3\\4", expr)
        if next_expr == expr:
            break
        expr = next_expr
    return next_expr
# ---
def test_profiler_get_base_id(self):
        prof = profiler._Profiler("secret", base_id="1", parent_id="2")
        self.assertEqual(prof.get_base_id(), "1")
# ---
def tok():
    return TreeDiffusionTokenizer(max_seq_len=512)
# ---
def alertPopup(title, msg):
    popup = Popup(title = title,
                      content=Label(text = msg),
                      size_hint=(None, None), size=(dp(600), dp(200)))
    popup.open()
# ---
def convert_svg(self, el, text, convert_as_inline):
        # ignore svg elements
        return ""
# ---
def volume_omxplayer(vol) :
    import math
    control = "/usr/local/bin/omxcontrol"
    if vol == 'up' :
        db = subprocess.check_output([control, "volumeup"])
    else :
        db = subprocess.check_output([control, "volumedown"])

    v = subprocess.check_output([control, "volume"])
    i = v.rfind(':')
    db = 10.0 * math.log(float(v[i+1:]), 10)
    volstring = "%-2.2f dB" % db
    return volstring
# ---
def volume_alsa(vol):
    # With ALSA on CHIP
    if vol == 'up':
        db = subprocess.check_output(["amixer set 'Power Amplifier' 5%+"], shell=True)
        #db = os.system("amixer set 'Power Amplifier' 5%+")
    if vol == 'down':
        db = subprocess.check_output(["amixer set 'Power Amplifier' 5%-"], shell=True)
        #db = os.system("amixer set 'Power Amplifier' 5%-")
    i = db.rfind(':')
    return db[i+1:]
# ---
def _volumes(self):
        self._prepare_volumes()
        try:
            yield
        finally:
            self._teardown_volumes()
# ---
def test_diff(n):
    x = np.array([1, 5, 3, 8, 7, 2, 6, 9])
    a = xp.asarray(x, chunks=(3,))
    b = xp.diff(a, n=n)

    assert_array_equal(b.compute(), np.diff(x, n=n))
# ---
def crispri(lab, award):
    return {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'CRISPR'
    }
# ---
def storage_config(request, tmp_path):
    """Parametrized fixture for different storage types."""
    if request.param == "memory":
        return RolloutStorageConfig(storage_type=StorageType.IN_MEMORY, queue_name="test_queue")
    else:
        return RolloutStorageConfig(storage_type=StorageType.FILE, path=str(tmp_path / "rollout_storage"))
# ---
def test_logistic():
    check_gen_is_equal(lambda k, s: jax.random.logistic(k, s), lambda k, s: hax.random.logistic(k, s))
# ---
def randomly_rotate(coords, return_second_coords=False, second_coords=None):
    R = random_rotations(len(coords), coords.dtype, coords.device)

    if return_second_coords:
        return torch.einsum("bmd,bds->bms", coords, R), torch.einsum(
            "bmd,bds->bms", second_coords, R
        ) if second_coords is not None else None

    return torch.einsum("bmd,bds->bms", coords, R)
# ---
def __repr__(self) -> str:
        op_symbol = "&" if self.op == "and" else "|"
        return f"({self.left} {op_symbol} {self.right})"
# ---
def delete_vf(self, context, vf):
        """remove a vf from the virtual_function_list
        if the vf does not exist, ignore it
        """
        for idx, exist_vf in self.virtual_function_list:
            if base.obj_equal_prims(vf, exist_vf):
                removed_vf = self.virtual_function_list.pop(idx)
                removed_vf.destroy(context)
                return
        LOG.warning("The removing vf does not exist!")
# ---
def has_active_session():
    """Check if there's an active chainlink session."""
    result = run_chainlink(["session", "status"])
    if result and "Session #" in result and "(started" in result:
        return True
    return False
# ---
def _validate_request(self, request):
        validate.check_tcp_request(request)
# ---
def max(x, /, *, axis=None, keepdims=False, split_every=None):
    if x.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in max")
    return reduction(
        x,
        nxp.max,
        axis=axis,
        dtype=x.dtype,
        split_every=split_every,
        keepdims=keepdims,
    )
# ---
def count_Set_Bits(n): 
    count = 0
    while (n): 
        count += n & 1
        n >>= 1
    return count
# ---
def _match_greater_than(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) > str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def test_fn():
        print("Hello from test")
# ---
def union_axes(a1: ShapeDict, a2: AxisSpec) -> ShapeDict: ...
# ---
def decode_step_callback(iteration: int):
            """Called at each decode iteration in the engine."""
            # Use the iteration number as the step for profiling
            saved_step = self._current_step
            self._current_step = iteration
            self._handle_profiler_step()
            self._current_step = saved_step
# ---
def setup_teardown(context):
        """A fixture with both `setup()` and `teardown()`."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        def teardown():
            """Check that `context.squee` has changed."""
            assert context == {"squee": "boing"}

        return setup, teardown
# ---
def effective_principals(self, request):
		return self.match(request).effective_principals(request)
# ---
def simhash(doc: dict[str, Any]) -> int:
    words = doc["body"].split()
    unique_words = sorted(set(words))
    return hash(" ".join(unique_words)) % 10000
# ---
def diff_offsets(offsets: np.ndarray):
            # fine to mutate since we have a copy
            # the array store has the number of rows in the 0th offset
            offsets[0] = 0
            return offsets[1:] - offsets[:-1]
# ---
def noise_distribution(self, batch_size):
        # note: in AF3 the sample is scaled by sigma_data while in EDM it is not
        # in practice this just means scaling P_mean by the log

        return (
            self.sigma_data
            * (
                self.P_mean
                + self.P_std * torch.randn((batch_size,), device=self.device)
            ).exp()
        )
# ---
def __call__(self, x: NamedArray) -> NamedArray:
        in_dtype = x.dtype
        x = x.astype(self.dtype)
        var = hax.mean(hax.square(x), axis=self.axis)
        inv = hax.rsqrt(var + self.eps)
        out = x * inv
        out = out.astype(in_dtype)

        if self.weight is not None:
            out = self.weight * out
        if self.bias is not None:
            out = out + self.bias
        return out
# ---
def filter_eval_shape(*args, **kwargs):
    import warnings

    warnings.warn(
        "filter_eval_shape is deprecated, use eqx.filter_eval_shape instead",
        DeprecationWarning,
    )
    return eqx.filter_eval_shape(*args, **kwargs)
# ---
def mutiple_tuple(nums):
    temp = list(nums)
    product = 1 
    for x in temp:
        product *= x
    return product
# ---
def discover_levanter_checkpoints(base_path: str):
    """
    Discover the Levanter checkpoints in the given path, sorted by the last modified time. (Most recent last)
    Args:
        base_path:  Fsspec Path to the directory containing the checkpoints, possibly in nested directories.
    Returns:
        List of paths to the checkpoints, sorted by the last modified time.
    """

    return discover_checkpoints(base_path, "**/metadata.json", ["metadata.json", "model"])
# ---
def _carry_ckpt_name(self):
        return f"BlockSeq[{self.Block}, {self.blocks[0].__class__.__name__}].carry"
# ---
def __getstate__(self):
        return {"host": self.host, "port": self.port, "queue_name": self.queue_name}
# ---
def Extract(lst): 
    return [item[-1] for item in lst]
# ---
def materialize_all():
            for shard in shards:
                yield from shard
# ---
def with_cpu(**kwargs) -> ResourceConfig:
        return ResourceConfig(device=CpuConfig(), **kwargs)
# ---
def body_push(self, data):
        self.body.append(data)
# ---
def __add_fuzzing_boundaries(self):
        lower_bound = -1
        if self.ui.spinBoxLowerBound.isEnabled():
            lower_bound = self.ui.spinBoxLowerBound.value()

        upper_bound = -1
        if self.ui.spinBoxUpperBound.isEnabled():
            upper_bound = self.ui.spinBoxUpperBound.value()

        num_vals = self.ui.spinBoxBoundaryNumber.value()
        self.fuzz_table_model.add_boundaries(lower_bound, upper_bound, num_vals)
# ---
def test_host_startup(self):
        self.assertRaises(NotImplementedError,
                          self.conn.host_power_action, 'host', 'startup')
# ---
def load_jupyter_server_extension(nb_app):
    web_app = nb_app.web_app
    base_url = web_app.settings['base_url']

    route_pattern = url_path_join(base_url, '/saagie')
    web_app.add_handlers('.*$', [(route_pattern, SaagieHandler)])

    route_pattern = url_path_join(base_url, '/saagie/check')
    web_app.add_handlers('.*$', [(route_pattern, SaagieCheckHandler)])
# ---
def select_subset_from_mask(mask, p, random: np.random.Generator) -> np.ndarray:
    num_true = np.sum(mask)
    v = random.geometric(p) + 1
    k = min(v, num_true)

    true_indices = np.where(mask)[0]

    # Randomly select k indices from the true_indices
    selected_indices = random.choice(true_indices, size=k, replace=False)

    new_mask = np.zeros_like(mask)
    new_mask[selected_indices] = 1

    return new_mask
# ---
def col(name: str) -> ColumnExpr:
    """Create a column reference expression.

    Example:
        >>> col("score") > 0.5
        (col('score') > lit(0.5))
    """
    return ColumnExpr(name)
# ---
def sample_data(self):
        super(AggregateChecks, self).sample_data()

        self.serial1 = "__TEST__1"
        self.serial2 = "__TEST__2"

        self.dawn = 8*3600
        self.dusk = 20*3600

        sampledata = check.generate_linear(0, self.dawn, self.dusk, 24*3600,
                                           0, 1)

        for ts, y in sampledata:
            self.db.add_historic(self.serial1, ts, y)
            self.db.add_historic(self.serial2, ts, 2*y)
# ---
def test_do_execute_wo_replace(self):
        self._test_do_execute(False)
# ---
def __init__(self, conf, topic_name):
        self.topic_name = topic_name
        self.producer = Producer(conf)
        self.counter = 0
        self.running = True
# ---
def Embed(self) -> Axis:
        return cast(Axis, self.token_embeddings.Embed)
# ---
def enterLonely(self):
        self.lonely.enter()
        self.acceptOnce(self.lonelyDoneEvent, self.__decideNextState)
# ---
def test_current_pixel_updates_on_status_read(self):
        self.assertEqual(self.mda.current_pixel, [0, 0])
        self.mda.io_read_byte(0x3BA)
        self.assertEqual(self.mda.current_pixel, [1, 0])
# ---
def arccosh(a: A) -> A:
    return wrap_elemwise_unary(jnp.arccosh, a)
# ---
def check_dashboard(cluster_name: str, dashboard_port: int) -> tuple[str, bool]:
        """Check if a dashboard is accessible."""
        try:
            response = requests.get(f"http://localhost:{dashboard_port}/api/version", timeout=3)
            return (cluster_name, response.status_code == 200)
        except (requests.ConnectionError, requests.Timeout):
            return (cluster_name, False)
# ---
def insert(self, string):
        """
        Insert string at the end. This always begins a new line.
        """
        if (self.num_lines >= MAX_NUM_LINES):
            pass

        input_num_lines = num_lines(string)

        #if (input_num_lines > self.remaining_lines):
        #    num = self.remaining_lines
        #else:
        #    num = input_num_lines
        num = input_num_lines

        new_lines = get_lines(string)

        self.lines += new_lines[-num:]
        self.update_num_lines()
# ---
def setup_class(cls):
        cls.test_patterns = MustHavePatterns(Successor)
# ---
def prod(x, axis=0):
    return _Nprod(x, axis)
# ---
def test_dispatch_disabled(self, dispatcher, loop, evt):
        called = 0

        @event.event(evt.name, enable=False)
        async def corofunc():
            nonlocal called
            called += 1

        h_inst = event.HandlerInstance.from_handler(corofunc)
        dispatcher.register(h_inst)
        loop.run_until_complete(dispatcher.dispatch(evt))
        assert called == 0
# ---
def _convert_frac_or_steps(frac_or_steps: float | int, num_train_steps: int):
    # if it's greater than 1, it must be a whole number of steps
    if frac_or_steps < 0.0 or (frac_or_steps > 1.0 and frac_or_steps % 1 != 0):
        raise ValueError(f"Invalid fraction {frac_or_steps}. Must be between 0 and 1. You can also use (whole) steps.")
    if frac_or_steps <= 1.0:
        return int(frac_or_steps * num_train_steps)

    return int(frac_or_steps)
# ---
def test_one_step_edit_identical_returns_none():
    source = "x = 1\n"
    assert one_step_edit(source, source) is None
# ---
def test_calculate_projected_mem():
    projected_mem = calculate_projected_mem(
        reserved_mem=3,
        inputs=[5, 7],
        operation=11,
        output=13,
        buffer_copies=BufferCopies(2, 2),
    )
    assert projected_mem == 3 + 5 + (5 * 2) + 7 + (7 * 2) + 11 + 13 + (13 * 2)
# ---
def test_check_health_returns_unhealthy_on_exception():
    """check_health returns unhealthy result on exception."""
    conn = MagicMock()
    conn.run.side_effect = Exception("Network error")
    result = check_health(conn, port=10001)
    assert result.healthy is False
    assert "Network error" in result.curl_error
# ---
def _connect_to_textboxes(self, unitview, textboxes):
        for target in textboxes:
                self.autocomp.add_widget(target)
# ---
def _try_load(path, metadata):
    try:
        ledger = CacheLedger.load(path, metadata)
        if ledger.is_finished:
            return ledger
        logger.debug(f"Cache exists but is not finished at {path}.")
        return None
    except FileNotFoundError:
        return None
# ---
def size(self) -> int:
        return math.prod(self.shape)
# ---
def minimum(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "minimum")
    if x1.dtype not in _real_numeric_dtypes or x2.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in minimum")
    return elemwise(nxp.minimum, x1, x2, dtype=result_type(x1, x2))
# ---
def unique_Element(arr,n):
    s = set(arr)
    if (len(s) == 1):
        return ('YES')
    else:
        return ('NO')
# ---
import re
def remove_lowercase(str1):
  remove_lower = lambda text: re.sub('[a-z]', '', text)
  result =  remove_lower(str1)
  return (result)
# ---
def inferred_head_dim(self) -> int:
        """Get per-head dimension."""
        if self.head_dim is not None:
            return self.head_dim
        return self.hidden_dim // self.num_heads
# ---
def __len__(self):
        return len(self._calls)
# ---
def test_logical_and(self):
        expr = (col("score") > 0.5) & (col("category") == "A")
        pa_expr = to_pyarrow_expr(expr)
        assert pa_expr is not None
# ---
def tree_leaves(tree, is_leaf=None):
    """
    Version of [jax.tree_util.tree_leaves][] that automatically treats NamedArrays as leaves.
    """
    if is_leaf is None:
        is_leaf = lambda x: isinstance(x, NamedArray)
    else:
        is_leaf = lambda x: is_leaf(x) or is_named_array(x)

    return jax.tree_util.tree_leaves(tree, is_leaf=is_leaf)
# ---
def set_Bit_Number(n): 
    if (n == 0): 
        return 0; 
    msb = 0; 
    n = int(n / 2); 
    while (n > 0): 
        n = int(n / 2); 
        msb += 1; 
    return (1 << msb)
# ---
def nm(node_attrs1, node_attrs2):
        label1 = node_attrs1["label"]
        label2 = node_attrs2["label"]
        # - in a label indicates that the node is an operator; don't compare these names
        if "-" in label1:
            return "-" in label2
        return label1 == label2
# ---
def state_dict(self):
    return {'epoch': self.epoch, 'counter': self.counter}
# ---
def getName(self):
        return "NoncF({0},{1},{2})".format(self.df1, self.df2, self.lmbda)
# ---
def basesnum_coresspondingnum(bases_num,index):
  result = list(map(pow, bases_num, index))
  return result
# ---
def testFormatFilename(self):
    """Tests the _FormatFilename function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    filename_string = test_helper._FormatFilename(
        event, event_data, event_data_stream)
    self.assertEqual(filename_string, 'log/syslog.1')
# ---
def Auth(self): # I really don't know how to call this.
        return self.__data['cls_auth']
# ---
def selected_pane_index(self):
        return self._selected_pane_index
# ---
def _overlap(
    a,
    overlap_func=None,
    depth=None,
    boundary=None,
    numblocks=None,
    has_block_id_kw=False,
    block_id=None,
    **kwargs,
):
    depth = depth[0]
    boundary = boundary[0]
    out = _pad_boundaries(a, depth, boundary, numblocks, block_id)
    if has_block_id_kw:
        return overlap_func(out, block_id=block_id, **kwargs)
    else:
        return overlap_func(out, **kwargs)
# ---
def matchfn(self, f):
        return self._m1(f) and (not self._m2(f) or self._m1.exact(f))
# ---
def convert_environment(env: EnvironmentConfig | None) -> EnvironmentSpec | None:
    """Convert fray v2 EnvironmentConfig to Iris EnvironmentSpec."""
    if env is None:
        return None
    from iris.cluster.types import EnvironmentSpec

    return EnvironmentSpec(
        pip_packages=list(env.pip_packages),
        env_vars=dict(env.env_vars),
        extras=list(env.extras),
    )
# ---
def test_transaction_connection_fn_rollback(self):
        fn = self._trans_rollback_fn()
        conn = testing.db.connect()
        assert_raises(
            Exception,
            conn.transaction, fn, 5, value=8
        )
        self._assert_no_data()
# ---
import re
def match_num(string):
    text = re.compile(r"^5")
    if text.match(string):
        return True
    else:
        return False
# ---
def _filter_weights_to_components(weights: dict[str, float]) -> dict[str, float]:
    """Return a copy of weights keeping only keys present in components.

    This avoids passing weights for datasets that aren't included in the mixture,
    which Levanter correctly rejects.
    """
    allowed = set(components.keys())
    return {k: v for k, v in weights.items() if k in allowed}
# ---
def insert_data(cls, connection):
        connection.execute(
            cls.tables.square.insert(),
            [{"id": 1, "side": 10}, {"id": 10, "side": 42}],
        )
# ---
def test_keyboardinterrupt_disables_capturing(self, testdir):
        p = testdir.makepyfile("""
            def test_hello(capfd):
                import os
                os.write(1, str(42).encode('ascii'))
                raise KeyboardInterrupt()
        """)
        result = testdir.runpytest_subprocess(p)
        result.stdout.fnmatch_lines([
            "*KeyboardInterrupt*"
        ])
        assert result.ret == 2
# ---
def item(self, *args, **kwargs):  # pragma: no cover - exercised via test assertions
            raise AssertionError("item should not be called on tracer-like indices")
# ---
def sort_tuple(tup): 
	n = len(tup) 
	for i in range(n): 
		for j in range(n-i-1): 
			if tup[j][0] > tup[j + 1][0]: 
				tup[j], tup[j + 1] = tup[j + 1], tup[j] 
	return tup
# ---
def swap_numbers(a,b):
 temp = a
 a = b
 b = temp
 return (a,b)
# ---
def test_make_blockwise_key_function_map():
    func = lambda x: 0

    key_fn = make_blockwise_key_function(
        func, "z", "ij", "x", "ij", numblocks={"x": (2, 2)}
    )

    graph = make_blockwise_graph(func, "z", "ij", "x", "ij", numblocks={"x": (2, 2)})
    check_consistent_with_graph(key_fn, graph)
# ---
def _lm_head_from_sink_params(params: dict) -> jax.Array:
    return params["core"].output_proj
# ---
def test_connect_to_region_creds(self):
        """Can connect to a dynamo region with credentials"""
        conn = DynamoDBConnection.connect(
            "us-west-1", access_key="abc", secret_key="12345"
        )
        self.assertIsNotNone(conn.host)
# ---
def test_filter_nested_field(backend):
    """Test filter with nested field access."""
    from zephyr import col

    ds = Dataset.from_list(
        [
            {"id": 1, "meta": {"score": 0.9}},
            {"id": 2, "meta": {"score": 0.3}},
            {"id": 3, "meta": {"score": 0.7}},
        ]
    ).filter(col("meta")["score"] > 0.5)

    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 2
    assert all(r["meta"]["score"] > 0.5 for r in results)
# ---
def current_date_time():
    # Get the current local time and format as MM-DD-YYYY-HH-MM-SS
    formatted_time = time.strftime("%m-%d-%Y-%H-%M-%S", time.localtime())

    return formatted_time
# ---
def __getitem__(self, i):
        return WrappedArray(self.x[i])
# ---
def default_config(self) -> LevConfig:
        return self.config_from_hf_config(self.default_hf_config)
# ---
def device_variant(self) -> str | None:
        """Device variant from worker metadata."""
        return get_device_variant(self.metadata.device)
# ---
def add_meter(self, name, meter):
        self.meters[name] = meter
# ---
def fsspec_exists(file_path):
    """
    Check if a file exists in a fsspec filesystem.

    Args:
        file_path (str): The path of the file

    Returns:
        bool: True if the file exists, False otherwise.
    """

    # Use fsspec to check if the file exists
    fs = fsspec.core.url_to_fs(file_path)[0]
    return fs.exists(file_path)
# ---
def outfit():
    collection = []

    for _ in range(0, 5):
        collection.append("Item{}".format(_))

    return {
        "data": collection,
    }
# ---
def model_type(self) -> Type["ToyLmHeadModel"]:
        return ToyLmHeadModel
# ---
def __call__(self, x, *, key):
            return self.layers.fold(x, key=jax.random.split(key, self.layers.Block.size))
# ---
def log_time_series(self) -> bool:
        return self._log_time_series
# ---
def __call__(self, text, add_special_tokens=False, **kwargs):
        """Make tokenizer callable like HuggingFace tokenizers."""
        if isinstance(text, list):
            input_ids = [self.encode(t, add_special_tokens) for t in text]
        else:
            input_ids = self.encode(text, add_special_tokens)
        return {"input_ids": input_ids}
# ---
def linear_fit(self, var: xr.DataArray) -> tuple[float, float]:
        coeffs_trend = np.polyfit(np.arange(var.size), var, 1)
        return coeffs_trend[0], coeffs_trend[1]
# ---
def read(self):
        return ''
# ---
def count_dir(path: str) -> int:
        pattern = os.path.join(path.rstrip("/"), "**", "*.jsonl*")
        pipeline = Dataset.from_files(pattern, empty_glob_ok=True).flat_map(load_file).map(lambda _: 1).reduce(sum)
        results = Backend.execute(pipeline)
        return results[0]
# ---
def get_transactions(self, limit: int = 100) -> list[TransactionLog]:
        """Return recent transactions for debugging."""
        with self._lock:
            return list(self._transactions)[-limit:]
# ---
def get_endpoint(self, template, *args):
        return template % ((self.service_port,) + tuple(args))
# ---
def the_variable_key_is_value(key, value):
    variables[key] = eval(value)
# ---
def create_mesh():
    """Create a simple JAX mesh for testing."""
    devices = jax.local_devices()[:1]
    return jax.sharding.Mesh(np.array(devices), axis_names=("batch",))
# ---
def test_get_process_logs_no_buffer():
    """Test GetProcessLogs returns empty when buffer is None."""
    state = ControllerState()
    mock_scheduler = MockSchedulerWake()
    service = ControllerServiceImpl(state, mock_scheduler, bundle_prefix="file:///tmp/test-bundles", log_buffer=None)

    response = service.get_process_logs(cluster_pb2.Controller.GetProcessLogsRequest(prefix="", limit=0), None)
    assert len(response.records) == 0
# ---
def adamh_transform():
                components = []
                if self.max_grad_norm:
                    components.append(optax.clip_by_global_norm(self.max_grad_norm))
                components.append(scale_by_adamh(self.beta1, self.beta2, self.epsilon, learning_rate))
                optimizer = optax.chain(*components)
                return optimizer
# ---
def __len__(self):
        return self._size
# ---
def create_item(queries, label, thumb='', fanart='', is_folder=None, is_playable=None, total_items=0, menu_items=None, replace_menu=False):
    list_item = xbmcgui.ListItem(label, iconImage=thumb, thumbnailImage=thumb)
    add_item(queries, list_item, fanart, is_folder, is_playable, total_items, menu_items, replace_menu)
# ---
def create_vm_record(self, conn, os_type, name):
        instances = conn.list_instances()
        self.assertEquals(instances, [name])

        # Get Nova record for VM
        vm_info = conn.get_info({'name': name})
        # Get XenAPI record for VM
        vms = [rec for ref, rec
               in xenapi_fake.get_all_records('VM').iteritems()
               if not rec['is_control_domain']]
        vm = vms[0]
        self.vm_info = vm_info
        self.vm = vm
# ---
def compute(obj):  # Submit task
    return obj
# ---
def num_cpus(self) -> int:
        return 0
# ---
def active_count(self) -> int:
        """Number of active nodes."""
        return len(self.active_nodes)
# ---
def _get_disk_format(self):
        fmt = self._vminfo.get('format', 'raw').lower()
        return "qcow2" if fmt == "cow" else fmt
# ---
def test_single_slice_catches_failure():
    """Test that run_on_pod_new correctly handles a failing function after retries."""
    with pytest.raises(RayTaskError) as excinfo:
        run_on_pod(failing_fn, "v5litepod-4", num_slices=1, max_retries_failure=0, max_retries_preemption=0)

    assert "DeliberatelyRaisedException" in str(
        excinfo.value
    ), f"Expected 'Failed too many times' but got: {excinfo.value}"
# ---
def md5_for_file(fname):
    hash_md5 = hashlib.md5()
    with open(fname, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return str(hash_md5.hexdigest())
# ---
def is_started(self):
        return self.last_run is not None
# ---
def ray_cluster():
    from fray.cluster.ray import RayCluster

    if not ray.is_initialized():
        logging.info("Initializing Ray cluster")
        ray.init(
            address="local",
            num_cpus=8,
            ignore_reinit_error=True,
            logging_level="info",
            log_to_driver=True,
            resources={"head_node": 1},
        )
    yield RayCluster()
# ---
def move_tip(self, number):
        self.tip = self.blocks[number]
# ---
def test_completely_different_programs():
    source = "x = 1\n"
    target = "y = 'hello'\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1
# ---
def shard_names(self) -> Sequence[str]:
            return [f"shard_{i}" for i in range(4)]
# ---
def __repr__(self):
        return formatting.vars_repr(self)
# ---
def create_process(self, process_id, vpnservice, namespace):
        return fedora_strongswan_ipsec.FedoraStrongSwanProcess(
            self.conf,
            process_id,
            vpnservice,
            namespace)
# ---
def set_epoch(self, epoch: int) -> None:
        """Set the epoch for deterministic shuffling across workers."""
        self.epoch = epoch
# ---
def test_mem_write_word_at_bottom_right_just_past(self):
        self.mda.mem_write_word(3999, 0xFF08) # 'Z' with intensity.
        self.assertEqual(self.mda.video_ram[3998], 0x00) # Should be unmodified.
        self.assertEqual(self.mda.video_ram[3999], 0x08)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x00, MDA_BRIGHT_GREEN, MDA_BLACK))
# ---
def module_custom_product(module_org):
    return entities.Product(organization=module_org).create()
# ---
def create_index(self, index=None):
        if index is None:
            index = self.index
        try:
            get_indices(self.es).create(self.index)
        except elasticsearch.TransportError:
            pass
# ---
def testDeleteAttribute(self):
    self.assertEqual((0, 'False\n'), _GrumpRun(textwrap.dedent("""\
        class Foo(object):
          bar = 42
        del Foo.bar
        print hasattr(Foo, 'bar')""")))
# ---
def test_get_host(self):
        self.assertEqual("www.python.org", self.get.get_host())
# ---
def compute_aggregated_metric(logits, end=1.0):
    # Compute aggregated metric from logits
    num_bins = logits.shape[-1]
    bin_width = end / num_bins
    bounds = torch.arange(
        start=0.5 * bin_width, end=end, step=bin_width, device=logits.device
    )
    probs = nn.functional.softmax(logits, dim=-1)
    plddt = torch.sum(
        probs * bounds.view(*((1,) * len(probs.shape[:-1])), *bounds.shape),
        dim=-1,
    )
    return plddt
# ---
def len_log(list1):
    min=len(list1[0])
    for i in list1:
        if len(i)<min:
            min=len(i)
    return min
# ---
def test_best_of_n_respects_n(params, model_cfg, tokenizer):
    """Should not return more unique candidates than n."""
    n = 4
    results = best_of_n(
        params=params,
        source="x = 1\n",
        cfg=model_cfg,
        tokenizer=tokenizer,
        key=jax.random.PRNGKey(7),
        n=n,
        max_depth=2,
    )
    assert len(results) <= n
# ---
def kill(self, container_id: str, force: bool = False) -> None: ...
# ---
def reasonable_default(module, path):
            # TODO: gross
            if "LayerNorm" in path:
                return False
            if "RMSNorm" in path:
                return False
            if "RmsNorm" in path:
                return False
            if "Embedding" in path:
                return False
            if path.endswith("bias"):
                return False
            return None
# ---
def gumbel(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.gumbel(key, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def test_fuse_mixed_levels_and_diamond_complex(spec):
    a = xp.ones((2, 2), chunks=(2, 2), spec=spec)
    b = xp.ones((2, 2), chunks=(2, 2), spec=spec)
    c = xp.add(a, b)
    d = xp.positive(c)
    e = d[1:, :]  # operation can't be fused
    f = xp.add(e, c)  # this order exposed a bug in argument ordering

    opt_fn = multiple_inputs_optimize_dag

    f.visualize(optimize_function=opt_fn)
    result = f.compute(optimize_function=opt_fn)
    assert_array_equal(result, 4 * np.ones((2, 2)))
# ---
def __init__(self):
            super().__init__()
            self.a = jnp.zeros(1)
            self.b = self.a
# ---
def is_good(n):

    return 1 + ((int(n) - 1) % 9) == 9
# ---
def step_impl(context):
    context.execute_steps(u'''
        given I open History dialog
    ''')
    history = context.browser.find_element_by_id("HistoryPopup")
    entries = history.find_elements_by_xpath('.//li[not(@data-clone-template)]')
    assert len(entries) > 0, "There are no entries in the history"
    item = entries[0]
    item.find_elements_by_xpath('.//*[@data-share-item]')[0].click()
# ---
def test_moe_linear_out_first_property():
    E, In, Out = hax.make_axes(E=2, In=4, Out=3)
    moe = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(0), out_first=True)
    assert moe.out_first
    assert moe.weight.axes[:3] == (E, Out, In)

    moe2 = MoELinear.init(E, In, Out, key=jrandom.PRNGKey(1), out_first=False)
    assert not moe2.out_first
    assert moe2.weight.axes[:3] == (E, In, Out)
# ---
def __iter__(self):
        return iter(self.options)
# ---
def min_distances(
    coords1: np.ndarray,
    coords2: np.ndarray,
    random: np.random.Generator,
    noise_std: float = 1,
    axis: int = 1,
):
    distances = cdist(coords1, coords2)
    distances[np.isnan(distances)] = np.inf
    min_distances = np.min(distances, axis=axis)
    noisy_distances = (
        min_distances + random.normal(size=min_distances.shape) * noise_std
    )
    return noisy_distances
# ---
def __call__(self, x, *, key):
            return x + self.array + self.static
# ---
def __init__(self, Height: Axis, Width: Axis, begin, end, stride):
        super().__init__()
        self.Height = Height
        self.Width = Width
        self.begin = begin
        self.end = end
        self.stride = stride
# ---
def unique_sublists(list1):
    result ={}
    for l in list1: 
        result.setdefault(tuple(l), list()).append(1) 
    for a, b in result.items(): 
        result[a] = sum(b)
    return result
# ---
def count_reverse_pairs(test_list):
  res = sum([1 for idx in range(0, len(test_list)) for idxn in range(idx, len( 
	test_list)) if test_list[idxn] == str(''.join(list(reversed(test_list[idx]))))]) 
  return str(res)
# ---
def __init__(self):
        self._actor_pool: list[ActorPoolMember] = []
# ---
def test_scatter_set():
    B, V = Axis("batch", 2), Axis("vocab", 6)
    x = hax.zeros((B, V))
    idx = hax.named(jnp.array([1, 4]), B)
    val = hax.ones(B) * 9
    y = x.at[{V: idx}].set(val)
    ref = jnp.zeros((2, 6)).at[jnp.arange(2), idx.array].set(9)
    assert jnp.array_equal(y.array, ref)
# ---
def get_currency_id(self, cr, uid, picking):
        return False
# ---
def move_shadow_params_to_device(self, device):
    self.shadow_params = [i.to(device) for i in self.shadow_params]
# ---
def silence_transformer_nag():
    # this is a hack to silence the transformers' "None of PyTorch, TensorFlow 2.0 or Flax have been found..." thing
    # which is annoying and not useful
    # Often we won't call this early enough, but it helps with multiprocessing stuff
    if os.getenv("TRANSFORMERS_VERBOSITY") is None:
        os.environ["TRANSFORMERS_VERBOSITY"] = "error"

    import transformers
# ---
def tree_unflatten(cls, aux, tree: Any) -> Any:
        assert len(tree) == 1
        return cls(tree[0], main_axes=aux)
# ---
def isexact(self):
        return self._m1.isexact() or self._m2.isexact()
# ---
def first_Digit(n) :  
    while n >= 10:  
        n = n / 10; 
    return int(n)
# ---
def sample(self, records: List[Record]) -> list[Sample]:
        """Sample a structure from the dataset infinitely.

        Parameters
        ----------
        records : List[Record]
            The records to sample from.

        Returns
        -------
        List[Sample]
            The samples.

        """
        raise NotImplementedError
# ---
def get_item_attribute(parent, attribute_value=''):
	if not frappe.has_permission("Item"):
		frappe.msgprint(_("No Permission"), raise_exception=1)

	return frappe.get_all("Item Attribute Value", fields = ["attribute_value"],
		filters = {'parent': parent, 'attribute_value': ("like", "%%%s%%" % attribute_value)})
# ---
def print_field(label, value):
    if get_delimiter() is not None:
        sys.stdout.write(label + get_delimiter() + value + '\n')
    else:
        sys.stdout.write(
            label + " " * (FIELD_NAME_WIDTH-len(label)) + fill(value,
                                                               subsequent_indent=' '*FIELD_NAME_WIDTH,
                                                               width_adjustment=-FIELD_NAME_WIDTH) +
            '\n')
# ---
def allocate_ports():
        ports = allocator.allocate(count=5)
        results.append(ports)
# ---
def pids(self):
        return [p.pid for p in self._procs]
# ---
def backend(request):
    """Parametrized fixture providing all job contexts for testing."""
    return request.param
# ---
def __init__(self, data_format='default'):
        if data_format == 'default':
            data_format = default_data_format
        self.dim_ordering = data_format
# ---
def __init__(self, secret, callback, realm='Realm'):
		self.secret = secret
		self.callback = callback
		self.realm = realm
# ---
def __init__(self,**kwargs):
        self.register_event_type('on_ok')
        super(OkPopup,self).__init__(**kwargs)
# ---
def key():
    return jax.random.PRNGKey(0)
# ---
class Node: 
	def __init__(self, data): 
		self.data = data 
		self.left = None
		self.right = None
def max_height(node): 
	if node is None: 
		return 0 ; 
	else : 
		left_height = max_height(node.left) 
		right_height = max_height(node.right) 
		if (left_height > right_height): 
			return left_height+1
		else: 
			return right_height+1
# ---
def slice(
    array: NamedArray,
    axis: AxisSelector,
    new_axis: AxisSelector | None = None,
    start: int = 0,
    length: int | None = None,
) -> NamedArray:
    pass
# ---
def is_too_large(size: int) -> bool:
    return server.config.max_size is not None and size > server.config.max_size
# ---
def __default(self, param):
        attr1 = getattr(self.want, param)
        try:
            attr2 = getattr(self.have, param)
            if attr1 != attr2:
                return attr1
        except AttributeError:
            return attr1
# ---
def __repr__(self):
        return '<MultiCoverage %r: %r>' % (self.extent.llbbox, self.coverages)
# ---
def on_epoch_end(self, model: LightningModule) -> None:
        # For now all was dumped into the common operation in the parent Validator class
        self.common_on_epoch_end(model)
# ---
def create_step():
        a = ExecutorStep(name="a", fn=fn, config=None)
        b = ExecutorStep(
            name="b",
            fn=fn,
            config=MyConfig(
                input_path=output_path_of(a, "sub"),
                output_path=this_output_path(),
                n=versioned(3),
                m=4,
            ),
        )
        return b
# ---


def largest_divisor(n: int) -> int:
    """ For a given number n, find the largest number that divides n evenly, smaller than n
    >>> largest_divisor(15)
    5
    """
    for i in reversed(range(n)):
        if n % i == 0:
            return i
# ---
def cleared(self) -> "TokenQueue":
        """
        Returns a new JitScheduler with all buffers cleared.
        This is useful for resetting the scheduler state.
        """
        return TokenQueue.init(
            max_queued_tokens=self.queued_tokens.axis_size("position"),
        )
# ---
def numblocks(chunks: T_RectangularChunks) -> Tuple[int, ...]:
    return tuple(map(len, chunks))
# ---
def register(self) -> None:
        Callback.active.add(self)
# ---
def load_tokenizer(model_name: str) -> bool:
    """Load a tokenizer and set it as current."""
    global current_tokenizer
    console.print(f"[blue]Loading {model_name}...[/blue]")
    current_tokenizer = AutoTokenizer.from_pretrained(model_name)
    console.print(f"[green] Loaded {model_name}[/green]")
# ---
def create_jsonl_gz_file(data: list[dict], filepath: str):
    """Helper function to create a JSONL.GZ file"""
    with fsspec.open(filepath, "wt", encoding="utf-8", compression="infer") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")
# ---
def to_active_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            return leaf.as_scanned_result(leading_axis)
        else:
            return leaf
# ---
def next(self, psm: PSM):
        if psm.char in string.hexdigits:
            self.pattern.pattern += psm.char
            count = len(self.pattern.pattern)
            return self.prev if count >= 4 else self
        else:
            psm.error = "expected ASCII hexadecimal character"
# ---
def _copy(_):
            src_page = decode_state.sequences.page_indices["seq", src_slot_id, "page", last_idx].scalar()
            dst_page = decode_state.sequences.page_indices["seq", dst_slot_id, "page", last_idx].scalar()
            return cache.copy_page(src_page, dst_page)
# ---
def __init__(self, *args, **kwargs):
        Signature.__init__(self, *args, **kwargs)
# ---
def counting_sort(my_list):
    max_value = 0
    for i in range(len(my_list)):
        if my_list[i] > max_value:
            max_value = my_list[i]
    buckets = [0] * (max_value + 1)
    for i in my_list:
        buckets[i] += 1
    i = 0
    for j in range(max_value + 1):
         for a in range(buckets[j]):
             my_list[i] = j
             i += 1
    return my_list
# ---
def TextBeforeCursor():
  """Returns the text before CurrentColumn."""
  return ToUnicode( vim.current.line[ :CurrentColumn() ] )
# ---
def test_map_overlap_1d():
    x = np.arange(6)
    a = xp.asarray(x, chunks=(3,))

    b = cubed.map_overlap(
        lambda x: x,
        a,
        dtype=a.dtype,
        chunks=((5, 5),),
        depth=1,
        boundary=0,
        trim=False,
    )

    assert_array_equal(b.compute(), np.array([0, 0, 1, 2, 3, 2, 3, 4, 5, 0]))
# ---
def _parse_range(data):
    """Parses the format range properties - min, max."""
    input_type = {}
    try:
        input_type['min'] = data['range'][0]
    except (KeyError, TypeError):  # set default value
        input_type['min'] = float('-inf')
    try:
        input_type['max'] = data['range'][1]
    except (KeyError, TypeError):  # set default value
        input_type['max'] = float('inf')
    return input_type
# ---
def finished(status: JobStatus) -> bool:
        return status in (JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.STOPPED)
# ---
def __str__(self):
        year = ""
        if self.__year is not None:
            year = " in {0}".format(self.__year)
        return "{0} by {1}{2}".format(self.__title, self.__artist, year)
# ---
def to_dict(self):
        return {
            f"dedup/{self.method}/{self.level}/total": self.total,
            f"dedup/{self.method}/{self.level}/dups": self.dups,
            f"dedup/{self.method}/{self.level}/unique": self.unique,
            f"dedup/{self.method}/{self.level}/dup_clusters": self.dup_clusters,
        }
# ---
def setplayer(p):
    global player
    player = p
# ---
def to_bool(val):
  if val is None:
    return false
  return val == 1
# ---
def __call__(
        self, in_channels: int, out_channels: int
    ) -> BilinearUpsample | TransposedConvUpsample: ...
# ---
import re
def string_literals(patterns,text):
  for pattern in patterns:
     if re.search(pattern,  text):
       return ('Matched!')
     else:
       return ('Not Matched!')
# ---
def test_byte_length_of_token_gpt2():
    tok = load_tokenizer("gpt2")
    ids = tok("this is hello a test", add_special_tokens=False)["input_ids"]
    assert byte_length_of_token(tok, ids[2]) == len(" hello".encode("utf-8"))

    eos = tok.eos_token_id
    assert byte_length_of_token(tok, eos) == 0
# ---
def _setup_regular(self, env):
        super(Char, self)._setup_regular(env)
        assert isinstance(self.size, (NoneType, int)), \
            "Char field %s with non-integer size %r" % (self, self.size)
# ---
def main():
    if os.getenv("CI", None) is not None:
        logger.info("Skipping experiment execution on CI environment, needs HF access.")
        return

    for step in build_steps():
        executor_main(steps=[step])
# ---
def test_grad_scan():
    X = hax.Axis("x", 4)

    def f(x):
        x_ref = hax.new_ref(hax.zeros(X))

        def scan_fn(_, i):
            slice = x_ref.slice({"x": i})
            slice[...] = jnp.sin(x * i)
            return None, None

        hax.scan(scan_fn, X)(None, jnp.arange(X.size))
        return x_ref[...].sum().scalar()

    df = jax.grad(f)(1.0)
    assert pytest.approx(df) == jnp.sum(jnp.cos(jnp.arange(X.size)) * jnp.arange(X.size))
# ---
def requires(self):
        yield DownloadRITACatalogs()
        yield DownloadRITAData()
# ---
def arcsinh(a: A) -> A:
    return wrap_elemwise_unary(jnp.arcsinh, a)
# ---
def _get_key_str(self, minion_id, status):
        '''
        Return the key string in the form of:

        pub: <pub>
        verify: <verify>
        '''
        path = os.path.join(self.opts['pki_dir'], status, minion_id)
        with salt.utils.fopen(path, 'r') as fp_:
            keydata = self.serial.loads(fp_.read())
            return 'pub: {0}\nverify: {1}'.format(
                    keydata['pub'],
                    keydata['verify'])
# ---
def init_accumulators():
        m_scratch_ref[...] = jnp.full_like(m_scratch_ref, -jnp.inf)
        l_scratch_ref[...] = jnp.zeros_like(l_scratch_ref)
        label_logits_scratch_ref[...] = jnp.zeros_like(label_logits_scratch_ref)
# ---
def __init__(self, string=None, **kwargs):
        kwargs['string'] = string
        attrs = {key: val for key, val in kwargs.iteritems() if val is not None}
        self._attrs = attrs or EMPTY_DICT
# ---
def test_encrypt_many_newlines_at_end(self):
        self._test_encryption('Message with lotsa newlines.\n\n\n')
# ---
def test_vmap_mapped_kwarg():
    Batch = Axis("Batch", 10)
    Width = Axis("Width", 3)
    Depth = Axis("Depth", 4)
    named1 = hax.random.uniform(PRNGKey(0), (Batch, Width, Depth))

    def vmap_fun(x):
        return x.sum(Width)

    selected = hax.vmap(vmap_fun, Batch)(x=named1)

    expected_jax = jnp.array([named1.sum(Width).array for _ in range(Batch.size)])
    expected_names = (Batch, Depth)

    assert jnp.all(jnp.equal(selected.array, expected_jax))
    assert selected.axes == expected_names
# ---
def assertVolumeTypeListEqual(self, expected, observed):
        self.assertEqual(len(expected), len(observed))
        expected = sorted(expected, key=lambda item: item['id'])
        observed = sorted(observed, key=lambda item: item['id'])
        for d1, d2 in zip(expected, observed):
            self.assertEqual(d1['id'], d2['id'])
# ---
def test_rename_variables_no_names_returns_none(rng):
    # Only builtins and constants.
    source = "print(42)"
    result = rename_variables(source, rng)
    # "print" is protected, "42" is a constant  no names to rename.
    assert result is None
# ---
def to_return(self):
        result = {}
        try:
            for returnable in self.returnables:
                result[returnable] = getattr(self, returnable)
            result = self._filter_params(result)
        except Exception:
            raise
        return result
# ---
def _is_retryable_hf_exception(exc: Exception) -> bool:
    if isinstance(exc, HfHubHTTPError):
        status_code = getattr(getattr(exc, "response", None), "status_code", None)
        return status_code in {408, 429} or (status_code is not None and 500 <= status_code < 600)

    return isinstance(exc, (requests.exceptions.Timeout, requests.exceptions.ConnectionError))
# ---
def get_data(self) -> dict[str, torch.Tensor]:
        if self._n_timesteps == 0 or self._data is None:
            raise ValueError("No data recorded.")

        assert self._n_samples is not None  # for type checker

        ret = {}
        names = sorted(list(self._data.keys()))  # sort for rank-consistent order
        for name in names:
            value = self._data[name]
            gen = all_reduce_mean(value / self._n_timesteps / self._n_samples)
            ret[name] = gen
        return ret
# ---
def __matmul__(self, other) -> "NamedArray":  # pragma: no cover
        raise ValueError("matmul is too ambiguous with NamedArrays. Use dot instead.")
# ---
def reset_outdir(self):
        if self.outdir.exists():
            shutil.rmtree(self.outdir)
        self.outdir.mkdir(parents=True, exist_ok=True)
        self.top_dir.mkdir(parents=True, exist_ok=True)
        self.div_dir.mkdir(parents=True, exist_ok=True)
# ---
def bar(x):
            return x
# ---
def genetic_modification_8(lab, award):
    return {
        'purpose': 'analysis',
        'category': 'interference',
        'award': award['uuid'],
        'lab': lab['uuid'],
        "method": "CRISPR",
    }
# ---
def normal(key, shape: AxisSpec, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.normal(key=key, shape=jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def convert_to_export(self, value, env):
        if value or value == 0:
            return value if env.context.get('export_raw_data') else ustr(value)
        return ''
# ---
def convert_to_display_name(self, value, record=None):
        return ustr(value and value.display_name)
# ---
def get(self, ref: Any) -> Any:
        """Retrieve an object from its reference.

        Args:
            ref: Reference to retrieve

        Returns:
            The stored object
        """
        ...
# ---
def test_wait_for_condition_immediate() -> None:
    """Test wait_for_condition returns immediately when condition is already true."""
    start = time.monotonic()
    wait_for_condition(lambda: True, timeout=Duration.from_seconds(1.0))
    elapsed = time.monotonic() - start

    # Should return almost immediately
    assert elapsed < 0.1
# ---
def map_blocks_multiple_outputs(
    func,
    *args,
    shapes,
    dtypes,
    chunkss,
    **kwargs,
):
    def key_function(out_key):
        return tuple((array.name,) + out_key[1:] for array in args)

    return general_blockwise(
        func,
        key_function,
        *args,
        shapes=shapes,
        dtypes=dtypes,
        chunkss=chunkss,
        target_stores=[None] * len(dtypes),
        **kwargs,
    )
# ---
MAX = 3000 
def smartNumber(n): 
	primes = [0] * MAX 
	result = [] 
	for i in range(2, MAX): 
		if (primes[i] == 0): 
			primes[i] = 1 
			j = i * 2 
			while (j < MAX): 
				primes[j] -= 1 
				if ( (primes[j] + 3) == 0): 
					result.append(j) 
				j = j + i 
	result.sort() 
	return result[n - 1]
# ---
def decode_sequence(tokenizer, tensor: Sequence[int]) -> str:
    return tokenizer.decode(list(tensor), skip_special_tokens=False)
# ---
def _run_viz():
        with remove_tpu_lockfile_on_exit():
            do_viz_lm(levanter_config)
# ---
def __init__(self, driver):
        """Creates a new ActionChains.
        Args:
            driver: The WebDriver instance which performs user actions.
        """
        self._driver = driver
        self._actions = []
# ---
def unique_Characters(str):
    for i in range(len(str)):
        for j in range(i + 1,len(str)): 
            if (str[i] == str[j]):
                return False;
    return True;
# ---
def loss_fn(self) -> WrappedLossFunction:
        """
        Wrapped loss function that always returns (loss, metrics_dict).
        Casts the model to compute precision and sets the context axis mapping to compute.
        """
        return WrappedLossFunction(
            self._raw_loss_function,
            self.mp,
            self.compute_axis_mapping,
        )
# ---
def add_ms(self, milliseconds: int) -> "Timestamp":
        """Return new timestamp offset by milliseconds (may be negative)."""
        return Timestamp(self._epoch_ms + milliseconds)
# ---
def rms_norm(x: Float[Array, "... D"], weight: Float[Array, "D"], eps: float) -> Float[Array, "... D"]:
    weight = unshard(weight)
    dtype = x.dtype
    x = x.astype(jnp.float32)
    variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
    normed = x * jax.lax.rsqrt(variance + eps)
    out = normed * weight
    return out.astype(dtype)
# ---
def test_equal_1(self):
        self.assertEqual(string_color('Jack'), '79CAE5')
# ---
def enterWalk(self):
        self.notify.debug('going for a walk')
        self.walk.enter()
        self.acceptOnce(self.walkDoneEvent, self.__decideNextState)
# ---
def foo2(x):
            return hax.shard(x, resource_map)
# ---
def __lt__(self, other):
        if other is None:
            return False
        return self.id < other.id
# ---
def _create_and_run_worker(self):
        """Create and run the worker. Must be implemented by subclasses."""
        pass
# ---
def set_instances(self, instances: list[dict]) -> None:
        self._instances = instances
# ---
def _remove_endpoints_for_task(self, task_id: JobName) -> list[ControllerEndpoint]:
        """Remove all endpoints associated with a task."""
        endpoint_ids = list(self._endpoints_by_task.get(task_id, []))
        removed = []
        for eid in endpoint_ids:
            endpoint = self._endpoints.pop(eid, None)
            if endpoint:
                removed.append(endpoint)
        self._endpoints_by_task.pop(task_id, None)
        return removed
# ---
def resource_type(self):
        """Returns the primary type of resource this client works with."""
        return 'image'
# ---
def _scale_up(self, count: int):
        """Add new actors to the pool."""
        logger.info(f"Scaling up: adding {count} new actors")
        for _ in range(count):
            self._create_and_register_actor()
        logger.info(f"Pool size now: {len(self.actors)} actors")
# ---
def init(cls, Vocab: Axis, config: LlamaConfig, *, key) -> "LlamaLMHeadModel":
        k_t, k_emb = jrandom.split(key, 2)
        transformer = LlamaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)

        return LlamaLMHeadModel(transformer, embeddings, lm_head)
# ---
def _charge_class(res):
    if res in POSITIVE:
        return "positive"
    elif res in NEGATIVE:
        return "negative"
    else:
        return "uncharged"
# ---
def create_test_inference_server_config(model_config: LlamaConfig, output_dir: str | Path):
    return InferenceServerConfig(
        trainer=create_nano_trainer_config(output_dir),
        tokenizer=DummyTokenizer(),
        service=InferenceEngineConfig(
            max_seqs=8,
            page_size=8,
            max_seq_len=64,
            max_queued_tokens=8,
        ),
        temperature=1.0,
        port=find_open_port(),
    )
# ---
def test_unique_str_parallel(self):
        # TODO: test without file
        def test_impl():
            df = pq.read_table('example.parquet').to_pandas()
            return (df.two.unique() == 'foo').sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
# ---
def __repr__(self):
		return "%s(file=%r, build=%r)" % (self.__class__.__name__, self.file, self.build)
# ---
def forward(self, input_ids):
        """
        input_ids: (batch, seqlen)
        """
        return self.word_embeddings(input_ids)
# ---
def str_to_tuple(test_str):
  res = tuple(map(int, test_str.split(', ')))
  return (res)
# ---
def __init__(self, urls, columns=None):
        super().__init__(urls)
        self.columns = columns
# ---
def num_source_arrays(dag, name):
    """Return the number of (non-hidden) arrays that are inputs to an op.

    Hidden arrays are used for internal bookkeeping, are very small virtual arrays
    (empty, or offsets for example), and are not shown on the plan visualization.
    For these reasons they shouldn't count towards ``max_total_source_arrays``.
    """
    nodes = dict(dag.nodes(data=True))
    return sum(
        not nodes[array]["hidden"] for array in predecessors_unordered(dag, name)
    )
# ---
def isOK( self ):
    return self.valid
# ---
def prep_signature(self):
        self._call_all('prep_signature')
# ---
def __init__(
        self,
        val_names: List[str],
        confidence_prediction: bool = False,
        override_val_method: Optional[str] = None,
    ) -> None:
        super().__init__(
            val_names=val_names,
            confidence_prediction=confidence_prediction,
            override_val_method=override_val_method,
        )
# ---
def parse_block_html(self, m):
        tag = m.group(1)
        if not tag:
            text = m.group(0)
            self.tokens.append({
                'type': 'close_html',
                'text': text
            })
        else:
            attr = m.group(2)
            text = m.group(3)
            self.tokens.append({
                'type': 'open_html',
                'tag': tag,
                'extra': attr,
                'text': text
            })
# ---
def _chips_per_slice(tpu_type: str) -> tuple[int, int]:
    config = TPU_CONFIG_BY_NAME.get(tpu_type)
    if config is None:
        raise ValueError(f"Unknown TPU type: {tpu_type}")

    try:
        _, suffix = tpu_type.split("-", maxsplit=1)
        reported_per_slice = int(suffix)
    except (ValueError, TypeError) as exc:  # pragma: no cover - defensive
        raise ValueError(f"Unexpected TPU type format: {tpu_type}") from exc

    return config.chip_count, reported_per_slice
# ---
def load_apartments():
        # type: () -> List[ApartmentDTO]
        apartments = []
        for apartment_orm in Apartment.select():
            apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)
            apartments.append(apartment_dto)
        return apartments
# ---
def target_account_names(self):
        return [tr('< New Account >')] + [a.name for a in self.target_accounts]
# ---
def __set__(self, record, value):
        raise TypeError("field 'id' cannot be assigned")
# ---
def is_dir_of_checkpoints(path):
    fs = fsspec.filesystem("gcs")
    # if the children are named like step-XXXXX, then it's a checkpoint directory
    children = fs.ls(path)
    return any("step-" in child for child in children)
# ---
def tearDown(self):
        self.es_client.indices.delete(index="*", ignore=404)
        self.es_client.indices.delete_template(name="*", ignore=404)
# ---
def build(
        self,
        context: Path,
        dockerfile_content: str,
        tag: str,
        task_logs: TaskLogs | None = None,
    ) -> None: ...
# ---
def GetVimCommand( user_command, default = 'edit' ):
  vim_command = BUFFER_COMMAND_MAP.get( user_command, default )
  if vim_command == 'edit' and not BufferIsUsable( vim.current.buffer ):
    vim_command = 'split'
  return vim_command
# ---
def __init__(self, max_threads: int = 8):
        self._executor = ThreadPoolExecutor(max_workers=max_threads)
        self._jobs: list[LocalJobHandle] = []
# ---
def stop(self):
        """Stop the event loop and join the thread."""
        if self._loop is None:
            return

        self._loop.call_soon_threadsafe(self._loop.stop)
        if self._thread is not None:
            self._thread.join(timeout=5)
        self._loop = None
        self._thread = None
# ---
def standardchar():
        return redirect(
            url_for("chargen", a=3, b=5, c=7, abia=5, abib=9, abic=13, shuffle=1)
        )
# ---
def test_spatial_dropout_1d(self):
    testing_utils.layer_test(
        keras.layers.SpatialDropout1D,
        kwargs={'rate': 0.5},
        input_shape=(2, 3, 4))
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        self.run.log_artifact(artifact_path, name=name, type=type)
# ---
def get_account_id(
    profile_name, aws_access_key_id, aws_secret_access_key, region=None,
):
    """Query STS for a users' account_id"""
    client = get_client(
        "sts", profile_name, aws_access_key_id, aws_secret_access_key, region,
    )
    return client.get_caller_identity().get("Account")
# ---
def test_formatted_string_table(datadir):
    datadir.join('reader').chdir()
    src = 'shared-strings-rich.xml'
    with open(src) as content:
        assert read_string_table(content.read()) == [
            'Welcome', 'to the best shop in town', "     let's play "]
# ---
def test_to_chunksize():
    assert to_chunksize(((3, 3, 3, 1),)) == (3,)
    assert to_chunksize(((0,),)) == (1,)  # Zarr doesn't support zero-length chunks
    with pytest.raises(ValueError, match="Array must have regular chunks"):
        to_chunksize(((3, 2, 3, 3, 1),))
# ---
def barf(*args):
        raise NotImplementedError('Access denied')
# ---
def preprocess_example(self, example, mode, hparams):
    if not self._was_reversed:
      example["inputs"] = tf.image.per_image_standardization(example["inputs"])
    return example
# ---
def time_slice(self) -> slice:
        return slice(self.start.datetime, self.end.datetime)
# ---
def on_step(self, step_info: S, cb_info: dict[str, jax.Array | Histogram]):
        levanter.tracker.log(cb_info, step=int(step_info.step))
# ---
def contains(self, bbox, srs):
        return any(c.contains(bbox, srs) for c in self.coverages)
# ---
def _invert_amounts(self, apply_to_all):
        if apply_to_all:
            panes = self.panes
        else:
            panes = [self.selected_pane]
        for pane in panes:
            entries = self.loader.accounts.entries_for_account(pane.account)
            txns = dedupe(e.transaction for e in entries)
            for txn in txns:
                for split in txn.splits:
                    split.amount = -split.amount
        self.import_table.refresh()
# ---
def _suffix_grid(
    channel_vars: DictSingleChannelVar, src: DataSource
) -> DictSingleChannelVar:
    out = {}
    for k, v in channel_vars.items():
        out[f"{k}/{gridstr(src)}"] = v
    return out
# ---
def _get_width(Width: int | Axis) -> Axis:
    if isinstance(Width, int):
        return Axis(DEFAULT_WIDTH_NAME, Width)
    else:
        return Width
# ---
def loadProfile(self, profile):
        """Load profile"""

        self.saveBasicSettings()

        self.prefsDict['activeProfile'] = profile
        _settingsManager.setProfile(profile[1])
        self.prefsDict = _settingsManager.getGeneralSettings(profile[1])

        orca.loadUserSettings(skipReloadMessage=True)

        self._initGUIState()

        braille.checkBrailleSetting()

        self._initSpeechState()

        self._populateKeyBindings()

        self.__initProfileCombo()
# ---
def __call__(
        self, x: NamedArray, attn_mask: Optional[NamedArray | AttentionMask], *, key, pos_ids: NamedArray | None = None
    ) -> NamedArray:
        keys = maybe_rng_split(key, self.config.num_layers) if key is not None else None
        x = cast(NamedArray, self.layers.fold(x, mask=attn_mask, key=keys, pos_ids=pos_ids))
        x = self.norm(x)
        return x
# ---
def __eq__(self, other):
        return super().__eq__(other) and \
            self.subqueries == other.subqueries
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.var()
# ---
def add_child(graph, child_id, child_label, parent_id, colour):
    """
    http://www.graphviz.org/doc/info/shapes.html#polygon
    """
    node = pydot.Node(child_id, style="filled", fillcolor=colour, label=child_label, shape="polygon", fontname=FONT)
    graph.add_node(node)
    graph.add_edge(pydot.Edge(parent_id, node))
# ---
def bitwise_not(a: A) -> A:
    return wrap_elemwise_unary(jnp.bitwise_not, a)
# ---
def _to_np(x):
    return np.array(x.detach().cpu().numpy())
# ---
def _get_field(node: ast.AST, field_name: str):
    """Get a field value from an AST node, or None if missing."""
    try:
        return getattr(node, field_name)
    except AttributeError:
        return None
# ---
def permute_string(str):
    if len(str) == 0:
        return ['']
    prev_list = permute_string(str[1:len(str)])
    next_list = []
    for i in range(0,len(prev_list)):
        for j in range(0,len(str)):
            new_str = prev_list[i][0:j]+str[0]+prev_list[i][j:len(str)-1]
            if new_str not in next_list:
                next_list.append(new_str)
    return next_list
# ---
def amin(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    """
    Aliax for min. See min for details.
    """
    return wrap_reduction_call(jnp.amin, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def call(self, request: actor__pb2.ActorCall, ctx: RequestContext) -> actor__pb2.ActorResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def initial_cache(pages: int) -> int:
            table = PageTable.init(pages, config.max_seqs, config.page_size, max_pages_per_seq)
            cache_shape = model.initial_cache(table.spec(), dtype=config.compute_dtype)
            return cache_shape
# ---
import math
def sqrt_root(num):
 sqrt_root = math.pow(num, 0.5)
 return sqrt_root
# ---
def _new_model(old_model):
        return jax.tree_util.tree_map(lambda x: x * 0, old_model)
# ---
def _get_partner_to_invoice(self, cr, uid, picking, context=None):
        """ Gets the partner that will be invoiced
            Note that this function is inherited in the sale and purchase modules
            @param picking: object of the picking for which we are selecting the partner to invoice
            @return: object of the partner to invoice
        """
        return picking.partner_id and picking.partner_id.id
# ---
def test_not_equal(self):
        expr = col("score") != 100
        assert expr.evaluate({"score": 100}) is False
        assert expr.evaluate({"score": 50}) is True
# ---
def GetSession(self, request, context):
        """Gets a session. Returns `NOT_FOUND` if the session does not exist.
    This is mainly useful for determining whether a session is still
    alive.
    """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def _state_dict_key_map(self) -> Dict[str, Optional[str]]:
        """Map model parameter names to HF parameter names"""
        return {
            "transformer": "model",
            "embeddings": "model",
            "lm_head": "lm_head",
        }
# ---
def wait(self):
        if self._thread is not None and self._thread.is_alive():
            self._thread.join()
# ---
def _update_current_step(info: levanter.callbacks.StepInfo):
            self.replay_buffer.set_current_step(info.step)
# ---
from itertools import groupby
def consecutive_duplicates(nums):
    return [key for key, group in groupby(nums)]
# ---
def pow(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "pow")
    if x1.dtype not in _numeric_dtypes or x2.dtype not in _numeric_dtypes:
        raise TypeError("Only numeric dtypes are allowed in pow")
    return elemwise(nxp.pow, x1, x2, dtype=result_type(x1, x2))
# ---
def get(self) -> int:
        """Get current counter value."""
        return self._value
# ---
def step_impl(context, username, field, value):
    user = context.user_service.exists(username)

    if user is not None:
        assert user[field] == value
    else:
        raise UserNotFound(username, "User was not found")
# ---
def convert_to_cache(self, value, record, validate=True):
        return value or {}
# ---
def log_summary(self, metrics: dict[str, Any]):
        pass
# ---
def find_equal_tuple(Input, k):
  flag = 1
  for tuple in Input:
    if len(tuple) != k:
      flag = 0
      break
  return flag
def get_equal(Input, k):
  if find_equal_tuple(Input, k) == 1:
    return ("All tuples have same length")
  else:
    return ("All tuples do not have same length")
# ---
def __getitem__(self, idx):
        x_index = self._get_x_index(idx)
        data_in = self._get_prognostic(x_index)
        data_in_boundary = self._get_boundary(x_index)
        data_in = torch.cat((data_in, data_in_boundary), dim=1)
        label = self._get_label(x_index)
        return (data_in, label)
# ---
def pick_remove_edge(g):
        u = nx.utils.arbitrary_element(g)
        possible_nodes = list(g.neighbors(u))
        v = nx.utils.arbitrary_element(possible_nodes)
        return (u, v)
# ---
def _rm_checkpoint(self, checkpoint):
        if jax.process_index() == 0:
            logger.info(f"Removing checkpoint {checkpoint}")
            self._async_checkpoint_remover_queue.put(checkpoint)
# ---
def count_Pairs(arr,n): 
    cnt = 0; 
    for i in range(n): 
        for j in range(i + 1,n): 
            if (arr[i] != arr[j]): 
                cnt += 1; 
    return cnt;
# ---
def generation_kwargs(self):
        """Get the generation kwargs from the worker."""
        return self.leader._generation_kwargs
# ---
def BufferIsVisibleForFilename( filename ):
  """Check if a buffer exists for a specific file."""
  buffer_number = GetBufferNumberForFilename( filename, False )
  return BufferIsVisible( buffer_number )
# ---
import cmath
def polar_rect(x,y):
 cn = complex(x,y)
 cn=cmath.polar(cn)
 cn1 = cmath.rect(2, cmath.pi)
 return (cn,cn1)
# ---
def skip_if_no_torch(f):
    return pytest.mark.torch(pytest.mark.skipif(not has_torch(), reason="torch not installed")(f))
# ---
def long_job(s):
            s.wait()
# ---
def phoenixSystems(self):
        return OrderedDict(sorted(self.phoenixSystemDatabase.items(), key=lambda t: t[0]))
# ---
def __repr__(self):
        'Returns the text.'
        return self.text
# ---
def test_tracker_plugin_default_works():
    config = """
    tracker:
        entity: foo
    """
    parsed = yaml.safe_load(config)

    @dataclasses.dataclass
    class ConfigHolder:
        tracker: TrackerConfig

    import draccus

    tconfig = draccus.decode(ConfigHolder, parsed).tracker

    assert isinstance(tconfig, TrackerConfig.get_choice_class("wandb"))

    assert tconfig.entity == "foo"
# ---
def _add_property(self, name, value=None):
        self.command('add_property', name, value)
# ---
def to_path(val):
  return val
# ---
def from_read_rows_response(message):
        schema_type = message._pb.WhichOneof("schema")
        if schema_type == "avro_schema":
            return _AvroStreamParser(message)
        elif schema_type == "arrow_schema":
            return _ArrowStreamParser(message)
        else:
            raise TypeError(
                "Unsupported schema type in message: {0}".format(schema_type)
            )
# ---
def __is_public_valid_method(self,attr):
        if inspect.ismethod(getattr(self, attr)) and attr[0] != '_' and\
                attr != 'register_rpc' and attr!='register_method_args':
                    return True
        return False
# ---
def virtual_empty(
    shape: T_Shape, *, dtype: T_DType, chunks: T_RegularChunks, **kwargs
) -> VirtualEmptyArray:
    return VirtualEmptyArray(shape, dtype, chunks, **kwargs)
# ---
def get_available_workers(self) -> list[ControllerWorker]:
        with self._lock:
            return [w for w in self._workers.values() if w.healthy]
# ---
def get_task_status(
        self,
        request: cluster_pb2.Worker.GetTaskStatusRequest,
    ) -> cluster_pb2.TaskStatus: ...
# ---
def can_accept_demand(self, timestamp: Timestamp | None = None) -> bool:
        """Whether this group can accept demand for waterfall routing."""
        return self.availability(timestamp).status == GroupAvailability.AVAILABLE
# ---
def bindproperty(MPV, name, proptype, access, decode_str=False):
    getter = lambda self: self._get_property(name, proptype, decode_str)
    setter = lambda self, value: self._set_property(name, value, proptype)

    def barf(*args):
        raise NotImplementedError('Access denied')

    setattr(MPV, name.replace('-', '_'), property(getter if 'r' in access else barf, setter if 'w' in access else barf))
# ---
def unit_of_measurement(self):
        """Return the unit of measurement of this entity, if any."""
        return self._unit_of_measurement
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        return 1 / hax.axis_size(Out)
# ---
def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the OrCheck object for convenience.
        """

        self.rules.append(rule)
        return self
# ---
def test_sharded_tree_size_named_array_with_abstract_mesh():
    mesh = jax.sharding.AbstractMesh((2, 2), ("data", "model"))

    Batch = hax.Axis("batch", 8)
    Hidden = hax.Axis("hidden", 16)
    named_array = hax.ones((Batch, Hidden), dtype=jnp.float32)

    mapping = {"batch": ResourceAxis.DATA, "hidden": ResourceAxis.MODEL}

    per_device_bytes = sharded_tree_size(named_array, mesh=mesh, mapping=mapping)

    assert per_device_bytes == named_array.array.nbytes // 4
# ---
def test_health_endpoint_empty_cluster(client):
    """Health endpoint returns ok for empty cluster (no workers, no jobs)."""
    resp = client.get("/health")

    assert resp.status_code == 200
    data = resp.json()
    assert data["status"] == "ok"
    assert data["workers"] == 0
    assert data["jobs"] == 0
# ---
def __gt__(self, other: object) -> CompareExpr:
        return CompareExpr(self, _to_expr(other), "gt")
# ---
def without_axes(axis_spec: Sequence[Axis], to_remove: AxisSelection, allow_mismatched_sizes=False) -> tuple[Axis, ...]:  # type: ignore
    ...
# ---
def _build_explicit_mesh():
    devices = jax.devices()
    mesh = Mesh(np.array(devices), ("data",), axis_types=(AxisType.Explicit,))
    return mesh
# ---
def model_type(self) -> Type["WhisperModel"]:
        return WhisperModel
# ---
def test_linspace(spec, endpoint):
    a = xp.linspace(6, 49, 50, endpoint=endpoint, chunks=5, spec=spec)
    npa = np.linspace(6, 49, 50, endpoint=endpoint)
    assert_allclose(a, npa)

    a = xp.linspace(1.4, 4.9, 13, endpoint=endpoint, chunks=5, spec=spec)
    npa = np.linspace(1.4, 4.9, 13, endpoint=endpoint)
    assert_allclose(a, npa)

    a = xp.linspace(0, 0, 0, endpoint=endpoint)
    npa = np.linspace(0, 0, 0, endpoint=endpoint)
    assert_allclose(a, npa)
# ---
def _remove_tpu_lockfile_on_exit_cm():
    try:
        yield
    finally:
        _hacky_remove_tpu_lockfile()
# ---
def asyncio_run(coro):
    try:
        asyncio.get_running_loop()  # Triggers RuntimeError if no running event loop
    except RuntimeError:
        return asyncio.run(coro)
    else:
        # Create a separate thread so we can block before returning
        with ThreadPoolExecutor(1) as pool:
            return pool.submit(lambda: asyncio.run(coro)).result()
# ---
def __init__(self, logger, level=None, handler=None, close=True):
    self.logger = logger
    self.level = level
    self.handler = handler
    self.close = close
# ---
def to_state_dict(self, prefix: Optional[str] = None) -> StateDict:
        w = [self.w1.weight, self.w2.weight, self.w3.weight]
        out = {}

        num_experts = self.w1.Experts.size
        for i in range(num_experts):
            for j in range(3):
                key = f"{prefix}.{i}.w{j + 1}.weight"
                val = w[j]["experts", i].array
                # out[key] = val
                out[key] = jnp.swapaxes(val, -1, -2)

        return out
# ---
def testSmallMaxRange(self):
        self.assertConfigureFails(HPCP(), {'maxFrequency':1199, 'splitFrequency':1000})
# ---
def stop(self) -> None:
        """Signal the thread to stop (but don't wait for it to exit).

        To wait for the thread to exit, call join() separately. This allows
        ThreadContainer to manage multiple thread timeouts globally.
        """
        self._stop_event.set()
        logger.debug("Signaled thread %s to stop", self._thread.name)
# ---
def check_monthnumber_number(monthnum3):
  if(monthnum3==4 or monthnum3==6 or monthnum3==9 or monthnum3==11):
    return True
  else:
    return False
# ---
def filter(self, record: Record) -> bool:
        """Filter structures based on their resolution.

        Parameters
        ----------
        record : Record
            The record to filter.

        Returns
        -------
        bool
            Whether the record should be filtered.

        """
        num_residues = sum(chain.num_residues for chain in record.chains)
        return num_residues <= self.max_residues and num_residues >= self.min_residues
# ---
def i_delete_the_selected_objects():
    bpy.ops.object.delete()
    blenderbim.bim.handler.active_object_callback()
# ---
def get_radio_pair(self):
		dst = self._get_radio()
		src = self._get_radio()

		dst.neighbor = src.addr
		src.neighbor = dst.addr

		return dst, src
# ---
def conceptos(self):
        return self.__conceptos
# ---
class Pair(object): 
	def __init__(self, a, b): 
		self.a = a 
		self.b = b 
def max_chain_length(arr, n): 
	max = 0
	mcl = [1 for i in range(n)] 
	for i in range(1, n): 
		for j in range(0, i): 
			if (arr[i].a > arr[j].b and
				mcl[i] < mcl[j] + 1): 
				mcl[i] = mcl[j] + 1
	for i in range(n): 
		if (max < mcl[i]): 
			max = mcl[i] 
	return max
# ---
def sim_fn(i, j):
            if i == j:
                return 1.0
            key = tuple(sorted((i, j)))
            if key not in pid_cache:
                seq1 = seqs[i]
                seq2 = seqs[j]
                aln = aligner.align(seq1, seq2)[0]
                pid_cache[key] = aln.score / max(len(seqs[i]), len(seqs[j]))
            return pid_cache[key]
# ---
def add_env_value(self, key, value):
        ''' add key, value pair to env array '''
        rval = False
        env = self.get_env_vars()
        if env:
            env.append({'name': key, 'value': value})
            rval = True
        else:
            result = self.put(DeploymentConfig.env_path, {'name': key, 'value': value})
            rval = result[0]

        return rval
# ---
def log_artifact(self, artifact_path, *, name: Optional[str] = None, type: Optional[str] = None):
        del name, type
        logger.warning("Trackio does not currently support artifacts. Skipping upload for %s", artifact_path)
# ---
def toggle_osd(self):
        self.command('osd')
# ---
def pop(self):
        if not self.tokens:
            return None
        self.token = self.tokens.pop()
        return self.token
# ---
def _apertus_rope_scaling(max_position_embeddings: int) -> dict:
    original_max_position_embeddings = max_position_embeddings // 8
    return {
        "rope_type": "llama3",
        "factor": 8.0,
        "original_max_position_embeddings": original_max_position_embeddings,
        "low_freq_factor": 1.0,
        "high_freq_factor": 4.0,
    }
# ---
def tril(x, /, *, k=0) -> "Array":
    from cubed.array_api.searching_functions import where

    if x.ndim < 2:
        raise ValueError("x must be at least 2-dimensional for tril")

    mask = _tri_mask(x.shape[-2], x.shape[-1], k, x.chunks[-2:], x.spec)
    return where(mask, x, zeros_like(x))
# ---
def __getattr__(self, attr):
        if attr not in self.lookup:
            raise AttributeError
        return self.lookup[attr]
# ---
def allowed_mem(self) -> int:
        """
        The total memory available to a worker for running a task, in bytes.

        This includes any ``reserved_mem`` that has been set.
        """
        return self._allowed_mem
# ---
def get_address_state(self, address: bytes) -> AddressState:
        with self.lock:
            return self._state.get_address_state(address)
# ---
import re
def text_match_wordz(text):
        patterns = '\w*z.\w*'
        if re.search(patterns,  text):
                return 'Found a match!'
        else:
                return('Not matched!')
# ---
def test_classify_indices_to_db_no_connection(mock_db, mock_jw):
    mock_db.connected_to_db.return_value = False

    with s.app.test_client() as c:
        resp = c.get('/api/v1/classify_documents/to_database?directory=test')
        assert not mock_jw.called
# ---
def num_cpus(self) -> float:
        return 0.1
# ---
def zeros(shape: AxisSpec, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to 0"""
    if dtype is None:
        dtype = jnp.float32
    return full(shape, 0, dtype)
# ---
def set_level(self,
                  level,
                  console_only=False):
        """
        Defines the logging level (from standard logging module) for log messages.
        :param level:           Level of logging for the file logger.
        :param console_only:    [Optional] If True then the file logger will not be affected.
        """
        self.queue.put(dill.dumps(SetLevelCommand(level=level,
                                                  console_only=console_only)))
# ---
def _update(self, records, value):
        # special case, when an integer field is used as inverse for a one2many
        records._cache[self] = value.id or 0
# ---
def _tokenize_batches(
    *, ctx: JobContext, tokenizer_ref: Any, config: TokenizeConfig | HfTokenizeConfig, batches: Iterator[dict]
) -> Iterator[dict]:
    """Tokenize a list of batches using the specified tokenizer and format."""
    tokenizer: transformers.PreTrainedTokenizer = ctx.get(tokenizer_ref)
    batch_processor = preprocessor_for_format(config.format, tokenizer)

    for batch in batches:
        yield from batch_processor(batch)
# ---
def __getitem__(self, key):
        if isinstance(key, str):
            return super().__getitem__(key)
        return {field: zarray[key] for field, zarray in self.items()}
# ---
def message_data(self):
        if self.proto_view == 0:
            return self.message.plain_bits_str
        elif self.proto_view == 1:
            return self.message.plain_hex_str
        elif self.proto_view == 2:
            return self.message.plain_ascii_str
        else:
            return None
# ---
def bernoulli(key, shape: AxisSpec, p: NamedOrNumeric):
    shape = axis_spec_to_shape_dict(shape)
    p = broadcast_to(p, shape).array
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.bernoulli(key=key, p=p, shape=jax_shape)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def default_get(self, cr, uid, fields_list, context=None):
        # merge defaults from stock.picking with possible defaults defined on stock.picking.out
        defaults = self.pool['stock.picking'].default_get(cr, uid, fields_list, context=context)
        out_defaults = super(stock_picking_out, self).default_get(cr, uid, fields_list, context=context)
        defaults.update(out_defaults)
        return defaults
# ---
def test_contains(self):
        prio_set_list = event._PrioritizedSetList()
        obj = object()

        prio_set_list.add(0, obj)
        assert obj in prio_set_list
# ---
def restore(self):
        if self.ex is not None:
            exc_value = self.ex.with_traceback(self.tb.as_traceback())
            return (self.ex.__class__, exc_value, self.tb.as_traceback())
        else:
            return (Exception, Exception("Process failed with no exception"), self.tb.as_traceback())
# ---
def f(x):
      return x + 1
# ---
def find_lists(Input): 
	if isinstance(Input, list): 
		return 1
	else: 
		return len(Input)
# ---
def clip_max(x_, max_):
            return nxp.clip(x_, max=max_)
# ---
def real_worker(cache_dir, docker_cleanup_scope):
    """Create Worker with real components (not mocks)."""
    config = WorkerConfig(
        port=0,
        cache_dir=cache_dir,
        registry="localhost:5000",
        port_range=(40000, 40100),
        poll_interval=Duration.from_seconds(0.5),  # Faster polling for tests
    )
    return Worker(config)
# ---
def invalidate_cache_for_item(doc):
	invalidate_cache_for(doc, doc.item_group)

	website_item_groups = list(set((doc.get("old_website_item_groups") or [])
								+ [d.item_group for d in doc.get({"doctype": "Website Item Group"}) if d.item_group]))

	for item_group in website_item_groups:
		invalidate_cache_for(doc, item_group)

	if doc.get("old_item_group") and doc.get("old_item_group") != doc.item_group:
		invalidate_cache_for(doc, doc.old_item_group)
# ---
def hard_tanh(a: A) -> A:
    return wrap_elemwise_unary(jnn.hard_tanh, a)
# ---
def dispatch_task(state: ControllerState, task: ControllerTask, worker_id: WorkerId) -> None:
    """Dispatch a task to a worker: assign + mark running."""
    state.handle_event(
        TaskAssignedEvent(
            task_id=task.task_id,
            worker_id=worker_id,
        )
    )
    state.handle_event(
        TaskStateChangedEvent(
            task_id=task.task_id,
            new_state=cluster_pb2.TASK_STATE_RUNNING,
            attempt_id=task.current_attempt_id,
        )
    )
# ---
def max(self):
        return max(self.deque)
# ---
def find_lcm(num1, num2): 
	if(num1>num2): 
		num = num1 
		den = num2 
	else: 
		num = num2 
		den = num1 
	rem = num % den 
	while (rem != 0): 
		num = den 
		den = rem 
		rem = num % den 
	gcd = den 
	lcm = int(int(num1 * num2)/int(gcd)) 
	return lcm 
def get_lcm(l):
  num1 = l[0]
  num2 = l[1]
  lcm = find_lcm(num1, num2)
  for i in range(2, len(l)):
    lcm = find_lcm(lcm, l[i])
  return lcm
# ---
def test_connection_region(self):
        """Connection can access name of connected region"""
        self.assertTrue(isinstance(self.dynamo.region, str))
# ---
def select_none(
        self,
        tokens: np.ndarray,
        random: np.random.Generator,
    ):
        return tokens
# ---
def is_subset(arr1, m, arr2, n): 
	hashset = set() 
	for i in range(0, m): 
		hashset.add(arr1[i]) 
	for i in range(0, n): 
		if arr2[i] in hashset: 
			continue
		else: 
			return False
	return True
# ---
def __init__(self, min_residues: int = 1, max_residues: int = 100000) -> None:
        """Initialize the filter.

        Parameters
        ----------
        min_chains : int
            The minimum number of chains allowed.
        max_chains : int
            The maximum number of chains allowed.

        """
        self.min_residues = min_residues
        self.max_residues = max_residues
# ---
def resiliparse_kwargs(self) -> dict:
        exclude = {"markdownify_config", "prepend_title"}
        return {f.name: getattr(self, f.name) for f in fields(self) if f.name not in exclude}
# ---
def __init__(self, *args, **kwargs): # real signature unknown
        pass
# ---
def test_no_params_option(self):
        stmt = "SELECT '%'" + testing.db.dialect.statement_compiler(
                                    testing.db.dialect, None).default_from()

        conn = testing.db.connect()
        result = conn.\
                execution_options(no_parameters=True).\
                scalar(stmt)
        eq_(result, '%')
# ---
def _schedulable(self, node=None, selector=None, schedulable=True):
        ''' perform oadm manage-node scheduable '''
        cmd = ['manage-node']
        if node:
            cmd.extend(node)
        else:
            cmd.append('--selector={}'.format(selector))

        cmd.append('--schedulable={}'.format(schedulable))

        return self.openshift_cmd(cmd, oadm=True, output=True, output_type='raw')
# ---
def is_local_path(path: PathType):
    """Determine if a path string is for the local filesystem."""
    return urlsplit(str(path)).scheme in ("", "file")
# ---

def is_equal_to_sum_even(n):
    """Evaluate whether the given number n can be written as the sum of exactly 4 positive even numbers
    Example
    is_equal_to_sum_even(4) == False
    is_equal_to_sum_even(6) == False
    is_equal_to_sum_even(8) == True
    """
    return n%2 == 0 and n >= 8
# ---
def server(service):
    """Create WorkerDashboard."""
    return WorkerDashboard(service=service, host="127.0.0.1", port=0)
# ---
def __del__(self):
        """ Cleanup the GPIO before being destroyed """
        if self._MOSI > 0:
            GPIO.cleanup(self._CS_bar)
            GPIO.cleanup(self._CLK)
            GPIO.cleanup(self._MOSI)
            GPIO.cleanup(self._MISO)
# ---
def bar_open(self): pass
# ---
def cumprod(a: NamedArray, axis: AxisSelector, dtype: DTypeLike | None = None) -> NamedArray:
    """
    Named version of [jax.numpy.cumprod](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumprod.html)
    """
    return wrap_axiswise_call(jnp.cumprod, a, axis, dtype=dtype, single_axis_only=True)
# ---
def bitwise_count(a: A) -> A:
    return wrap_elemwise_unary(jnp.bitwise_count, a)
# ---
def withdraw(amount):
    global balance
    balance -= amount
    return balance
# ---
def definition(self):
    """Function definition proto."""
    self._create_definition_if_needed()
    if self._c_func:
      with c_api_util.tf_buffer() as buf:
        c_api.TF_FunctionToFunctionDef(self._c_func.func, buf)
        fdef = function_pb2.FunctionDef()
        proto_data = c_api.TF_GetBuffer(buf)
        fdef.ParseFromString(compat.as_bytes(proto_data))
      return fdef
    return self._definition
# ---
def default_choice_name(cls) -> str | None:
        return "url"
# ---
def allocate(self, num_bytes):
        self.current_mem += num_bytes
        self.peak_mem = max(self.peak_mem, self.current_mem)
# ---
def input_reparam(use_mup: bool = True) -> type[AbstractLinearReparam]:
        """Return the reparameterization class for an input linear layer."""

        return mup.InputLinearMup if use_mup else mup.LinearStandardParam
# ---
def test_pack_and_unpack_simple():
    tree = {"a": np.arange(3, dtype=np.float32), "b": np.arange(4, dtype=np.float32).reshape(2, 2)}
    offsets, packed = eqx.filter_jit(pack_pytree)(tree, dtype=jnp.float32)
    rebuilt = eqx.filter_jit(unpack_pytree)(offsets, packed)
    for orig, new in zip(jax.tree_util.tree_leaves(tree), jax.tree_util.tree_leaves(rebuilt)):
        np.testing.assert_array_equal(np.asarray(orig, dtype=np.float32), np.array(new))
# ---
def isreal(a: A) -> A:
    return wrap_elemwise_unary(jnp.isreal, a)
# ---
def normalized_cross(a, b):
    """
    Returns the normalized cross product between vectors.
    Uses numpy.cross().

    Parameters:
        a: First vector.
        b: Second vector.
    """
    c = np.cross(a, b)
    length = sqrt(c.dot(c))
    return c/length if length > 0 else c
# ---
def test_permutation_is_deterministic(PermutationClass):
    length = 4
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    indices = np.arange(length, dtype=np.uint64)
    results = permutation(indices)
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    results2 = permutation(indices)
    assert jnp.all(results == results2)
# ---
def start(self):
        attributes = json.loads(self.api_job['attributes'].decode('utf-8'))
        vol_name = attributes['volname']
        option = attributes['option_name']
        option_value = attributes['option_value']
        self.atom().start(vol_name, option, option_value)
        self.api_job['status'] = "finished"
        etcd.Client().write(self.api_job['request_id'],
                            json.dumps(self.api_job))
# ---
def list_tasks(
        self,
        _request: cluster_pb2.Worker.ListTasksRequest,
        _ctx: RequestContext,
    ) -> cluster_pb2.Worker.ListTasksResponse:
        """List all tasks on this worker."""
        tasks = self._provider.list_tasks()
        return cluster_pb2.Worker.ListTasksResponse(
            tasks=[task.to_proto() for task in tasks],
        )
# ---
def _safe_remove(path: str):
    try:
        if fsspec_exists(path):
            fsspec_remove(path, recursive=True)
    except Exception:  # noqa: BLE001
        logger.exception(f"Failed to remove temporary cache path {path}")
# ---
def int_to_roman( num):
        val = [1000, 900, 500, 400,100, 90, 50, 40,10, 9, 5, 4,1]
        syb = ["M", "CM", "D", "CD","C", "XC", "L", "XL","X", "IX", "V", "IV","I"]
        roman_num = ''
        i = 0
        while  num > 0:
            for _ in range(num // val[i]):
                roman_num += syb[i]
                num -= val[i]
            i += 1
        return roman_num
# ---
def __iter__(cls):
        yield from cls.members
# ---
def show_serialdlg(self):
        dlg = SerialDialog(self.settings, self)
        dlg.exec_()
# ---
def ip(self):
        if not hasattr(self, "_socket"):
            self._get_address()
        return self._ip
# ---
def write_provenance_json(output_path, metadata: dict[str, Any]) -> None:
    print(f"[*] Writing Dataset `provenance.json` to `{output_path}`")
    metadata["access_time"] = datetime.now(timezone.utc).isoformat()

    with fsspec.open(f"{output_path}/provenance.json", "w") as f:
        json.dump(metadata, f, indent=4, sort_keys=True)
# ---
def __post_init__(self):
        if self.temperature < 1e-4:
            logger.warning(
                "SamplingParams.temperature is very low (%f). Greedy decoding is generally "
                "not useful for RL training as it limits exploration.",
                self.temperature,
            )
        if self.top_k == 1:
            logger.warning("SamplingParams.top_k is 1. Greedy decoding is generally not useful for RL training.")
# ---
def max_length_list(input_list):
    max_length = max(len(x) for x in input_list )   
    max_list = max(input_list, key = lambda i: len(i))    
    return(max_length, max_list)
# ---
def set_iter(self, max_iter):
        self.max_iter = max_iter
# ---
from collections import Counter
def count_variable(a,b,c,d):
  c = Counter(p=a, q=b, r=c, s=d)
  return list(c.elements())
# ---
def test_connection_available_accepts_duration_timeout():
    """connection_available works with Duration timeout (regression for TypeError)."""
    conn = FakeSshConnection()
    assert connection_available(conn, timeout=Duration.from_seconds(5)) is True
    assert conn.last_timeout == Duration.from_seconds(5)
# ---
def remove_matching_tuple(test_list1, test_list2):
  res = [sub for sub in test_list1 if sub not in test_list2]
  return (res)
# ---
def build_test_opener(*handler_instances):
    opener = OpenerDirector()
    for h in handler_instances:
        opener.add_handler(h)
    return opener
# ---
def __len__(self):
		return len(self._addresses)
# ---
def __init__(self, uri, vminfo, job_id, irs):
        super(XenCommand, self).__init__(vminfo, job_id, irs)
        self._uri = uri
        self._ssh_agent = SSHAgent()
# ---
def __call__(
        self,
        mel: NamedArray,
        input_ids: NamedArray,
        attn_mask: Optional[AttentionMask | NamedArray] = None,
        *,
        key=None,
    ) -> NamedArray:
        pass
# ---
def zeros_like_tree(tree: T, axis_mapping: Optional[ResourceMapping] = None, dtype: Optional[jnp.dtype] = None) -> T:
    """
    Creates a tree of zeros with the same structure as the input tree. If the input tree contains NamedArrays, then
    those will be sharded according to the axis_mapping (or the context axis mapping if not provided).
    """
    _zeros = functools.partial(_zeros_like, axis_mapping, dtype)
    acc = jax.tree_util.tree_map(_zeros, tree, is_leaf=is_named_array)
    return acc
# ---
def test_unnormalize_prognostic_tensor(normalize_input, fill_value):
    normalize, wet_mask = normalize_input
    data = torch.randn([1, normalize._prognostic_std_np.shape[0], *wet_mask.shape])
    input_data = data * wet_mask
    normalized = normalize.normalize_tensor_prognostic(input_data)
    unnormalized = normalize.unnormalize_tensor_prognostic(normalized, fill_value)
    assert (torch.sum(torch.isnan(unnormalized)) > 0) == (math.isnan(fill_value))
# ---
def materialize_mask(
    mask: Optional[NamedArray | AttentionMask],
    QPos: Axis,
    KPos: Axis,
    q_slice: Optional[haliax.dslice] = None,
    k_slice: Optional[haliax.dslice] = None,
) -> Optional[NamedArray]: ...
# ---
def __init__(self, l):
                self.l = l
# ---
def parse_slurm_args(args_list: list[list[str]]) -> dict[str, str]:
    slurm_args = DEFAULT_SLURM_ARGS.copy()
    if not args_list:
        return slurm_args
    for item in args_list:
        if len(item) != 2:
            logger.error(f"Invalid SLURM argument format: {item}. Expected 'KEY VALUE' format.")
            sys.exit(1)
        key, value = item
        slurm_args[key] = value
    return slurm_args
# ---
def do_vmap(*args, **kwargs) -> OutputT_co:
            # Create a function that captures the additional arguments
            def do_block_with_args(block: M, *args, **kwargs) -> OutputT_co:
                return fn(block, *args, **kwargs)

            return haliax.vmap(do_block_with_args, self.Block)(self.stacked, *args, **kwargs)
# ---
def testAssignSubscript(self):
    self.assertEqual((0, "{'bar': None}\n"), _GrumpRun(textwrap.dedent("""\
        foo = {}
        foo['bar'] = None
        print foo""")))
# ---
def key_function(out_key):
        out_coords = out_key[1:]
        in_name = array_names[out_coords[axis]]
        return ((in_name, *(out_coords[:axis] + out_coords[(axis + 1) :])),)
# ---
def clean_title(title):
    """Clean up the title by removing 'Experiment:' prefix."""
    if title.startswith("Experiment:"):
        return title[len("Experiment:") :].strip()
    return title
# ---
def execute(
                self,
                conn,
                execute,
                clauseelement,
                *multiparams,
                **params
                ):
                stmts.append((str(clauseelement), params, multiparams))
                return execute(clauseelement, *multiparams, **params)
# ---
def read(self, bufsiz, flags=0):
        return self.recv(bufsiz, flags)
# ---
def remove_all_iris_containers(self) -> int:
        """Force remove all iris-managed containers. Returns count attempted."""
        container_ids = self.list_iris_containers(all_states=True)
        if not container_ids:
            return 0

        subprocess.run(
            ["docker", "rm", "-f", *container_ids],
            capture_output=True,
            check=False,
        )
        return len(container_ids)
# ---
def radians(a: A) -> A:
    return wrap_elemwise_unary(jnp.radians, a)
# ---
def test_terminate_job_not_found(service):
    """Verify terminate_job raises ConnectError for unknown job."""
    request = cluster_pb2.Controller.TerminateJobRequest(job_id=JobName.root("nonexistent").to_wire())

    with pytest.raises(ConnectError) as exc_info:
        service.terminate_job(request, None)

    assert exc_info.value.code == Code.NOT_FOUND
    assert "nonexistent" in exc_info.value.message
# ---
def __bool__(self):
        if self.transport:
            return True
        return False
# ---
def largest_pos(list1): 
    max = list1[0] 
    for x in list1: 
        if x > max : 
             max = x  
    return max
# ---
def open_shard_at_row(self, shard_name: str, row: int) -> Iterator[list[int]]:
            raise RuntimeError("This should not be called")
# ---
def count_Intgral_Points(x1,y1,x2,y2): 
    return ((y2 - y1 - 1) * (x2 - x1 - 1))
# ---
def add_K_element(test_list, K):
  res = [tuple(j + K for j in sub ) for sub in test_list]
  return (res)
# ---
def _patch_up_reduce_fn(reduce_fn):
    if reduce_fn is haliax.max:
        reduce_fn = jax.lax.max
    elif reduce_fn is haliax.min:
        reduce_fn = jax.lax.min
    elif reduce_fn is haliax.sum:
        reduce_fn = jax.lax.add
    elif reduce_fn is haliax.prod:
        reduce_fn = jax.lax.mul

    return reduce_fn
# ---
def make_token_dataset(cache: TreeCache[dict], *, seq_len: int) -> TokenSeqDataset:
    """Thin wrapper so callers don't touch Levanter internals directly."""

    return TokenSeqDataset(cache, seq_len)
# ---
def withdraw(account, amount):
    account['balance'] -= amount
    return account['balance']
# ---
def __init__(self):
        self.lexer = lex.lex(module=self)
# ---
def get_lm_head(self) -> NamedArray:
        return hax.named(self.lm_head_fn(self.params), (self.Embed, self.Vocab))
# ---
def test_latex(self):
        body, mime_type = self._run_handler(
            EXPERIMENT, SESSION_GROUPS, download_data.OutputFormat.LATEX
        )
        self.assertEqual("application/x-latex", mime_type)
        self.assertEqual(EXPECTED_LATEX, body)
# ---
def lang(self, val):
        self._lang = _EpubLang(val)
# ---
def test_list_tasks(worker):
    """Test listing all tasks."""
    requests = [create_run_task_request(task_id=JobName.root("test-job").task(i).to_wire()) for i in range(3)]

    for request in requests:
        worker.submit_task(request)

    tasks = worker.list_tasks()
    assert len(tasks) == 3
# ---
def _list_to_dict(list_, key_label='key'):
    """
    Transforms a list of dictionaries into a dictionary of dictionaries.

    Original dictionaries are assigned key specified in each of them
    by key_label.
    """
    dict_ = {}
    for item in list_:
        dict_[item[key_label]] = item
    return dict_
# ---
def test_ckpt_path_invalid_input_type():
    with pytest.raises(ValueError, match="Unknown type"):
        ckpt_path_to_step_name(12345)
# ---
def min_Swaps(str1,str2) : 
    count = 0
    for i in range(len(str1)) :  
        if str1[i] != str2[i] : 
            count += 1
    if count % 2 == 0 : 
        return (count // 2) 
    else : 
        return ("Not Possible")
# ---
def _get_num_train_steps(param_count: int, batch_size: int, max_seq_len: int, tpp: int = 20) -> int:
    total_tokens = param_count * tpp
    return max(1, total_tokens // (batch_size * max_seq_len))
# ---
def test_lambda_output_shape_autocalculate_multiple_inputs(self):

    def lambda_fn(x):
      return math_ops.matmul(x[0], x[1])

    l = keras.layers.Lambda(lambda_fn)
    output_shape = l.compute_output_shape([(10, 10), (10, 20)])
    self.assertAllEqual((10, 20), output_shape)
# ---
def should_apply_ema(self, step: int) -> bool:
        """Check if EMA should be applied at the current step.

        Parameters
        ----------
        step: int
            The current global step.

        Returns
        -------
        bool
            True if EMA should be applied, False otherwise.

        """
        return (
            step != self._cur_step
            and step >= self.start_step
            and step % self.apply_ema_every_n_steps == 0
        )
# ---
def to_named_array(self):
        return NamedArray(self.array, self.main_axes)
# ---
def content_type(self):
        return self.headers.get("Content-Type", DEFAULT_HTTP_CONTENT_TYPE)
# ---
def get_gen_shape(gen_data: dict[str, torch.Tensor]):
    for name in gen_data:
        return gen_data[name].shape
# ---
def crt(congruences):
        x = 0
        M = 1
        for i in xrange(len(congruences)):
                M *= congruences[i][2]
                congruences[i] = reduceCongr(congruences[i][0], congruences[i][1], congruences[i][2])

        for j in xrange(len(congruences)):
                m = congruences[j][2]
                if gcd(m, M/m) != 1:
                        return None
                x += congruences[j][1] * modInverse(M/m, m) * M / m

        return x % M
# ---
def as_sync_dataset(self) -> "SyncDataset[T_co]":
        return self
# ---
def fail():
                raise ValueError("intentional error")
# ---
def drag_and_drop_by_offset(self, source, xoffset, yoffset):
        """Holds down the left mouse button on the source element,
           then moves to the target element and releases the mouse button.
        Args:
            source: The element to mouse down.
            xoffset: X offset to move to.
            yoffset: Y offset to move to.
        """
        self.click_and_hold(source)
        self.move_by_offset(xoffset, yoffset)
        self.release(source)
        return self
# ---
def init_fn(params):
        return {"count": jnp.array(0, dtype=jnp.int32)}
# ---
def tag(self, client, tags, images, **kwds):
        if tags is None:
            tags = ['latest']
        for image in images:
            self.getImage(image).tag(client, tags, **kwds)
# ---
def paragraph(self, text):
        """Rendering paragraph tags. Like ``<p>``."""
        return '<p>%s</p>\n' % text.strip(' ')
# ---
def print_nans(tensor, name):
  if torch.isnan(tensor).any():
    print(name, tensor)
# ---
def test_find_span_end_expression():
    source = "x = 1 + 2\n"
    # The BinOp "1 + 2" starts at offset 4.
    end = _find_span_end(source, 4)
    assert end is not None
    assert source[4:end] == "1 + 2"
# ---
def bitwise_xor(test_tup1, test_tup2):
  res = tuple(ele1 ^ ele2 for ele1, ele2 in zip(test_tup1, test_tup2))
  return (res)
# ---
def populateComboBox(self, combobox, items):
        """Populates the combobox with the items provided.

        Arguments:
        - combobox: the GtkComboBox to populate
        - items: the list of strings with which to populate it
        """

        model = Gtk.ListStore(str)
        for item in items:
            model.append([item])
        combobox.set_model(model)
# ---
import math 
def get_Pos_Of_Right_most_Set_Bit(n): 
    return int(math.log2(n&-n)+1)   
def set_Right_most_Unset_Bit(n): 
    if (n == 0): 
        return 1
    if ((n & (n + 1)) == 0):     
        return n 
    pos = get_Pos_Of_Right_most_Set_Bit(~n)      
    return ((1 << (pos - 1)) | n)
# ---
def log(self, metrics: typing.Mapping[str, Any], *, step, commit=None):
        for tracker in self.loggers:
            tracker.log(metrics, step=step, commit=commit)
# ---
def compute_cats_reward(response: str) -> float:
    """Compute reward for cat-themed responses using MoarCatsTask logic."""
    num_cats = response.lower().count("cat")
    love_cats = response.lower().count("love cats")
    return (num_cats + (10 * love_cats)) / (1 + len(response))
# ---
def test_get_lines_with_long_text_string():
        text = "This is a test string, which should simulate real text. The command should" \
         + " correctly split this text into two lines."
        LINEWIDTH = 80
        correct_lines = [text[:LINEWIDTH], text[LINEWIDTH:]]
        assert len(get_lines(text)) == len(text) // LINEWIDTH + 1
        assert get_lines(text) == correct_lines
# ---
def _materialize_sliding_window_mask(
    window: int, QPos: Axis, KPos: Axis, q_slice: haliax.dslice, k_slice: haliax.dslice
) -> NamedArray:
    """Materialize a causal sliding window mask."""
    sub_q = QPos.resize(q_slice.size)
    sub_k = KPos.resize(k_slice.size)
    q_pos = hax.arange(sub_q) + q_slice.start
    k_pos = hax.arange(sub_k) + k_slice.start
    diff = q_pos.broadcast_axis(sub_k) - k_pos.broadcast_axis(sub_q)
    return (diff >= 0) & (diff < window)
# ---
def _construct_direct(cls, variables, coord_names, dims, attrs,
                          file_obj=None):
        """Shortcut around __init__ for internal use when we want to skip
        costly validation
        """
        obj = object.__new__(cls)
        obj._variables = variables
        obj._coord_names = coord_names
        obj._dims = dims
        obj._attrs = attrs
        obj._file_obj = file_obj
        return obj
# ---
def test_convert_to_bytes(input_value, expected_value):
    spec = cubed.Spec(allowed_mem=input_value)
    assert spec.allowed_mem == expected_value
# ---
def set_middle_bits(n):  
    n |= n >> 1; 
    n |= n >> 2; 
    n |= n >> 4; 
    n |= n >> 8; 
    n |= n >> 16;  
    return (n >> 1) ^ 1
def toggle_middle_bits(n): 
    if (n == 1): 
        return 1
    return n ^ set_middle_bits(n)
# ---
def clear(self):
        self._items.clear()
        self.model.clear()
# ---
def author(self, val):
        if isinstance(val, Author) or isinstance(val, str):
            authors = [val]
        else:
            authors = val
        for aut in authors:
            try:
                self._authors.append(Author('' + aut))
            except TypeError:
                # aut is not a string, so it should be an Author object
                self._authors.append(aut)
# ---
def _around_frequency(self, frequency):
        # return ceil(frequency/self._calibration_step)*self._calibration_step
        return round(frequency / self._calibration_step) * self._calibration_step
# ---
def test_scalar_updated_slice():
    # Base case: scalar start on a 1D array
    Seq = hax.Axis("seq", 5)
    arr = hax.arange((Seq,), dtype=int)
    # replace positions 2 and 3 with [100, 101]
    upd = hax.named([100, 101], "seq")

    result = updated_slice(arr, {"seq": 2}, upd)
    # expect [0,1,100,101,4]
    assert np.array_equal(result.array, np.array([0, 1, 100, 101, 4]))
# ---
def top_level_startup_industry(self):
        industry = (
            self.startup.primary_industry if self._get_startup() else None)
        return industry.parent if industry and industry.parent else industry
# ---
def get_task(self, task_id: str) -> TaskInfo | None:
        """Get a task by ID.

        Returns TaskInfo view (implemented by TaskAttempt) to decouple callers
        from execution internals.
        """
        return self._tasks.get(task_id)
# ---
def test_tensordot(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    c = xp.astype(a, xp.float32)
    d = xp.astype(b, xp.float32)
    e = xp.tensordot(c, d, axes=1)
    run_operation(tmp_path, executor, "tensordot", e)
# ---
def vmap_via(self, fn: Callable[..., OutputT_co]) -> Callable[..., OutputT_co]: ...
# ---
def find_Volume(l,b,h) : 
    return ((l * b * h) / 2)
# ---
def clean_segundo_numero(self):
        cleaned_data = self.cleaned_data

        telefone = Telefone()
        telefone.tipo = self.data['segundo_tipo']
        telefone.ddd = self.data['segundo_ddd']
        telefone.numero = self.data['segundo_numero']
        telefone.principal = self.data['segundo_principal']

        cleaned_data['segundo_telefone'] = telefone
        return cleaned_data
# ---
def last_two_digits(year):
    return year - ((year // 100) * 100)
# ---
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
# ---
def test_stdin_restored(self):
        old = sys.stdin
        with self.getcapture(in_=True):
            newstdin = sys.stdin
        assert newstdin != sys.stdin
        assert sys.stdin is old
# ---
def load_state_dict(path):
    """
    Load a model's state dict from a file, bringing all tensors to the CPU first and then converting to numpy.
    This will load using safetensors format
    """
    if safetensors_numpy is None:
        raise ImportError("safetensors_numpy is not installed")
    state_dict = safetensors_numpy.load_file(path)
    return state_dict
# ---
def fn_pass(config: MyConfig | None):
        append_log(log, config)
# ---
def records():
        batch = []
        pbar = tqdm_logging(desc=f"Shard {shard_name}")
        for example in source.open_shard_at_row(shard_name, 0):
            batch.append(example)
            if len(batch) >= options.batch_size:
                processed = processor(batch)
                yield from _canonicalize_batch(processed)
                batch.clear()
            pbar.update(1)
        if batch:
            processed = processor(batch)
            yield from _canonicalize_batch(processed)
# ---
def saveable_state(self) -> FilterTree:
        return eqx.filter(self, saveable_training_mask(self, self.is_trainable))
# ---
def __init__(self, cache: TreeCache[AudioTextDict]):
        super().__init__()
        self.cache = cache
# ---
def init_outdir(self, design_dir):
        self.outdir = Path(design_dir) / const.affinity_dirname
        self.outdir.mkdir(exist_ok=True, parents=True)
        self.failed = 0
# ---
def get_item(tup1,index):
  item = tup1[index]
  return item
# ---
def offset_to_block_id(offset: int, numblocks: Tuple[int, ...]) -> Tuple[int, ...]:
    """Convert an index offset to a block ID (chunk coordinates)."""
    return tuple(int(i) for i in np.unravel_index(offset, numblocks))
# ---
def get_updated_line_contents(updates=None):
    test_contents = copy.deepcopy(test_line_contents)
    if updates is not None:
        test_contents.update(updates)
    return test_contents
# ---
def unique(
    array: NamedArray,
    Unique: Axis,
    *,
    return_index: bool = False,
    return_inverse: bool = False,
    return_counts: bool = False,
    axis: AxisSelector | None = None,
    fill_value: ArrayLike | None = None,
) -> NamedArray | tuple[NamedArray, ...]: ...
# ---
def test_get_full_url(self):
        self.assertEqual("http://www.python.org/~jeremy/",
                         self.get.get_full_url())
# ---
def finish(self):
        excs = []
        for tracker in self.loggers:
            try:
                tracker.finish()
            except Exception as e:
                excs.append(e)

        if excs:
            raise RuntimeError("Errors occurred when finishing trackers") from excs[0]
# ---

def x_or_y(n, x, y):
    """A simple program which should return the value of x if n is 
    a prime number and should return the value of y otherwise.

    Examples:
    for x_or_y(7, 34, 12) == 34
    for x_or_y(15, 8, 5) == 5
    
    """
    if n == 1:
        return y
    for i in range(2, n):
        if n % i == 0:
            return y
            break
    else:
        return x
# ---
def testMaxShifted(self):
        # Tests whether a HPCP reading with only the dominant semitone
        # activated is correctly shifted so that the dominant is at the
        # position 0
        tonic = 440
        dominant = tonic * 2**(7./12.)
        hpcp = HPCP(maxShifted=True)([dominant], [1])

        self.assertEqualVector(hpcp, [1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])
# ---
def init_scale(In: AxisSpec, Out: AxisSpec):
        return 1 / math.sqrt(hax.axis_size(In))
# ---
def __init__(self, text: str):
        self.text = text
# ---
def wait_for_ready(expression: str, label: str) -> None:
            try:
                page.wait_for_function(expression, timeout=30000)
            except PlaywrightTimeoutError:
                logger.warning("Timed out waiting for %s; capturing anyway", label)
# ---
def _cached_load_tokenizer(tokenizer_name: str):
    return load_tokenizer(tokenizer_name)
# ---
def editor_popup(title, content, answerCallback):
    content = EditorPopup(content=content)
    content.bind(on_answer=answerCallback)
    popup = Popup(title=title,
                    content=content,
                    size_hint=(0.7, 0.8),
                    auto_dismiss= False,
                  title_size=sp(18))
    popup.open()
    return popup
# ---
def open(exemplar: T, path: str, *, mode="a", cache_metadata: bool = False) -> "TreeStore":
        """
        Open a TreeStoreBuilder from a file.
        """
        tree = _construct_builder_tree(exemplar, path, mode, cache_metadata)
        return TreeStore(tree, path, mode)
# ---
def checkpoint_exists(repo_id: str, step: int, version_name: str) -> bool:
    """Check if a specific revision exists in a Hugging Face repository."""
    try:
        api = HfApi()
        commits = api.list_repo_commits(repo_id=repo_id)
        for commit in commits:
            if f"step {step}" in commit.title:
                return True
        return False
    except Exception:
        return False
# ---
def rearrange(array: NamedArray, axes: Sequence[AxisSelector | EllipsisType]) -> NamedArray:
    pass
# ---
def push(self, batch: RolloutBatch) -> None:
        """Push batch to queue, blocking if full (when bounded)."""
        with self._not_full:
            if self._maxlen is not None:
                while len(self._queue) >= self._maxlen:
                    self._not_full.wait()
            self._queue.append(batch)
            self._not_empty.notify()
# ---
def gcd(p,q):
    while q != 0:
        p, q = q,p%q
    return p
def is_coprime(x,y):
    return gcd(x,y) == 1
# ---
def bias_init_one_(bias):
    with torch.no_grad():
        bias.fill_(1.0)
# ---
def __init__(self,
                 filename=None,
                 content=None,
                 content_type='yaml',
                 separator='.',
                 backup=False):
        self.content = content
        self._separator = separator
        self.filename = filename
        self.__yaml_dict = content
        self.content_type = content_type
        self.backup = backup
        self.load(content_type=self.content_type)
        if self.__yaml_dict is None:
            self.__yaml_dict = {}
# ---
def zip_browsers_logs():
    if platform.system() == "Windows":
        cmd = "Compress-Archive browserlogs browserlogs-$(date +%Y-%m-%d-%H%M).zip"
        subprocess.call(["powershell.exe", cmd])

    elif platform.system() == "Linux" or platform.system() == "Darwin":
        cmd = "zip -vr browserlogs-$(date +%Y-%m-%d-%H%M).zip" + " browserlogs/"
        cr.run_command(cmd)
# ---
def pair_OR_Sum(arr,n) : 
    ans = 0 
    for i in range(0,n) :    
        for j in range(i + 1,n) :   
            ans = ans + (arr[i] ^ arr[j])          
    return ans
# ---
def _convert_disk_format(format):
    # TODO: move to volume format when storage/volume.py
    #       will be accessible for /lib/vdsm/v2v.py
    if format == 'qcow2':
        return 'COW'
    elif format == 'raw':
        return 'RAW'
    raise KeyError
# ---
def controller_metadata_key(label_prefix: str) -> str:
    """Metadata key to mark a VM as the controller for a given prefix."""
    return f"iris-controller-{label_prefix}"
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})
            return df.A.describe()
# ---
def geometric_sum(n):
  if n < 0:
    return 0
  else:
    return 1 / (pow(2, n)) + geometric_sum(n - 1)
# ---
def __getitem__(self,a):
        return self.arguments[a]
# ---
def _get_member(self):
        if not self.team_member:
            self.team_member = self.startupteammember_set.last()
        return self.team_member
# ---
def remove_elements(list1, list2):
    result = [x for x in list1 if x not in list2]
    return result
# ---
def load_workers_from_config(self, configs: list[WorkerConfig]) -> None:
        """Load workers from static configuration."""
        now = Timestamp.now()
        for cfg in configs:
            worker = ControllerWorker(
                worker_id=WorkerId(cfg.worker_id),
                address=cfg.address,
                metadata=cfg.metadata,
                last_heartbeat=now,
            )
            self.add_worker(worker)
# ---
def admin_url(self):
        return get_absolute_saagie_url('/#/manager/%s/job/%s'
                                       % (self.platform_id, self.id))
# ---
def __init__(self, start, end):
        if start is not None and end is not None and not start < end:
            raise ValueError("start date {} is not before end date {}"
                             .format(start, end))
        self.start = start
        self.end = end
# ---
def __init__(self, Vocab: hax.AxisSelector = "vocab"):
        self.Vocab = Vocab
# ---
def get(self):
        raise self.exception
# ---
def build(
        self, optimizer: torch.optim.Optimizer, epochs: int
    ) -> torch.optim.lr_scheduler.LRScheduler:
        max_epochs = self.target_epochs if self.target_epochs is not None else epochs
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)
# ---
def __init__(self, uri, username, password, vminfo, vmid, irs):
        super(KVMCommand, self).__init__(vminfo, vmid, irs)
        self._uri = uri
        self._username = username
        self._password = password
# ---
def __hash__(self):
        return hash((self.pattern, tuple(self.fields), self.query_class))
# ---
def convert_to_read(self, value, use_name_get=True):
        # Integer values greater than 2^31-1 are not supported in pure XMLRPC,
        # so we have to pass them as floats :-(
        if value and value > xmlrpclib.MAXINT:
            return float(value)
        return value
# ---
def main(ad_exchange_buyer, owner_name, body, is_transient):
  try:
    # Construct and execute the request.
    filter_set = ad_exchange_buyer.bidders().filterSets().create(
        ownerName=owner_name, isTransient=is_transient, body=body).execute()
    print(f'FilterSet created for bidder: "{owner_name}".')
    pprint.pprint(filter_set)
  except HttpError as e:
    print(e)
# ---
def ipa_point_weights_init_(weights):
    with torch.no_grad():
        softplus_inverse_1 = 0.541324854612918
        weights.fill_(softplus_inverse_1)
# ---
def salinity_global(self, data: xr.Dataset) -> xr.DataArray:
        rho_0 = 1025  # kg/m^3
        salinity = ((data["so"] * rho_0) * data["areacello"] * data["dz"]).sum(
            ["x", "y", "lev"]
        )
        salinity = salinity.rename("Salinity")
        salinity = salinity.assign_attrs(units="g")
        return salinity
# ---
def download_artifact(artifact: "wandb.sdk.Artifact", root: Optional[Path]) -> Path:
    if root is None:
        root = Path(tempfile.mkdtemp(prefix="wandb-profile-"))
    else:
        root.mkdir(parents=True, exist_ok=True)
    download_path = Path(artifact.download(root=str(root)))
    return download_path
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            df.A.fillna(5.0, inplace=True)
            return df.A.sum()
# ---
def testFunctionDef(self):
    self.assertEqual((0, 'bar baz\n'), _GrumpRun(textwrap.dedent("""\
        def foo(a, b):
          print a, b
        foo('bar', 'baz')""")))
# ---
def evaluate(self, record: dict) -> bool:
        return self.child.evaluate(record) is None
# ---
def post_backlog_processing(self, context):
        pass
# ---
def spacesCount(text):
            return len(text) - len(text.rstrip(' '))
# ---
def encode_dtype(d):
    if d.fields is None:
        return d.str
    else:
        return d.descr
# ---
def move_last(num_list):
    a = [num_list[0] for i in range(num_list.count(num_list[0]))]
    x = [ i for i in num_list if i != num_list[0]]
    x.extend(a)
    return (x)
# ---


def below_threshold(l: list, t: int):
    """Return True if all numbers in the list l are below threshold t.
    >>> below_threshold([1, 2, 4, 10], 100)
    True
    >>> below_threshold([1, 20, 4, 10], 5)
    False
    """
    for e in l:
        if e >= t:
            return False
    return True
# ---
def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):
        super(LW_MaxPooling3D, self).__init__(pool_size, strides, border_mode, dim_ordering)
# ---
def make_upcase(line):
                    return line.strip().upper()
# ---
def with_segment_ids(
        self, q_segment_ids: Int[Array, "..."], kv_segment_ids: Int[Array, "..."] | None = None
    ) -> "AttentionMask":
        kv_ids = q_segment_ids if kv_segment_ids is None else kv_segment_ids
        return AttentionMask(
            is_causal=self.is_causal,
            segment_ids=(q_segment_ids, kv_ids),
            sliding_window=self.sliding_window,
        )
# ---
def __init__(self, axes: Sequence[Axis]):
        self.axes = tuple(axes)
# ---
def getBackend(name):
        if not name in ObjectBackendRegistry.backends:
            raise ValueError(C.make_error("BACKEND_NOT_FOUND", name))

        return ObjectBackendRegistry.backends[name]
# ---
def max_stop_seq_len(self) -> int:
        """Maximum number of stop sequences for each sequence."""
        if self.stop_tokens is None:
            return 0
        return self.stop_tokens.axis_size("position")
# ---
def upload_face_image(self, img):
        raise NotImplementedError
# ---
from collections import defaultdict
def max_occurrences(nums):
    dict = defaultdict(int)
    for i in nums:
        dict[i] += 1
    result = max(dict.items(), key=lambda x: x[1]) 
    return result
# ---
def test_augment_bank_no_duplicates(bank, rng):
    augmented = augment_bank(bank, rng, n_renamed=2, n_perturbed=2, synthetic_count=20)
    for node_type, entries in augmented.entries.items():
        sources = [e.source for e in entries]
        assert len(sources) == len(set(sources)), f"Duplicates in {node_type}"
# ---
import cmath
def len_complex(a,b):
  cn=complex(a,b)
  length=abs(cn)
  return length
# ---
def __init__(self):
        self.metrics: dict[str, Any] = {}
# ---
def __str__(self):
        return self.get_url()
# ---
def serialize_keras_class_and_config(cls_name, cls_config):
  """Returns the serialization of the class with the given config."""
  return {'class_name': cls_name, 'config': cls_config}
# ---
def _compute_unpermuted_ids(self, counts_per_block):
        unpermuted_ids = np.zeros(int(counts_per_block.sum()), dtype=np.int64)
        start = 0
        for i, dsname in enumerate(self.dataset_index):
            count = counts_per_block[i]
            unpermuted_ids[start : start + count] = (i << 16) + np.arange(count)
            start += count
        return unpermuted_ids
# ---
def __init__(self, path: str):
        self._path = path
# ---
def test_var(spec, axis, correction, keepdims):
    a = xp.asarray(
        [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], chunks=(2, 2), spec=spec
    )
    b = xp.var(a, axis=axis, correction=correction, keepdims=keepdims)
    assert_array_equal(
        b.compute(optimize_graph=False),
        np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]).var(
            axis=axis, ddof=correction, keepdims=keepdims
        ),
    )
# ---
def test_column_std(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = 4.0
            df = pd.DataFrame({'A': A})
            return df.A.std()

        hpat_func = self.jit(test_impl)
        np.testing.assert_almost_equal(hpat_func(), test_impl())
# ---
def find_free_port() -> int:
    """Find an available port."""
    with socket.socket() as s:
        s.bind(("", 0))
        return s.getsockname()[1]
# ---
def hf_config_from_hf_checkpoint(self, ref: Optional[Union[str, RepoRef]] = None) -> HfConfig:
        path, rev = self._get_ref(ref)

        with _patch_hf_hub_download():
            config = AutoConfig.from_pretrained(path, revision=rev, trust_remote_code=self.trust_remote_code)
        return config
# ---
def zone(self) -> str:
        return ""
# ---
def is_param(value):
    """Determine whether given value is a parameter string."""
    if not isinstance(value, str):
        return False
    return RE_PARAM.match(value)
# ---
def shutdown(self) -> None:
        """Kill all Ray actors."""
        for handle in self._handles:
            try:
                ray.kill(handle._actor_ref)
            except Exception as e:
                logger.warning("Failed to kill Ray actor: %s", e)
# ---
def step_impl(context):
    # Import request
    context.browser.find_element_by_xpath("//*[@id='ImportRequestForm']//input[@value='Import']").click()
# ---
def disable_wandb(monkeypatch):
    """Disable WANDB logging during tests."""
    monkeypatch.setenv("WANDB_MODE", "disabled")
# ---
def __call__(self, target, creds, enforcer):
        """Check http: rules by calling to a remote server.

        This example implementation simply verifies that the response
        is exactly 'True'.
        """

        url = ('http:' + self.match) % target
        data = {'target': jsonutils.dumps(target),
                'credentials': jsonutils.dumps(creds)}
        post_data = urllib.urlencode(data)
        f = urllib2.urlopen(url, post_data)
        return f.read() == "True"
# ---
def health_check(self, request: cluster__pb2.Empty, ctx: RequestContext) -> cluster__pb2.Worker.HealthResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def __init__(self):
        self.level = 0
        self.req_headers = []
        self.data = None
        self.raise_on_endheaders = False
        self._tunnel_headers = {}
# ---
def root(self):
        return self.__root
# ---
def pytest_addoption(parser):
    """Add CLI options related to Testimony token based mark collection"""
    parser.addoption(
        '--importance',
        help='Comma separated list of importance levels to include in test collection',
    )
    parser.addoption(
        '--component',
        help='Comma separated list of component names to include in test collection',
    )
    parser.addoption(
        '--assignee',
        help='Comma separated list of assignees to include in test collection',
    )
# ---
def from_string(s: str) -> "RepoRef":
        if "@" not in s:
            return RepoRef(s)
        model_name_or_path, revision = s.split("@")
        return RepoRef(model_name_or_path, revision)
# ---
def open_shard_at_row(self, shard_name: str, row: int):
        return self.docs[row:]
# ---
def _load_data(self, directory):
        try:
            with open(directory, 'r') as f:
                data = f.read()
                return literal_eval(data)
        except OSError as e:
            raise
# ---
def atan2(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "atan2")
    if x1.dtype not in _real_floating_dtypes or x2.dtype not in _real_floating_dtypes:
        raise TypeError("Only real floating-point dtypes are allowed in atan2")
    return elemwise(nxp.atan2, x1, x2, dtype=result_type(x1, x2))
# ---
import re
def camel_to_snake(text):
  str1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
  return re.sub('([a-z0-9])([A-Z])', r'\1_\2', str1).lower()
# ---
def the_object_name_exists(name):
    obj = bpy.data.objects.get(name)
    if not obj:
        assert False, f'The object "{name}" does not exist'
    return obj
# ---
def key_iterator(key: PRNGKeyArray | int):
    if isinstance(key, int):
        key = jax.random.PRNGKey(key)
    while True:
        key, subkey = jax.random.split(key)
        yield subkey
# ---
def decorator(request, *args, **kwargs):
            if not request.user.has_perm(self.perm):
                raise PermissionRequired(self.perm)
            return view_func(request, *args, **kwargs)
# ---
def log_summary(self, metrics: dict[str, Any]):
        for tracker in self.loggers:
            tracker.log_summary(metrics)
# ---
def fn(input_ids, mask):
        return MistralLMHeadModel.init(Vocab=Vocab, config=mistral_config, key=random.PRNGKey(0))(input_ids, mask)
# ---
def forward(self, x, c):
    return self.linear(self.norm_final(x))
# ---
def initial_cache(self, spec: PageTableSpec, *, dtype) -> ListCache[KvPageCache]:
        return hax.auto_sharded(self.transformer.initial_cache(spec, dtype=dtype))
# ---
def does_Contain_B(a,b,c): 
    if (a == b): 
        return True
    if ((b - a) * c > 0 and (b - a) % c == 0): 
        return True
    return False
# ---
def define_tables(cls, metadata):
        Table(
            "stuff",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("data", String(50)),
        )
# ---
def checksum(sscc):
        salt = '31' * 8 + '3'
        sum = 0
        for sscc_part, salt_part in zip(sscc, salt):
            sum += int(sscc_part) * int(salt_part)
        return (10 - (sum % 10)) % 10
# ---
def get_match(self):
        match = Forseti.Match()
        self._set_match_panel(match, 0, 0)
        self._set_match_panel(match, 1, 2)
        self._set_match_panel(match, 2, 1)
        self._set_match_panel(match, 3, 3)
        try:
            match.match_number = int(self.match_num_ctrl.GetValue())
        except ValueError:
            match.match_number = random.getrandbits(31)
        return match
# ---
def dot(
    *arrays: NamedArray,
    axis: AxisSelection | None,
    precision: PrecisionLike = None,
    preferred_element_type: DTypeLike | None = None,
    out_axes: PartialAxisSpec | None = ...,
    dot_general=jax.lax.dot_general,
) -> NamedArray: ...
# ---
def _pad_like_unet(feature: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:
    crop = np.array(feature.shape[2:])
    target = np.array(skip.shape[2:])
    pads = target - crop
    if np.any(pads < 0):
        raise ValueError("Downsample/upsample feature is larger than skip connection")
    pads = [
        int(pads[1] // 2),
        int(pads[1] - pads[1] // 2),
        int(pads[0] // 2),
        int(pads[0] - pads[0] // 2),
    ]
    return torch.nn.functional.pad(feature, pads)
# ---
def _reset_chaos():
    yield
    reset_chaos()
# ---
def _merge_into_single_chunk(x, split_every=4):
    # do a tree merge along first axis
    while x.numblocks[0] > 1:
        chunks = (x.chunksize[0] * split_every,) + x.chunksize[1:]
        x = merge_chunks(x, chunks)
    return x
# ---
def port(self):
        if not hasattr(self, "_socket"):
            self._get_address()
        return self._port
# ---
def assign_task(self, task_id: JobName, resources: cluster_pb2.ResourceSpecProto) -> None:
        """Assign a task to this worker, updating committed resources."""
        self.running_tasks.add(task_id)
        self.committed_cpu += resources.cpu
        self.committed_mem += resources.memory_bytes
        self.committed_gpu += get_gpu_count(resources.device)
        self.committed_tpu += get_tpu_chip_count(resources.device)
# ---
def test_olmo3_param_counts_dont_change_with_seqlen():
    """Test that parameter counts are independent of sequence length."""
    from levanter.models.olmo3 import Olmo3LMHeadModel

    model = Olmo3LMHeadModel.init(hax.Axis("v", 2048), _get_olmo3_config(seq_len=128), key=random.PRNGKey(0))
    model2 = Olmo3LMHeadModel.init(hax.Axis("v", 2048), _get_olmo3_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def test_worker_sequential_jobs(cluster):
    """Sequential jobs verify reconciliation works across job boundaries.
    Worker state is consistent between tasks."""
    _url, client = cluster
    for i in range(3):
        job = submit(client, _quick, f"seq-{i}")
        status = wait(client, job, timeout=30)
        assert status.state == cluster_pb2.JOB_STATE_SUCCEEDED
# ---
def on_compute_end(self, event):
        events_df = pd.DataFrame(self.events)
        fig = generate_timeline(events_df)

        self.dst = Path(f"history/{event.compute_id}")
        self.dst.mkdir(parents=True, exist_ok=True)
        self.dst = self.dst / f"timeline.{self.format}"

        fig.savefig(self.dst)
# ---
def close(self): pass
# ---
def __str__(self) -> str:
        if self.revision is None:
            return self.model_name_or_path
        return f"{self.model_name_or_path}@{self.revision}"
# ---
def test_worker_failed_maps_to_failed(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_WORKER_FAILED) == JobStatus.FAILED
# ---
def get_all_coeffs():
    """Get all available calibration coefficients for the satellites."""
    coeffs = {}

    for platform in URLS:
        if platform not in coeffs:
            coeffs[platform] = {}
        for chan in URLS[platform].keys():
            url = URLS[platform][chan]
            print(url)
            page = get_page(url)
            coeffs[platform][chan] = get_coeffs(page)

    return coeffs
# ---
def layer(x_arg, weights_arg, bias_arg):
            return with_sharding_constraint(jnp.dot(x_arg, weights_arg) + bias_arg, P())
# ---
def get_range_bin(value: float, range_dict: Dict[Tuple[float, float], int], default=0):
    """Get the bin of a value given a range dictionary."""
    value = float(value)
    for k, idx in range_dict.items():
        if k == "other":
            continue
        low, high = k
        if low <= value < high:
            return idx
    return default
# ---
def __init__(self, artist, title, year=None):
        self.__artist = artist
        self.__title = title
        self.__year = year
# ---
def fuse_multiple_levels(
    *,
    max_total_source_arrays=DEFAULT_MAX_TOTAL_SOURCE_ARRAYS,
    max_total_num_input_blocks=DEFAULT_MAX_TOTAL_NUM_INPUT_BLOCKS,
):
    # use multiple_inputs_optimize_dag to test multiple levels of fusion
    return partial(
        multiple_inputs_optimize_dag,
        max_total_source_arrays=max_total_source_arrays,
        max_total_num_input_blocks=max_total_num_input_blocks,
    )
# ---
def create_agent():
        return Agent.objects.create(
            type=AgentType.objects.create(main_type=MainAgentType.objects.create()),
            ref_code=RefCode.objects.create(
                country=Country.objects.get(iso='SE'),
                repository_code='repo',
            ),
            level_of_detail=0,
            record_status=0,
            script=0,
            language=Language.objects.get(iso_639_1='sv'),
            create_date=timezone.now(),
        )
# ---
def __setitem__(self, field, value):
        setattr(self, field, value)
# ---
def do(state):
                ref_counts, sequences = state
                pages_row = sequences.page_indices["seq", i]
                ref_counts = dec_refcounts_for_seq(pages_row, ref_counts)
                sequences = sequences.release_slot(i)
                return ref_counts, sequences
# ---
def __repr__(self):
        return f"ReshardOp(num_shards={self.num_shards})"
# ---
def __init__(self, config_path=CONFIG_PATH):
        self.config = configparser.ConfigParser(allow_no_value=True)
        self.config.read(config_path)
# ---
def flops_per_token(self, vocab_size: int, context_length: int) -> float | None:
        return lm_flops_per_token(
            hidden_dim=self.hidden_dim,
            intermediate_dim=self.intermediate_dim,
            num_layers=self.num_layers,
            num_kv_heads=self.num_kv_heads,
            num_heads=self.num_heads,
            seq_len=context_length,
            vocab_size=vocab_size,
            glu=False,
        )
# ---
def sector_area(r,a):
    pi=22/7
    if a >= 360:
        return None
    sectorarea = (pi*r**2) * (a/360)
    return sectorarea
# ---
def lcm(x, y):
   if x > y:
       z = x
   else:
       z = y
   while(True):
       if((z % x == 0) and (z % y == 0)):
           lcm = z
           break
       z += 1
   return lcm
# ---
def poke_partition(partition):

            schema, table, partition = self.parse_partition_name(partition)

            logging.info(
                'Poking for {schema}.{table}/{partition}'.format(**locals())
            )
            return self.hook.check_for_named_partition(
                schema, table, partition)
# ---
def test_eq(self):
        """
        Two instances with equal C{__dict__}s are equal.
        """
        assert _api._SimpleNamespace(a=1) == _api._SimpleNamespace(a=1)
# ---
def run(self, fn: Callable, *args, name: str | None = None) -> Any:
        """Execute a function with arguments and return a future.

        Args:
            fn: Function to execute
            *args: Arguments to pass to function
            name: Optional task name for debugging/monitoring (used by RayContext)

        Returns:
            Future representing the execution (type depends on context)
        """
        ...
# ---
def update_events(self):
        '''
        We may attach events to this track before setting their `track` parameter.
        This method will move through all events and set their track to this track.
        '''
        for event in self.events:
            event.track = self
# ---
def notify(header=None, msg='', duration=2000, sound=None):
    if header is None: header = get_name()
    if sound is None: sound = get_setting('mute_notifications') == 'false'
    icon_path = os.path.join(get_path(), 'icon.png')
    try:
        xbmcgui.Dialog().notification(header, msg, icon_path, duration, sound)
    except:
        builtin = "XBMC.Notification(%s,%s, %s, %s)" % (header, msg, duration, icon_path)
        xbmc.executebuiltin(builtin)
# ---
def test_read_string_table(datadir):
    datadir.join('reader').chdir()
    src = 'sharedStrings.xml'
    with open(src) as content:
        assert read_string_table(content.read()) == [
            'This is cell A1 in Sheet 1', 'This is cell G5']
# ---
def __init__(self) -> None:
            super().__init__()
            self.write_calls: list[int] = []
# ---
def transgene_insertion(testapp, lab, award, ctcf):
    item = {
        'lab': lab['@id'],
        'award': award['@id'],
        'category': 'insertion',
        'purpose': 'in vivo enhancer characterization',
        'nucleic_acid_delivery_method': ['mouse pronuclear microinjection'],
        'modified_site_by_gene_id': ctcf['@id'],
        'introduced_sequence': 'ATCGTA'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def test_forward_padding_masked(params, tiny_cfg):
    """Padding tokens should not affect non-padding logits."""
    tokens = jnp.array([[10, 20, 30, 0, 0]])  # Last two are padding.
    logits_a = forward(params, tokens, tiny_cfg)

    tokens_b = jnp.array([[10, 20, 30, 50, 60]])  # No padding.
    logits_b = forward(params, tokens_b, tiny_cfg)

    # First 3 positions should be the same (padding doesn't attend).
    assert jnp.allclose(logits_a[0, :3], logits_b[0, :3], atol=1e-5)
# ---
def test_attentionmask_materialize_sliding_window_only():
    mask = AttentionMask(is_causal=False, sliding_window=1)
    allowed = mask.materialize_mask(4, 4)
    expected = jnp.array(
        [
            [True, True, True, True],
            [False, True, True, True],
            [False, False, True, True],
            [False, False, False, True],
        ],
        dtype=bool,
    )
    assert allowed is not None
    assert allowed.shape == (4, 4)
    assert jnp.array_equal(allowed, expected)
# ---
def load_web_rules(chainlink_dir):
    """Load web.md rules from .chainlink/rules/."""
    if not chainlink_dir:
        return get_fallback_rules()

    rules_path = os.path.join(chainlink_dir, 'rules', 'web.md')
    try:
        with open(rules_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except (OSError, IOError):
        return get_fallback_rules()
# ---
def test_instance_auto_disk_config_doesnt_pass_fail_safes(self):
        """Should not partition unless fail safes pass"""
        self.instance_values['auto_disk_config'] = True

        def fake_get_partitions(dev):
            return [(1, 0, 100, 'ext4'), (2, 100, 200, 'ext4')]
        self.stubs.Set(vm_utils, "_get_partitions",
                       fake_get_partitions)

        self.assertIsPartitionCalled(False)
# ---
def match(self, c):
        return self.c == '.' or self.c == c
# ---
def __init__(
        self,
        slice_id: str,
        scale_group: str,
        zone: str,
        vms: list[FakeVm],
        created_at_ms: int | None = None,
    ):
        self._slice_id = slice_id
        self._scale_group = scale_group
        self._zone = zone
        self._vms = vms
        self._created_at = Timestamp.from_ms(created_at_ms) if created_at_ms is not None else Timestamp.now()
        self._terminated = False
# ---
def test_braces_in_config_values_preserved(self, config_with_special_chars: config_pb2.BootstrapConfig):
        """Config values with braces are preserved in output."""
        script = _build_bootstrap_script(config_with_special_chars, vm_address="10.0.0.1")

        assert "v1.0-{tag}" in script
        assert "/cache/{project}/iris" in script
        assert "Hello {world}!" in script
        assert '{"key": "value"}' in script
# ---
def test_index(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = a[1:, :]
    run_operation(tmp_path, executor, "index", b)
# ---
def set_directory(self, directory):
        self.directory = directory
# ---
def output_strikethrough(self, m):
        text = self.output(m.group(1))
        return self.renderer.strikethrough(text)
# ---
def is_slow(self):
        for sort in self.sorts:
            if sort.is_slow():
                return True
        return False
# ---
def remainder(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "remainder")
    if x1.dtype not in _real_numeric_dtypes or x2.dtype not in _real_numeric_dtypes:
        raise TypeError("Only real numeric dtypes are allowed in remainder")
    return elemwise(nxp.remainder, x1, x2, dtype=result_type(x1, x2))
# ---
def find_Rotations(str): 
    tmp = str + str
    n = len(str) 
    for i in range(1,n + 1): 
        substring = tmp[i: i+n] 
        if (str == substring): 
            return i 
    return n
# ---
def _to_unbatched_named_array(axis_to_strip: AxisSelector):
    def to_unbatched_named_array(leaf):
        if isinstance(leaf, _PassiveNamedArray):
            if selects_axis(leaf.main_axes, axis_to_strip):
                return leaf.strip_axis(axis_to_strip)
            else:
                return leaf.to_named_array()
        else:
            return leaf

    return to_unbatched_named_array
# ---
def key_function(out_key):
        out_coords = out_key[1:]

        # return a tuple with a single item that is an iterator of input keys to be combined
        in_keys = [
            list(
                range(
                    bi * split_every,
                    min((bi + 1) * split_every, numblocks),
                )
            )
            for bi in out_coords
        ]
        return (iter([(name,) + tuple(p) for p in product(*in_keys)]),)
# ---
def _call_splash_attention(q_, k_, v_, sinks_):
            return jax.vmap(lambda q_b, k_b, v_b: kernel(q_b, k_b, v_b, sinks=sinks_), in_axes=(0, 0, 0))(q_, k_, v_)
# ---
def sub_list(nums1,nums2):
  result = map(lambda x, y: x - y, nums1, nums2)
  return list(result)
# ---
def is_trainable_and_floating_point(x):
        if x is True:
            return is_inexact_arrayish
        elif x is False:
            return False
        else:
            return lambda y: is_inexact_arrayish(y) and x(y)
# ---
def test_default_spec_allowed_mem_exceeded():
    # default spec fails for large computations
    a = xp.ones((20000, 10000), chunks=(10000, 10000))
    with pytest.raises(
        ValueError,
        match=r"Projected blockwise memory \(\d+\) exceeds allowed_mem \(\d+\), including reserved_mem \(\d+\)",
    ):
        xp.negative(a)
# ---
def build(
        self,
        bundle_path: Path,
        dockerfile: str,
        job_id: str,
        task_logs: TaskLogs | None = None,
    ) -> BuildResult:
        del bundle_path, dockerfile, job_id, task_logs
        return BuildResult(
            image_tag="local:latest",
            build_time_ms=0,
            from_cache=True,
        )
# ---
def remove_references(html: BeautifulSoup):
    # Remove the reference section
    references = html.findAll("section", {"id": "ltx_bibliography"})
    for ref in references:
        ref.decompose()

    # Remove the references section
    references = html.findAll("ul", {"class": "ltx_biblist"})
    for ref in references:
        ref.decompose()
# ---
from collections import defaultdict 
def get_unique(test_list):
  res = defaultdict(list)
  for sub in test_list:
    res[sub[1]].append(sub[0])
  res = dict(res)
  res_dict = dict()
  for key in res:
    res_dict[key] = len(list(set(res[key])))
  return (str(res_dict))
# ---
def intersection_nested_lists(l1, l2):
    result = [[n for n in lst if n in l1] for lst in l2]
    return result
# ---
def pancake_sort(nums):
    arr_len = len(nums)
    while arr_len > 1:
        mi = nums.index(max(nums[0:arr_len]))
        nums = nums[mi::-1] + nums[mi+1:len(nums)]
        nums = nums[arr_len-1::-1] + nums[arr_len:len(nums)]
        arr_len -= 1
    return nums
# ---
def blocking_wait(coro):
    """
    This will only work if there are fewer than 10 levels of nested coroutines...
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop is not None and loop.is_running():
        future = _executor.submit(lambda: asyncio.run(coro))
        return future.result()
    else:
        return asyncio.run(coro)
# ---
def _normalize(s):
        """Normalize a Unicode string's representation (used on both
        patterns and matched values).
        """
        return unicodedata.normalize('NFC', s)
# ---
def area_weighted_rmse(
    target: torch.Tensor, gen: torch.Tensor, area_weights: torch.Tensor
) -> torch.Tensor:
    area_weights = area_weights.to(target.device)
    return torch.sqrt(area_weighted_mean((gen - target) ** 2, area_weights))
# ---
def ensure():
    """One-time check to ensure desired number of TPU VMs are running across all zones."""
    tpu_client = tpu_v2.TpuClient()
    for zone, count in config.TPU_ZONES_CONFIG.items():
        ensure_tpu_vms(tpu_client, zone, count)
# ---
def re_order(A):
    k = 0
    for i in A:
        if i:
            A[k] = i
            k = k + 1
    for i in range(k, len(A)):
        A[i] = 0
    return A
# ---
def OpenFileInPreviewWindow( filename ):
  """ Open the supplied filename in the preview window """
  vim.command( 'silent! pedit! ' + filename )
# ---
def post(self):
        self.handle_request('POST')
# ---
def __init__(self, api_job):
        super(SetVolumeOption, self).__init__()
        self.api_job = api_job
        self.atom = SetVolumeOption
# ---
def attention_config(self) -> AttentionConfig:
        """Convert this MistralConfig to an AttentionConfig for use with Attention."""
        return AttentionConfig(
            Embed=self.Embed,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            use_bias=self.use_bias,
            upcast_attn=self.upcast_attn,
            attn_backend=self.attn_backend,
            flash_attention_block_size=self.flash_attention_block_size,
            rope=self.rope,
        )
# ---
def test_copy_experiment_wrong_course(self):
        """ Tests that copy_experiment fails if experiment is different coruse """
        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)
        url = reverse("ab_testing_tool_copy_experiment", args=(experiment.id,))
        response = self.client.post(url, follow=True)
        self.assertError(response, UNAUTHORIZED_ACCESS)
# ---
def test_reshape(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    # need intermediate reshape due to limitations in Dask's reshape_rechunk
    b = xp.reshape(a, (5000, 2, 10000))
    c = xp.reshape(b, (5000, 20000))
    run_operation(tmp_path, executor, "reshape", c)
# ---
def on_compute_end(self, ComputeEndEvent):
        """Called when the computation has finished.

        Parameters
        ----------
        event : ComputeStartEvent
            Information about the computation.
        """
        pass
# ---
def on_step(self, step_info: S, cb_info: CBInfo):
        """
        This function is called after the JIT-compiled function has completed. You have access to the `step_info`
        which contains information about the step that just completed, as well as the `cb_info` which is whatever
        was returned from `inside_step`.
        """
        ...
# ---
def test_find_path_respects_max_steps():
    source = "a = 1\nb = 2\nc = 3\n"
    target = "a = 10\nb = 20\nc = 30\n"
    path = find_path(source, target, max_steps=1)
    assert len(path) <= 1
# ---
def h_fs_touch (_,path): open(path,'w').close()
# ---
def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.host = 'localhost'
        self.port = 8000
# ---
def _get_target_gen_pairs(self) -> list[_TargetGenPair]:
        target_data = self._target_agg.get_data()
        gen_data = self._gen_agg.get_data()

        ret = []
        for name in gen_data.keys():
            ret.append(
                _TargetGenPair(
                    gen=gen_data[name],
                    target=target_data[name],
                    name=name,
                    area_weights=self._area_weights,
                )
            )
        return ret
# ---
def _identity(_):
            return cache
# ---
def mark_requesting(self, timestamp: Timestamp, timeout: Duration) -> None:
        """Mark this group as REQUESTING (scale-up in progress).

        Args:
            timestamp: Current timestamp
            timeout: How long to stay in REQUESTING state
        """
        self._requesting_until = timestamp.add(timeout)
# ---
def test_layer_norm():
    H = Axis("H", 10)
    hax_ln = hax.nn.LayerNorm.init(H)
    eqx_ln = eqx.nn.LayerNorm(shape=(H.size,))

    f = _compare_eqx_and_haliax(hax_ln, eqx_ln)
    out = f(hax.random.uniform(jrandom.PRNGKey(0), (H,)))

    assert out.axes == (H,)
# ---
def locked_call():
                with self._lock:
                    return self._method(*args, **kwargs)
# ---
def init(cls, Vocab: Axis, config: Qwen3Config, *, key):  # type: ignore[override]
        k_t, k_emb = jrandom.split(key, 2)
        transformer = LlamaTransformer.init(config, key=k_t)
        embeddings = LlamaEmbedding.init(Vocab, config, key=k_emb)
        if config.tie_word_embeddings:
            lm_head = None
        else:
            lm_head = hnn.Linear.init(In=config.Embed, Out=Vocab, key=k_emb, use_bias=False, out_first=True)
        return Qwen3LMHeadModel(transformer, embeddings, lm_head)
# ---
def build_push(source_tag: str, region: tuple[str, ...], project: str, image_name: str, version: str):
    """Push a local Docker image to GCP Artifact Registry."""
    _push_to_registries(
        source_tag,
        region,
        project,
        image_name=image_name,
        version=version,
    )
# ---
def filter(self, structure: Structure) -> np.ndarray:
        """Filter chains in a structure.

        Parameters
        ----------
        structure : Structure
            The structure to filter chains from.

        Returns
        -------
        np.ndarray
            The chains to keep, as a boolean mask.

        """
        raise NotImplementedError
# ---
def tree_broadcast_to(prefix: PyTree[L], t: T, *, is_leaf: Optional[Callable[[Any], bool]] = None) -> T:
    """
    Broadcasts a prefix tree to match the structure of a full tree. This is useful when you need to
    tree_map over t and prefix (using t as the leaves) but prefix is a tree prefix of t.
    """
    return jax.tree.map(
        # note the swap
        lambda pref, xtree: jax.tree.map(lambda x: pref, xtree, is_leaf=is_leaf),
        prefix,
        t,
        is_leaf=is_leaf,
    )
# ---
def test_mem_write_byte_write_off_screen(self):
        self.mda.mem_write_byte(4000, 0xFF)
        self.assertEqual(self.cg.last_blit, None)
# ---
def changeColumnInLines(self):
        """
        Implements function to transform columns in lines to make tests eaiser.
        :return: a reverse matrice
        """
        column = []
        for x in xrange(7):
            col = ''
            for y in xrange(6):
                col += self.grille[y][x]
            column.append(col)
        return column
# ---
def check():
            try:
                re.compile(regex_edit.text())
            except re.error as e:
                raise OptionsCheckError(_("Regex Error"), string_(e))
# ---
def read(self, path):
            return True
# ---
def update(self: S, model: M, step: int) -> S:
        pass
# ---
def convert_list_dictionary(l1, l2, l3):
     result = [{x: {y: z}} for (x, y, z) in zip(l1, l2, l3)]
     return result
# ---
def find_free_port(start_port: int = 9000, max_attempts: int = 1000) -> int:
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(("", port))
                return port
        except OSError:
            continue
    raise RuntimeError(f"No free port found in range {start_port}-{start_port + max_attempts}")
# ---
def _task_detail_page(self, request: Request) -> HTMLResponse:
        return HTMLResponse(html_shell("Task Detail", "/static/worker/task-detail.js"))
# ---
def stop(self) -> None:
        """Stop all container threads managed by this VM manager."""
        self._threads.stop(timeout=Duration.from_seconds(5.0))
# ---
def action_cancel(self, cr, uid, ids, context=None):
        """ Changes picking state to cancel.
        @return: True
        """
        for pick in self.browse(cr, uid, ids, context=context):
            ids2 = [move.id for move in pick.move_lines]
            self.pool.get('stock.move').action_cancel(cr, uid, ids2, context)
        self.write(cr, uid, ids, {'state': 'cancel', 'invoice_state': 'none'})
        return True
# ---
def open_link(self):
        "Open the selected item with the webbrowser"

        data = self.get_selected_item()
        url = data.get('permalink')
        if url:
            self.term.open_browser(url)
        else:
            self.term.flash()
# ---
def tokenizer():
    return TreeDiffusionTokenizer(max_seq_len=64)
# ---
def __call__(
        self, batch: Sequence[T_contra]
    ) -> Sequence[U_co] | U_co:  # U can be batched "structure of arrays" form
        """
        Process a batch of data. You should return a sequence of dicts (one per output
        example), or a dict of sequences (one per output field).
        """
        raise NotImplementedError
# ---
def terminate_job(self, job_id: JobName) -> None:
        request = cluster_pb2.Controller.TerminateJobRequest(job_id=job_id.to_wire())
        self._client.terminate_job(request)
# ---
def update_image_metadata(self, image_id, meta):
        """Updates the metadata for an image."""
        post_body = json.dumps({'metadata': meta})
        resp, body = self.post('images/%s/metadata' % str(image_id), post_body)
        body = json.loads(body)
        self.validate_response(schema.image_metadata, resp, body)
        return service_client.ResponseBody(resp, body['metadata'])
# ---
def test_rerank_empty_candidates():
    ranked = rerank_candidates([], ["assert True"])
    assert ranked == []
# ---
def f(llama_model, input_ids, mask):
        out = llama_model(input_ids, mask)
        return hax.sum(out).scalar()
# ---
def unwrap_metrics(pytree):
    """
    Walk a pytree and extract .value() from all Metric objects.
    """

    def _unwrap(x):
        if isinstance(x, Metric):
            return x.value()
        return x

    return jax.tree_util.tree_map(_unwrap, pytree, is_leaf=lambda x: isinstance(x, Metric))
# ---
def main(config: InferenceReplConfig):
    """Main entry point."""
    commands = ReplContext(config)
    os.environ["EQX_ON_ERROR"] = "nan"

    # Determine mode
    if config.command:
        cli_mode(config, commands)
    else:
        repl_mode(config, commands)
# ---
def _find_dim(n_rot: float):
                return (head_dim * math.log(self.config.original_max_position_embeddings / (n_rot * 2 * math.pi))) / (
                    2 * math.log(self.config.theta)
                )
# ---
def count_tuplex(tuplex,value):  
  count = tuplex.count(value)
  return count
# ---
def i_press_operator(operator):
    if "(" in operator:
        exec(f"bpy.ops.{operator}")
    else:
        exec(f"bpy.ops.{operator}()")
# ---
def batch_completions(
        self,
        prompts: list[str] | list[list[dict]],
        temperature: float,
        n: int,
        max_tokens: int | None = None,
        top_k: int | None = None,
        stop: list[str] | None = None,
        system_prompt: str | None = None,
    ) -> list[ChatCompletion]:
        """Batch completions from the inference server."""
        raise NotImplementedError
# ---
def visit_msubsup(self, element):
        children = self._get_clean_children(element)
        if len(children) == 3:
            base = self._visit(children[0])
            sub = self._visit(children[1])
            sup = self._visit(children[2])
            return BracedNode(f"{{{base}}}_{{{sub}}}^{{{sup}}}")
        return TextNode("")
# ---
def __getitem__(self, key):

        # self.lib.getHMatrix.restype=ctypes.POINTER(_C_HMatrix)
        # self.lib.getHMatrix.argtypes=[ctypes.POINTER(_C_MultiHMatrix), ctypes.c_int]
        # c_data_hmatrix = self.lib.getHMatrix(self.c_data,key)
        # return HMatrix(c_data_hmatrix,**self.params)
        return self.hmatrices[key]
# ---
def test_divide(self):
        expr = col("a") / col("b")
        assert expr.evaluate({"a": 10, "b": 4}) == 2.5
# ---
def _remove_right_units(string):
    # "\\text{ " only ever occurs (at least in the val set) when describing units
    if "\\text{ " in string:
        splits = string.split("\\text{ ")
        assert len(splits) == 2
        return splits[0]
    else:
        return string
# ---
def exitWalk(self):
        self.ignore(self.walkDoneEvent)
        self.walk.exit()
# ---
def saveProfileImage(self, filestorage):
        buffer = filestorage.stream
        buffer.seek(0)
        image = Image.open(buffer)
        image = ImageUtil.crop_image(image, 64)
        current_app.logger.info(image)
        dirpath = getDirectoryPath(current_app, '_settings')
        filepath = os.path.join(dirpath, "profile.png")
        image.save(filepath, optimize=True)
# ---
def find_longest_repeating_subseq(str): 
	n = len(str) 
	dp = [[0 for k in range(n+1)] for l in range(n+1)] 
	for i in range(1, n+1): 
		for j in range(1, n+1): 
			if (str[i-1] == str[j-1] and i != j): 
				dp[i][j] = 1 + dp[i-1][j-1] 
			else: 
				dp[i][j] = max(dp[i][j-1], dp[i-1][j]) 
	return dp[n][n]
# ---
def scan_fun(acc, z, x):
        out = acc + z * x, x * z
        return out
# ---
def getIcon(self):
        return  QtGui.QIcon(os.path.dirname(__file__) + "/../images/saga.png")
# ---
def __shortCodeToWordnetCode(self, shortCode):
        """
        It returns the wordnet code from a given language short code
        """
        if not self.__isLanguageAvailable(code=shortCode):
            raise Exception("Wordnet code not found for the language short code %s " % shortCode)

        code = shortCode.lower()
        wordnetCode = AVAILABLE_LANGUAGES[code]
        return wordnetCode
# ---
def spec(self) -> PageTableSpec:
        return PageTableSpec(num_pages=self.num_pages, page_size=self.page_size)
# ---
def __init__(self, num_shards: int = 4, rows_per_shard: int = 10):
        self._num_shards = num_shards
        self._rows_per_shard = rows_per_shard
# ---
def _create_lot(self, cr, uid, ids, product_id, prefix=False):
        """ Creates production lot
        @return: Production lot id
        """
        prodlot_obj = self.pool.get('stock.production.lot')
        prodlot_id = prodlot_obj.create(cr, uid, {'prefix': prefix, 'product_id': product_id})
        return prodlot_id
# ---
def _dt_struct_to_array(struct):
        if not isinstance(struct, ShapeDtypeStruct):
            return struct
        return jnp.zeros(struct.shape, struct.dtype)
# ---
def __init__(self, job: IrisJob):
        self._job = job
# ---
def config(self):
        return self.embeddings.config
# ---
def ema_initialized(self) -> bool:
        """Check if EMA weights have been initialized.

        Returns
        -------
        bool
            Whether the EMA weights have been initialized.

        """
        return self._ema_weights is not None
# ---
def copy_to(self, model: BaseModel | nn.parallel.DistributedDataParallel):
        """
        Copy the averaged parameters to the model, overwriting its values.
        """
        m_param = dict(model.named_parameters())
        for key in m_param:
            if m_param[key].requires_grad:
                m_param[key].data.copy_(self._ema_params[self._get_ema_name(key)].data)
            else:
                assert self._get_ema_name(key) not in self._ema_params
# ---
def cube_Sum(n): 
    sum = 0   
    for i in range(0,n) : 
        sum += (2*i+1)*(2*i+1)*(2*i+1) 
    return sum
# ---
def sum_Of_Series(n): 
    sum = 0
    for i in range(1,n + 1): 
        sum += i * i*i       
    return sum
# ---
def native_value(self):
        """Return sensor state."""
        printer: OctoprintPrinterInfo = self.coordinator.data["printer"]
        if not printer:
            return None

        return printer.state.text
# ---
def _put(self, item):
        if item not in self.all_items:
            Queue._put(self, item) 
            self.all_items.add(item)
# ---
from itertools import combinations_with_replacement 
def combinations_colors(l, n):
    return list(combinations_with_replacement(l,n))
# ---
def _invoice_line_hook(self, cr, uid, move_line, invoice_line_id):
        '''Call after the creation of the invoice line'''
        return
# ---
def test_conversion_to_jnp_bfloat16():
    import torch

    x = torch.arange(10, dtype=torch.bfloat16) / 3.14
    with pytest.raises(TypeError):
        x.cpu().numpy()

    x_jnp = _convert_to_jnp(x, None)
    assert x_jnp.dtype == jnp.bfloat16
    assert x_jnp.shape == x.shape
    assert_trees_all_close(x_jnp, jnp.arange(10, dtype=jnp.bfloat16) / 3.14)
# ---
def __len__(self):
        return len(self._dict)
# ---
def _attrs_copy(self):
        return None if self._attrs is None else OrderedDict(self._attrs)
# ---
def remove_router_interface_precommit(self, context, r_port_context):
        pass
# ---
def test_without_private(self, mock_start, mock_stop):
        fake_cls = FakeTraceClassHideArgs()
        self.assertEqual(10, fake_cls._method(10))
        self.assertFalse(mock_start.called)
        self.assertFalse(mock_stop.called)
# ---
def make_state(key):
            model = Gpt2Mlp.init(Embed, Intermediate, jax.nn.relu, key=key)
            optim = optax.adam(1e-4)
            opt_state = optim.init(arrays_only(model))

            return arrays_only(model), arrays_only(opt_state), key
# ---
def to_ee(ddf, *args, **kwargs):
  raise NotImplementedError('This has not yet been designed.')
# ---
def __init__(
        self,
        cache_dir: Path,
        registry: str,
        max_images: int = 100,
    ):
        self._cache_dir = cache_dir / "images"
        self._registry = registry
        self._max_images = max_images
        self._cache_dir.mkdir(parents=True, exist_ok=True)
        self._docker = DockerImageBuilder(registry)
        # Refcount of images currently in use by jobs (should not be evicted)
        self._image_refcounts: dict[str, int] = {}
# ---
def with_default_renderer_for(self, *syntax_names: str) -> RenderContext:
        renderers = dict(self.renderers)
        for syntax in syntax_names:
            if syntax in DEFAULT_RENDERERS:
                renderers[syntax] = DEFAULT_RENDERERS[syntax]
            else:
                renderers.pop(syntax, None)
        return RenderContext(
            MappingProxyType(renderers), self.postprocessors, self.options, self.env
        )
# ---
def get_label(self, step: int) -> Prognostic:
        return self[step][1]
# ---
def genetic_modification_RNAi(testapp, lab, award):
    item = {
        'award': award['@id'],
        'lab': lab['@id'],
        'modified_site_by_coordinates': {
            'assembly': 'GRCh38',
            'chromosome': '11',
            'start': 20000,
            'end': 21000
        },
        'purpose': 'repression',
        'category': 'deletion',
        'method': 'RNAi'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def update_fn(updates, state, params, **extra_args):
        levanter.tracker.jit_log({desc: optax.tree_utils.tree_l2_norm(updates)})
        return updates, None
# ---
def tearDown(self):
        del self.cj
# ---
def setUp(self):
        self.cg = CharacterGeneratorMock(width = 9, height = 14)
        self.mda = MonochromeDisplayAdapter(self.cg)

        # Hijack reset so it doesn't call into Pygame during the tests.
        self.reset_count = 0
        self.mda.reset = self.reset_testable
# ---
def init_fn(params):
        del params
        buffer_size = rolling_interval_length
        update_norms = jnp.zeros((buffer_size,), dtype=jnp.float32)
        valid_mask = jnp.full((buffer_size,), False, dtype=jnp.bool_)
        current_idx = jnp.zeros((), dtype=jnp.int32)
        count = jnp.zeros((), dtype=jnp.int32)

        return ClipUpdateNormState(
            update_norms=update_norms,
            current_idx=current_idx,
            count=count,
            valid_mask=valid_mask,
        )
# ---
def remove_authors(html: BeautifulSoup):
    # Remove authors since we only care about information after first section
    authors = html.findAll("div", {"class": "ltx_authors"})
    for author in authors:
        author.decompose()
        section = author.previous_sibling
        while section:
            new_section = section.previous_sibling
            section.decompose()
            section = new_section
    return html
# ---
def batch_schedule(self):
        return BatchSchedule(self.train_batch_size)
# ---
def __init__(
        self,
        config: WeightTransferConfig,
        axis_mapping: ResourceMapping | None = None,
        mesh: Mesh | None = None,
    ):
        self.config = config
        self.axis_mapping = axis_mapping
        self.mesh = mesh
        self.weight_step = -1
        self.metrics = WeightTransferClientMetrics()
# ---
def workspace_path(self) -> str:
        """Path to the workspace root directory (temp dir root)."""
        if not self._temp_dir:
            raise RuntimeError("TemporaryVenv must be entered before accessing workspace_path")
        return self._temp_dir.name
# ---
def cube_nums(nums):
 cube_nums = list(map(lambda x: x ** 3, nums))
 return cube_nums
# ---
def __exit__(self, *exc) -> None:
        self.shutdown()
# ---
def list_nodes() -> list[dict[str, Any]]:
    """Get list of Ray nodes."""
    result = run_ray_command(
        ["ray", "list", "nodes", "--format=json", "--limit=10000"],
    )
    return json.loads(result.stdout)
# ---

def circular_shift(x, shift):
    """Circular shift the digits of the integer x, shift the digits right by shift
    and return the result as a string.
    If shift > number of digits, return digits reversed.
    >>> circular_shift(12, 1)
    "21"
    >>> circular_shift(12, 2)
    "12"
    """
    s = str(x)
    if shift > len(s):
        return s[::-1]
    else:
        return s[len(s) - shift:] + s[:len(s) - shift]
# ---
def fsspec_listdir(dirname):
  """Listdir in manner compatible with fsspec."""
  fs, _ = fsspec.core.url_to_fs(dirname)
  return fs.ls(dirname)
# ---
def _docker_inspect(container_name: str) -> str:
    result = subprocess.run(
        ["docker", "inspect", container_name],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        stderr = (result.stderr or "").strip()
        return f"failed to inspect container {container_name}: {stderr}"
    return result.stdout
# ---
def _get_l3_plugin(self):
        return directory.get_plugin(constants.L3)
# ---
def setcommission(self, commission=0.0, margin=None, mult=1.0, name=None):
        comm = CommissionInfo(commission=commission, margin=margin, mult=mult)
        self.comminfo[name] = comm
# ---
def setMaterial(self, material):
        self.__material = material
# ---
def get_by_id(self, id):
        Util.validate_type(id, "str")
        return self._get_by_id(id)
# ---
from collections import defaultdict
def count_Substrings(s,n):
    count,sum = 0,0
    mp = defaultdict(lambda : 0)
    mp[0] += 1
    for i in range(n):
        sum += ord(s[i]) - ord('0')
        count += mp[sum - (i + 1)]
        mp[sum - (i + 1)] += 1
    return count
# ---
def __call__(self, batch: Sequence[Sequence[int]]):
                raise RuntimeError("This should not be called")
# ---
def Check_Solution(a,b,c) : 
    if ((b*b) - (4*a*c)) > 0 : 
        return ("2 solutions") 
    elif ((b*b) - (4*a*c)) == 0 : 
        return ("1 solution") 
    else : 
        return ("No solutions")
# ---
def partition(self, tensor):
        """Partition tensor into blocks."""

        assert tensor.shape == self._shape
        tensors = [tensor]
        for i, indices in self._splits:
            tensors_local = []
            for t in tensors:
                tensors_local.extend(jnp.split(t, indices_or_sections=indices, axis=i))
            tensors = tensors_local
        return tuple(tensors)
# ---
def on_predict_batch_start(self, trainer, pl_module, batch, batch_idx):
        """Called before each prediction batch - update progress bars."""
        super().on_predict_batch_start(trainer, pl_module, batch, batch_idx)
        self._update_all_progress_bars()
# ---
def output_exemplar(self) -> dict[str, Sequence[int]]:
        return {"data": np.array([0], dtype=np.int64)}
# ---
def store(self) -> TreeStore[T_co]:
        return self._store
# ---
def init(config: AttentionConfig, *, key) -> "AttentionWithSink":
        base = Attention.init(config, key=key)
        sinks = hax.zeros((config.KVHeads, config.QHeadsPerGroup), dtype=jnp.float32)
        return AttentionWithSink(
            base.config,
            base.q_proj,
            base.k_proj,
            base.v_proj,
            base.o_proj,
            base.q_norm,
            base.k_norm,
            base.rot_embs,
            sinks,
        )
# ---
def test_empty_set_against_string(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(table.c.z.in_(bindparam("q", expanding=True)))
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def extract_id(self, item):
        raise NotImplementedError
# ---
def find_First_Missing(array,start,end): 
    if (start > end): 
        return end + 1
    if (start != array[start]): 
        return start; 
    mid = int((start + end) / 2) 
    if (array[mid] == mid): 
        return find_First_Missing(array,mid+1,end) 
    return find_First_Missing(array,start,mid)
# ---
def subject_marks(subjectmarks):
#subject_marks = [('English', 88), ('Science', 90), ('Maths', 97), ('Social sciences', 82)])
 subjectmarks.sort(key = lambda x: x[1])
 return subjectmarks
# ---
def surfacearea_cuboid(l,w,h):
  SA = 2*(l*w + l * h + w * h)
  return SA
# ---
def __get__(self, obj, cls):
        if obj is None:
            return None
        value = self.fget(obj)
        setattr(obj, self.func_name, value)
        return value
# ---
def uipath(self, f):
        return self._matcher.uipath(self._path + "/" + f)
# ---
def to_hf_config(self) -> tuple[float, dict | None]:
        """Returns the rope_theta and config dict for the HF config."""
        pass
# ---
def on_upper_bound_checked_changed(self):
        if self.ui.checkBoxUpperBound.isChecked():
            self.ui.spinBoxUpperBound.setEnabled(True)
            self.ui.spinBoxBoundaryNumber.setEnabled(True)
        elif not self.ui.checkBoxLowerBound.isChecked():
            self.ui.spinBoxUpperBound.setEnabled(False)
            self.ui.spinBoxBoundaryNumber.setEnabled(False)
        else:
            self.ui.spinBoxUpperBound.setEnabled(False)
# ---
def main(islinput, inputfile, pluginData, globalData):
	currentIndex = 0
	for item in islinput:
		item = normalizeEnter(item) #Deletes not wanted line breaks in order to prevent the problem we have with Markdown.
		islinput[currentIndex] = item
		currentIndex += 1
	return islinput, pluginData, globalData
# ---
def parse_key(key, sep='.'):
        '''parse the key allowing the appropriate separator'''
        common_separators = list(Yedit.com_sep - set([sep]))
        return re.findall(Yedit.re_key.format(''.join(common_separators)), key)
# ---
def platform(self):
        """Create Platform instance from config.

        Returns:
            Platform implementation (GCP, Manual, or Local)
        """
        from iris.cluster.vm.platform import create_platform

        return create_platform(
            platform_config=self._proto.platform,
            bootstrap_config=self._proto.defaults.bootstrap,
            timeout_config=self._proto.defaults.timeouts,
            ssh_config=self._proto.defaults.ssh,
        )
# ---
def triangle_area(r) :  
    if r < 0 : 
        return -1
    return r * r
# ---
def test_get_stats_invalid_container(docker_runtime):
    """Test that get_stats returns available=False for invalid container ID."""
    invalid_container_id = "nonexistent_container_12345"

    stats = docker_runtime.get_stats(invalid_container_id)

    assert stats.available is False
# ---
def doMonteCarloQP(pointa, pointb, clf, nopoint):
    #print "weights ", weight
    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    #print dataset_Monte
    return getMisMatchesQP(dataset_Monte, clf)
# ---
def issort_list(list1):
    result = all(list1[i] <= list1[i+1] for i in range(len(list1)-1))
    return result
# ---
def GetCurrentBufferFilepath():
  return GetBufferFilepath( vim.current.buffer )
# ---
def test_fft_freq_and_shift():
    N = Axis("n", 8)
    x = hax.arange(N)

    f = hax.fftfreq(N)
    assert f.axes == (N,)
    assert jnp.allclose(f.array, jfft.fftfreq(8))

    rf = hax.rfftfreq(N)
    assert rf.axes[0].size == 5
    assert jnp.allclose(rf.array, jfft.rfftfreq(8))

    shifted = hax.fftshift(x)
    assert jnp.allclose(shifted.array, jfft.fftshift(x.array))
    unshifted = hax.ifftshift(shifted)
    assert jnp.allclose(unshifted.array, x.array)
# ---
def decomposed_mse_cos_weighted(
    pred: torch.Tensor, target: torch.Tensor, cos: torch.Tensor
) -> torch.Tensor:
    """MSE loss weighted by cosine of latitude."""
    weights = cos.view(1, 1, -1, 1)  # Reshape for broadcasting
    mse = F.mse_loss(pred, target, reduction="none")
    weighted_mse = mse * weights
    return weighted_mse.mean(dim=(0, 2, 3))
# ---
def node_types(self) -> list[str]:
        return sorted(self.entries.keys())
# ---
def binary_to_integer(test_tup):
  res = int("".join(str(ele) for ele in test_tup), 2)
  return (str(res))
# ---
def test_reduce_count(backend):
    """Test count reduction."""
    ds = Dataset.from_list([{"id": i} for i in range(50)]).reduce(
        local_reducer=lambda items: sum(1 for _ in items),
        global_reducer=sum,
    )
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 1
    assert results[0] == 50
# ---
def get_variable_loss_dict(
    label: str, loss_per_channel: torch.Tensor
) -> dict[str, torch.Tensor]:
    tensor_map = TensorMap.get_instance()
    metrics = {}
    for variable in tensor_map.VAR_SET:
        metrics[f"{label}/loss/variable/{variable}_loss"] = loss_per_channel[
            tensor_map.VAR_3D_IDX[variable]
        ].mean()
    return metrics
# ---
def html_to_md(cfg: SimpleHtmlToMdConfig):
    """Transform HTML content to markdown using the specified extraction method."""
    pipeline = (
        Dataset.from_files(f"{cfg.input_path}/**/*.jsonl.gz")
        .flat_map(load_jsonl)
        .map(lambda data: _html_to_md(data, cfg.extract_method, cfg.config))
        .write_jsonl(f"{cfg.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )
    list(Backend.execute(pipeline))
# ---
def is_scalar(input_type):
    """Returns True if input_type is scalar."""
    return input_type['base_type'] in SCALAR
# ---
def _go(conn):
            assert_raises_message(
                tsa.exc.StatementError,
                r"nope \(original cause: Exception: nope\) u?'SELECT 1 ",
                conn.execute,
                    select([1]).\
                        where(
                            column('foo') == literal('bar', MyType())
                        )
            )
# ---
def broadcast_arrays(*arrays):
    # From dask broadcast_arrays

    # Unify uneven chunking
    inds = [list(reversed(range(x.ndim))) for x in arrays]
    uc_args = tlz.concat(zip(arrays, inds))
    _, args = unify_chunks(*uc_args, warn=False)

    shape = np.broadcast_shapes(*(e.shape for e in args))
    chunks = broadcast_chunks(*(e.chunks for e in args))

    result = tuple(broadcast_to(e, shape=shape, chunks=chunks) for e in args)

    return result
# ---
def _assert_coefficients(coeffs: NamedArray, expected: jnp.ndarray, axis: Axis) -> None:
    assert jnp.allclose(coeffs.array, expected)  # type: ignore[union-attr]
    assert coeffs.axes[0] == axis.resize(coeffs.array.shape[0])
# ---
def __init__(self, methods):
        self._define_methods(methods)
# ---
def __repr__(self):
        return f"SelectOp(columns={self.columns})"
# ---
def test_list(self):
        results = spell_checker.check([u' .', u'  .'])
        assert results[0].checked == u'.'
        assert results[1].checked == u' .'
# ---
def _collect_rollouts(self) -> None:
        """Collect available rollouts from reader and add to buffer."""
        batches = self.rollout_reader.read_all_available()

        if not batches:
            return

        start_time = time.time()

        self.replay_buffer.add_batches(batches)

        elapsed = time.time() - start_time
        if batches:
            logger.info(f"Collected {len(batches)} rollout batches, updated replay buffer in {elapsed}")
# ---
def chunk_len_for_indexer(ia, c):
        if not isinstance(ia, ndindex.Slice):
            return c
        return max(c // ia.step, 1)
# ---
def get(self, item):
        """Return the element in the queue identical to `item`.

        Parameters
        ----------
        item :
            The element to search for.

        Returns
        -------
        The element in the queue identical to `item`. If the element
        was not found, None is returned.

        """
        try:
            index = self._queue.index(item)
            return self._queue[index]
        except Exception:
            return None
# ---
def model_cfg(tokenizer):
    return TreeDiffusionConfig(
        vocab_size=tokenizer.vocab_size,
        hidden_dim=64,
        intermediate_dim=128,
        num_layers=2,
        num_heads=4,
        num_kv_heads=4,
        max_seq_len=MAX_SEQ_LEN,
    )
# ---
def perform_create(self, serializer):
        self.check_hosts(serializer)
        instance = serializer.save()
        instance.user = self.request.user
        instance.save()
        cols = self.request.query_params.get("cols", '80')
        rows = self.request.query_params.get("rows", '24')
        transaction.on_commit(lambda: run_command_execution.apply_async(
            args=(instance.id,), kwargs={"cols": cols, "rows": rows},
            task_id=str(instance.id)
        ))
# ---
def sub_reload(self, sub_id=None):
        self.command('sub_reload', sub_id)
# ---
def reset(self):
        return GenState(
            cache=self.cache.reset(),
            decode_state=self.decode_state.reset(),
        )
# ---
def test_add_permission(self):
        """
        Tests that staff cannot add entries
        """
        self.assertFalse(self.creator_admin.has_add_permission(self.request))
# ---
def build_image():
    """Build and push the TPU CI Docker image to ghcr.io."""
    logging.info("Building TPU CI Docker image...")
    build_and_push_docker_image()
    logging.info(" Image build complete")
# ---
def check_datasets_equal(ds1, ds2):
    ds1 = list(ds1)
    ds2 = list(ds2)
    assert len(ds1) == len(ds2)
    for r1, r2 in zip(ds1, ds2):
        assert r1.keys() == r2.keys()
        for key in r1.keys():
            np.testing.assert_array_equal(r1[key], r2[key])
# ---
def _pure_pattern(regex):
    pattern = regex.pattern
    if pattern.startswith('^'):
        pattern = pattern[1:]
    return pattern
# ---
def _invoice_hook(self, cr, uid, picking, invoice_id):
        '''Call after the creation of the invoice'''
        return
# ---
def test_empty_heterogeneous_tuples(self):
        table = self.tables.some_table

        stmt = (
            select([table.c.id])
            .where(
                tuple_(table.c.x, table.c.z).in_(
                    bindparam("q", expanding=True)
                )
            )
            .order_by(table.c.id)
        )

        self._assert_result(stmt, [], params={"q": []})
# ---
def to_svg(self, size=500):
        """Convert chunks from Cubed Array into an SVG Image

        Parameters
        ----------
        chunks: tuple
        size: int
            Rough size of the image

        Examples
        --------
        >>> x.to_svg(size=500)  # doctest: +SKIP

        Returns
        -------
        text: An svg string depicting the array as a grid of chunks
        """
        from cubed.vendor.dask.array.svg import svg

        return svg(self.chunks, size=size)
# ---
def ListTasks(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def class_labels(self):
    return ["ID_%d" % i for i in range(self.num_classes)]
# ---
def dtype(self) -> DType: ...
# ---
def unsize_axes(axis_spec: AxisSelection) -> AxisSelection: ...
# ---
def test_add_to_aggregate_called(self):
        def fake_add_to_aggregate(context, aggregate, host):
            fake_add_to_aggregate.called = True
        self.stubs.Set(self.conn._pool,
                       "add_to_aggregate",
                       fake_add_to_aggregate)

        self.conn.add_to_aggregate(None, None, None)
        self.assertTrue(fake_add_to_aggregate.called)
# ---
def __repr__(self):
        return "{0.__class__.__name__}({0.subquery!r})".format(self)
# ---
def __init__(
        self,
        dataset: AsyncDataset[T],
        fn: MapFunction[Sequence[U]],
        *extra_args,
        **extra_kwargs,
    ):
        super().__init__()
        self.dataset = dataset
        self.fn = fn
        self._extra_args = extra_args
        self._extra_kwargs = extra_kwargs
# ---
def profile_mean(ds: xr.Dataset) -> xr.Dataset:
    """
    Compute the mean of each variable for each time step.
    """
    return ds.weighted(ds.areacello).mean(["y", "x"])
# ---
def __init__(self, capacity):
        self.examplers = deque(maxlen=capacity)
        self.capacity = capacity
# ---
def _parse_avro_schema(self):
        """Extract and parse Avro schema from a read session."""
        if self._avro_schema_json:
            return

        self._avro_schema_json = json.loads(self._first_message.avro_schema.schema)
        self._column_names = tuple(
            (field["name"] for field in self._avro_schema_json["fields"])
        )
        self._first_message = None
# ---
def current_len(self) -> Optional[int]:
        return self._run_coroutine(self.dataset.current_len())
# ---
def __repr__(self) -> str:
        return f"Duration({self.to_seconds():.3f}s)"
# ---
def finger(self, match):
        '''
        Print out the fingerprints for the matched keys

        :param str match: A string to match against. i.e. 'web*'
        '''
        matches = self.key.finger(match)
        salt.output.display_output(
                matches,
                'key',
                self.opts)
# ---
def test_spawn_not_enough_memory(self):
        self.assertRaises(exception.InsufficientFreeMemory,
                          self._test_spawn,
                          1, 2, 3, "4")
# ---
def fpik(inp, pref="FPIK"):
            vals = list(inp)
            vals = [(v if v != 2 else (2 if sum(vals) < 4 else 1)) for v in vals]
            for i, p in enumerate(pref):
                yield '"' + p + '": ' + str(vals[i])
# ---
def test_is_empty(self):
        assert not os.path.exists(self.dbname)
# ---
def parent(self) -> "JobName | None":
        """Get parent job name, or None if this is a root job."""
        if len(self._parts) == 1:
            return None
        return JobName(self._parts[:-1])
# ---
def scan_fun(acc, x):
        return acc + jnp.sum(x.array), x.take("Width", 2)
# ---
def _munge_address_port(address: str):
        # the coordinator address typically includes a port that jax wants to use. we want to use our own port
        # we add a deterministic number to the chosen port and then cycle through the ephemeral range
        # this is a hack, but it works
        host, port_str = address.split(":")
        port = int(port_str)
        return host, port
# ---
def dn2uuid(self, backend, dn, from_db_only=False):
        uuid = ObjectBackendRegistry.backends[backend].dn2uuid(dn)
        if uuid is None and from_db_only is True:
            # fallback to db
            if self.__index is None:
                self.__index = PluginRegistry.getInstance("ObjectIndex")
            res = self.__index.search({'dn': dn}, {'uuid': 1})
            if len(res) == 1:
                uuid = res[0]['_uuid']
        return uuid
# ---
def bell_number(n):   
    bell = [[0 for i in range(n+1)] for j in range(n+1)] 
    bell[0][0] = 1
    for i in range(1, n+1): 
        bell[i][0] = bell[i-1][i-1]  
        for j in range(1, i+1): 
            bell[i][j] = bell[i-1][j-1] + bell[i][j-1]   
    return bell[n][0]
# ---
def healthy(self) -> bool:
        return not self._failed and not self.is_being_preempted()
# ---
def initialize(self, base_block):
        self.vtx = copy.deepcopy(base_block.vtx)
        self.hashMerkleRoot = self.calc_merkle_root()
# ---
def test_asdict_excluding_simple():
    """Test asdict_excluding with a simple dataclass."""
    config = NestedConfig(name="test", value=42)
    result = asdict_excluding(config, exclude={"runtime_env"})
    assert result == {"name": "test", "value": 42}
    assert "runtime_env" not in result
# ---
def __init__(self, p_str):
        TodoBase.__init__(self, p_str)
        self.attributes = {}
# ---
import math  
def fourth_Power_Sum(n): 
    sum = 0
    for i in range(1,n+1) : 
        sum = sum + (i*i*i*i) 
    return sum
# ---
def test_one_step_edit_invalid_source_returns_none():
    assert one_step_edit("not valid{{{", "x = 1\n") is None
# ---
def test_permutation_with_array_returns_correct_values(PermutationClass):
    length = 10
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    indices = jnp.arange(length)
    results = permutation(indices)
    assert isinstance(results, numpy.ndarray)
    assert len(results) == length
    assert jnp.sum(results == indices) <= 2
# ---
def test_get_task_not_found(client):
    """Test GetTaskStatus RPC with nonexistent task returns error."""
    response = rpc_post(
        client,
        "GetTaskStatus",
        {"taskId": JobName.root("nonexistent").task(0).to_wire()},
    )
    assert response.status_code != 200
# ---
def __init__(self, config_node, private=False):
        assert isinstance(config_node, EnkfConfigNode)

        if private:
            c_pointer = EnkfNode.cNamespace().alloc_private(config_node)
        else:
            c_pointer = EnkfNode.cNamespace().alloc(config_node)

        super(EnkfNode, self).__init__(c_pointer, config_node, True)
# ---
def __init__(self, checkpoint_dir: Path):
        self.checkpoint_dir = checkpoint_dir
# ---
def list_actors(self, request: actor__pb2.ListActorsRequest, ctx: RequestContext) -> actor__pb2.ListActorsResponse:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def loss_ref(x_raw, w_raw):
        loss_ref, lse_ref = linear_softmax_cross_entropy_loss_reference(
            x_raw.reshape(6, 4),
            y.reshape(6),
            w_raw,
        )
        loss_ref = loss_ref + logsumexp_weight * (lse_ref**2)
        return loss_ref.mean()
# ---
def __iter__(self) -> typing.Iterator[int]:
    n = len(self.data_source)

    self.state = self.generator.get_state()
    indices = torch.randperm(n, generator=self.generator).tolist()

    if not self.restarting:
      self.counter = 0
    else:
      indices = indices[self.counter:]
      self.restarting = False

    for index in indices:
      self.counter += 1
      yield index

    self.counter = 0
# ---
def key_function(out_key):
        # compute the selection on x required to get the relevant chunk for out_key
        in_sel = selection_function(out_key)

        # use a Zarr indexer to convert selection to input coordinates
        indexer = _create_zarr_indexer(in_sel, x.shape, x.chunksize)

        return (iter(tuple((x.name,) + cp.chunk_coords for cp in indexer)),)
# ---
def entry(self,codeBibtex):
        return codeBibtex[self.label]
# ---
def zeros(shape, *, dtype=None, device=None, chunks="auto", spec=None) -> "Array":
    dtypes = __array_namespace_info__().default_dtypes(device=device)

    if dtype is None:
        dtype = dtypes["real floating"]
    return full(shape, 0, dtype=dtype, device=device, chunks=chunks, spec=spec)
# ---
def on_btn_del_row_clicked(self):
        min_row, max_row, _, _ = self.ui.tblFuzzingValues.selection_range()
        self.delete_lines(min_row, max_row)
# ---
def __init__(self, actor_ref: ray.actor.ActorHandle):
        # Store under a mangled name so __getattr__ doesn't recurse
        object.__setattr__(self, "_actor_ref", actor_ref)
# ---
def _get_renderer(model_name: str, tokenizer) -> Renderer:
        """Get the appropriate renderer based on model name."""
        model_name_lower = model_name.lower()
        if "qwen" in model_name_lower:
            return Qwen3Renderer(tokenizer)
        elif "llama" in model_name_lower:
            return Llama3Renderer(tokenizer)
        else:
            raise ValueError(f"Unsupported model type for {model_name}. Only Qwen3 and Llama3.1 models are supported.")
# ---
def test_plan_scaling(tmp_path, factor):
    spec = cubed.Spec(tmp_path, allowed_mem="2GB")
    chunksize = 5000
    a = cubed.random.random(
        (factor * chunksize, factor * chunksize), chunks=chunksize, spec=spec
    )
    b = cubed.random.random(
        (factor * chunksize, factor * chunksize), chunks=chunksize, spec=spec
    )
    c = xp.matmul(a, b)

    assert c.plan._finalize().num_tasks() > 0
    c.visualize(filename=tmp_path / "c")
# ---
def test_new_websocket_client_token_invalid(self, check_token):
        check_token.return_value = False

        self.wh.path = "ws://127.0.0.1/?token=XXX"

        self.assertRaises(exception.InvalidToken,
                          self.wh.new_websocket_client)
        check_token.assert_called_with(mock.ANY, token="XXX")
# ---
def suppress_prints(is_master):
    """This function disables printing when not in master process."""
    if not is_master:
        builtins.print = print
# ---
from itertools import groupby
def encode_list(list1):
    return [[len(list(group)), key] for key, group in groupby(list1)]
# ---
def on_task_end(self, event):
        self.pbars[event.name].update(event.num_tasks)
# ---
def int_step(self) -> int:
        """
        Returns the step as an int. On multinode, doing
        """
        return int(self.step)
# ---
def global_stats(shard_stats):
        stats_list = list(shard_stats)
        return {
            "sum": sum(s["sum"] for s in stats_list),
            "count": sum(s["count"] for s in stats_list),
            "min": min(s["min"] for s in stats_list),
            "max": max(s["max"] for s in stats_list),
        }
# ---
def broadcast_qs(_, ps, q, s):
            stack_n = ps[0]
            if partition_grads_into_blocks:
                # add leading dim for stacked partitions
                q = jax.tree.map(lambda x: jnp.repeat(jnp.expand_dims(x, 0), stack_n, axis=0), q)
            if s > 0:
                # add leading dim if we're scanning this layer
                q = jax.tree.map(lambda d: jnp.repeat(jnp.expand_dims(d, 0), s, axis=0), q)
            return q
# ---
def add(self, func):
        self[func.__name__] = func
        return func
# ---
def logs_tail(self, handle: VllmServerHandle, *, max_lines: int = 200) -> str:
        return _docker_logs_tail(handle.docker_container_name, max_lines=max_lines)
# ---
def main(inference_config: InferenceConfig):
    ray.get(run_inference.remote(inference_config))
# ---
from collections import Counter 
def most_common_elem(s,a):
  most_common_elem=Counter(s).most_common(a)
  return most_common_elem
# ---
def test_multiple_processes_cleanup():
    venv = TemporaryVenv()
    processes = []
    with venv:
        for _ in range(3):
            proc = venv.run_async([venv.python_path, "-c", "import time; time.sleep(100)"])
            processes.append(proc)

    # wait for processes to terminate.
    time.sleep(0.5)

    for proc in processes:
        print(proc, proc.poll())
        assert proc.poll() is not None
# ---
def build_shard_bloom(records: Iterator[dict]) -> Iterator[bytes]:
        """Build bloom filter from a shard of records and yield serialized bytes."""
        bf = Bloom(config.estimated_doc_count, config.false_positive_rate)

        for record in records:
            text = record.get(config.text_field, "")
            for feature in extract_features(text, config.ngram):
                bf.add(_bloom_hash(feature))

        yield bf.save_bytes()
# ---
def test_remove_from_aggregate_called(self):
        def fake_remove_from_aggregate(context, aggregate, host):
            fake_remove_from_aggregate.called = True
        self.stubs.Set(self.conn._pool,
                       "remove_from_aggregate",
                       fake_remove_from_aggregate)

        self.conn.remove_from_aggregate(None, None, None)
        self.assertTrue(fake_remove_from_aggregate.called)
# ---
def matches_target(self, key_path):
        if isinstance(self.target_modules, str):
            compiled = re.compile(self.target_modules)
            return compiled.match(key_path) is not None
        elif self.target_modules is None:
            return True
        else:
            return any(key_path.endswith(target) for target in self.target_modules)
# ---
def compile_function(func):
        raise NotImplementedError(f"Cannot compile {func}")
# ---
def _wait_for_inflight(self) -> None:
        """Wait for in-flight scale-ups to complete without terminating anything.

        Test-only: Waits for all scale-up threads to complete.
        """
        # Wait for all threads in the container to finish
        self._threads.wait()
# ---
def sum_average(number):
 total = 0
 for value in range(1, number + 1):
    total = total + value
 average = total / number
 return (total,average)
# ---
def config():
    # Later: only send the necessary parts of config
    return jsonify(asdict(server.config))
# ---
def __repr__(self):
        return f"FlatMapOp(fn={_get_fn_name(self.fn)})"
# ---
def getName(self):
        return "NoncT({0},{1})".format(self.df, self.mu)
# ---
def ds_prediction_validate(ds_prediction: xr.Dataset, deep=False):
    warnings.warn("This checks nothing yet")
    ds_prediction_schema.validate(ds_prediction)
    ds_prediction_coords_schema.validate(ds_prediction.coords)
    if deep:
        _nan_test_deep(ds_prediction)
# ---
def _analytic_update(self, x, t, step_size):
    curr_sigma, _ = self.noise(t)
    next_sigma, _ = self.noise(t - step_size)
    dsigma = curr_sigma - next_sigma
    score = self.get_score(x, curr_sigma)
    stag_score = self._staggered_score(score, dsigma)
    probs = stag_score * self._transp_transition(x, dsigma)
    return _sample_categorical(probs)
# ---
def test_urlwith_fragment(self):
        req = Request("http://www.python.org/?qs=query#fragment=true")
        self.assertEqual("/?qs=query", req.get_selector())
        req = Request("http://www.python.org/#fun=true")
        self.assertEqual("/", req.get_selector())
# ---
def sign_tx(self, tx, spend_tx):
        scriptPubKey = bytearray(spend_tx.vout[0].scriptPubKey)
        if (scriptPubKey[0] == OP_TRUE):  # an anyone-can-spend
            tx.vin[0].scriptSig = CScript()
            return
        (sighash, err) = SignatureHash(spend_tx.vout[0].scriptPubKey, tx, 0, SIGHASH_ALL)
        tx.vin[0].scriptSig = CScript([self.coinbase_key.sign(sighash) + bytes(bytearray([SIGHASH_ALL]))])
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("x", Integer),
            Column("y", Integer),
            Column("z", String(50)),
        )
# ---
def __gt__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.greater(self, other)
# ---
def q_xt(self, x, move_chance):
    """Computes the noisy sample xt.

    Args:
      x: int torch.Tensor with shape (batch_size,
          diffusion_model_input_length), input. 
      move_chance: float torch.Tensor with shape (batch_size, 1).
    """
    move_indices = torch.rand(
      * x.shape, device=x.device) < move_chance
    xt = torch.where(move_indices, self.mask_index, x)
    return xt
# ---
def test_fillna(self):
        def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            B = df.A.fillna(5.0)
            return B.sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
# ---
def get_if_frequency(self):
        return self._if_frequency
# ---
def read_jsonl_gz():
    """Fixture to read JSONL gzipped files."""

    def _read(path: Path) -> list[dict]:
        records = []
        with gzip.open(path, "rt", encoding="utf-8") as handle:
            for line in handle:
                if line.strip():
                    records.append(json.loads(line))
        return records

    return _read
# ---
def add_lists(test_list, test_tup):
  res = tuple(list(test_tup) + test_list)
  return (res)
# ---
def __ge__(self, other: "Timestamp") -> bool:
        return self._epoch_ms >= other._epoch_ms
# ---
def test_conf_default(self):
        with mock.patch.object(memcache, 'ConfigParser', EmptyConfigParser):
            app = memcache.MemcacheMiddleware(FakeApp(), {})
        self.assertEqual(app.memcache_servers, '127.0.0.1:11211')
        self.assertEqual(app.memcache._allow_pickle, False)
        self.assertEqual(app.memcache._allow_unpickle, False)
        self.assertEqual(
            app.memcache._client_cache['127.0.0.1:11211'].max_size, 2)
# ---
def _unflatten_module(module, template):
        if isinstance(module, ModuleWithStateDictSerialization):
            module = module.unflatten_from_export(template)
            module = scan_aware_tree_map(
                _unflatten_module,
                module,
                template,
                is_leaf=lambda x: x is not module and isinstance(x, ModuleWithStateDictSerialization),
            )
        return module
# ---
def template_paired_end(bool):
    # bool, paired_end
    if bool:
        return "paired_end\t{}".format(str(bool)), None
    else:
        return "paired_end\t{}".format(str(bool)), ["-SE"]
# ---
def render (self):
    with Page (self.canvas) as pg:
      for obj in self.objects:
        coords = pg.next (obj.asset)
        with XlateFrame (self.canvas, obj.tile_type, *coords,
                         inset_by = "margin"):
          # print ("Obj: ", obj.asset)
          obj.render ()
# ---
def _tail_file(path: str, max_lines: int) -> str:
    try:
        with open(path, "r") as f:
            lines = f.readlines()
        return "".join(lines[-max_lines:])
    except Exception as exc:
        return f"<failed to read {path}: {exc}>"
# ---
def flatten_axes(array: NamedArray, old_axes: AxisSelection, new_axis: AxisSelector) -> NamedArray:
    pass
# ---
def __enter__(self):
    if self.level is not None:
      self.old_level = self.logger.level
      self.logger.setLevel(self.level)
    if self.handler:
      self.logger.addHandler(self.handler)
# ---
def _cleanup_stale_vpn_processes(self, vpn_router_ids):
        process_ids = [pid for pid in self.processes
                       if pid not in vpn_router_ids]
        for process_id in process_ids:
            self.destroy_process(process_id)
# ---
def move_zero(num_list):
    a = [0 for i in range(num_list.count(0))]
    x = [ i for i in num_list if i != 0]
    x.extend(a)
    return (x)
# ---
def synthetic_batch_iterator(
    *,
    rng: jax.Array,
    batch_size: int,
    seq_len: int,
    vocab_size: int,
) -> Iterator[dict[str, jax.Array]]:
    """Infinite generator of random token/label pairs."""

    def _step(key: jax.Array) -> dict[str, jax.Array]:
        tokens = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)
        return {"tokens": tokens[:, :-1], "labels": tokens[:, 1:]}

    while True:
        rng, key = jax.random.split(rng)
        yield _step(key)
# ---
def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Call method synchronously."""
        return self._method(*args, **kwargs)
# ---
import re
def remove_uppercase(str1):
  remove_upper = lambda text: re.sub('[A-Z]', '', text)
  result =  remove_upper(str1)
  return (result)
# ---
def chip_count(self) -> int:
        return self.count
# ---
def _hook_flip(self):
        self.update()
        self._window_flip()
# ---
def to_str(self):
        """
        Returns the string representation of the model
        """
        return pformat(self.to_dict())
# ---
def _version(self):
        ''' return the openshift version'''
        return self.openshift_cmd(['version'], output=True, output_type='raw')
# ---
def h_fs_get(_,path,eltName=''):
        from stat import S_ISDIR
        data = (escape(open(path).read())
                if not S_ISDIR(os.stat(path).st_mode)
                else [(p,S_ISDIR(os.stat(path+'/'+p).st_mode))
                        for p in os.listdir(path)])
        _.ws.send(json.dumps({"method":"fs_get","result":[path,data,eltName]}))
        pass
# ---
def find_version(filename):
    _version_re = re.compile(r"__version__ = '(.*)'")
    for line in open(filename):
        version_match = _version_re.match(line)
        if version_match:
            return version_match.group(1)
# ---
def sample_data():
    """Sample data for testing."""
    return list(range(1, 11))
# ---
def tree_unflatten(cls, aux, tree: Any) -> Any:
        assert len(tree) == 1
        # We don't want check shapes b/c there are intermediate states where the shape is wrong
        # e.g. in eqxi.while_loop
        with enable_shape_checks(False):
            return cls(tree[0], axis_names=aux)
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: Optional[str] = None
    ) -> HFCheckpointConverter["Olmo2Config"]:  # type: ignore
        return HFCheckpointConverter(
            self.__class__,
            reference_checkpoint=self.reference_checkpoint if ref_checkpoint is None else ref_checkpoint,
            trust_remote_code=True,
            tokenizer=ref_checkpoint if self.tokenizer is None else self.tokenizer,
            HfConfigClass=HfOlmo2Config,
        )
# ---
def shutdown(self) -> None:
        raise NotImplementedError
# ---
def _pick_first_element_of_missing_dims(mask: xr.DataArray, data: xr.DataArray):
    missing_dims = [di for di in mask.dims if di not in data.dims]
    if len(missing_dims) == 0:
        return mask
    else:
        return mask.isel({di: 0 for di in missing_dims})
# ---
def ascii_value(k):
  ch=k
  return ord(ch)
# ---
def linspace(
    axis: AxisSelector, *, start: float, stop: float, endpoint: bool = True, dtype: DTypeLike | None = None
) -> NamedArray:
    """
    Version of jnp.linspace that returns a NamedArray.
    If `axis` is a string, the default number of samples (50, per numpy) will be used.
    """
    if isinstance(axis, str):
        axis = Axis(axis, 50)
    return NamedArray(jnp.linspace(start, stop, axis.size, endpoint=endpoint, dtype=dtype), (axis,))
# ---
def match(self, item):
        return not self.subquery.match(item)
# ---
def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return dist.get_world_size()
# ---
def relationship(request, partner_username):
    partner = get_object_or_404(Profile, user__username=partner_username)
    if partner == request.profile:
        raise Http404  # Can't have relationship with yourself.
    account = request.profile.account(partner)
    if account:
        entries = account.entries 
        balance = account.balance
    else:
        entries = []
        balance = 0
    profile = partner  # For profile_base.html.
    return locals()
# ---
def main():
    pass
# ---
def broadcast_arrays_and_return_axes(
    *arrays: NamedOrNumeric | None, require_subset: bool = True, ensure_order: bool = True
) -> tuple[tuple[NamedOrNumeric | None, ...], tuple[Axis, ...]]: ...
# ---
def test_reduce_empty(backend):
    """Test reduce on empty dataset."""
    ds = Dataset.from_list([]).reduce(sum)
    results = list(Backend.execute(ds, context=backend))
    assert len(results) == 0
# ---
def increment(self, amount: int = 1) -> int:
        self._value += amount
        return self._value
# ---
def stats(x):
        # note dtype matches output_dtypes in apply_gufunc below
        return nxp.mean(x, axis=-1, dtype=np.float32)
# ---
def init(
        cls,
        axis: AxisSpec,
        eps: float = 1e-5,
        *,
        use_weight: bool = True,
        use_bias: bool = True,
        dtype: jnp.dtype | None = None,
    ):
        if use_weight:
            weight = hax.ones(axis)
        else:
            weight = None

        if use_bias:
            bias = hax.zeros(axis)
        else:
            bias = None

        return cls(axis, weight, bias, eps, dtype)
# ---
import math
def volume_sphere(r):
  volume=(4/3)*math.pi*r*r*r
  return volume
# ---
def strip(s):
    return s.strip()
# ---
def visit_mover(self, element):
        children = self._get_clean_children(element)
        if len(children) == 2:
            base = self._visit(children[0])
            over = self._visit(children[1])
            return BracedNode(f"\\overset{{{over}}}{{{base}}}")
        return TextNode("")
# ---
def scan_compatible_fn(carry, *args, **kwargs):
        return fn(carry, *args, **kwargs), None
# ---
def do_scan(init: CarryT, *args, **kwargs) -> tuple[CarryT, OutputT_co]:
            return haliax.scan(
                do_block,
                self.Block,
                remat=self.gradient_checkpointing,
                unroll=resolved_unroll,
            )(init, self.stacked, *args, **kwargs)
# ---
def make_map_blocks_key_function(*names):
    def key_function(out_key):
        out_coords = out_key[1:]
        return tuple((name, *out_coords) for name in names)

    return key_function
# ---
def run_with_env(_env=replica_env):
                    with temporary_env_vars(_env):
                        return callable_ep.callable(*callable_ep.args, **callable_ep.kwargs)
# ---
def __init__(self, side):
        self.side = side
# ---
def test_image():
    """Build test image with cloudpickle."""
    if not build_test_image():
        pytest.skip("Failed to build test image")
    return TEST_IMAGE
# ---
def testSmallMinRange(self):
        self.assertConfigureFails(HPCP(), {'minFrequency':1, 'splitFrequency':200})
# ---
def log(a: A) -> A:
    return wrap_elemwise_unary(jnp.log, a)
# ---
def compute_loss(model: LmHeadModel, input_ids):
            example = LmExample.causal(input_ids, eos_id=converter.tokenizer.eos_token_id)
            return model.compute_next_token_loss(example, key=None).scalar()
# ---
def create_manual_group(vms: list[MagicMock]) -> VmGroupProtocol:
        from iris.time_utils import Timestamp

        return ManualVmGroup(
            group_id="test-slice-001",
            scale_group="test-group",
            vms=vms,
            vm_registry=registry,
            created_at=Timestamp.from_ms(1234567890),
        )
# ---
def __init__(
        self,
        host_alias: str,
        command_str: str,
        sync_path: str,
        env_dict: dict[str, str] | None = None,
    ):
        self._lock = threading.Lock()
        self._process: subprocess.Popen | None = None
        self._command_str = command_str
        self._host_alias = host_alias
        self._sync_path = sync_path
        self._env_dict = env_dict or {}
# ---
def dtype(self) -> np.dtype:
        return self.array.dtype.numpy_dtype
# ---
def get_output_shape_for(self, input_shape):
        return (input_shape[0], input_shape[2])
# ---
def name_or_path(self):
        return self.tokenizer.name_or_path
# ---
def get_experiment_url(self) -> str:
        """Return the URL where the experiment can be viewed."""
        if self.prefix.startswith("gs://"):
            host = "https://marin.community/data-browser"
        else:
            host = f"http://localhost:{_get_local_data_browser_port()}"

        return host + "/experiment?path=" + urllib.parse.quote(self.executor_info_path)
# ---
def ba_axis(self) -> Axis:
        # [b | a]; per value head:  = (b), g uses a via Mamba2-style discretization
        return Axis("ba", self.num_v_heads * 2)
# ---
def varied_metrics_loss_fn(model, batch, key=None):
    """Loss function with different metric types for aggregation testing."""
    loss = hax.sum(batch * model.weight)
    metrics = {
        "accuracy": jnp.array(0.95),
        "num_tokens": jnp.array(128.0),
        "max_logit": jnp.array(5.0),
    }
    return loss, metrics
# ---
def to_numpy_histogram(self) -> tuple[np.ndarray, np.ndarray]:
        return np.array(self.bucket_counts), np.array(self.bucket_limits)
# ---
def _power_off(driver_info):
    """Turn the power OFF for this node.

    :param driver_info: the ipmitool parameters for accessing a node.
    :returns: one of ironic.common.states POWER_OFF or ERROR.
    :raises: IPMIFailure on an error from ipmitool (from _power_status call).

    """
    return _set_and_wait(states.POWER_OFF, driver_info)
# ---
def __le__(self, other, /):
        other = self._check_allowed_dtypes(other, "all", "__le__")
        if other is NotImplemented:
            return other
        return elemwise(nxp.less_equal, self, other, dtype=nxp.bool)
# ---
def min_Swaps(str1,str2) : 
    count = 0
    for i in range(len(str1)) : 
        if str1[i] != str2[i] : 
            count += 1
    if count % 2 == 0 : 
        return (count // 2) 
    else : 
        return ("Not Possible")
# ---
def update_fn(updates, state, params=None):
        buf = state.momentum_buffer
        buf = jax.tree.map(
            lambda m, g: None if g is None else momentum * m + (1 - momentum) * g,
            buf,
            updates,
            is_leaf=lambda x: x is None,
        )

        updates = jax.tree_map(lambda u: None if u is None else jnp.sign(u), buf, is_leaf=lambda x: x is None)

        return updates, ScaleByScionState(momentum_buffer=buf)
# ---
def test_skip_step_config_wrap_basic_setup():
    mock_optimizer = mock_optimizer_transform()
    skip_config = SkipStepConfig()

    wrapped_optimizer = skip_config.wrap(mock_optimizer)

    assert isinstance(wrapped_optimizer, optax.GradientTransformation)
    # Check if it has init and update attributes
    assert hasattr(wrapped_optimizer, "init")
    assert hasattr(wrapped_optimizer, "update")
# ---
def test_classify_indices_to_db(mock_rcw, mock_rw, mock_jw):
    with s.app.test_client() as c:
        resp = c.get('/api/v1/classify_documents/to_database?directory=test')

        assert mock_rcw.called
        assert mock_rw.called
        assert mock_jw.called
# ---
def median_numbers(a,b,c):
 if a > b:
    if a < c:
        median = a
    elif b > c:
        median = b
    else:
        median = c
 else:
    if a > c:
        median = a
    elif b < c:
        median = b
    else:
        median = c
 return median
# ---
def name(self):
        return self.__data['str_name']
# ---
def adam_transform():
                components = []
                if self.max_grad_norm:
                    components.append(optax.clip_by_global_norm(self.max_grad_norm))
                components.append(optax.scale_by_adam(self.beta1, self.beta2, self.epsilon))
                components.append(optax.scale(-adam_lr))
                optimizer = optax.chain(*components)
                return optimizer
# ---
def JumpToPreviewWindow():
  """ Jump the vim cursor to the preview window, which must be active. Returns
  boolean indicating if the cursor ended up in the preview window """
  vim.command( 'silent! wincmd P' )
  return vim.current.window.options[ 'previewwindow' ]
# ---
def loss_api(x_raw, w_raw):
        return fused_api.fused_cross_entropy_loss_and_logsumexp_penalty(
            x_raw.reshape(6, 4),
            y.reshape(6),
            w_raw,
            reduction="mean",
            logsumexp_weight=logsumexp_weight,
            implementation="xla",
        )
# ---
def __missing__(self, key):
        """Implements the default rule handling."""

        # If the default rule isn't actually defined, do something
        # reasonably intelligent
        if not self.default_rule or self.default_rule not in self:
            raise KeyError(key)

        return self[self.default_rule]
# ---
def parameter_axis_mapping(self) -> ResourceMapping:
        return self.mesh.resolved_param_mapping
# ---
def length_Of_Last_Word(a): 
    l = 0
    x = a.strip() 
    for i in range(len(x)): 
        if x[i] == " ": 
            l = 0
        else: 
            l += 1
    return l
# ---
def max_sum(tri, n): 
	if n > 1: 
		tri[1][1] = tri[1][1]+tri[0][0] 
		tri[1][0] = tri[1][0]+tri[0][0] 
	for i in range(2, n): 
		tri[i][0] = tri[i][0] + tri[i-1][0] 
		tri[i][i] = tri[i][i] + tri[i-1][i-1] 
		for j in range(1, i): 
			if tri[i][j]+tri[i-1][j-1] >= tri[i][j]+tri[i-1][j]: 
				tri[i][j] = tri[i][j] + tri[i-1][j-1] 
			else: 
				tri[i][j] = tri[i][j]+tri[i-1][j] 
	return (max(tri[n-1]))
# ---
def testGetFormattedField(self):
    """Tests the GetFormattedField function."""
    output_mediator = self._CreateOutputMediator()
    test_helper = TestFieldFormattingHelper(output_mediator)

    event, event_data, event_data_stream = (
        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))
    zone_string = test_helper.GetFormattedField(
        'zone', event, event_data, event_data_stream, None)
    self.assertEqual(zone_string, 'UTC')
# ---
def _quick_task_job(task_id: int):
    """Quick job that sleeps and returns."""
    import time as time_module

    time_module.sleep(2.0)
    print(f"Task {task_id} completed")
    return task_id
# ---
INT_BITS = 32
def left_Rotate(n,d):   
    return (n << d)|(n >> (INT_BITS - d))
# ---
def test_check_password(self):
        # Ensure given password is correct after unhashing.
        user = User.query.filter_by(email="ad@min.com").first()
        self.assertTrue(
            bcrypt.check_password_hash(user.password, "admin_user")
        )
        self.assertFalse(bcrypt.check_password_hash(user.password, "foobar"))
# ---
def test_expr_as_dict_key(self):
        expr = col("score") > 0.5
        d = {expr: "value"}
        assert d[expr] == "value"
# ---
def test_per_connection_plus_engine(self):
        canary = Mock()
        e1 = testing_engine(config.db_url)

        event.listen(e1, "before_execute", canary.be1)

        conn = e1.connect()
        event.listen(conn, "before_execute", canary.be2)
        conn.execute(select([1]))

        eq_(canary.be1.call_count, 1)
        eq_(canary.be2.call_count, 1)

        conn._branch().execute(select([1]))
        eq_(canary.be1.call_count, 2)
        eq_(canary.be2.call_count, 2)
# ---
def test_profiler_hmac(self):
        hmac = "secret"
        prof = profiler._Profiler(hmac, base_id="1", parent_id="2")
        self.assertEqual(hmac, prof.hmac_key)
# ---
def uipath(self, f):
        """Convert repo path to a display path.  If patterns or -I/-X were used
        to create this matcher, the display path will be relative to cwd.
        Otherwise it is relative to the root of the repo."""
        return (self._relativeuipath and self.rel(f)) or self.abs(f)
# ---
def check_none(test_tup):
  res = any(map(lambda ele: ele is None, test_tup))
  return (res)
# ---
def next_power_of_two(n: int) -> int:
    return 1 << (n - 1).bit_length()
# ---
def get_profile():
    return addon.getAddonInfo('profile').decode('utf-8')
# ---
def read_config():
    with open('config.cfg', 'r') as cfg_file:
        lines = cfg_file.readlines()
        lines = [
            line.strip().replace(' ', '').split('=')
            for line in lines
            if line.strip() and '=' in line
        ]
        cfg = {key:try_parse(value) for key,value in lines}
        return cfg
# ---
def enterTransitionToCostume(self):
        pass
# ---
def fit_baseline_amp(self,z_data,lam,p,niter=10):
		'''
		for this to work, you need to analyze a large part of the baseline
		tune lam and p until you get the desired result
		'''
		return self._baseline_als(np.absolute(z_data),lam,p,niter=niter)
# ---
def max_len_sub( arr, n): 
	mls=[] 
	max = 0
	for i in range(n): 
		mls.append(1) 
	for i in range(n): 
		for j in range(i): 
			if (abs(arr[i] - arr[j]) <= 1 and mls[i] < mls[j] + 1): 
				mls[i] = mls[j] + 1
	for i in range(n): 
		if (max < mls[i]): 
			max = mls[i] 
	return max
# ---
def counting_sample_batch(lesson_id, n_examples, n_generations, mode, rng):
            batch_data, metrics = original_sample_batch(lesson_id, n_examples, n_generations, mode, rng)
            if batch_data is None or metrics is None:
                return None, None
            self._track_rollout_generation()
            # Add metadata about rollout
            metrics["rollout_number"] = self.rollouts_generated
            return batch_data, metrics
# ---
def flatten(test_tuple): 
	for tup in test_tuple: 
		if isinstance(tup, tuple): 
			yield from flatten(tup) 
		else: 
			yield tup 
def count_element_freq(test_tuple):
  res = {}
  for ele in flatten(test_tuple):
    if ele not in res:
      res[ele] = 0
    res[ele] += 1
  return (res)
# ---
def test_beam_candidate_with_edits():
    m = Mutation(start=4, end=5, replacement="2", node_type="Constant", original="1")
    c = BeamCandidate(source="x = 2\n", score=-1.0, depth=1, edits=(m,))
    assert len(c.edits) == 1
    assert c.edits[0].replacement == "2"
# ---
def _make_table(pages=8, seqs=4, page_size=2, pages_per_seq=2):
    return PageTable.init(pages, seqs, page_size, pages_per_seq)
# ---
def interference_genetic_modification(
        testapp,
        lab,
        award,
        document,
        target):
    item = {
        'award': award['@id'],
        'documents': [document['@id']],
        'lab': lab['@id'],
        'category': 'interference',
        'purpose': 'repression',
        'method': 'RNAi',        
        'modified_site_by_target_id': target['@id']
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def func(a):
        np.ones(100_000_000)  # blow memory
        return a
# ---
def is_octagonal(n): 
	return 3 * n * n - 2 * n
# ---
def user_twitter_handle(self):
        return self._get_profile().twitter_handle
# ---
def _generate_digest_challenge(ts, secret, realm, opaque, stale=False):
	nonce = _generate_nonce(ts, secret)
	return 'Digest %s' % (_format_kvpairs(
		realm=realm,
		qop='auth',
		nonce=nonce,
		opaque=opaque,
		algorithm='MD5',
		stale='true' if stale else 'false'
	),)
# ---
def config_to_dict(config: config_pb2.IrisClusterConfig) -> dict:
    """Convert config to dict for YAML serialization."""
    return MessageToDict(config, preserving_proto_field_name=True)
# ---
def status(self) -> JobStatus:
        if self._terminated.is_set():
            return JobStatus.STOPPED
        if not self._future.done():
            return JobStatus.RUNNING
        exc = self._future.exception()
        if exc is not None:
            return JobStatus.FAILED
        return JobStatus.SUCCEEDED
# ---
def test_not_true(self):
        expr = ~(col("flag") == True)  # noqa: E712
        assert expr.evaluate({"flag": True}) is False
        assert expr.evaluate({"flag": False}) is True
# ---
def _nodes_equal(a: ast.AST, b: ast.AST) -> bool:
    """Check structural equality of two AST nodes (recursively).

    Uses ast.dump for a deep comparison that ignores source positions
    and formatting differences.
    """
    return ast.dump(a) == ast.dump(b)
# ---
def print_generic_desc(desc):
    for field in desc:
        print_json_field(field, desc[field])
# ---
def test_init_and_get_instance():
    with MultitonScope():
        instance = DummyMultiton.init_instance(42)
        got_instance = DummyMultiton.get_instance()
        assert instance is got_instance
        assert instance.value == 42
# ---
def activate(self, func):
        evaldict = {'ldap3mock': self, 'func': func}
        return get_wrapped(func, _wrapper_template, evaldict)
# ---
def validate_name_with_item_group(self):
		# causes problem with tree build
		if frappe.db.exists("Item Group", self.name):
			frappe.throw(
				_("An Item Group exists with same name, please change the item name or rename the item group"))
# ---
def slow_create(tags=None):
            time.sleep(0.5)  # Simulate slow VM creation
            return original_create(tags)
# ---
def _get_random_inputs(config: MixtralConfig, override_Pos=None):
    Embed = config.Embed
    if override_Pos is not None:
        Pos = override_Pos
    else:
        Pos = config.max_Pos
    Batch = hax.Axis("batch", 2)
    x = hax.random.normal(random.PRNGKey(0), (Batch, Pos, Embed))
    mask = AttentionMask.causal()
    return x, mask
# ---
def where(
    condition: NamedOrNumeric | bool,
    x: NamedOrNumeric,
    y: NamedOrNumeric,
) -> NamedArray: ...
# ---

def digitSum(s):
    """Task
    Write a function that takes a string as input and returns the sum of the upper characters only'
    ASCII codes.

    Examples:
        digitSum("") => 0
        digitSum("abAB") => 131
        digitSum("abcCd") => 67
        digitSum("helloE") => 69
        digitSum("woArBld") => 131
        digitSum("aAaaaXa") => 153
    """
    if s == "": return 0
    return sum(ord(char) if char.isupper() else 0 for char in s)
# ---
def signbit(a: A) -> A:
    return wrap_elemwise_unary(jnp.signbit, a)
# ---
def remember(self, request, principal, **kw):
		return self.match(request).remember(request, principal, **kw)
# ---
def trace_with_result_func(a, i=10):
    return (a, i)
# ---
def list_services() -> list[ServiceInfo]:
    """List all registered services."""
    register_services()
    return list(SERVICES.values())
# ---
def clause(self):
        clause, subvals = self.subquery.clause()
        if clause:
            return f'not ({clause})', subvals
        else:
            # If there is no clause, there is nothing to negate. All the logic
            # is handled by match() for slow queries.
            return clause, subvals
# ---
def header(self, text, level, raw=None):
        """Rendering header/heading tags like ``<h1>`` ``<h2>``.

        :param text: rendered text content for the header.
        :param level: a number for the header level, for example: 1.
        :param raw: raw text content of the header.
        """
        return '<h%d>%s</h%d>\n' % (level, text, level)
# ---
def do_block(carry, block, *args, **kwargs):
            carry, out = block(carry, *args, **kwargs)
            return carry, out
# ---
def Average(lst): 
    return sum(lst) / len(lst)
# ---
def less_equal(x1, x2, /):
    x1, x2 = _promote_scalars(x1, x2, "less_equal")
    return elemwise(nxp.less_equal, x1, x2, dtype=nxp.bool)
# ---
def always(root, cwd):
    return alwaysmatcher(root, cwd)
# ---
def number_of_substrings(str): 
	str_len = len(str); 
	return int(str_len * (str_len + 1) / 2);
# ---
def test_olmo2_lm_head_model(num_kv_heads):
    config = _get_olmo2_config(num_kv_heads=num_kv_heads)
    Batch = hax.Axis("batch", 2)
    Vocab = hax.Axis("vocab", 1000)
    Pos = config.max_Pos
    input_ids = hax.random.randint(random.PRNGKey(0), (Batch, Pos), 0, Vocab.size)
    mask = AttentionMask.causal()

    olmo2_model = Olmo2LMHeadModel.init(Vocab=Vocab, config=config, key=random.PRNGKey(0))
    out = olmo2_model(input_ids, mask)
    assert out.array.shape == (Batch.size, Pos.size, Vocab.size)
# ---
def visit_node_generations(dag):
    """Return a generator that visits the nodes in the DAG in groups of topological generations."""
    nodes = {n: d for (n, d) in dag.nodes(data=True)}
    for names in nx.topological_generations(dag):
        gen = [(name, nodes[name]) for name in names if not skip_node(name, dag, nodes)]
        if len(gen) > 0:
            yield gen
# ---
def forward(
        self,
        fts: Float[torch.Tensor, "batch channel height width"],
        resolution: tuple[Lat, Lon],
    ) -> Float[torch.Tensor, "batch channel+3 height width"]:
        grid = (
            make_3d_coordinate_grid(*resolution)
            .to(fts.device)
            .expand(fts.shape[0], -1, -1, -1)
        )
        return torch.cat((fts, grid), dim=1)
# ---
def empty_autoscaler(scale_group_config):
    """Empty autoscaler ready for scale-up tests."""
    manager = make_mock_vm_manager()
    group = ScalingGroup(scale_group_config, manager, scale_up_cooldown=Duration.from_ms(0))
    autoscaler = make_autoscaler({"test-group": group})
    yield autoscaler
    autoscaler.shutdown()
# ---
def path(self) -> str:
        """Returns the URL path to mount the application to when serving multiple applications."""
        return "/iris.actor.ActorService"
# ---
def __init__(self, handles: list[ActorHandle]):
        self._handles = handles
        self._yielded = False
# ---
def __len__(self) -> int:
        """
        Returns the final length of the data store.
        May raise if the length is not known.
        """
# ---
def _search_related(self, records, operator, value):
        """ Determine the domain to search on field ``self``. """
        return [('.'.join(self.related), operator, value)]
# ---
def test_actual_sizeof():
    d1 = {"a": 1, "b": 2}
    d2 = {"a": "this is a string", "b": "this is another string"}

    assert actual_sizeof(d1) < actual_sizeof(d2)
# ---
def get_ls_l_header():
    return (BOLD() +
            'State' + DELIMITER('   ') +
            'Last modified' + DELIMITER('       ') +
            'Size' + DELIMITER('      ') +
            'Name' + DELIMITER(' (') +
            'ID' + DELIMITER(')') +
            ENDC())
# ---
def _on_eval_start(self, pl_module: LightningModule) -> None:
        """Use EMA weights for evaluation.

        Parameters
        ----------
        pl_module: LightningModule
            The LightningModule instance.

        """
        if self.ema_initialized and self.eval_with_ema:
            self.replace_model_weights(pl_module)
# ---
def __truediv__(self, other: str) -> "InputName":
        """Alias for `cd`. That looks more Pythonic."""
        return InputName(self, name=other)
# ---
def test_attach_volume_not_available(self):
        server = dict(id='server001')
        volume = dict(id='volume001', status='error', attachments=[])

        with testtools.ExpectedException(
            openstack.cloud.OpenStackCloudException,
            "Volume %s is not available. Status is '%s'" % (
                volume['id'], volume['status'])
        ):
            self.cloud.attach_volume(server, volume)
        self.assertEqual(0, len(self.adapter.request_history))
# ---
def max(x, axis=0):
    return _Nmax(x, axis)
# ---
def __isLanguageAvailable(self, code=None, language_name=None):
        """
        Check if a language is available
        """
        if code is None and language_name is None:
            raise Exception("Error evaluating the correct language")

        if code is not None and code.lower() in AVAILABLE_LANGUAGES:

            return True

        if language_name is not None and language_name.lower() in AVAILABLE_LANGUAGES_NAMES:
            return True

        return False
# ---
def make_render_children(separator: str) -> Render:
    def render_children(
        node: RenderTreeNode,
        context: RenderContext,
    ) -> str:
        return separator.join(child.render(context) for child in node.children)

    return render_children
# ---
def exists(url, **kwargs) -> bool:
    """Check if a file exists on a remote filesystem."""
    fs, path = fsspec.core.url_to_fs(url, **kwargs)
    return fs.exists(path)
# ---
def validate_url(url: str) -> str | None:
    """Validate URL and return error message if invalid."""
    try:
        parsed = urlparse(url)
        if parsed.scheme not in ('http', 'https'):
            return f"Invalid URL scheme: {parsed.scheme}. Only http/https allowed."
        if not parsed.netloc:
            return "Invalid URL: missing host"
        return None
    except Exception as e:
        return f"Invalid URL: {e}"
# ---
def selected_pane(self):
        return self.panes[self.selected_pane_index] if self.panes else None
# ---
def task_schedule_status(self, task: ControllerTask, context: SchedulingContext) -> TaskScheduleResult:
        """Get the current scheduling status of a task (for dashboard display)."""
        ...
# ---
def find_volume_by_name(self, volume, mounts=False):
        ''' return the index of a volume '''
        volumes = []
        if mounts:
            volumes = self.get_volume_mounts()
        else:
            volumes = self.get_volumes()
        for exist_volume in volumes:
            if exist_volume['name'] == volume['name']:
                return exist_volume

        return None
# ---
def concatenate_elements(test_tup):
  res = tuple(i + j for i, j in zip(test_tup, test_tup[1:]))
  return (res)
# ---
def test_dontreadfrominput_buffer_python2():
    from _pytest.capture import DontReadFromInput
    f = DontReadFromInput()
    with pytest.raises(AttributeError):
        f.buffer
    f.close()
# ---
def list_jobs(self) -> list[JobInfo]:
        return [job.get_info() for job in self._jobs.values()]
# ---
def convert_to_display_name(self, value, record=None):
        assert record, 'Record expected'
        return Datetime.to_string(Datetime.context_timestamp(record, Datetime.from_string(value)))
# ---
import re 
regex = '^[aeiouAEIOU][A-Za-z0-9_]*'
def check_str(string): 
	if(re.search(regex, string)): 
		return ("Valid") 
	else: 
		return ("Invalid")
# ---
import math
def lateralsurface_cone(r,h):
  l = math.sqrt(r * r + h * h)
  LSA = math.pi * r  * l
  return LSA
# ---
def pos_count(list):
  pos_count= 0
  for num in list: 
    if num >= 0: 
      pos_count += 1
  return pos_count
# ---
def __init__(
        self,
        cache_dir: str,
        exemplar: T_co,
        ledger: "CacheLedger",
    ):
        super().__init__()
        self.cache_dir = cache_dir
        self.ledger = ledger
        self._exemplar = exemplar

        if not ledger.is_finished:
            raise RuntimeError(f"Cache at {cache_dir} is not finished.")

        self._store = TreeStore.open(self._exemplar, self.cache_dir, mode="r", cache_metadata=False)
# ---
def _msgprint(msg, verbose):
	if verbose:
		msgprint(msg, raise_exception=True)
	else:
		raise frappe.ValidationError(msg)
# ---
def disabled(self):
        if not app.player.paused:
            self.plugin_on_paused()
# ---
def _add_row(self, id, address, reclen):
		if id in self._addresses: # Something's wrong here
			log.warning("Multiple instances of row %r found in %s" % (id, self.file.name))
		self._addresses[id] = (address, reclen)
# ---
def __iter__(self):
        """Flat map over all chunks."""
        for chunk_data in self.iter_chunks():
            yield from chunk_data
# ---
def test_cpu_defaults(self):
        resources = ResourceConfig()
        spec = convert_resources(resources)
        assert spec.cpu == 1
        assert spec.memory == "128m"
        assert spec.disk == "1g"
        assert spec.device is None
# ---
def get_tracker(name: str) -> Tracker: ...
# ---
def max_queued_tokens(self) -> int:
        """Expose queue capacity from ``TokenQueue``."""
        return self.tqueue.max_queued_tokens
# ---
def extra_config_args(request) -> list[str]:
    return request.param
# ---
def test_validate_invalid_password(self):
        # Ensure user can't login when the pasword is incorrect.
        with self.client:
            response = self.client.post(
                "/login",
                data=dict(email="ad@min.com", password="foo_bar"),
                follow_redirects=True,
            )
        self.assertIn(b"Invalid email and/or password.", response.data)
# ---
def __delitem__(self, key):
        del self._data[id(key)]
# ---
def unload(self):
        """Unload the inference model to free up resources."""
        self.inference_context.unload()
# ---
def copy(self):
        return DSN(self.get_url())
# ---
def visitdir(self, dir):
        dir = normalizerootdir(dir, "visitdir")
        visit1 = self._m1.visitdir(dir)
        if visit1 == "all":
            return self._m2.visitdir(dir)
        # bool() because visit1=True + visit2='all' should not be 'all'
        return bool(visit1 and self._m2.visitdir(dir))
# ---
def available_gpus(self) -> int:
        """Available GPU count after subtracting committed resources."""
        return get_gpu_count(self.metadata.device) - self.committed_gpu
# ---
def power_digit_sum(exponent):
    power_of_2 = str(2 ** exponent)
    return sum([int(x) for x in power_of_2])
# ---
def convert_to_display_name(self, value, record=None):
        raise NotImplementedError()
# ---
def _matches_target(key_path, config: QuantizationConfig) -> bool:
    if not key_path:
        key = ""
    else:
        key = _key_path_to_str(key_path[-1:])

    if config.targets is None:
        return True
    if isinstance(config.targets, list):
        return key in config.targets

    import re

    key_path_str = _key_path_to_str(key_path)
    return re.match(config.targets, key_path_str) is not None
# ---
def test_setup_only():
    """`setup_only` fixture works as expected"""

    def setup_only(context):
        """A fixture with no `teardown()`."""

        def setup():
            """Add something to the context."""
            assert context == {}
            context.squee = "kapow"

        return setup

    @with_fixture(setup_only)
    def case(context):
        """Check that the context has been set up."""
        assert context == {"squee": "kapow"}

    case()
# ---
def test_map_shard_empty_result(backend):
    """Test map_shard that filters everything out."""

    def filter_all(items):
        for _ in items:
            pass  # Consume but don't yield
        return iter([])  # Return empty iterator

    ds = Dataset.from_list([list(range(1, 6))]).flat_map(lambda x: x).map_shard(filter_all)
    result = list(Backend.execute(ds, context=backend))
    assert result == []
# ---
def at(cls, timestamp: Timestamp, source: str, data: str) -> "LogLine":
        """Create a log line with an explicit timestamp."""
        return cls(
            timestamp=datetime.fromtimestamp(timestamp.epoch_seconds(), tz=timezone.utc),
            source=source,
            data=data,
        )
# ---
def docs():
    test_resources = Path(__file__).parent.joinpath("resources", "docs")
    docs = {}
    for doc_file in test_resources.glob("*.txt"):
        docs[doc_file.stem] = doc_file.read_text()
    return docs
# ---
def test_lat_lon_output(self):
        """Asserts that the vertices in the lat-lon output are in the
        right order (lat before long)."""
        for vertex in self.polycircle.to_lat_lon():
            assert_almost_equal(vertex[0], self.latitude, places=2)
            assert_almost_equal(vertex[1], self.longitude, places=2)
# ---
def __call__(self, x: hax.NamedArray) -> hax.NamedArray:
        return hax.sin(self.freq * x)
# ---
def test_delete_permission(self):
        """
        Tests that staff cannot delete entries
        """
        self.assertFalse(self.creator_admin.has_delete_permission(self.request))
# ---
def freeze(ref: NamedRef) -> NamedArray:
    """Freeze the reference and return its current contents."""
    return ref.value()
# ---
def ping(self):
        return True
# ---
def rearrange(array: NamedArray, expression: str, **bindings: AxisSelector | int) -> NamedArray:
    pass
# ---
def test_directed_raises(self):
        with pytest.raises(nx.NetworkXNotImplemented):
            dir_G = nx.gn_graph(n=5)
            prev_cc = None
            edge = self.pick_add_edge(dir_G)
            insert = True
            nx.incremental_closeness_centrality(dir_G, edge, prev_cc, insert)
# ---
def __init__(self, source: ShardedDataSource[T_co], fn: Callable[[T_co], T]):
        self.source = source
        self.fn = fn
        self._transform = _MapTransform(fn)
# ---
def tracker1(name):
            def go(*args, **kw):
                canary1.append(name)
            return go
# ---
def get_lm_head(self) -> hax.NamedArray:
        """
        The language modeling head of the model. Should have shape {Embed, Vocab}.
        """
        raise NotImplementedError("get_lm_head not implemented")
# ---
def method2(self, d, e):
        return d - e
# ---
def test_count_statements_nested():
    tree = ast.parse("if True:\n    x = 1\n    y = 2\n")
    # If + Assign + Assign = 3 statements.
    assert count_statements(tree) == 3
# ---
def dump_to_store(self, store, encoder=None, sync=True):
        """Store dataset contents to a backends.*DataStore object."""
        variables, attrs = conventions.encode_dataset_coordinates(self)
        if encoder:
            variables, attrs = encoder(variables, attrs)
        store.store(variables, attrs)
        if sync:
            store.sync()
# ---
def metadata(self) -> Dict[str, Any]:
        return {
            "tokenizer": self.bt.metadata,
            "processor": self.feature_extractor.to_dict(),
        }
# ---
def test_bool(self):
        prio_set_list = event._PrioritizedSetList()

        assert bool(prio_set_list) is False

        prio_set_list.add(0, None)
        assert bool(prio_set_list) is True
# ---
def visit_math(self, element):
        return self._visit_children(element)
# ---
def decodeNextSeg(self):
        # if we're not running send an instant kill switch.
        if ( not self.running ): return -1

        # try to grab a segment from the cache to decode.
        seg = None
        try:
            seg = self.cache.pop()
        except:
            pass	

        if ( seg == None ) and ( self.all_decoded ):
            return -1
        return seg
# ---
def _promote_scalars(x1, x2, op):
    """Promote at most one of x1 or x2 to an array from a Python scalar"""
    x1_is_scalar = isinstance(x1, (int, float, complex, bool))
    x2_is_scalar = isinstance(x2, (int, float, complex, bool))
    if x1_is_scalar and x2_is_scalar:
        raise TypeError(f"At least one of x1 and x2 must be an array in {op}")
    elif x1_is_scalar:
        x1 = x2._promote_scalar(x1)
    elif x2_is_scalar:
        x2 = x1._promote_scalar(x2)
    return x1, x2
# ---
def is_null(self) -> IsNullExpr:
        return IsNullExpr(self)
# ---
def __call__(self, carry):
            return carry + self.w
# ---
def notify_task_update(self, request: cluster__pb2.Controller.NotifyTaskUpdateRequest, ctx: RequestContext) -> cluster__pb2.Empty:
        raise ConnectError(Code.UNIMPLEMENTED, "Not implemented")
# ---
def execute(self, hook):
        """
        Executes the check.

        :param hook: The name of the hook being run.
        :type hook: :class:`str`
        :returns: ``True`` if the check passed, ``False`` if not.
        :rtype: :class:`bool`

        """
        pass
# ---
def __init__(self, content):
        '''Service constructor'''
        super(Service, self).__init__(content=content)
# ---
def testRaiseTypeAndArg(self):
    self.assertEqual((0, 'foo\n'), _GrumpRun(textwrap.dedent("""\
        try:
          raise KeyError('foo')
          print 'bad'
        except KeyError as e:
          print e""")))
# ---
def test_duplicate_trace_disallow(self, mock_start, mock_stop):

        @profiler.trace("test")
        def trace_me():
            pass

        self.assertRaises(
            ValueError,
            profiler.trace("test-again", allow_multiple_trace=False),
            trace_me)
# ---
def _match_less_than_or_equal(search_base, attribute, value, candidates):
        matches = list()
        for entry in candidates:
            dn = entry.get("dn")
            if not dn.endswith(search_base):
                continue

            value_from_directory = entry.get("attributes").get(attribute)
            if str(value_from_directory) <= str(value):
                entry["type"] = "searchResEntry"
                matches.append(entry)

        return matches
# ---
def __init__(self):
        self.lf = 0.2  # Learning factor lambda
        self.data = []  # The features' values for all the games
        self.rewards = []  # Reward values for moving from 1 state to the next
        self.rt = np.array([])
        self.max_iter = 50
# ---
def foreign_tasks(self, tasks, person, roles):
        """List of other instructors' tasks, per event."""
        return [
            task.event.task_set.filter(role__in=roles)
                               .exclude(person=person)
                               .select_related('person')
            for task in tasks
        ]
# ---
def test_profile__loader__1gb(train_config, loader_version, benchmark):
    cfg = train_config

    with make_loader(cfg, version=loader_version) as loader:

        @benchmark
        def bench():
            indices = np.random.randint(0, len(loader), size=len(loader))
            for idx in indices:
                _ = loader.dataset[int(idx)]
# ---
def Check_Solution(a,b,c): 
    if (a == c): 
        return ("Yes"); 
    else: 
        return ("No");
# ---
def testSComplexRandom(self):
    for _ in range(10):
      n, k, m = np.random.randint(1, 100, size=3)
      x = self._randMatrix(n, k, np.complex64)
      y = self._randMatrix(k, m, np.complex64)
      self._testCpuMatmul(x, y)
# ---
def can_fuse_primitive_ops(
    primitive_op1: PrimitiveOperation, primitive_op2: PrimitiveOperation
) -> bool:
    if is_fuse_candidate(primitive_op1) and is_fuse_candidate(primitive_op2):
        return primitive_op1.num_tasks == primitive_op2.num_tasks
    return False
# ---
def to_task_spec(self) -> list[str | dict]:
        """
        Convert task specifications to a list of dictionaries or strings.

        Returns:
            List of task specifications, with TaskConfig objects converted to dictionaries
        """
        return [task.to_dict() if isinstance(task, TaskConfig) else task for task in self.task_spec]
# ---
def remove_all_iris_containers(self) -> int: ...
# ---
def match(self, item):
        if self.field not in item:
            return False
        timestamp = float(item[self.field])
        date = datetime.fromtimestamp(timestamp)
        return self.interval.contains(date)
# ---
def ptp(array: NamedArray, axis: AxisSelection | None = None, *, where: NamedArray | None = None) -> NamedArray:
    return wrap_reduction_call(jnp.ptp, array, axis, where, single_axis_only=False, supports_where=True)
# ---
def create_executor(temp_dir: str):
    """Create an Executor that lives in a temporary directory."""
    return Executor(prefix=temp_dir, executor_info_base_path=temp_dir)
# ---
def lsof_check():
    pid = os.getpid()
    try:
        out = py.process.cmdexec("lsof -p %d" % pid)
    except (py.process.cmdexec.Error, UnicodeDecodeError):
        # about UnicodeDecodeError, see note on pytester
        pytest.skip("could not run 'lsof'")
    yield
    out2 = py.process.cmdexec("lsof -p %d" % pid)
    len1 = len([x for x in out.split("\n") if "REG" in x])
    len2 = len([x for x in out2.split("\n") if "REG" in x])
    assert len2 < len1 + 3, out2
# ---
def run():
    """

    """
    # figure 1 from dudko 2008
    data = Example_Data.Dudko2008Fig1_Probabilities()
    PlotFit(data,"../Out/Dudko2008_Fig1")
    # figure 2 frm dudko 2008
    data = Example_Data.Dudko2008Fig2_Probabilities()
    PlotFit(data,"../Out/Dudko2008_Fig2")
# ---
def is_nonagonal(n): 
	return int(n * (7 * n - 5) / 2)
# ---
def get_data(indices):
                return get_local_data_for_leaf(indices, leaf_index)
# ---
def test_check_share_in_use_invalid_conn(self):
        drv = self._driver
        share = drv._check_share_in_use(':8989', '/dir')
        if share:
            self.fail('Unexpected share detected.')
# ---
def _log_samples_hook(info: levanter.callbacks.StepInfo):
            rollouts = self.data_loader._last_rollouts
            if rollouts is not None:
                self._log_samples(trainer, info.step, rollouts)
# ---
def remove_kth_element(list1, L):
    return  list1[:L-1] + list1[L:]
# ---
def test_mem_write_byte_calls_char_generator_bottom_right(self):
        self.mda.mem_write_byte(3998, 0xFF)
        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_GREEN, MDA_BLACK))
# ---
def convert_to_levanter_task_config(tasks: Sequence[EvalTaskConfig]) -> list[TaskConfig]:
    """
    Convert a list of EvalTaskConfig to a list of TaskConfig that Levanter's eval_harness expects.
    """
    return [
        TaskConfig(
            task=task.name,
            num_fewshot=task.num_fewshot,
            task_alias=task.task_alias,
        )
        for task in tasks
    ]
# ---
def delete_on_exit(context_dir: str) -> None:
    if context_dir not in CONTEXT_DIRS and is_local_path(context_dir):
        atexit.register(lambda: shutil.rmtree(context_dir, ignore_errors=True))
        CONTEXT_DIRS.add(context_dir)
# ---
def config_from_hf_config(self, hf_config, overrides: Optional[dict] = None) -> LevConfig:
        config = self.LevConfigClass.from_hf_config(hf_config)
        if overrides is not None:
            config = dataclasses.replace(config, **overrides)  # type: ignore
        return config
# ---
def update(self, other):
		"""
		Update file from iterable other
		"""
		for k in other:
			self[k] = other[k]
# ---
def return_host(self, host: str) -> None:
        """Return a host to the available pool.

        Called when a VM group is terminated to make the host available again.
        Safe to call multiple times for the same host.
        """
        self._return_hosts([host])
# ---
def _ensure_batched(x: jax.Array) -> tuple[jax.Array, bool]:
    """Ensure `x` has a leading batch dimension.

    Template convention:
    - if `x.ndim == 1`, treat it as a single example and add a batch axis.
    - otherwise treat it as already batched.

    Returns:
        (x_batched, added_batch_dim)
    """

    if x.ndim == 1:
        return x[None, :], True
    return x, False
# ---
def decorator(fn):
        if split not in SNAPSHOT_PATHS:
            raise ValueError(f"Invalid split: {split}")

        input_path = SNAPSHOT_PATHS[split]["inputs"]

        files = [os.path.splitext(os.path.basename(f))[0] for f in os.listdir(input_path) if f.endswith(ext)]
        return pytest.mark.parametrize("input_name", files)(fn)
# ---
def total_noise(self, t):
    """
    Total noise ie \int_0^t g(t) dt + g(0)
    """
    pass
# ---
def is_nested_node(el):
            return el and el.name in ["ol", "ul", "li", "table", "thead", "tbody", "tfoot", "tr", "td", "th"]
# ---
def test_concat_series_str(self):
        def test_impl():
            df1 = pq.read_table('example.parquet').to_pandas()
            df2 = pq.read_table('example.parquet').to_pandas()
            A3 = pd.concat([df1.two, df2.two])
            return (A3 == 'foo').sum()

        hpat_func = self.jit(test_impl)
        self.assertEqual(hpat_func(), test_impl())
        self.assertEqual(count_array_REPs(), 0)
        self.assertEqual(count_parfor_REPs(), 0)
# ---
def __init__(self, pluginname, account=None):
        self.cookies = {}
        self.plugin = pluginname
        self.account = account
# ---
def where(
    condition: NamedArray,
    *,
    fill_value: int,
    new_axis: Axis,
) -> tuple[NamedArray, ...]: ...
# ---
def test_fixed_window(self):
        storage = MemcachedStorage("memcached://localhost:22122")
        limiter = FixedWindowRateLimiter(storage)
        per_min = RateLimitItemPerSecond(10)
        start = time.time()
        count = 0

        while time.time() - start < 0.5 and count < 10:
            assert limiter.hit(per_min)
            count += 1
        assert not limiter.hit(per_min)

        while time.time() - start <= 1:
            time.sleep(0.1)
        assert limiter.hit(per_min)
# ---
def extra(self, response):
        """Add extra info to response."""
        if 'facets' in self.hits:
            response['_facets'] = self.hits['facets']
        if 'aggregations' in self.hits:
            response['_aggregations'] = self.hits['aggregations']
# ---
def __init__(self, fallback):
        self._fallback = fallback
# ---
def broadcast_qs(_, ps, q, s):
                stack_n = ps[0]
                if partition_grads_into_blocks:
                    # add leading dim for stacked partitions
                    q = jax.tree.map(lambda x: jnp.repeat(jnp.expand_dims(x, 0), stack_n, axis=0), q)
                if s > 0:
                    # add leading dim if we're scanning this layer
                    q = jax.tree.map(lambda d: jnp.repeat(jnp.expand_dims(d, 0), s, axis=0), q)
                return q
# ---
def _reshape_chunk(x, template):
    return nxp.reshape(x, template.shape)
# ---
def _precond_grad(Q, G, exprs):
    """Precondition gradient G with preconditioner Q."""
    exprP = exprs[-1]
    return jnp.einsum(exprP, *Q, *Q, G)
# ---
def format_example(data: dict) -> str:
    """
    Converts example to fastText training data format.
    """
    text = re.sub(r"[\n\r]", " ", data["text"])
    return f"__label__{data['label']}" + " " + text
# ---
def pareto(key, shape: AxisSpec, b: NamedOrNumeric, dtype=float):
    shape = axis_spec_to_shape_dict(shape)
    b = broadcast_to(b, shape)
    jax_shape = to_jax_shape(shape)
    jax_array = jrandom.pareto(key, b.array, jax_shape, dtype=dtype)
    return haliax.auto_sharded(NamedArray(jax_array, shape))
# ---
def __init__(self, manager: ClusterManager, remote_client: RemoteClusterClient):
        self._manager = manager
        self._remote_client = remote_client
# ---
def sanepathname2url(path):
    urlpath = urllib.request.pathname2url(path)
    if os.name == "nt" and urlpath.startswith("///"):
        urlpath = urlpath[2:]
    # XXX don't ask me about the mac...
    return urlpath
# ---
def _unnormalize_fts_prognostic(self, fts: Prognostic) -> Prognostic:
        # Corrector is run in float64 to avoid precision loss
        fts = fts.to(torch.float64)
        return self.normalize.unnormalize_tensor_prognostic(fts, fill_value=0.0)
# ---
def result(self, timeout: float | None = None) -> Any:
        """Block until result is available."""
        ...
# ---
def __getstate__(self):
        return {
            "path": self.path,
            "fs_args": self.fs_args,
        }
# ---
def assign(dest):
                page_idx = pos_id // self.page_size
                page_offset = pos_id % self.page_size
                page = page_indices["seq", seq_id, "page", page_idx]
                dest_value = hax.where(is_invalid(page), INVALID, page * self.page_size + page_offset)
                return dest.at["position", i].set(dest_value)
# ---
def eval_data(self) -> Iterator[DataExample]:
        test_dataset = _get_hendrycks_math_test()

        for idx, item in enumerate(test_dataset):
            raw_prompt = item["problem"]
            raw_answer = item["solution"]
            example_id = f"test_{idx}"
            yield self.clean_example(raw_prompt, raw_answer, example_id)
# ---
def is_Sub_Array(A,B,n,m): 
    i = 0; j = 0; 
    while (i < n and j < m):  
        if (A[i] == B[j]): 
            i += 1; 
            j += 1; 
            if (j == m): 
                return True;  
        else: 
            i = i - j + 1; 
            j = 0;       
    return False;
# ---
def scaling_action_name(action: int) -> str:
    """Return enum name like 'SCALING_ACTION_SCALE_UP'."""
    try:
        return _SCALING_ACTION.values_by_number[action].name
    except KeyError:
        return f"UNKNOWN({action})"
# ---
def __setstate__(self, state: dict) -> None:
        self._name = state["name"]
        self._count = state["count"]
        self._job_id = state["job_id"]
        self._handles = []
        self._discovered_names = set()
# ---
import math 
def divSum(n): 
    sum = 1; 
    i = 2; 
    while(i * i <= n): 
        if (n % i == 0): 
            sum = (sum + i +math.floor(n / i)); 
        i += 1; 
    return sum; 
def areEquivalent(num1,num2): 
    return divSum(num1) == divSum(num2);
# ---
def max_projected_mem(self):
        """Return the maximum projected memory across all tasks to execute this plan."""
        projected_mem_values = [
            node["primitive_op"].projected_mem for _, node in visit_nodes(self.dag)
        ]
        return max(projected_mem_values) if len(projected_mem_values) > 0 else 0
# ---
def invert(a: A) -> A:
    return wrap_elemwise_unary(jnp.invert, a)
# ---
def rms_norm(x: Float[Array, "... D"], weight: Float[Array, "D"], eps: float) -> Float[Array, "... D"]:
    """RMS normalization.

    Kept local rather than delegating to Grug's version because Grug's
    calls unshard(weight) which requires a JAX mesh context.
    """
    dtype = x.dtype
    x = x.astype(jnp.float32)
    variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
    normed = x * jax.lax.rsqrt(variance + eps)
    out = normed * weight
    return out.astype(dtype)
# ---
def gen_codes():
            """Generate the 702 possible input codes"""
            # First, the 1-character codes
            for c in string.ascii_lowercase:
                yield c

            # Next, the 2-characters-with-wildcard codes
            for t in itertools.product(string.ascii_lowercase, repeat=2):
                yield '*'.join(t)
# ---
def _distributed_work_job():
    """Coscheduled job that uses job context."""
    from iris.cluster.client import get_job_info

    info = get_job_info()
    if info is None:
        raise RuntimeError("Not running in an Iris job context")
    print(f"Task {info.task_index} of {info.num_tasks} on worker {info.worker_id}")
    return f"Task {info.task_index} done"
# ---
def delete_all(self):
        '''
        Delete all keys
        '''
        self.delete('*')
# ---
def test_attentionmask_materialize_causal():
    mask = AttentionMask.causal()
    allowed = mask.materialize_mask(4, 4)
    expected = jnp.array(
        [
            [True, False, False, False],
            [True, True, False, False],
            [True, True, True, False],
            [True, True, True, True],
        ],
        dtype=bool,
    )
    assert allowed is not None
    assert allowed.shape == (4, 4)
    assert jnp.array_equal(allowed, expected)
# ---
def find_result(results, _name):
        ''' Find the specified result by name'''
        rval = None
        for result in results:
            if 'metadata' in result and result['metadata']['name'] == _name:
                rval = result
                break

        return rval
# ---
def length(self):
        """
        Returns the length (in days) of the task, by considering the start date
        and the due date. When there is no start date, its creation date is
        used. Returns 0 when one of these dates is missing.
        """
        start = self.start_date() or self.creation_date()
        due = self.due_date()

        if start and due and start < due:
            diff = due - start
            return diff.days
        else:
            return 0
# ---
def active_scale(self):
        return 1
# ---
def wrapped(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return fn(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    logger.warning(f"{fn.__name__} failed on attempt {attempt + 1}, retrying: {e}")
                    time.sleep(delay)

            raise RuntimeError(f"{fn.__name__} failed after {max_retries} attempts")
# ---
def __init__(self, content):
        '''secret constructor'''
        super(Secret, self).__init__(content=content)
        self._secrets = None
# ---
def last_scale_down_ms(self) -> int:
        """Timestamp of last scale-down operation."""
        return self._last_scale_down.epoch_ms()
# ---
def __init__(
        self,
        pooling: int = 2,
    ):
        super().__init__()
        self.maxpool = torch.nn.MaxPool2d(pooling)
# ---
def __call__(self, module: M_contra, carry: CarryT, *args: P.args, **kwargs: P.kwargs) -> CarryT: ...
# ---
def status(self) -> ControllerStatus:
        """Get controller status from GCP."""
        address = self.discover()
        if not address:
            return ControllerStatus(running=False, address=None, healthy=False, vm_name=None)

        vm_name = self._find_controller_vm_name()
        healthy = _check_health_rpc(address)

        return ControllerStatus(
            running=True,
            address=address,
            healthy=healthy,
            vm_name=vm_name,
        )
# ---
def raise_if_expired(self, message: str = "Deadline exceeded") -> None:
        """Raise TimeoutError if deadline has passed."""
        if self.expired():
            raise TimeoutError(message)
# ---
def jit_grad_accum(mlp, x):
        grad_fn = eqx.filter_value_and_grad(loss_fn, has_aux=True)
        grad_fn = microbatched(grad_fn, Batch, parallelism, axis_mapping, axis_mapping)
        (acc_v, acc_aux), acc_g = grad_fn(
            mlp,
            x,
        )
        return acc_v, acc_g
# ---
def inspect_side_effect(container_id):
        call_count[0] += 1
        if call_count[0] == 1:
            return ContainerStatus(running=True)
        return ContainerStatus(running=False, exit_code=0)
# ---
def __init__(self, delta, *args, **kwargs):
        super(TimeDeltaSensor, self).__init__(*args, **kwargs)
        self.delta = delta
# ---
def get_next_file(self):
    to_return = self.files[self.idx]

    self.idx += 1 
    self.idx %= len(self.files) 

    return PARAMS["PATH"]+to_return
# ---
def is_status_updated(self, process, previous_status):
        if process.updated_pending_status:
            return True
        if process.status != previous_status['status']:
            return True
        if (process.connection_status !=
                previous_status['ipsec_site_connections']):
            return True
# ---
def test_domain_id(self):
        eq_(self.record.domain_id, None)
# ---
def should_allow_eval(expr: str):
    # we don't want to try parsing unknown text or functions of more than two variables
    if count_unknown_letters_in_expr(expr) > 2:
        return False

    for bad_string in BAD_SUBSTRINGS:
        if bad_string in expr:
            return False

    return all(re.search(bad_regex, expr) is not None for bad_regex in BAD_REGEXES)
# ---
def _format_run_name(
    budget: float,
    hidden_size: int,
    num_layers: int,
    batch_size: int,
    experiment_name: str,
) -> str:
    """Format run name using architecture details (hidden size and layers).

    Format: isoflop-{budget}-d{hidden}-L{layers}-B{batch}-{experiment_name}
    """
    return f"isoflop-{budget:.0e}-d{hidden_size}-L{num_layers}-B{batch_size}-{experiment_name}"
# ---
def test_wrong_size_prev_cc_raises(self):
        with pytest.raises(nx.NetworkXError):
            G = self.undirected_G.copy()
            edge = self.pick_add_edge(G)
            insert = True
            prev_cc = self.undirected_G_cc.copy()
            prev_cc.pop(0)
            nx.incremental_closeness_centrality(G, edge, prev_cc, insert)
# ---
def test_is_stop_signal_empty_stop_sequences():
    # stop_sequences is empty
    tail_tokens = hax.named(jnp.array([1, 2, 3], dtype=jnp.int32), axis=("position",))
    stop_sequences = hax.named(jnp.zeros((0, 3), dtype=jnp.int32), axis=("seq", "position"))
    assert not is_stop_signal(tail_tokens, stop_sequences)
# ---
def __repr__(self):
        return ['NONE', 'STRING', 'OSD_STRING', 'FLAG', 'INT64', 'DOUBLE', 'NODE', 'NODE_ARRAY', 'NODE_MAP',
                'BYTE_ARRAY'][self.value]
# ---
def test_actor_options_non_preemptible_pins_head_node():
    from fray.v2.ray_backend.backend import _actor_ray_options

    options = _actor_ray_options(ResourceConfig(preemptible=False))
    assert options["num_cpus"] == 1
    assert options["resources"] == {"head_node": 0.0001}
# ---
def test_with_unflatten_flatten_unordered():
    Z = Axis("Z", B.size * C.size)
    assert einops_rearrange(zq, "{ W D C (Q: B H)} -> D (Z: B C) W H", H=H).axes == (D, Z, W, H)
    # make sure the values are right too
    z_t = (
        zq.array.reshape((B.size, H.size, D.size, W.size, C.size))
        .transpose((2, 0, 4, 3, 1))
        .reshape((D.size, Z.size, W.size, H.size))
    )
    assert (einops_rearrange(zq, "{ W D C (Q: B H)} -> D (Z: B C) W H", H=H).array == z_t).all()
# ---
def _arg_map_func(a, axis, arg_func=None, size=None, block_id=None):
    i = arg_func(a, axis=axis, keepdims=True)
    # note that the array API doesn't have take_along_axis, so this may fail
    v = nxp.take_along_axis(a, i, axis=axis)
    # add block offset to i so it is absolute index within whole array
    offset = block_id[axis] * size
    return {"i": i + offset, "v": v}
# ---
def __call__(self, x):
            return self.transform(x)
# ---
def __iter__(self):
        """
        Iterate over all data in the dataset, in order.
        """
        for shard_name in self.shard_names:
            for doc in self.open_shard(shard_name):
                yield doc
# ---
def chunkmem(self):
        """Amount of memory in bytes that a single chunk uses."""
        return array_memory(self.dtype, self.chunksize)
# ---
def create_zstd_compressed_jsonl(records: list[dict]) -> bytes:
    """Create zstd compressed JSONL content."""
    jsonl_content = "\n".join(json.dumps(record) for record in records) + "\n"
    jsonl_bytes = jsonl_content.encode("utf-8")
    cctx = zstd.ZstdCompressor()
    return cctx.compress(jsonl_bytes)
# ---
def count(self) -> int:
        """Total number of items across all chunks."""
        return sum(c.count for c in self.chunks)
# ---
def test_wait_for_connection_returns_false_on_timeout(mock_conn_avail, _mock_sleep):
    """wait_for_connection returns False when timeout expires."""
    mock_conn_avail.return_value = False
    conn = MagicMock()
    # Use a very short timeout so the real monotonic deadline expires quickly
    assert wait_for_connection(conn, timeout=Duration.from_ms(50), poll_interval=Duration.from_ms(10)) is False
# ---
def url(self):
        return urlunparse(
            (self.scheme, self.host, self.path, None, self.query_string, None)
        )
# ---
def quit():
        app_lock.signal()
# ---
def f(c, foo):
        return c, foo.my_array
# ---
def executor(self) -> Optional[Executor]:
        """The default executor for running computations."""
        if self._executor is not None:
            return self._executor
        elif self.executor_name is not None:
            return create_executor(self.executor_name, self.executor_options)
        return None
# ---
def is_point_on_line(self, point):
        return self.a * point[0] + self.b * point[1] + self.c == 0
# ---
def test_uses_defaults_when_cluster_ssh_config_empty(self):
        """get_ssh_config uses built-in defaults when cluster config empty."""
        from iris.time_utils import Duration

        config = config_pb2.IrisClusterConfig()

        ssh_config = get_ssh_config(config)

        assert ssh_config.user == "root"
        assert ssh_config.key_file is None
        assert ssh_config.port == 22
        assert ssh_config.connect_timeout == Duration.from_seconds(30)
# ---
def copy(self, cr, uid, id, default=None, context=None):
        context = context or {}
        default = default and default.copy() or {}
        default.update(date=time.strftime('%Y-%m-%d %H:%M:%S'), move_ids=[])
        return super(stock_production_lot, self).copy(cr, uid, id, default=default, context=context)
# ---
def svdvals(x, /):
    _, S, _ = svd(x, full_matrices=False)
    return S
# ---
def validateReply(self,req):
    if req.status_code >= 400 and req.status_code <= 500:
      try:
        j = req.json()
      except ValueError:
        raise exception.InternalError(req.text,req.status_code)
      raise exception.jsonToException(req.json())
# ---
def __init__(self, dataset_id: "TorchTrainDataset.Id"):
        self.dataset_id: TorchTrainDataset.Id = dataset_id
        self.raw_data: list[tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = []
        self.load_stats: LoadStats | None = None
# ---
def nonexist(id) :
    db = cherrypy.session['database']
    sql =  "UPDATE Radio set exist = 0 WHERE id = '%s'" % (id)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        ret = True
    except:
        ret = False

    updateversiondb(cur)
    con.commit()
    con.close()
    return ret
# ---
def end_stage(self) -> None:
        """Flush pending ops and close current stage."""
        self.flush_pending()
        if self.current_ops:
            self.stages.append(
                PhysicalStage(
                    operations=self.current_ops[:],
                    stage_type=self.stage_type,
                    output_shards=self.output_shards,
                )
            )
            self.current_ops = []
            self.output_shards = None
            self.stage_type = StageType.WORKER
# ---
def deposit(self, amount):
        self.balance += amount
        return self.balance
# ---
def files(self):
        if self.parsed_files is None:
            self.form  # compute form to get files

        return self.parsed_files
# ---
import re
def replace_specialchar(text):
 return (re.sub("[ ,.]", ":", text))
# ---
def testImportFutureLateRaises(self):
    regexp = 'from __future__ imports must occur at the beginning of the file'
    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,
                            'foo = bar\nfrom __future__ import print_function')
# ---
def __init__(self, maxlen: int = 5000):
        self._buffer: deque[BufferedLogRecord] = deque(maxlen=maxlen)
        self._lock = Lock()
# ---
def _infer_num_tensorcores() -> int:
    if jax.default_backend() != "tpu":
        return 1
    device_kind = jax.devices()[0].device_kind.lower()
    if "tpu v4" in device_kind or ("tpu v5" in device_kind and "v5e" not in device_kind) or "tpu v7" in device_kind:
        return 2
    return 1
# ---
def SetVariableValue( variable, value ):
  vim.command( "let {0} = {1}".format( variable, json.dumps( value ) ) )
# ---
def address(self) -> str:
        return f"{self.host}:{self.port}"
# ---
def __init__(self, output_path: str, worker_id: str):
        self.output_path = output_path
        self.path = get_status_path(output_path)
        self.worker_id = worker_id
        self._lock_path = self.path + ".lock"
        self.fs = fsspec.core.url_to_fs(self.path, use_listings_cache=False)[0]
# ---
def test_creation_with_path_string(self):
        """Test LocalLocation creation with string path."""
        loc = LocalLocation(path=Path("/tmp/data"))
        assert loc.path == Path("/tmp/data")
# ---
def __init__(self, shape: Tuple[int, ...], chunks: Tuple[int, ...]):
        self.shape = shape
        self.chunks = chunks
# ---
def model_params(self) -> M:
        return self.model
# ---
def get_archive_object_tar(self):
        '''
        return tarfile object and its members
        '''
        tfile = tarfile.open(name=self.filename)
        members = tfile.getnames()
        return tfile, members
# ---
def downgrade():
    conn = op.get_bind()
    acl_template_ids = _find_acl_templates(conn, ACL_TEMPLATES)
    if not acl_template_ids:
        return

    policy_uuid = _get_policy_uuid(conn, POLICY_NAME)
    if not policy_uuid:
        return

    delete_query = policy_template.delete().where(
        sa.sql.and_(
            policy_template.c.policy_uuid == policy_uuid,
            policy_template.c.template_id.in_(acl_template_ids),
        )
    )
    op.execute(delete_query)
# ---
def permute_final_dims(tensor: torch.Tensor, inds: List[int]):
    zero_index = -1 * len(inds)
    first_inds = list(range(len(tensor.shape[:zero_index])))
    return tensor.permute(first_inds + [zero_index + i for i in inds])
# ---
def test_map_blocks_with_kwargs(spec, executor):
    # based on dask test
    a = xp.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], chunks=5, spec=spec)
    b = cubed.map_blocks(nxp.max, a, axis=0, keepdims=True, dtype=a.dtype, chunks=(1,))
    assert_array_equal(b.compute(executor=executor), np.array([4, 9]))
# ---
def format_category(category):
    return categories_to_out_names[category]
# ---
def drop_empty(dict1):
  dict1 = {key:value for (key, value) in dict1.items() if value is not None}
  return dict1
# ---
def _test_do_execute_no_params(self, retval):
        with self._run_test(retval) as (conn, m1):
            result = conn.execution_options(no_parameters=True).\
                execute("insert into table foo")
        self._assert(
            retval,
            m1.do_execute_no_params, m1.real_do_execute_no_params,
            [call(
                    result.context.cursor,
                    "insert into table foo", result.context)]
        )
# ---
def _kill_old_container(name):
    try:
        logger.info(f"Killing old container {name}")
        _run_command("sudo", "docker", "rm", "-f", name)
    except subprocess.CalledProcessError:
        pass
# ---
def _get_acl_template_ids(conn, policy_uuid):
    query = sa.sql.select([policy_template.c.template_id]).where(
        policy_template.c.policy_uuid == policy_uuid
    )
    return [acl_template_id for (acl_template_id,) in conn.execute(query).fetchall()]
# ---
def calls(self):
        return self._calls
# ---
def status(self) -> cluster_pb2.JobStatus:
        """Get current job status.

        Returns:
            JobStatus proto with current state, task counts, and error info
        """
        return self._client._cluster_client.get_job_status(self._job_id)
# ---
def copy(self) -> "TransformAdapter":
        return dataclasses.replace(self)
# ---
def isPrime(num):
    if num < 2:
        return False  # 0, 1

    # num100, 50. 60 * ? = 100, , sqrt(), 
    boundary = int(math.sqrt(num)) + 1
    for i in range(2, boundary):
        if num % i == 0:
            return False

    return True
# ---
def test_callable_entrypoint(self):
        entry = Entrypoint.from_callable(_dummy_fn, args=(42,))
        iris_entry = convert_entrypoint(entry)
        assert iris_entry.is_callable
# ---
def tearDown(self):
        pass
# ---
def __init__(self):
        self._start_time = time.time()
        self._elapsed = 0.0
        self._n = 0
# ---
def _apply(x: Float[Array, "B S H D"]) -> Float[Array, "B S H D"]:
        x1, x2 = jnp.split(x, 2, axis=-1)
        return jnp.concatenate([x1 * cos - x2 * sin, x2 * cos + x1 * sin], axis=-1)
# ---
def task_func():
        print("hello from callable")
# ---
def list_status(self, status):
        self._call_all('list_status', status)
# ---
def testTryFinally(self):
    result = _GrumpRun(textwrap.dedent("""\
        try:
          print 'foo',
        finally:
          print 'bar'
        try:
          print 'foo',
          raise Exception
        finally:
          print 'bar'"""))
    self.assertEqual(1, result[0])
    self.assertIn('foo bar\nfoo bar\n', result[1])
    self.assertIn('Exception\n', result[1])
# ---
def position_token_offset(self) -> int:
        """First position token ID."""
        return 3
# ---
def append_log(path: str, obj: dataclass):
    with open(path, "a") as f:
        print(json.dumps(asdict(obj) if obj else None), file=f)
# ---
def test_impl():
            A = np.array([1., 2., 3.])
            A[0] = np.nan
            df = pd.DataFrame({'A': A})
            B = df.A.fillna(5.0)
            return B.sum()
# ---
def test_cats_task_reward():
    task = MoarCatsTask()

    assert task.compute_reward("cats", "cats cats cats") > task.compute_reward("cats", "cats")
    assert task.compute_reward("cats", "i love cats") > task.compute_reward("cats", "i like cats")

    assert task.compute_reward("cats", "cat") > 0
    assert task.compute_reward("cats", "dog") == 0
# ---
def attention_config(self) -> AttentionConfig:
        return AttentionConfig(
            Embed=self.Embed,
            num_heads=self.num_heads,
            num_kv_heads=self.num_kv_heads,
            head_dim=self.head_dim,
            use_bias=self.use_bias,
            upcast_attn=self.upcast_attn,
            attn_backend=self.attn_backend,
            flash_attention_block_size=self.flash_attention_block_size,
            rope=self.rope,
            qk_norm=self.qk_norm,
        )
# ---
def __init__(self):
                OpenerDirector.__init__(self)
                self.recorded = []
# ---
def __call__(self, x: NamedArray, group_sizes: NamedArray, *, key=None) -> NamedArray:
        k1, k2, k3 = maybe_rng_split(key, 3)
        hidden_states = self.w1(x, group_sizes, key=k1)
        hidden_states = self.act(hidden_states)
        hidden_states = hidden_states * self.w3(x, group_sizes, key=k3)
        outputs = self.w2(hidden_states, group_sizes, key=k2)
        return outputs
# ---
def test_kml_equals_lon_lat(self):
        """Asserts that the return value of to_kml() property is identical to
        the return value of to_lon_lat()."""
        assert_equal(self.polycircle.to_kml(), self.polycircle.to_lon_lat())
# ---
def __init__(self):
        self.comminfo = dict()
        self.init()
# ---
def _cuda_diagnostics() -> str:
    """Return a short string that helps debug why CUDA is (not) available."""
    visible = os.getenv("CUDA_VISIBLE_DEVICES", None)
    return (
        "CUDA diagnostics: "
        f"torch={torch.__version__}, "
        f"torch.version.cuda={torch.version.cuda}, "
        f"torch.cuda.is_available()={torch.cuda.is_available()}, "
        f"torch.cuda.device_count()={torch.cuda.device_count()}, "
        f"CUDA_VISIBLE_DEVICES={visible!r}"
    )
# ---
def tearDown(self):
        super(TestHooks, self).tearDown()
        for hooks in self.dynamo._hooks.values():
            while hooks:
                hooks.pop()
# ---
def bmarks():
    return_data = do_login()
    return return_data
# ---
def patkind(pattern, default=None):
    """If pattern is 'kind:pat' with a known kind, return kind."""
    return _patsplit(pattern, default)[0]
# ---
def is_valid(x, invalid=INVALID):
    """
    Returns a boolean array indicating whether each token in the input is valid.
    A token is considered valid if it is not negative and not equal to INVALID.
    """
    return (x >= 0) & (x != invalid)
# ---
def the_collection_name1_is_in_the_collection_name2(name1, name2):
    assert bpy.data.collections.get(name2).children.get(name1)
# ---
def send_reset(self):
        jsonstring = json.dumps({"resetpid": 1})
        self.serial.write(bytearray(jsonstring, 'utf-8'))
# ---
def max_of_three(num1,num2,num3): 
    if (num1 >= num2) and (num1 >= num3):
       lnum = num1
    elif (num2 >= num1) and (num2 >= num3):
       lnum = num2
    else:
       lnum = num3
    return lnum
# ---
def add(self, a: int, b: int) -> int:
        return a + b
# ---
def _flatten(metrics: Mapping[str, Any], prefix: str = "") -> dict[str, Any]:
    out: dict[str, Any] = {}
    for k, v in metrics.items():
        name = f"{prefix}/{k}" if prefix else k
        if isinstance(v, Mapping):
            out.update(_flatten(v, name))
        else:
            out[name] = v
    return out
# ---
def vmap_fun(x, y):
        return x.sum(Width) if y else x
# ---
def test_sharded_tree_size_shape_dtype_struct_without_sharding():
    mesh = jax.sharding.AbstractMesh((4,), ("data",))
    struct = jax.ShapeDtypeStruct((8, 4), jnp.float32)

    per_device_bytes = sharded_tree_size(struct, mesh=mesh)

    assert per_device_bytes == (8 * 4 * jnp.dtype(jnp.float32).itemsize)
# ---
def all(x, /, *, axis=None, keepdims=False, split_every=None):
    if x.size == 0:
        return asarray(True, dtype=x.dtype)
    return reduction(
        x,
        nxp.all,
        axis=axis,
        dtype=nxp.bool,
        keepdims=keepdims,
        split_every=split_every,
    )
# ---
def compute_reward(self, correct_answer: str, actual_response: str, tokenizer=None) -> float:
        # how many cats
        num_cats = actual_response.lower().count("cat")
        love_cats = actual_response.lower().count("love cats")

        return (num_cats + (10 * love_cats)) / np.sqrt(1 + len(actual_response))
# ---
def join(self, timeout=5):
        """Wait for thread completion."""
        if self.thread:
            self.thread.join(timeout)
# ---
def __init__(self, topic):
        target = oslo_messaging.Target(topic=topic, version='1.0')
        self.client = n_rpc.get_client(target)
# ---
def test_cursor_execute_wo_replace(self):
        self._test_cursor_execute(False)
# ---
def exception(self) -> BaseException | None:
        return self._exception
# ---
def __init__(self, *args, **kwargs):
        self.module = kwargs.get('module', None)
        self.client = F5RestClient(**self.module.params)
        self.want = ModuleParameters(params=self.module.params, client=self.client)
        self.have = ApiParameters()
        self.changes = UsableChanges()
# ---
def logical_not(x, /):
    if x.dtype not in _boolean_dtypes:
        raise TypeError("Only boolean dtypes are allowed in logical_not")
    return elemwise(nxp.logical_not, x, dtype=nxp.bool)
# ---
def filter(self, record: Record) -> bool:
        """
        Filters a record based on whether its ID is in the loaded set.

        Parameters
        ----------
        record : Record
            The record to filter.

        Returns
        -------
        bool
            True if the record should be kept, False if it should be discarded.
        """
        is_present = record.id.lower() in self.id_set

        return is_present if self.reverse else not is_present
# ---
def test_grids():
    L = 10
    thetas, phis = standard_grid(L)

    # Can't really test much here
    assert thetas.size == L
    assert phis.size == L**2

    grid = get_cartesian_grid(thetas, phis)
    assert grid.shape == (L**2, 3)
# ---
def setUpTestData(cls):
        cls.url = reverse('search-list')
        Feature.objects.create(name='archival descriptions', enabled=True)
        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)
        cls.security_levels = [1, 2, 3, 4, 5]
# ---
def count_digits(num1,num2):
    number=num1+num2
    count = 0
    while(number > 0):
        number = number // 10
        count = count + 1
    return count
# ---
def merge_prognostic_and_boundary(self, prognostic: torch.Tensor, step: int):
        x_index = self._get_x_index(step)
        boundary = self._get_boundary(x_index).to(prognostic.device)
        data = torch.cat((prognostic, boundary), dim=1)
        return data
# ---
def imag(x, /):
    if x.dtype == complex64:
        dtype = float32
    elif x.dtype == complex128:
        dtype = float64
    else:
        raise TypeError("Only complex floating-point dtypes are allowed in imag")
    return elemwise(nxp.imag, x, dtype=dtype)
# ---
def __init__(self, policy_file=None, rules=None, default_rule=None):
        self.rules = Rules(rules)
        self.default_rule = default_rule or CONF.policy_default_rule

        self.policy_path = None
        self.policy_file = policy_file or CONF.policy_file
# ---
def num_gpus(self):
        return self._num_gpus
# ---
def capitalize_first_last_letters(str1):
     str1 = result = str1.title()
     result =  ""
     for word in str1.split():
        result += word[:-1] + word[-1].upper() + " "
     return result[:-1]
# ---
def config(self) -> LmConfigT:
        pass
# ---
def usage():
	"""usage de la ligne de commande"""
	print ("usage : " + sys.argv[0] + "-h --help -s --server someurl.com -u --user login -p --password password")
# ---
def test_permute(self):
    testing_utils.layer_test(
        keras.layers.Permute, kwargs={'dims': (2, 1)}, input_shape=(3, 2, 4))
# ---
def config(self):
        @dataclass
        class Config:
            model_name: str = "test-model"

        return Config()
# ---
def __rlshift__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.left_shift(other, self)
# ---
def config_from_hf_checkpoint(self, ref: Optional[Union[str, RepoRef]] = None) -> LevConfig:
        config = self.hf_config_from_hf_checkpoint(ref)
        return self.config_from_hf_config(config)
# ---
def test_permutation_handles_edge_case_length_one(PermutationClass):
    length = 1
    prng_key = jrandom.PRNGKey(0)
    permutation = PermutationClass(length, prng_key)
    result = permutation(0)
    assert result == 0
# ---
def the_processor(self) -> ProcessorMixin:
        return load_processor(self.processor)
# ---
def setUp(self):
        crypto._fernet = None
# ---
def is_active(self):
        """
        Returns True when the start date is today or in the past and the
        task has not yet been completed.
        """
        start = self.start_date()
        return not self.is_completed() and (not start or start <= date.today())
# ---
def __enter__(self) -> "IrisClient":
        return self
# ---
def _join_prefix(prefix: str, tag: str) -> str:
    if prefix:
        return f"{prefix}/{tag}"
    return tag
# ---
def metadata(self):
        return {}
# ---
def _fake_clock_call(clock_id, timespec):
            calls.append((clock_id, timespec))
            timespec[0] = goal_timespec[0]
            return 0
# ---
def __init__(self, x=0.0, y=0.0, z=0.0):
        self.x = x
        self.y = y
        self.z = z
# ---
def get_host_name(provider):
    cfme_host = random.choice(provider.data["hosts"])
    return cfme_host.name
# ---
def initialize(self, io_loop=None, num_threads=10):
        from concurrent.futures import ThreadPoolExecutor
        super(ThreadedResolver, self).initialize(
            io_loop=io_loop, executor=ThreadPoolExecutor(num_threads))
# ---
def reset(self):
        self.flat_map_count = 0
        self.map_count = 0
        self.processed_ids = []
# ---
def __init__(self, mem_threshold=50_000_000) -> None:
        self.mem_threshold = mem_threshold
        self.allocations: Dict[str, Allocation] = {}
        self.stats: Dict[str, Stats] = {}
# ---
def retrieve_detail(item):
    '''Retrive detaill for news item
    return is tuple (id, title, url, text)
    '''
    url = item[2]
    html = urlopen(url)
    soup = BeautifulSoup(html, 'html.parser')
    detail = soup.find(class_='divNewsDetailsText')
    detail = detail.get_text()
    _list  = list(item)
    _list.insert(3, detail)
    item = tuple(_list)
    return item
# ---
def rechunk_new(x, chunks, *, min_mem=None):
    """Change the chunking of an array without changing its shape or data.

    Parameters
    ----------
    chunks : tuple
        The desired chunks of the array after rechunking.

    Returns
    -------
    cubed.Array
        An array with the desired chunks.
    """
    out = x
    for copy_chunks, target_chunks in _rechunk_plan(x, chunks, min_mem=min_mem):
        out = _rechunk(out, copy_chunks, target_chunks)
    return out
# ---
def __init__(self, entity_id, entity_type, ident):
        self.entity_id = int(entity_id)
        self.entity_type = entity_type
        self.ident = ident
# ---
def get_random_stacks(self, num_samples, stack_size):

        start_indices = random.sample(range(len(self.examplers)), num_samples)
        return [self.get_stack(start_index, stack_size) for start_index in start_indices]
# ---
def _make_state(step, key, depth=3):
    model = MLP(in_size=2, out_size=1, width_size=2, depth=depth, key=key)
    optim = optax.adam(1e-4)
    opt_state = optim.init(arrays_only(model))

    return TrainerState(step, model, optim, opt_state, key, is_trainable=True, mp=None, model_averaging=None)
# ---
def limit(self, count):
        Util.validate_type(count, "int")
        return self._limit(count)
# ---
def test_validate_edit_valid():
    source = "x = 1 + 2\n"
    mutation = Mutation(start=4, end=9, replacement="3 * 4", node_type="BinOp", original="1 + 2")
    assert validate_edit(source, mutation)
# ---
def _default_cache_key():
    assert False
# ---
def test_log_summary(monkeypatch):
    monkeypatch.setenv("HF_HUB_OFFLINE", "1")
    run = trackio.init(project="test-log-summary")
    tracker = TrackioTracker(run)
    tracker.log_summary({"float": 2.0})
    tracker.log_summary({"str": "test"})
    tracker.log_summary({"scalar_jax_array": jnp.array(3.0)})
    tracker.log_summary({"scalar_np_array": np.array(3.0)})
    trackio.finish()
# ---
def to_string(value):
        """ Convert a :class:`date` value into the format expected by the ORM. """
        return value.strftime(DATE_FORMAT) if value else False
# ---
def empty(shape, *, dtype=None, device=None, chunks="auto", spec=None) -> "Array":
    shape = normalize_shape(shape)
    return empty_virtual_array(
        shape, dtype=dtype, device=device, chunks=chunks, spec=spec, hidden=False
    )
# ---
def ret_self():
            self.objects.update(other.objects)
            return [self]
# ---
def triu(x, /, *, k=0) -> "Array":
    from cubed.array_api.searching_functions import where

    if x.ndim < 2:
        raise ValueError("x must be at least 2-dimensional for triu")

    mask = _tri_mask(x.shape[-2], x.shape[-1], k - 1, x.chunks[-2:], x.spec)
    return where(mask, zeros_like(x), x)
# ---
def _build_cmd(self, command: str) -> list[str]:
        return [
            "gcloud",
            "compute",
            "ssh",
            self.vm_name,
            f"--zone={self.zone}",
            f"--project={self.project_id}",
            "--quiet",
            "--command",
            command,
        ]
# ---
def open(self) -> zarr.Array:
        """Open the Zarr array for reading or writing and return it.

        Note that the Zarr array must have been created or this method will raise an exception.
        """
        # r+ means read/write, fail if it doesn't exist
        return open_backend_array(
            self.store,
            mode="r+",
            shape=self.shape,
            dtype=self.dtype,
            chunks=self.chunks,
            path=self.path,
            **self.kwargs,
        )
# ---
def __getitem__(self, i):
                return list.__getitem__(self.l, i)
# ---
def test_negative(spec, executor):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.negative(a)
    assert_array_equal(
        b.compute(executor=executor),
        np.array([[-1, -2, -3], [-4, -5, -6], [-7, -8, -9]]),
    )
# ---
def __init__(self):
        self._current_time = 0.0
        self._sleepers = []  # min-heap of SleepEvent
        self._lock = threading.Lock()
# ---
def getCCLocation(self):
        if self.diffPath == None:
            return 1
        else:
            return 0
        return
# ---
def _trans_fn(self, is_transaction=False):
        def go(conn, x, value=None):
            if is_transaction:
                conn = conn.connection
            conn.execute(self.table.insert().values(a=x, b=value))
        return go
# ---
def lateralsurface_cube(l):
  LSA = 4 * (l * l)
  return LSA
# ---
def advertiseAction(self, action):
        """Should the web interface even show the form for ACTION?"""
        if action not in self.knownActions:
            raise KeyError("unknown action")
        cfg = self.config.get(action, False)
        if cfg:
            return True
        return False
# ---
def test_check_compilation(spec, executor, compile_function):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]], chunks=(2, 2), spec=spec)
    c = xp.add(a, b)
    assert_array_equal(
        c.compute(executor=executor, compile_function=compile_function),
        np.array([[2, 3, 4], [5, 6, 7], [8, 9, 10]]),
    )
# ---
def test_index_2d_step(spec, shape, chunks, ind, new_chunks_expected):
    a = xp.ones(shape, chunks=chunks, spec=spec)
    b = a[ind]
    assert_array_equal(b.compute(), np.ones(shape)[ind])
    assert b.chunks == new_chunks_expected
# ---
def _to_tuple(index: tuple[slice, ...]) -> tuple[tuple[int, int], ...]:
            my_indices: tuple[tuple[int, int], ...] = tuple(
                s.indices(axis_size)[0:2] for axis_size, s in zip(array.shape, index, strict=False)
            )

            return my_indices
# ---
def __init__(_,ws):_.ws=ws
# ---
def test_groupby_reduction_axis0():
    a = xp.full((4 * 6, 5), 7.0, chunks=(4, 2))
    b = xp.asarray([0, 1, 0, 1] * 6, chunks=(4,))
    c = mean_groupby_reduction(a, b, axis=0, num_groups=2)
    assert_array_equal(c.compute(), np.full((2, 5), 7))
# ---
def __init__(self, *args, **kwargs):
        super(EspecialidadeMedicoFilterSet, self).__init__(*args, **kwargs)

        row1 = to_row([('especialidade', 12)])

        self.form.helper = FormHelper()
        self.form.helper.form_method = 'GET'
        self.form.helper.layout = Layout(
            Fieldset(_('Pesquisar Mdico'),
                     row1, form_actions(save_label='Filtrar'))
        )
# ---
def update_env_var(self, key, value):
        '''place an env in the env var list'''

        env_vars_array = self.get_env_vars()
        idx = None
        for env_idx, env_var in enumerate(env_vars_array):
            if env_var['name'] == key:
                idx = env_idx
                break

        if idx:
            env_vars_array[idx]['value'] = value
        else:
            self.add_env_value(key, value)

        return True
# ---
def observe(self, sample):
        self.steps += 1
        self.memory.add(sample)

        # Reduces exploration rate linearly
        self.explore_rate = self.MIN_EXPLORATION_RATE + (self.MAX_EXPLORATION_RATE - self.MIN_EXPLORATION_RATE) * math.exp(-self.DECAY_RATE * self.steps)
# ---
def test_squeeze_1d(spec, executor):
    a = xp.asarray([[1, 2, 3]], chunks=(1, 2), spec=spec)
    b = xp.squeeze(a, 0)
    assert_array_equal(b.compute(executor=executor), np.squeeze([[1, 2, 3]], 0))
# ---
def __init__(self, tpu_type: str):
        super().__init__()
        self._tpu_type = tpu_type
        self._last_scale_multislice_time: float | None = None
        self._last_check_should_scale_up_multislice_time: float | None = None
# ---
def __call__(self, x: NamedArray, *, key):
        k1, k2, k3, k4 = haliax.jax_utils.maybe_rng_split(key, 4)

        x_for_hyena = self.ln_1(x)
        hyena_output = self.hyena_operator(x_for_hyena, key=k1)
        hyena_output = self.resid_dropout(hyena_output, key=k2)
        x = x + hyena_output

        ff_output = self.mlp(self.ln_2(x), key=k3)
        ff_output = self.resid_dropout(ff_output, key=k4)
        x = x + ff_output

        return x
# ---
def get_logs(self, container_id: str, since: "Timestamp | None" = None) -> list[LogLine]: ...
# ---
def test_fn():
        print("Hello from test task!")
        return 42
# ---
def median_trapezium(base1,base2,height):
 median = 0.5 * (base1+ base2)
 return median
# ---
def py3_info(info):
    # NOTE(boris-42): py33 I hate you.
    info_py3 = copy.deepcopy(info)
    new_name = re.sub("FakeTrace[^.]*", "FakeTracedCls",
                      info_py3["function"]["name"])
    info_py3["function"]["name"] = new_name
    return info_py3
# ---
def __init__(self):
        urllib.request.AbstractHTTPHandler.__init__(self)
        self.httpconn = MockHTTPClass()
# ---
def genetic_modification(testapp, lab, award):
    item = {
        'award': award['@id'],
        'lab': lab['@id'],
        'modified_site_by_coordinates': {
            'assembly': 'GRCh38',
            'chromosome': '11',
            'start': 20000,
            'end': 21000
        },
        'purpose': 'repression',
        'category': 'deletion',
        'method': 'CRISPR',
        'zygosity': 'homozygous'
    }
    return testapp.post_json('/genetic_modification', item).json['@graph'][0]
# ---
def get_all_api_keys(user_profile: UserProfile) -> List[str]:
    # Users can only have one API key for now
    return [user_profile.api_key]
# ---
def authenticated_userid(self, request):
		params = _parse_authorization(request, self.secret, self.realm)
		if params is None:
			return None
		if not _is_valid_nonce(params['nonce'], self.secret):
			_add_www_authenticate(request, self.secret, self.realm)
			return None
		userid = params['username']
		if self.callback(params, request) is not None:
			return 'u:%s' % userid
		_add_www_authenticate(request, self.secret, self.realm)
# ---
def test_format_shard_path_local_paths():
    """Test that local paths also get normalized."""
    pattern = "/tmp//output//data-{shard:05d}.jsonl"
    result = format_shard_path(pattern, 0, 1)
    assert result == "/tmp/output/data-00000.jsonl"
# ---
def log_summary(self, metrics: Mapping[str, Any]):
        record = _to_jsonable(
            {
                "tracker": self.name,
                "event": "summary",
                "metrics": metrics,
            }
        )
        self.logger.info(json.dumps(record))
        self._summary_metrics.update(_flatten(metrics))
# ---
def nCr_mod_p(n, r, p): 
	if (r > n- r): 
		r = n - r 
	C = [0 for i in range(r + 1)] 
	C[0] = 1 
	for i in range(1, n + 1): 
		for j in range(min(i, r), 0, -1): 
			C[j] = (C[j] + C[j-1]) % p 
	return C[r]
# ---
def with_config_overrides(self, config_overrides: dict, merge: bool = True) -> "HFCheckpointConverter":
        if self.config_overrides is not None and merge:
            config_overrides = mergedeep.merge({}, self.config_overrides, config_overrides)
        return dataclasses.replace(self, config_overrides=config_overrides)
# ---
def resolve_axis(self, axes: AxisSelection) -> AxisSpec: ...
# ---
def test_makes_rectangular_patches():
    x = torch.randn(1, 10, 4, 8)

    patch_embed = PerceiverEncoder(
        in_channels=10,
        out_channels=4,
        patch_extent=(180, 90),
        perceiver=make_perceiver(10, 4),
    )

    patches = patch_embed(x, make_resolution(x))

    assert patches.shape == (
        1,
        4,
        1,
        4,
    )
# ---
def inner(args, kwargs):
            return f(*args, **kwargs)
# ---
def before(self, other: "Timestamp") -> bool:
        """Check if this timestamp is before another."""
        return self._epoch_ms < other._epoch_ms
# ---
def test_stack(spec, executor):
    a = xp.full((4, 6), 1, chunks=(2, 3), spec=spec)
    b = xp.full((4, 6), 2, chunks=(2, 3), spec=spec)
    c = xp.full((4, 6), 3, chunks=(2, 3), spec=spec)
    d = xp.stack([a, b, c], axis=0)
    assert_array_equal(
        d.compute(executor=executor),
        np.stack([np.full((4, 6), 1), np.full((4, 6), 2), np.full((4, 6), 3)], axis=0),
    )
# ---
def __init__(self):
        self.reused_arrays: list[tuple[ArrayLike, list[str]]] = []
        self.static_arrays: list[str] = []
# ---
def _get_job(self, job_id: JobId) -> "_LocalJob":
        if job_id not in self._jobs:
            raise KeyError(f"Job {job_id} not found")
        return self._jobs[job_id]
# ---
def count_Odd_Squares(n,m): 
    return int(m**0.5) - int((n-1)**0.5)
# ---
def overlapping(list1,list2):  
    c=0
    d=0
    for i in list1: 
        c+=1
    for i in list2: 
        d+=1
    for i in range(0,c): 
        for j in range(0,d): 
            if(list1[i]==list2[j]): 
                return 1
    return 0
# ---
def _all_input_axes(arrays):
    return ensure_tuple(functools.reduce(union_axes, (a.axes for a in arrays), ()))
# ---
def start(self) -> None:
        self._thread.start()
# ---
def ensure_roundtrip(td_str, expected_seconds):
        # we don't enforce that the output is the same as the input,
        # but we do enforce that it can be parsed to the same timedelta
        td = parse_timedelta(td_str)
        assert td.total_seconds() == expected_seconds
        assert parse_timedelta(encode_timedelta(td)) == td, f"Failed to roundtrip {td_str}: {encode_timedelta(td)}"
# ---
def _extend_and_expr(self, and_expr, _and, check):
        """Extend an 'and_expr' by adding one more check."""

        return [('and_expr', and_expr.add_check(check))]
# ---
def replace_model_weights(self, pl_module: LightningModule) -> None:
        """Replace model weights with EMA weights.

        Parameters
        ----------
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._weights_buffer = {
            k: p.detach().clone().to("cpu") for k, p in pl_module.state_dict().items()
        }
        pl_module.load_state_dict(self._ema_weights, strict=False)
# ---
def __init__(
        self,
        device: torch.device,
        compute_metric: AreaWeightedSingleTargetFunction,
        n_timesteps: int,
    ):
        self._compute_metric = compute_metric
        self._total: torch.Tensor | None = None
        self._n_batches = torch.zeros(
            n_timesteps, dtype=torch.int32, device=get_device()
        )
        self._device = device
        self._n_timesteps = n_timesteps
# ---
def total_coin_supply(self):
        with self.lock:
            return self._state.total_coin_supply
# ---
def get_bit_length(self):
        """Return the number of bits that will be read"""
        return self._BitLength
# ---
def test_from_array_zarr(tmp_path, spec):
    store = store = tmp_path / "source.zarr"
    za = create_zarr(
        [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
        chunks=(2, 2),
        store=store,
    )
    a = cubed.from_array(za, spec=spec)
    assert_array_equal(a, za[:])
# ---
def convert_atom_name(name: str) -> Tuple[int, int, int, int]:
    """Convert an atom name to a standard format.

    Parameters
    ----------
    name : str
        The atom name.

    Returns
    -------
    Tuple[int, int, int, int]
        The converted atom name.

    """

    name = name.strip().upper()
    name = [ord(c) - 32 for c in name]
    name = name + [0] * (4 - len(name))
    return tuple(name)
# ---
def __str__(self):
        return f"{self.__class__.__name__}(shape={self.shape})"
# ---
def min_k(test_list, K):
  res = sorted(test_list, key = lambda x: x[1])[:K]
  return (res)
# ---
def key_function(out_key):
        return tuple((array.name,) + out_key[1:] for array in args)
# ---
def baby_llama_config():
    return InferenceServerConfig(
        service=InferenceEngineConfig(
            max_seq_len=32,
            max_seqs=2,
            page_size=4,
            max_queued_tokens=32,
            hbm_utilization=0.1,
        ),
        temperature=0.7,
        seed=42,
    )
# ---
def find_Product(arr,n): 
    arr.sort() 
    prod = 1
    for i in range(0,n,1): 
        if (arr[i - 1] != arr[i]): 
            prod = prod * arr[i] 
    return prod;
# ---
def _conjB(Q, G, V):
    """Compute conjB."""
    order = G.ndim
    p = list(range(order))
    conjB = jnp.transpose(V, p[1:] + p[:1])
    for i, q in enumerate(Q):
        conjB = conjB / q if q.ndim < 2 else _solve_triangular_right(conjB, q)
        if i < order - 1:
            conjB = jnp.swapaxes(conjB, i, order - 1)
    return conjB
# ---
def create_vm_group(self, tags: dict[str, str] | None = None) -> VmGroupProtocol:
        """Create a new VM group. Returns a ready-to-use VmGroup object.

        Args:
            tags: Optional labels/tags for the VM group and its VMs

        Returns:
            A VmGroup object with lifecycle management
        """
        ...
# ---
def test_impl(n):
            df = pd.DataFrame({'A': np.arange(n), 'B': np.arange(n) + 1.0})
            df1 = df[df.A > 5]
            return len(df1.B)
# ---
def decode_tokens(token_list: list[int]):
    """Helper to decode tokens."""
    decoded = current_tokenizer.decode(token_list)
    console.print(f"[green]Decoded:[/green] {decoded!r}")
# ---
import math 
def find_Digits(n): 
    if (n < 0): 
        return 0;
    if (n <= 1): 
        return 1; 
    x = ((n * math.log10(n / math.e) + math.log10(2 * math.pi * n) /2.0)); 
    return math.floor(x) + 1;
# ---
def play(self, clock=None, server=None):
        import supriya.patterns
        import supriya.realtime

        event_player = supriya.patterns.RealtimeEventPlayer(
            self, clock=clock, server=server or supriya.realtime.Server.default()
        )
        event_player.start()
        return event_player
# ---
def exists(self, tag: str) -> bool: ...
# ---
def run(self) -> None:
        import uvicorn

        uvicorn.run(self._app, host=self._host, port=self._port)
# ---
def loss_blk_fn(x):
        return fused_linear_softmax_cross_entropy_loss(
            x,
            lm_head,
            labels,
            reduction="mean",
            dtype=jnp.float32,
            precision=jax.lax.Precision.HIGHEST,
        )
# ---
def build_optimizer_config(self) -> AdamConfig:
        return AdamConfig(
            learning_rate=self.lr,
            weight_decay=self.weight_decay,
            lr_schedule=self.lr_schedule,
            decay=self.lr_cooldown_duration,
            min_lr_ratio=self.min_lr_ratio,
        )
# ---
def __str__(self):
        return f"{self.name}=={self.version}" if self.version else self.name
# ---
def test_is_cloneable_share_goodformat2(self):
        drv = self._driver
        mox = self.mox
        strg = 'nfs://10.61.222.333:8080/share/img'
        mox.StubOutWithMock(drv, '_check_share_in_use')
        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')
        mox.ReplayAll()
        drv._is_cloneable_share(strg)
        mox.VerifyAll()
# ---
def __post_init__(self):
        if len(self._axes) != len(self._prefix):
            raise ValueError("Prefix entries must align with axes")
# ---
def test_optimize_stack(spec):
    # This test fails if stack's general_blockwise call doesn't have fusable_with_predecessors=False
    a = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    b = cubed.random.random((10, 10), chunks=(5, 5), spec=spec)
    c = xp.stack((a, b), axis=0)
    d = c + 1
    # try to fuse all ops into one (d will fuse with c, but c won't fuse with a and b)
    d.compute(optimize_function=fuse_multiple_levels())
# ---
def generate_tags() -> Iterator[str]:
    """Generate chain tags.

    Yields
    ------
    str
        The next chain tag

    """
    for i in range(1, 4):
        for j in range(len(string.ascii_uppercase) ** i):
            tag = ""
            for k in range(i):
                tag += string.ascii_uppercase[
                    j
                    // (len(string.ascii_uppercase) ** k)
                    % len(string.ascii_uppercase)
                ]
            yield tag
# ---
def getTransformation( self, transName, extraParams = False, rpc = '', url = '', timeout = None ):
    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )
    return rpcClient.getTransformation( transName, extraParams )
# ---
def reset_table_style(self):
        if self._row_matched:
            self._row_matched = False
            self.model.reset_rows_highlight()
        self.table.refresh()
# ---
def _description_help(self, env):
        if self.help and env.lang:
            name = "%s,%s" % (self.model_name, self.name)
            trans = env['ir.translation']._get_source(name, 'help', env.lang)
            return trans or self.help
        return self.help
# ---
def test_matrix_transpose(tmp_path, spec, executor):
    a = cubed.random.random(
        (10000, 10000), chunks=(5000, 5000), spec=spec
    )  # 200MB chunks
    b = xp.matrix_transpose(a)
    run_operation(tmp_path, executor, "matrix_transpose", b)
# ---
def setUp(self):
        super(JsonToYamlTest, self).setUp()
        self.expected_test_count = 2
        self.longMessage = True
        self.maxDiff = None
# ---
import sys
def next_smallest_palindrome(num):
    numstr = str(num)
    for i in range(num+1,sys.maxsize):
        if str(i) == str(i)[::-1]:
            return i
# ---
def __call__(self, x: NamedArray, *, key=None) -> NamedArray:
        keys = maybe_rng_split(key, len(self.layers))
        for layer, k in zip(self.layers[:-1], keys):
            x = self.activation(layer(x, key=k))
        return self.layers[-1](x, key=keys[-1])
# ---
def test_deduplicate_with_num_output_shards(backend):
    """Test deduplication with explicit num_output_shards."""
    data = [{"id": i % 3, "val": f"item_{i}"} for i in range(20)]

    ds = Dataset.from_list(data).deduplicate(key=lambda x: x["id"], num_output_shards=5)

    results = list(Backend.execute(ds, context=backend))

    # Should have exactly 3 unique items (ids 0, 1, 2)
    assert len(results) == 3
    ids = sorted([r["id"] for r in results])
    assert ids == [0, 1, 2]
# ---
def run_training_mode(args):
    """Run in training worker mode."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    logger = logging.getLogger("training_worker")

    logger.info("Starting training worker mode...")
    cleanup()

    worker_config = llama_small_training_worker_config(CHECKPOINT_DIR, str(RUN_ID))
    worker = TrainWorker(
        config=worker_config,
    )

    worker.train()
    logger.info("Training worker completed")
# ---
def __and__(self, other: object) -> LogicalExpr:
        return LogicalExpr(self, _to_expr(other), "and")
# ---
def selected_pane_index(self, value):
        if value >= len(self.panes):
            return
        self._selected_pane_index = value
        self._refresh_target_selection()
        self._update_selected_pane()
# ---
def get_local_val_index(self, model: LightningModule, idx_dataset: int) -> int:
        """Get the local validation index.

        Parameters
        ----------
        idx_dataset : int
            The dataset index.

        Returns
        -------
        int
            The local validation index.
        """
        val_name = model.val_group_mapper[idx_dataset]["label"]
        return self.val_names.index(val_name)
# ---
def configure_callbacks(self) -> List[Callback]:
        """Configure model callbacks.

        Returns
        -------
        List[Callback]
            List of callbacks to be used in the model.

        """
        return [EMA(self.ema_decay)] if self.use_ema else []
# ---
def start_container(self, container_id: str) -> None:
        result = subprocess.run(
            ["docker", "start", container_id],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode != 0:
            raise RuntimeError(f"Failed to start container: {result.stderr}")
# ---
def _ensure_first(axis):
    def ensure_first(leaf):
        if isinstance(leaf, NamedArray):
            return leaf.rearrange((axis, ...))
        elif isinstance(leaf, _PassiveNamedArray):
            assert False, "PassiveNamedArray should not be present in the tree"
        else:
            return leaf

    return ensure_first
# ---
def hamming_Distance(n1,n2) : 
    x = n1 ^ n2  
    setBits = 0
    while (x > 0) : 
        setBits += x & 1
        x >>= 1
    return setBits
# ---
def __call__(self, batch: Sequence[Sequence[int]]) -> Sequence[dict[str, np.ndarray]]:
        # return pa.RecordBatch.from_arrays([pa.array(batch)], ["test"])
        return [{"test": np.asarray(x)} for x in batch]
# ---
import math
def largest_triangle(a,b): 
    if (a < 0 or b < 0): 
        return -1 
    area = (3 * math.sqrt(3) * pow(a, 2)) / (4 * b);  
    return area
# ---
def __init__(self, date, precision):
        """Create a period with the given date (a `datetime` object) and
        precision (a string, one of "year", "month", "day", "hour", "minute",
        or "second").
        """
        if precision not in Period.precisions:
            raise ValueError(f'Invalid precision {precision}')
        self.date = date
        self.precision = precision
# ---
def _getN(self, n):
        n -= 1
        p = self.head
        while n:
            p = p.next
            n -= 1
        return p
# ---
def fai_learned(domain, b):
    if domain.unflooded_domain == None:
        print('No unflooded training domain provided.')
        return None
    unflooded_b = modis_utilities.compute_modis_indices(domain.unflooded_domain)
    water_mask  = modis_utilities.get_permanent_water_mask()

    threshold = modis_utilities.compute_binary_threshold(get_fai(unflooded_b), water_mask, domain.bounds)
    return fai(domain, b, threshold)
# ---
def start(self):
        """Start worker in background thread."""
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
# ---
def signup():
    # Create new user
    new_user = User()
    new_user.name = request.form['name']
    new_user.email = request.form['email']
    new_user.password = sha1(request.form['password']).hexdigest()
    new_user.token = str(uuid.uuid4())
    new_user.save()
    return JSON(message='User created successfully')
# ---
def chip_count(self) -> int:
        """Total number of GPU chips."""
        return self.count
# ---
import re
def snake_to_camel(word):
  return ''.join(x.capitalize() or '_' for x in word.split('_'))
# ---
def Step (self,
              Message,
              Delay_In_Seconds = 0.0):
        if self.Next_Step is None:
            self.Next_Step = 1
        if self.Start_Time is None:
            self.Start_Time = clock ()

        logging.info ("Step " + str (self.Next_Step) + ": " + Message)
        sleep (Delay_In_Seconds)
        self.Next_Step += 1
# ---
def tree_unflatten(cls, aux_data, children):
        inner_state = jax.tree.unflatten(aux_data, children[:-5])

        return cls(
            inner_opt_state=inner_state,
            losses=children[-5],
            grad_norms=children[-4],
            valid_mask=children[-3],
            current_idx=children[-2],
            count=children[-1],
        )
# ---
def Embed(self) -> Axis:
        return Axis("embed", self.embed_dim)
# ---
def test_capturing_readouterr(self):
        with self.getcapture() as cap:
            print("hello world")
            sys.stderr.write("hello error\n")
            out, err = cap.readouterr()
            assert out == "hello world\n"
            assert err == "hello error\n"
            sys.stderr.write("error2")
            out, err = cap.readouterr()
        assert err == "error2"
# ---
def acos(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in acos")
    return elemwise(nxp.acos, x, dtype=x.dtype)
# ---
def visit_mtext(self, element):
        text = element.get_text()
        return BracedNode(f"\\text{{{text}}}")
# ---
def _load_from_checkpoint(config, tokenizer):
  if 'hf' in config.backbone:
    return diffusion.Diffusion(
      config, tokenizer=tokenizer).to('cuda')

  return diffusion.Diffusion.load_from_checkpoint(
    config.eval.checkpoint_path,
    tokenizer=tokenizer,
    config=config)
# ---
def std(x, /, *, axis=None, correction=0.0, keepdims=False, split_every=None):
    return sqrt(
        var(
            x,
            axis=axis,
            correction=correction,
            keepdims=keepdims,
            split_every=split_every,
        )
    )
# ---
def done(queue_name: str, lease_id: str = Body(...), timestamp: float = Body(...)):
            if queue_name not in self.queues:
                return Response(status_code=404)
            lease = Lease(item=None, lease_id=lease_id, timestamp=timestamp)
            self.queues[queue_name].done(lease)
            return {"status": "ok"}
# ---
def test_unstack_single_array(spec):
    a = xp.full((1, 4, 6), 1, chunks=(1, 2, 3), spec=spec)
    (b,) = xp.unstack(a)
    assert_array_equal(b.compute(), np.full((4, 6), 1))
# ---
def render(self, text):
        """Render the Markdown text.

        :param text: markdown formatted text content.
        """
        return self.parse(text)
# ---
def get_final_weighted_score(self):
        return self.final_weighted
# ---
def asin(x, /):
    if x.dtype not in _floating_dtypes:
        raise TypeError("Only floating-point dtypes are allowed in asin")
    return elemwise(nxp.asin, x, dtype=x.dtype)
# ---
def __init__(self, input_channels, hidden_channels, output_channels):
        super(CNN_with_loop, self).__init__()
        self.conv1 = nn.Conv2d(
            input_channels, hidden_channels, kernel_size=3, padding=1
        )
        self.bn1 = nn.BatchNorm2d(hidden_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(
            hidden_channels, output_channels, kernel_size=3, padding=1
        )
        self.bn2 = nn.BatchNorm2d(output_channels)
# ---
def create_actor_group(
        self,
        actor_class: type,
        *args: Any,
        name: str,
        count: int,
        resources: ResourceConfig = ResourceConfig(),
        **kwargs: Any,
    ) -> ActorGroup:
        """Create N instances of an actor, returning a group handle."""
        ...
# ---
def get_timerules(hid):
    """Gets list of timerules for given hid"""
    timerules= home_services.read_time_rules(hid)
    return jsonify(timerules)
# ---
def test_llama_flops():
    # Check that the forward flops is within 10% of the naive calculation
    hf_config = transformers.LlamaConfig.from_pretrained("NousResearch/Llama-2-7b-hf")
    llama_config = LlamaConfig.from_hf_config(hf_config)
    n_params = 6.738415616e9
    ratio = llama_config.flops_per_token(hf_config.vocab_size, llama_config.max_seq_len) / (2 * n_params)
    assert ratio > 1.1, f"ratio {ratio} < 1.1"
    assert ratio < 1.2, f"ratio {ratio} > 1.2"
# ---
def test_ndim_distance():
    """Test to see if changing val by 1 does what it ought to do
    convert to float to integer because floating arithmetic makes testing
    analytic functions a mess"""
    rand = random.random
    point1 = [rand(), rand(), rand(), rand(), rand(), rand()]
    point2 = [point1[0]+1] + point1[1:] # just shift x to the right by 1
    assert int(round(kmeans.ndim_euclidean_distance(point1, point2))) == 1
# ---
def update_elements():
    """Updates all elements with all new values received from the user application"""
    received_elements = request.get_json()
    home_services.update_elements(received_elements)
    return 'OK'
# ---
def _print_file_list(file_list: list[Path]):
    for f in file_list[:10]:
        print(f"  - {f}")
    if len(file_list) > 10:
        print(f"  ... and {len(file_list) - 10} more")
# ---
def logs_tail(self, *, max_lines: int = 200) -> str:
        if self.vllm_server is None:
            raise RuntimeError("vLLM server is not running in this environment.")
        return self._backend.logs_tail(self.vllm_server, max_lines=max_lines)
# ---
def key_function(out_key):
        out_coords = out_key[1:]

        k = merge_factor
        out_coord = out_coords[0]  # this is just 1d
        # return a tuple with a single item that is the list of input keys to be merged
        return (
            [
                (in_name, out_coord * k + i)
                for i in range(k)
                if out_coord * k + i < numblocks
            ],
        )
# ---
def ready_count(self) -> int:
        """All Ray actors are ready immediately after creation."""
        return len(self._handles)
# ---
def loss_fast_batched(v):
        return jnp.sum(template_op(v))
# ---
def tokenizer():
    return TreeDiffusionTokenizer(max_seq_len=MAX_SEQ_LEN)
# ---
import re
def remove_spaces(text):
 return (re.sub(' +',' ',text))
# ---
def __init__(self, *args, **kwargs):
        super(User, self).__init__(*args, **kwargs)
        self.startup = None
        self.team_member = None
        self.profile = None
        self.user_finalist_roles = None
# ---
def decreasing_trend(nums):
    if (sorted(nums)== nums):
        return True
    else:
        return False
# ---
def __class_getitem__(self, axes_spec_: tuple[type[NamedArray], NamedArrayAxesSpec]):
            _, axes_spec = axes_spec_
            axes = _parse_namedarray_axes(axes_spec)
            return Annotated[NamedArray, axes]
# ---
def test_impl(df):
            return df.A.str.split()
# ---
def test_utils_to_probs_raises():
    with pytest.raises(RuntimeError):
        mnl.utils_to_probs(
            pd.DataFrame([[1, 2, np.inf, 3]]))
# ---
def named_call(f: F, name: str | None = None) -> F: ...
# ---
def is_finite(self) -> bool:
        return self.dataset.is_finite()
# ---
def export(self, request):
        serializer = ConditionExportSerializer(self.get_queryset(), many=True)
        xml = ConditionRenderer().render(serializer.data)
        return XMLResponse(xml, name='conditions')
# ---
def _resolve(self) -> ResolveResult:
        result = self._resolver.resolve(self._name)
        with self._lock:
            self._cached_result = result
        return result
# ---
def action_explode(self, cr, uid, moves, context=None):
        """Hook to allow other modules to split the moves of a picking."""
        return moves
# ---
def check_bot_name_available(realm_id: int, full_name: str) -> None:
    dup_exists = UserProfile.objects.filter(
        realm_id=realm_id,
        full_name=full_name.strip(),
        is_active=True,
    ).exists()

    if dup_exists:
        raise JsonableError(_("Name is already in use!"))
# ---
def test_unrescue(self):
        instance = self._create_instance()
        conn = xenapi_conn.get_connection(False)
        # Unrescue expects the original instance to be powered off
        conn.power_off(instance)
        rescue_vm = xenapi_fake.create_vm(instance.name + '-rescue', 'Running')
        conn.unrescue(instance, None)
# ---
def signin():
    # Retorna a user data
    user_info = User.objects(email=request.form['email'], password=sha1(
        request.form['password']).hexdigest())
    if user_info.count():
        return JSON(token=user_info.get().token, roles=user_info.get().roles)
    else:
        return JSON(message='User not found')
# ---
def __init__(
        self,
        *,
        env: dict[str, str] | None = None,
        cwd: str | None = None,
    ):
        """Initialize job group.

        Args:
            env: Base environment dict for commands. If None, uses os.environ.
            cwd: Working directory for commands. If None, uses current directory.
        """
        self._base_env = env
        self._cwd = cwd
        self._processes: list[subprocess.Popen] = []
        self._entered = False
# ---
def test_create_job_ctx_defaults_to_ray_when_initialized():
    """Test that create_job_ctx returns ray context when Ray is initialized."""
    from fray.job import RayContext

    if ray.is_initialized():
        ray.shutdown()

    try:
        ray.init(ignore_reinit_error=True)
        ctx = create_job_ctx()
        assert isinstance(ctx, RayContext)
    finally:
        ray.shutdown()
# ---
def enabled(self):
        if not app.player.paused:
            self.plugin_on_unpaused()
# ---
def test_bool(self):
        """Store and retrieve a boolean"""
        self.make_table()
        self.dynamo.put_item("foobar", {"id": "abc", "b": True})
        item = list(self.dynamo.scan("foobar"))[0]
        self.assertEqual(item["b"], True)
        self.assertTrue(isinstance(item["b"], bool))
# ---
def Vocab(self) -> Axis:
        pass
# ---
def __repr__(self) -> str:
        return f"Timestamp({self.as_formatted_date()})"
# ---
def test___cmp__lt(self):
        self._test__cmp__(
            lambda left, right: left < right,
            (
                False,
                False,
                False,
                True,
                False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
                TypeError if PY3 else False,
            ),
            '<'
        )
# ---
def array_3d(m,n,o):
 array_3d = [[ ['*' for col in range(m)] for col in range(n)] for row in range(o)]
 return array_3d
# ---
def on_begin_job_fetch(self, max):
        self.main_view.show_job_fetch_progress_dialog(max)
# ---
def create_buffer():
        buffer = io.BytesIO()
        test_data.to_parquet(buffer, index=False)
        buffer.seek(0)
        return buffer
# ---
def test_single_expression_change():
    source = "x = 1 + 2\n"
    target = "x = 3 + 4\n"
    edits = tree_diff(source, target)
    assert len(edits) >= 1

    # Applying all edits should produce valid Python.
    current = source
    for edit in edits:
        m = edit.to_mutation(current)
        current = m.apply(current)
    ast.parse(current)
# ---
def _merge_expand(aligned_self, other, overwrite_vars, compat):
    possible_conflicts = dict((k, v) for k, v in aligned_self._variables.items()
                              if k not in overwrite_vars)
    new_vars, new_coord_names = _expand_variables(other, possible_conflicts, compat)
    replace_vars = aligned_self._variables.copy()
    replace_vars.update(new_vars)
    return replace_vars, new_vars, new_coord_names
# ---
def test_basic_check(self):
        result = spell_checker.check(u' .   .   .')

        assert result.errors == 4
        assert result.checked == u'.  .    .'
# ---
def product_types(cls):
    return ['go']
# ---
def _expand_dims(a, *args, **kwargs):
    if isinstance(a, dict):
        return {k: nxp.expand_dims(v, *args, **kwargs) for k, v in a.items()}
    return nxp.expand_dims(a, *args, **kwargs)
# ---
def cube_Sum(n): 
    sum = 0
    for i in range(1,n + 1): 
        sum += (2*i)*(2*i)*(2*i) 
    return sum
# ---
def do_pause(self, e):
        self.remote.do_pause()
# ---
def setUpClass(cls):
    ee.Initialize()
# ---
def __index__(self, /):
        if self.ndim != 0:
            raise TypeError("index is only allowed on arrays with 0 dimensions")
        return operator.index(self.compute())
# ---
def _reset_CC(self):
        """reset CC back to defaults"""
        self.CC.p_h = default_physiological_p_h
        self.CC.p_mg = default_physiological_p_mg
        self.CC.temperature = default_physiological_temperature
        self.CC.ionic_strength = default_physiological_ionic_strength
# ---
def test_llama_tokenizer_needs_long_sequence_workaround():
    tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-hf")
    batch_tokenizer = BatchTokenizer(tokenizer)
    assert batch_tokenizer._needs_long_sequence_workaround
# ---
def test_flip(executor, shape, chunks, axis):
    x = np.random.randint(10, size=shape)
    a = xp.asarray(x, chunks=chunks)
    b = xp.flip(a, axis=axis)

    assert b.chunks == a.chunks

    assert_array_equal(
        b.compute(executor=executor),
        np.flip(x, axis=axis),
    )
# ---
def _get_cached_model_files(cache_dir: str) -> list[str]:
    """Find all model weight files in the HF cache directory."""
    model_files = []
    if os.path.exists(cache_dir):
        for root, _, files in os.walk(cache_dir):
            for f in files:
                if f.endswith((".safetensors", ".bin", ".pt", ".pth")):
                    model_files.append(os.path.join(root, f))
    return model_files
# ---
def _compute_block_assignment(base_ids, index, key):
    rng = jax.random.fold_in(key, index)
    permuted_ids = jax.random.permutation(rng, base_ids)
    return permuted_ids
# ---
def __call__(self, x, *, key):
            return x + self.array + self.static + hax.random.normal(key, x.axes)
# ---
def _resource_preset(*, use_tpu: bool) -> ResourceConfig:
    if use_tpu:
        return ResourceConfig.with_tpu("v5p-8")
    return ResourceConfig.with_gpu("A100-80G", count=1)
# ---
def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True
# ---
def find_peak_util(arr, low, high, n): 
	mid = low + (high - low)/2
	mid = int(mid) 
	if ((mid == 0 or arr[mid - 1] <= arr[mid]) and
		(mid == n - 1 or arr[mid + 1] <= arr[mid])): 
		return mid 
	elif (mid > 0 and arr[mid - 1] > arr[mid]): 
		return find_peak_util(arr, low, (mid - 1), n) 
	else: 
		return find_peak_util(arr, (mid + 1), high, n) 
def find_peak(arr, n): 
	return find_peak_util(arr, 0, n - 1, n)
# ---
def _hf_sleep_with_jitter(delay: float, max_delay: float) -> tuple[float, float]:
    jitter = random.uniform(0.5, 1.5)
    sleep_seconds = min(delay * jitter, max_delay)
    time.sleep(sleep_seconds)
    next_delay = min(delay * 2, max_delay)
    return sleep_seconds, next_delay
# ---
def __init__(self, context, sock):
        self._context = context
        self._sock = sock
        self._connection = OpenSSL.SSL.Connection(context, sock)
        self._makefile_refs = 0
# ---
def __init__(self, offset: int, dtype: np.dtype | type) -> None:
        """Create a description of a bit field.

        Arguments:
            offset: int - The offset of the field within the uint64, in bits.
            dtype: np.dtype | type - The type of the field. The underlying bytes
            of this type are what is stored in the uint64 at this offset.
        """
        self.offset = np.uint64(offset)
        self.dtype = np.dtype(dtype)
# ---
def start(self):
        self.fsm.request('Lonely')
# ---
def environment_type(s):
    if s not in _VALID_ENVIRONMENTS:
      raise argparse.ArgumentTypeError(
          f'Invalid Environment specified: "{s}".')
    return s
# ---
def get_team_code(full_name):
    for code, name in team_mapping.items():
        if name == full_name:
            return code
    return full_name
# ---
def query(self, *, prefix: str | None = None, limit: int = 200) -> list[BufferedLogRecord]: ...
# ---
def __iter__(self):
        """A ``ReadRowsPage`` is an iterator."""
        return self
# ---
def axis_spec_to_tuple(axis_spec: AxisSelection) -> tuple[AxisSelector, ...]: ...
# ---
def __init__(self, opts):
        self.opts = opts
        kind = self.opts.get('__role', '')  # application kind
        if kind not in kinds.APPL_KINDS:
            emsg = ("Invalid application kind = '{0}'.".format(kind))
            log.error(emsg + '\n')
            raise ValueError(emsg)
        self.event = salt.utils.event.get_event(
                kind,
                opts['sock_dir'],
                opts['transport'],
                opts=opts,
                listen=False)
# ---
def GetTaskStatus(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def __init_log(self):
        log = logger.Logger()
        self.logger = log.logger
# ---
def __init__(self, driver):
        super(BrowseMoviePage, self).__init__(driver)
        self.nav = NavBlock(driver)
# ---
def requires(self):
        return RawData()
# ---
def Pos(self) -> Axis:
        pass
# ---
def create_item(cls, text):
        return MyMusicItem(text)
# ---
def visit_Compare(self, node: ast.Compare) -> ast.Compare:
        node.ops = [self._maybe_swap(op, _COMPARE_OPS) for op in node.ops]
        self.generic_visit(node)
        return node
# ---
def execute(self, stmt, params=None, **kw):
                if "test unicode returns" in stmt:
                    raise self.engine.dialect.dbapi.DatabaseError("boom")
                else:
                    return super(MockCursor, self).execute(stmt, params, **kw)
# ---
def total_host_count(self) -> int:
        """Total number of hosts in the pool."""
        return len(self._hosts)
# ---
def setUp(self):
        self.fd, self.path = tempfile.mkstemp()
# ---
def find(cls, params=None):
        if params is None:
            params = dict()
        response = cls(Api.call('sales/detail_sale', params))
        return response.sale
# ---
def default_run_id():
    """Generate a run ID for wandb and continuation.

    Wandb expects a base36 encoded ID of exactly 8 lowercase characters
    or it won't generate a display name."""
    rng_bytes = os.urandom(16)
    run_id = base64.b32encode(rng_bytes)[:8].lower()
    run_id = run_id.decode("utf-8")
    assert len(run_id) == 8
    for char in run_id:
        assert char in "abcdefghijklmnopqrstuvwxyz0123456789"
    return run_id
# ---
def method1(self, a, b, c=10):
        return a + b + c
# ---
def generate_messages(database, writes, stream_id, stream_token):
    # writes can be an array and append to the messages, so it can write multiple Write
    # here just write one as example
    messages = [
            firestore_pb2.WriteRequest(database=database, writes = []),
            firestore_pb2.WriteRequest(database=database, writes = [writes],  stream_id = stream_id, stream_token = stream_token) 
    ]
    for msg in messages:
            yield msg
# ---
def test_valid_relative_path(self):
        """Test that relative paths are accepted."""
        loc = UnresolvedLocation(path="data/test.zarr")
        assert loc.path == "data/test.zarr"
# ---
import re
def change_date_format(dt):
        return re.sub(r'(\d{4})-(\d{1,2})-(\d{1,2})', '\\3-\\2-\\1', dt)
        return change_date_format(dt)
# ---
def echo(self, msg: str) -> str:
        return f"echo: {msg}"
# ---
def cummulative_sum(test_list):
  res = sum(map(sum, test_list))
  return (res)
# ---
def hf_checkpoint_converter(
        self, ref_checkpoint: str = "openai/whisper-base"
    ) -> HFCheckpointConverter["WhisperModel"]:  # type: ignore
        return HFCheckpointConverter(self, reference_checkpoint=ref_checkpoint, ignore_prefix="model")
# ---
def main():
    ragged = build_config(use_gmm=False, name="ragged-dot")
    gmm = build_config(use_gmm=True, name="grouped-matmul")
    executor_main(steps=default_speedrun("pranshu_mixtral_moe_compare_ragged", ragged))
    executor_main(steps=default_speedrun("pranshu_mixtral_moe_compare_gmm", gmm))
# ---
def _wrap_in_buffered_base(fileobj):
    """
    Wraps a file-like object in a BufferedIOBase object.
    This is necessary because HF's upload_folder function expects a BufferedIOBase object.
    """
    if isinstance(fileobj, io.BufferedIOBase):
        return fileobj
    else:
        return io.BufferedReader(fileobj)
# ---
def set_sort(query, sort):
    query['sort'] = []
    for (key, sortdir) in sort:
        sort_dict = dict([(key, 'asc' if sortdir > 0 else 'desc')])
        query['sort'].append(sort_dict)
# ---
def _to_dataframe(self, ordered_dims):
        columns = [k for k in self if k not in self.dims]
        data = [self._variables[k].expand_dims(ordered_dims).values.reshape(-1)
                for k in columns]
        index = self.coords.to_index(ordered_dims)
        return pd.DataFrame(OrderedDict(zip(columns, data)), index=index)
# ---
def default_attention_type() -> AttentionBackend:
    accelerator_type = jax.local_devices()[0].platform
    if accelerator_type == "gpu":
        return AttentionBackend.NVTE
    elif accelerator_type == "tpu":
        return AttentionBackend.SPLASH
    else:
        return AttentionBackend.JAX_FLASH
# ---
def print_user_desc(desc):
    print_field("ID", desc["id"])
    print_field("Name", desc["first"] + " " + ((desc["middle"] + " ") if desc["middle"] != '' else '') + desc["last"])
    if "email" in desc:
        print_field("Email", desc["email"])

    bill_to_label = "Default bill to"
    if "billTo" in desc:
        print_field(bill_to_label, desc["billTo"])

    if "appsInstalled" in desc:
        print_list_field("Apps installed", desc["appsInstalled"])
# ---
def device_is_id(self):
        pattern = r'[A-Za-z0-9]{8}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{12}'
        if re.match(pattern, self.device):
            return True
        return False
# ---
def commalist(propval=''):
    return str(propval).split(',')
# ---
def try_import_module(module):
        try:
            __import__(module)
        except ImportError:
            return False
        else:
            return True
# ---
def testInitialization(self):
    """Tests the initialization."""
    event_formatter = chrome_cache.ChromeCacheEntryEventFormatter()
    self.assertIsNotNone(event_formatter)
# ---
def receive_election():
    '''
    This is a test route to be able to test that callbacks are correctly sent
    '''
    print("ATTENTION received election callback: ")
    print(request.get_json(force=True, silent=True))
    return make_response("", 202)
# ---
def is_wandb_enabled(self):
        return self.wandb_logger.enabled and is_main_process()
# ---
def test_copy_experiment_unauthorized(self):
        """ Tests that copy_experiment fails when unauthorized """
        self.set_roles([])
        experiment = self.create_test_experiment()
        url = reverse("ab_testing_tool_copy_experiment", args=(experiment.id,))
        response = self.client.post(url, follow=True)
        self.assertTemplateUsed(response, "ab_tool/not_authorized.html")
# ---
def __float__(self) -> float:
        """Coerce Metric to float outside of a JIT context."""
        return float(self.value())
# ---
def throw(**_):
            """Throw an exception to terminate the request"""
            raise Exception()
# ---
def list_to_float(test_list):
  res = []
  for tup in test_list:
    temp = []
    for ele in tup:
      if ele.isalpha():
        temp.append(ele)
      else:
        temp.append(float(ele))
    res.append((temp[0],temp[1])) 
  return (str(res))
# ---
def set_content(content):
    xbmcplugin.setContent(int(sys.argv[1]), content)
# ---
def conjugate(a: A) -> A:
    return wrap_elemwise_unary(jnp.conjugate, a)
# ---
def argmax(array: NamedArray, axis: AxisSelector | None) -> NamedArray:
    return wrap_reduction_call(jnp.argmax, array, axis, None, single_axis_only=True, supports_where=False)
# ---
def test_pad(spec):
    an = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = cubed.pad(a, ((1, 0), (0, 0)), mode="symmetric")
    assert b.chunks == ((2, 2), (2, 1))

    assert_array_equal(b.compute(), np.pad(an, ((1, 0), (0, 0)), mode="symmetric"))
# ---
def test_initial_state(self):
        self.assertEqual(self.mda.control_reg, 0x00)
        self.assertEqual(self.mda.control_reg, 0x00)
        self.assertEqual(self.mda.screen, None)
        self.assertEqual(self.mda.char_generator, self.cg)
        self.assertEqual(len(self.mda.video_ram), 4096)
# ---
def test_write_vortex_basic(self, tmp_path):
        """Test basic vortex file writing."""
        records = [{"id": i, "value": i * 2} for i in range(10)]
        output_path = tmp_path / "output.vortex"

        result = write_vortex_file(records, str(output_path))

        assert result["count"] == 10
        assert output_path.exists()

        # Verify roundtrip
        loaded = list(load_vortex(str(output_path)))
        assert loaded == records
# ---
def __init__(self):
        self.rebus_controller = None
# ---
def __init__(self, controller_url: str, log_tree: LogTree, logger: SmokeTestLogger):
        self._client = ControllerServiceClientSync(controller_url)
        self._scheduling_dir = log_tree.get_dir("scheduling", "Task scheduling snapshots")
        self._logger = logger
        self._stop_event = threading.Event()
        self._thread: threading.Thread | None = None
        self._tracked_jobs: set[str] = set()
# ---
def __init__(self, rules=None, **kwargs):
        self.tokens = []
        self.def_links = {}
        self.def_footnotes = {}

        if not rules:
            rules = self.grammar_class()

        self.rules = rules
# ---
def test_collect_workdir_size_mb_with_temp_directory(tmp_path):
    """Test workdir size calculation with a temporary directory."""
    (tmp_path / "file1.txt").write_text("x" * 1024 * 100)  # 100 KB
    (tmp_path / "file2.txt").write_text("y" * 1024 * 100)  # 100 KB

    size_mb = collect_workdir_size_mb(tmp_path)

    assert size_mb >= 1
# ---
def get_directory_friendly_dataset_name(hf_dataset_id: str) -> str:
    dataset_name = hf_dataset_id.replace("/", "--")
    dataset_name = dataset_name.replace(".", "-")
    dataset_name = dataset_name.replace("#", "-")
    return dataset_name
# ---
def name(self) -> str:
        return "ray"
# ---
def test_encrypt_newlines_inside_message(self):
        self._test_encryption('Message\nwith\ninterior\nnewlines.')
# ---
def he_normal_init_(weights):
    trunc_normal_init_(weights, scale=2.0)
# ---


def median(l: list):
    """Return median of elements in the list l.
    >>> median([3, 1, 2, 4, 5])
    3
    >>> median([-10, 4, 6, 1000, 10, 20])
    15.0
    """
    l = sorted(l)
    if len(l) % 2 == 1:
        return l[len(l) // 2]
    else:
        return (l[len(l) // 2 - 1] + l[len(l) // 2]) / 2.0
# ---
def test_fixed_window_with_elastic_expiry_cluster(self):
        storage = MemcachedStorage("memcached://localhost:22122,localhost:22123")
        limiter = FixedWindowElasticExpiryRateLimiter(storage)
        per_sec = RateLimitItemPerSecond(2, 2)

        assert limiter.hit(per_sec)
        time.sleep(1)
        assert limiter.hit(per_sec)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert not limiter.test(per_sec)
        time.sleep(1)
        assert limiter.test(per_sec)
# ---
def _trigger_callback(self):
        self.callback()
# ---
def __init__(self, method: Any, executor: ThreadPoolExecutor):
        self._method = method
        self._executor = executor
# ---
def f(idx):
        return named1["H", idx]
# ---
def to_jax_shape(shape):
    from haliax.core import Axis, ensure_tuple

    shape = ensure_tuple(shape)
    return tuple(axis.size if isinstance(axis, Axis) else axis for axis in shape)
# ---
def vm_count(appliance, metrics_tbl, mgmt_system_id):
    return bool(appliance.db.client.session.query(metrics_tbl).filter(
        metrics_tbl.parent_ems_id == mgmt_system_id).filter(
        metrics_tbl.resource_type == "VmOrTemplate").count()
    )
# ---
def __repr__(self):
        return f"TakePerShardOp(n={self.n})"
# ---
def get_converted_vm(job_id):
    try:
        job = _get_job(job_id)
        _validate_job_done(job)
        ovf = _read_ovf(job_id)
    except ClientError as e:
        logging.info('Converted VM error %s', e)
        return errCode[e.err_name]
    except V2VError as e:
        logging.error('Converted VM error %s', e)
        return errCode[e.err_name]
    return {'status': doneCode, 'ovf': ovf}
# ---
def inner(method, notebook, data, *args, **kwargs):
        if not is_logged():
            return views.render('login_form', notebook)
        return view(method, notebook, data, *args, **kwargs)
# ---
def wait_async_x_writes():
        x_write_future.wait()
# ---
def temporal_diff_sum(self, m, k):
        Nm = self.data[m].shape[0] - 1
        result = 0

        for s in range(k, Nm):
            result += self.lf**(s - k) * self.temporal_diff(m, s)

        return result
# ---
def getshort(code) :
    maxl = 5
    newcode = code.replace('http://', '')
    if len(newcode) > maxl :
          newcode = newcode[0:maxl]
    return str(newcode)
# ---
def logout(method, notebook, data):
    global SAAGIE_BASIC_AUTH_TOKEN
    global SAAGIE_ROOT_URL
    global SAAGIE_USERNAME
    SAAGIE_BASIC_AUTH_TOKEN = None
    SAAGIE_ROOT_URL = None
    SAAGIE_USERNAME = None
    return {}
# ---
def test_ema_phase():
    ma = EmaDecaySqrtModelAveraging(model=_dummy(0.0), beta=0.5, switch_step=5, decay_steps=10)
    ma = ma.update(_dummy(1.0), step=0)
    assert jnp.allclose(ma.model["p"], 1.0)
    assert jnp.isclose(ma.tot_weight, 0.5)
# ---
def init_worker(ctx, name):
    """Initialize Ray on a manual TPU worker."""
    config_obj = ctx.obj.config_obj
    print(f"Initializing Ray on worker {name}...")
    _initialize_manual_worker(config_obj.config_file, name)
    print("Worker initialized successfully!")
# ---
def hf_checkpoint_converter(cls) -> "HFCheckpointConverter":
        """The default HFCheckpointConverter to use for this config class. We recommend that you
        define this as a @cached_property on your config class."""
        pass
# ---
def test_multiple_jobs_complete(self, test_cluster):
        """Multiple jobs complete successfully."""
        run_id = uuid.uuid4().hex[:8]

        def fast_job(n):
            return n * 2

        job_ids = [test_cluster.submit(fast_job, i, name=f"job-{run_id}-{i}") for i in range(5)]
        for job_id in job_ids:
            status = test_cluster.wait(job_id, timeout=30)
            assert status["state"] == "JOB_STATE_SUCCEEDED"
# ---
def Seq_Linear(seq_nums):
  seq_nums = [seq_nums[x] - seq_nums[x-1] for x in range(1, len(seq_nums))]
  if len(set(seq_nums)) == 1: 
    return "Linear Sequence"
  else:
    return "Non Linear Sequence"
# ---
def __init__(self, df, lmbda = 0):
        pass
# ---
def __rdivmod__(self, other) -> "NamedArray":  # pragma: no cover
        return haliax.divmod(other, self)
# ---
def style_css(self, val):
        self._style_css = '' + val
# ---
def add_price(self, price_list=None):
		'''Add a new price'''
		if not price_list:
			price_list = (frappe.db.get_single_value('Selling Settings', 'selling_price_list')
						or frappe.db.get_value('Price List', _('Standard Selling')))
		if price_list:
			item_price = frappe.get_doc({
				"doctype": "Item Price",
				"price_list": price_list,
				"item_code": self.name,
				"currency": erpnext.get_default_currency(),
				"price_list_rate": self.standard_rate
			})
			item_price.insert()
# ---
def auto_stop(cluster: Cluster, job_id: str, should_stop: bool):
    """Terminate `job_id` on exit if `should_stop` is True."""
    if not should_stop:
        yield
        return

    try:
        yield
    finally:
        click.echo(f"Auto-stopping job {job_id}...")
        try:
            cluster.terminate(job_id)
        except Exception as e:
            logger.error(f"Failed to stop: {e}")
# ---
def train_set(
        self,
        options: CacheOptions = CacheOptions.default(),
        *,
        key: Optional[PRNGKeyArray] = None,
    ) -> ProcessedAudioCache:
        ds = self.build_or_load_cache(self.train_split)
        if ds is None:
            raise ValueError("No training set!")
        return ds
# ---
def format_move_dict_to_root(obj, field):
    for attr in obj[field]:
        obj["%s/%s" % (field, attr)] = obj[field][attr]
    del obj[field]
# ---
def load_mongo(self, mongo_uri: Union[str, None] = None):
        if mongo_uri:
            self.mongo_uri = mongo_uri
            self.client = MongoClient(mongo_uri)
        else:
            self.mongo_uri = "localhost:27017"
            self.client = MongoClient()

        self._core = self.client["core"]
# ---
def test_pending(self):
        from iris.rpc import cluster_pb2

        assert map_iris_job_state(cluster_pb2.JOB_STATE_PENDING) == JobStatus.PENDING
# ---
def read_int_from_file(path):
    with fsspec.open(path) as f:
        return int(f.read())
# ---
def Heads(self) -> Axis:
        return Axis("heads", self.num_heads)
# ---
def __str__(self):
        if not self:
            return "No problems found"
        else:
            return "\n".join(
                [
                    "Found some problems with your module:",
                    *self._format_reused_arrays(),
                    *self._format_static_arrays(),
                ]
            )
# ---
def _to_list_of_dicts(batch: dict) -> list[dict]:
    """
    Convert a batch of dictionaries to a list of dictionaries, suitable for writing to a cache.
    """
    keys = list(batch.keys())
    values = list(batch.values())
    num_rows = len(values[0])
    return [{key: values[i][j] for i, key in enumerate(keys)} for j in range(num_rows)]
# ---
def residual_linear(x, W, x_skip, residual_scale):
  """x_skip + residual_scale * W @ x"""
  dim_out, dim_in = W.shape[0], W.shape[1]
  return torch.addmm(
    x_skip.view(-1, dim_out),
    x.view(-1, dim_in),
    W.T,
    alpha=residual_scale,
  ).view(*x.shape[:-1], dim_out)
# ---
def push(self, item: T_co) -> None:
        """Add an item to the queue."""
        ...
# ---
def test_asarray_from_array(spec):
    a = xp.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]], chunks=(2, 2), spec=spec)
    b = xp.asarray(a)
    assert b is a
    assert_array_equal(b.compute(), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))
# ---
def fechatimbrado(self):
        return self.__fechatimbrado
# ---
def _assert_all_in_dataset(self, names, virtual_okay=False):
        bad_names = set(names) - set(self._variables)
        if virtual_okay:
            bad_names -= self.virtual_variables
        if bad_names:
            raise ValueError('One or more of the specified variables '
                             'cannot be found in this dataset')
# ---
def fsspec_mtime(file_path: str) -> datetime:
    """Get file modification time (in seconds since epoch) of a file on an `fsspec` filesystem."""
    fs = fsspec.core.url_to_fs(file_path)[0]

    return fs.modified(file_path)
# ---
def discnav(self, command):
        self.command('discnav', command)
# ---
def wait_ready(self, count: int | None = None, timeout: float = 300.0) -> list[ActorHandle]:
        """Return ready actor handles. Local actors are ready immediately."""
        if count is None:
            count = len(self._handles)
        self._yielded = True
        return self._handles[:count]
# ---
def genetic_modification_1(lab, award):
    return {
        'modification_type': 'deletion',
        'award': award['uuid'],
        'lab': lab['uuid'],
        'modifiction_description': 'some description'
    }
# ---
def invalidate_deadline_caches(sender, **kwargs):  # pylint: disable=unused-argument
    """Invalidate the cached verification deadline information. """
    cache.delete(VerificationDeadline.ALL_DEADLINES_CACHE_KEY)
# ---
def __init__(self, routes, **kwargs):
        super(ProxyArchivalRouter, self).__init__(routes, **kwargs)
        self.proxy = ProxyRouter(routes, **kwargs)
# ---
def build(self, HeadSize: Axis) -> RotaryEmbeddings:
        pass
# ---
def __call__(
        self,
        lhs,
        rhs,
        dimension_numbers,
        precision: PrecisionLike = None,
        preferred_element_type: DTypeLike | None = None,
        **kwargs,
    ) -> jnp.ndarray: ...
# ---
def test_duplicate(arraynums):
    nums_set = set(arraynums)    
    return len(arraynums) != len(nums_set)
# ---
def __lt__(self, other: "Duration") -> bool:
        return self._ms < other._ms
# ---
def test_olmo2_decoder_layer(use_flash, num_kv_heads):
    config = _get_olmo2_config(use_flash=use_flash, num_kv_heads=num_kv_heads)
    key = random.PRNGKey(0)

    layer = Olmo2DecoderLayer.init(config=config, key=key)

    x, mask = _get_random_inputs(config)
    out = layer(x, mask)

    # Check output has correct shape
    assert out.array.shape == x.array.shape
    assert out.axes == x.axes
# ---
def convert_all_json_to_yaml(self, dirpath):
        for path in os.listdir(dirpath):
            if not path.endswith('.template') and not path.endswith('.json'):
                continue
            f = open(os.path.join(dirpath, path), 'r')
            json_str = f.read()

            yml_str = template_format.convert_json_to_yaml(json_str)
            yield (json_str, yml_str, f.name)
# ---
def make_author():
    return Author(
        id=fake.random_int(),
        first_name=fake.first_name(),
        last_name=fake.last_name(),
        twitter=fake.domain_word(),
    )
# ---
def log(self, action: str, entity_id: str, **details: Any) -> None:
        """Record an action taken during event handling."""
        self.actions.append(
            Action(
                timestamp=Timestamp.now(),
                action=action,
                entity_id=str(entity_id),
                details=details,
            )
        )
# ---
def main(argv=sys.argv[1:]):
    try:
        docopt.docopt(__doc__, argv=argv, version=hello.__version__)
    except docopt.DocoptExit as e:
        print(str(e), file=sys.stderr)
        return 2
    except SystemExit as e:
        return 0
# ---
def __list_handlers(self):
        """ Return a dict of { handler_name, method, ... }.
        All methods that do not being with an underscore will be exposed.
        We also make sure to not expose our register_rpc method.
        """
        handlers = {}
        for attr in dir(self):
            if self.__is_public_valid_method(attr):
                handlers[attr] = getattr(self, attr)
        return handlers
# ---
def bitwise_invert(x, /):
    if x.dtype not in _integer_or_boolean_dtypes:
        raise TypeError("Only integer or boolean dtypes are allowed in bitwise_invert")
    return elemwise(nxp.bitwise_invert, x, dtype=x.dtype)
# ---
def __init__(self, bot):
        self.bot = bot
        """
        imLink = http://services.runescape.com/m=hiscore_ironman/index_lite.ws?player=
        nmLink = http://services.runescape.com/m=hiscore/index_lite.ws?player=
        """
# ---
def _encode_options(options):
        return ','.join('{}={}'.format(str(key), str(val)) for key, val in options.items())
# ---
def prefix_info(lines, software, version, author, comment_mark='//'):
    """Prefix information to the given lines with given comment-mark.
    """
    prefix = ['%s Generated by the %s v%s' % (comment_mark,
              software, version)]
    prefix += ['%s    !author: %s' % (comment_mark, author)]
    prefix += ['%s    !trail: %s %s' % (comment_mark,
               os.path.basename(sys.argv[0]), ' '.join(sys.argv[1:]))]
    return prefix + lines
# ---
def _unflatten_bshd(attn_output, q_class, v_class):
    attn_output = attn_output.unflatten_axis("B", q_class["B"])
    attn_output = attn_output.unflatten_axis("S", q_class["S"])
    attn_output = attn_output.unflatten_axis("H", q_class["H"])
    attn_output = attn_output.unflatten_axis("D", v_class["D"])
    return attn_output
# ---
def okPopup(title, msg, answerCallback):
    content = OkPopup(text=msg)
    content.bind(on_ok=answerCallback)
    popup = Popup(title=title,
                    content=content,
                    size_hint=(None, None),
                    size=(dp(600),dp(200)),
                    auto_dismiss= False)
    popup.open()
    return popup
# ---
def test_as_remote_kwargs_gpu():
    from fray.v2.ray_backend.resources import as_remote_kwargs

    config = ResourceConfig(device=GpuConfig(variant="H100", count=2))
    kwargs = as_remote_kwargs(config)
    assert kwargs["num_gpus"] == 2
    assert kwargs["accelerator_type"] == "H100"
# ---
def get_transfer_info(self) -> tuple[int | None, str | None]:
        """
        Returns the latest weight ID and transfer server address without blocking.
        Returns (None, None) if no weights are available or server not registered.
        """
        return self._latest_weight_id, self.transfer_server_address
# ---
def test_dslice_with_selector():
    B, S, V = Axis("batch", 2), Axis("seq", 5), Axis("vocab", 10)
    x = hax.arange((B, S, V))
    idx = (hax.arange((B, S), dtype=jnp.int32) + 2) % 4
    shard = V.resize(4)
    x_shard = x["vocab", dslice(0, shard)]
    out = x_shard["vocab", idx]
    assert out.axes == (B, S)
    ref = x.array[:, :, :4][jnp.arange(B.size)[:, None], jnp.arange(S.size)[None, :], idx.array]
    assert jnp.array_equal(out.array, ref)
# ---
def remaining_seconds(self) -> float:
        """Get remaining seconds until deadline (0.0 if expired)."""
        return max(0.0, self._deadline - time.monotonic())
# ---
def test_visualize_shardings_inside_jit(capsys):
    mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(-1, 1), (ResourceAxis.DATA, ResourceAxis.MODEL))

    @named_jit
    def fn(x):
        x = hax.shard(x)
        visualize_shardings(x)
        return x

    with axis_mapping({"dim1": ResourceAxis.DATA}), mesh:
        x = hax.ones({"dim1": 8 * len(jax.devices())})
        x = hax.shard(x)
        fn(x)

    out = capsys.readouterr().out
    assert "dim1" in out
# ---
def GetBufferFilepath( buffer_object ):
  if buffer_object.name:
    return buffer_object.name
  # Buffers that have just been created by a command like :enew don't have any
  # buffer name so we use the buffer number for that.
  return os.path.join( GetCurrentDirectory(), str( buffer_object.number ) )
# ---
def super_seq(X, Y, m, n):
	if (not m):
		return n
	if (not n):
		return m
	if (X[m - 1] == Y[n - 1]):
		return 1 + super_seq(X, Y, m - 1, n - 1)
	return 1 + min(super_seq(X, Y, m - 1, n),	super_seq(X, Y, m, n - 1))
# ---
def update_gg(idx, gg):
        if gg is None:
            return None
        outer_product = jnp.tensordot(
            grad,
            grad,
            axes=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]] * 2,
            precision=precision,
        )
        return lerp(gg, outer_product, 1 - beta)
# ---
def ones(shape: AxisSpec, dtype: DTypeLike | None = None) -> NamedArray:
    """Creates a NamedArray with all elements set to 1"""
    if dtype is None:
        dtype = jnp.float32
    return full(shape, 1, dtype)
# ---
def matches_pattern(file_path: pathlib.Path, patterns: list[str]) -> bool:
    relative_path = str(file_path.relative_to(ROOT_DIR))
    for pattern in patterns:
        if fnmatch.fnmatch(relative_path, pattern):
            return True
    return False
# ---
def try_load_hf(model_id):
        try:
            AutoConfig.from_pretrained(model_id)
        except Exception:
            return False
        else:
            return True
# ---
def _cdata_shape(self) -> T_Shape:
        """The shape of the chunk grid for this array."""
        return tuple(
            starmap(
                lambda s, c: math.ceil(s / c),
                zip(self.shape, self.chunks, strict=False),
            )
        )
# ---
def log_summary(self, metrics: dict[str, Any]):
        for k, v in metrics.items():
            if _is_scalar(v):
                self.writer.add_scalar(k, v, global_step=None)
            elif isinstance(v, str):
                self.writer.add_text(k, v, global_step=None)
            else:
                pylogger.error(f"Unsupported metric type: {type(v)} for key {k}")
# ---
def decorate_as_link(self, url, link_type, name):
        allure_link_marker = '{prefix}.{link_type}'.format(prefix=ALLURE_LINK_PREFIX, link_type=link_type)
        pattern = dict(self.config.option.allure_link_pattern).get(link_type, u'{}')
        url = pattern.format(url)
        allure_link = getattr(pytest.mark, allure_link_marker)
        return allure_link(url, name=name, link_type=link_type)
# ---
def release(self, lease: Lease[T]) -> None:
        timestamp = 0.0
        unique_id = uuid.uuid4()
        filename = f"{timestamp:.6f}_{unique_id}.pkl"

        try:
            self.fs.mv(self.processing_dir / lease.lease_id, self.pending_dir / filename)
        except FileNotFoundError:
            raise ValueError(f"Invalid lease: {lease.lease_id} not found during release") from None
# ---
def set_debuglevel(self, level):
        self.level = level
# ---
def execution_timing(func):
    """Decorator to measure timing information of a function call."""

    return partial(execute_with_timing, func)
# ---
def tile(x, repetitions, /):
    N = len(x.shape)
    M = len(repetitions)
    if N > M:
        repetitions = (1,) * (N - M) + repetitions
    elif N < M:
        for _ in range(M - N):
            x = expand_dims(x, axis=0)
    out = x
    for i, nrep in enumerate(repetitions):
        if nrep > 1:
            out = concat([out] * nrep, axis=i)
    return out
# ---
def the_object_name_is_not_a_filling(name):
    ifc = IfcStore.get_file()
    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)
    if any(element.FillsVoids):
        assert False, "A filling was found"
# ---
def _parse_ports_from_env(env: dict[str, str] | None = None) -> dict[str, int]:
    source = env if env is not None else os.environ
    ports = {}
    for key, value in source.items():
        if key.startswith("IRIS_PORT_"):
            port_name = key[len("IRIS_PORT_") :].lower()
            ports[port_name] = int(value)
    return ports
# ---
def sort(self, objs):
        # TODO: Conversion and null-detection here. In Python 3,
        # comparisons with None fail. We should also support flexible
        # attributes with different types without falling over.

        def key(item):
            field_val = item.get(self.field, '')
            if self.case_insensitive and isinstance(field_val, str):
                field_val = field_val.lower()
            return field_val

        return sorted(objs, key=key, reverse=not self.ascending)
# ---
def test_slice_nd_array_present_dims():
    # tests slicing with arrays that are already present in the named array, which is sometimes ok
    H, W, D = hax.make_axes(H=10, W=20, D=30)

    named1 = hax.random.uniform(PRNGKey(0), (H, W, D))

    index1 = hax.random.randint(PRNGKey(0), (H,), 0, H.size)

    # this is ok, since the H would be eliminated anyway
    assert jnp.all(jnp.equal(named1[{"H": index1}].array, named1.array[index1.array, :, :]))
# ---
def has_notes(self):
        '''Return True/False if this track has any note-on/note-off pairs defined.
        '''
        for event in self.events:
            if event.is_note_on():
                return True
        return False
# ---
def setup(self, links, footnotes):
        self.footnote_index = 0
        self.links = links or {}
        self.footnotes = footnotes or {}
# ---
def get_lm_head(self) -> hax.NamedArray:
        return self.embeddings.token_embeddings.weight
# ---
def socket(self):
        if not hasattr(self, "_socket"):
            self._get_address()
        return self._socket
# ---
def __int__(self, /):
        if self.ndim != 0:
            raise TypeError("int is only allowed on arrays with 0 dimensions")
        if self.dtype in _complex_floating_dtypes:
            raise TypeError("int is not allowed on complex floating-point arrays")
        return int(self.compute())
# ---
def to_sharding(node: typing.Any, spec: typing.Any):
        if spec is None:
            if isinstance(node, NamedArray):
                return getattr(node.array, "sharding", None)
            elif is_jax_array_like(node):
                return getattr(node, "sharding", None)
            else:
                return None
        else:
            return NamedSharding(resolved_mesh, spec)
# ---
def test_bank_has_if_and_for(bank):
    assert bank.has_type("If"), "Should extract If statements"
    assert bank.has_type("For"), "Should extract For loops"
# ---
def _coerce_iterator_output(self, expr, state=None):
        import supriya.patterns

        if not isinstance(expr, supriya.patterns.Event):
            expr = supriya.patterns.NoteEvent(**expr)
        if expr.get("uuid") is None:
            expr = new(expr, uuid=uuid.uuid4())
        return expr
# ---
def Find_Max_Length(lst):  
    maxLength = max(len(x) for x in lst )
    return maxLength
# ---
def setUp(self):
        super(JsonYamlResolvedCompareTest, self).setUp()
        self.longMessage = True
        self.maxDiff = None
# ---
def _clamp(minimum: int, x: int, maximum: int) -> int:
    return max(minimum, min(x, maximum))
# ---
def _convert_to_jnp(v, dtype):
    import torch

    # we'd rather not convert to float32 to conserve memory, so we convert direct to jax.numpy
    with use_cpu_device():
        if v is None:
            return None
        elif v.dtype == torch.bfloat16:
            arr = jax.numpy.array(v.cpu().view(torch.float16).numpy()).view(jax.numpy.bfloat16)
        else:
            arr = jax.numpy.array(v.cpu().numpy())

        if dtype is not None:
            arr = arr.astype(dtype)

        return arr
# ---
def on_deactivate():
            """The window was deactivated.

            This event can be triggered by clicking on another application
            window.  When a window is deactivated it no longer has the
            keyboard focus.

            :event:
            """
# ---
def normalize_math_symbols(text: str) -> str:
    """Replace LaTeX symbols with text equivalents."""
    for latex, replacement in MATH_SYMBOL_REPLACEMENTS.items():
        text = text.replace(latex, replacement)
    return text
# ---
def test_end_with_ellipsis():
    partial_order = ("apple", ..., "banana", ...)
    candidates = ("banana", "apple", "cherry")
    actual_output = rearrange_for_partial_order(partial_order, candidates)
    assert_partial_order_respected(partial_order, actual_output)

    # this one could be either but we'll assert the order so we catch changes
    assert actual_output == ("apple", "banana", "cherry")
# ---
def forward(self, dist):
        dist = dist - self.offset.view(1, 1, 1, -1)
        return torch.exp(self.coeff * torch.pow(dist, 2))
# ---
def _is_special_module(module):
        # TODO: add conv?
        return isinstance(module, hnn.Linear) or isinstance(module, hnn.Stacked)
# ---
def reduce_edge_list(l):
        tally = 1
        for i in range(len(l)):
            reversed_idx = -1 * (i + 1)
            l[reversed_idx] *= tally
            tally = l[reversed_idx]
# ---
def _try_delete_vm(vm: dict, zone: str) -> bool:
    """
    Try to delete a VM. Returns True if deletion was successful.
    """
    logging.warning(f"[{zone}] TPU {vm['name']} is in state {vm['state']}, deleting...")
    try:
        delete_tpu_vm(vm["name"], zone)
        return True
    except Exception as e:
        logging.error(f"[{zone}] Failed to delete {vm['name']}: {e}")
        return False
# ---
def _is_object_store_path(path: str) -> bool:
    parsed = urlparse(path)
    return parsed.scheme in {"gs", "s3"}
# ---
def __init__(self, name, params):
        base.InstantiationValidationBenchmark.__init__(self, name, params)

        if common.RELEASE == 'liberty':
            temp_name = 'stress_workload_liberty.yaml'
        else:
            temp_name = 'stress_workload.yaml'

        self.template_file = common.get_template_dir() + \
            temp_name
        self.stack_name = 'neighbour'
        self.neighbor_stack_names = list()
# ---
def __post_init__(self):
        if " " in self.name:
            raise ValueError("Job name must not contain spaces")
# ---
def workspace(tmp_path):
    """Create a test workspace."""
    (tmp_path / "pyproject.toml").write_text("[project]\nname = 'test'\n")
    (tmp_path / "src").mkdir()
    (tmp_path / "src" / "main.py").write_text("print('hello')")
    (tmp_path / "__pycache__").mkdir()
    (tmp_path / "__pycache__" / "main.cpython-312.pyc").write_bytes(b"cached")
    return tmp_path
# ---
def _partitions(lst):
    """Generate all partitions of a list."""
    if not lst:
        yield [[]]
    else:
        for i in range(len(lst)):
            for part in _partitions(lst[i + 1 :]):
                yield [lst[: i + 1]] + part
# ---
def filter_stackexchange(config: FilterStackExchangeConfig):
    """Filter StackExchange data by vote threshold and remove duplicates."""
    pipeline = (
        Dataset.from_files(f"{config.input_path}/*.jsonl.gz")
        .flat_map(lambda path: _process_file_with_filtering(path, config))
        .write_jsonl(f"{config.output_path}/data-{{shard:05d}}-of-{{total:05d}}.jsonl.gz")
    )

    Backend.execute(pipeline)
# ---
def test_optimize_with_take():
    """Take should be fused with map and filter."""
    ds = Dataset(
        source=[1, 2, 3],
        operations=[
            MapOp(lambda x: x * 2),
            TakePerShardOp(10),
            FilterOp(lambda x: x > 5),
        ],
    )
    plan = compute_plan(ds)

    assert len(plan.stages) == 1
    fused_op = plan.stages[0].operations[0]
    assert isinstance(fused_op, Map)
# ---
def __getAvailableProfiles(self):
        """Get available user profiles."""
        return _settingsManager.availableProfiles()
# ---
def close(self):
        self._file.close()
# ---
def fake_vdi_resize(*args, **kwargs):
            self.called = True
# ---
def ohc_map(ohc_intz):
    # return last year - first year
    return ohc_intz.isel(time=slice(-73, None)).mean("time") - ohc_intz.isel(
        time=slice(0, 73)
    ).mean("time")
# ---
def dim2loss(d, dim0=max_dim):
        """A heuristic map from dim to loss with the least loss occurs at dim0."""
        loss = 0
        if d < dim0:
            loss += np.log2(dim0 / d)
            too_small = dim0 / 8
            if d < too_small:
                loss += 100 * np.log2(too_small / d)
        else:
            loss += 10 * np.log2(d / dim0)
            too_large = 8 * dim0
            if d > too_large:
                loss += 1000 * np.log2(d / too_large)
        return loss
# ---
def info(self):
        raise NotImplementedError
# ---
import heapq as hq
def raw_heap(rawheap):
  hq.heapify(rawheap)
  return rawheap
# ---
def chip_count(self) -> int:
        return 0
# ---
def test_from_iterable(backend):
    """Test creating dataset from iterable."""
    ds = Dataset.from_iterable(range(5))
    assert list(Backend.execute(ds, context=backend)) == [0, 1, 2, 3, 4]
# ---
def top_right_corner3d(self):
        return self.edge_points3d[1]
# ---
def tester(code):
            def func(cls):
                return cls.run_test(code)
            return func
# ---
def closeEvent(self, event: QCloseEvent):
        settings.write("{}/geometry".format(self.__class__.__name__), self.saveGeometry())
        super().closeEvent(event)
# ---
def __init__(self, subtype, msg=None):
        if msg is None:
            msg = "An error occured for subtype {}".format(subtype)
        super(PhylotyperError, self).__init__(msg)
        self.subtype = subtype
# ---
def RunTask(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")
# ---
def updatelastradio(url) :
    db = cherrypy.session['database']
    sql = "UPDATE Radio SET url='%s' WHERE id=0" % (url)
    try:
        con = lite.connect( db )
        cur = con.cursor()
        cur.execute(sql)
        con.commit()
        con.close()
    except:
        return
# ---
def optimizer_step(self, *args, **kwargs):
    super().optimizer_step(*args, **kwargs)
    if self.ema:
      self.ema.update(itertools.chain(
        self.backbone.parameters(),
        self.noise.parameters()))
# ---
def height(self):
        """The height of the window, in pixels.  Read-write.

        :type: int
        """
        return self.get_size()[1]
# ---
def _test_do_executemany(self, retval):
        with self._run_test(retval) as (conn, m1):
            result = conn.execute("insert into table foo",
                            [{"foo": "bar"}, {"foo": "bar"}])
        self._assert(
            retval,
            m1.do_executemany, m1.real_do_executemany,
            [call(
                    result.context.cursor,
                    "insert into table foo",
                    [{"foo": "bar"}, {"foo": "bar"}], result.context)]
        )
# ---
def _stack_batch_encodings(a: BatchEncoding, b: BatchEncoding) -> BatchEncoding:
    """Stacks two batch encodings together, assuming that the keys are the same."""

    def _ensure_batched(x):
        if len(x) == 0:
            return list(x)
        elif isinstance(x[0], Sequence) or isinstance(x[0], np.ndarray):
            return list(x)
        else:
            return [x]

    return BatchEncoding({k: _ensure_batched(a[k]) + _ensure_batched(b[k]) for k in a.keys()})
# ---
def _run_binary(entry: BinaryEntrypoint) -> None:
    subprocess.run(
        [entry.command, *entry.args],
        check=True,
        capture_output=True,
        text=True,
    )
# ---
def on_test_end(
        self,
        trainer: Trainer,  # noqa: ARG002
        pl_module: LightningModule,
    ) -> None:
        """Restore original weights after testing.

        Parameters
        ----------
        trainer: Trainer
            The Trainer instance.
        pl_module: LightningModule
            The LightningModule instance.

        """
        self._on_eval_end(pl_module)
# ---
def discreize(self, x):
    hard_sample = self._hard_sample(x)
    soft_sample = self._soft_sample(x)
    return soft_sample + (hard_sample - soft_sample).detach()
# ---
def get_bias_dropout_add_scale(training):
  def _bias_dropout_add(x, bias, scale, residual, prob):
    return bias_dropout_add_scale(
      x, bias, scale, residual, prob, training
    )

  return _bias_dropout_add
# ---
def sync_backend(request):
    """Backend fixture for sync and threadpool backends."""
    return create_job_ctx(request.param, max_workers=2)
# ---
def define_tables(cls, metadata):
        Table(
            "some_table",
            metadata,
            Column("id", Integer, primary_key=True),
            Column("x", Integer),
            Column("y", Integer),
            Column("q", String(50)),
            Column("p", String(50)),
        )
# ---
def quiet_startup() -> None:
    warnings.filterwarnings("ignore", message=r".*predict_dataloader.*num_workers.*")
    warnings.filterwarnings(
        "ignore", message=r".*tensorboardX.*removed as a dependency.*"
    )
    warnings.filterwarnings(
        "ignore",
        message=r"The pynvml package is deprecated",
        category=FutureWarning,
        module=r"torch\.cuda",
    )

    logging.getLogger("pytorch_lightning").setLevel(logging.ERROR)
# ---
def add_model(self, model):
        assert isinstance(model, Polyhedron)
        self.models.append(model)
# ---
def testFunctionDecoratorWithArg(self):
    self.assertEqual((0, '<b id=red>foo</b>\n'), _GrumpRun(textwrap.dedent("""\
        def tag(name):
          def bold(fn):
            return lambda: '<b id=' + name + '>' + fn() + '</b>'
          return bold
        @tag('red')
        def foo():
          return 'foo'
        print foo()""")))
# ---
def set_class_name(self, cls, name):
        super(Many2one, self).set_class_name(cls, name)
        # determine self.delegate
        if not self.delegate:
            self.delegate = name in cls._inherits.values()
# ---
def delete(name: str, zone: str):
    """Delete a TPU VM by name in the specified zone."""
    delete_tpu_vm(name, zone)
# ---
def create_dict(self):
        ''' assign the correct properties for a secret dict '''
        self.data['apiVersion'] = 'v1'
        self.data['kind'] = 'Secret'
        self.data['metadata'] = {}
        self.data['metadata']['name'] = self.name
        self.data['metadata']['namespace'] = self.namespace
        self.data['data'] = {}
        if self.secrets:
            for key, value in self.secrets.items():
                self.data['data'][key] = value
# ---
def add_worker(ctx, tpu_type, capacity, name):
    """Add manual TPU worker to cluster."""
    config_obj = ctx.obj.config_obj
    print(f"Adding {tpu_type} worker with {capacity} capacity...")
    _add_manual_worker(config_obj, tpu_type, capacity, name)
    print("Worker added successfully!")
# ---
def wait_ready(self, count: int | None = None, timeout: float = 300.0) -> list[ActorHandle]: ...
# ---
def test_olmo2_param_counts_dont_change_with_seqlen():
    model = Olmo2LMHeadModel.init(hax.Axis("v", 2048), _get_olmo2_config(seq_len=128), key=random.PRNGKey(0))
    model2 = Olmo2LMHeadModel.init(hax.Axis("v", 2048), _get_olmo2_config(seq_len=256), key=random.PRNGKey(0))
    assert parameter_count(model) == parameter_count(model2)
# ---
def build(self, ctx: LrScheduleContext) -> Callable:
        raise NotImplementedError
# ---
def testImportGrumpy(self):
    self.assertEqual((0, ''), _GrumpRun(textwrap.dedent("""\
        from "__go__/grumpy" import Assert
        Assert(__frame__(), True, 'bad')""")))
# ---
def __call__(
        self,
        in_channels: int,
        out_channels: int,
        dilation: int,
        n_layers: int,
        pad: str,
        checkpoint_simple: bool,
    ) -> CoreBlock: ...
# ---
def sharded_route(router_probs_):
            selected_weights_, selected_experts_ = jax.lax.top_k(router_probs_, TopExperts.size)
            selected_weights_ = selected_weights_ / selected_weights_.sum(-1, keepdims=True)

            return selected_weights_, selected_experts_
# ---
def module_org():
    return entities.Organization().create()
