Question
========

Title: Why does Stephen Hawking say "Artificial Intelligence will kill
us all"? This quote by Stephen Hawking has been in headlines for quite
some time:

> Artificial Intelligence could wipe out humanity when it gets too
> clever as humans will be like ants.

Why does he say this? To put it simply: what are the possible threats
from AI (that Stephen Hawking is worried about)? If we know that AI is
so dangerous, why are we still promoting it? Why is it not banned?

What are the adverse consequences of the so-called Technological
Singularity?

Answer
======

> 5 votes It's not just Hawking, you hear variations on this refrain
> from a lot of people. And given that they're mostly very smart, well
> educated, well informed people (Elon Musk is another, for example), it
> probably shouldn't be dismissed out of hand.

Anyway, the basic idea seems to be this: If we create "real" artificial
intelligence, at some point, it will be able to improve itself, which
improves it's ability to improve itself, which means it can improve it's
ability to improve itself even more, and so on... a runaway cascade
leading to "superhuman intelligence". That is to say, leading to
something that more intelligent than we area.

So what happens if there is an entity on this planet which is literally
more intelligent than us (humans)? Would it be a threat to us? Well, it
certainly seems reasonable to speculate that it *could* be so. OTOH, we
have no particular reason, right now, to think that it *will* be so.

So it seems that Hawking, Musk, etc. are just coming down on the more
cautious / fearful side of things. Since we don't *know* if a superhuman
AI will be dangerous or not, and given that it could be unstoppable if
it were to become malicious (remember, it's smarter than we are!), it's
a reasonable thing to take under consideration.

Eliezer Yudkowsky has also written quite a bit on this subject,
including come up with the famous "AI Box" experiment. I think anybody
interested in this topic should read some of his material.

http://www.yudkowsky.net/singularity/aibox/

Answer
======

> 4 votes Because he did not yet know how far away current AI is...
> Working in an media AI lab, I get this question a lot. But really...
> we are still a long way from this. The robots still do everything we
> detailledly describe them to do. Instead of seeing the robot as
> intelligent, I would look to the human programmer for where the
> creativity really happens.

Answer
======

> 4 votes As Andrew Ng said, worrying about such threat from AI is like
> worrying about of overpopulation on Mars. It is science fiction.

That being said, given the rise of (much weaker) robots and other
(semi-)autonomous agents, the fields of the law and ethics are
increasingly incorporating them, e.g.Â see Roboethics.

Answer
======

> 3 votes To put it simply in layman terms, what are the possible
> threats from AI?

Currently, there are no threat.

The threat comes if humans create a so-called ultraintelligent machine,
a machine that can surpass all intellectual activities by any human.
This would be the last invention man would need to do, since this
machine is better in inventing machines than humans are (since that is
an intellectual activity). However, this could cause the machine to
invent machines that can destruct humans, and we can't stop them because
they are so much smarter than we are.

This is all hypothetical, no one has even a clue of what an
ultraintelligent machine looks like.

> If we know that AI is so dangerous why are we still promoting it? Why
> is it not banned?

As I said before, the existence of a ultraintelligent machine is
hypothetical. Artificial Intelligence has lots of useful applications
(more than this answer can contain), and if we develop it, we get even
more useful applications. We just have to be careful that the machines
won't overtake us.

Answer
======

> 3 votes There are a number of long resources to answer this sort of
> question: consider Stuart Armstrong's book Smarter Than Us, Nick
> Bostrom's book Superintelligence, which grew out of this edge.org
> answer, Tim Urban's explanation, or Michael Cohen's explanation.

But here's my (somewhat shorter) answer: intelligence is all about
decision-making, and we don't have any reason to believe that humans are
anywhere near close to being the best possible at decision-making. Once
we are able to build an AI AI researcher (that is, a computer that knows
how to make computers better at thinking), the economic and military
relevance of humans will rapidly disappear as any decision that could be
made by a human could be made better by a computer. (Why have human
generals instead of robot generals, human engineers instead of robot
engineers, and so on.)

This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow
and brought better decision-making to Earth, we could avoid a lot of
misery. The hard part is making sure that what we get are Vulcans who
want us around and happy, instead of something that doesn't share our
values.

Answer
======

> 2 votes He says this because it can happen. If something becomes
> smarter than us, why would it continue to serve us? The worst case
> scenario is that it takes over all manufacturing processes and
> consumes all matter to convert it into material capable of
> computation, extending outward infinitely until all matter is
> consumed.

We know that AI is dangerous but it doesn't matter because most people
don't believe in it. It goes against every comfort religion has to
offer. Man is the end-all-be-all of the universe and if that fact is
disputed, people will feel out of place and purposeless.

The fact is most people just don't acknowledge it's possible, or that it
will happen in our lifetimes, even though many reputable AI experts put
the occurrence of the singularity within two decades. If people truly
acknowledged that AI that was smarter than them was possible, wouldn't
they be living differently? Wouldn't they be looking to do things that
they enjoy, knowing that whatever it is they do that they dread will be
automated? Wouldn't everyone be calling for a universal basic income?

The other reason we don't ban it is because its promise is so great. One
researcher could be augmented by 1,000 digital research assistants. All
manual labor could be automated. For the first time, technology offers
us real freedom to do whatever we please.

But even in this best case scenario where it doesn't overtake us, humans
still have to adapt and alter their economic system to one where labor
isn't necessary. Otherwise, those who aren't technically-trained will
starve and revolt.
