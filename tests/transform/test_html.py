# Copyright 2025 The Marin Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Tests for HTML conversion transforms."""

from bs4 import BeautifulSoup
from marin.convert import HtmlToMarkdownConfig
from marin.transform.ar5iv.transform_ar5iv import clean_html as ar5iv_clean_html
from marin.transform.ar5iv.transform_ar5iv import process_record as ar5iv_process_record
from marin.transform.simple_html_to_md.process import SimpleHtmlToMdConfig, html_to_md
from marin.transform.wikipedia.transform_wikipedia import (
    clean_wiki_html,
    process_record as wiki_process_record,
)
from marin.convert import convert_page

SAMPLE_AR5IV_HTML = """<!DOCTYPE html>
<html>
<head><title>Sample Paper</title></head>
<body>
<div class="ltx_titlepage">
  <h1>Paper Title</h1>
</div>
<div class="ltx_authors">
  <span>John Doe</span>
</div>
<section>
  <h6 class="ltx_title_abstract">Abstract</h6>
  <p>This is the abstract.</p>
</section>
<section>
  <h2>Introduction</h2>
  <p>Inline equation: <math alttext="E = mc^2" display="inline">E = mc^2</math></p>
  <p>Display equation:</p>
  <table class="ltx_eqn_table">
    <tr class="ltx_eqn_row">
      <td class="ltx_eqn_cell">
        <math alttext="\\int_{0}^{\\infty} e^{-x^2} dx" display="block">integral</math>
      </td>
    </tr>
  </table>
  <ul>
    <li><span class="ltx_tag_item">1.</span> First item</li>
    <li><span class="ltx_tag_item">2.</span> Second item</li>
  </ul>
  <div class="ltx_role_footnote">This is a footnote.</div>
  <figcaption class="ltx_caption">Figure 1: A figure</figcaption>
</section>
<section id="bib">
  <h2>Bibliography</h2>
  <ul class="ltx_biblist">
    <li>Reference 1</li>
  </ul>
</section>
<section id="ltx_bibliography">
  <h2>References</h2>
  <a class="ltx_ref" href="#ref1">1</a>
</section>
<footer>Generated by ar5iv</footer>
</body>
</html>"""

SAMPLE_WIKIPEDIA_HTML = """<!DOCTYPE html>
<html>
<head><title>Sample Article</title></head>
<body>
<h1>Sample Article</h1>
<table class="infobox">
  <tr><th>Type</th><td>Example</td></tr>
</table>
<p>Introduction with inline math: <span class="mwe-math-element">
  <math><annotation encoding="application/x-tex">E = mc^2</annotation></math>
</span></p>
<dl>
  <dd>
    <span class="mwe-math-element">
      <math><annotation encoding="application/x-tex">\\int_0^\\infty e^{-x} dx = 1</annotation></math>
    </span>
  </dd>
</dl>
<h2 id="References">References</h2>
<div class="reflist">
  <ol>
    <li>Reference 1</li>
  </ol>
</div>
</body>
</html>"""

SAMPLE_FINEWEB_HTML_RECORD = {
    "id": "test-123",
    "text": (
        """<!DOCTYPE html>
<html>
<head><title>Test Page</title></head>
<body>
<h1>Main Title</h1>
<p>This is content with <a href="/relative">a link</a>.</p>
<img src="/image.png" alt="image">
</body>
</html>"""
    ),
    "source": "fineweb",
    "metadata": {
        "fineweb_metadata": {
            "url": "https://example.com/page",
            "language": "en",
            "timestamp": "2025-01-01",
        }
    },
}


# Integration test of ar5iv cleaning pipeline
def test_ar5iv_clean_html():
    cleaned = ar5iv_clean_html(SAMPLE_AR5IV_HTML, remove_reference_section=True)
    soup = BeautifulSoup(cleaned, "html.parser")

    # Verify removals
    assert soup.find("div", {"class": "ltx_titlepage"}) is None
    assert soup.find("div", {"class": "ltx_authors"}) is None
    assert soup.find("footer") is None
    assert soup.find("section", {"id": "bib"}) is None
    assert soup.find("section", {"id": "ltx_bibliography"}) is None
    assert soup.find("div", {"class": "ltx_role_footnote"}) is None
    assert soup.find("figcaption", {"class": "ltx_caption"}) is None

    # Verify transformations
    assert soup.find("h2", {"class": "ltx_title_abstract"}) is not None
    assert "$E = mc^2$" in str(cleaned)


# Integration test of wiki cleaning
def test_clean_wiki_html():
    cleaned = clean_wiki_html(SAMPLE_WIKIPEDIA_HTML, remove_reference_section=True)
    soup = BeautifulSoup(cleaned, "html.parser")

    # References should be removed
    assert soup.find("div", {"class": "reflist"}) is None
    assert soup.find("h2", {"id": "References"}) is None

    # Infobox should be moved
    assert soup.find("h2", string="InfoBox") is not None

    # Equations should be unwrapped
    assert "$E = mc^2$" in str(cleaned)


# One extraction method test (readability variant only)
def test_convert_page_with_different_methods(sample_html_simple):
    config = HtmlToMarkdownConfig.default_config()
    result = convert_page(sample_html_simple, url=None, extract_method="readability", config=config)

    assert "content" in result
    assert "title" in result
    assert "html" in result
    assert result["content"] is not None
    assert len(result["content"]) > 0


# Full record processing
def test_ar5iv_process_record():
    row = {"filename": "arxiv_12345", "content": SAMPLE_AR5IV_HTML}
    config = HtmlToMarkdownConfig.default_config()

    result = ar5iv_process_record(
        row, extract_method="readability", extract_config=config, remove_reference_section=True
    )

    assert result["id"] == "arxiv_12345"
    assert result["source"] == "ar5iv"
    assert result["format"] == "text"
    assert "text" in result
    assert len(result["text"]) > 0
    # Should have cleaned HTML
    assert "ltx_titlepage" not in result["text"]


# Full wiki processing
def test_wiki_process_record_with_html():
    row = {
        "identifier": "wiki_123",
        "url": "https://en.wikipedia.org/wiki/Test",
        "name": "Test Article",
        "abstract": "This is a test",
        "date_created": "2025-01-01",
        "article_body": {"html": SAMPLE_WIKIPEDIA_HTML},
    }
    config = HtmlToMarkdownConfig.default_config()

    result = wiki_process_record(
        row,
        extract_method="readability",
        extract_config=config,
        remove_reference_section=True,
        digit_threshold=50,
        word_threshold=5,
        special_char_threshold=50,
    )

    assert result is not None
    assert result["id"] == "wiki_123"
    assert result["url"] == "https://en.wikipedia.org/wiki/Test"
    assert result["title"] == "Test Article"
    assert "text" in result
    assert len(result["text"]) > 0


# Critical filtering
def test_wiki_process_record_filters_bad_content():
    row = {
        "identifier": "wiki_123",
        "url": "https://en.wikipedia.org/wiki/Test",
        "name": "Test Article",
        "article_body": {"html": SAMPLE_WIKIPEDIA_HTML},
    }
    config = HtmlToMarkdownConfig.default_config()

    # Set unreasonable thresholds to trigger filtering
    result = wiki_process_record(
        row,
        extract_method="readability",
        extract_config=config,
        remove_reference_section=True,
        digit_threshold=0,  # Filter if any digits
        word_threshold=1000,  # Require 1000+ words
        special_char_threshold=0,  # Filter if any special chars
    )

    assert result is None


# E2E pipeline
def test_ar5iv_pipeline_integration(tmp_path, write_jsonl_gz, read_all_jsonl_gz):
    input_dir = tmp_path / "input"
    output_dir = tmp_path / "output"

    # Write test data
    write_jsonl_gz(
        input_dir / "test.jsonl.gz",
        [
            {"filename": "paper1", "content": SAMPLE_AR5IV_HTML},
            {"filename": "paper2", "content": SAMPLE_AR5IV_HTML},
        ],
    )

    from marin.transform.ar5iv.transform_ar5iv import Ar5ivExtractionConfig, process_ar5iv_dump

    config = Ar5ivExtractionConfig(
        input_path=str(input_dir),
        output_path=str(output_dir),
        revision="test",
        remove_reference_section=True,
        extract_method="readability",
        extract_config=HtmlToMarkdownConfig.default_config(),
    )

    process_ar5iv_dump(config)

    # Verify output
    results = read_all_jsonl_gz(output_dir)
    assert len(results) == 2
    for result in results:
        assert result["source"] == "ar5iv"
        assert result["format"] == "text"
        assert "text" in result
        assert len(result["text"]) > 0


# E2E pipeline
def test_simple_html_to_md_pipeline(tmp_path, write_jsonl_gz, read_all_jsonl_gz):
    input_dir = tmp_path / "input"
    output_dir = tmp_path / "output"

    # Write test data
    write_jsonl_gz(
        input_dir / "test.jsonl.gz",
        [SAMPLE_FINEWEB_HTML_RECORD, SAMPLE_FINEWEB_HTML_RECORD],
    )

    config = SimpleHtmlToMdConfig(
        input_path=str(input_dir),
        output_path=str(output_dir),
        extract_method="readability",
        config=HtmlToMarkdownConfig.default_config(),
    )

    html_to_md(config)

    # Verify output
    results = read_all_jsonl_gz(output_dir)
    assert len(results) == 2
    for result in results:
        assert result["id"] == "test-123"
        assert result["format"] == "md"
        assert "text" in result
