Take the change set below:


```
diff --git c/experiments/pretraining_datasets/nemotron.py w/experiments/pretraining_datasets/nemotron.py
index 34099ec54..d3f4d1c27 100644
--- c/experiments/pretraining_datasets/nemotron.py
+++ w/experiments/pretraining_datasets/nemotron.py
@@ -64,7 +64,11 @@ def _get_nemotron_split_paths(split: str):
 
 
 def tokenize_nemotron(
-    *, tokenizer: str | None = None, window_size_bytes: int = 10_000_000_000
+    *,
+    tokenizer: str | None = None,
+    window_size_bytes: int = 10_000_000_000,
+    max_workers: int = 4096,
+    writer_batch_size: int = 65536,
 ) -> dict[str, TokenizerStep]:
     """Generate tokenization steps for all Nemotron CC dataset splits."""
     if tokenizer is None:
@@ -85,6 +89,8 @@ def tokenize_nemotron(
                 cache_path=this_output_path(),
                 tokenizer=versioned(tokenizer),
                 window_size_bytes=window_size_bytes,
+                max_workers=max_workers,
+                writer_batch_size=writer_batch_size,
             ),
         )
 
diff --git c/lib/fray/src/fray/v2/ray_backend/backend.py w/lib/fray/src/fray/v2/ray_backend/backend.py
index 36a70ae13..a24e89a34 100644
--- c/lib/fray/src/fray/v2/ray_backend/backend.py
+++ w/lib/fray/src/fray/v2/ray_backend/backend.py
@@ -409,6 +409,12 @@ def _actor_ray_options(resources: ResourceConfig) -> dict[str, Any]:
         # node is preempted. Without this, the actor dies permanently and the
         # pool degrades over time (see marin#2943).
         options["max_restarts"] = -1
+    # Explicit max_restarts takes precedence over the preemptible default.
+    # Useful for actors like coordinators that are preemptible (not pinned to
+    # head node) but should NOT auto-restart because they require remote
+    # initialization beyond __init__.
+    if resources.max_restarts is not None:
+        options["max_restarts"] = resources.max_restarts
     if resources.max_concurrency > 1:
         options["max_concurrency"] = resources.max_concurrency
     return options
diff --git c/lib/fray/src/fray/v2/types.py w/lib/fray/src/fray/v2/types.py
index 7c9c99c7c..740495b5d 100644
--- c/lib/fray/src/fray/v2/types.py
+++ w/lib/fray/src/fray/v2/types.py
@@ -322,6 +322,8 @@ class ResourceConfig:
     regions: Sequence[str] | None = None
     replicas: int = 1
     max_concurrency: int = 1
+    # TODO(rav): is this the right place for max_restarts?
+    max_restarts: int | None = None
 
     def chip_count(self) -> int:
         """Total accelerator chips across all replicas."""
diff --git c/lib/fray/tests/test_v2_ray.py w/lib/fray/tests/test_v2_ray.py
index 08723da52..40a304c34 100644
--- c/lib/fray/tests/test_v2_ray.py
+++ w/lib/fray/tests/test_v2_ray.py
@@ -271,3 +271,11 @@ def test_actor_options_preemptible_sets_max_restarts():
     options = _actor_ray_options(ResourceConfig(preemptible=True))
     assert options["max_restarts"] == -1
     assert "resources" not in options
+
+
+def test_actor_options_explicit_max_restarts_overrides_preemptible():
+    from fray.v2.ray_backend.backend import _actor_ray_options
+
+    options = _actor_ray_options(ResourceConfig(preemptible=True, max_restarts=0))
+    assert options["max_restarts"] == 0
+    assert "resources" not in options
diff --git c/lib/marin/src/marin/execution/disk_cache.py w/lib/marin/src/marin/execution/disk_cache.py
index ae8206068..5735b67ce 100644
--- c/lib/marin/src/marin/execution/disk_cache.py
+++ w/lib/marin/src/marin/execution/disk_cache.py
@@ -4,69 +4,124 @@
 """Run a function once, cache the result, and return it on subsequent calls."""
 
 from __future__ import annotations
+import os
 
+import functools
 import logging
 from collections.abc import Callable
-from typing import TypeVar
+from typing import TypeVar, Generic
+
+import cloudpickle
+import fsspec
+from iris.temp_buckets import get_temp_bucket_path
 
 from marin.execution.distributed_lock import StepAlreadyDone
-from marin.execution.executor_step_status import (
-    STATUS_FAILED,
-    STATUS_SUCCESS,
-    StatusFile,
-    worker_id,
-)
+import hashlib
+import pickle
 
 logger = logging.getLogger(__name__)
 
 T = TypeVar("T")
 
 
-def disk_cached(
-    fn: Callable[[str], T],
-    output_path: str,
-    *,
-    save: Callable[[T, str], None] | None = None,
-    load: Callable[[str], T] | None = None,
-) -> T | None:
-    """Run *fn* once and track completion via a status file.
+class disk_cache(Generic[T]):
+    """Decorator that caches function results to disk via a status file.
 
-    When *save* and *load* are provided, the result of *fn* is persisted and
-    deserialized on cache hits.  When they are ``None`` (the default), *fn* is
-    expected to handle its own reading/writing at *output_path* and is called
-    again on cache hits to load the result.
+    Supports bare decoration, decoration with arguments, and direct wrapping::
 
-    By itself this function does not provide any locking guarantees, e.g. if
-    you run disk_cached on multiple workers/shards, there may be a race
-    condition where multiple workers evaluate the function and write to the same
-    output path at the same time. To avoid this, compose with
-    :func:`distributed_lock` to ensure that only one worker executes the
-    function while the others wait for the result. For example::
+        @disk_cached
+        def my_fn(*args):
+            ...
 
-        result = disk_cached(
-            distributed_lock(get_tokenizer),
-            output_path,
-            save=Artifact.save,
-            load=lambda p: Artifact.load(p, TokenizerInfo),
-        )
+        @disk_cached(output_path="gs://...", save_fn=save, load_fn=load)
+        def my_fn(*args):
+            ...
+
+        cached_fn = disk_cached(fn, output_path=path, save_fn=save, load_fn=load)
+        result = cached_fn(output_path)
+
+    When *save_fn* and *load_fn* are provided, the result is persisted and
+    deserialized on cache hits.  When they are ``None`` (the default),
+    cloudpickle is used.
+
+    The decorated function is called with its original arguments; the
+    *output_path* is used only for cache bookkeeping.
+
+    By itself this class does not provide any locking guarantees.  Compose with
+    :func:`distributed_lock` to ensure single-writer semantics.
     """
-    status_file = StatusFile(output_path, worker_id())
-    if status_file.status == STATUS_SUCCESS:
-        logger.info(f"disk_cached: cache hit for {output_path}")
-        return load(output_path) if load is not None else fn(output_path)
 
-    try:
-        result = fn(output_path)
-    except StepAlreadyDone:
-        # NOTE: this is leaky but this branch handles the case of distributed lock wrapper
-        logger.info(f"disk_cached: completed by another worker for {output_path}")
-        return load(output_path) if load is not None else fn(output_path)
-    except Exception:
-        StatusFile(output_path, worker_id()).write_status(STATUS_FAILED)
-        raise
+    def __init__(
+        self,
+        fn: Callable[..., T] | None = None,
+        *,
+        output_path: str | None = None,
+        save_fn: Callable[[T, str], None] | None = None,
+        load_fn: Callable[[str], T] | None = None,
+    ):
+        self._output_path = output_path
+        self._save_fn = save_fn
+        self._load_fn = load_fn
+        self._fn: Callable[..., T] | None = None
 
-    if save is not None:
-        save(result, output_path)
-    StatusFile(output_path, worker_id()).write_status(STATUS_SUCCESS)
-    logger.info(f"disk_cached: computed and cached {output_path}")
-    return result
+        if callable(fn):
+            self._fn = fn
+            functools.update_wrapper(self, fn)
+
+    # TODO: add ParamSpec
+    def __call__(self, *args, **kwargs):
+        if self._fn is None:
+            # @disk_cached(...) — receiving the function to wrap
+            if len(args) == 1 and callable(args[0]) and not kwargs:
+                self._fn = args[0]  # type: ignore[bad-assignment]
+                functools.update_wrapper(self, self._fn)
+                return self
+            raise TypeError("disk_cached() expected a callable")
+
+        return self._execute(*args, **kwargs)
+
+    def _execute(self, *args, **kwargs):
+        # Compute a fingerprint of the function arguments for cache keying.
+
+        def fingerprint_args(*args, **kwargs) -> str:
+            """Create a deterministic fingerprint for args and kwargs."""
+            # Include the function name in the fingerprint to avoid collisions across different functions.
+            fn_name = getattr(self._fn, "__name__", None)
+            data = pickle.dumps((fn_name, args, kwargs))
+            return hashlib.sha256(data).hexdigest()[:8]
+
+        output_path = self._output_path
+        if output_path is None:
+            args_fingerprint = fingerprint_args(*args, **kwargs)
+            output_path = get_temp_bucket_path(1, f"disk_cache_{args_fingerprint}")
+            if output_path is None:
+                output_path = os.environ["MARIN_PREFIX"] + f"/disk_cache_{args_fingerprint}"
+
+        def load_result():
+            assert output_path is not None
+            if self._load_fn is not None:
+                return self._load_fn(output_path)
+            with fsspec.open(output_path + "/data.pkl", "rb") as f:
+                return cloudpickle.loads(f.read())
+
+        try:
+            return load_result()
+        except FileNotFoundError:
+            logger.info(f"disk_cached: cache miss for {output_path}")
+
+        assert self._fn is not None
+        try:
+            result = self._fn(*args, **kwargs)
+        except StepAlreadyDone:
+            # NOTE: this is leaky but this branch handles the case of distributed lock wrapper
+            logger.info(f"disk_cached: completed by another worker for {output_path}")
+            return load_result()
+
+        if self._save_fn is not None:
+            self._save_fn(result, output_path)
+        else:
+            with fsspec.open(output_path + "/data.pkl", "wb") as f:
+                f.write(cloudpickle.dumps(result))
+
+        logger.info(f"disk_cached: computed and cached {output_path}")
+        return result
diff --git c/lib/marin/src/marin/execution/step_runner.py w/lib/marin/src/marin/execution/step_runner.py
index 8d5a5575a..c55de576e 100644
--- c/lib/marin/src/marin/execution/step_runner.py
+++ w/lib/marin/src/marin/execution/step_runner.py
@@ -262,7 +262,7 @@ class StepRunner:
     def _launch_step(self, step: StepSpec, *, force_run_failed: bool, dry_run: bool) -> JobHandle | None:
         """Launch a single step as a fray job. Returns None if skipped."""
         from marin.execution.artifact import Artifact
-        from marin.execution.disk_cache import disk_cached
+        from marin.execution.disk_cache import disk_cache
         from marin.execution.distributed_lock import distributed_lock
 
         output_path = step.output_path
@@ -291,14 +291,15 @@ class StepRunner:
         # All caching, locking, heartbeat, artifact saving, and status writes
         # happen on the worker.
         step_fn = step.fn
+        cached_step = disk_cache(
+            distributed_lock(step_fn, force_run_failed=force_run_failed),
+            output_path=output_path,
+            save_fn=Artifact.save,
+            load_fn=Artifact.load,
+        )
 
         def worker_fn():
-            disk_cached(
-                distributed_lock(step_fn, force_run_failed=force_run_failed),
-                output_path,
-                save=Artifact.save,
-                load=Artifact.load,
-            )
+            cached_step(output_path)
 
         worker_fn.__qualname__ = step_name
         worker_fn.__name__ = step_name
diff --git c/lib/marin/src/marin/processing/tokenize/tokenize.py w/lib/marin/src/marin/processing/tokenize/tokenize.py
index 3a78a164c..1f511079a 100644
--- c/lib/marin/src/marin/processing/tokenize/tokenize.py
+++ w/lib/marin/src/marin/processing/tokenize/tokenize.py
@@ -7,6 +7,9 @@ Tokenize datasets using zephyr pipeline and write to Levanter cache format.
 Supports both regular file paths and HuggingFace datasets. For HF datasets, downloads
 them first then tokenizes the downloaded files.
 """
+from functools import cache
+from marin.utilities.time_logger import log_time
+from marin.execution.disk_cache import disk_cache
 
 import abc
 import dataclasses
@@ -33,7 +36,7 @@ from levanter.data.text import (
 )
 from levanter.store.cache import consolidate_shard_caches
 from levanter.store.tree_store import TreeStore
-from zephyr import Dataset, ZephyrContext, zephyr_worker_ctx
+from zephyr import Dataset, ZephyrContext
 from zephyr.readers import load_file
 
 from marin.execution.executor import ExecutorStep, InputName, VersionedValue
@@ -83,7 +86,7 @@ class TokenizeConfig(TokenizeConfigBase):
     """Files are bundled into groups up to this size; each group becomes one shard.
     Smaller values produce more shards and thus more parallelism (up to max_workers)."""
     max_workers: int = 4096
-    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram="5g", disk="5g"))
+    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram="10g", disk="5g"))
     writer_batch_size: int = 65536
     """Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op
     overhead. Too large a value increases memory usage and delays progress checkpointing."""
@@ -147,7 +150,7 @@ class HfTokenizeConfig(TokenizeConfigBase):
     window_size_bytes: int = 10_000_000_000
     """Files are bundled into groups up to this size; each group becomes one shard.
     Smaller values produce more shards and thus more parallelism (up to max_workers)."""
-    max_workers: int = 4096
+    max_workers: int = 512
     worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram="5g", disk="5g"))
     writer_batch_size: int = 65536
     """Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op
@@ -255,7 +258,7 @@ def _bundle_files_by_size(file_infos, max_bytes: int):
 
 def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Iterator[Sequence[dict]]) -> Iterator[dict]:
     """Tokenize a list of batches using the specified tokenizer and format."""
-    tokenizer: transformers.PreTrainedTokenizer = zephyr_worker_ctx().get_shared("tokenizer")
+    tokenizer: transformers.PreTrainedTokenizer = get_tokenizer(config.tokenizer)
     batch_processor = preprocessor_for_format(config.format, tokenizer)
 
     batch_count = 0
@@ -289,6 +292,12 @@ def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Ite
     )
 
 
+@cache
+@disk_cache
+def get_tokenizer(tokenizer: str) -> transformers.PreTrainedTokenizer:
+    return transformers.AutoTokenizer.from_pretrained(tokenizer)
+
+
 def tokenize(config: TokenizeConfigBase):
     """Tokenize datasets using zephyr pipeline.
 
@@ -376,8 +385,8 @@ def tokenize(config: TokenizeConfigBase):
             )
         )
 
-        # Broadcast the tokenizer to all workers via ZephyrContext
-        ctx.put("tokenizer", transformers.AutoTokenizer.from_pretrained(config.tokenizer))
+        with log_time(f"{config.tokenizer} cache warmup"):
+            get_tokenizer(config.tokenizer)
 
         tokenize_start = time.monotonic()
         shard_paths = ctx.execute(temp_shards)
@@ -422,7 +431,7 @@ def tokenize(config: TokenizeConfigBase):
             resources=config.worker_resources,
             max_workers=min(config.max_workers, len(train_groups)),
             name="tokenize-train",
-            no_workers_timeout=20 * 60,
+            no_workers_timeout=6 * 60 * 60,
         ) as ctx:
             run_pipeline(ctx, train_groups, "train")
 
@@ -432,7 +441,7 @@ def tokenize(config: TokenizeConfigBase):
             resources=config.worker_resources,
             max_workers=min(config.max_workers, len(validation_groups)),
             name="tokenize-validation",
-            no_workers_timeout=20 * 60,
+            no_workers_timeout=6 * 60 * 60,
         ) as ctx:
             run_pipeline(ctx, validation_groups, "validation")
 
diff --git c/lib/zephyr/src/zephyr/execution.py w/lib/zephyr/src/zephyr/execution.py
index 87715feee..385585d9c 100644
--- c/lib/zephyr/src/zephyr/execution.py
+++ w/lib/zephyr/src/zephyr/execution.py
@@ -10,6 +10,8 @@ protocol.
 """
 
 from __future__ import annotations
+from datetime import timedelta
+import contextlib
 
 import enum
 import logging
@@ -47,6 +49,14 @@ from zephyr.writers import ensure_parent_dir
 logger = logging.getLogger(__name__)
 
 
+@contextlib.contextmanager
+def log_time(label: str, level: int = logging.INFO) -> Iterator[None]:
+    t_start = time.perf_counter()
+    yield
+    t_end = time.perf_counter()
+    logger.log(level, f"{label} took {timedelta(seconds=t_end - t_start)}")
+
+
 class Chunk(Protocol):
     def __iter__(self) -> Iterator: ...
 
@@ -247,6 +257,8 @@ class ZephyrCoordinator:
     """
 
     def __init__(self):
+        from fray.v2 import current_actor
+
         # Task management state
         self._task_queue: deque[ShardTask] = deque()
         self._results: dict[int, TaskResult] = {}
@@ -268,11 +280,15 @@ class ZephyrCoordinator:
         self._worker_handles: dict[str, ActorHandle] = {}
         self._coordinator_thread: threading.Thread | None = None
         self._shutdown: bool = False
+        self._is_last_stage: bool = False
         self._initialized: bool = False
 
         # Lock for accessing coordinator state from background thread
         self._lock = threading.Lock()
 
+        actor_ctx = current_actor()
+        self._name = f"{actor_ctx.group_name}"
+
     def initialize(
         self,
         chunk_prefix: str,
@@ -314,6 +330,10 @@ class ZephyrCoordinator:
                 self._worker_handles[worker_id] = worker_handle
                 self._worker_states[worker_id] = WorkerState.READY
                 self._last_seen[worker_id] = time.monotonic()
+                # NOTE: if there was a task assigned to the worker, there's a race condidtion between marking
+                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently
+                # lose tasks.
+                self._maybe_requeue_worker_task(worker_id)
                 return
 
             self._worker_handles[worker_id] = worker_handle
@@ -330,9 +350,9 @@ class ZephyrCoordinator:
         last_log_time = 0.0
 
         while not self._shutdown:
-            # Check heartbeats, re-queue stale tasks
-            with self._lock:
-                self._check_worker_heartbeats()
+            # Scheduler check heartbeats, re-queue stale tasks
+            # This is called in a remote fashion to account for potential congestion.
+            self._self_handle.check_heartbeats.remote()
 
             # Log status periodically during active execution
             now = time.monotonic()
@@ -360,6 +380,16 @@ class ZephyrCoordinator:
             dead,
         )
 
+    def _maybe_requeue_worker_task(self, worker_id: str) -> None:
+        """If the worker has a task in-flight, re-queue it and mark the worker as failed."""
+        task_and_attempt = self._in_flight.pop(worker_id, None)
+        if task_and_attempt is not None:
+            logger.info("Worker %s had an in-flight task, re-queuing", worker_id)
+            task, _old_attempt = task_and_attempt
+            self._task_attempts[task.shard_idx] += 1
+            self._task_queue.append(task)
+            self._retries += 1
+
     def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:
         """Internal heartbeat check (called with lock held)."""
         now = time.monotonic()
@@ -367,13 +397,7 @@ class ZephyrCoordinator:
             if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:
                 logger.warning(f"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)")
                 self._worker_states[worker_id] = WorkerState.FAILED
-                task_and_attempt = self._in_flight.pop(worker_id, None)
-                if task_and_attempt is not None:
-                    logger.info("Removed task %s from worker %s, re-queueing", task_and_attempt, worker_id)
-                    task, _old_attempt = task_and_attempt
-                    self._task_attempts[task.shard_idx] += 1
-                    self._task_queue.append(task)
-                    self._retries += 1
+                self._maybe_requeue_worker_task(worker_id)
 
     def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:
         """Called by workers to get next task.
@@ -394,6 +418,8 @@ class ZephyrCoordinator:
                 return None
 
             if not self._task_queue:
+                if self._is_last_stage:
+                    return "SHUTDOWN"
                 return None
 
             task = self._task_queue.popleft()
@@ -408,9 +434,25 @@ class ZephyrCoordinator:
             }
             return (task, attempt, config)
 
+    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:
+        """Assert _in_flight[worker_id], if present, matches the reported shard.
+
+        Workers block on report_result/report_error before calling pull_task,
+        so _in_flight can never point to a different shard. It may be absent
+        if a heartbeat timeout already re-queued the task.
+        """
+        in_flight = self._in_flight.get(worker_id)
+        if in_flight is not None:
+            assert in_flight[0].shard_idx == shard_idx, (
+                f"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, "
+                f"but _in_flight tracks shard {in_flight[0].shard_idx}. "
+                f"This indicates report_result/pull_task reordering — workers must block on report_result."
+            )
+
     def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:
         with self._lock:
             self._last_seen[worker_id] = time.monotonic()
+            self._assert_in_flight_consistent(worker_id, shard_idx)
 
             current_attempt = self._task_attempts.get(shard_idx, 0)
             if attempt != current_attempt:
@@ -429,13 +471,14 @@ class ZephyrCoordinator:
         """Worker reports a task failure. All errors are fatal."""
         with self._lock:
             self._last_seen[worker_id] = time.monotonic()
+            self._assert_in_flight_consistent(worker_id, shard_idx)
             self._in_flight.pop(worker_id, None)
             self._fatal_error = error_info
             self._worker_states[worker_id] = WorkerState.DEAD
 
     def heartbeat(self, worker_id: str) -> None:
-        with self._lock:
-            self._last_seen[worker_id] = time.monotonic()
+        # NOTE: there's no need for lock here
+        self._last_seen[worker_id] = time.monotonic()
 
     def get_status(self) -> JobStatus:
         with self._lock:
@@ -461,7 +504,7 @@ class ZephyrCoordinator:
         with self._lock:
             return self._fatal_error
 
-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:
+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:
         """Load a new stage's tasks into the queue."""
         with self._lock:
             self._task_queue = deque(tasks)
@@ -473,13 +516,14 @@ class ZephyrCoordinator:
             self._in_flight = {}
             self._task_attempts = {task.shard_idx: 0 for task in tasks}
             self._fatal_error = None
+            self._is_last_stage = is_last_stage
 
     def _wait_for_stage(self) -> None:
         """Block until current stage completes or error occurs."""
         backoff = ExponentialBackoff(initial=0.05, maximum=1.0)
         last_log_completed = -1
         start_time = time.monotonic()
-        warned_no_workers = False
+        all_dead_since: float | None = None
         no_workers_timeout = self._no_workers_timeout
 
         while True:
@@ -487,27 +531,37 @@ class ZephyrCoordinator:
                 if self._fatal_error:
                     raise ZephyrWorkerError(self._fatal_error)
 
-                num_workers = len(self._worker_handles)
                 completed = self._completed_shards
                 total = self._total_shards
 
                 if completed >= total:
                     return
 
-                # Fail fast if no workers appear within timeout
-                if num_workers == 0:
-                    elapsed = time.monotonic() - start_time
-                    if elapsed > no_workers_timeout:
+                # Count alive workers (READY or BUSY), not just total registered.
+                # Dead/failed workers stay in _worker_handles but can't make progress.
+                alive_workers = sum(
+                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}
+                )
+
+                if alive_workers == 0:
+                    now = time.monotonic()
+                    elapsed = now - start_time
+
+                    if all_dead_since is None:
+                        all_dead_since = now
+                        logger.warning("All workers are dead/failed. Waiting for workers to recover...")
+
+                    dead_duration = now - all_dead_since
+                    if dead_duration > no_workers_timeout:
                         raise ZephyrWorkerError(
-                            f"No workers available after {elapsed:.1f}s. "
+                            f"No alive workers for {dead_duration:.1f}s "
+                            f"(total elapsed {elapsed:.1f}s). "
+                            f"All {len(self._worker_handles)} registered workers are dead/failed. "
                             "Check cluster resources and worker group configuration."
                         )
-                    if not warned_no_workers and elapsed > 5.0:
-                        logger.warning(
-                            "No workers available yet after %.1fs, waiting for discovery...",
-                            elapsed,
-                        )
-                        warned_no_workers = True
+                else:
+                    # Workers are alive — reset the dead timer
+                    all_dead_since = None
 
             if completed != last_log_completed:
                 logger.info("[%s] %d/%d tasks completed", self._stage_name, completed, total)
@@ -521,6 +575,7 @@ class ZephyrCoordinator:
         with self._lock:
             return dict(self._results)
 
+    # TODO: this is not used anywhere!?
     def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:
         """Set config for the current execution."""
         with self._lock:
@@ -543,6 +598,15 @@ class ZephyrCoordinator:
         if not shards:
             return []
 
+        # Identify the last stage that dispatches work to workers (non-RESHARD).
+        # On that stage, idle workers receive SHUTDOWN once all tasks are
+        # in-flight, so they exit eagerly instead of polling until
+        # coordinator.shutdown().
+        last_worker_stage_idx = max(
+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),
+            default=-1,
+        )
+
         for stage_idx, stage in enumerate(plan.stages):
             stage_label = f"stage{stage_idx}-{stage.stage_name(max_length=40)}"
 
@@ -556,7 +620,7 @@ class ZephyrCoordinator:
             # Build and submit tasks
             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)
             logger.info("Starting stage %s with %d tasks", stage_label, len(tasks))
-            self._start_stage(stage_label, tasks)
+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))
 
             # Wait for stage completion
             self._wait_for_stage()
@@ -570,6 +634,10 @@ class ZephyrCoordinator:
         for shard in shards:
             for chunk in shard.chunks:
                 flat_result.extend(list(chunk))
+
+        # Signal workers to shut down now that all stages are complete.
+        self.shutdown()
+
         return flat_result
 
     def _compute_join_aux(
@@ -649,9 +717,9 @@ class ZephyrCoordinator:
                 "execution_id": self._execution_id,
             }
 
-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:
+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:
         """Load a new stage's tasks into the queue (legacy compat)."""
-        self._start_stage(stage_name, tasks)
+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)
 
     def check_heartbeats(self, timeout: float = 30.0) -> None:
         """Marks stale workers as FAILED, re-queues their in-flight tasks."""
@@ -666,6 +734,9 @@ class ZephyrCoordinator:
         """Signal workers that no more stages will be submitted (legacy compat)."""
         self._shutdown = True
 
+    def __repr__(self) -> str:
+        return f"ZephyrCoordinator(name={self._name})"
+
 
 # ---------------------------------------------------------------------------
 # ZephyrWorker
@@ -695,8 +766,8 @@ class ZephyrWorker:
         actor_ctx = current_actor()
         self._worker_id = f"{actor_ctx.group_name}-{actor_ctx.index}"
 
-        # Register with coordinator
-        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle)
+        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity
+        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)
 
         # Start polling in a background thread
         self._polling_thread = threading.Thread(
@@ -735,7 +806,7 @@ class ZephyrWorker:
         heartbeat_count = 0
         while not self._shutdown_event.is_set():
             try:
-                coordinator.heartbeat.remote(self._worker_id)
+                coordinator.heartbeat.remote(self._worker_id).result()
                 heartbeat_count += 1
                 if heartbeat_count % 10 == 1:
                     logger.debug("[%s] Sent heartbeat #%d", self._worker_id, heartbeat_count)
@@ -779,9 +850,12 @@ class ZephyrWorker:
 
             logger.info("[%s] Executing task for shard %d (attempt %d)", self._worker_id, task.shard_idx, attempt)
             try:
-                result = self._execute_shard(task, config)
-                logger.info("[%s] Task complete, reporting result for shard %d", self._worker_id, task.shard_idx)
-                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result)
+                with log_time(f"Worker {self._worker_id} executing shard {task.shard_idx}"):
+                    result = self._execute_shard(task, config)
+                # Block until coordinator records the result. This ensures
+                # report_result is fully processed before the next pull_task,
+                # preventing _in_flight tracking races.
+                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()
                 task_count += 1
             except Exception as e:
                 logger.error("Worker %s error on shard %d: %s", self._worker_id, task.shard_idx, e)
@@ -791,7 +865,7 @@ class ZephyrWorker:
                     self._worker_id,
                     task.shard_idx,
                     "".join(traceback.format_exc()),
-                )
+                ).result()
 
     def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:
         """Execute a stage's operations on a single shard.
@@ -860,6 +934,9 @@ class ZephyrWorker:
         if hasattr(self, "_polling_thread") and self._polling_thread.is_alive():
             self._polling_thread.join(timeout=5.0)
 
+    def __repr__(self) -> str:
+        return f"ZephyrWorker(id={self._worker_id})"
+
 
 def _regroup_result_refs(
     result_refs: dict[int, TaskResult],
@@ -883,12 +960,10 @@ def _regroup_result_refs(
 class ZephyrContext:
     """Execution context for Zephyr pipelines.
 
-    Creates a coordinator actor on __enter__ which owns and manages the worker
-    pool. Workers are created lazily on the first execute() call, sized to
-    min(max_workers, plan.num_shards) to avoid over-provisioning. Workers
-    persist across pipeline stages and execute() calls, allowing cached state
-    (tokenizers, models) to be reused. Shared data broadcast via put() is
-    delivered to workers with each task.
+    Each execute() call creates a fresh coordinator and worker pool, runs
+    the pipeline, then tears everything down. Workers are sized to
+    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared
+    data broadcast via put() is delivered to workers with each task.
 
     Args:
         client: The fray client to use. If None, auto-detects using current_client().
@@ -920,6 +995,8 @@ class ZephyrContext:
     _coordinator_group: Any = field(default=None, repr=False)
     _worker_group: Any = field(default=None, repr=False)
     _worker_count: int = field(default=0, repr=False)
+    # NOTE: execute calls increment this at the very beginning
+    _pipeline_id: int = field(default=-1, repr=False)
 
     def __post_init__(self):
         if self.client is None:
@@ -971,29 +1048,36 @@ class ZephyrContext:
         verbose: bool = False,
         dry_run: bool = False,
     ) -> Sequence:
-        """Execute a dataset pipeline on the worker pool.
+        """Execute a dataset pipeline.
 
-        Workers persist across execute() calls, so cached state (tokenizers,
-        models) will be reused. If the coordinator dies mid-execution (e.g.,
-        VM preemption), the pipeline is retried with a fresh coordinator and
-        worker pool up to ``max_execution_retries`` times. Application errors
-        (``ZephyrWorkerError``) are never retried.
+        Each call creates a fresh coordinator and worker pool, runs the
+        pipeline, then tears everything down. If the coordinator dies
+        mid-execution (e.g., VM preemption), the pipeline is retried
+        with fresh actors up to ``max_execution_retries`` times.
+        Application errors (``ZephyrWorkerError``) are never retried.
         """
         plan = compute_plan(dataset, hints)
         if dry_run:
             _print_plan(dataset.operations, plan)
             return []
 
+        # NOTE: pipeline ID incremented on clean completion only
+        self._pipeline_id += 1
         last_exception: Exception | None = None
         for attempt in range(self.max_execution_retries + 1):
             execution_id = _generate_execution_id()
-            logger.info("Starting zephyr pipeline: %s (attempt %d)", execution_id, attempt)
+            logger.info(
+                "Starting zephyr pipeline: %s (pipeline %d, attempt %d)", execution_id, self._pipeline_id, attempt
+            )
 
             try:
-                self._ensure_coordinator()
-                self._ensure_workers(plan.num_shards)
+                self._create_coordinator(attempt)
+                self._create_workers(plan.num_shards, attempt)
 
-                # Run pipeline on coordinator (blocking call)
+                # Run pipeline on coordinator (blocking call).
+                # run_pipeline() calls coordinator.shutdown() at the end,
+                # which causes workers to receive SHUTDOWN on their next
+                # pull_task() call.
                 results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()
 
                 return results
@@ -1007,34 +1091,34 @@ class ZephyrContext:
                     raise
 
                 logger.warning(
-                    "Pipeline attempt %d/%d failed, retrying with fresh coordinator: %s",
-                    attempt + 1,
-                    self.max_execution_retries,
+                    "Pipeline attempt %d failed (%d retries left), retrying: %s",
+                    attempt,
+                    self.max_execution_retries - attempt,
                     e,
                 )
-                # Tear down dead coordinator and orphaned workers so
-                # _ensure_coordinator / _ensure_workers create fresh ones.
-                self.shutdown()
 
             finally:
-                # Clean up chunks for this execution only (workers persist)
+                # Tear down coordinator and workers for this pipeline
+                self.shutdown()
+                # Clean up chunks for this execution
                 _cleanup_execution(self.chunk_storage_prefix, execution_id)
 
         # Should be unreachable, but just in case
         raise last_exception  # type: ignore[misc]
 
-    def _ensure_coordinator(self) -> None:
-        """Create coordinator if not already initialized."""
-        if self._coordinator is not None:
-            return
-
-        # Create coordinator actor with high max_concurrency to allow
-        # workers to call pull_task/report_result while run_pipeline blocks
-        logger.info("Starting coordinator for %s", self.name)
-        coordinator_resources = ResourceConfig(cpu=1, ram="2g", max_concurrency=100)
+    def _create_coordinator(self, attempt: int = 0) -> None:
+        """Create a fresh coordinator actor."""
+        # max_concurrency allows workers to call pull_task/report_result
+        # while run_pipeline blocks.
+        # max_restarts=0: coordinator must NOT auto-restart on preemption.
+        # A restarted coordinator only re-runs __init__, not initialize() or
+        # run_pipeline(), so it becomes a zombie. Instead, let it die and let
+        # the execute() retry loop tear down everything and start fresh.
+        logger.info("Starting coordinator for %s (pipeline %d, attempt %d)", self.name, self._pipeline_id, attempt)
+        coordinator_resources = ResourceConfig(cpu=1, ram="2g", max_concurrency=200, max_restarts=0)
         self._coordinator_group = self.client.create_actor_group(
             ZephyrCoordinator,
-            name=f"zephyr-{self.name}-coord",
+            name=f"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord",
             count=1,
             resources=coordinator_resources,
         )
@@ -1048,27 +1132,28 @@ class ZephyrContext:
 
         logger.info("Coordinator initialized for %s", self.name)
 
-    def _ensure_workers(self, num_shards: int) -> None:
-        """Create workers if not already initialized, sized to demand.
+    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:
+        """Create a fresh worker pool sized to demand.
 
         The worker count is min(max_workers, num_shards) to avoid
         over-provisioning when there are fewer shards than the cap.
         """
-        if self._worker_group is not None or num_shards == 0:
+        if num_shards == 0:
             return
 
         assert self.max_workers is not None  # set by __post_init__
         actual_workers = min(self.max_workers, num_shards)
         logger.info(
-            "Starting worker group: %d workers (max_workers=%d, num_shards=%d)",
+            "Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)",
             actual_workers,
             self.max_workers,
             num_shards,
+            attempt,
         )
         self._worker_group = self.client.create_actor_group(
             ZephyrWorker,
             self._coordinator,  # Pass coordinator handle as init arg
-            name=f"zephyr-{self.name}-workers",
+            name=f"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers",
             count=actual_workers,
             resources=self.resources,
         )
@@ -1097,8 +1182,6 @@ class ZephyrContext:
         self._worker_group = None
 
     def __enter__(self) -> ZephyrContext:
-        # Eagerly initialize coordinator; workers are deferred to first execute()
-        self._ensure_coordinator()
         return self
 
     def __exit__(self, *exc) -> None:
diff --git c/lib/zephyr/tests/test_execution.py w/lib/zephyr/tests/test_execution.py
index 2ac8e7f60..bc851c6cd 100644
--- c/lib/zephyr/tests/test_execution.py
+++ w/lib/zephyr/tests/test_execution.py
@@ -344,8 +344,8 @@ def test_chunk_streaming_low_memory(tmp_path):
     assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]
 
 
-def test_workers_persist_across_executes(fray_client, tmp_path):
-    """Workers persist across multiple execute() calls within a context."""
+def test_fresh_actors_per_execute(fray_client, tmp_path):
+    """Each execute() creates and tears down its own coordinator and workers."""
     chunk_prefix = str(tmp_path / "chunks")
 
     with ZephyrContext(
@@ -359,18 +359,19 @@ def test_workers_persist_across_executes(fray_client, tmp_path):
         results = list(zctx.execute(ds))
         assert sorted(results) == [2, 3, 4]
 
-        # After execute(): coordinator still exists (workers persist)
-        assert zctx._coordinator is not None
-        assert zctx._coordinator_group is not None
+        # After execute(): everything is torn down
+        assert zctx._coordinator is None
+        assert zctx._worker_group is None
+        assert zctx._pipeline_id == 1
 
-        # Can execute again (reuses same workers)
+        # Can execute again (creates fresh coordinator + workers)
         ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)
         results2 = list(zctx.execute(ds2))
         assert sorted(results2) == [20, 40]
 
-    # After context exit: all resources are cleaned up
-    assert zctx._coordinator is None
-    assert zctx._coordinator_group is None
+        assert zctx._coordinator is None
+        assert zctx._worker_group is None
+        assert zctx._pipeline_id == 2
 
 
 def test_fatal_errors_fail_fast(fray_client, tmp_path):
@@ -438,16 +439,15 @@ def test_workers_capped_to_shard_count(fray_client, tmp_path):
         chunk_storage_prefix=str(tmp_path / "chunks"),
         name=f"test-execution-{uuid.uuid4().hex[:8]}",
     ) as ctx:
-        # Workers not yet created before execute()
-        assert ctx._worker_group is None
         results = list(ctx.execute(ds.map(lambda x: x * 2)))
         assert sorted(results) == [2, 4, 6]
-        # Worker group sized to min(10, 3) = 3
-        assert ctx._worker_count == 3
+        # Everything torn down after execute; correct results prove workers
+        # were created and sized properly (min(10, 3) = 3)
+        assert ctx._pipeline_id == 1
 
 
-def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):
-    """Second execute() reuses the same worker group object (no re-creation)."""
+def test_pipeline_id_increments(fray_client, tmp_path):
+    """Pipeline ID increments after each execute(), ensuring unique actor names."""
     with ZephyrContext(
         client=fray_client,
         max_workers=10,
@@ -456,9 +456,65 @@ def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):
         name=f"test-execution-{uuid.uuid4().hex[:8]}",
     ) as ctx:
         ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))
-        group_after_first = ctx._worker_group
+        assert ctx._pipeline_id == 1
+
         ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))
-        assert ctx._worker_group is group_after_first
+        assert ctx._pipeline_id == 2
+
+
+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):
+    """When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN."""
+    from unittest.mock import MagicMock
+
+    from fray.v2.actor import ActorContext, _set_current_actor, _reset_current_actor
+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator
+
+    token = _set_current_actor(ActorContext(handle=MagicMock(), index=0, group_name="test-coord"))
+    try:
+        coord = ZephyrCoordinator()
+    finally:
+        _reset_current_actor(token)
+    coord.set_chunk_config(str(tmp_path / "chunks"), "test-exec")
+    coord.set_shared_data({})
+
+    task = ShardTask(
+        shard_idx=0,
+        total_shards=1,
+        chunk_size=100,
+        shard=Shard(chunks=[]),
+        operations=[],
+        stage_name="test",
+    )
+
+    # Non-last stage: empty queue returns None
+    coord.start_stage("stage-0", [task], is_last_stage=False)
+    pulled = coord.pull_task("worker-A")
+    assert pulled is not None and pulled != "SHUTDOWN"
+    _task, attempt, _config = pulled
+    coord.report_result("worker-A", 0, attempt, TaskResult(chunks=[]))
+
+    # Queue empty, but not last stage -> None
+    result = coord.pull_task("worker-A")
+    assert result is None
+
+    # Last stage: empty queue returns SHUTDOWN
+    task2 = ShardTask(
+        shard_idx=0,
+        total_shards=1,
+        chunk_size=100,
+        shard=Shard(chunks=[]),
+        operations=[],
+        stage_name="test-last",
+    )
+    coord.start_stage("stage-1", [task2], is_last_stage=True)
+    pulled = coord.pull_task("worker-A")
+    assert pulled is not None and pulled != "SHUTDOWN"
+    _task, attempt, _config = pulled
+    coord.report_result("worker-A", 0, attempt, TaskResult(chunks=[]))
+
+    # Queue empty on last stage -> SHUTDOWN
+    result = coord.pull_task("worker-A")
+    assert result == "SHUTDOWN"
 
 
 def test_execute_retries_on_coordinator_death(tmp_path):
@@ -482,26 +538,32 @@ def test_execute_retries_on_coordinator_death(tmp_path):
         name=f"test-execution-{uuid.uuid4().hex[:8]}",
     )
 
-    # First execute() succeeds normally — establishes coordinator + workers
-    ctx._ensure_coordinator()
-    coord_endpoint = ctx._coordinator._endpoint
-
+    # First execute() succeeds normally
     results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))
     assert sorted(results) == [2, 4, 6]
 
-    # Kill the coordinator by removing it from the local actor registry.
-    # This simulates VM preemption — the handle becomes stale and any RPC
-    # through it will raise RuntimeError("Actor not found in registry").
-    _local_actor_registry.pop(coord_endpoint, None)
+    # Sabotage the registry so the *next* coordinator creation attempt fails
+    # on the first try. We do this by patching create_actor_group to fail once.
+    original_create = client.create_actor_group
+    fail_count = [0]
+
+    def flaky_create(*args, **kwargs):
+        group = original_create(*args, **kwargs)
+        if fail_count[0] == 0 and "coord" in kwargs.get("name", ""):
+            fail_count[0] += 1
+            # Kill the coordinator immediately after creation to simulate death
+            handles = group.wait_ready()
+            endpoint = handles[0]._endpoint
+            _local_actor_registry.pop(endpoint, None)
+        return group
+
+    client.create_actor_group = flaky_create
 
     # Next execute() should: fail on attempt 0 (dead coordinator),
-    # shutdown + recreate everything, then succeed on attempt 1.
+    # then succeed on attempt 1 with a fresh coordinator.
     results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))
     assert sorted(results) == [11, 21]
 
-    # Verify coordinator was recreated (handle is not None after retry)
-    assert ctx._coordinator is not None
-
     ctx.shutdown()
     client.shutdown(wait=True)
 
diff --git c/tests/execution/test_disk_cache.py w/tests/execution/test_disk_cache.py
index d9b4b6274..d25d1b036 100644
--- c/tests/execution/test_disk_cache.py
+++ w/tests/execution/test_disk_cache.py
@@ -6,7 +6,7 @@ import os
 from pathlib import Path
 
 from marin.execution.artifact import Artifact
-from marin.execution.disk_cache import disk_cached
+from marin.execution.disk_cache import disk_cache
 from marin.execution.distributed_lock import distributed_lock
 from marin.execution.executor_step_status import STATUS_SUCCESS, StatusFile
 from marin.execution.step_spec import StepSpec
@@ -36,12 +36,14 @@ def test_disk_cached_runs_and_caches(tmp_path: Path):
     fn, get_count = _make_fn()
     output_path = StepSpec(name="step", output_path_prefix=tmp_path.as_posix()).output_path
 
-    result1 = disk_cached(fn, output_path)
+    cached_fn = disk_cache(fn, output_path=output_path)
+
+    result1 = cached_fn(output_path)
     assert get_count() == 1
     assert result1 == {"value": 42, "computed": True}
 
     # Cache hit: fn is called to load but no recomputation
-    result2 = disk_cached(fn, output_path)
+    result2 = cached_fn(output_path)
     assert get_count() == 1
     assert result2 == result1
 
@@ -56,7 +58,8 @@ def test_disk_cached_skips_when_another_worker_completed(tmp_path: Path):
         json.dump({"value": 99, "from_other": True}, f)
     StatusFile(spec.output_path, "other-worker").write_status(STATUS_SUCCESS)
 
-    result = disk_cached(fn, spec.output_path)
+    cached_fn = disk_cache(fn, output_path=spec.output_path)
+    result = cached_fn(spec.output_path)
 
     assert get_count() == 0
     assert result == {"value": 99, "from_other": True}
@@ -74,20 +77,17 @@ def test_composition_with_save_load(tmp_path: Path):
 
     output_path = StepSpec(name="comp", output_path_prefix=tmp_path.as_posix()).output_path
 
-    result1 = disk_cached(
+    cached_fn = disk_cache(
         distributed_lock(counting_fn),
-        output_path,
-        save=Artifact.save,
-        load=Artifact.load,
+        output_path=output_path,
+        save_fn=Artifact.save,
+        load_fn=Artifact.load,
     )
+
+    result1 = cached_fn(output_path)
     assert call_count == 1
     assert result1 == {"value": 42}
 
-    result2 = disk_cached(
-        distributed_lock(counting_fn),
-        output_path,
-        save=Artifact.save,
-        load=Artifact.load,
-    )
+    result2 = cached_fn(output_path)
     assert call_count == 1
     assert result2 == result1

```

extract the parts that a related to the zephyr worker pool restarts, but keep the zephyr worker downsizing separate

---

[Request interrupted by user]

---

Take the change set below:


```
diff --git c/experiments/pretraining_datasets/nemotron.py w/experiments/pretraining_datasets/nemotron.py
index 34099ec54..d3f4d1c27 100644
--- c/experiments/pretraining_datasets/nemotron.py
+++ w/experiments/pretraining_datasets/nemotron.py
@@ -64,7 +64,11 @@ def _get_nemotron_split_paths(split: str):
 
 
 def tokenize_nemotron(
-    *, tokenizer: str | None = None, window_size_bytes: int = 10_000_000_000
+    *,
+    tokenizer: str | None = None,
+    window_size_bytes: int = 10_000_000_000,
+    max_workers: int = 4096,
+    writer_batch_size: int = 65536,
 ) -> dict[str, TokenizerStep]:
     """Generate tokenization steps for all Nemotron CC dataset splits."""
     if tokenizer is None:
@@ -85,6 +89,8 @@ def tokenize_nemotron(
                 cache_path=this_output_path(),
                 tokenizer=versioned(tokenizer),
                 window_size_bytes=window_size_bytes,
+                max_workers=max_workers,
+                writer_batch_size=writer_batch_size,
             ),
         )
 
diff --git c/lib/fray/src/fray/v2/ray_backend/backend.py w/lib/fray/src/fray/v2/ray_backend/backend.py
index 36a70ae13..a24e89a34 100644
--- c/lib/fray/src/fray/v2/ray_backend/backend.py
+++ w/lib/fray/src/fray/v2/ray_backend/backend.py
@@ -409,6 +409,12 @@ def _actor_ray_options(resources: ResourceConfig) -> dict[str, Any]:
         # node is preempted. Without this, the actor dies permanently and the
         # pool degrades over time (see marin#2943).
         options["max_restarts"] = -1
+    # Explicit max_restarts takes precedence over the preemptible default.
+    # Useful for actors like coordinators that are preemptible (not pinned to
+    # head node) but should NOT auto-restart because they require remote
+    # initialization beyond __init__.
+    if resources.max_restarts is not None:
+        options["max_restarts"] = resources.max_restarts
     if resources.max_concurrency > 1:
         options["max_concurrency"] = resources.max_concurrency
     return options
diff --git c/lib/fray/src/fray/v2/types.py w/lib/fray/src/fray/v2/types.py
index 7c9c99c7c..740495b5d 100644
--- c/lib/fray/src/fray/v2/types.py
+++ w/lib/fray/src/fray/v2/types.py
@@ -322,6 +322,8 @@ class ResourceConfig:
     regions: Sequence[str] | None = None
     replicas: int = 1
     max_concurrency: int = 1
+    # TODO(rav): is this the right place for max_restarts?
+    max_restarts: int | None = None
 
     def chip_count(self) -> int:
         """Total accelerator chips across all replicas."""
diff --git c/lib/fray/tests/test_v2_ray.py w/lib/fray/tests/test_v2_ray.py
index 08723da52..40a304c34 100644
--- c/lib/fray/tests/test_v2_ray.py
+++ w/lib/fray/tests/test_v2_ray.py
@@ -271,3 +271,11 @@ def test_actor_options_preemptible_sets_max_restarts():
     options = _actor_ray_options(ResourceConfig(preemptible=True))
     assert options["max_restarts"] == -1
     assert "resources" not in options
+
+
+def test_actor_options_explicit_max_restarts_overrides_preemptible():
+    from fray.v2.ray_backend.backend import _actor_ray_options
+
+    options = _actor_ray_options(ResourceConfig(preemptible=True, max_restarts=0))
+    assert options["max_restarts"] == 0
+    assert "resources" not in options
diff --git c/lib/marin/src/marin/execution/disk_cache.py w/lib/marin/src/marin/execution/disk_cache.py
index ae8206068..5735b67ce 100644
--- c/lib/marin/src/marin/execution/disk_cache.py
+++ w/lib/marin/src/marin/execution/disk_cache.py
@@ -4,69 +4,124 @@
 """Run a function once, cache the result, and return it on subsequent calls."""
 
 from __future__ import annotations
+import os
 
+import functools
 import logging
 from collections.abc import Callable
-from typing import TypeVar
+from typing import TypeVar, Generic
+
+import cloudpickle
+import fsspec
+from iris.temp_buckets import get_temp_bucket_path
 
 from marin.execution.distributed_lock import StepAlreadyDone
-from marin.execution.executor_step_status import (
-    STATUS_FAILED,
-    STATUS_SUCCESS,
-    StatusFile,
-    worker_id,
-)
+import hashlib
+import pickle
 
 logger = logging.getLogger(__name__)
 
 T = TypeVar("T")
 
 
-def disk_cached(
-    fn: Callable[[str], T],
-    output_path: str,
-    *,
-    save: Callable[[T, str], None] | None = None,
-    load: Callable[[str], T] | None = None,
-) -> T | None:
-    """Run *fn* once and track completion via a status file.
+class disk_cache(Generic[T]):
+    """Decorator that caches function results to disk via a status file.
 
-    When *save* and *load* are provided, the result of *fn* is persisted and
-    deserialized on cache hits.  When they are ``None`` (the default), *fn* is
-    expected to handle its own reading/writing at *output_path* and is called
-    again on cache hits to load the result.
+    Supports bare decoration, decoration with arguments, and direct wrapping::
 
-    By itself this function does not provide any locking guarantees, e.g. if
-    you run disk_cached on multiple workers/shards, there may be a race
-    condition where multiple workers evaluate the function and write to the same
-    output path at the same time. To avoid this, compose with
-    :func:`distributed_lock` to ensure that only one worker executes the
-    function while the others wait for the result. For example::
+        @disk_cached
+        def my_fn(*args):
+            ...
 
-        result = disk_cached(
-            distributed_lock(get_tokenizer),
-            output_path,
-            save=Artifact.save,
-            load=lambda p: Artifact.load(p, TokenizerInfo),
-        )
+        @disk_cached(output_path="gs://...", save_fn=save, load_fn=load)
+        def my_fn(*args):
+            ...
+
+        cached_fn = disk_cached(fn, output_path=path, save_fn=save, load_fn=load)
+        result = cached_fn(output_path)
+
+    When *save_fn* and *load_fn* are provided, the result is persisted and
+    deserialized on cache hits.  When they are ``None`` (the default),
+    cloudpickle is used.
+
+    The decorated function is called with its original arguments; the
+    *output_path* is used only for cache bookkeeping.
+
+    By itself this class does not provide any locking guarantees.  Compose with
+    :func:`distributed_lock` to ensure single-writer semantics.
     """
-    status_file = StatusFile(output_path, worker_id())
-    if status_file.status == STATUS_SUCCESS:
-        logger.info(f"disk_cached: cache hit for {output_path}")
-        return load(output_path) if load is not None else fn(output_path)
 
-    try:
-        result = fn(output_path)
-    except StepAlreadyDone:
-        # NOTE: this is leaky but this branch handles the case of distributed lock wrapper
-        logger.info(f"disk_cached: completed by another worker for {output_path}")
-        return load(output_path) if load is not None else fn(output_path)
-    except Exception:
-        StatusFile(output_path, worker_id()).write_status(STATUS_FAILED)
-        raise
+    def __init__(
+        self,
+        fn: Callable[..., T] | None = None,
+        *,
+        output_path: str | None = None,
+        save_fn: Callable[[T, str], None] | None = None,
+        load_fn: Callable[[str], T] | None = None,
+    ):
+        self._output_path = output_path
+        self._save_fn = save_fn
+        self._load_fn = load_fn
+        self._fn: Callable[..., T] | None = None
 
-    if save is not None:
-        save(result, output_path)
-    StatusFile(output_path, worker_id()).write_status(STATUS_SUCCESS)
-    logger.info(f"disk_cached: computed and cached {output_path}")
-    return result
+        if callable(fn):
+            self._fn = fn
+            functools.update_wrapper(self, fn)
+
+    # TODO: add ParamSpec
+    def __call__(self, *args, **kwargs):
+        if self._fn is None:
+            # @disk_cached(...) — receiving the function to wrap
+            if len(args) == 1 and callable(args[0]) and not kwargs:
+                self._fn = args[0]  # type: ignore[bad-assignment]
+                functools.update_wrapper(self, self._fn)
+                return self
+            raise TypeError("disk_cached() expected a callable")
+
+        return self._execute(*args, **kwargs)
+
+    def _execute(self, *args, **kwargs):
+        # Compute a fingerprint of the function arguments for cache keying.
+
+        def fingerprint_args(*args, **kwargs) -> str:
+            """Create a deterministic fingerprint for args and kwargs."""
+            # Include the function name in the fingerprint to avoid collisions across different functions.
+            fn_name = getattr(self._fn, "__name__", None)
+            data = pickle.dumps((fn_name, args, kwargs))
+            return hashlib.sha256(data).hexdigest()[:8]
+
+        output_path = self._output_path
+        if output_path is None:
+            args_fingerprint = fingerprint_args(*args, **kwargs)
+            output_path = get_temp_bucket_path(1, f"disk_cache_{args_fingerprint}")
+            if output_path is None:
+                output_path = os.environ["MARIN_PREFIX"] + f"/disk_cache_{args_fingerprint}"
+
+        def load_result():
+            assert output_path is not None
+            if self._load_fn is not None:
+                return self._load_fn(output_path)
+            with fsspec.open(output_path + "/data.pkl", "rb") as f:
+                return cloudpickle.loads(f.read())
+
+        try:
+            return load_result()
+        except FileNotFoundError:
+            logger.info(f"disk_cached: cache miss for {output_path}")
+
+        assert self._fn is not None
+        try:
+            result = self._fn(*args, **kwargs)
+        except StepAlreadyDone:
+            # NOTE: this is leaky but this branch handles the case of distributed lock wrapper
+            logger.info(f"disk_cached: completed by another worker for {output_path}")
+            return load_result()
+
+        if self._save_fn is not None:
+            self._save_fn(result, output_path)
+        else:
+            with fsspec.open(output_path + "/data.pkl", "wb") as f:
+                f.write(cloudpickle.dumps(result))
+
+        logger.info(f"disk_cached: computed and cached {output_path}")
+        return result
diff --git c/lib/marin/src/marin/execution/step_runner.py w/lib/marin/src/marin/execution/step_runner.py
index 8d5a5575a..c55de576e 100644
--- c/lib/marin/src/marin/execution/step_runner.py
+++ w/lib/marin/src/marin/execution/step_runner.py
@@ -262,7 +262,7 @@ class StepRunner:
     def _launch_step(self, step: StepSpec, *, force_run_failed: bool, dry_run: bool) -> JobHandle | None:
         """Launch a single step as a fray job. Returns None if skipped."""
         from marin.execution.artifact import Artifact
-        from marin.execution.disk_cache import disk_cached
+        from marin.execution.disk_cache import disk_cache
         from marin.execution.distributed_lock import distributed_lock
 
         output_path = step.output_path
@@ -291,14 +291,15 @@ class StepRunner:
         # All caching, locking, heartbeat, artifact saving, and status writes
         # happen on the worker.
         step_fn = step.fn
+        cached_step = disk_cache(
+            distributed_lock(step_fn, force_run_failed=force_run_failed),
+            output_path=output_path,
+            save_fn=Artifact.save,
+            load_fn=Artifact.load,
+        )
 
         def worker_fn():
-            disk_cached(
-                distributed_lock(step_fn, force_run_failed=force_run_failed),
-                output_path,
-                save=Artifact.save,
-                load=Artifact.load,
-            )
+            cached_step(output_path)
 
         worker_fn.__qualname__ = step_name
         worker_fn.__name__ = step_name
diff --git c/lib/marin/src/marin/processing/tokenize/tokenize.py w/lib/marin/src/marin/processing/tokenize/tokenize.py
index 3a78a164c..1f511079a 100644
--- c/lib/marin/src/marin/processing/tokenize/tokenize.py
+++ w/lib/marin/src/marin/processing/tokenize/tokenize.py
@@ -7,6 +7,9 @@ Tokenize datasets using zephyr pipeline and write to Levanter cache format.
 Supports both regular file paths and HuggingFace datasets. For HF datasets, downloads
 them first then tokenizes the downloaded files.
 """
+from functools import cache
+from marin.utilities.time_logger import log_time
+from marin.execution.disk_cache import disk_cache
 
 import abc
 import dataclasses
@@ -33,7 +36,7 @@ from levanter.data.text import (
 )
 from levanter.store.cache import consolidate_shard_caches
 from levanter.store.tree_store import TreeStore
-from zephyr import Dataset, ZephyrContext, zephyr_worker_ctx
+from zephyr import Dataset, ZephyrContext
 from zephyr.readers import load_file
 
 from marin.execution.executor import ExecutorStep, InputName, VersionedValue
@@ -83,7 +86,7 @@ class TokenizeConfig(TokenizeConfigBase):
     """Files are bundled into groups up to this size; each group becomes one shard.
     Smaller values produce more shards and thus more parallelism (up to max_workers)."""
     max_workers: int = 4096
-    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram="5g", disk="5g"))
+    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram="10g", disk="5g"))
     writer_batch_size: int = 65536
     """Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op
     overhead. Too large a value increases memory usage and delays progress checkpointing."""
@@ -147,7 +150,7 @@ class HfTokenizeConfig(TokenizeConfigBase):
     window_size_bytes: int = 10_000_000_000
     """Files are bundled into groups up to this size; each group becomes one shard.
     Smaller values produce more shards and thus more parallelism (up to max_workers)."""
-    max_workers: int = 4096
+    max_workers: int = 512
     worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram="5g", disk="5g"))
     writer_batch_size: int = 65536
     """Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op
@@ -255,7 +258,7 @@ def _bundle_files_by_size(file_infos, max_bytes: int):
 
 def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Iterator[Sequence[dict]]) -> Iterator[dict]:
     """Tokenize a list of batches using the specified tokenizer and format."""
-    tokenizer: transformers.PreTrainedTokenizer = zephyr_worker_ctx().get_shared("tokenizer")
+    tokenizer: transformers.PreTrainedTokenizer = get_tokenizer(config.tokenizer)
     batch_processor = preprocessor_for_format(config.format, tokenizer)
 
     batch_count = 0
@@ -289,6 +292,12 @@ def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Ite
     )
 
 
+@cache
+@disk_cache
+def get_tokenizer(tokenizer: str) -> transformers.PreTrainedTokenizer:
+    return transformers.AutoTokenizer.from_pretrained(tokenizer)
+
+
 def tokenize(config: TokenizeConfigBase):
     """Tokenize datasets using zephyr pipeline.
 
@@ -376,8 +385,8 @@ def tokenize(config: TokenizeConfigBase):
             )
         )
 
-        # Broadcast the tokenizer to all workers via ZephyrContext
-        ctx.put("tokenizer", transformers.AutoTokenizer.from_pretrained(config.tokenizer))
+        with log_time(f"{config.tokenizer} cache warmup"):
+            get_tokenizer(config.tokenizer)
 
         tokenize_start = time.monotonic()
         shard_paths = ctx.execute(temp_shards)
@@ -422,7 +431,7 @@ def tokenize(config: TokenizeConfigBase):
             resources=config.worker_resources,
             max_workers=min(config.max_workers, len(train_groups)),
             name="tokenize-train",
-            no_workers_timeout=20 * 60,
+            no_workers_timeout=6 * 60 * 60,
         ) as ctx:
             run_pipeline(ctx, train_groups, "train")
 
@@ -432,7 +441,7 @@ def tokenize(config: TokenizeConfigBase):
             resources=config.worker_resources,
             max_workers=min(config.max_workers, len(validation_groups)),
             name="tokenize-validation",
-            no_workers_timeout=20 * 60,
+            no_workers_timeout=6 * 60 * 60,
         ) as ctx:
             run_pipeline(ctx, validation_groups, "validation")
 
diff --git c/lib/zephyr/src/zephyr/execution.py w/lib/zephyr/src/zephyr/execution.py
index 87715feee..385585d9c 100644
--- c/lib/zephyr/src/zephyr/execution.py
+++ w/lib/zephyr/src/zephyr/execution.py
@@ -10,6 +10,8 @@ protocol.
 """
 
 from __future__ import annotations
+from datetime import timedelta
+import contextlib
 
 import enum
 import logging
@@ -47,6 +49,14 @@ from zephyr.writers import ensure_parent_dir
 logger = logging.getLogger(__name__)
 
 
+@contextlib.contextmanager
+def log_time(label: str, level: int = logging.INFO) -> Iterator[None]:
+    t_start = time.perf_counter()
+    yield
+    t_end = time.perf_counter()
+    logger.log(level, f"{label} took {timedelta(seconds=t_end - t_start)}")
+
+
 class Chunk(Protocol):
     def __iter__(self) -> Iterator: ...
 
@@ -247,6 +257,8 @@ class ZephyrCoordinator:
     """
 
     def __init__(self):
+        from fray.v2 import current_actor
+
         # Task management state
         self._task_queue: deque[ShardTask] = deque()
         self._results: dict[int, TaskResult] = {}
@@ -268,11 +280,15 @@ class ZephyrCoordinator:
         self._worker_handles: dict[str, ActorHandle] = {}
         self._coordinator_thread: threading.Thread | None = None
         self._shutdown: bool = False
+        self._is_last_stage: bool = False
         self._initialized: bool = False
 
         # Lock for accessing coordinator state from background thread
         self._lock = threading.Lock()
 
+        actor_ctx = current_actor()
+        self._name = f"{actor_ctx.group_name}"
+
     def initialize(
         self,
         chunk_prefix: str,
@@ -314,6 +330,10 @@ class ZephyrCoordinator:
                 self._worker_handles[worker_id] = worker_handle
                 self._worker_states[worker_id] = WorkerState.READY
                 self._last_seen[worker_id] = time.monotonic()
+                # NOTE: if there was a task assigned to the worker, there's a race condidtion between marking
+                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently
+                # lose tasks.
+                self._maybe_requeue_worker_task(worker_id)
                 return
 
             self._worker_handles[worker_id] = worker_handle
@@ -330,9 +350,9 @@ class ZephyrCoordinator:
         last_log_time = 0.0
 
         while not self._shutdown:
-            # Check heartbeats, re-queue stale tasks
-            with self._lock:
-                self._check_worker_heartbeats()
+            # Scheduler check heartbeats, re-queue stale tasks
+            # This is called in a remote fashion to account for potential congestion.
+            self._self_handle.check_heartbeats.remote()
 
             # Log status periodically during active execution
             now = time.monotonic()
@@ -360,6 +380,16 @@ class ZephyrCoordinator:
             dead,
         )
 
+    def _maybe_requeue_worker_task(self, worker_id: str) -> None:
+        """If the worker has a task in-flight, re-queue it and mark the worker as failed."""
+        task_and_attempt = self._in_flight.pop(worker_id, None)
+        if task_and_attempt is not None:
+            logger.info("Worker %s had an in-flight task, re-queuing", worker_id)
+            task, _old_attempt = task_and_attempt
+            self._task_attempts[task.shard_idx] += 1
+            self._task_queue.append(task)
+            self._retries += 1
+
     def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:
         """Internal heartbeat check (called with lock held)."""
         now = time.monotonic()
@@ -367,13 +397,7 @@ class ZephyrCoordinator:
             if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:
                 logger.warning(f"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)")
                 self._worker_states[worker_id] = WorkerState.FAILED
-                task_and_attempt = self._in_flight.pop(worker_id, None)
-                if task_and_attempt is not None:
-                    logger.info("Removed task %s from worker %s, re-queueing", task_and_attempt, worker_id)
-                    task, _old_attempt = task_and_attempt
-                    self._task_attempts[task.shard_idx] += 1
-                    self._task_queue.append(task)
-                    self._retries += 1
+                self._maybe_requeue_worker_task(worker_id)
 
     def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:
         """Called by workers to get next task.
@@ -394,6 +418,8 @@ class ZephyrCoordinator:
                 return None
 
             if not self._task_queue:
+                if self._is_last_stage:
+                    return "SHUTDOWN"
                 return None
 
             task = self._task_queue.popleft()
@@ -408,9 +434,25 @@ class ZephyrCoordinator:
             }
             return (task, attempt, config)
 
+    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:
+        """Assert _in_flight[worker_id], if present, matches the reported shard.
+
+        Workers block on report_result/report_error before calling pull_task,
+        so _in_flight can never point to a different shard. It may be absent
+        if a heartbeat timeout already re-queued the task.
+        """
+        in_flight = self._in_flight.get(worker_id)
+        if in_flight is not None:
+            assert in_flight[0].shard_idx == shard_idx, (
+                f"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, "
+                f"but _in_flight tracks shard {in_flight[0].shard_idx}. "
+                f"This indicates report_result/pull_task reordering — workers must block on report_result."
+            )
+
     def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:
         with self._lock:
             self._last_seen[worker_id] = time.monotonic()
+            self._assert_in_flight_consistent(worker_id, shard_idx)
 
             current_attempt = self._task_attempts.get(shard_idx, 0)
             if attempt != current_attempt:
@@ -429,13 +471,14 @@ class ZephyrCoordinator:
         """Worker reports a task failure. All errors are fatal."""
         with self._lock:
             self._last_seen[worker_id] = time.monotonic()
+            self._assert_in_flight_consistent(worker_id, shard_idx)
             self._in_flight.pop(worker_id, None)
             self._fatal_error = error_info
             self._worker_states[worker_id] = WorkerState.DEAD
 
     def heartbeat(self, worker_id: str) -> None:
-        with self._lock:
-            self._last_seen[worker_id] = time.monotonic()
+        # NOTE: there's no need for lock here
+        self._last_seen[worker_id] = time.monotonic()
 
     def get_status(self) -> JobStatus:
         with self._lock:
@@ -461,7 +504,7 @@ class ZephyrCoordinator:
         with self._lock:
             return self._fatal_error
 
-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:
+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:
         """Load a new stage's tasks into the queue."""
         with self._lock:
             self._task_queue = deque(tasks)
@@ -473,13 +516,14 @@ class ZephyrCoordinator:
             self._in_flight = {}
             self._task_attempts = {task.shard_idx: 0 for task in tasks}
             self._fatal_error = None
+            self._is_last_stage = is_last_stage
 
     def _wait_for_stage(self) -> None:
         """Block until current stage completes or error occurs."""
         backoff = ExponentialBackoff(initial=0.05, maximum=1.0)
         last_log_completed = -1
         start_time = time.monotonic()
-        warned_no_workers = False
+        all_dead_since: float | None = None
         no_workers_timeout = self._no_workers_timeout
 
         while True:
@@ -487,27 +531,37 @@ class ZephyrCoordinator:
                 if self._fatal_error:
                     raise ZephyrWorkerError(self._fatal_error)
 
-                num_workers = len(self._worker_handles)
                 completed = self._completed_shards
                 total = self._total_shards
 
                 if completed >= total:
                     return
 
-                # Fail fast if no workers appear within timeout
-                if num_workers == 0:
-                    elapsed = time.monotonic() - start_time
-                    if elapsed > no_workers_timeout:
+                # Count alive workers (READY or BUSY), not just total registered.
+                # Dead/failed workers stay in _worker_handles but can't make progress.
+                alive_workers = sum(
+                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}
+                )
+
+                if alive_workers == 0:
+                    now = time.monotonic()
+                    elapsed = now - start_time
+
+                    if all_dead_since is None:
+                        all_dead_since = now
+                        logger.warning("All workers are dead/failed. Waiting for workers to recover...")
+
+                    dead_duration = now - all_dead_since
+                    if dead_duration > no_workers_timeout:
                         raise ZephyrWorkerError(
-                            f"No workers available after {elapsed:.1f}s. "
+                            f"No alive workers for {dead_duration:.1f}s "
+                            f"(total elapsed {elapsed:.1f}s). "
+                            f"All {len(self._worker_handles)} registered workers are dead/failed. "
                             "Check cluster resources and worker group configuration."
                         )
-                    if not warned_no_workers and elapsed > 5.0:
-                        logger.warning(
-                            "No workers available yet after %.1fs, waiting for discovery...",
-                            elapsed,
-                        )
-                        warned_no_workers = True
+                else:
+                    # Workers are alive — reset the dead timer
+                    all_dead_since = None
 
             if completed != last_log_completed:
                 logger.info("[%s] %d/%d tasks completed", self._stage_name, completed, total)
@@ -521,6 +575,7 @@ class ZephyrCoordinator:
         with self._lock:
             return dict(self._results)
 
+    # TODO: this is not used anywhere!?
     def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:
         """Set config for the current execution."""
         with self._lock:
@@ -543,6 +598,15 @@ class ZephyrCoordinator:
         if not shards:
             return []
 
+        # Identify the last stage that dispatches work to workers (non-RESHARD).
+        # On that stage, idle workers receive SHUTDOWN once all tasks are
+        # in-flight, so they exit eagerly instead of polling until
+        # coordinator.shutdown().
+        last_worker_stage_idx = max(
+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),
+            default=-1,
+        )
+
         for stage_idx, stage in enumerate(plan.stages):
             stage_label = f"stage{stage_idx}-{stage.stage_name(max_length=40)}"
 
@@ -556,7 +620,7 @@ class ZephyrCoordinator:
             # Build and submit tasks
             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)
             logger.info("Starting stage %s with %d tasks", stage_label, len(tasks))
-            self._start_stage(stage_label, tasks)
+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))
 
             # Wait for stage completion
             self._wait_for_stage()
@@ -570,6 +634,10 @@ class ZephyrCoordinator:
         for shard in shards:
             for chunk in shard.chunks:
                 flat_result.extend(list(chunk))
+
+        # Signal workers to shut down now that all stages are complete.
+        self.shutdown()
+
         return flat_result
 
     def _compute_join_aux(
@@ -649,9 +717,9 @@ class ZephyrCoordinator:
                 "execution_id": self._execution_id,
             }
 
-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:
+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:
         """Load a new stage's tasks into the queue (legacy compat)."""
-        self._start_stage(stage_name, tasks)
+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)
 
     def check_heartbeats(self, timeout: float = 30.0) -> None:
         """Marks stale workers as FAILED, re-queues their in-flight tasks."""
@@ -666,6 +734,9 @@ class ZephyrCoordinator:
         """Signal workers that no more stages will be submitted (legacy compat)."""
         self._shutdown = True
 
+    def __repr__(self) -> str:
+        return f"ZephyrCoordinator(name={self._name})"
+
 
 # ---------------------------------------------------------------------------
 # ZephyrWorker
@@ -695,8 +766,8 @@ class ZephyrWorker:
         actor_ctx = current_actor()
         self._worker_id = f"{actor_ctx.group_name}-{actor_ctx.index}"
 
-        # Register with coordinator
-        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle)
+        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity
+        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)
 
         # Start polling in a background thread
         self._polling_thread = threading.Thread(
@@ -735,7 +806,7 @@ class ZephyrWorker:
         heartbeat_count = 0
         while not self._shutdown_event.is_set():
             try:
-                coordinator.heartbeat.remote(self._worker_id)
+                coordinator.heartbeat.remote(self._worker_id).result()
                 heartbeat_count += 1
                 if heartbeat_count % 10 == 1:
                     logger.debug("[%s] Sent heartbeat #%d", self._worker_id, heartbeat_count)
@@ -779,9 +850,12 @@ class ZephyrWorker:
 
             logger.info("[%s] Executing task for shard %d (attempt %d)", self._worker_id, task.shard_idx, attempt)
             try:
-                result = self._execute_shard(task, config)
-                logger.info("[%s] Task complete, reporting result for shard %d", self._worker_id, task.shard_idx)
-                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result)
+                with log_time(f"Worker {self._worker_id} executing shard {task.shard_idx}"):
+                    result = self._execute_shard(task, config)
+                # Block until coordinator records the result. This ensures
+                # report_result is fully processed before the next pull_task,
+                # preventing _in_flight tracking races.
+                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()
                 task_count += 1
             except Exception as e:
                 logger.error("Worker %s error on shard %d: %s", self._worker_id, task.shard_idx, e)
@@ -791,7 +865,7 @@ class ZephyrWorker:
                     self._worker_id,
                     task.shard_idx,
                     "".join(traceback.format_exc()),
-                )
+                ).result()
 
     def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:
         """Execute a stage's operations on a single shard.
@@ -860,6 +934,9 @@ class ZephyrWorker:
         if hasattr(self, "_polling_thread") and self._polling_thread.is_alive():
             self._polling_thread.join(timeout=5.0)
 
+    def __repr__(self) -> str:
+        return f"ZephyrWorker(id={self._worker_id})"
+
 
 def _regroup_result_refs(
     result_refs: dict[int, TaskResult],
@@ -883,12 +960,10 @@ def _regroup_result_refs(
 class ZephyrContext:
     """Execution context for Zephyr pipelines.
 
-    Creates a coordinator actor on __enter__ which owns and manages the worker
-    pool. Workers are created lazily on the first execute() call, sized to
-    min(max_workers, plan.num_shards) to avoid over-provisioning. Workers
-    persist across pipeline stages and execute() calls, allowing cached state
-    (tokenizers, models) to be reused. Shared data broadcast via put() is
-    delivered to workers with each task.
+    Each execute() call creates a fresh coordinator and worker pool, runs
+    the pipeline, then tears everything down. Workers are sized to
+    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared
+    data broadcast via put() is delivered to workers with each task.
 
     Args:
         client: The fray client to use. If None, auto-detects using current_client().
@@ -920,6 +995,8 @@ class ZephyrContext:
     _coordinator_group: Any = field(default=None, repr=False)
     _worker_group: Any = field(default=None, repr=False)
     _worker_count: int = field(default=0, repr=False)
+    # NOTE: execute calls increment this at the very beginning
+    _pipeline_id: int = field(default=-1, repr=False)
 
     def __post_init__(self):
         if self.client is None:
@@ -971,29 +1048,36 @@ class ZephyrContext:
         verbose: bool = False,
         dry_run: bool = False,
     ) -> Sequence:
-        """Execute a dataset pipeline on the worker pool.
+        """Execute a dataset pipeline.
 
-        Workers persist across execute() calls, so cached state (tokenizers,
-        models) will be reused. If the coordinator dies mid-execution (e.g.,
-        VM preemption), the pipeline is retried with a fresh coordinator and
-        worker pool up to ``max_execution_retries`` times. Application errors
-        (``ZephyrWorkerError``) are never retried.
+        Each call creates a fresh coordinator and worker pool, runs the
+        pipeline, then tears everything down. If the coordinator dies
+        mid-execution (e.g., VM preemption), the pipeline is retried
+        with fresh actors up to ``max_execution_retries`` times.
+        Application errors (``ZephyrWorkerError``) are never retried.
         """
         plan = compute_plan(dataset, hints)
         if dry_run:
             _print_plan(dataset.operations, plan)
             return []
 
+        # NOTE: pipeline ID incremented on clean completion only
+        self._pipeline_id += 1
         last_exception: Exception | None = None
         for attempt in range(self.max_execution_retries + 1):
             execution_id = _generate_execution_id()
-            logger.info("Starting zephyr pipeline: %s (attempt %d)", execution_id, attempt)
+            logger.info(
+                "Starting zephyr pipeline: %s (pipeline %d, attempt %d)", execution_id, self._pipeline_id, attempt
+            )
 
             try:
-                self._ensure_coordinator()
-                self._ensure_workers(plan.num_shards)
+                self._create_coordinator(attempt)
+                self._create_workers(plan.num_shards, attempt)
 
-                # Run pipeline on coordinator (blocking call)
+                # Run pipeline on coordinator (blocking call).
+                # run_pipeline() calls coordinator.shutdown() at the end,
+                # which causes workers to receive SHUTDOWN on their next
+                # pull_task() call.
                 results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()
 
                 return results
@@ -1007,34 +1091,34 @@ class ZephyrContext:
                     raise
 
                 logger.warning(
-                    "Pipeline attempt %d/%d failed, retrying with fresh coordinator: %s",
-                    attempt + 1,
-                    self.max_execution_retries,
+                    "Pipeline attempt %d failed (%d retries left), retrying: %s",
+                    attempt,
+                    self.max_execution_retries - attempt,
                     e,
                 )
-                # Tear down dead coordinator and orphaned workers so
-                # _ensure_coordinator / _ensure_workers create fresh ones.
-                self.shutdown()
 
             finally:
-                # Clean up chunks for this execution only (workers persist)
+                # Tear down coordinator and workers for this pipeline
+                self.shutdown()
+                # Clean up chunks for this execution
                 _cleanup_execution(self.chunk_storage_prefix, execution_id)
 
         # Should be unreachable, but just in case
         raise last_exception  # type: ignore[misc]
 
-    def _ensure_coordinator(self) -> None:
-        """Create coordinator if not already initialized."""
-        if self._coordinator is not None:
-            return
-
-        # Create coordinator actor with high max_concurrency to allow
-        # workers to call pull_task/report_result while run_pipeline blocks
-        logger.info("Starting coordinator for %s", self.name)
-        coordinator_resources = ResourceConfig(cpu=1, ram="2g", max_concurrency=100)
+    def _create_coordinator(self, attempt: int = 0) -> None:
+        """Create a fresh coordinator actor."""
+        # max_concurrency allows workers to call pull_task/report_result
+        # while run_pipeline blocks.
+        # max_restarts=0: coordinator must NOT auto-restart on preemption.
+        # A restarted coordinator only re-runs __init__, not initialize() or
+        # run_pipeline(), so it becomes a zombie. Instead, let it die and let
+        # the execute() retry loop tear down everything and start fresh.
+        logger.info("Starting coordinator for %s (pipeline %d, attempt %d)", self.name, self._pipeline_id, attempt)
+        coordinator_resources = ResourceConfig(cpu=1, ram="2g", max_concurrency=200, max_restarts=0)
         self._coordinator_group = self.client.create_actor_group(
             ZephyrCoordinator,
-            name=f"zephyr-{self.name}-coord",
+            name=f"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord",
             count=1,
             resources=coordinator_resources,
         )
@@ -1048,27 +1132,28 @@ class ZephyrContext:
 
         logger.info("Coordinator initialized for %s", self.name)
 
-    def _ensure_workers(self, num_shards: int) -> None:
-        """Create workers if not already initialized, sized to demand.
+    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:
+        """Create a fresh worker pool sized to demand.
 
         The worker count is min(max_workers, num_shards) to avoid
         over-provisioning when there are fewer shards than the cap.
         """
-        if self._worker_group is not None or num_shards == 0:
+        if num_shards == 0:
             return
 
         assert self.max_workers is not None  # set by __post_init__
         actual_workers = min(self.max_workers, num_shards)
         logger.info(
-            "Starting worker group: %d workers (max_workers=%d, num_shards=%d)",
+            "Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)",
             actual_workers,
             self.max_workers,
             num_shards,
+            attempt,
         )
         self._worker_group = self.client.create_actor_group(
             ZephyrWorker,
             self._coordinator,  # Pass coordinator handle as init arg
-            name=f"zephyr-{self.name}-workers",
+            name=f"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers",
             count=actual_workers,
             resources=self.resources,
         )
@@ -1097,8 +1182,6 @@ class ZephyrContext:
         self._worker_group = None
 
     def __enter__(self) -> ZephyrContext:
-        # Eagerly initialize coordinator; workers are deferred to first execute()
-        self._ensure_coordinator()
         return self
 
     def __exit__(self, *exc) -> None:
diff --git c/lib/zephyr/tests/test_execution.py w/lib/zephyr/tests/test_execution.py
index 2ac8e7f60..bc851c6cd 100644
--- c/lib/zephyr/tests/test_execution.py
+++ w/lib/zephyr/tests/test_execution.py
@@ -344,8 +344,8 @@ def test_chunk_streaming_low_memory(tmp_path):
     assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]
 
 
-def test_workers_persist_across_executes(fray_client, tmp_path):
-    """Workers persist across multiple execute() calls within a context."""
+def test_fresh_actors_per_execute(fray_client, tmp_path):
+    """Each execute() creates and tears down its own coordinator and workers."""
     chunk_prefix = str(tmp_path / "chunks")
 
     with ZephyrContext(
@@ -359,18 +359,19 @@ def test_workers_persist_across_executes(fray_client, tmp_path):
         results = list(zctx.execute(ds))
         assert sorted(results) == [2, 3, 4]
 
-        # After execute(): coordinator still exists (workers persist)
-        assert zctx._coordinator is not None
-        assert zctx._coordinator_group is not None
+        # After execute(): everything is torn down
+        assert zctx._coordinator is None
+        assert zctx._worker_group is None
+        assert zctx._pipeline_id == 1
 
-        # Can execute again (reuses same workers)
+        # Can execute again (creates fresh coordinator + workers)
         ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)
         results2 = list(zctx.execute(ds2))
         assert sorted(results2) == [20, 40]
 
-    # After context exit: all resources are cleaned up
-    assert zctx._coordinator is None
-    assert zctx._coordinator_group is None
+        assert zctx._coordinator is None
+        assert zctx._worker_group is None
+        assert zctx._pipeline_id == 2
 
 
 def test_fatal_errors_fail_fast(fray_client, tmp_path):
@@ -438,16 +439,15 @@ def test_workers_capped_to_shard_count(fray_client, tmp_path):
         chunk_storage_prefix=str(tmp_path / "chunks"),
         name=f"test-execution-{uuid.uuid4().hex[:8]}",
     ) as ctx:
-        # Workers not yet created before execute()
-        assert ctx._worker_group is None
         results = list(ctx.execute(ds.map(lambda x: x * 2)))
         assert sorted(results) == [2, 4, 6]
-        # Worker group sized to min(10, 3) = 3
-        assert ctx._worker_count == 3
+        # Everything torn down after execute; correct results prove workers
+        # were created and sized properly (min(10, 3) = 3)
+        assert ctx._pipeline_id == 1
 
 
-def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):
-    """Second execute() reuses the same worker group object (no re-creation)."""
+def test_pipeline_id_increments(fray_client, tmp_path):
+    """Pipeline ID increments after each execute(), ensuring unique actor names."""
     with ZephyrContext(
         client=fray_client,
         max_workers=10,
@@ -456,9 +456,65 @@ def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):
         name=f"test-execution-{uuid.uuid4().hex[:8]}",
     ) as ctx:
         ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))
-        group_after_first = ctx._worker_group
+        assert ctx._pipeline_id == 1
+
         ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))
-        assert ctx._worker_group is group_after_first
+        assert ctx._pipeline_id == 2
+
+
+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):
+    """When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN."""
+    from unittest.mock import MagicMock
+
+    from fray.v2.actor import ActorContext, _set_current_actor, _reset_current_actor
+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator
+
+    token = _set_current_actor(ActorContext(handle=MagicMock(), index=0, group_name="test-coord"))
+    try:
+        coord = ZephyrCoordinator()
+    finally:
+        _reset_current_actor(token)
+    coord.set_chunk_config(str(tmp_path / "chunks"), "test-exec")
+    coord.set_shared_data({})
+
+    task = ShardTask(
+        shard_idx=0,
+        total_shards=1,
+        chunk_size=100,
+        shard=Shard(chunks=[]),
+        operations=[],
+        stage_name="test",
+    )
+
+    # Non-last stage: empty queue returns None
+    coord.start_stage("stage-0", [task], is_last_stage=False)
+    pulled = coord.pull_task("worker-A")
+    assert pulled is not None and pulled != "SHUTDOWN"
+    _task, attempt, _config = pulled
+    coord.report_result("worker-A", 0, attempt, TaskResult(chunks=[]))
+
+    # Queue empty, but not last stage -> None
+    result = coord.pull_task("worker-A")
+    assert result is None
+
+    # Last stage: empty queue returns SHUTDOWN
+    task2 = ShardTask(
+        shard_idx=0,
+        total_shards=1,
+        chunk_size=100,
+        shard=Shard(chunks=[]),
+        operations=[],
+        stage_name="test-last",
+    )
+    coord.start_stage("stage-1", [task2], is_last_stage=True)
+    pulled = coord.pull_task("worker-A")
+    assert pulled is not None and pulled != "SHUTDOWN"
+    _task, attempt, _config = pulled
+    coord.report_result("worker-A", 0, attempt, TaskResult(chunks=[]))
+
+    # Queue empty on last stage -> SHUTDOWN
+    result = coord.pull_task("worker-A")
+    assert result == "SHUTDOWN"
 
 
 def test_execute_retries_on_coordinator_death(tmp_path):
@@ -482,26 +538,32 @@ def test_execute_retries_on_coordinator_death(tmp_path):
         name=f"test-execution-{uuid.uuid4().hex[:8]}",
     )
 
-    # First execute() succeeds normally — establishes coordinator + workers
-    ctx._ensure_coordinator()
-    coord_endpoint = ctx._coordinator._endpoint
-
+    # First execute() succeeds normally
     results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))
     assert sorted(results) == [2, 4, 6]
 
-    # Kill the coordinator by removing it from the local actor registry.
-    # This simulates VM preemption — the handle becomes stale and any RPC
-    # through it will raise RuntimeError("Actor not found in registry").
-    _local_actor_registry.pop(coord_endpoint, None)
+    # Sabotage the registry so the *next* coordinator creation attempt fails
+    # on the first try. We do this by patching create_actor_group to fail once.
+    original_create = client.create_actor_group
+    fail_count = [0]
+
+    def flaky_create(*args, **kwargs):
+        group = original_create(*args, **kwargs)
+        if fail_count[0] == 0 and "coord" in kwargs.get("name", ""):
+            fail_count[0] += 1
+            # Kill the coordinator immediately after creation to simulate death
+            handles = group.wait_ready()
+            endpoint = handles[0]._endpoint
+            _local_actor_registry.pop(endpoint, None)
+        return group
+
+    client.create_actor_group = flaky_create
 
     # Next execute() should: fail on attempt 0 (dead coordinator),
-    # shutdown + recreate everything, then succeed on attempt 1.
+    # then succeed on attempt 1 with a fresh coordinator.
     results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))
     assert sorted(results) == [11, 21]
 
-    # Verify coordinator was recreated (handle is not None after retry)
-    assert ctx._coordinator is not None
-
     ctx.shutdown()
     client.shutdown(wait=True)
 
diff --git c/tests/execution/test_disk_cache.py w/tests/execution/test_disk_cache.py
index d9b4b6274..d25d1b036 100644
--- c/tests/execution/test_disk_cache.py
+++ w/tests/execution/test_disk_cache.py
@@ -6,7 +6,7 @@ import os
 from pathlib import Path
 
 from marin.execution.artifact import Artifact
-from marin.execution.disk_cache import disk_cached
+from marin.execution.disk_cache import disk_cache
 from marin.execution.distributed_lock import distributed_lock
 from marin.execution.executor_step_status import STATUS_SUCCESS, StatusFile
 from marin.execution.step_spec import StepSpec
@@ -36,12 +36,14 @@ def test_disk_cached_runs_and_caches(tmp_path: Path):
     fn, get_count = _make_fn()
     output_path = StepSpec(name="step", output_path_prefix=tmp_path.as_posix()).output_path
 
-    result1 = disk_cached(fn, output_path)
+    cached_fn = disk_cache(fn, output_path=output_path)
+
+    result1 = cached_fn(output_path)
     assert get_count() == 1
     assert result1 == {"value": 42, "computed": True}
 
     # Cache hit: fn is called to load but no recomputation
-    result2 = disk_cached(fn, output_path)
+    result2 = cached_fn(output_path)
     assert get_count() == 1
     assert result2 == result1
 
@@ -56,7 +58,8 @@ def test_disk_cached_skips_when_another_worker_completed(tmp_path: Path):
         json.dump({"value": 99, "from_other": True}, f)
     StatusFile(spec.output_path, "other-worker").write_status(STATUS_SUCCESS)
 
-    result = disk_cached(fn, spec.output_path)
+    cached_fn = disk_cache(fn, output_path=spec.output_path)
+    result = cached_fn(spec.output_path)
 
     assert get_count() == 0
     assert result == {"value": 99, "from_other": True}
@@ -74,20 +77,17 @@ def test_composition_with_save_load(tmp_path: Path):
 
     output_path = StepSpec(name="comp", output_path_prefix=tmp_path.as_posix()).output_path
 
-    result1 = disk_cached(
+    cached_fn = disk_cache(
         distributed_lock(counting_fn),
-        output_path,
-        save=Artifact.save,
-        load=Artifact.load,
+        output_path=output_path,
+        save_fn=Artifact.save,
+        load_fn=Artifact.load,
     )
+
+    result1 = cached_fn(output_path)
     assert call_count == 1
     assert result1 == {"value": 42}
 
-    result2 = disk_cached(
-        distributed_lock(counting_fn),
-        output_path,
-        save=Artifact.save,
-        load=Artifact.load,
-    )
+    result2 = cached_fn(output_path)
     assert call_count == 1
     assert result2 == result1

```

extract the parts that a related to zephyr worker pool worker shutdown when it's the last stage and there's no work left in the queue to release resources

---

yes