{"type":"file-history-snapshot","messageId":"cc436027-b77a-4d55-a974-63b2251a1c92","snapshot":{"messageId":"cc436027-b77a-4d55-a974-63b2251a1c92","trackedFileBackups":{},"timestamp":"2026-02-23T23:45:57.678Z"},"isSnapshotUpdate":false}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"progress","data":{"type":"hook_progress","hookEvent":"SessionStart","hookName":"SessionStart:clear","command":"entire hooks claude-code session-start"},"parentToolUseID":"d2199ab1-e2b0-48ed-acec-38f5a7a59143","toolUseID":"d2199ab1-e2b0-48ed-acec-38f5a7a59143","timestamp":"2026-02-23T23:45:56.021Z","uuid":"13083a6d-0808-49af-891e-38b4fc5d3d8b"}
{"parentUuid":"13083a6d-0808-49af-891e-38b4fc5d3d8b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":"<local-command-caveat>Caveat: The messages below were generated by the user while running local commands. DO NOT respond to these messages or otherwise consider them in your response unless the user explicitly asks you to.</local-command-caveat>"},"isMeta":true,"uuid":"70b767ba-4488-45ca-b7f9-b062be1ebf02","timestamp":"2026-02-23T23:45:57.678Z"}
{"parentUuid":"70b767ba-4488-45ca-b7f9-b062be1ebf02","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":"<command-name>/clear</command-name>\n            <command-message>clear</command-message>\n            <command-args></command-args>"},"uuid":"cc436027-b77a-4d55-a974-63b2251a1c92","timestamp":"2026-02-23T23:45:55.195Z"}
{"parentUuid":"cc436027-b77a-4d55-a974-63b2251a1c92","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":"<local-command-stdout></local-command-stdout>"},"uuid":"0e15c332-070b-4b3c-848e-8c8ad99eabfa","timestamp":"2026-02-23T23:45:57.678Z"}
{"type":"file-history-snapshot","messageId":"cf7910b2-5d49-4872-9860-587dbf22b1ba","snapshot":{"messageId":"cf7910b2-5d49-4872-9860-587dbf22b1ba","trackedFileBackups":{},"timestamp":"2026-02-23T23:46:26.246Z"},"isSnapshotUpdate":false}
{"parentUuid":"0e15c332-070b-4b3c-848e-8c8ad99eabfa","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":"Take the change set below:\n\n\n```\ndiff --git c/experiments/pretraining_datasets/nemotron.py w/experiments/pretraining_datasets/nemotron.py\nindex 34099ec54..d3f4d1c27 100644\n--- c/experiments/pretraining_datasets/nemotron.py\n+++ w/experiments/pretraining_datasets/nemotron.py\n@@ -64,7 +64,11 @@ def _get_nemotron_split_paths(split: str):\n \n \n def tokenize_nemotron(\n-    *, tokenizer: str | None = None, window_size_bytes: int = 10_000_000_000\n+    *,\n+    tokenizer: str | None = None,\n+    window_size_bytes: int = 10_000_000_000,\n+    max_workers: int = 4096,\n+    writer_batch_size: int = 65536,\n ) -> dict[str, TokenizerStep]:\n     \"\"\"Generate tokenization steps for all Nemotron CC dataset splits.\"\"\"\n     if tokenizer is None:\n@@ -85,6 +89,8 @@ def tokenize_nemotron(\n                 cache_path=this_output_path(),\n                 tokenizer=versioned(tokenizer),\n                 window_size_bytes=window_size_bytes,\n+                max_workers=max_workers,\n+                writer_batch_size=writer_batch_size,\n             ),\n         )\n \ndiff --git c/lib/fray/src/fray/v2/ray_backend/backend.py w/lib/fray/src/fray/v2/ray_backend/backend.py\nindex 36a70ae13..a24e89a34 100644\n--- c/lib/fray/src/fray/v2/ray_backend/backend.py\n+++ w/lib/fray/src/fray/v2/ray_backend/backend.py\n@@ -409,6 +409,12 @@ def _actor_ray_options(resources: ResourceConfig) -> dict[str, Any]:\n         # node is preempted. Without this, the actor dies permanently and the\n         # pool degrades over time (see marin#2943).\n         options[\"max_restarts\"] = -1\n+    # Explicit max_restarts takes precedence over the preemptible default.\n+    # Useful for actors like coordinators that are preemptible (not pinned to\n+    # head node) but should NOT auto-restart because they require remote\n+    # initialization beyond __init__.\n+    if resources.max_restarts is not None:\n+        options[\"max_restarts\"] = resources.max_restarts\n     if resources.max_concurrency > 1:\n         options[\"max_concurrency\"] = resources.max_concurrency\n     return options\ndiff --git c/lib/fray/src/fray/v2/types.py w/lib/fray/src/fray/v2/types.py\nindex 7c9c99c7c..740495b5d 100644\n--- c/lib/fray/src/fray/v2/types.py\n+++ w/lib/fray/src/fray/v2/types.py\n@@ -322,6 +322,8 @@ class ResourceConfig:\n     regions: Sequence[str] | None = None\n     replicas: int = 1\n     max_concurrency: int = 1\n+    # TODO(rav): is this the right place for max_restarts?\n+    max_restarts: int | None = None\n \n     def chip_count(self) -> int:\n         \"\"\"Total accelerator chips across all replicas.\"\"\"\ndiff --git c/lib/fray/tests/test_v2_ray.py w/lib/fray/tests/test_v2_ray.py\nindex 08723da52..40a304c34 100644\n--- c/lib/fray/tests/test_v2_ray.py\n+++ w/lib/fray/tests/test_v2_ray.py\n@@ -271,3 +271,11 @@ def test_actor_options_preemptible_sets_max_restarts():\n     options = _actor_ray_options(ResourceConfig(preemptible=True))\n     assert options[\"max_restarts\"] == -1\n     assert \"resources\" not in options\n+\n+\n+def test_actor_options_explicit_max_restarts_overrides_preemptible():\n+    from fray.v2.ray_backend.backend import _actor_ray_options\n+\n+    options = _actor_ray_options(ResourceConfig(preemptible=True, max_restarts=0))\n+    assert options[\"max_restarts\"] == 0\n+    assert \"resources\" not in options\ndiff --git c/lib/marin/src/marin/execution/disk_cache.py w/lib/marin/src/marin/execution/disk_cache.py\nindex ae8206068..5735b67ce 100644\n--- c/lib/marin/src/marin/execution/disk_cache.py\n+++ w/lib/marin/src/marin/execution/disk_cache.py\n@@ -4,69 +4,124 @@\n \"\"\"Run a function once, cache the result, and return it on subsequent calls.\"\"\"\n \n from __future__ import annotations\n+import os\n \n+import functools\n import logging\n from collections.abc import Callable\n-from typing import TypeVar\n+from typing import TypeVar, Generic\n+\n+import cloudpickle\n+import fsspec\n+from iris.temp_buckets import get_temp_bucket_path\n \n from marin.execution.distributed_lock import StepAlreadyDone\n-from marin.execution.executor_step_status import (\n-    STATUS_FAILED,\n-    STATUS_SUCCESS,\n-    StatusFile,\n-    worker_id,\n-)\n+import hashlib\n+import pickle\n \n logger = logging.getLogger(__name__)\n \n T = TypeVar(\"T\")\n \n \n-def disk_cached(\n-    fn: Callable[[str], T],\n-    output_path: str,\n-    *,\n-    save: Callable[[T, str], None] | None = None,\n-    load: Callable[[str], T] | None = None,\n-) -> T | None:\n-    \"\"\"Run *fn* once and track completion via a status file.\n+class disk_cache(Generic[T]):\n+    \"\"\"Decorator that caches function results to disk via a status file.\n \n-    When *save* and *load* are provided, the result of *fn* is persisted and\n-    deserialized on cache hits.  When they are ``None`` (the default), *fn* is\n-    expected to handle its own reading/writing at *output_path* and is called\n-    again on cache hits to load the result.\n+    Supports bare decoration, decoration with arguments, and direct wrapping::\n \n-    By itself this function does not provide any locking guarantees, e.g. if\n-    you run disk_cached on multiple workers/shards, there may be a race\n-    condition where multiple workers evaluate the function and write to the same\n-    output path at the same time. To avoid this, compose with\n-    :func:`distributed_lock` to ensure that only one worker executes the\n-    function while the others wait for the result. For example::\n+        @disk_cached\n+        def my_fn(*args):\n+            ...\n \n-        result = disk_cached(\n-            distributed_lock(get_tokenizer),\n-            output_path,\n-            save=Artifact.save,\n-            load=lambda p: Artifact.load(p, TokenizerInfo),\n-        )\n+        @disk_cached(output_path=\"gs://...\", save_fn=save, load_fn=load)\n+        def my_fn(*args):\n+            ...\n+\n+        cached_fn = disk_cached(fn, output_path=path, save_fn=save, load_fn=load)\n+        result = cached_fn(output_path)\n+\n+    When *save_fn* and *load_fn* are provided, the result is persisted and\n+    deserialized on cache hits.  When they are ``None`` (the default),\n+    cloudpickle is used.\n+\n+    The decorated function is called with its original arguments; the\n+    *output_path* is used only for cache bookkeeping.\n+\n+    By itself this class does not provide any locking guarantees.  Compose with\n+    :func:`distributed_lock` to ensure single-writer semantics.\n     \"\"\"\n-    status_file = StatusFile(output_path, worker_id())\n-    if status_file.status == STATUS_SUCCESS:\n-        logger.info(f\"disk_cached: cache hit for {output_path}\")\n-        return load(output_path) if load is not None else fn(output_path)\n \n-    try:\n-        result = fn(output_path)\n-    except StepAlreadyDone:\n-        # NOTE: this is leaky but this branch handles the case of distributed lock wrapper\n-        logger.info(f\"disk_cached: completed by another worker for {output_path}\")\n-        return load(output_path) if load is not None else fn(output_path)\n-    except Exception:\n-        StatusFile(output_path, worker_id()).write_status(STATUS_FAILED)\n-        raise\n+    def __init__(\n+        self,\n+        fn: Callable[..., T] | None = None,\n+        *,\n+        output_path: str | None = None,\n+        save_fn: Callable[[T, str], None] | None = None,\n+        load_fn: Callable[[str], T] | None = None,\n+    ):\n+        self._output_path = output_path\n+        self._save_fn = save_fn\n+        self._load_fn = load_fn\n+        self._fn: Callable[..., T] | None = None\n \n-    if save is not None:\n-        save(result, output_path)\n-    StatusFile(output_path, worker_id()).write_status(STATUS_SUCCESS)\n-    logger.info(f\"disk_cached: computed and cached {output_path}\")\n-    return result\n+        if callable(fn):\n+            self._fn = fn\n+            functools.update_wrapper(self, fn)\n+\n+    # TODO: add ParamSpec\n+    def __call__(self, *args, **kwargs):\n+        if self._fn is None:\n+            # @disk_cached(...) — receiving the function to wrap\n+            if len(args) == 1 and callable(args[0]) and not kwargs:\n+                self._fn = args[0]  # type: ignore[bad-assignment]\n+                functools.update_wrapper(self, self._fn)\n+                return self\n+            raise TypeError(\"disk_cached() expected a callable\")\n+\n+        return self._execute(*args, **kwargs)\n+\n+    def _execute(self, *args, **kwargs):\n+        # Compute a fingerprint of the function arguments for cache keying.\n+\n+        def fingerprint_args(*args, **kwargs) -> str:\n+            \"\"\"Create a deterministic fingerprint for args and kwargs.\"\"\"\n+            # Include the function name in the fingerprint to avoid collisions across different functions.\n+            fn_name = getattr(self._fn, \"__name__\", None)\n+            data = pickle.dumps((fn_name, args, kwargs))\n+            return hashlib.sha256(data).hexdigest()[:8]\n+\n+        output_path = self._output_path\n+        if output_path is None:\n+            args_fingerprint = fingerprint_args(*args, **kwargs)\n+            output_path = get_temp_bucket_path(1, f\"disk_cache_{args_fingerprint}\")\n+            if output_path is None:\n+                output_path = os.environ[\"MARIN_PREFIX\"] + f\"/disk_cache_{args_fingerprint}\"\n+\n+        def load_result():\n+            assert output_path is not None\n+            if self._load_fn is not None:\n+                return self._load_fn(output_path)\n+            with fsspec.open(output_path + \"/data.pkl\", \"rb\") as f:\n+                return cloudpickle.loads(f.read())\n+\n+        try:\n+            return load_result()\n+        except FileNotFoundError:\n+            logger.info(f\"disk_cached: cache miss for {output_path}\")\n+\n+        assert self._fn is not None\n+        try:\n+            result = self._fn(*args, **kwargs)\n+        except StepAlreadyDone:\n+            # NOTE: this is leaky but this branch handles the case of distributed lock wrapper\n+            logger.info(f\"disk_cached: completed by another worker for {output_path}\")\n+            return load_result()\n+\n+        if self._save_fn is not None:\n+            self._save_fn(result, output_path)\n+        else:\n+            with fsspec.open(output_path + \"/data.pkl\", \"wb\") as f:\n+                f.write(cloudpickle.dumps(result))\n+\n+        logger.info(f\"disk_cached: computed and cached {output_path}\")\n+        return result\ndiff --git c/lib/marin/src/marin/execution/step_runner.py w/lib/marin/src/marin/execution/step_runner.py\nindex 8d5a5575a..c55de576e 100644\n--- c/lib/marin/src/marin/execution/step_runner.py\n+++ w/lib/marin/src/marin/execution/step_runner.py\n@@ -262,7 +262,7 @@ class StepRunner:\n     def _launch_step(self, step: StepSpec, *, force_run_failed: bool, dry_run: bool) -> JobHandle | None:\n         \"\"\"Launch a single step as a fray job. Returns None if skipped.\"\"\"\n         from marin.execution.artifact import Artifact\n-        from marin.execution.disk_cache import disk_cached\n+        from marin.execution.disk_cache import disk_cache\n         from marin.execution.distributed_lock import distributed_lock\n \n         output_path = step.output_path\n@@ -291,14 +291,15 @@ class StepRunner:\n         # All caching, locking, heartbeat, artifact saving, and status writes\n         # happen on the worker.\n         step_fn = step.fn\n+        cached_step = disk_cache(\n+            distributed_lock(step_fn, force_run_failed=force_run_failed),\n+            output_path=output_path,\n+            save_fn=Artifact.save,\n+            load_fn=Artifact.load,\n+        )\n \n         def worker_fn():\n-            disk_cached(\n-                distributed_lock(step_fn, force_run_failed=force_run_failed),\n-                output_path,\n-                save=Artifact.save,\n-                load=Artifact.load,\n-            )\n+            cached_step(output_path)\n \n         worker_fn.__qualname__ = step_name\n         worker_fn.__name__ = step_name\ndiff --git c/lib/marin/src/marin/processing/tokenize/tokenize.py w/lib/marin/src/marin/processing/tokenize/tokenize.py\nindex 3a78a164c..1f511079a 100644\n--- c/lib/marin/src/marin/processing/tokenize/tokenize.py\n+++ w/lib/marin/src/marin/processing/tokenize/tokenize.py\n@@ -7,6 +7,9 @@ Tokenize datasets using zephyr pipeline and write to Levanter cache format.\n Supports both regular file paths and HuggingFace datasets. For HF datasets, downloads\n them first then tokenizes the downloaded files.\n \"\"\"\n+from functools import cache\n+from marin.utilities.time_logger import log_time\n+from marin.execution.disk_cache import disk_cache\n \n import abc\n import dataclasses\n@@ -33,7 +36,7 @@ from levanter.data.text import (\n )\n from levanter.store.cache import consolidate_shard_caches\n from levanter.store.tree_store import TreeStore\n-from zephyr import Dataset, ZephyrContext, zephyr_worker_ctx\n+from zephyr import Dataset, ZephyrContext\n from zephyr.readers import load_file\n \n from marin.execution.executor import ExecutorStep, InputName, VersionedValue\n@@ -83,7 +86,7 @@ class TokenizeConfig(TokenizeConfigBase):\n     \"\"\"Files are bundled into groups up to this size; each group becomes one shard.\n     Smaller values produce more shards and thus more parallelism (up to max_workers).\"\"\"\n     max_workers: int = 4096\n-    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram=\"5g\", disk=\"5g\"))\n+    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram=\"10g\", disk=\"5g\"))\n     writer_batch_size: int = 65536\n     \"\"\"Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op\n     overhead. Too large a value increases memory usage and delays progress checkpointing.\"\"\"\n@@ -147,7 +150,7 @@ class HfTokenizeConfig(TokenizeConfigBase):\n     window_size_bytes: int = 10_000_000_000\n     \"\"\"Files are bundled into groups up to this size; each group becomes one shard.\n     Smaller values produce more shards and thus more parallelism (up to max_workers).\"\"\"\n-    max_workers: int = 4096\n+    max_workers: int = 512\n     worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram=\"5g\", disk=\"5g\"))\n     writer_batch_size: int = 65536\n     \"\"\"Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op\n@@ -255,7 +258,7 @@ def _bundle_files_by_size(file_infos, max_bytes: int):\n \n def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Iterator[Sequence[dict]]) -> Iterator[dict]:\n     \"\"\"Tokenize a list of batches using the specified tokenizer and format.\"\"\"\n-    tokenizer: transformers.PreTrainedTokenizer = zephyr_worker_ctx().get_shared(\"tokenizer\")\n+    tokenizer: transformers.PreTrainedTokenizer = get_tokenizer(config.tokenizer)\n     batch_processor = preprocessor_for_format(config.format, tokenizer)\n \n     batch_count = 0\n@@ -289,6 +292,12 @@ def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Ite\n     )\n \n \n+@cache\n+@disk_cache\n+def get_tokenizer(tokenizer: str) -> transformers.PreTrainedTokenizer:\n+    return transformers.AutoTokenizer.from_pretrained(tokenizer)\n+\n+\n def tokenize(config: TokenizeConfigBase):\n     \"\"\"Tokenize datasets using zephyr pipeline.\n \n@@ -376,8 +385,8 @@ def tokenize(config: TokenizeConfigBase):\n             )\n         )\n \n-        # Broadcast the tokenizer to all workers via ZephyrContext\n-        ctx.put(\"tokenizer\", transformers.AutoTokenizer.from_pretrained(config.tokenizer))\n+        with log_time(f\"{config.tokenizer} cache warmup\"):\n+            get_tokenizer(config.tokenizer)\n \n         tokenize_start = time.monotonic()\n         shard_paths = ctx.execute(temp_shards)\n@@ -422,7 +431,7 @@ def tokenize(config: TokenizeConfigBase):\n             resources=config.worker_resources,\n             max_workers=min(config.max_workers, len(train_groups)),\n             name=\"tokenize-train\",\n-            no_workers_timeout=20 * 60,\n+            no_workers_timeout=6 * 60 * 60,\n         ) as ctx:\n             run_pipeline(ctx, train_groups, \"train\")\n \n@@ -432,7 +441,7 @@ def tokenize(config: TokenizeConfigBase):\n             resources=config.worker_resources,\n             max_workers=min(config.max_workers, len(validation_groups)),\n             name=\"tokenize-validation\",\n-            no_workers_timeout=20 * 60,\n+            no_workers_timeout=6 * 60 * 60,\n         ) as ctx:\n             run_pipeline(ctx, validation_groups, \"validation\")\n \ndiff --git c/lib/zephyr/src/zephyr/execution.py w/lib/zephyr/src/zephyr/execution.py\nindex 87715feee..385585d9c 100644\n--- c/lib/zephyr/src/zephyr/execution.py\n+++ w/lib/zephyr/src/zephyr/execution.py\n@@ -10,6 +10,8 @@ protocol.\n \"\"\"\n \n from __future__ import annotations\n+from datetime import timedelta\n+import contextlib\n \n import enum\n import logging\n@@ -47,6 +49,14 @@ from zephyr.writers import ensure_parent_dir\n logger = logging.getLogger(__name__)\n \n \n+@contextlib.contextmanager\n+def log_time(label: str, level: int = logging.INFO) -> Iterator[None]:\n+    t_start = time.perf_counter()\n+    yield\n+    t_end = time.perf_counter()\n+    logger.log(level, f\"{label} took {timedelta(seconds=t_end - t_start)}\")\n+\n+\n class Chunk(Protocol):\n     def __iter__(self) -> Iterator: ...\n \n@@ -247,6 +257,8 @@ class ZephyrCoordinator:\n     \"\"\"\n \n     def __init__(self):\n+        from fray.v2 import current_actor\n+\n         # Task management state\n         self._task_queue: deque[ShardTask] = deque()\n         self._results: dict[int, TaskResult] = {}\n@@ -268,11 +280,15 @@ class ZephyrCoordinator:\n         self._worker_handles: dict[str, ActorHandle] = {}\n         self._coordinator_thread: threading.Thread | None = None\n         self._shutdown: bool = False\n+        self._is_last_stage: bool = False\n         self._initialized: bool = False\n \n         # Lock for accessing coordinator state from background thread\n         self._lock = threading.Lock()\n \n+        actor_ctx = current_actor()\n+        self._name = f\"{actor_ctx.group_name}\"\n+\n     def initialize(\n         self,\n         chunk_prefix: str,\n@@ -314,6 +330,10 @@ class ZephyrCoordinator:\n                 self._worker_handles[worker_id] = worker_handle\n                 self._worker_states[worker_id] = WorkerState.READY\n                 self._last_seen[worker_id] = time.monotonic()\n+                # NOTE: if there was a task assigned to the worker, there's a race condidtion between marking\n+                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n+                # lose tasks.\n+                self._maybe_requeue_worker_task(worker_id)\n                 return\n \n             self._worker_handles[worker_id] = worker_handle\n@@ -330,9 +350,9 @@ class ZephyrCoordinator:\n         last_log_time = 0.0\n \n         while not self._shutdown:\n-            # Check heartbeats, re-queue stale tasks\n-            with self._lock:\n-                self._check_worker_heartbeats()\n+            # Scheduler check heartbeats, re-queue stale tasks\n+            # This is called in a remote fashion to account for potential congestion.\n+            self._self_handle.check_heartbeats.remote()\n \n             # Log status periodically during active execution\n             now = time.monotonic()\n@@ -360,6 +380,16 @@ class ZephyrCoordinator:\n             dead,\n         )\n \n+    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n+        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n+        task_and_attempt = self._in_flight.pop(worker_id, None)\n+        if task_and_attempt is not None:\n+            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n+            task, _old_attempt = task_and_attempt\n+            self._task_attempts[task.shard_idx] += 1\n+            self._task_queue.append(task)\n+            self._retries += 1\n+\n     def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n         \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n         now = time.monotonic()\n@@ -367,13 +397,7 @@ class ZephyrCoordinator:\n             if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                 logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                 self._worker_states[worker_id] = WorkerState.FAILED\n-                task_and_attempt = self._in_flight.pop(worker_id, None)\n-                if task_and_attempt is not None:\n-                    logger.info(\"Removed task %s from worker %s, re-queueing\", task_and_attempt, worker_id)\n-                    task, _old_attempt = task_and_attempt\n-                    self._task_attempts[task.shard_idx] += 1\n-                    self._task_queue.append(task)\n-                    self._retries += 1\n+                self._maybe_requeue_worker_task(worker_id)\n \n     def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n         \"\"\"Called by workers to get next task.\n@@ -394,6 +418,8 @@ class ZephyrCoordinator:\n                 return None\n \n             if not self._task_queue:\n+                if self._is_last_stage:\n+                    return \"SHUTDOWN\"\n                 return None\n \n             task = self._task_queue.popleft()\n@@ -408,9 +434,25 @@ class ZephyrCoordinator:\n             }\n             return (task, attempt, config)\n \n+    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n+        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n+\n+        Workers block on report_result/report_error before calling pull_task,\n+        so _in_flight can never point to a different shard. It may be absent\n+        if a heartbeat timeout already re-queued the task.\n+        \"\"\"\n+        in_flight = self._in_flight.get(worker_id)\n+        if in_flight is not None:\n+            assert in_flight[0].shard_idx == shard_idx, (\n+                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n+                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n+                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n+            )\n+\n     def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n         with self._lock:\n             self._last_seen[worker_id] = time.monotonic()\n+            self._assert_in_flight_consistent(worker_id, shard_idx)\n \n             current_attempt = self._task_attempts.get(shard_idx, 0)\n             if attempt != current_attempt:\n@@ -429,13 +471,14 @@ class ZephyrCoordinator:\n         \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n         with self._lock:\n             self._last_seen[worker_id] = time.monotonic()\n+            self._assert_in_flight_consistent(worker_id, shard_idx)\n             self._in_flight.pop(worker_id, None)\n             self._fatal_error = error_info\n             self._worker_states[worker_id] = WorkerState.DEAD\n \n     def heartbeat(self, worker_id: str) -> None:\n-        with self._lock:\n-            self._last_seen[worker_id] = time.monotonic()\n+        # NOTE: there's no need for lock here\n+        self._last_seen[worker_id] = time.monotonic()\n \n     def get_status(self) -> JobStatus:\n         with self._lock:\n@@ -461,7 +504,7 @@ class ZephyrCoordinator:\n         with self._lock:\n             return self._fatal_error\n \n-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue.\"\"\"\n         with self._lock:\n             self._task_queue = deque(tasks)\n@@ -473,13 +516,14 @@ class ZephyrCoordinator:\n             self._in_flight = {}\n             self._task_attempts = {task.shard_idx: 0 for task in tasks}\n             self._fatal_error = None\n+            self._is_last_stage = is_last_stage\n \n     def _wait_for_stage(self) -> None:\n         \"\"\"Block until current stage completes or error occurs.\"\"\"\n         backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n         last_log_completed = -1\n         start_time = time.monotonic()\n-        warned_no_workers = False\n+        all_dead_since: float | None = None\n         no_workers_timeout = self._no_workers_timeout\n \n         while True:\n@@ -487,27 +531,37 @@ class ZephyrCoordinator:\n                 if self._fatal_error:\n                     raise ZephyrWorkerError(self._fatal_error)\n \n-                num_workers = len(self._worker_handles)\n                 completed = self._completed_shards\n                 total = self._total_shards\n \n                 if completed >= total:\n                     return\n \n-                # Fail fast if no workers appear within timeout\n-                if num_workers == 0:\n-                    elapsed = time.monotonic() - start_time\n-                    if elapsed > no_workers_timeout:\n+                # Count alive workers (READY or BUSY), not just total registered.\n+                # Dead/failed workers stay in _worker_handles but can't make progress.\n+                alive_workers = sum(\n+                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n+                )\n+\n+                if alive_workers == 0:\n+                    now = time.monotonic()\n+                    elapsed = now - start_time\n+\n+                    if all_dead_since is None:\n+                        all_dead_since = now\n+                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n+\n+                    dead_duration = now - all_dead_since\n+                    if dead_duration > no_workers_timeout:\n                         raise ZephyrWorkerError(\n-                            f\"No workers available after {elapsed:.1f}s. \"\n+                            f\"No alive workers for {dead_duration:.1f}s \"\n+                            f\"(total elapsed {elapsed:.1f}s). \"\n+                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                             \"Check cluster resources and worker group configuration.\"\n                         )\n-                    if not warned_no_workers and elapsed > 5.0:\n-                        logger.warning(\n-                            \"No workers available yet after %.1fs, waiting for discovery...\",\n-                            elapsed,\n-                        )\n-                        warned_no_workers = True\n+                else:\n+                    # Workers are alive — reset the dead timer\n+                    all_dead_since = None\n \n             if completed != last_log_completed:\n                 logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n@@ -521,6 +575,7 @@ class ZephyrCoordinator:\n         with self._lock:\n             return dict(self._results)\n \n+    # TODO: this is not used anywhere!?\n     def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n         \"\"\"Set config for the current execution.\"\"\"\n         with self._lock:\n@@ -543,6 +598,15 @@ class ZephyrCoordinator:\n         if not shards:\n             return []\n \n+        # Identify the last stage that dispatches work to workers (non-RESHARD).\n+        # On that stage, idle workers receive SHUTDOWN once all tasks are\n+        # in-flight, so they exit eagerly instead of polling until\n+        # coordinator.shutdown().\n+        last_worker_stage_idx = max(\n+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n+            default=-1,\n+        )\n+\n         for stage_idx, stage in enumerate(plan.stages):\n             stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n \n@@ -556,7 +620,7 @@ class ZephyrCoordinator:\n             # Build and submit tasks\n             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n             logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n-            self._start_stage(stage_label, tasks)\n+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))\n \n             # Wait for stage completion\n             self._wait_for_stage()\n@@ -570,6 +634,10 @@ class ZephyrCoordinator:\n         for shard in shards:\n             for chunk in shard.chunks:\n                 flat_result.extend(list(chunk))\n+\n+        # Signal workers to shut down now that all stages are complete.\n+        self.shutdown()\n+\n         return flat_result\n \n     def _compute_join_aux(\n@@ -649,9 +717,9 @@ class ZephyrCoordinator:\n                 \"execution_id\": self._execution_id,\n             }\n \n-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n-        self._start_stage(stage_name, tasks)\n+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)\n \n     def check_heartbeats(self, timeout: float = 30.0) -> None:\n         \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n@@ -666,6 +734,9 @@ class ZephyrCoordinator:\n         \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n         self._shutdown = True\n \n+    def __repr__(self) -> str:\n+        return f\"ZephyrCoordinator(name={self._name})\"\n+\n \n # ---------------------------------------------------------------------------\n # ZephyrWorker\n@@ -695,8 +766,8 @@ class ZephyrWorker:\n         actor_ctx = current_actor()\n         self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n \n-        # Register with coordinator\n-        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle)\n+        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n+        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n \n         # Start polling in a background thread\n         self._polling_thread = threading.Thread(\n@@ -735,7 +806,7 @@ class ZephyrWorker:\n         heartbeat_count = 0\n         while not self._shutdown_event.is_set():\n             try:\n-                coordinator.heartbeat.remote(self._worker_id)\n+                coordinator.heartbeat.remote(self._worker_id).result()\n                 heartbeat_count += 1\n                 if heartbeat_count % 10 == 1:\n                     logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n@@ -779,9 +850,12 @@ class ZephyrWorker:\n \n             logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n             try:\n-                result = self._execute_shard(task, config)\n-                logger.info(\"[%s] Task complete, reporting result for shard %d\", self._worker_id, task.shard_idx)\n-                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result)\n+                with log_time(f\"Worker {self._worker_id} executing shard {task.shard_idx}\"):\n+                    result = self._execute_shard(task, config)\n+                # Block until coordinator records the result. This ensures\n+                # report_result is fully processed before the next pull_task,\n+                # preventing _in_flight tracking races.\n+                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                 task_count += 1\n             except Exception as e:\n                 logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n@@ -791,7 +865,7 @@ class ZephyrWorker:\n                     self._worker_id,\n                     task.shard_idx,\n                     \"\".join(traceback.format_exc()),\n-                )\n+                ).result()\n \n     def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n         \"\"\"Execute a stage's operations on a single shard.\n@@ -860,6 +934,9 @@ class ZephyrWorker:\n         if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n             self._polling_thread.join(timeout=5.0)\n \n+    def __repr__(self) -> str:\n+        return f\"ZephyrWorker(id={self._worker_id})\"\n+\n \n def _regroup_result_refs(\n     result_refs: dict[int, TaskResult],\n@@ -883,12 +960,10 @@ def _regroup_result_refs(\n class ZephyrContext:\n     \"\"\"Execution context for Zephyr pipelines.\n \n-    Creates a coordinator actor on __enter__ which owns and manages the worker\n-    pool. Workers are created lazily on the first execute() call, sized to\n-    min(max_workers, plan.num_shards) to avoid over-provisioning. Workers\n-    persist across pipeline stages and execute() calls, allowing cached state\n-    (tokenizers, models) to be reused. Shared data broadcast via put() is\n-    delivered to workers with each task.\n+    Each execute() call creates a fresh coordinator and worker pool, runs\n+    the pipeline, then tears everything down. Workers are sized to\n+    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n+    data broadcast via put() is delivered to workers with each task.\n \n     Args:\n         client: The fray client to use. If None, auto-detects using current_client().\n@@ -920,6 +995,8 @@ class ZephyrContext:\n     _coordinator_group: Any = field(default=None, repr=False)\n     _worker_group: Any = field(default=None, repr=False)\n     _worker_count: int = field(default=0, repr=False)\n+    # NOTE: execute calls increment this at the very beginning\n+    _pipeline_id: int = field(default=-1, repr=False)\n \n     def __post_init__(self):\n         if self.client is None:\n@@ -971,29 +1048,36 @@ class ZephyrContext:\n         verbose: bool = False,\n         dry_run: bool = False,\n     ) -> Sequence:\n-        \"\"\"Execute a dataset pipeline on the worker pool.\n+        \"\"\"Execute a dataset pipeline.\n \n-        Workers persist across execute() calls, so cached state (tokenizers,\n-        models) will be reused. If the coordinator dies mid-execution (e.g.,\n-        VM preemption), the pipeline is retried with a fresh coordinator and\n-        worker pool up to ``max_execution_retries`` times. Application errors\n-        (``ZephyrWorkerError``) are never retried.\n+        Each call creates a fresh coordinator and worker pool, runs the\n+        pipeline, then tears everything down. If the coordinator dies\n+        mid-execution (e.g., VM preemption), the pipeline is retried\n+        with fresh actors up to ``max_execution_retries`` times.\n+        Application errors (``ZephyrWorkerError``) are never retried.\n         \"\"\"\n         plan = compute_plan(dataset, hints)\n         if dry_run:\n             _print_plan(dataset.operations, plan)\n             return []\n \n+        # NOTE: pipeline ID incremented on clean completion only\n+        self._pipeline_id += 1\n         last_exception: Exception | None = None\n         for attempt in range(self.max_execution_retries + 1):\n             execution_id = _generate_execution_id()\n-            logger.info(\"Starting zephyr pipeline: %s (attempt %d)\", execution_id, attempt)\n+            logger.info(\n+                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n+            )\n \n             try:\n-                self._ensure_coordinator()\n-                self._ensure_workers(plan.num_shards)\n+                self._create_coordinator(attempt)\n+                self._create_workers(plan.num_shards, attempt)\n \n-                # Run pipeline on coordinator (blocking call)\n+                # Run pipeline on coordinator (blocking call).\n+                # run_pipeline() calls coordinator.shutdown() at the end,\n+                # which causes workers to receive SHUTDOWN on their next\n+                # pull_task() call.\n                 results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n \n                 return results\n@@ -1007,34 +1091,34 @@ class ZephyrContext:\n                     raise\n \n                 logger.warning(\n-                    \"Pipeline attempt %d/%d failed, retrying with fresh coordinator: %s\",\n-                    attempt + 1,\n-                    self.max_execution_retries,\n+                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n+                    attempt,\n+                    self.max_execution_retries - attempt,\n                     e,\n                 )\n-                # Tear down dead coordinator and orphaned workers so\n-                # _ensure_coordinator / _ensure_workers create fresh ones.\n-                self.shutdown()\n \n             finally:\n-                # Clean up chunks for this execution only (workers persist)\n+                # Tear down coordinator and workers for this pipeline\n+                self.shutdown()\n+                # Clean up chunks for this execution\n                 _cleanup_execution(self.chunk_storage_prefix, execution_id)\n \n         # Should be unreachable, but just in case\n         raise last_exception  # type: ignore[misc]\n \n-    def _ensure_coordinator(self) -> None:\n-        \"\"\"Create coordinator if not already initialized.\"\"\"\n-        if self._coordinator is not None:\n-            return\n-\n-        # Create coordinator actor with high max_concurrency to allow\n-        # workers to call pull_task/report_result while run_pipeline blocks\n-        logger.info(\"Starting coordinator for %s\", self.name)\n-        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\", max_concurrency=100)\n+    def _create_coordinator(self, attempt: int = 0) -> None:\n+        \"\"\"Create a fresh coordinator actor.\"\"\"\n+        # max_concurrency allows workers to call pull_task/report_result\n+        # while run_pipeline blocks.\n+        # max_restarts=0: coordinator must NOT auto-restart on preemption.\n+        # A restarted coordinator only re-runs __init__, not initialize() or\n+        # run_pipeline(), so it becomes a zombie. Instead, let it die and let\n+        # the execute() retry loop tear down everything and start fresh.\n+        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n+        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\", max_concurrency=200, max_restarts=0)\n         self._coordinator_group = self.client.create_actor_group(\n             ZephyrCoordinator,\n-            name=f\"zephyr-{self.name}-coord\",\n+            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n             count=1,\n             resources=coordinator_resources,\n         )\n@@ -1048,27 +1132,28 @@ class ZephyrContext:\n \n         logger.info(\"Coordinator initialized for %s\", self.name)\n \n-    def _ensure_workers(self, num_shards: int) -> None:\n-        \"\"\"Create workers if not already initialized, sized to demand.\n+    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n+        \"\"\"Create a fresh worker pool sized to demand.\n \n         The worker count is min(max_workers, num_shards) to avoid\n         over-provisioning when there are fewer shards than the cap.\n         \"\"\"\n-        if self._worker_group is not None or num_shards == 0:\n+        if num_shards == 0:\n             return\n \n         assert self.max_workers is not None  # set by __post_init__\n         actual_workers = min(self.max_workers, num_shards)\n         logger.info(\n-            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d)\",\n+            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n             actual_workers,\n             self.max_workers,\n             num_shards,\n+            attempt,\n         )\n         self._worker_group = self.client.create_actor_group(\n             ZephyrWorker,\n             self._coordinator,  # Pass coordinator handle as init arg\n-            name=f\"zephyr-{self.name}-workers\",\n+            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n             count=actual_workers,\n             resources=self.resources,\n         )\n@@ -1097,8 +1182,6 @@ class ZephyrContext:\n         self._worker_group = None\n \n     def __enter__(self) -> ZephyrContext:\n-        # Eagerly initialize coordinator; workers are deferred to first execute()\n-        self._ensure_coordinator()\n         return self\n \n     def __exit__(self, *exc) -> None:\ndiff --git c/lib/zephyr/tests/test_execution.py w/lib/zephyr/tests/test_execution.py\nindex 2ac8e7f60..bc851c6cd 100644\n--- c/lib/zephyr/tests/test_execution.py\n+++ w/lib/zephyr/tests/test_execution.py\n@@ -344,8 +344,8 @@ def test_chunk_streaming_low_memory(tmp_path):\n     assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n \n \n-def test_workers_persist_across_executes(fray_client, tmp_path):\n-    \"\"\"Workers persist across multiple execute() calls within a context.\"\"\"\n+def test_fresh_actors_per_execute(fray_client, tmp_path):\n+    \"\"\"Each execute() creates and tears down its own coordinator and workers.\"\"\"\n     chunk_prefix = str(tmp_path / \"chunks\")\n \n     with ZephyrContext(\n@@ -359,18 +359,19 @@ def test_workers_persist_across_executes(fray_client, tmp_path):\n         results = list(zctx.execute(ds))\n         assert sorted(results) == [2, 3, 4]\n \n-        # After execute(): coordinator still exists (workers persist)\n-        assert zctx._coordinator is not None\n-        assert zctx._coordinator_group is not None\n+        # After execute(): everything is torn down\n+        assert zctx._coordinator is None\n+        assert zctx._worker_group is None\n+        assert zctx._pipeline_id == 1\n \n-        # Can execute again (reuses same workers)\n+        # Can execute again (creates fresh coordinator + workers)\n         ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)\n         results2 = list(zctx.execute(ds2))\n         assert sorted(results2) == [20, 40]\n \n-    # After context exit: all resources are cleaned up\n-    assert zctx._coordinator is None\n-    assert zctx._coordinator_group is None\n+        assert zctx._coordinator is None\n+        assert zctx._worker_group is None\n+        assert zctx._pipeline_id == 2\n \n \n def test_fatal_errors_fail_fast(fray_client, tmp_path):\n@@ -438,16 +439,15 @@ def test_workers_capped_to_shard_count(fray_client, tmp_path):\n         chunk_storage_prefix=str(tmp_path / \"chunks\"),\n         name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n     ) as ctx:\n-        # Workers not yet created before execute()\n-        assert ctx._worker_group is None\n         results = list(ctx.execute(ds.map(lambda x: x * 2)))\n         assert sorted(results) == [2, 4, 6]\n-        # Worker group sized to min(10, 3) = 3\n-        assert ctx._worker_count == 3\n+        # Everything torn down after execute; correct results prove workers\n+        # were created and sized properly (min(10, 3) = 3)\n+        assert ctx._pipeline_id == 1\n \n \n-def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):\n-    \"\"\"Second execute() reuses the same worker group object (no re-creation).\"\"\"\n+def test_pipeline_id_increments(fray_client, tmp_path):\n+    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n     with ZephyrContext(\n         client=fray_client,\n         max_workers=10,\n@@ -456,9 +456,65 @@ def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):\n         name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n     ) as ctx:\n         ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n-        group_after_first = ctx._worker_group\n+        assert ctx._pipeline_id == 1\n+\n         ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n-        assert ctx._worker_group is group_after_first\n+        assert ctx._pipeline_id == 2\n+\n+\n+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):\n+    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"\n+    from unittest.mock import MagicMock\n+\n+    from fray.v2.actor import ActorContext, _set_current_actor, _reset_current_actor\n+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n+\n+    token = _set_current_actor(ActorContext(handle=MagicMock(), index=0, group_name=\"test-coord\"))\n+    try:\n+        coord = ZephyrCoordinator()\n+    finally:\n+        _reset_current_actor(token)\n+    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n+    coord.set_shared_data({})\n+\n+    task = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test\",\n+    )\n+\n+    # Non-last stage: empty queue returns None\n+    coord.start_stage(\"stage-0\", [task], is_last_stage=False)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty, but not last stage -> None\n+    result = coord.pull_task(\"worker-A\")\n+    assert result is None\n+\n+    # Last stage: empty queue returns SHUTDOWN\n+    task2 = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test-last\",\n+    )\n+    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty on last stage -> SHUTDOWN\n+    result = coord.pull_task(\"worker-A\")\n+    assert result == \"SHUTDOWN\"\n \n \n def test_execute_retries_on_coordinator_death(tmp_path):\n@@ -482,26 +538,32 @@ def test_execute_retries_on_coordinator_death(tmp_path):\n         name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n     )\n \n-    # First execute() succeeds normally — establishes coordinator + workers\n-    ctx._ensure_coordinator()\n-    coord_endpoint = ctx._coordinator._endpoint\n-\n+    # First execute() succeeds normally\n     results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))\n     assert sorted(results) == [2, 4, 6]\n \n-    # Kill the coordinator by removing it from the local actor registry.\n-    # This simulates VM preemption — the handle becomes stale and any RPC\n-    # through it will raise RuntimeError(\"Actor not found in registry\").\n-    _local_actor_registry.pop(coord_endpoint, None)\n+    # Sabotage the registry so the *next* coordinator creation attempt fails\n+    # on the first try. We do this by patching create_actor_group to fail once.\n+    original_create = client.create_actor_group\n+    fail_count = [0]\n+\n+    def flaky_create(*args, **kwargs):\n+        group = original_create(*args, **kwargs)\n+        if fail_count[0] == 0 and \"coord\" in kwargs.get(\"name\", \"\"):\n+            fail_count[0] += 1\n+            # Kill the coordinator immediately after creation to simulate death\n+            handles = group.wait_ready()\n+            endpoint = handles[0]._endpoint\n+            _local_actor_registry.pop(endpoint, None)\n+        return group\n+\n+    client.create_actor_group = flaky_create\n \n     # Next execute() should: fail on attempt 0 (dead coordinator),\n-    # shutdown + recreate everything, then succeed on attempt 1.\n+    # then succeed on attempt 1 with a fresh coordinator.\n     results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))\n     assert sorted(results) == [11, 21]\n \n-    # Verify coordinator was recreated (handle is not None after retry)\n-    assert ctx._coordinator is not None\n-\n     ctx.shutdown()\n     client.shutdown(wait=True)\n \ndiff --git c/tests/execution/test_disk_cache.py w/tests/execution/test_disk_cache.py\nindex d9b4b6274..d25d1b036 100644\n--- c/tests/execution/test_disk_cache.py\n+++ w/tests/execution/test_disk_cache.py\n@@ -6,7 +6,7 @@ import os\n from pathlib import Path\n \n from marin.execution.artifact import Artifact\n-from marin.execution.disk_cache import disk_cached\n+from marin.execution.disk_cache import disk_cache\n from marin.execution.distributed_lock import distributed_lock\n from marin.execution.executor_step_status import STATUS_SUCCESS, StatusFile\n from marin.execution.step_spec import StepSpec\n@@ -36,12 +36,14 @@ def test_disk_cached_runs_and_caches(tmp_path: Path):\n     fn, get_count = _make_fn()\n     output_path = StepSpec(name=\"step\", output_path_prefix=tmp_path.as_posix()).output_path\n \n-    result1 = disk_cached(fn, output_path)\n+    cached_fn = disk_cache(fn, output_path=output_path)\n+\n+    result1 = cached_fn(output_path)\n     assert get_count() == 1\n     assert result1 == {\"value\": 42, \"computed\": True}\n \n     # Cache hit: fn is called to load but no recomputation\n-    result2 = disk_cached(fn, output_path)\n+    result2 = cached_fn(output_path)\n     assert get_count() == 1\n     assert result2 == result1\n \n@@ -56,7 +58,8 @@ def test_disk_cached_skips_when_another_worker_completed(tmp_path: Path):\n         json.dump({\"value\": 99, \"from_other\": True}, f)\n     StatusFile(spec.output_path, \"other-worker\").write_status(STATUS_SUCCESS)\n \n-    result = disk_cached(fn, spec.output_path)\n+    cached_fn = disk_cache(fn, output_path=spec.output_path)\n+    result = cached_fn(spec.output_path)\n \n     assert get_count() == 0\n     assert result == {\"value\": 99, \"from_other\": True}\n@@ -74,20 +77,17 @@ def test_composition_with_save_load(tmp_path: Path):\n \n     output_path = StepSpec(name=\"comp\", output_path_prefix=tmp_path.as_posix()).output_path\n \n-    result1 = disk_cached(\n+    cached_fn = disk_cache(\n         distributed_lock(counting_fn),\n-        output_path,\n-        save=Artifact.save,\n-        load=Artifact.load,\n+        output_path=output_path,\n+        save_fn=Artifact.save,\n+        load_fn=Artifact.load,\n     )\n+\n+    result1 = cached_fn(output_path)\n     assert call_count == 1\n     assert result1 == {\"value\": 42}\n \n-    result2 = disk_cached(\n-        distributed_lock(counting_fn),\n-        output_path,\n-        save=Artifact.save,\n-        load=Artifact.load,\n-    )\n+    result2 = cached_fn(output_path)\n     assert call_count == 1\n     assert result2 == result1\n\n```\n\nextract the parts that a related to the zephyr worker pool restarts, but keep the zephyr worker downsizing separate"},"uuid":"cf7910b2-5d49-4872-9860-587dbf22b1ba","timestamp":"2026-02-23T23:46:24.468Z","permissionMode":"acceptEdits"}
{"parentUuid":"cf7910b2-5d49-4872-9860-587dbf22b1ba","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":[{"type":"text","text":"[Request interrupted by user]"}]},"uuid":"c0cc83d7-643b-4454-be67-447b11f1424c","timestamp":"2026-02-23T23:46:26.343Z"}
{"type":"file-history-snapshot","messageId":"ed8d1bd5-fd67-4d50-aed5-3722f2092cba","snapshot":{"messageId":"ed8d1bd5-fd67-4d50-aed5-3722f2092cba","trackedFileBackups":{},"timestamp":"2026-02-23T23:47:07.500Z"},"isSnapshotUpdate":false}
{"parentUuid":"c0cc83d7-643b-4454-be67-447b11f1424c","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":"Take the change set below:\n\n\n```\ndiff --git c/experiments/pretraining_datasets/nemotron.py w/experiments/pretraining_datasets/nemotron.py\nindex 34099ec54..d3f4d1c27 100644\n--- c/experiments/pretraining_datasets/nemotron.py\n+++ w/experiments/pretraining_datasets/nemotron.py\n@@ -64,7 +64,11 @@ def _get_nemotron_split_paths(split: str):\n \n \n def tokenize_nemotron(\n-    *, tokenizer: str | None = None, window_size_bytes: int = 10_000_000_000\n+    *,\n+    tokenizer: str | None = None,\n+    window_size_bytes: int = 10_000_000_000,\n+    max_workers: int = 4096,\n+    writer_batch_size: int = 65536,\n ) -> dict[str, TokenizerStep]:\n     \"\"\"Generate tokenization steps for all Nemotron CC dataset splits.\"\"\"\n     if tokenizer is None:\n@@ -85,6 +89,8 @@ def tokenize_nemotron(\n                 cache_path=this_output_path(),\n                 tokenizer=versioned(tokenizer),\n                 window_size_bytes=window_size_bytes,\n+                max_workers=max_workers,\n+                writer_batch_size=writer_batch_size,\n             ),\n         )\n \ndiff --git c/lib/fray/src/fray/v2/ray_backend/backend.py w/lib/fray/src/fray/v2/ray_backend/backend.py\nindex 36a70ae13..a24e89a34 100644\n--- c/lib/fray/src/fray/v2/ray_backend/backend.py\n+++ w/lib/fray/src/fray/v2/ray_backend/backend.py\n@@ -409,6 +409,12 @@ def _actor_ray_options(resources: ResourceConfig) -> dict[str, Any]:\n         # node is preempted. Without this, the actor dies permanently and the\n         # pool degrades over time (see marin#2943).\n         options[\"max_restarts\"] = -1\n+    # Explicit max_restarts takes precedence over the preemptible default.\n+    # Useful for actors like coordinators that are preemptible (not pinned to\n+    # head node) but should NOT auto-restart because they require remote\n+    # initialization beyond __init__.\n+    if resources.max_restarts is not None:\n+        options[\"max_restarts\"] = resources.max_restarts\n     if resources.max_concurrency > 1:\n         options[\"max_concurrency\"] = resources.max_concurrency\n     return options\ndiff --git c/lib/fray/src/fray/v2/types.py w/lib/fray/src/fray/v2/types.py\nindex 7c9c99c7c..740495b5d 100644\n--- c/lib/fray/src/fray/v2/types.py\n+++ w/lib/fray/src/fray/v2/types.py\n@@ -322,6 +322,8 @@ class ResourceConfig:\n     regions: Sequence[str] | None = None\n     replicas: int = 1\n     max_concurrency: int = 1\n+    # TODO(rav): is this the right place for max_restarts?\n+    max_restarts: int | None = None\n \n     def chip_count(self) -> int:\n         \"\"\"Total accelerator chips across all replicas.\"\"\"\ndiff --git c/lib/fray/tests/test_v2_ray.py w/lib/fray/tests/test_v2_ray.py\nindex 08723da52..40a304c34 100644\n--- c/lib/fray/tests/test_v2_ray.py\n+++ w/lib/fray/tests/test_v2_ray.py\n@@ -271,3 +271,11 @@ def test_actor_options_preemptible_sets_max_restarts():\n     options = _actor_ray_options(ResourceConfig(preemptible=True))\n     assert options[\"max_restarts\"] == -1\n     assert \"resources\" not in options\n+\n+\n+def test_actor_options_explicit_max_restarts_overrides_preemptible():\n+    from fray.v2.ray_backend.backend import _actor_ray_options\n+\n+    options = _actor_ray_options(ResourceConfig(preemptible=True, max_restarts=0))\n+    assert options[\"max_restarts\"] == 0\n+    assert \"resources\" not in options\ndiff --git c/lib/marin/src/marin/execution/disk_cache.py w/lib/marin/src/marin/execution/disk_cache.py\nindex ae8206068..5735b67ce 100644\n--- c/lib/marin/src/marin/execution/disk_cache.py\n+++ w/lib/marin/src/marin/execution/disk_cache.py\n@@ -4,69 +4,124 @@\n \"\"\"Run a function once, cache the result, and return it on subsequent calls.\"\"\"\n \n from __future__ import annotations\n+import os\n \n+import functools\n import logging\n from collections.abc import Callable\n-from typing import TypeVar\n+from typing import TypeVar, Generic\n+\n+import cloudpickle\n+import fsspec\n+from iris.temp_buckets import get_temp_bucket_path\n \n from marin.execution.distributed_lock import StepAlreadyDone\n-from marin.execution.executor_step_status import (\n-    STATUS_FAILED,\n-    STATUS_SUCCESS,\n-    StatusFile,\n-    worker_id,\n-)\n+import hashlib\n+import pickle\n \n logger = logging.getLogger(__name__)\n \n T = TypeVar(\"T\")\n \n \n-def disk_cached(\n-    fn: Callable[[str], T],\n-    output_path: str,\n-    *,\n-    save: Callable[[T, str], None] | None = None,\n-    load: Callable[[str], T] | None = None,\n-) -> T | None:\n-    \"\"\"Run *fn* once and track completion via a status file.\n+class disk_cache(Generic[T]):\n+    \"\"\"Decorator that caches function results to disk via a status file.\n \n-    When *save* and *load* are provided, the result of *fn* is persisted and\n-    deserialized on cache hits.  When they are ``None`` (the default), *fn* is\n-    expected to handle its own reading/writing at *output_path* and is called\n-    again on cache hits to load the result.\n+    Supports bare decoration, decoration with arguments, and direct wrapping::\n \n-    By itself this function does not provide any locking guarantees, e.g. if\n-    you run disk_cached on multiple workers/shards, there may be a race\n-    condition where multiple workers evaluate the function and write to the same\n-    output path at the same time. To avoid this, compose with\n-    :func:`distributed_lock` to ensure that only one worker executes the\n-    function while the others wait for the result. For example::\n+        @disk_cached\n+        def my_fn(*args):\n+            ...\n \n-        result = disk_cached(\n-            distributed_lock(get_tokenizer),\n-            output_path,\n-            save=Artifact.save,\n-            load=lambda p: Artifact.load(p, TokenizerInfo),\n-        )\n+        @disk_cached(output_path=\"gs://...\", save_fn=save, load_fn=load)\n+        def my_fn(*args):\n+            ...\n+\n+        cached_fn = disk_cached(fn, output_path=path, save_fn=save, load_fn=load)\n+        result = cached_fn(output_path)\n+\n+    When *save_fn* and *load_fn* are provided, the result is persisted and\n+    deserialized on cache hits.  When they are ``None`` (the default),\n+    cloudpickle is used.\n+\n+    The decorated function is called with its original arguments; the\n+    *output_path* is used only for cache bookkeeping.\n+\n+    By itself this class does not provide any locking guarantees.  Compose with\n+    :func:`distributed_lock` to ensure single-writer semantics.\n     \"\"\"\n-    status_file = StatusFile(output_path, worker_id())\n-    if status_file.status == STATUS_SUCCESS:\n-        logger.info(f\"disk_cached: cache hit for {output_path}\")\n-        return load(output_path) if load is not None else fn(output_path)\n \n-    try:\n-        result = fn(output_path)\n-    except StepAlreadyDone:\n-        # NOTE: this is leaky but this branch handles the case of distributed lock wrapper\n-        logger.info(f\"disk_cached: completed by another worker for {output_path}\")\n-        return load(output_path) if load is not None else fn(output_path)\n-    except Exception:\n-        StatusFile(output_path, worker_id()).write_status(STATUS_FAILED)\n-        raise\n+    def __init__(\n+        self,\n+        fn: Callable[..., T] | None = None,\n+        *,\n+        output_path: str | None = None,\n+        save_fn: Callable[[T, str], None] | None = None,\n+        load_fn: Callable[[str], T] | None = None,\n+    ):\n+        self._output_path = output_path\n+        self._save_fn = save_fn\n+        self._load_fn = load_fn\n+        self._fn: Callable[..., T] | None = None\n \n-    if save is not None:\n-        save(result, output_path)\n-    StatusFile(output_path, worker_id()).write_status(STATUS_SUCCESS)\n-    logger.info(f\"disk_cached: computed and cached {output_path}\")\n-    return result\n+        if callable(fn):\n+            self._fn = fn\n+            functools.update_wrapper(self, fn)\n+\n+    # TODO: add ParamSpec\n+    def __call__(self, *args, **kwargs):\n+        if self._fn is None:\n+            # @disk_cached(...) — receiving the function to wrap\n+            if len(args) == 1 and callable(args[0]) and not kwargs:\n+                self._fn = args[0]  # type: ignore[bad-assignment]\n+                functools.update_wrapper(self, self._fn)\n+                return self\n+            raise TypeError(\"disk_cached() expected a callable\")\n+\n+        return self._execute(*args, **kwargs)\n+\n+    def _execute(self, *args, **kwargs):\n+        # Compute a fingerprint of the function arguments for cache keying.\n+\n+        def fingerprint_args(*args, **kwargs) -> str:\n+            \"\"\"Create a deterministic fingerprint for args and kwargs.\"\"\"\n+            # Include the function name in the fingerprint to avoid collisions across different functions.\n+            fn_name = getattr(self._fn, \"__name__\", None)\n+            data = pickle.dumps((fn_name, args, kwargs))\n+            return hashlib.sha256(data).hexdigest()[:8]\n+\n+        output_path = self._output_path\n+        if output_path is None:\n+            args_fingerprint = fingerprint_args(*args, **kwargs)\n+            output_path = get_temp_bucket_path(1, f\"disk_cache_{args_fingerprint}\")\n+            if output_path is None:\n+                output_path = os.environ[\"MARIN_PREFIX\"] + f\"/disk_cache_{args_fingerprint}\"\n+\n+        def load_result():\n+            assert output_path is not None\n+            if self._load_fn is not None:\n+                return self._load_fn(output_path)\n+            with fsspec.open(output_path + \"/data.pkl\", \"rb\") as f:\n+                return cloudpickle.loads(f.read())\n+\n+        try:\n+            return load_result()\n+        except FileNotFoundError:\n+            logger.info(f\"disk_cached: cache miss for {output_path}\")\n+\n+        assert self._fn is not None\n+        try:\n+            result = self._fn(*args, **kwargs)\n+        except StepAlreadyDone:\n+            # NOTE: this is leaky but this branch handles the case of distributed lock wrapper\n+            logger.info(f\"disk_cached: completed by another worker for {output_path}\")\n+            return load_result()\n+\n+        if self._save_fn is not None:\n+            self._save_fn(result, output_path)\n+        else:\n+            with fsspec.open(output_path + \"/data.pkl\", \"wb\") as f:\n+                f.write(cloudpickle.dumps(result))\n+\n+        logger.info(f\"disk_cached: computed and cached {output_path}\")\n+        return result\ndiff --git c/lib/marin/src/marin/execution/step_runner.py w/lib/marin/src/marin/execution/step_runner.py\nindex 8d5a5575a..c55de576e 100644\n--- c/lib/marin/src/marin/execution/step_runner.py\n+++ w/lib/marin/src/marin/execution/step_runner.py\n@@ -262,7 +262,7 @@ class StepRunner:\n     def _launch_step(self, step: StepSpec, *, force_run_failed: bool, dry_run: bool) -> JobHandle | None:\n         \"\"\"Launch a single step as a fray job. Returns None if skipped.\"\"\"\n         from marin.execution.artifact import Artifact\n-        from marin.execution.disk_cache import disk_cached\n+        from marin.execution.disk_cache import disk_cache\n         from marin.execution.distributed_lock import distributed_lock\n \n         output_path = step.output_path\n@@ -291,14 +291,15 @@ class StepRunner:\n         # All caching, locking, heartbeat, artifact saving, and status writes\n         # happen on the worker.\n         step_fn = step.fn\n+        cached_step = disk_cache(\n+            distributed_lock(step_fn, force_run_failed=force_run_failed),\n+            output_path=output_path,\n+            save_fn=Artifact.save,\n+            load_fn=Artifact.load,\n+        )\n \n         def worker_fn():\n-            disk_cached(\n-                distributed_lock(step_fn, force_run_failed=force_run_failed),\n-                output_path,\n-                save=Artifact.save,\n-                load=Artifact.load,\n-            )\n+            cached_step(output_path)\n \n         worker_fn.__qualname__ = step_name\n         worker_fn.__name__ = step_name\ndiff --git c/lib/marin/src/marin/processing/tokenize/tokenize.py w/lib/marin/src/marin/processing/tokenize/tokenize.py\nindex 3a78a164c..1f511079a 100644\n--- c/lib/marin/src/marin/processing/tokenize/tokenize.py\n+++ w/lib/marin/src/marin/processing/tokenize/tokenize.py\n@@ -7,6 +7,9 @@ Tokenize datasets using zephyr pipeline and write to Levanter cache format.\n Supports both regular file paths and HuggingFace datasets. For HF datasets, downloads\n them first then tokenizes the downloaded files.\n \"\"\"\n+from functools import cache\n+from marin.utilities.time_logger import log_time\n+from marin.execution.disk_cache import disk_cache\n \n import abc\n import dataclasses\n@@ -33,7 +36,7 @@ from levanter.data.text import (\n )\n from levanter.store.cache import consolidate_shard_caches\n from levanter.store.tree_store import TreeStore\n-from zephyr import Dataset, ZephyrContext, zephyr_worker_ctx\n+from zephyr import Dataset, ZephyrContext\n from zephyr.readers import load_file\n \n from marin.execution.executor import ExecutorStep, InputName, VersionedValue\n@@ -83,7 +86,7 @@ class TokenizeConfig(TokenizeConfigBase):\n     \"\"\"Files are bundled into groups up to this size; each group becomes one shard.\n     Smaller values produce more shards and thus more parallelism (up to max_workers).\"\"\"\n     max_workers: int = 4096\n-    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram=\"5g\", disk=\"5g\"))\n+    worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram=\"10g\", disk=\"5g\"))\n     writer_batch_size: int = 65536\n     \"\"\"Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op\n     overhead. Too large a value increases memory usage and delays progress checkpointing.\"\"\"\n@@ -147,7 +150,7 @@ class HfTokenizeConfig(TokenizeConfigBase):\n     window_size_bytes: int = 10_000_000_000\n     \"\"\"Files are bundled into groups up to this size; each group becomes one shard.\n     Smaller values produce more shards and thus more parallelism (up to max_workers).\"\"\"\n-    max_workers: int = 4096\n+    max_workers: int = 512\n     worker_resources: ResourceConfig = dataclasses.field(default_factory=lambda: ResourceConfig(ram=\"5g\", disk=\"5g\"))\n     writer_batch_size: int = 65536\n     \"\"\"Larger values mean fewer, bigger writes to the Levanter cache, which reduces per-op\n@@ -255,7 +258,7 @@ def _bundle_files_by_size(file_infos, max_bytes: int):\n \n def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Iterator[Sequence[dict]]) -> Iterator[dict]:\n     \"\"\"Tokenize a list of batches using the specified tokenizer and format.\"\"\"\n-    tokenizer: transformers.PreTrainedTokenizer = zephyr_worker_ctx().get_shared(\"tokenizer\")\n+    tokenizer: transformers.PreTrainedTokenizer = get_tokenizer(config.tokenizer)\n     batch_processor = preprocessor_for_format(config.format, tokenizer)\n \n     batch_count = 0\n@@ -289,6 +292,12 @@ def _tokenize_batches(*, config: TokenizeConfig | HfTokenizeConfig, batches: Ite\n     )\n \n \n+@cache\n+@disk_cache\n+def get_tokenizer(tokenizer: str) -> transformers.PreTrainedTokenizer:\n+    return transformers.AutoTokenizer.from_pretrained(tokenizer)\n+\n+\n def tokenize(config: TokenizeConfigBase):\n     \"\"\"Tokenize datasets using zephyr pipeline.\n \n@@ -376,8 +385,8 @@ def tokenize(config: TokenizeConfigBase):\n             )\n         )\n \n-        # Broadcast the tokenizer to all workers via ZephyrContext\n-        ctx.put(\"tokenizer\", transformers.AutoTokenizer.from_pretrained(config.tokenizer))\n+        with log_time(f\"{config.tokenizer} cache warmup\"):\n+            get_tokenizer(config.tokenizer)\n \n         tokenize_start = time.monotonic()\n         shard_paths = ctx.execute(temp_shards)\n@@ -422,7 +431,7 @@ def tokenize(config: TokenizeConfigBase):\n             resources=config.worker_resources,\n             max_workers=min(config.max_workers, len(train_groups)),\n             name=\"tokenize-train\",\n-            no_workers_timeout=20 * 60,\n+            no_workers_timeout=6 * 60 * 60,\n         ) as ctx:\n             run_pipeline(ctx, train_groups, \"train\")\n \n@@ -432,7 +441,7 @@ def tokenize(config: TokenizeConfigBase):\n             resources=config.worker_resources,\n             max_workers=min(config.max_workers, len(validation_groups)),\n             name=\"tokenize-validation\",\n-            no_workers_timeout=20 * 60,\n+            no_workers_timeout=6 * 60 * 60,\n         ) as ctx:\n             run_pipeline(ctx, validation_groups, \"validation\")\n \ndiff --git c/lib/zephyr/src/zephyr/execution.py w/lib/zephyr/src/zephyr/execution.py\nindex 87715feee..385585d9c 100644\n--- c/lib/zephyr/src/zephyr/execution.py\n+++ w/lib/zephyr/src/zephyr/execution.py\n@@ -10,6 +10,8 @@ protocol.\n \"\"\"\n \n from __future__ import annotations\n+from datetime import timedelta\n+import contextlib\n \n import enum\n import logging\n@@ -47,6 +49,14 @@ from zephyr.writers import ensure_parent_dir\n logger = logging.getLogger(__name__)\n \n \n+@contextlib.contextmanager\n+def log_time(label: str, level: int = logging.INFO) -> Iterator[None]:\n+    t_start = time.perf_counter()\n+    yield\n+    t_end = time.perf_counter()\n+    logger.log(level, f\"{label} took {timedelta(seconds=t_end - t_start)}\")\n+\n+\n class Chunk(Protocol):\n     def __iter__(self) -> Iterator: ...\n \n@@ -247,6 +257,8 @@ class ZephyrCoordinator:\n     \"\"\"\n \n     def __init__(self):\n+        from fray.v2 import current_actor\n+\n         # Task management state\n         self._task_queue: deque[ShardTask] = deque()\n         self._results: dict[int, TaskResult] = {}\n@@ -268,11 +280,15 @@ class ZephyrCoordinator:\n         self._worker_handles: dict[str, ActorHandle] = {}\n         self._coordinator_thread: threading.Thread | None = None\n         self._shutdown: bool = False\n+        self._is_last_stage: bool = False\n         self._initialized: bool = False\n \n         # Lock for accessing coordinator state from background thread\n         self._lock = threading.Lock()\n \n+        actor_ctx = current_actor()\n+        self._name = f\"{actor_ctx.group_name}\"\n+\n     def initialize(\n         self,\n         chunk_prefix: str,\n@@ -314,6 +330,10 @@ class ZephyrCoordinator:\n                 self._worker_handles[worker_id] = worker_handle\n                 self._worker_states[worker_id] = WorkerState.READY\n                 self._last_seen[worker_id] = time.monotonic()\n+                # NOTE: if there was a task assigned to the worker, there's a race condidtion between marking\n+                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n+                # lose tasks.\n+                self._maybe_requeue_worker_task(worker_id)\n                 return\n \n             self._worker_handles[worker_id] = worker_handle\n@@ -330,9 +350,9 @@ class ZephyrCoordinator:\n         last_log_time = 0.0\n \n         while not self._shutdown:\n-            # Check heartbeats, re-queue stale tasks\n-            with self._lock:\n-                self._check_worker_heartbeats()\n+            # Scheduler check heartbeats, re-queue stale tasks\n+            # This is called in a remote fashion to account for potential congestion.\n+            self._self_handle.check_heartbeats.remote()\n \n             # Log status periodically during active execution\n             now = time.monotonic()\n@@ -360,6 +380,16 @@ class ZephyrCoordinator:\n             dead,\n         )\n \n+    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n+        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n+        task_and_attempt = self._in_flight.pop(worker_id, None)\n+        if task_and_attempt is not None:\n+            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n+            task, _old_attempt = task_and_attempt\n+            self._task_attempts[task.shard_idx] += 1\n+            self._task_queue.append(task)\n+            self._retries += 1\n+\n     def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n         \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n         now = time.monotonic()\n@@ -367,13 +397,7 @@ class ZephyrCoordinator:\n             if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                 logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                 self._worker_states[worker_id] = WorkerState.FAILED\n-                task_and_attempt = self._in_flight.pop(worker_id, None)\n-                if task_and_attempt is not None:\n-                    logger.info(\"Removed task %s from worker %s, re-queueing\", task_and_attempt, worker_id)\n-                    task, _old_attempt = task_and_attempt\n-                    self._task_attempts[task.shard_idx] += 1\n-                    self._task_queue.append(task)\n-                    self._retries += 1\n+                self._maybe_requeue_worker_task(worker_id)\n \n     def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n         \"\"\"Called by workers to get next task.\n@@ -394,6 +418,8 @@ class ZephyrCoordinator:\n                 return None\n \n             if not self._task_queue:\n+                if self._is_last_stage:\n+                    return \"SHUTDOWN\"\n                 return None\n \n             task = self._task_queue.popleft()\n@@ -408,9 +434,25 @@ class ZephyrCoordinator:\n             }\n             return (task, attempt, config)\n \n+    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n+        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n+\n+        Workers block on report_result/report_error before calling pull_task,\n+        so _in_flight can never point to a different shard. It may be absent\n+        if a heartbeat timeout already re-queued the task.\n+        \"\"\"\n+        in_flight = self._in_flight.get(worker_id)\n+        if in_flight is not None:\n+            assert in_flight[0].shard_idx == shard_idx, (\n+                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n+                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n+                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n+            )\n+\n     def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n         with self._lock:\n             self._last_seen[worker_id] = time.monotonic()\n+            self._assert_in_flight_consistent(worker_id, shard_idx)\n \n             current_attempt = self._task_attempts.get(shard_idx, 0)\n             if attempt != current_attempt:\n@@ -429,13 +471,14 @@ class ZephyrCoordinator:\n         \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n         with self._lock:\n             self._last_seen[worker_id] = time.monotonic()\n+            self._assert_in_flight_consistent(worker_id, shard_idx)\n             self._in_flight.pop(worker_id, None)\n             self._fatal_error = error_info\n             self._worker_states[worker_id] = WorkerState.DEAD\n \n     def heartbeat(self, worker_id: str) -> None:\n-        with self._lock:\n-            self._last_seen[worker_id] = time.monotonic()\n+        # NOTE: there's no need for lock here\n+        self._last_seen[worker_id] = time.monotonic()\n \n     def get_status(self) -> JobStatus:\n         with self._lock:\n@@ -461,7 +504,7 @@ class ZephyrCoordinator:\n         with self._lock:\n             return self._fatal_error\n \n-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue.\"\"\"\n         with self._lock:\n             self._task_queue = deque(tasks)\n@@ -473,13 +516,14 @@ class ZephyrCoordinator:\n             self._in_flight = {}\n             self._task_attempts = {task.shard_idx: 0 for task in tasks}\n             self._fatal_error = None\n+            self._is_last_stage = is_last_stage\n \n     def _wait_for_stage(self) -> None:\n         \"\"\"Block until current stage completes or error occurs.\"\"\"\n         backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n         last_log_completed = -1\n         start_time = time.monotonic()\n-        warned_no_workers = False\n+        all_dead_since: float | None = None\n         no_workers_timeout = self._no_workers_timeout\n \n         while True:\n@@ -487,27 +531,37 @@ class ZephyrCoordinator:\n                 if self._fatal_error:\n                     raise ZephyrWorkerError(self._fatal_error)\n \n-                num_workers = len(self._worker_handles)\n                 completed = self._completed_shards\n                 total = self._total_shards\n \n                 if completed >= total:\n                     return\n \n-                # Fail fast if no workers appear within timeout\n-                if num_workers == 0:\n-                    elapsed = time.monotonic() - start_time\n-                    if elapsed > no_workers_timeout:\n+                # Count alive workers (READY or BUSY), not just total registered.\n+                # Dead/failed workers stay in _worker_handles but can't make progress.\n+                alive_workers = sum(\n+                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n+                )\n+\n+                if alive_workers == 0:\n+                    now = time.monotonic()\n+                    elapsed = now - start_time\n+\n+                    if all_dead_since is None:\n+                        all_dead_since = now\n+                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n+\n+                    dead_duration = now - all_dead_since\n+                    if dead_duration > no_workers_timeout:\n                         raise ZephyrWorkerError(\n-                            f\"No workers available after {elapsed:.1f}s. \"\n+                            f\"No alive workers for {dead_duration:.1f}s \"\n+                            f\"(total elapsed {elapsed:.1f}s). \"\n+                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                             \"Check cluster resources and worker group configuration.\"\n                         )\n-                    if not warned_no_workers and elapsed > 5.0:\n-                        logger.warning(\n-                            \"No workers available yet after %.1fs, waiting for discovery...\",\n-                            elapsed,\n-                        )\n-                        warned_no_workers = True\n+                else:\n+                    # Workers are alive — reset the dead timer\n+                    all_dead_since = None\n \n             if completed != last_log_completed:\n                 logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n@@ -521,6 +575,7 @@ class ZephyrCoordinator:\n         with self._lock:\n             return dict(self._results)\n \n+    # TODO: this is not used anywhere!?\n     def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n         \"\"\"Set config for the current execution.\"\"\"\n         with self._lock:\n@@ -543,6 +598,15 @@ class ZephyrCoordinator:\n         if not shards:\n             return []\n \n+        # Identify the last stage that dispatches work to workers (non-RESHARD).\n+        # On that stage, idle workers receive SHUTDOWN once all tasks are\n+        # in-flight, so they exit eagerly instead of polling until\n+        # coordinator.shutdown().\n+        last_worker_stage_idx = max(\n+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n+            default=-1,\n+        )\n+\n         for stage_idx, stage in enumerate(plan.stages):\n             stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n \n@@ -556,7 +620,7 @@ class ZephyrCoordinator:\n             # Build and submit tasks\n             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n             logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n-            self._start_stage(stage_label, tasks)\n+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))\n \n             # Wait for stage completion\n             self._wait_for_stage()\n@@ -570,6 +634,10 @@ class ZephyrCoordinator:\n         for shard in shards:\n             for chunk in shard.chunks:\n                 flat_result.extend(list(chunk))\n+\n+        # Signal workers to shut down now that all stages are complete.\n+        self.shutdown()\n+\n         return flat_result\n \n     def _compute_join_aux(\n@@ -649,9 +717,9 @@ class ZephyrCoordinator:\n                 \"execution_id\": self._execution_id,\n             }\n \n-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n-        self._start_stage(stage_name, tasks)\n+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)\n \n     def check_heartbeats(self, timeout: float = 30.0) -> None:\n         \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n@@ -666,6 +734,9 @@ class ZephyrCoordinator:\n         \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n         self._shutdown = True\n \n+    def __repr__(self) -> str:\n+        return f\"ZephyrCoordinator(name={self._name})\"\n+\n \n # ---------------------------------------------------------------------------\n # ZephyrWorker\n@@ -695,8 +766,8 @@ class ZephyrWorker:\n         actor_ctx = current_actor()\n         self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n \n-        # Register with coordinator\n-        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle)\n+        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n+        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n \n         # Start polling in a background thread\n         self._polling_thread = threading.Thread(\n@@ -735,7 +806,7 @@ class ZephyrWorker:\n         heartbeat_count = 0\n         while not self._shutdown_event.is_set():\n             try:\n-                coordinator.heartbeat.remote(self._worker_id)\n+                coordinator.heartbeat.remote(self._worker_id).result()\n                 heartbeat_count += 1\n                 if heartbeat_count % 10 == 1:\n                     logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n@@ -779,9 +850,12 @@ class ZephyrWorker:\n \n             logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n             try:\n-                result = self._execute_shard(task, config)\n-                logger.info(\"[%s] Task complete, reporting result for shard %d\", self._worker_id, task.shard_idx)\n-                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result)\n+                with log_time(f\"Worker {self._worker_id} executing shard {task.shard_idx}\"):\n+                    result = self._execute_shard(task, config)\n+                # Block until coordinator records the result. This ensures\n+                # report_result is fully processed before the next pull_task,\n+                # preventing _in_flight tracking races.\n+                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                 task_count += 1\n             except Exception as e:\n                 logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n@@ -791,7 +865,7 @@ class ZephyrWorker:\n                     self._worker_id,\n                     task.shard_idx,\n                     \"\".join(traceback.format_exc()),\n-                )\n+                ).result()\n \n     def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n         \"\"\"Execute a stage's operations on a single shard.\n@@ -860,6 +934,9 @@ class ZephyrWorker:\n         if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n             self._polling_thread.join(timeout=5.0)\n \n+    def __repr__(self) -> str:\n+        return f\"ZephyrWorker(id={self._worker_id})\"\n+\n \n def _regroup_result_refs(\n     result_refs: dict[int, TaskResult],\n@@ -883,12 +960,10 @@ def _regroup_result_refs(\n class ZephyrContext:\n     \"\"\"Execution context for Zephyr pipelines.\n \n-    Creates a coordinator actor on __enter__ which owns and manages the worker\n-    pool. Workers are created lazily on the first execute() call, sized to\n-    min(max_workers, plan.num_shards) to avoid over-provisioning. Workers\n-    persist across pipeline stages and execute() calls, allowing cached state\n-    (tokenizers, models) to be reused. Shared data broadcast via put() is\n-    delivered to workers with each task.\n+    Each execute() call creates a fresh coordinator and worker pool, runs\n+    the pipeline, then tears everything down. Workers are sized to\n+    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n+    data broadcast via put() is delivered to workers with each task.\n \n     Args:\n         client: The fray client to use. If None, auto-detects using current_client().\n@@ -920,6 +995,8 @@ class ZephyrContext:\n     _coordinator_group: Any = field(default=None, repr=False)\n     _worker_group: Any = field(default=None, repr=False)\n     _worker_count: int = field(default=0, repr=False)\n+    # NOTE: execute calls increment this at the very beginning\n+    _pipeline_id: int = field(default=-1, repr=False)\n \n     def __post_init__(self):\n         if self.client is None:\n@@ -971,29 +1048,36 @@ class ZephyrContext:\n         verbose: bool = False,\n         dry_run: bool = False,\n     ) -> Sequence:\n-        \"\"\"Execute a dataset pipeline on the worker pool.\n+        \"\"\"Execute a dataset pipeline.\n \n-        Workers persist across execute() calls, so cached state (tokenizers,\n-        models) will be reused. If the coordinator dies mid-execution (e.g.,\n-        VM preemption), the pipeline is retried with a fresh coordinator and\n-        worker pool up to ``max_execution_retries`` times. Application errors\n-        (``ZephyrWorkerError``) are never retried.\n+        Each call creates a fresh coordinator and worker pool, runs the\n+        pipeline, then tears everything down. If the coordinator dies\n+        mid-execution (e.g., VM preemption), the pipeline is retried\n+        with fresh actors up to ``max_execution_retries`` times.\n+        Application errors (``ZephyrWorkerError``) are never retried.\n         \"\"\"\n         plan = compute_plan(dataset, hints)\n         if dry_run:\n             _print_plan(dataset.operations, plan)\n             return []\n \n+        # NOTE: pipeline ID incremented on clean completion only\n+        self._pipeline_id += 1\n         last_exception: Exception | None = None\n         for attempt in range(self.max_execution_retries + 1):\n             execution_id = _generate_execution_id()\n-            logger.info(\"Starting zephyr pipeline: %s (attempt %d)\", execution_id, attempt)\n+            logger.info(\n+                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n+            )\n \n             try:\n-                self._ensure_coordinator()\n-                self._ensure_workers(plan.num_shards)\n+                self._create_coordinator(attempt)\n+                self._create_workers(plan.num_shards, attempt)\n \n-                # Run pipeline on coordinator (blocking call)\n+                # Run pipeline on coordinator (blocking call).\n+                # run_pipeline() calls coordinator.shutdown() at the end,\n+                # which causes workers to receive SHUTDOWN on their next\n+                # pull_task() call.\n                 results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n \n                 return results\n@@ -1007,34 +1091,34 @@ class ZephyrContext:\n                     raise\n \n                 logger.warning(\n-                    \"Pipeline attempt %d/%d failed, retrying with fresh coordinator: %s\",\n-                    attempt + 1,\n-                    self.max_execution_retries,\n+                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n+                    attempt,\n+                    self.max_execution_retries - attempt,\n                     e,\n                 )\n-                # Tear down dead coordinator and orphaned workers so\n-                # _ensure_coordinator / _ensure_workers create fresh ones.\n-                self.shutdown()\n \n             finally:\n-                # Clean up chunks for this execution only (workers persist)\n+                # Tear down coordinator and workers for this pipeline\n+                self.shutdown()\n+                # Clean up chunks for this execution\n                 _cleanup_execution(self.chunk_storage_prefix, execution_id)\n \n         # Should be unreachable, but just in case\n         raise last_exception  # type: ignore[misc]\n \n-    def _ensure_coordinator(self) -> None:\n-        \"\"\"Create coordinator if not already initialized.\"\"\"\n-        if self._coordinator is not None:\n-            return\n-\n-        # Create coordinator actor with high max_concurrency to allow\n-        # workers to call pull_task/report_result while run_pipeline blocks\n-        logger.info(\"Starting coordinator for %s\", self.name)\n-        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\", max_concurrency=100)\n+    def _create_coordinator(self, attempt: int = 0) -> None:\n+        \"\"\"Create a fresh coordinator actor.\"\"\"\n+        # max_concurrency allows workers to call pull_task/report_result\n+        # while run_pipeline blocks.\n+        # max_restarts=0: coordinator must NOT auto-restart on preemption.\n+        # A restarted coordinator only re-runs __init__, not initialize() or\n+        # run_pipeline(), so it becomes a zombie. Instead, let it die and let\n+        # the execute() retry loop tear down everything and start fresh.\n+        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n+        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\", max_concurrency=200, max_restarts=0)\n         self._coordinator_group = self.client.create_actor_group(\n             ZephyrCoordinator,\n-            name=f\"zephyr-{self.name}-coord\",\n+            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n             count=1,\n             resources=coordinator_resources,\n         )\n@@ -1048,27 +1132,28 @@ class ZephyrContext:\n \n         logger.info(\"Coordinator initialized for %s\", self.name)\n \n-    def _ensure_workers(self, num_shards: int) -> None:\n-        \"\"\"Create workers if not already initialized, sized to demand.\n+    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n+        \"\"\"Create a fresh worker pool sized to demand.\n \n         The worker count is min(max_workers, num_shards) to avoid\n         over-provisioning when there are fewer shards than the cap.\n         \"\"\"\n-        if self._worker_group is not None or num_shards == 0:\n+        if num_shards == 0:\n             return\n \n         assert self.max_workers is not None  # set by __post_init__\n         actual_workers = min(self.max_workers, num_shards)\n         logger.info(\n-            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d)\",\n+            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n             actual_workers,\n             self.max_workers,\n             num_shards,\n+            attempt,\n         )\n         self._worker_group = self.client.create_actor_group(\n             ZephyrWorker,\n             self._coordinator,  # Pass coordinator handle as init arg\n-            name=f\"zephyr-{self.name}-workers\",\n+            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n             count=actual_workers,\n             resources=self.resources,\n         )\n@@ -1097,8 +1182,6 @@ class ZephyrContext:\n         self._worker_group = None\n \n     def __enter__(self) -> ZephyrContext:\n-        # Eagerly initialize coordinator; workers are deferred to first execute()\n-        self._ensure_coordinator()\n         return self\n \n     def __exit__(self, *exc) -> None:\ndiff --git c/lib/zephyr/tests/test_execution.py w/lib/zephyr/tests/test_execution.py\nindex 2ac8e7f60..bc851c6cd 100644\n--- c/lib/zephyr/tests/test_execution.py\n+++ w/lib/zephyr/tests/test_execution.py\n@@ -344,8 +344,8 @@ def test_chunk_streaming_low_memory(tmp_path):\n     assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n \n \n-def test_workers_persist_across_executes(fray_client, tmp_path):\n-    \"\"\"Workers persist across multiple execute() calls within a context.\"\"\"\n+def test_fresh_actors_per_execute(fray_client, tmp_path):\n+    \"\"\"Each execute() creates and tears down its own coordinator and workers.\"\"\"\n     chunk_prefix = str(tmp_path / \"chunks\")\n \n     with ZephyrContext(\n@@ -359,18 +359,19 @@ def test_workers_persist_across_executes(fray_client, tmp_path):\n         results = list(zctx.execute(ds))\n         assert sorted(results) == [2, 3, 4]\n \n-        # After execute(): coordinator still exists (workers persist)\n-        assert zctx._coordinator is not None\n-        assert zctx._coordinator_group is not None\n+        # After execute(): everything is torn down\n+        assert zctx._coordinator is None\n+        assert zctx._worker_group is None\n+        assert zctx._pipeline_id == 1\n \n-        # Can execute again (reuses same workers)\n+        # Can execute again (creates fresh coordinator + workers)\n         ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)\n         results2 = list(zctx.execute(ds2))\n         assert sorted(results2) == [20, 40]\n \n-    # After context exit: all resources are cleaned up\n-    assert zctx._coordinator is None\n-    assert zctx._coordinator_group is None\n+        assert zctx._coordinator is None\n+        assert zctx._worker_group is None\n+        assert zctx._pipeline_id == 2\n \n \n def test_fatal_errors_fail_fast(fray_client, tmp_path):\n@@ -438,16 +439,15 @@ def test_workers_capped_to_shard_count(fray_client, tmp_path):\n         chunk_storage_prefix=str(tmp_path / \"chunks\"),\n         name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n     ) as ctx:\n-        # Workers not yet created before execute()\n-        assert ctx._worker_group is None\n         results = list(ctx.execute(ds.map(lambda x: x * 2)))\n         assert sorted(results) == [2, 4, 6]\n-        # Worker group sized to min(10, 3) = 3\n-        assert ctx._worker_count == 3\n+        # Everything torn down after execute; correct results prove workers\n+        # were created and sized properly (min(10, 3) = 3)\n+        assert ctx._pipeline_id == 1\n \n \n-def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):\n-    \"\"\"Second execute() reuses the same worker group object (no re-creation).\"\"\"\n+def test_pipeline_id_increments(fray_client, tmp_path):\n+    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n     with ZephyrContext(\n         client=fray_client,\n         max_workers=10,\n@@ -456,9 +456,65 @@ def test_worker_group_identity_stable_across_executes(fray_client, tmp_path):\n         name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n     ) as ctx:\n         ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n-        group_after_first = ctx._worker_group\n+        assert ctx._pipeline_id == 1\n+\n         ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n-        assert ctx._worker_group is group_after_first\n+        assert ctx._pipeline_id == 2\n+\n+\n+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):\n+    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"\n+    from unittest.mock import MagicMock\n+\n+    from fray.v2.actor import ActorContext, _set_current_actor, _reset_current_actor\n+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n+\n+    token = _set_current_actor(ActorContext(handle=MagicMock(), index=0, group_name=\"test-coord\"))\n+    try:\n+        coord = ZephyrCoordinator()\n+    finally:\n+        _reset_current_actor(token)\n+    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n+    coord.set_shared_data({})\n+\n+    task = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test\",\n+    )\n+\n+    # Non-last stage: empty queue returns None\n+    coord.start_stage(\"stage-0\", [task], is_last_stage=False)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty, but not last stage -> None\n+    result = coord.pull_task(\"worker-A\")\n+    assert result is None\n+\n+    # Last stage: empty queue returns SHUTDOWN\n+    task2 = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test-last\",\n+    )\n+    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty on last stage -> SHUTDOWN\n+    result = coord.pull_task(\"worker-A\")\n+    assert result == \"SHUTDOWN\"\n \n \n def test_execute_retries_on_coordinator_death(tmp_path):\n@@ -482,26 +538,32 @@ def test_execute_retries_on_coordinator_death(tmp_path):\n         name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n     )\n \n-    # First execute() succeeds normally — establishes coordinator + workers\n-    ctx._ensure_coordinator()\n-    coord_endpoint = ctx._coordinator._endpoint\n-\n+    # First execute() succeeds normally\n     results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))\n     assert sorted(results) == [2, 4, 6]\n \n-    # Kill the coordinator by removing it from the local actor registry.\n-    # This simulates VM preemption — the handle becomes stale and any RPC\n-    # through it will raise RuntimeError(\"Actor not found in registry\").\n-    _local_actor_registry.pop(coord_endpoint, None)\n+    # Sabotage the registry so the *next* coordinator creation attempt fails\n+    # on the first try. We do this by patching create_actor_group to fail once.\n+    original_create = client.create_actor_group\n+    fail_count = [0]\n+\n+    def flaky_create(*args, **kwargs):\n+        group = original_create(*args, **kwargs)\n+        if fail_count[0] == 0 and \"coord\" in kwargs.get(\"name\", \"\"):\n+            fail_count[0] += 1\n+            # Kill the coordinator immediately after creation to simulate death\n+            handles = group.wait_ready()\n+            endpoint = handles[0]._endpoint\n+            _local_actor_registry.pop(endpoint, None)\n+        return group\n+\n+    client.create_actor_group = flaky_create\n \n     # Next execute() should: fail on attempt 0 (dead coordinator),\n-    # shutdown + recreate everything, then succeed on attempt 1.\n+    # then succeed on attempt 1 with a fresh coordinator.\n     results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))\n     assert sorted(results) == [11, 21]\n \n-    # Verify coordinator was recreated (handle is not None after retry)\n-    assert ctx._coordinator is not None\n-\n     ctx.shutdown()\n     client.shutdown(wait=True)\n \ndiff --git c/tests/execution/test_disk_cache.py w/tests/execution/test_disk_cache.py\nindex d9b4b6274..d25d1b036 100644\n--- c/tests/execution/test_disk_cache.py\n+++ w/tests/execution/test_disk_cache.py\n@@ -6,7 +6,7 @@ import os\n from pathlib import Path\n \n from marin.execution.artifact import Artifact\n-from marin.execution.disk_cache import disk_cached\n+from marin.execution.disk_cache import disk_cache\n from marin.execution.distributed_lock import distributed_lock\n from marin.execution.executor_step_status import STATUS_SUCCESS, StatusFile\n from marin.execution.step_spec import StepSpec\n@@ -36,12 +36,14 @@ def test_disk_cached_runs_and_caches(tmp_path: Path):\n     fn, get_count = _make_fn()\n     output_path = StepSpec(name=\"step\", output_path_prefix=tmp_path.as_posix()).output_path\n \n-    result1 = disk_cached(fn, output_path)\n+    cached_fn = disk_cache(fn, output_path=output_path)\n+\n+    result1 = cached_fn(output_path)\n     assert get_count() == 1\n     assert result1 == {\"value\": 42, \"computed\": True}\n \n     # Cache hit: fn is called to load but no recomputation\n-    result2 = disk_cached(fn, output_path)\n+    result2 = cached_fn(output_path)\n     assert get_count() == 1\n     assert result2 == result1\n \n@@ -56,7 +58,8 @@ def test_disk_cached_skips_when_another_worker_completed(tmp_path: Path):\n         json.dump({\"value\": 99, \"from_other\": True}, f)\n     StatusFile(spec.output_path, \"other-worker\").write_status(STATUS_SUCCESS)\n \n-    result = disk_cached(fn, spec.output_path)\n+    cached_fn = disk_cache(fn, output_path=spec.output_path)\n+    result = cached_fn(spec.output_path)\n \n     assert get_count() == 0\n     assert result == {\"value\": 99, \"from_other\": True}\n@@ -74,20 +77,17 @@ def test_composition_with_save_load(tmp_path: Path):\n \n     output_path = StepSpec(name=\"comp\", output_path_prefix=tmp_path.as_posix()).output_path\n \n-    result1 = disk_cached(\n+    cached_fn = disk_cache(\n         distributed_lock(counting_fn),\n-        output_path,\n-        save=Artifact.save,\n-        load=Artifact.load,\n+        output_path=output_path,\n+        save_fn=Artifact.save,\n+        load_fn=Artifact.load,\n     )\n+\n+    result1 = cached_fn(output_path)\n     assert call_count == 1\n     assert result1 == {\"value\": 42}\n \n-    result2 = disk_cached(\n-        distributed_lock(counting_fn),\n-        output_path,\n-        save=Artifact.save,\n-        load=Artifact.load,\n-    )\n+    result2 = cached_fn(output_path)\n     assert call_count == 1\n     assert result2 == result1\n\n```\n\nextract the parts that a related to zephyr worker pool worker shutdown when it's the last stage and there's no work left in the queue to release resources"},"uuid":"ed8d1bd5-fd67-4d50-aed5-3722f2092cba","timestamp":"2026-02-23T23:47:03.018Z","permissionMode":"acceptEdits"}
{"parentUuid":"ed8d1bd5-fd67-4d50-aed5-3722f2092cba","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","message":{"model":"claude-opus-4-6","id":"msg_01Vn6o5VTc654S81D9wNhuz4","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user wants me to extract only the parts of the diff that are related to \"zephyr worker pool worker shutdown when it's the last stage and there's no work left in the queue to release resources.\" Let me identify those changes:\n\n1. **`_is_last_stage` field** in `ZephyrCoordinator.__init__` - tracks whether current stage is the last\n2. **`pull_task` returning \"SHUTDOWN\"** when `_is_last_stage` and queue is empty\n3. **`_start_stage` accepting `is_last_stage` parameter**\n4. **`run_pipeline` computing `last_worker_stage_idx`** and passing `is_last_stage` to `_start_stage`\n\n\n5. **`run_pipeline` calling `self.shutdown()`** at the end to signal workers to terminate\n6. **`start_stage` (public) forwarding the `is_last_stage` parameter** through to the internal method\n7. The worker's `_poll_loop` already handles the \"SHUTDOWN\" response from `pull_task` to exit gracefully\n8. There's a test case for verifying `pull_task` returns shutdown on the last stage when the queue is empty\n9. **`ZephyrContext.execute`** has comments explaining how `run_pipeline()` manages the shutdown sequence by calling `coordinator.shutdown()`\n\nNow I'm tracing through the complete \"last stage shutdown\" implementation across the codebase to verify all the pieces connect properly.\n\nIn the test file, there's a specific test case for when workers pull tasks on the last stage with an empty queue and receive the shutdown signal instead.\n\nThe key distinction here is that this early shutdown mechanism is separate from the broader \"fresh coordinator/workers per execute\" change—it's specifically about letting idle workers exit gracefully when they're on the final stage and there's no more work to process. of polling forever. This is distinct from the \"fresh actors per execute\" change.\n\nThe key changes are:\n1. `_is_last_stage` field\n2. `pull_task` returning \"SHUTDOWN\" on last stage when queue is empty\n3. `_start_stage` accepting and storing `is_last_stage`\n4. `run_pipeline` computing `last_worker_stage_idx` and passing it\n5. `run_pipeline` calling `self.shutdown()` after all stages complete\n6. `start_stage` (public) forwarding `is_last_stage`\n7. The test for this behavior\n\nNow I'm thinking through whether to include the `self.shutdown()` call at the end of `run_pipeline` — it's related to the SHUTDOWN signal mechanism but also ties into the broader \"fresh actors per execute\" change. Since the user specifically asked about signaling shutdown when the last stage has no work left, I should include both the SHUTDOWN logic and the cleanup call, along with the supporting infrastructure like the `_is_last_stage` field and parameter threading through the stage initialization methods. Let me construct the diff with all these pieces.","signature":"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"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":39331,"cache_read_input_tokens":23218,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":39331},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtkeurLRVn2HCR2Wj2i","type":"assistant","uuid":"5a35e6a9-3da2-4878-b466-8b2c84ec4e69","timestamp":"2026-02-23T23:47:33.811Z"}
{"parentUuid":"5a35e6a9-3da2-4878-b466-8b2c84ec4e69","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","message":{"model":"claude-opus-4-6","id":"msg_01Vn6o5VTc654S81D9wNhuz4","type":"message","role":"assistant","content":[{"type":"text","text":"Here are the extracted changes for the \"last stage early worker shutdown\" feature:\n\n**`lib/zephyr/src/zephyr/execution.py`:**\n\nIn `ZephyrCoordinator.__init__`:\n```python\nself._shutdown: bool = False\n+self._is_last_stage: bool = False\nself._initialized: bool = False\n```\n\nIn `pull_task`:\n```python\nif not self._task_queue:\n+    if self._is_last_stage:\n+        return \"SHUTDOWN\"\n    return None\n```\n\nIn `_start_stage`:\n```python\n-def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n     \"\"\"Load a new stage's tasks into the queue.\"\"\"\n     with self._lock:\n         ...\n         self._fatal_error = None\n+        self._is_last_stage = is_last_stage\n```\n\nIn `run_pipeline`, before the stage loop:\n```python\n+# Identify the last stage that dispatches work to workers (non-RESHARD).\n+# On that stage, idle workers receive SHUTDOWN once all tasks are\n+# in-flight, so they exit eagerly instead of polling until\n+# coordinator.shutdown().\n+last_worker_stage_idx = max(\n+    (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n+    default=-1,\n+)\n```\n\nIn `run_pipeline`, the `_start_stage` call:\n```python\n-self._start_stage(stage_label, tasks)\n+self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))\n```\n\nIn `run_pipeline`, after the final result assembly:\n```python\n+# Signal workers to shut down now that all stages are complete.\n+self.shutdown()\n```\n\nIn `start_stage` (public):\n```python\n-def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n     \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n-    self._start_stage(stage_name, tasks)\n+    self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)\n```\n\n**`lib/zephyr/tests/test_execution.py`:**\n\nThe new test `test_pull_task_returns_shutdown_on_last_stage_empty_queue`.\n\n---\n\nWant me to produce a clean patch file with just these changes?"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":39331,"cache_read_input_tokens":23218,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":39331},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtkeurLRVn2HCR2Wj2i","type":"assistant","uuid":"fa8cf349-269c-427b-8bc2-95aee5c03545","timestamp":"2026-02-23T23:47:40.338Z"}
{"parentUuid":"fa8cf349-269c-427b-8bc2-95aee5c03545","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"entire hooks claude-code stop"},"parentToolUseID":"5f4137dd-c032-4e42-be1a-8e842caf3a9f","toolUseID":"5f4137dd-c032-4e42-be1a-8e842caf3a9f","timestamp":"2026-02-23T23:47:40.441Z","uuid":"27550dd6-703e-4573-b71c-0e0d8ca90e70"}
{"parentUuid":"27550dd6-703e-4573-b71c-0e0d8ca90e70","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"system","subtype":"stop_hook_summary","hookCount":1,"hookInfos":[{"command":"entire hooks claude-code stop","durationMs":2484}],"hookErrors":[],"preventedContinuation":false,"stopReason":"","hasOutput":true,"level":"suggestion","timestamp":"2026-02-23T23:47:42.925Z","uuid":"1a60ab4d-5eae-4d27-aa56-437afbf9fc78","toolUseID":"5f4137dd-c032-4e42-be1a-8e842caf3a9f"}
{"parentUuid":"1a60ab4d-5eae-4d27-aa56-437afbf9fc78","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"system","subtype":"turn_duration","durationMs":35426,"timestamp":"2026-02-23T23:47:42.927Z","uuid":"23d6e565-df39-4f2e-b70f-19e1a9f1b8d5","isMeta":false}
{"type":"file-history-snapshot","messageId":"215cd644-6ea1-4f1a-a834-3de8a4126e9b","snapshot":{"messageId":"215cd644-6ea1-4f1a-a834-3de8a4126e9b","trackedFileBackups":{},"timestamp":"2026-02-23T23:48:18.496Z"},"isSnapshotUpdate":false}
{"parentUuid":"23d6e565-df39-4f2e-b70f-19e1a9f1b8d5","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","type":"user","message":{"role":"user","content":"yes"},"uuid":"215cd644-6ea1-4f1a-a834-3de8a4126e9b","timestamp":"2026-02-23T23:48:14.463Z","permissionMode":"acceptEdits"}
{"parentUuid":"215cd644-6ea1-4f1a-a834-3de8a4126e9b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01E6rAkrenPbKTVN8y8Tuars","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HYQaWf6bPrdG1fCL9y6cVA","name":"Read","input":{"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1796,"cache_read_input_tokens":62549,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1796},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtqtjSg3J4kHC7BcuUh","type":"assistant","uuid":"5fbe99d2-8cfb-4dd2-ba77-8234459e7264","timestamp":"2026-02-23T23:48:22.997Z"}
{"parentUuid":"5fbe99d2-8cfb-4dd2-ba77-8234459e7264","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01HYQaWf6bPrdG1fCL9y6cVA","toolUseID":"toolu_01HYQaWf6bPrdG1fCL9y6cVA","timestamp":"2026-02-23T23:48:23.152Z","uuid":"e7cda979-0659-4365-aeb9-9411521cac9e"}
{"parentUuid":"5fbe99d2-8cfb-4dd2-ba77-8234459e7264","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HYQaWf6bPrdG1fCL9y6cVA","type":"tool_result","content":"     1→# Copyright 2025 The Marin Authors\n     2→# SPDX-License-Identifier: Apache-2.0\n     3→\n     4→\"\"\"Actor-based execution engine for Zephyr pipelines.\n     5→\n     6→Workers pull tasks from the coordinator, execute shard operations, and report\n     7→results back. This enables persistent worker state (caches, loaded models),\n     8→transient error recovery, and backend-agnostic dispatch via fray v2's Client\n     9→protocol.\n    10→\"\"\"\n    11→\n    12→from __future__ import annotations\n    13→import enum\n    14→import logging\n    15→import os\n    16→import pickle\n    17→import threading\n    18→import time\n    19→import uuid\n    20→from collections import defaultdict, deque\n    21→from collections.abc import Iterator, Sequence\n    22→from contextlib import suppress\n    23→from contextvars import ContextVar\n    24→from dataclasses import dataclass, field\n    25→from typing import Any, Protocol\n    26→\n    27→import fsspec\n    28→from fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\n    29→from iris.temp_buckets import get_temp_bucket_path\n    30→from iris.time_utils import ExponentialBackoff\n    31→\n    32→from zephyr.dataset import Dataset\n    33→from zephyr.plan import (\n    34→    ExecutionHint,\n    35→    Join,\n    36→    PhysicalOp,\n    37→    PhysicalPlan,\n    38→    SourceItem,\n    39→    StageContext,\n    40→    StageType,\n    41→    compute_plan,\n    42→    run_stage,\n    43→)\n    44→from zephyr.writers import ensure_parent_dir\n    45→\n    46→logger = logging.getLogger(__name__)\n    47→\n    48→\n    49→class Chunk(Protocol):\n    50→    def __iter__(self) -> Iterator: ...\n    51→\n    52→\n    53→@dataclass(frozen=True)\n    54→class DiskChunk:\n    55→    \"\"\"Reference to a chunk stored on disk.\n    56→\n    57→    Each write goes to a UUID-unique path to avoid collisions when multiple\n    58→    workers race on the same shard.  No coordinator-side rename is needed;\n    59→    the winning result's paths are used directly and the entire execution\n    60→    directory is cleaned up after the pipeline completes.\n    61→    \"\"\"\n    62→\n    63→    path: str\n    64→    count: int\n    65→\n    66→    def __iter__(self) -> Iterator:\n    67→        return iter(self.read())\n    68→\n    69→    @classmethod\n    70→    def write(cls, path: str, data: list) -> DiskChunk:\n    71→        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n    72→\n    73→        The UUID suffix avoids collisions when multiple workers race on\n    74→        the same shard.  The resulting path is used directly for reads —\n    75→        no rename step is required.\n    76→        \"\"\"\n    77→        from zephyr.writers import unique_temp_path\n    78→\n    79→        ensure_parent_dir(path)\n    80→        data = list(data)\n    81→        count = len(data)\n    82→\n    83→        unique_path = unique_temp_path(path)\n    84→        with fsspec.open(unique_path, \"wb\") as f:\n    85→            pickle.dump(data, f)\n    86→        return cls(path=unique_path, count=count)\n    87→\n    88→    def read(self) -> list:\n    89→        \"\"\"Load chunk data from disk.\"\"\"\n    90→        with fsspec.open(self.path, \"rb\") as f:\n    91→            return pickle.load(f)\n    92→\n    93→\n    94→@dataclass\n    95→class MemChunk:\n    96→    \"\"\"In-memory chunk.\"\"\"\n    97→\n    98→    items: list[Any]\n    99→\n   100→    def __iter__(self) -> Iterator:\n   101→        return iter(self.items)\n   102→\n   103→\n   104→@dataclass\n   105→class Shard:\n   106→    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n   107→\n   108→    chunks: list[Chunk]\n   109→\n   110→    def __iter__(self) -> Iterator:\n   111→        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n   112→        for chunk in self.chunks:\n   113→            yield from chunk\n   114→\n   115→    def iter_chunks(self) -> Iterator[list]:\n   116→        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n   117→        for chunk in self.chunks:\n   118→            yield list(chunk)\n   119→\n   120→\n   121→@dataclass\n   122→class ResultChunk:\n   123→    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n   124→\n   125→    source_shard: int\n   126→    target_shard: int\n   127→    data: Chunk\n   128→\n   129→\n   130→@dataclass\n   131→class TaskResult:\n   132→    \"\"\"Result of a single task.\"\"\"\n   133→\n   134→    chunks: list[ResultChunk]\n   135→\n   136→\n   137→def _generate_execution_id() -> str:\n   138→    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n   139→    return uuid.uuid4().hex[:12]\n   140→\n   141→\n   142→def _chunk_path(\n   143→    prefix: str,\n   144→    execution_id: str,\n   145→    stage_name: str,\n   146→    shard_idx: int,\n   147→    chunk_idx: int,\n   148→) -> str:\n   149→    \"\"\"Generate path for a chunk file.\n   150→\n   151→    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n   152→    \"\"\"\n   153→    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n   154→\n   155→\n   156→def _cleanup_execution(prefix: str, execution_id: str) -> None:\n   157→    \"\"\"Remove all chunk files for an execution.\"\"\"\n   158→    exec_dir = f\"{prefix}/{execution_id}\"\n   159→    fs = fsspec.core.url_to_fs(exec_dir)[0]\n   160→\n   161→    if fs.exists(exec_dir):\n   162→        try:\n   163→            fs.rm(exec_dir, recursive=True)\n   164→        except Exception as e:\n   165→            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n   166→\n   167→\n   168→class WorkerState(enum.Enum):\n   169→    INIT = \"init\"\n   170→    READY = \"ready\"\n   171→    BUSY = \"busy\"\n   172→    FAILED = \"failed\"\n   173→    DEAD = \"dead\"\n   174→\n   175→\n   176→@dataclass\n   177→class ShardTask:\n   178→    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n   179→\n   180→    shard_idx: int\n   181→    total_shards: int\n   182→    chunk_size: int\n   183→    shard: Shard\n   184→    operations: list[PhysicalOp]\n   185→    stage_name: str = \"output\"\n   186→    aux_shards: dict[int, Shard] | None = None\n   187→\n   188→\n   189→class ZephyrWorkerError(RuntimeError):\n   190→    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n   191→\n   192→\n   193→# Application errors that should never be retried by the execute() retry loop.\n   194→# These are deterministic errors (bad plan, invalid config, programming bugs)\n   195→# that would fail identically on every attempt. Infrastructure errors (OSError,\n   196→# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n   197→# remain retryable.\n   198→_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n   199→\n   200→\n   201→# ---------------------------------------------------------------------------\n   202→# WorkerContext protocol — the public interface exposed to user task code\n   203→# ---------------------------------------------------------------------------\n   204→\n   205→\n   206→class WorkerContext(Protocol):\n   207→    def get_shared(self, name: str) -> Any: ...\n   208→\n   209→\n   210→_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n   211→\n   212→\n   213→def zephyr_worker_ctx() -> WorkerContext:\n   214→    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n   215→    ctx = _worker_ctx_var.get()\n   216→    if ctx is None:\n   217→        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n   218→    return ctx\n   219→\n   220→\n   221→# ---------------------------------------------------------------------------\n   222→# ZephyrCoordinator\n   223→# ---------------------------------------------------------------------------\n   224→\n   225→\n   226→@dataclass\n   227→class JobStatus:\n   228→    stage: str\n   229→    completed: int\n   230→    total: int\n   231→    retries: int\n   232→    in_flight: int\n   233→    queue_depth: int\n   234→    done: bool\n   235→    fatal_error: str\n   236→    workers: dict[str, dict[str, Any]]\n   237→\n   238→\n   239→class ZephyrCoordinator:\n   240→    \"\"\"Central coordinator actor that owns and manages the worker pool.\n   241→\n   242→    The coordinator creates workers via current_client(), runs a background\n   243→    loop for discovery and heartbeat checking, and manages all pipeline\n   244→    execution internally. Workers poll the coordinator for tasks until\n   245→    receiving a SHUTDOWN signal.\n   246→    \"\"\"\n   247→\n   248→    def __init__(self):\n   249→        # Task management state\n   250→        self._task_queue: deque[ShardTask] = deque()\n   251→        self._results: dict[int, TaskResult] = {}\n   252→        self._worker_states: dict[str, WorkerState] = {}\n   253→        self._last_seen: dict[str, float] = {}\n   254→        self._shared_data: dict[str, Any] = {}\n   255→        self._stage_name: str = \"\"\n   256→        self._total_shards: int = 0\n   257→        self._completed_shards: int = 0\n   258→        self._retries: int = 0\n   259→        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n   260→        self._task_attempts: dict[int, int] = {}\n   261→        self._fatal_error: str | None = None\n   262→        self._chunk_prefix: str = \"\"\n   263→        self._execution_id: str = \"\"\n   264→        self._no_workers_timeout: float = 60.0\n   265→\n   266→        # Worker management state (workers self-register via register_worker)\n   267→        self._worker_handles: dict[str, ActorHandle] = {}\n   268→        self._coordinator_thread: threading.Thread | None = None\n   269→        self._shutdown: bool = False\n   270→        self._initialized: bool = False\n   271→\n   272→        # Lock for accessing coordinator state from background thread\n   273→        self._lock = threading.Lock()\n   274→\n   275→    def initialize(\n   276→        self,\n   277→        chunk_prefix: str,\n   278→        coordinator_handle: ActorHandle,\n   279→        no_workers_timeout: float = 60.0,\n   280→    ) -> None:\n   281→        \"\"\"Initialize coordinator for push-based worker registration.\n   282→\n   283→        Workers register themselves by calling register_worker() when they start.\n   284→        This eliminates polling overhead and log spam from discover_new() calls.\n   285→\n   286→        Args:\n   287→            chunk_prefix: Storage prefix for intermediate chunks\n   288→            coordinator_handle: Handle to this coordinator actor (passed from context)\n   289→            no_workers_timeout: Seconds to wait for at least one worker before failing.\n   290→        \"\"\"\n   291→        self._chunk_prefix = chunk_prefix\n   292→        self._self_handle = coordinator_handle\n   293→        self._no_workers_timeout = no_workers_timeout\n   294→\n   295→        logger.info(\"Coordinator initialized\")\n   296→\n   297→        # Start coordinator background loop (heartbeat checking only)\n   298→        self._coordinator_thread = threading.Thread(\n   299→            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n   300→        )\n   301→        self._coordinator_thread.start()\n   302→        self._initialized = True\n   303→\n   304→    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n   305→        \"\"\"Called by workers when they come online to register with coordinator.\n   306→\n   307→        Handles re-registration from reconstructed workers (e.g. after node\n   308→        preemption) by updating the stale handle and resetting worker state.\n   309→        \"\"\"\n   310→        with self._lock:\n   311→            if worker_id in self._worker_handles:\n   312→                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n   313→                self._worker_handles[worker_id] = worker_handle\n   314→                self._worker_states[worker_id] = WorkerState.READY\n   315→                self._last_seen[worker_id] = time.monotonic()\n   316→                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n   317→                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n   318→                # lose tasks.\n   319→                self._maybe_requeue_worker_task(worker_id)\n   320→                return\n   321→\n   322→            self._worker_handles[worker_id] = worker_handle\n   323→            self._worker_states[worker_id] = WorkerState.READY\n   324→            self._last_seen[worker_id] = time.monotonic()\n   325→\n   326→            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n   327→\n   328→    def _coordinator_loop(self) -> None:\n   329→        \"\"\"Background loop for heartbeat checking only.\n   330→\n   331→        Workers register themselves via register_worker() - no polling needed.\n   332→        \"\"\"\n   333→        last_log_time = 0.0\n   334→\n   335→        while not self._shutdown:\n   336→            # Check heartbeats, re-queue stale tasks.\n   337→            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n   338→            # serialize this with worker RPCs (pull_task, report_result) via the\n   339→            # concurrency queue instead of running it inline.\n   340→            self.check_heartbeats()\n   341→\n   342→            # Log status periodically during active execution\n   343→            now = time.monotonic()\n   344→            if self._has_active_execution() and now - last_log_time > 5.0:\n   345→                self._log_status()\n   346→                last_log_time = now\n   347→\n   348→            time.sleep(0.5)\n   349→\n   350→    def _has_active_execution(self) -> bool:\n   351→        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n   352→\n   353→    def _log_status(self) -> None:\n   354→        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n   355→        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n   356→        logger.info(\n   357→            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n   358→            self._stage_name,\n   359→            self._completed_shards,\n   360→            self._total_shards,\n   361→            len(self._in_flight),\n   362→            len(self._task_queue),\n   363→            alive,\n   364→            len(self._worker_handles),\n   365→            dead,\n   366→        )\n   367→\n   368→    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n   369→        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n   370→        task_and_attempt = self._in_flight.pop(worker_id, None)\n   371→        if task_and_attempt is not None:\n   372→            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n   373→            task, _old_attempt = task_and_attempt\n   374→            self._task_attempts[task.shard_idx] += 1\n   375→            self._task_queue.append(task)\n   376→            self._retries += 1\n   377→\n   378→    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n   379→        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n   380→        now = time.monotonic()\n   381→        for worker_id, last in list(self._last_seen.items()):\n   382→            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n   383→                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n   384→                self._worker_states[worker_id] = WorkerState.FAILED\n   385→                self._maybe_requeue_worker_task(worker_id)\n   386→\n   387→    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n   388→        \"\"\"Called by workers to get next task.\n   389→\n   390→        Returns:\n   391→            - (task, attempt, config) tuple if task available\n   392→            - \"SHUTDOWN\" string if coordinator is shutting down\n   393→            - None if no task available (worker should backoff and retry)\n   394→        \"\"\"\n   395→        with self._lock:\n   396→            self._last_seen[worker_id] = time.monotonic()\n   397→            self._worker_states[worker_id] = WorkerState.READY\n   398→\n   399→            if self._shutdown:\n   400→                return \"SHUTDOWN\"\n   401→\n   402→            if self._fatal_error:\n   403→                return None\n   404→\n   405→            if not self._task_queue:\n   406→                return None\n   407→\n   408→            task = self._task_queue.popleft()\n   409→            attempt = self._task_attempts[task.shard_idx]\n   410→            self._in_flight[worker_id] = (task, attempt)\n   411→            self._worker_states[worker_id] = WorkerState.BUSY\n   412→\n   413→            config = {\n   414→                \"shared_data\": self._shared_data,\n   415→                \"chunk_prefix\": self._chunk_prefix,\n   416→                \"execution_id\": self._execution_id,\n   417→            }\n   418→            return (task, attempt, config)\n   419→\n   420→    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n   421→        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n   422→\n   423→        Workers block on report_result/report_error before calling pull_task,\n   424→        so _in_flight can never point to a different shard. It may be absent\n   425→        if a heartbeat timeout already re-queued the task.\n   426→        \"\"\"\n   427→        in_flight = self._in_flight.get(worker_id)\n   428→        if in_flight is not None:\n   429→            assert in_flight[0].shard_idx == shard_idx, (\n   430→                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n   431→                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n   432→                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n   433→            )\n   434→\n   435→    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n   436→        with self._lock:\n   437→            self._last_seen[worker_id] = time.monotonic()\n   438→            self._assert_in_flight_consistent(worker_id, shard_idx)\n   439→\n   440→            current_attempt = self._task_attempts.get(shard_idx, 0)\n   441→            if attempt != current_attempt:\n   442→                logger.warning(\n   443→                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n   444→                    f\"(attempt {attempt}, current {current_attempt})\"\n   445→                )\n   446→                return\n   447→\n   448→            self._results[shard_idx] = result\n   449→            self._completed_shards += 1\n   450→            self._in_flight.pop(worker_id, None)\n   451→            self._worker_states[worker_id] = WorkerState.READY\n   452→\n   453→    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n   454→        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n   455→        with self._lock:\n   456→            self._last_seen[worker_id] = time.monotonic()\n   457→            self._assert_in_flight_consistent(worker_id, shard_idx)\n   458→            self._in_flight.pop(worker_id, None)\n   459→            self._fatal_error = error_info\n   460→            self._worker_states[worker_id] = WorkerState.DEAD\n   461→\n   462→    def heartbeat(self, worker_id: str) -> None:\n   463→        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n   464→        # (which holds the lock), and monotonic float writes are atomic on CPython.\n   465→        self._last_seen[worker_id] = time.monotonic()\n   466→\n   467→    def get_status(self) -> JobStatus:\n   468→        with self._lock:\n   469→            return JobStatus(\n   470→                stage=self._stage_name,\n   471→                completed=self._completed_shards,\n   472→                total=self._total_shards,\n   473→                retries=self._retries,\n   474→                in_flight=len(self._in_flight),\n   475→                queue_depth=len(self._task_queue),\n   476→                done=self._shutdown,\n   477→                fatal_error=self._fatal_error,\n   478→                workers={\n   479→                    wid: {\n   480→                        \"state\": state.value,\n   481→                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n   482→                    }\n   483→                    for wid, state in self._worker_states.items()\n   484→                },\n   485→            )\n   486→\n   487→    def get_fatal_error(self) -> str | None:\n   488→        with self._lock:\n   489→            return self._fatal_error\n   490→\n   491→    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n   492→        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n   493→        with self._lock:\n   494→            self._task_queue = deque(tasks)\n   495→            self._results = {}\n   496→            self._stage_name = stage_name\n   497→            self._total_shards = len(tasks)\n   498→            self._completed_shards = 0\n   499→            self._retries = 0\n   500→            self._in_flight = {}\n   501→            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n   502→            self._fatal_error = None\n   503→\n   504→    def _wait_for_stage(self) -> None:\n   505→        \"\"\"Block until current stage completes or error occurs.\"\"\"\n   506→        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n   507→        last_log_completed = -1\n   508→        start_time = time.monotonic()\n   509→        all_dead_since: float | None = None\n   510→        no_workers_timeout = self._no_workers_timeout\n   511→\n   512→        while True:\n   513→            with self._lock:\n   514→                if self._fatal_error:\n   515→                    raise ZephyrWorkerError(self._fatal_error)\n   516→\n   517→                completed = self._completed_shards\n   518→                total = self._total_shards\n   519→\n   520→                if completed >= total:\n   521→                    return\n   522→\n   523→                # Count alive workers (READY or BUSY), not just total registered.\n   524→                # Dead/failed workers stay in _worker_handles but can't make progress.\n   525→                alive_workers = sum(\n   526→                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n   527→                )\n   528→\n   529→                if alive_workers == 0:\n   530→                    now = time.monotonic()\n   531→                    elapsed = now - start_time\n   532→\n   533→                    if all_dead_since is None:\n   534→                        all_dead_since = now\n   535→                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n   536→\n   537→                    dead_duration = now - all_dead_since\n   538→                    if dead_duration > no_workers_timeout:\n   539→                        raise ZephyrWorkerError(\n   540→                            f\"No alive workers for {dead_duration:.1f}s \"\n   541→                            f\"(total elapsed {elapsed:.1f}s). \"\n   542→                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n   543→                            \"Check cluster resources and worker group configuration.\"\n   544→                        )\n   545→                else:\n   546→                    # Workers are alive — reset the dead timer\n   547→                    all_dead_since = None\n   548→\n   549→            if completed != last_log_completed:\n   550→                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n   551→                last_log_completed = completed\n   552→                backoff.reset()\n   553→\n   554→            time.sleep(backoff.next_interval())\n   555→\n   556→    def _collect_results(self) -> dict[int, TaskResult]:\n   557→        \"\"\"Return results for the completed stage.\"\"\"\n   558→        with self._lock:\n   559→            return dict(self._results)\n   560→\n   561→    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n   562→        \"\"\"Set config for the current execution.\"\"\"\n   563→        with self._lock:\n   564→            self._shared_data = shared_data\n   565→            self._execution_id = execution_id\n   566→\n   567→    def run_pipeline(\n   568→        self,\n   569→        plan: PhysicalPlan,\n   570→        shared_data: dict[str, Any],\n   571→        execution_id: str,\n   572→        hints: ExecutionHint,\n   573→    ) -> list:\n   574→        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n   575→        with self._lock:\n   576→            self._shared_data = shared_data\n   577→            self._execution_id = execution_id\n   578→\n   579→        shards = _build_source_shards(plan.source_items)\n   580→        if not shards:\n   581→            return []\n   582→\n   583→        for stage_idx, stage in enumerate(plan.stages):\n   584→            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n   585→\n   586→            if stage.stage_type == StageType.RESHARD:\n   587→                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n   588→                continue\n   589→\n   590→            # Compute aux data for joins\n   591→            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n   592→\n   593→            # Build and submit tasks\n   594→            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n   595→            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n   596→            self._start_stage(stage_label, tasks)\n   597→\n   598→            # Wait for stage completion\n   599→            self._wait_for_stage()\n   600→\n   601→            # Collect and regroup results for next stage\n   602→            result_refs = self._collect_results()\n   603→            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n   604→\n   605→        # Flatten final results\n   606→        flat_result = []\n   607→        for shard in shards:\n   608→            for chunk in shard.chunks:\n   609→                flat_result.extend(list(chunk))\n   610→\n   611→        # Signal workers to shut down now that all stages are complete.\n   612→        self.shutdown()\n   613→\n   614→        return flat_result\n   615→\n   616→    def _compute_join_aux(\n   617→        self,\n   618→        operations: list[PhysicalOp],\n   619→        shard_refs: list[Shard],\n   620→        hints: ExecutionHint,\n   621→        parent_stage_idx: int,\n   622→    ) -> list[dict[int, Shard]] | None:\n   623→        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n   624→        all_right_shard_refs: dict[int, list[Shard]] = {}\n   625→\n   626→        for i, op in enumerate(operations):\n   627→            if not isinstance(op, Join) or op.right_plan is None:\n   628→                continue\n   629→\n   630→            right_refs = _build_source_shards(op.right_plan.source_items)\n   631→\n   632→            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n   633→                if right_stage.stage_type == StageType.RESHARD:\n   634→                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n   635→                    continue\n   636→\n   637→                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n   638→                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n   639→                self._start_stage(join_stage_label, right_tasks)\n   640→                self._wait_for_stage()\n   641→                raw = self._collect_results()\n   642→                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n   643→\n   644→            if len(shard_refs) != len(right_refs):\n   645→                raise ValueError(\n   646→                    f\"Sorted merge join requires equal shard counts. \"\n   647→                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n   648→                )\n   649→            all_right_shard_refs[i] = right_refs\n   650→\n   651→        if not all_right_shard_refs:\n   652→            return None\n   653→\n   654→        return [\n   655→            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n   656→            for shard_idx in range(len(shard_refs))\n   657→        ]\n   658→\n   659→    def shutdown(self) -> None:\n   660→        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n   661→        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n   662→        self._shutdown = True\n   663→\n   664→        # Wait for coordinator thread to exit\n   665→        if self._coordinator_thread is not None:\n   666→            self._coordinator_thread.join(timeout=5.0)\n   667→\n   668→        logger.info(\"Coordinator shutdown complete\")\n   669→\n   670→    # Legacy compatibility methods (used by tests)\n   671→    def set_shared_data(self, data: dict[str, Any]) -> None:\n   672→        with self._lock:\n   673→            self._shared_data = data\n   674→\n   675→    def get_shared_data(self) -> dict[str, Any]:\n   676→        with self._lock:\n   677→            return self._shared_data\n   678→\n   679→    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n   680→        \"\"\"Configure chunk storage for this execution.\"\"\"\n   681→        with self._lock:\n   682→            self._chunk_prefix = prefix\n   683→            self._execution_id = execution_id\n   684→\n   685→    def get_chunk_config(self) -> dict:\n   686→        \"\"\"Return chunk storage configuration for workers.\"\"\"\n   687→        with self._lock:\n   688→            return {\n   689→                \"prefix\": self._chunk_prefix,\n   690→                \"execution_id\": self._execution_id,\n   691→            }\n   692→\n   693→    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n   694→        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n   695→        self._start_stage(stage_name, tasks)\n   696→\n   697→    def check_heartbeats(self, timeout: float = 30.0) -> None:\n   698→        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n   699→        with self._lock:\n   700→            self._check_worker_heartbeats(timeout)\n   701→\n   702→    def collect_results(self) -> dict[int, TaskResult]:\n   703→        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n   704→        return self._collect_results()\n   705→\n   706→    def signal_done(self) -> None:\n   707→        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n   708→        self._shutdown = True\n   709→\n   710→\n   711→# ---------------------------------------------------------------------------\n   712→# ZephyrWorker\n   713→# ---------------------------------------------------------------------------\n   714→\n   715→\n   716→class ZephyrWorker:\n   717→    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n   718→\n   719→    Workers register themselves with the coordinator on startup via\n   720→    register_worker(), then poll continuously until receiving a SHUTDOWN\n   721→    signal. Each task includes the execution config (shared data, chunk\n   722→    prefix, execution ID), so workers can handle multiple executions\n   723→    without restart.\n   724→    \"\"\"\n   725→\n   726→    def __init__(self, coordinator_handle: ActorHandle):\n   727→        from fray.v2 import current_actor\n   728→\n   729→        self._coordinator = coordinator_handle\n   730→        self._shared_data: dict[str, Any] = {}\n   731→        self._shutdown_event = threading.Event()\n   732→        self._chunk_prefix: str = \"\"\n   733→        self._execution_id: str = \"\"\n   734→\n   735→        # Build descriptive worker ID from actor context\n   736→        actor_ctx = current_actor()\n   737→        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n   738→\n   739→        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n   740→        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n   741→\n   742→        # Start polling in a background thread\n   743→        self._polling_thread = threading.Thread(\n   744→            target=self._run_polling,\n   745→            args=(coordinator_handle,),\n   746→            daemon=True,\n   747→            name=f\"zephyr-poll-{self._worker_id}\",\n   748→        )\n   749→        self._polling_thread.start()\n   750→\n   751→    def get_shared(self, name: str) -> Any:\n   752→        return self._shared_data[name]\n   753→\n   754→    def _run_polling(self, coordinator: ActorHandle) -> None:\n   755→        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n   756→        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n   757→\n   758→        # Start heartbeat thread\n   759→        heartbeat_thread = threading.Thread(\n   760→            target=self._heartbeat_loop,\n   761→            args=(coordinator,),\n   762→            daemon=True,\n   763→            name=f\"zephyr-hb-{self._worker_id}\",\n   764→        )\n   765→        heartbeat_thread.start()\n   766→\n   767→        try:\n   768→            self._poll_loop(coordinator)\n   769→        finally:\n   770→            self._shutdown_event.set()\n   771→            heartbeat_thread.join(timeout=5.0)\n   772→            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n   773→\n   774→    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n   775→        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n   776→        heartbeat_count = 0\n   777→        while not self._shutdown_event.is_set():\n   778→            try:\n   779→                # Block on result to avoid congesting the coordinator RPC pipe\n   780→                # with fire-and-forget heartbeats.\n   781→                coordinator.heartbeat.remote(self._worker_id).result()\n   782→                heartbeat_count += 1\n   783→                if heartbeat_count % 10 == 1:\n   784→                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n   785→            except Exception as e:\n   786→                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n   787→            self._shutdown_event.wait(timeout=interval)\n   788→        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n   789→\n   790→    def _poll_loop(self, coordinator: ActorHandle) -> None:\n   791→        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n   792→        loop_count = 0\n   793→        task_count = 0\n   794→        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n   795→\n   796→        while not self._shutdown_event.is_set():\n   797→            loop_count += 1\n   798→            if loop_count % 100 == 1:\n   799→                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n   800→\n   801→            try:\n   802→                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n   803→            except Exception as e:\n   804→                # Coordinator is dead or unreachable - exit gracefully\n   805→                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n   806→                break\n   807→\n   808→            # SHUTDOWN signal - exit cleanly\n   809→            if response == \"SHUTDOWN\":\n   810→                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n   811→                break\n   812→\n   813→            # No task available - backoff and retry\n   814→            if response is None:\n   815→                time.sleep(backoff.next_interval())\n   816→                continue\n   817→\n   818→            backoff.reset()\n   819→\n   820→            # Unpack task and config\n   821→            task, attempt, config = response\n   822→\n   823→            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n   824→            try:\n   825→                result = self._execute_shard(task, config)\n   826→                # Block until coordinator records the result. This ensures\n   827→                # report_result is fully processed before the next pull_task,\n   828→                # preventing _in_flight tracking races.\n   829→                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n   830→                task_count += 1\n   831→            except Exception as e:\n   832→                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n   833→                import traceback\n   834→\n   835→                coordinator.report_error.remote(\n   836→                    self._worker_id,\n   837→                    task.shard_idx,\n   838→                    \"\".join(traceback.format_exc()),\n   839→                ).result()\n   840→\n   841→    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n   842→        \"\"\"Execute a stage's operations on a single shard.\n   843→\n   844→        Returns list[TaskResult].\n   845→        \"\"\"\n   846→        # Update config for this execution\n   847→        self._shared_data = config[\"shared_data\"]\n   848→        self._chunk_prefix = config[\"chunk_prefix\"]\n   849→        self._execution_id = config[\"execution_id\"]\n   850→\n   851→        _worker_ctx_var.set(self)\n   852→\n   853→        logger.info(\n   854→            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n   855→            task.shard_idx,\n   856→            task.total_shards,\n   857→            task.stage_name,\n   858→            len(task.shard.chunks),\n   859→            len(task.operations),\n   860→        )\n   861→\n   862→        stage_ctx = StageContext(\n   863→            shard=task.shard,\n   864→            shard_idx=task.shard_idx,\n   865→            total_shards=task.total_shards,\n   866→            chunk_size=task.chunk_size,\n   867→            aux_shards=task.aux_shards,\n   868→        )\n   869→\n   870→        results: list[ResultChunk] = []\n   871→        chunk_idx = 0\n   872→\n   873→        for stage_output in run_stage(stage_ctx, task.operations):\n   874→            chunk_path = _chunk_path(\n   875→                self._chunk_prefix,\n   876→                self._execution_id,\n   877→                task.stage_name,\n   878→                task.shard_idx,\n   879→                chunk_idx,\n   880→            )\n   881→            chunk = list(stage_output.chunk)\n   882→            chunk_ref = DiskChunk.write(chunk_path, chunk)\n   883→            results.append(\n   884→                ResultChunk(\n   885→                    source_shard=stage_output.source_shard,\n   886→                    target_shard=stage_output.target_shard,\n   887→                    data=chunk_ref,\n   888→                )\n   889→            )\n   890→            chunk_idx += 1\n   891→            if chunk_idx % 10 == 0:\n   892→                logger.info(\n   893→                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n   894→                    task.shard_idx,\n   895→                    chunk_idx,\n   896→                    len(stage_output.chunk),\n   897→                )\n   898→\n   899→        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n   900→        return TaskResult(chunks=results)\n   901→\n   902→    def shutdown(self) -> None:\n   903→        \"\"\"Stop the worker's polling loop.\"\"\"\n   904→        self._shutdown_event.set()\n   905→        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n   906→            self._polling_thread.join(timeout=5.0)\n   907→\n   908→\n   909→def _regroup_result_refs(\n   910→    result_refs: dict[int, TaskResult],\n   911→    input_shard_count: int,\n   912→    output_shard_count: int | None = None,\n   913→) -> list[Shard]:\n   914→    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n   915→    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n   916→\n   917→    for _input_idx, result in result_refs.items():\n   918→        for chunk in result.chunks:\n   919→            output_by_shard[chunk.target_shard].append(chunk.data)\n   920→\n   921→    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n   922→    if output_shard_count is not None:\n   923→        num_output = max(num_output, output_shard_count)\n   924→    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n   925→\n   926→\n   927→@dataclass\n   928→class ZephyrContext:\n   929→    \"\"\"Execution context for Zephyr pipelines.\n   930→\n   931→    Each execute() call creates a fresh coordinator and worker pool, runs\n   932→    the pipeline, then tears everything down. Workers are sized to\n   933→    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n   934→    data broadcast via put() is delivered to workers with each task.\n   935→\n   936→    Args:\n   937→        client: The fray client to use. If None, auto-detects using current_client().\n   938→        max_workers: Upper bound on worker count. The actual count is\n   939→            min(max_workers, num_shards), computed at first execute(). If None,\n   940→            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n   941→        resources: Resource config per worker.\n   942→        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n   943→            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n   944→        name: Descriptive name for this context, used in actor group names for debugging.\n   945→            Defaults to a random 8-character hex string.\n   946→        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n   947→            Defaults to 600s.\n   948→        max_execution_retries: Maximum number of times to retry a pipeline execution after\n   949→            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n   950→            (ZephyrWorkerError) are never retried. Defaults to 3.\n   951→    \"\"\"\n   952→\n   953→    client: Client | None = None\n   954→    max_workers: int | None = None\n   955→    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n   956→    chunk_storage_prefix: str | None = None\n   957→    name: str = \"\"\n   958→    no_workers_timeout: float | None = None\n   959→    max_execution_retries: int = 3\n   960→\n   961→    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n   962→    _coordinator: ActorHandle | None = field(default=None, repr=False)\n   963→    _coordinator_group: Any = field(default=None, repr=False)\n   964→    _worker_group: Any = field(default=None, repr=False)\n   965→    _worker_count: int = field(default=0, repr=False)\n   966→    # NOTE: execute calls increment this at the very beginning\n   967→    _pipeline_id: int = field(default=-1, repr=False)\n   968→\n   969→    def __post_init__(self):\n   970→        if self.client is None:\n   971→            from fray.v2.client import current_client\n   972→\n   973→            self.client = current_client()\n   974→\n   975→        if self.max_workers is None:\n   976→            from fray.v2.local_backend import LocalClient\n   977→\n   978→            if isinstance(self.client, LocalClient):\n   979→                self.max_workers = os.cpu_count() or 1\n   980→            else:\n   981→                # Default to 128 for distributed, but allow override\n   982→                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n   983→                self.max_workers = int(env_val) if env_val else 128\n   984→\n   985→        if self.no_workers_timeout is None:\n   986→            self.no_workers_timeout = 600.0\n   987→\n   988→        if self.chunk_storage_prefix is None:\n   989→            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n   990→            if temp_prefix is None:\n   991→                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n   992→                if not marin_prefix:\n   993→                    raise RuntimeError(\n   994→                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n   995→                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n   996→                    )\n   997→                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n   998→\n   999→            self.chunk_storage_prefix = temp_prefix\n  1000→\n  1001→        # make sure each context is unique\n  1002→        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n  1003→\n  1004→    def put(self, name: str, obj: Any) -> None:\n  1005→        \"\"\"Register shared data to broadcast to all workers.\n  1006→\n  1007→        Must be called before execute(). The object must be picklable.\n  1008→        Workers access it via zephyr_worker_ctx().get_shared(name).\n  1009→        \"\"\"\n  1010→        self._shared_data[name] = obj\n  1011→\n  1012→    def execute(\n  1013→        self,\n  1014→        dataset: Dataset,\n  1015→        hints: ExecutionHint = ExecutionHint(),\n  1016→        verbose: bool = False,\n  1017→        dry_run: bool = False,\n  1018→    ) -> Sequence:\n  1019→        \"\"\"Execute a dataset pipeline.\n  1020→\n  1021→        Each call creates a fresh coordinator and worker pool, runs the\n  1022→        pipeline, then tears everything down. If the coordinator dies\n  1023→        mid-execution (e.g., VM preemption), the pipeline is retried\n  1024→        with fresh actors up to ``max_execution_retries`` times.\n  1025→        Application errors (``ZephyrWorkerError``) are never retried.\n  1026→        \"\"\"\n  1027→        plan = compute_plan(dataset, hints)\n  1028→        if dry_run:\n  1029→            _print_plan(dataset.operations, plan)\n  1030→            return []\n  1031→\n  1032→        # NOTE: pipeline ID incremented on clean completion only\n  1033→        self._pipeline_id += 1\n  1034→        last_exception: Exception | None = None\n  1035→        for attempt in range(self.max_execution_retries + 1):\n  1036→            execution_id = _generate_execution_id()\n  1037→            logger.info(\n  1038→                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n  1039→            )\n  1040→\n  1041→            try:\n  1042→                self._create_coordinator(attempt)\n  1043→                self._create_workers(plan.num_shards, attempt)\n  1044→\n  1045→                # Run pipeline on coordinator (blocking call).\n  1046→                # run_pipeline() calls coordinator.shutdown() at the end,\n  1047→                # which causes workers to receive SHUTDOWN on their next\n  1048→                # pull_task() call.\n  1049→                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n  1050→\n  1051→                return results\n  1052→\n  1053→            except _NON_RETRYABLE_ERRORS:\n  1054→                raise\n  1055→\n  1056→            except Exception as e:\n  1057→                last_exception = e\n  1058→                if attempt >= self.max_execution_retries:\n  1059→                    raise\n  1060→\n  1061→                logger.warning(\n  1062→                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n  1063→                    attempt,\n  1064→                    self.max_execution_retries - attempt,\n  1065→                    e,\n  1066→                )\n  1067→\n  1068→            finally:\n  1069→                # Tear down coordinator and workers for this pipeline\n  1070→                self.shutdown()\n  1071→                # Clean up chunks for this execution\n  1072→                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n  1073→\n  1074→        # Should be unreachable, but just in case\n  1075→        raise last_exception  # type: ignore[misc]\n  1076→\n  1077→    def _create_coordinator(self, attempt: int = 0) -> None:\n  1078→        \"\"\"Create a fresh coordinator actor.\"\"\"\n  1079→        # max_concurrency allows workers to call pull_task/report_result\n  1080→        # while run_pipeline blocks.\n  1081→        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n  1082→        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n  1083→        coordinator_actor_config = ActorConfig(max_concurrency=100)\n  1084→        self._coordinator_group = self.client.create_actor_group(\n  1085→            ZephyrCoordinator,\n  1086→            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n  1087→            count=1,\n  1088→            resources=coordinator_resources,\n  1089→            actor_config=coordinator_actor_config,\n  1090→        )\n  1091→        self._coordinator = self._coordinator_group.wait_ready()[0]\n  1092→\n  1093→        self._coordinator.initialize.remote(\n  1094→            self.chunk_storage_prefix,\n  1095→            self._coordinator,\n  1096→            self.no_workers_timeout,\n  1097→        ).result()\n  1098→\n  1099→        logger.info(\"Coordinator initialized for %s\", self.name)\n  1100→\n  1101→    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n  1102→        \"\"\"Create a fresh worker pool sized to demand.\n  1103→\n  1104→        The worker count is min(max_workers, num_shards) to avoid\n  1105→        over-provisioning when there are fewer shards than the cap.\n  1106→        \"\"\"\n  1107→        if num_shards == 0:\n  1108→            return\n  1109→\n  1110→        assert self.max_workers is not None  # set by __post_init__\n  1111→        actual_workers = min(self.max_workers, num_shards)\n  1112→        logger.info(\n  1113→            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n  1114→            actual_workers,\n  1115→            self.max_workers,\n  1116→            num_shards,\n  1117→            attempt,\n  1118→        )\n  1119→        self._worker_group = self.client.create_actor_group(\n  1120→            ZephyrWorker,\n  1121→            self._coordinator,  # Pass coordinator handle as init arg\n  1122→            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n  1123→            count=actual_workers,\n  1124→            resources=self.resources,\n  1125→        )\n  1126→\n  1127→        self._worker_count = actual_workers\n  1128→\n  1129→        # Wait for at least one worker to be ready before proceeding\n  1130→        self._worker_group.wait_ready(count=1, timeout=3600.0)\n  1131→\n  1132→        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n  1133→\n  1134→    def shutdown(self) -> None:\n  1135→        \"\"\"Shutdown coordinator and all workers.\"\"\"\n  1136→        # Terminate worker group\n  1137→        if self._worker_group is not None:\n  1138→            with suppress(Exception):\n  1139→                self._worker_group.shutdown()\n  1140→\n  1141→        # Terminate coordinator actor group\n  1142→        if self._coordinator_group is not None:\n  1143→            with suppress(Exception):\n  1144→                self._coordinator_group.shutdown()\n  1145→\n  1146→        self._coordinator = None\n  1147→        self._coordinator_group = None\n  1148→        self._worker_group = None\n  1149→\n  1150→\n  1151→def _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n  1152→    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n  1153→    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n  1154→    output_idx = 0\n  1155→    for shard in shards:\n  1156→        for chunk in shard.chunks:\n  1157→            output_by_shard[output_idx].append(chunk)\n  1158→            output_idx = (output_idx + 1) % num_shards\n  1159→    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n  1160→\n  1161→\n  1162→def _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n  1163→    \"\"\"Build shard data from source items.\n  1164→\n  1165→    Each source item becomes a single-element chunk in its assigned shard.\n  1166→    \"\"\"\n  1167→    items_by_shard: dict[int, list] = defaultdict(list)\n  1168→    for item in source_items:\n  1169→        items_by_shard[item.shard_idx].append(item.data)\n  1170→\n  1171→    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n  1172→    shards = []\n  1173→    for i in range(num_shards):\n  1174→        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n  1175→\n  1176→    return shards\n  1177→\n  1178→\n  1179→def _compute_tasks_from_shards(\n  1180→    shard_refs: list[Shard],\n  1181→    stage,\n  1182→    hints: ExecutionHint,\n  1183→    aux_per_shard: list[dict[int, Shard]] | None = None,\n  1184→    stage_name: str | None = None,\n  1185→) -> list[ShardTask]:\n  1186→    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n  1187→    total = len(shard_refs)\n  1188→    tasks = []\n  1189→    output_stage_name = stage_name or stage.stage_name(max_length=60)\n  1190→\n  1191→    for i, shard in enumerate(shard_refs):\n  1192→        aux_shards = None\n  1193→        if aux_per_shard and aux_per_shard[i]:\n  1194→            aux_shards = aux_per_shard[i]\n  1195→\n  1196→        tasks.append(\n  1197→            ShardTask(\n  1198→                shard_idx=i,\n  1199→                total_shards=total,\n  1200→                chunk_size=hints.chunk_size,\n  1201→                shard=shard,\n  1202→                operations=stage.operations,\n  1203→                stage_name=output_stage_name,\n  1204→                aux_shards=aux_shards,\n  1205→            )\n  1206→        )\n  1207→\n  1208→    return tasks\n  1209→\n  1210→\n  1211→def _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n  1212→    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n  1213→    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n  1214→\n  1215→    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n  1216→    logger.info(f\"Shards: {plan.num_shards}\")\n  1217→    logger.info(f\"Original operations: {len(original_ops)}\")\n  1218→    logger.info(f\"Stages: {len(plan.stages)}\")\n  1219→    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n  1220→\n  1221→    logger.info(\"Original pipeline:\")\n  1222→    for i, op in enumerate(original_ops, 1):\n  1223→        logger.info(f\"  {i}. {op}\")\n  1224→\n  1225→    logger.info(\"\\nPhysical stages:\")\n  1226→    for i, stage in enumerate(plan.stages, 1):\n  1227→        stage_desc = stage.stage_name()\n  1228→        hint_parts = []\n  1229→        if stage.stage_type == StageType.RESHARD:\n  1230→            hint_parts.append(f\"reshard→{stage.output_shards}\")\n  1231→        if any(isinstance(op, Join) for op in stage.operations):\n  1232→            hint_parts.append(\"join\")\n  1233→        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n  1234→        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n  1235→\n  1236→    logger.info(\"\\n=== End Plan ===\\n\")\n  1237→"}]},"uuid":"f8773ad7-993a-47e3-a8a3-c00d8eef2ed9","timestamp":"2026-02-23T23:48:23.151Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","content":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","numLines":1237,"startLine":1,"totalLines":1237}},"sourceToolAssistantUUID":"5fbe99d2-8cfb-4dd2-ba77-8234459e7264"}
{"parentUuid":"f8773ad7-993a-47e3-a8a3-c00d8eef2ed9","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01E6rAkrenPbKTVN8y8Tuars","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SGSSpahJeJD8gXbT6SqpeJ","name":"Read","input":{"file_path":"/Users/rav/projects/marin/lib/zephyr/tests/test_execution.py"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1796,"cache_read_input_tokens":62549,"output_tokens":138,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":1796,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"}},"requestId":"req_011CYRtqtjSg3J4kHC7BcuUh","type":"assistant","uuid":"bde36dee-ac40-485e-800b-3c009c11ee4a","timestamp":"2026-02-23T23:48:23.421Z"}
{"parentUuid":"bde36dee-ac40-485e-800b-3c009c11ee4a","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01SGSSpahJeJD8gXbT6SqpeJ","toolUseID":"toolu_01SGSSpahJeJD8gXbT6SqpeJ","timestamp":"2026-02-23T23:48:23.508Z","uuid":"a1b2a471-cd93-4ca4-bfdd-67f1382ee49d"}
{"parentUuid":"bde36dee-ac40-485e-800b-3c009c11ee4a","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01SGSSpahJeJD8gXbT6SqpeJ","type":"tool_result","content":"     1→# Copyright 2025 The Marin Authors\n     2→# SPDX-License-Identifier: Apache-2.0\n     3→\n     4→\"\"\"Tests for the actor-based execution engine (ZephyrContext).\"\"\"\n     5→\n     6→from __future__ import annotations\n     7→\n     8→import json\n     9→import time\n    10→import uuid\n    11→from pathlib import Path\n    12→\n    13→import pytest\n    14→from fray.v2 import ResourceConfig\n    15→from zephyr.dataset import Dataset\n    16→from zephyr.execution import ZephyrContext, zephyr_worker_ctx\n    17→\n    18→\n    19→def test_simple_map(zephyr_ctx):\n    20→    \"\"\"Map pipeline produces correct results.\"\"\"\n    21→    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    22→    results = list(zephyr_ctx.execute(ds))\n    23→    assert sorted(results) == [2, 4, 6]\n    24→\n    25→\n    26→def test_filter(zephyr_ctx):\n    27→    \"\"\"Filter pipeline produces correct results.\"\"\"\n    28→    ds = Dataset.from_list([1, 2, 3, 4, 5]).filter(lambda x: x > 3)\n    29→    results = list(zephyr_ctx.execute(ds))\n    30→    assert sorted(results) == [4, 5]\n    31→\n    32→\n    33→def test_shared_data(fray_client):\n    34→    \"\"\"Workers can access shared data via zephyr_worker_ctx().\"\"\"\n    35→\n    36→    def use_shared(x):\n    37→        multiplier = zephyr_worker_ctx().get_shared(\"multiplier\")\n    38→        return x * multiplier\n    39→\n    40→    zctx = ZephyrContext(\n    41→        client=fray_client,\n    42→        max_workers=1,\n    43→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n    44→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    45→    )\n    46→    zctx.put(\"multiplier\", 10)\n    47→    ds = Dataset.from_list([1, 2, 3]).map(use_shared)\n    48→    results = list(zctx.execute(ds))\n    49→    assert sorted(results) == [10, 20, 30]\n    50→    zctx.shutdown()\n    51→\n    52→\n    53→def test_multi_stage(zephyr_ctx):\n    54→    \"\"\"Multi-stage pipeline (map + filter) works.\"\"\"\n    55→    ds = Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x * 2).filter(lambda x: x > 5)\n    56→    results = list(zephyr_ctx.execute(ds))\n    57→    assert sorted(results) == [6, 8, 10]\n    58→\n    59→\n    60→def test_context_manager(fray_client):\n    61→    \"\"\"ZephyrContext works without context manager.\"\"\"\n    62→    zctx = ZephyrContext(\n    63→        client=fray_client,\n    64→        max_workers=1,\n    65→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n    66→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    67→    )\n    68→    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x + 1)\n    69→    results = list(zctx.execute(ds))\n    70→    assert sorted(results) == [2, 3, 4]\n    71→\n    72→\n    73→def test_write_jsonl(tmp_path, zephyr_ctx):\n    74→    \"\"\"Pipeline writing to jsonl file.\"\"\"\n    75→    output = str(tmp_path / \"out-{shard}.jsonl\")\n    76→    ds = Dataset.from_list([{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]).write_jsonl(output)\n    77→    results = list(zephyr_ctx.execute(ds))\n    78→    assert len(results) == 3\n    79→    # Verify all files were written and contain correct data\n    80→    all_records = []\n    81→    for path_str in results:\n    82→        written = Path(path_str)\n    83→        assert written.exists()\n    84→        lines = written.read_text().strip().split(\"\\n\")\n    85→        all_records.extend(json.loads(line) for line in lines)\n    86→    assert sorted(all_records, key=lambda r: r[\"a\"]) == [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]\n    87→\n    88→\n    89→def test_dry_run(zephyr_ctx):\n    90→    \"\"\"Dry run shows plan without executing.\"\"\"\n    91→    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    92→    results = list(zephyr_ctx.execute(ds, dry_run=True))\n    93→    assert results == []\n    94→\n    95→\n    96→def test_flat_map(zephyr_ctx):\n    97→    \"\"\"FlatMap pipeline produces correct results.\"\"\"\n    98→    ds = Dataset.from_list([1, 2, 3]).flat_map(lambda x: [x, x * 10])\n    99→    results = list(zephyr_ctx.execute(ds))\n   100→    assert sorted(results) == [1, 2, 3, 10, 20, 30]\n   101→\n   102→\n   103→def test_empty_dataset(zephyr_ctx):\n   104→    \"\"\"Empty dataset produces empty results.\"\"\"\n   105→    ds = Dataset.from_list([])\n   106→    results = list(zephyr_ctx.execute(ds))\n   107→    assert results == []\n   108→\n   109→\n   110→def test_chunk_cleanup(fray_client, tmp_path):\n   111→    \"\"\"Verify chunks are cleaned up after execution.\"\"\"\n   112→    chunk_prefix = str(tmp_path / \"chunks\")\n   113→    ctx = ZephyrContext(\n   114→        client=fray_client,\n   115→        max_workers=2,\n   116→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   117→        chunk_storage_prefix=chunk_prefix,\n   118→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   119→    )\n   120→\n   121→    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n   122→    results = list(ctx.execute(ds))\n   123→\n   124→    assert sorted(results) == [2, 4, 6]\n   125→\n   126→    # Verify chunks directory is cleaned up\n   127→    import os\n   128→\n   129→    if os.path.exists(chunk_prefix):\n   130→        # Should be empty or not exist\n   131→        files = list(Path(chunk_prefix).rglob(\"*\"))\n   132→        assert len(files) == 0, f\"Expected cleanup but found: {files}\"\n   133→\n   134→\n   135→def test_status_reports_alive_workers_not_total(tmp_path):\n   136→    \"\"\"After heartbeat timeout, get_status workers dict reflects FAILED state,\n   137→    and the status log distinguishes alive from total workers.\n   138→\n   139→    Also verifies that re-registering a worker that had an in-flight task\n   140→    requeues that task so it is not silently lost.\n   141→    \"\"\"\n   142→    from unittest.mock import MagicMock\n   143→\n   144→    from zephyr.execution import Shard, ShardTask, ZephyrCoordinator\n   145→\n   146→    coord = ZephyrCoordinator()\n   147→    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n   148→    coord.set_shared_data({})\n   149→\n   150→    task = ShardTask(\n   151→        shard_idx=0,\n   152→        total_shards=1,\n   153→        chunk_size=100,\n   154→        shard=Shard(chunks=[]),\n   155→        operations=[],\n   156→        stage_name=\"test\",\n   157→    )\n   158→    coord.start_stage(\"test\", [task])\n   159→\n   160→    # Register 3 workers\n   161→    for i in range(3):\n   162→        coord.register_worker(f\"worker-{i}\", MagicMock())\n   163→\n   164→    status = coord.get_status()\n   165→    assert len(status.workers) == 3\n   166→    assert all(w[\"state\"] == \"ready\" for w in status.workers.values())\n   167→\n   168→    # worker-0 pulls the task so it becomes in-flight\n   169→    pulled = coord.pull_task(\"worker-0\")\n   170→    assert pulled is not None and pulled != \"SHUTDOWN\"\n   171→\n   172→    # Simulate 2 workers dying via heartbeat timeout\n   173→    coord._last_seen[\"worker-0\"] = 0.0\n   174→    coord._last_seen[\"worker-1\"] = 0.0\n   175→    coord.check_heartbeats(timeout=30.0)\n   176→\n   177→    status = coord.get_status()\n   178→    assert status.workers[\"worker-0\"][\"state\"] == \"failed\"\n   179→    assert status.workers[\"worker-1\"][\"state\"] == \"failed\"\n   180→    assert status.workers[\"worker-2\"][\"state\"] == \"ready\"\n   181→\n   182→    # Total workers in dict is still 3, but only 1 is alive\n   183→    alive = sum(1 for w in status.workers.values() if w[\"state\"] in (\"ready\", \"busy\"))\n   184→    assert alive == 1\n   185→    assert len(status.workers) == 3\n   186→\n   187→    # worker-2 picks up the requeued task\n   188→    pulled2 = coord.pull_task(\"worker-2\")\n   189→    assert pulled2 is not None and pulled2 != \"SHUTDOWN\"\n   190→\n   191→    # Simulate worker-0 re-registering while worker-2 holds the task in-flight\n   192→    # (race between heartbeat requeue and re-registration).\n   193→    # Since worker-0 has no in-flight task anymore, this is a no-op for requeueing.\n   194→    coord.register_worker(\"worker-0\", MagicMock())\n   195→    status = coord.get_status()\n   196→    assert status.workers[\"worker-0\"][\"state\"] == \"ready\"\n   197→    alive = sum(1 for w in status.workers.values() if w[\"state\"] in (\"ready\", \"busy\"))\n   198→    assert alive == 2  # worker-0 ready, worker-1 still failed, worker-2 busy\n   199→\n   200→    # Now test the direct re-registration requeue path:\n   201→    # worker-2 dies while holding the task, and before heartbeat fires,\n   202→    # it re-registers — the in-flight task should be requeued.\n   203→    assert \"worker-2\" in coord._in_flight  # worker-2 has the task\n   204→    coord.register_worker(\"worker-2\", MagicMock())\n   205→    assert \"worker-2\" not in coord._in_flight  # in-flight cleared\n   206→    assert len(coord._task_queue) == 1  # task was requeued\n   207→\n   208→\n   209→def test_no_duplicate_results_on_heartbeat_timeout(fray_client, tmp_path):\n   210→    \"\"\"When a task is requeued after heartbeat timeout, the original worker's\n   211→    stale result (from a previous attempt) is rejected by the coordinator.\"\"\"\n   212→    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n   213→\n   214→    coord = ZephyrCoordinator()\n   215→    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n   216→    coord.set_shared_data({})\n   217→\n   218→    task = ShardTask(\n   219→        shard_idx=0,\n   220→        total_shards=1,\n   221→        chunk_size=100,\n   222→        shard=Shard(chunks=[]),\n   223→        operations=[],\n   224→        stage_name=\"test\",\n   225→    )\n   226→    coord.start_stage(\"test\", [task])\n   227→\n   228→    # Worker A pulls task (attempt 0)\n   229→    pulled = coord.pull_task(\"worker-A\")\n   230→    assert pulled is not None\n   231→    assert pulled != \"SHUTDOWN\"\n   232→    _task_a, attempt_a, _config = pulled\n   233→\n   234→    # Simulate heartbeat timeout: mark worker-A stale and requeue\n   235→    coord._last_seen[\"worker-A\"] = 0.0\n   236→    coord.check_heartbeats(timeout=0.0)\n   237→\n   238→    # Task should be requeued with incremented attempt\n   239→    assert coord._task_attempts[0] == 1\n   240→\n   241→    # Worker B picks up the requeued task (attempt 1)\n   242→    pulled_b = coord.pull_task(\"worker-B\")\n   243→    assert pulled_b is not None\n   244→    assert pulled_b != \"SHUTDOWN\"\n   245→    _task_b, attempt_b, _config = pulled_b\n   246→    assert attempt_b == 1\n   247→\n   248→    # Worker B reports success\n   249→    coord.report_result(\"worker-B\", 0, attempt_b, TaskResult(chunks=[]))\n   250→\n   251→    # Worker A's stale result (attempt 0) should be ignored\n   252→    coord.report_result(\"worker-A\", 0, attempt_a, TaskResult(chunks=[]))\n   253→\n   254→    # Only one completion should be counted\n   255→    assert coord._completed_shards == 1\n   256→\n   257→\n   258→def test_disk_chunk_write_uses_unique_paths(tmp_path):\n   259→    \"\"\"Each DiskChunk.write() writes to a unique location, avoiding collisions.\"\"\"\n   260→    from zephyr.execution import DiskChunk\n   261→\n   262→    base_path = str(tmp_path / \"chunk.pkl\")\n   263→    refs = [DiskChunk.write(base_path, [i]) for i in range(3)]\n   264→\n   265→    # Each written to a distinct UUID path (no rename needed)\n   266→    paths = [r.path for r in refs]\n   267→    assert len(set(paths)) == 3\n   268→    for p in paths:\n   269→        assert \".tmp.\" in p\n   270→        assert Path(p).exists()\n   271→\n   272→    # Each chunk is directly readable\n   273→    for i, ref in enumerate(refs):\n   274→        assert ref.read() == [i]\n   275→\n   276→\n   277→def test_coordinator_accepts_winner_ignores_stale(tmp_path):\n   278→    \"\"\"Coordinator accepts the winning result and ignores stale ones.\n   279→\n   280→    Stale chunk files are left for context-dir cleanup (no per-chunk deletion).\n   281→    \"\"\"\n   282→    from zephyr.execution import DiskChunk, ResultChunk, Shard, ShardTask, TaskResult, ZephyrCoordinator\n   283→\n   284→    coord = ZephyrCoordinator()\n   285→    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n   286→    coord.set_shared_data({})\n   287→\n   288→    task = ShardTask(\n   289→        shard_idx=0,\n   290→        total_shards=1,\n   291→        chunk_size=100,\n   292→        shard=Shard(chunks=[]),\n   293→        operations=[],\n   294→        stage_name=\"test\",\n   295→    )\n   296→    coord.start_stage(\"test\", [task])\n   297→\n   298→    # Worker A pulls task (attempt 0)\n   299→    pulled_a = coord.pull_task(\"worker-A\")\n   300→    _task_a, attempt_a, _config = pulled_a\n   301→\n   302→    # Worker A writes a chunk (simulating slow completion)\n   303→    stale_ref = DiskChunk.write(str(tmp_path / \"stale-chunk.pkl\"), [1, 2, 3])\n   304→    assert Path(stale_ref.path).exists()\n   305→\n   306→    # Heartbeat timeout re-queues the task\n   307→    coord._last_seen[\"worker-A\"] = 0.0\n   308→    coord.check_heartbeats(timeout=0.0)\n   309→\n   310→    # Worker B pulls and completes the re-queued task (attempt 1)\n   311→    pulled_b = coord.pull_task(\"worker-B\")\n   312→    _task_b, attempt_b, _config = pulled_b\n   313→\n   314→    winner_ref = DiskChunk.write(str(tmp_path / \"winner-chunk.pkl\"), [4, 5, 6])\n   315→\n   316→    coord.report_result(\n   317→        \"worker-B\",\n   318→        0,\n   319→        attempt_b,\n   320→        TaskResult(chunks=[ResultChunk(source_shard=0, target_shard=0, data=winner_ref)]),\n   321→    )\n   322→\n   323→    # Worker A's stale result is rejected\n   324→    coord.report_result(\n   325→        \"worker-A\",\n   326→        0,\n   327→        attempt_a,\n   328→        TaskResult(chunks=[ResultChunk(source_shard=0, target_shard=0, data=stale_ref)]),\n   329→    )\n   330→\n   331→    # Winner's data is directly readable (no rename needed)\n   332→    assert Path(winner_ref.path).exists()\n   333→    assert winner_ref.read() == [4, 5, 6]\n   334→\n   335→    # Stale file still exists (cleaned up by context-dir cleanup, not coordinator)\n   336→    assert Path(stale_ref.path).exists()\n   337→    assert coord._completed_shards == 1\n   338→\n   339→\n   340→def test_chunk_streaming_low_memory(tmp_path):\n   341→    \"\"\"Shard loads chunks one at a time from disk via iter_chunks.\n   342→\n   343→    Verifies iter_chunks yields data lazily and flat iteration works.\n   344→    \"\"\"\n   345→    from zephyr.execution import DiskChunk, Shard\n   346→\n   347→    # Write 3 chunks to disk (directly readable, no finalize needed)\n   348→    refs = []\n   349→    for i in range(3):\n   350→        path = str(tmp_path / f\"chunk-{i}.pkl\")\n   351→        chunk = DiskChunk.write(path, [i * 10 + j for j in range(5)])\n   352→        refs.append(chunk)\n   353→\n   354→    shard = Shard(chunks=refs)\n   355→\n   356→    # iter_chunks yields correct data\n   357→    chunks = list(shard.iter_chunks())\n   358→    assert len(chunks) == 3\n   359→    assert chunks[0] == [0, 1, 2, 3, 4]\n   360→    assert chunks[2] == [20, 21, 22, 23, 24]\n   361→\n   362→    # flat iteration yields all items in order\n   363→    assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n   364→\n   365→    # Re-iteration works (reads from disk again, not cached)\n   366→    assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n   367→\n   368→\n   369→def test_wait_for_stage_fails_when_all_workers_die(tmp_path):\n   370→    \"\"\"When all registered workers become dead/failed, _wait_for_stage raises\n   371→    after the no_workers_timeout instead of waiting forever.\"\"\"\n   372→    from unittest.mock import MagicMock\n   373→\n   374→    from zephyr.execution import Shard, ShardTask, WorkerState, ZephyrCoordinator, ZephyrWorkerError\n   375→\n   376→    coord = ZephyrCoordinator()\n   377→    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n   378→    coord.set_shared_data({})\n   379→    coord._no_workers_timeout = 0.5  # short timeout for test\n   380→\n   381→    task = ShardTask(\n   382→        shard_idx=0,\n   383→        total_shards=1,\n   384→        chunk_size=100,\n   385→        shard=Shard(chunks=[]),\n   386→        operations=[],\n   387→        stage_name=\"test\",\n   388→    )\n   389→    coord.start_stage(\"test\", [task])\n   390→\n   391→    # Register 2 workers\n   392→    coord.register_worker(\"worker-0\", MagicMock())\n   393→    coord.register_worker(\"worker-1\", MagicMock())\n   394→\n   395→    # Kill all workers via heartbeat timeout\n   396→    coord._last_seen[\"worker-0\"] = 0.0\n   397→    coord._last_seen[\"worker-1\"] = 0.0\n   398→    coord.check_heartbeats(timeout=0.0)\n   399→\n   400→    assert coord._worker_states[\"worker-0\"] == WorkerState.FAILED\n   401→    assert coord._worker_states[\"worker-1\"] == WorkerState.FAILED\n   402→\n   403→    # _wait_for_stage should raise after the dead timer expires\n   404→    with pytest.raises(ZephyrWorkerError, match=\"No alive workers\"):\n   405→        coord._wait_for_stage()\n   406→\n   407→\n   408→def test_wait_for_stage_resets_dead_timer_on_recovery(tmp_path):\n   409→    \"\"\"When a worker recovers (re-registers) after all workers died,\n   410→    the dead timer resets and execution can continue.\"\"\"\n   411→    import threading\n   412→\n   413→    from unittest.mock import MagicMock\n   414→\n   415→    from zephyr.execution import Shard, ShardTask, TaskResult, WorkerState, ZephyrCoordinator\n   416→\n   417→    coord = ZephyrCoordinator()\n   418→    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n   419→    coord.set_shared_data({})\n   420→    coord._no_workers_timeout = 2.0\n   421→\n   422→    task = ShardTask(\n   423→        shard_idx=0,\n   424→        total_shards=1,\n   425→        chunk_size=100,\n   426→        shard=Shard(chunks=[]),\n   427→        operations=[],\n   428→        stage_name=\"test\",\n   429→    )\n   430→    coord.start_stage(\"test\", [task])\n   431→\n   432→    # Register and kill a worker\n   433→    coord.register_worker(\"worker-0\", MagicMock())\n   434→    coord._last_seen[\"worker-0\"] = 0.0\n   435→    coord.check_heartbeats(timeout=0.0)\n   436→    assert coord._worker_states[\"worker-0\"] == WorkerState.FAILED\n   437→\n   438→    # In a background thread, re-register the worker and complete the task\n   439→    # after a short delay (simulating recovery before timeout expires)\n   440→    def recover_and_complete():\n   441→        time.sleep(0.3)\n   442→        coord.register_worker(\"worker-0\", MagicMock())\n   443→        pulled = coord.pull_task(\"worker-0\")\n   444→        assert pulled is not None and pulled != \"SHUTDOWN\"\n   445→        _task, attempt, _config = pulled\n   446→        coord.report_result(\"worker-0\", 0, attempt, TaskResult(chunks=[]))\n   447→\n   448→    t = threading.Thread(target=recover_and_complete)\n   449→    t.start()\n   450→\n   451→    # _wait_for_stage should succeed (worker recovers before timeout)\n   452→    coord._wait_for_stage()\n   453→    t.join(timeout=5.0)\n   454→\n   455→    assert coord._completed_shards == 1\n   456→\n   457→\n   458→def test_fresh_actors_per_execute(fray_client, tmp_path):\n   459→    \"\"\"Each execute() creates and tears down its own coordinator and workers.\"\"\"\n   460→    chunk_prefix = str(tmp_path / \"chunks\")\n   461→\n   462→    zctx = ZephyrContext(\n   463→        client=fray_client,\n   464→        max_workers=2,\n   465→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   466→        chunk_storage_prefix=chunk_prefix,\n   467→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   468→    )\n   469→    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x + 1)\n   470→    results = list(zctx.execute(ds))\n   471→    assert sorted(results) == [2, 3, 4]\n   472→\n   473→    # After execute(): everything is torn down\n   474→    assert zctx._coordinator is None\n   475→    assert zctx._worker_group is None\n   476→    assert zctx._pipeline_id == 0\n   477→\n   478→    # Can execute again (creates fresh coordinator + workers)\n   479→    ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)\n   480→    results2 = list(zctx.execute(ds2))\n   481→    assert sorted(results2) == [20, 40]\n   482→\n   483→    assert zctx._coordinator is None\n   484→    assert zctx._worker_group is None\n   485→    assert zctx._pipeline_id == 1\n   486→\n   487→\n   488→def test_fatal_errors_fail_fast(fray_client, tmp_path):\n   489→    \"\"\"Application errors (e.g. ValueError) cause immediate failure, no retries.\"\"\"\n   490→    from zephyr.execution import ZephyrWorkerError\n   491→\n   492→    chunk_prefix = str(tmp_path / \"chunks\")\n   493→\n   494→    def exploding_map(x):\n   495→        raise ValueError(f\"bad value: {x}\")\n   496→\n   497→    zctx = ZephyrContext(\n   498→        client=fray_client,\n   499→        max_workers=1,\n   500→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   501→        chunk_storage_prefix=chunk_prefix,\n   502→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   503→    )\n   504→    ds = Dataset.from_list([1, 2, 3]).map(exploding_map)\n   505→\n   506→    start = time.monotonic()\n   507→    with pytest.raises(ZephyrWorkerError, match=\"ValueError\"):\n   508→        list(zctx.execute(ds))\n   509→    elapsed = time.monotonic() - start\n   510→\n   511→    # Should fail fast — well under the 30s heartbeat timeout\n   512→    assert elapsed < 15.0, f\"Took {elapsed:.1f}s, expected fast failure\"\n   513→\n   514→\n   515→def test_chunk_storage_with_join(fray_client, tmp_path):\n   516→    \"\"\"Verify chunk storage works with join operations.\"\"\"\n   517→    chunk_prefix = str(tmp_path / \"chunks\")\n   518→    ctx = ZephyrContext(\n   519→        client=fray_client,\n   520→        max_workers=2,\n   521→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   522→        chunk_storage_prefix=chunk_prefix,\n   523→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   524→    )\n   525→\n   526→    # Create datasets for join\n   527→    left = Dataset.from_list([{\"id\": 1, \"a\": \"x\"}, {\"id\": 2, \"a\": \"y\"}])\n   528→    right = Dataset.from_list([{\"id\": 1, \"b\": \"p\"}, {\"id\": 2, \"b\": \"q\"}])\n   529→\n   530→    joined = left.sorted_merge_join(\n   531→        right,\n   532→        left_key=lambda x: x[\"id\"],\n   533→        right_key=lambda x: x[\"id\"],\n   534→        combiner=lambda left, right: {**left, **right},\n   535→    )\n   536→\n   537→    results = sorted(list(ctx.execute(joined)), key=lambda x: x[\"id\"])\n   538→    assert len(results) == 2\n   539→    assert results[0] == {\"id\": 1, \"a\": \"x\", \"b\": \"p\"}\n   540→    assert results[1] == {\"id\": 2, \"a\": \"y\", \"b\": \"q\"}\n   541→\n   542→\n   543→def test_workers_capped_to_shard_count(fray_client, tmp_path):\n   544→    \"\"\"When max_workers > num_shards, only num_shards workers are created.\"\"\"\n   545→    ds = Dataset.from_list([1, 2, 3])  # 3 shards\n   546→    ctx = ZephyrContext(\n   547→        client=fray_client,\n   548→        max_workers=10,\n   549→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   550→        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n   551→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   552→    )\n   553→    results = list(ctx.execute(ds.map(lambda x: x * 2)))\n   554→    assert sorted(results) == [2, 4, 6]\n   555→    # Everything torn down after execute; correct results prove workers\n   556→    # were created and sized properly (min(10, 3) = 3)\n   557→    assert ctx._pipeline_id == 0\n   558→\n   559→\n   560→def test_pipeline_id_increments(fray_client, tmp_path):\n   561→    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n   562→    ctx = ZephyrContext(\n   563→        client=fray_client,\n   564→        max_workers=10,\n   565→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   566→        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n   567→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   568→    )\n   569→    ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n   570→    assert ctx._pipeline_id == 0\n   571→\n   572→    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n   573→    assert ctx._pipeline_id == 1\n   574→\n   575→\n   576→def test_execute_retries_on_coordinator_death(tmp_path):\n   577→    \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n   578→    coordinator and worker pool and eventually succeeds.\n   579→\n   580→    Uses LocalClient directly because simulating coordinator death requires\n   581→    manipulating the local actor registry.\n   582→    \"\"\"\n   583→    from fray.v2.local_backend import LocalClient, _local_actor_registry\n   584→\n   585→    client = LocalClient()\n   586→    chunk_prefix = str(tmp_path / \"chunks\")\n   587→\n   588→    ctx = ZephyrContext(\n   589→        client=client,\n   590→        max_workers=2,\n   591→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   592→        chunk_storage_prefix=chunk_prefix,\n   593→        max_execution_retries=2,\n   594→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   595→    )\n   596→\n   597→    # First execute() succeeds normally\n   598→    results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))\n   599→    assert sorted(results) == [2, 4, 6]\n   600→\n   601→    # Sabotage the registry so the *next* coordinator creation attempt fails\n   602→    # on the first try. We do this by patching create_actor_group to fail once.\n   603→    original_create = client.create_actor_group\n   604→    fail_count = [0]\n   605→\n   606→    def flaky_create(*args, **kwargs):\n   607→        group = original_create(*args, **kwargs)\n   608→        if fail_count[0] == 0 and \"coord\" in kwargs.get(\"name\", \"\"):\n   609→            fail_count[0] += 1\n   610→            # Kill the coordinator immediately after creation to simulate death\n   611→            handles = group.wait_ready()\n   612→            endpoint = handles[0]._endpoint\n   613→            _local_actor_registry.pop(endpoint, None)\n   614→        return group\n   615→\n   616→    client.create_actor_group = flaky_create\n   617→\n   618→    # Next execute() should: fail on attempt 0 (dead coordinator),\n   619→    # then succeed on attempt 1 with a fresh coordinator.\n   620→    results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))\n   621→    assert sorted(results) == [11, 21]\n   622→\n   623→    ctx.shutdown()\n   624→    client.shutdown(wait=True)\n   625→\n   626→\n   627→def test_execute_does_not_retry_worker_errors(fray_client, tmp_path):\n   628→    \"\"\"ZephyrWorkerError (application errors) are never retried.\"\"\"\n   629→    from zephyr.execution import ZephyrWorkerError\n   630→\n   631→    chunk_prefix = str(tmp_path / \"chunks\")\n   632→\n   633→    def exploding_map(x):\n   634→        raise ValueError(f\"bad value: {x}\")\n   635→\n   636→    ctx = ZephyrContext(\n   637→        client=fray_client,\n   638→        max_workers=1,\n   639→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   640→        chunk_storage_prefix=chunk_prefix,\n   641→        max_execution_retries=3,\n   642→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   643→    )\n   644→    ds = Dataset.from_list([1, 2, 3]).map(exploding_map)\n   645→\n   646→    start = time.monotonic()\n   647→    with pytest.raises(ZephyrWorkerError, match=\"ValueError\"):\n   648→        list(ctx.execute(ds))\n   649→    elapsed = time.monotonic() - start\n   650→\n   651→    # Should fail fast — no retries for application errors\n   652→    assert elapsed < 15.0, f\"Took {elapsed:.1f}s, expected fast failure (no retries)\"\n   653→"}]},"uuid":"f7aacf18-92d7-4b12-b173-6837cb354a40","timestamp":"2026-02-23T23:48:23.507Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/rav/projects/marin/lib/zephyr/tests/test_execution.py","content":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Tests for the actor-based execution engine (ZephyrContext).\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport time\nimport uuid\nfrom pathlib import Path\n\nimport pytest\nfrom fray.v2 import ResourceConfig\nfrom zephyr.dataset import Dataset\nfrom zephyr.execution import ZephyrContext, zephyr_worker_ctx\n\n\ndef test_simple_map(zephyr_ctx):\n    \"\"\"Map pipeline produces correct results.\"\"\"\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [2, 4, 6]\n\n\ndef test_filter(zephyr_ctx):\n    \"\"\"Filter pipeline produces correct results.\"\"\"\n    ds = Dataset.from_list([1, 2, 3, 4, 5]).filter(lambda x: x > 3)\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [4, 5]\n\n\ndef test_shared_data(fray_client):\n    \"\"\"Workers can access shared data via zephyr_worker_ctx().\"\"\"\n\n    def use_shared(x):\n        multiplier = zephyr_worker_ctx().get_shared(\"multiplier\")\n        return x * multiplier\n\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    zctx.put(\"multiplier\", 10)\n    ds = Dataset.from_list([1, 2, 3]).map(use_shared)\n    results = list(zctx.execute(ds))\n    assert sorted(results) == [10, 20, 30]\n    zctx.shutdown()\n\n\ndef test_multi_stage(zephyr_ctx):\n    \"\"\"Multi-stage pipeline (map + filter) works.\"\"\"\n    ds = Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x * 2).filter(lambda x: x > 5)\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [6, 8, 10]\n\n\ndef test_context_manager(fray_client):\n    \"\"\"ZephyrContext works without context manager.\"\"\"\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x + 1)\n    results = list(zctx.execute(ds))\n    assert sorted(results) == [2, 3, 4]\n\n\ndef test_write_jsonl(tmp_path, zephyr_ctx):\n    \"\"\"Pipeline writing to jsonl file.\"\"\"\n    output = str(tmp_path / \"out-{shard}.jsonl\")\n    ds = Dataset.from_list([{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]).write_jsonl(output)\n    results = list(zephyr_ctx.execute(ds))\n    assert len(results) == 3\n    # Verify all files were written and contain correct data\n    all_records = []\n    for path_str in results:\n        written = Path(path_str)\n        assert written.exists()\n        lines = written.read_text().strip().split(\"\\n\")\n        all_records.extend(json.loads(line) for line in lines)\n    assert sorted(all_records, key=lambda r: r[\"a\"]) == [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]\n\n\ndef test_dry_run(zephyr_ctx):\n    \"\"\"Dry run shows plan without executing.\"\"\"\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    results = list(zephyr_ctx.execute(ds, dry_run=True))\n    assert results == []\n\n\ndef test_flat_map(zephyr_ctx):\n    \"\"\"FlatMap pipeline produces correct results.\"\"\"\n    ds = Dataset.from_list([1, 2, 3]).flat_map(lambda x: [x, x * 10])\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [1, 2, 3, 10, 20, 30]\n\n\ndef test_empty_dataset(zephyr_ctx):\n    \"\"\"Empty dataset produces empty results.\"\"\"\n    ds = Dataset.from_list([])\n    results = list(zephyr_ctx.execute(ds))\n    assert results == []\n\n\ndef test_chunk_cleanup(fray_client, tmp_path):\n    \"\"\"Verify chunks are cleaned up after execution.\"\"\"\n    chunk_prefix = str(tmp_path / \"chunks\")\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    results = list(ctx.execute(ds))\n\n    assert sorted(results) == [2, 4, 6]\n\n    # Verify chunks directory is cleaned up\n    import os\n\n    if os.path.exists(chunk_prefix):\n        # Should be empty or not exist\n        files = list(Path(chunk_prefix).rglob(\"*\"))\n        assert len(files) == 0, f\"Expected cleanup but found: {files}\"\n\n\ndef test_status_reports_alive_workers_not_total(tmp_path):\n    \"\"\"After heartbeat timeout, get_status workers dict reflects FAILED state,\n    and the status log distinguishes alive from total workers.\n\n    Also verifies that re-registering a worker that had an in-flight task\n    requeues that task so it is not silently lost.\n    \"\"\"\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Register 3 workers\n    for i in range(3):\n        coord.register_worker(f\"worker-{i}\", MagicMock())\n\n    status = coord.get_status()\n    assert len(status.workers) == 3\n    assert all(w[\"state\"] == \"ready\" for w in status.workers.values())\n\n    # worker-0 pulls the task so it becomes in-flight\n    pulled = coord.pull_task(\"worker-0\")\n    assert pulled is not None and pulled != \"SHUTDOWN\"\n\n    # Simulate 2 workers dying via heartbeat timeout\n    coord._last_seen[\"worker-0\"] = 0.0\n    coord._last_seen[\"worker-1\"] = 0.0\n    coord.check_heartbeats(timeout=30.0)\n\n    status = coord.get_status()\n    assert status.workers[\"worker-0\"][\"state\"] == \"failed\"\n    assert status.workers[\"worker-1\"][\"state\"] == \"failed\"\n    assert status.workers[\"worker-2\"][\"state\"] == \"ready\"\n\n    # Total workers in dict is still 3, but only 1 is alive\n    alive = sum(1 for w in status.workers.values() if w[\"state\"] in (\"ready\", \"busy\"))\n    assert alive == 1\n    assert len(status.workers) == 3\n\n    # worker-2 picks up the requeued task\n    pulled2 = coord.pull_task(\"worker-2\")\n    assert pulled2 is not None and pulled2 != \"SHUTDOWN\"\n\n    # Simulate worker-0 re-registering while worker-2 holds the task in-flight\n    # (race between heartbeat requeue and re-registration).\n    # Since worker-0 has no in-flight task anymore, this is a no-op for requeueing.\n    coord.register_worker(\"worker-0\", MagicMock())\n    status = coord.get_status()\n    assert status.workers[\"worker-0\"][\"state\"] == \"ready\"\n    alive = sum(1 for w in status.workers.values() if w[\"state\"] in (\"ready\", \"busy\"))\n    assert alive == 2  # worker-0 ready, worker-1 still failed, worker-2 busy\n\n    # Now test the direct re-registration requeue path:\n    # worker-2 dies while holding the task, and before heartbeat fires,\n    # it re-registers — the in-flight task should be requeued.\n    assert \"worker-2\" in coord._in_flight  # worker-2 has the task\n    coord.register_worker(\"worker-2\", MagicMock())\n    assert \"worker-2\" not in coord._in_flight  # in-flight cleared\n    assert len(coord._task_queue) == 1  # task was requeued\n\n\ndef test_no_duplicate_results_on_heartbeat_timeout(fray_client, tmp_path):\n    \"\"\"When a task is requeued after heartbeat timeout, the original worker's\n    stale result (from a previous attempt) is rejected by the coordinator.\"\"\"\n    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Worker A pulls task (attempt 0)\n    pulled = coord.pull_task(\"worker-A\")\n    assert pulled is not None\n    assert pulled != \"SHUTDOWN\"\n    _task_a, attempt_a, _config = pulled\n\n    # Simulate heartbeat timeout: mark worker-A stale and requeue\n    coord._last_seen[\"worker-A\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n\n    # Task should be requeued with incremented attempt\n    assert coord._task_attempts[0] == 1\n\n    # Worker B picks up the requeued task (attempt 1)\n    pulled_b = coord.pull_task(\"worker-B\")\n    assert pulled_b is not None\n    assert pulled_b != \"SHUTDOWN\"\n    _task_b, attempt_b, _config = pulled_b\n    assert attempt_b == 1\n\n    # Worker B reports success\n    coord.report_result(\"worker-B\", 0, attempt_b, TaskResult(chunks=[]))\n\n    # Worker A's stale result (attempt 0) should be ignored\n    coord.report_result(\"worker-A\", 0, attempt_a, TaskResult(chunks=[]))\n\n    # Only one completion should be counted\n    assert coord._completed_shards == 1\n\n\ndef test_disk_chunk_write_uses_unique_paths(tmp_path):\n    \"\"\"Each DiskChunk.write() writes to a unique location, avoiding collisions.\"\"\"\n    from zephyr.execution import DiskChunk\n\n    base_path = str(tmp_path / \"chunk.pkl\")\n    refs = [DiskChunk.write(base_path, [i]) for i in range(3)]\n\n    # Each written to a distinct UUID path (no rename needed)\n    paths = [r.path for r in refs]\n    assert len(set(paths)) == 3\n    for p in paths:\n        assert \".tmp.\" in p\n        assert Path(p).exists()\n\n    # Each chunk is directly readable\n    for i, ref in enumerate(refs):\n        assert ref.read() == [i]\n\n\ndef test_coordinator_accepts_winner_ignores_stale(tmp_path):\n    \"\"\"Coordinator accepts the winning result and ignores stale ones.\n\n    Stale chunk files are left for context-dir cleanup (no per-chunk deletion).\n    \"\"\"\n    from zephyr.execution import DiskChunk, ResultChunk, Shard, ShardTask, TaskResult, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Worker A pulls task (attempt 0)\n    pulled_a = coord.pull_task(\"worker-A\")\n    _task_a, attempt_a, _config = pulled_a\n\n    # Worker A writes a chunk (simulating slow completion)\n    stale_ref = DiskChunk.write(str(tmp_path / \"stale-chunk.pkl\"), [1, 2, 3])\n    assert Path(stale_ref.path).exists()\n\n    # Heartbeat timeout re-queues the task\n    coord._last_seen[\"worker-A\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n\n    # Worker B pulls and completes the re-queued task (attempt 1)\n    pulled_b = coord.pull_task(\"worker-B\")\n    _task_b, attempt_b, _config = pulled_b\n\n    winner_ref = DiskChunk.write(str(tmp_path / \"winner-chunk.pkl\"), [4, 5, 6])\n\n    coord.report_result(\n        \"worker-B\",\n        0,\n        attempt_b,\n        TaskResult(chunks=[ResultChunk(source_shard=0, target_shard=0, data=winner_ref)]),\n    )\n\n    # Worker A's stale result is rejected\n    coord.report_result(\n        \"worker-A\",\n        0,\n        attempt_a,\n        TaskResult(chunks=[ResultChunk(source_shard=0, target_shard=0, data=stale_ref)]),\n    )\n\n    # Winner's data is directly readable (no rename needed)\n    assert Path(winner_ref.path).exists()\n    assert winner_ref.read() == [4, 5, 6]\n\n    # Stale file still exists (cleaned up by context-dir cleanup, not coordinator)\n    assert Path(stale_ref.path).exists()\n    assert coord._completed_shards == 1\n\n\ndef test_chunk_streaming_low_memory(tmp_path):\n    \"\"\"Shard loads chunks one at a time from disk via iter_chunks.\n\n    Verifies iter_chunks yields data lazily and flat iteration works.\n    \"\"\"\n    from zephyr.execution import DiskChunk, Shard\n\n    # Write 3 chunks to disk (directly readable, no finalize needed)\n    refs = []\n    for i in range(3):\n        path = str(tmp_path / f\"chunk-{i}.pkl\")\n        chunk = DiskChunk.write(path, [i * 10 + j for j in range(5)])\n        refs.append(chunk)\n\n    shard = Shard(chunks=refs)\n\n    # iter_chunks yields correct data\n    chunks = list(shard.iter_chunks())\n    assert len(chunks) == 3\n    assert chunks[0] == [0, 1, 2, 3, 4]\n    assert chunks[2] == [20, 21, 22, 23, 24]\n\n    # flat iteration yields all items in order\n    assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n\n    # Re-iteration works (reads from disk again, not cached)\n    assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n\n\ndef test_wait_for_stage_fails_when_all_workers_die(tmp_path):\n    \"\"\"When all registered workers become dead/failed, _wait_for_stage raises\n    after the no_workers_timeout instead of waiting forever.\"\"\"\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, WorkerState, ZephyrCoordinator, ZephyrWorkerError\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n    coord._no_workers_timeout = 0.5  # short timeout for test\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Register 2 workers\n    coord.register_worker(\"worker-0\", MagicMock())\n    coord.register_worker(\"worker-1\", MagicMock())\n\n    # Kill all workers via heartbeat timeout\n    coord._last_seen[\"worker-0\"] = 0.0\n    coord._last_seen[\"worker-1\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n\n    assert coord._worker_states[\"worker-0\"] == WorkerState.FAILED\n    assert coord._worker_states[\"worker-1\"] == WorkerState.FAILED\n\n    # _wait_for_stage should raise after the dead timer expires\n    with pytest.raises(ZephyrWorkerError, match=\"No alive workers\"):\n        coord._wait_for_stage()\n\n\ndef test_wait_for_stage_resets_dead_timer_on_recovery(tmp_path):\n    \"\"\"When a worker recovers (re-registers) after all workers died,\n    the dead timer resets and execution can continue.\"\"\"\n    import threading\n\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, TaskResult, WorkerState, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n    coord._no_workers_timeout = 2.0\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Register and kill a worker\n    coord.register_worker(\"worker-0\", MagicMock())\n    coord._last_seen[\"worker-0\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n    assert coord._worker_states[\"worker-0\"] == WorkerState.FAILED\n\n    # In a background thread, re-register the worker and complete the task\n    # after a short delay (simulating recovery before timeout expires)\n    def recover_and_complete():\n        time.sleep(0.3)\n        coord.register_worker(\"worker-0\", MagicMock())\n        pulled = coord.pull_task(\"worker-0\")\n        assert pulled is not None and pulled != \"SHUTDOWN\"\n        _task, attempt, _config = pulled\n        coord.report_result(\"worker-0\", 0, attempt, TaskResult(chunks=[]))\n\n    t = threading.Thread(target=recover_and_complete)\n    t.start()\n\n    # _wait_for_stage should succeed (worker recovers before timeout)\n    coord._wait_for_stage()\n    t.join(timeout=5.0)\n\n    assert coord._completed_shards == 1\n\n\ndef test_fresh_actors_per_execute(fray_client, tmp_path):\n    \"\"\"Each execute() creates and tears down its own coordinator and workers.\"\"\"\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x + 1)\n    results = list(zctx.execute(ds))\n    assert sorted(results) == [2, 3, 4]\n\n    # After execute(): everything is torn down\n    assert zctx._coordinator is None\n    assert zctx._worker_group is None\n    assert zctx._pipeline_id == 0\n\n    # Can execute again (creates fresh coordinator + workers)\n    ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)\n    results2 = list(zctx.execute(ds2))\n    assert sorted(results2) == [20, 40]\n\n    assert zctx._coordinator is None\n    assert zctx._worker_group is None\n    assert zctx._pipeline_id == 1\n\n\ndef test_fatal_errors_fail_fast(fray_client, tmp_path):\n    \"\"\"Application errors (e.g. ValueError) cause immediate failure, no retries.\"\"\"\n    from zephyr.execution import ZephyrWorkerError\n\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    def exploding_map(x):\n        raise ValueError(f\"bad value: {x}\")\n\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(exploding_map)\n\n    start = time.monotonic()\n    with pytest.raises(ZephyrWorkerError, match=\"ValueError\"):\n        list(zctx.execute(ds))\n    elapsed = time.monotonic() - start\n\n    # Should fail fast — well under the 30s heartbeat timeout\n    assert elapsed < 15.0, f\"Took {elapsed:.1f}s, expected fast failure\"\n\n\ndef test_chunk_storage_with_join(fray_client, tmp_path):\n    \"\"\"Verify chunk storage works with join operations.\"\"\"\n    chunk_prefix = str(tmp_path / \"chunks\")\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n\n    # Create datasets for join\n    left = Dataset.from_list([{\"id\": 1, \"a\": \"x\"}, {\"id\": 2, \"a\": \"y\"}])\n    right = Dataset.from_list([{\"id\": 1, \"b\": \"p\"}, {\"id\": 2, \"b\": \"q\"}])\n\n    joined = left.sorted_merge_join(\n        right,\n        left_key=lambda x: x[\"id\"],\n        right_key=lambda x: x[\"id\"],\n        combiner=lambda left, right: {**left, **right},\n    )\n\n    results = sorted(list(ctx.execute(joined)), key=lambda x: x[\"id\"])\n    assert len(results) == 2\n    assert results[0] == {\"id\": 1, \"a\": \"x\", \"b\": \"p\"}\n    assert results[1] == {\"id\": 2, \"a\": \"y\", \"b\": \"q\"}\n\n\ndef test_workers_capped_to_shard_count(fray_client, tmp_path):\n    \"\"\"When max_workers > num_shards, only num_shards workers are created.\"\"\"\n    ds = Dataset.from_list([1, 2, 3])  # 3 shards\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=10,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    results = list(ctx.execute(ds.map(lambda x: x * 2)))\n    assert sorted(results) == [2, 4, 6]\n    # Everything torn down after execute; correct results prove workers\n    # were created and sized properly (min(10, 3) = 3)\n    assert ctx._pipeline_id == 0\n\n\ndef test_pipeline_id_increments(fray_client, tmp_path):\n    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=10,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n    assert ctx._pipeline_id == 0\n\n    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):\n    \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n    coordinator and worker pool and eventually succeeds.\n\n    Uses LocalClient directly because simulating coordinator death requires\n    manipulating the local actor registry.\n    \"\"\"\n    from fray.v2.local_backend import LocalClient, _local_actor_registry\n\n    client = LocalClient()\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    ctx = ZephyrContext(\n        client=client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        max_execution_retries=2,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n\n    # First execute() succeeds normally\n    results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))\n    assert sorted(results) == [2, 4, 6]\n\n    # Sabotage the registry so the *next* coordinator creation attempt fails\n    # on the first try. We do this by patching create_actor_group to fail once.\n    original_create = client.create_actor_group\n    fail_count = [0]\n\n    def flaky_create(*args, **kwargs):\n        group = original_create(*args, **kwargs)\n        if fail_count[0] == 0 and \"coord\" in kwargs.get(\"name\", \"\"):\n            fail_count[0] += 1\n            # Kill the coordinator immediately after creation to simulate death\n            handles = group.wait_ready()\n            endpoint = handles[0]._endpoint\n            _local_actor_registry.pop(endpoint, None)\n        return group\n\n    client.create_actor_group = flaky_create\n\n    # Next execute() should: fail on attempt 0 (dead coordinator),\n    # then succeed on attempt 1 with a fresh coordinator.\n    results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))\n    assert sorted(results) == [11, 21]\n\n    ctx.shutdown()\n    client.shutdown(wait=True)\n\n\ndef test_execute_does_not_retry_worker_errors(fray_client, tmp_path):\n    \"\"\"ZephyrWorkerError (application errors) are never retried.\"\"\"\n    from zephyr.execution import ZephyrWorkerError\n\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    def exploding_map(x):\n        raise ValueError(f\"bad value: {x}\")\n\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        max_execution_retries=3,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(exploding_map)\n\n    start = time.monotonic()\n    with pytest.raises(ZephyrWorkerError, match=\"ValueError\"):\n        list(ctx.execute(ds))\n    elapsed = time.monotonic() - start\n\n    # Should fail fast — no retries for application errors\n    assert elapsed < 15.0, f\"Took {elapsed:.1f}s, expected fast failure (no retries)\"\n","numLines":653,"startLine":1,"totalLines":653}},"sourceToolAssistantUUID":"bde36dee-ac40-485e-800b-3c009c11ee4a"}
{"parentUuid":"f7aacf18-92d7-4b12-b173-6837cb354a40","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01HAbgHALdV2v7AWRzS6p8yB","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"Now I can see the current state of the files. I need to produce a clean patch that only contains the \"last stage early worker shutdown\" changes. Let me identify what needs to change from the current state:\n\nLooking at the current `execution.py`, I notice:\n1. Line 269: `self._shutdown: bool = False` — need to add `self._is_last_stage: bool = False` after this\n2. Lines 405-406: `pull_task` — queue empty returns None, need to add SHUTDOWN check\n3. Line 491: `_start_stage` — needs `is_last_stage` parameter\n\n\n4. Line 502: needs `self._is_last_stage = is_last_stage`\n5. Lines 583-596: `run_pipeline` — needs `last_worker_stage_idx` computation and passing `is_last_stage`\n6. Lines 611-612: `run_pipeline` — `self.shutdown()` already exists at line 612, so that's not part of this diff\n7. Line 693: `start_stage` — needs `is_last_stage` forwarding\n\nThe `self.shutdown()` call is already in the current file, so I need to focus on the other changes and ignore that part of the diff.\n\nNow I'm mapping out the exact locations where I need to add the `_is_last_stage` field initialization, the SHUTDOWN return logic in `pull_task`, the parameter additions to `_start_stage`, the computation of `last_worker_stage_idx` in `run_pipeline`, and the parameter forwarding to the public `start_stage` method.","signature":"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"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":26718,"cache_read_input_tokens":64345,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":26718},"output_tokens":13,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtrFsSG6h7x4fTSt9fk","type":"assistant","uuid":"bc93bbaa-3478-4248-8e52-e8294232db8a","timestamp":"2026-02-23T23:48:42.941Z"}
{"parentUuid":"bc93bbaa-3478-4248-8e52-e8294232db8a","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01HAbgHALdV2v7AWRzS6p8yB","type":"message","role":"assistant","content":[{"type":"text","text":"I'll apply the changes to both files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":26718,"cache_read_input_tokens":64345,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":26718},"output_tokens":13,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtrFsSG6h7x4fTSt9fk","type":"assistant","uuid":"007d0a3b-6902-4ac9-8f00-7d4391e9acd7","timestamp":"2026-02-23T23:48:43.188Z"}
{"type":"file-history-snapshot","messageId":"4569a4ce-3aa6-4b3a-9bbe-a1321e6a03b4","snapshot":{"messageId":"215cd644-6ea1-4f1a-a834-3de8a4126e9b","trackedFileBackups":{"lib/zephyr/src/zephyr/execution.py":{"backupFileName":"a08e09256175bf81@v1","version":1,"backupTime":"2026-02-23T23:48:44.115Z"}},"timestamp":"2026-02-23T23:48:18.496Z"},"isSnapshotUpdate":true}
{"parentUuid":"007d0a3b-6902-4ac9-8f00-7d4391e9acd7","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01HAbgHALdV2v7AWRzS6p8yB","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HBqaWsFNqXEaxEcZboJoRi","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","old_string":"        self._shutdown: bool = False\n        self._initialized: bool = False","new_string":"        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":26718,"cache_read_input_tokens":64345,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":26718},"output_tokens":13,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtrFsSG6h7x4fTSt9fk","type":"assistant","uuid":"4569a4ce-3aa6-4b3a-9bbe-a1321e6a03b4","timestamp":"2026-02-23T23:48:44.112Z"}
{"parentUuid":"4569a4ce-3aa6-4b3a-9bbe-a1321e6a03b4","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01HBqaWsFNqXEaxEcZboJoRi","toolUseID":"toolu_01HBqaWsFNqXEaxEcZboJoRi","timestamp":"2026-02-23T23:48:44.118Z","uuid":"6a3b480d-68bd-47fa-bce1-a71527dc184a"}
{"parentUuid":"4569a4ce-3aa6-4b3a-9bbe-a1321e6a03b4","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HBqaWsFNqXEaxEcZboJoRi","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py has been updated successfully."}]},"uuid":"6dba9eb8-dc35-4301-942a-6410cb7c4d42","timestamp":"2026-02-23T23:48:44.117Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","oldString":"        self._shutdown: bool = False\n        self._initialized: bool = False","newString":"        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","structuredPatch":[{"oldStart":267,"oldLines":6,"newStart":267,"newLines":7,"lines":["         self._worker_handles: dict[str, ActorHandle] = {}","         self._coordinator_thread: threading.Thread | None = None","         self._shutdown: bool = False","+        self._is_last_stage: bool = False","         self._initialized: bool = False"," ","         # Lock for accessing coordinator state from background thread"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"4569a4ce-3aa6-4b3a-9bbe-a1321e6a03b4"}
{"parentUuid":"6dba9eb8-dc35-4301-942a-6410cb7c4d42","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_018YzbrycGeW9SXhM3MSnaLj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HUHvtnRwJmUcqkLbZH7jVf","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","old_string":"            if not self._task_queue:\n                return None","new_string":"            if not self._task_queue:\n                if self._is_last_stage:\n                    return \"SHUTDOWN\"\n                return None"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":760,"cache_read_input_tokens":91063,"output_tokens":159,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":760,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"}},"requestId":"req_011CYRtsnPzoojYDo8tjVBpv","type":"assistant","uuid":"5c8e0e8b-f788-4e65-ac6e-8a6bca80303e","timestamp":"2026-02-23T23:48:58.754Z"}
{"parentUuid":"5c8e0e8b-f788-4e65-ac6e-8a6bca80303e","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01HUHvtnRwJmUcqkLbZH7jVf","toolUseID":"toolu_01HUHvtnRwJmUcqkLbZH7jVf","timestamp":"2026-02-23T23:48:58.763Z","uuid":"d4d0eb87-5221-4619-8166-2f622d170d0c"}
{"parentUuid":"5c8e0e8b-f788-4e65-ac6e-8a6bca80303e","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HUHvtnRwJmUcqkLbZH7jVf","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py has been updated successfully."}]},"uuid":"5d965408-f344-4e53-bb67-58a4e8917478","timestamp":"2026-02-23T23:48:58.762Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","oldString":"            if not self._task_queue:\n                return None","newString":"            if not self._task_queue:\n                if self._is_last_stage:\n                    return \"SHUTDOWN\"\n                return None","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","structuredPatch":[{"oldStart":404,"oldLines":6,"newStart":404,"newLines":8,"lines":["                 return None"," ","             if not self._task_queue:","+                if self._is_last_stage:","+                    return \"SHUTDOWN\"","                 return None"," ","             task = self._task_queue.popleft()"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"5c8e0e8b-f788-4e65-ac6e-8a6bca80303e"}
{"parentUuid":"5d965408-f344-4e53-bb67-58a4e8917478","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01YHmsfuTwH9jxgtFsNFZ2WX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AryP2rMQH3Kgd6JXbnZgUi","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","old_string":"    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None","new_string":"    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n            self._is_last_stage = is_last_stage"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":222,"cache_read_input_tokens":91823,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":222},"output_tokens":24,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRttrbHhoj9t9myoxG8x","type":"assistant","uuid":"2299ef5b-427f-4e51-9de9-866953359fb7","timestamp":"2026-02-23T23:49:05.933Z"}
{"parentUuid":"2299ef5b-427f-4e51-9de9-866953359fb7","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01AryP2rMQH3Kgd6JXbnZgUi","toolUseID":"toolu_01AryP2rMQH3Kgd6JXbnZgUi","timestamp":"2026-02-23T23:49:05.938Z","uuid":"7be184c9-b403-40a4-a503-e1094634df8e"}
{"parentUuid":"2299ef5b-427f-4e51-9de9-866953359fb7","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AryP2rMQH3Kgd6JXbnZgUi","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py has been updated successfully."}]},"uuid":"a5e3c011-5437-4369-8e24-373ede5d7f84","timestamp":"2026-02-23T23:49:05.937Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","oldString":"    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None","newString":"    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n            self._is_last_stage = is_last_stage","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                if self._is_last_stage:\n                    return \"SHUTDOWN\"\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","structuredPatch":[{"oldStart":491,"oldLines":7,"newStart":491,"newLines":7,"lines":["         with self._lock:","             return self._fatal_error"," ","-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:","+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:","         \"\"\"Load a new stage's tasks into the queue.\"\"\"","         with self._lock:","             self._task_queue = deque(tasks)"]},{"oldStart":503,"oldLines":6,"newStart":503,"newLines":7,"lines":["             self._in_flight = {}","             self._task_attempts = {task.shard_idx: 0 for task in tasks}","             self._fatal_error = None","+            self._is_last_stage = is_last_stage"," ","     def _wait_for_stage(self) -> None:","         \"\"\"Block until current stage completes or error occurs.\"\"\""]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"2299ef5b-427f-4e51-9de9-866953359fb7"}
{"parentUuid":"a5e3c011-5437-4369-8e24-373ede5d7f84","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_0189Nqm7tY63eQiSnmKLbyHU","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BJPCw2xAR9RrpXgt8B9mwa","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","old_string":"        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):","new_string":"        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        # Identify the last stage that dispatches work to workers (non-RESHARD).\n        # On that stage, idle workers receive SHUTDOWN once all tasks are\n        # in-flight, so they exit eagerly instead of polling until\n        # coordinator.shutdown().\n        last_worker_stage_idx = max(\n            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n            default=-1,\n        )\n\n        for stage_idx, stage in enumerate(plan.stages):"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":506,"cache_read_input_tokens":92045,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":506},"output_tokens":24,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtuPcMKosNRHJLk63KL","type":"assistant","uuid":"cc9da4ff-262f-4b81-bd2f-85903a777cff","timestamp":"2026-02-23T23:49:13.367Z"}
{"parentUuid":"cc9da4ff-262f-4b81-bd2f-85903a777cff","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01BJPCw2xAR9RrpXgt8B9mwa","toolUseID":"toolu_01BJPCw2xAR9RrpXgt8B9mwa","timestamp":"2026-02-23T23:49:13.372Z","uuid":"5802aedf-d327-4360-b722-88b6e84c9760"}
{"parentUuid":"cc9da4ff-262f-4b81-bd2f-85903a777cff","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BJPCw2xAR9RrpXgt8B9mwa","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py has been updated successfully."}]},"uuid":"0c3123b8-8c8f-4822-8ece-d1b45cab73ee","timestamp":"2026-02-23T23:49:13.372Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","oldString":"        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):","newString":"        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        # Identify the last stage that dispatches work to workers (non-RESHARD).\n        # On that stage, idle workers receive SHUTDOWN once all tasks are\n        # in-flight, so they exit eagerly instead of polling until\n        # coordinator.shutdown().\n        last_worker_stage_idx = max(\n            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n            default=-1,\n        )\n\n        for stage_idx, stage in enumerate(plan.stages):","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                if self._is_last_stage:\n                    return \"SHUTDOWN\"\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n            self._is_last_stage = is_last_stage\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","structuredPatch":[{"oldStart":584,"oldLines":6,"newStart":584,"newLines":15,"lines":["         if not shards:","             return []"," ","+        # Identify the last stage that dispatches work to workers (non-RESHARD).","+        # On that stage, idle workers receive SHUTDOWN once all tasks are","+        # in-flight, so they exit eagerly instead of polling until","+        # coordinator.shutdown().","+        last_worker_stage_idx = max(","+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),","+            default=-1,","+        )","+","         for stage_idx, stage in enumerate(plan.stages):","             stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\""," "]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"cc9da4ff-262f-4b81-bd2f-85903a777cff"}
{"parentUuid":"0c3123b8-8c8f-4822-8ece-d1b45cab73ee","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_017WKMCJwsk2rKJwUHjrhaNJ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FUd1odHfuZbEAJS37jGmWD","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","old_string":"            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)","new_string":"            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":376,"cache_read_input_tokens":92551,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":376},"output_tokens":24,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtuwPnuueLFnG2kySPt","type":"assistant","uuid":"ef77395f-0b42-4cc8-9be1-b2a5cbdf0c0f","timestamp":"2026-02-23T23:49:18.296Z"}
{"parentUuid":"ef77395f-0b42-4cc8-9be1-b2a5cbdf0c0f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01FUd1odHfuZbEAJS37jGmWD","toolUseID":"toolu_01FUd1odHfuZbEAJS37jGmWD","timestamp":"2026-02-23T23:49:18.306Z","uuid":"813e335b-ffa9-4280-82db-2c7c4a67ed5c"}
{"parentUuid":"ef77395f-0b42-4cc8-9be1-b2a5cbdf0c0f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FUd1odHfuZbEAJS37jGmWD","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py has been updated successfully."}]},"uuid":"d80ca646-e97c-4748-b3f8-95cb724768a7","timestamp":"2026-02-23T23:49:18.305Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","oldString":"            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)","newString":"            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                if self._is_last_stage:\n                    return \"SHUTDOWN\"\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n            self._is_last_stage = is_last_stage\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        # Identify the last stage that dispatches work to workers (non-RESHARD).\n        # On that stage, idle workers receive SHUTDOWN once all tasks are\n        # in-flight, so they exit eagerly instead of polling until\n        # coordinator.shutdown().\n        last_worker_stage_idx = max(\n            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n            default=-1,\n        )\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks)\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","structuredPatch":[{"oldStart":606,"oldLines":7,"newStart":606,"newLines":7,"lines":["             # Build and submit tasks","             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)","             logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))","-            self._start_stage(stage_label, tasks)","+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))"," ","             # Wait for stage completion","             self._wait_for_stage()"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"ef77395f-0b42-4cc8-9be1-b2a5cbdf0c0f"}
{"parentUuid":"d80ca646-e97c-4748-b3f8-95cb724768a7","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_013RdTeSu35rSRDfxE4YWdgF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PDP8HwZswMBCwhQ2eRD6vh","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","old_string":"    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)","new_string":"    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":266,"cache_read_input_tokens":92927,"output_tokens":267,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":266,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"}},"requestId":"req_011CYRtvJYmp4rLC63DTQL2s","type":"assistant","uuid":"86ad5c82-6b9a-4c87-ab32-f91e0b4b68ab","timestamp":"2026-02-23T23:49:23.078Z"}
{"parentUuid":"86ad5c82-6b9a-4c87-ab32-f91e0b4b68ab","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01PDP8HwZswMBCwhQ2eRD6vh","toolUseID":"toolu_01PDP8HwZswMBCwhQ2eRD6vh","timestamp":"2026-02-23T23:49:23.085Z","uuid":"1452f783-6535-47f2-a0c6-a590ed58100c"}
{"parentUuid":"86ad5c82-6b9a-4c87-ab32-f91e0b4b68ab","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PDP8HwZswMBCwhQ2eRD6vh","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py has been updated successfully."}]},"uuid":"b55d27db-c882-406d-969e-1f0683500ad9","timestamp":"2026-02-23T23:49:23.084Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/src/zephyr/execution.py","oldString":"    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)","newString":"    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Actor-based execution engine for Zephyr pipelines.\n\nWorkers pull tasks from the coordinator, execute shard operations, and report\nresults back. This enables persistent worker state (caches, loaded models),\ntransient error recovery, and backend-agnostic dispatch via fray v2's Client\nprotocol.\n\"\"\"\n\nfrom __future__ import annotations\nimport enum\nimport logging\nimport os\nimport pickle\nimport threading\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterator, Sequence\nfrom contextlib import suppress\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field\nfrom typing import Any, Protocol\n\nimport fsspec\nfrom fray.v2 import ActorConfig, ActorHandle, Client, ResourceConfig\nfrom iris.temp_buckets import get_temp_bucket_path\nfrom iris.time_utils import ExponentialBackoff\n\nfrom zephyr.dataset import Dataset\nfrom zephyr.plan import (\n    ExecutionHint,\n    Join,\n    PhysicalOp,\n    PhysicalPlan,\n    SourceItem,\n    StageContext,\n    StageType,\n    compute_plan,\n    run_stage,\n)\nfrom zephyr.writers import ensure_parent_dir\n\nlogger = logging.getLogger(__name__)\n\n\nclass Chunk(Protocol):\n    def __iter__(self) -> Iterator: ...\n\n\n@dataclass(frozen=True)\nclass DiskChunk:\n    \"\"\"Reference to a chunk stored on disk.\n\n    Each write goes to a UUID-unique path to avoid collisions when multiple\n    workers race on the same shard.  No coordinator-side rename is needed;\n    the winning result's paths are used directly and the entire execution\n    directory is cleaned up after the pipeline completes.\n    \"\"\"\n\n    path: str\n    count: int\n\n    def __iter__(self) -> Iterator:\n        return iter(self.read())\n\n    @classmethod\n    def write(cls, path: str, data: list) -> DiskChunk:\n        \"\"\"Write *data* to a UUID-unique path derived from *path*.\n\n        The UUID suffix avoids collisions when multiple workers race on\n        the same shard.  The resulting path is used directly for reads —\n        no rename step is required.\n        \"\"\"\n        from zephyr.writers import unique_temp_path\n\n        ensure_parent_dir(path)\n        data = list(data)\n        count = len(data)\n\n        unique_path = unique_temp_path(path)\n        with fsspec.open(unique_path, \"wb\") as f:\n            pickle.dump(data, f)\n        return cls(path=unique_path, count=count)\n\n    def read(self) -> list:\n        \"\"\"Load chunk data from disk.\"\"\"\n        with fsspec.open(self.path, \"rb\") as f:\n            return pickle.load(f)\n\n\n@dataclass\nclass MemChunk:\n    \"\"\"In-memory chunk.\"\"\"\n\n    items: list[Any]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.items)\n\n\n@dataclass\nclass Shard:\n    \"\"\"An ordered sequence of chunks assigned to a single worker.\"\"\"\n\n    chunks: list[Chunk]\n\n    def __iter__(self) -> Iterator:\n        \"\"\"Flatten iteration over all items, loading chunks as needed.\"\"\"\n        for chunk in self.chunks:\n            yield from chunk\n\n    def iter_chunks(self) -> Iterator[list]:\n        \"\"\"Yield each chunk as a materialized list. Used by k-way merge in Reduce.\"\"\"\n        for chunk in self.chunks:\n            yield list(chunk)\n\n\n@dataclass\nclass ResultChunk:\n    \"\"\"Output chunk from a single worker task before resharding.\"\"\"\n\n    source_shard: int\n    target_shard: int\n    data: Chunk\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task.\"\"\"\n\n    chunks: list[ResultChunk]\n\n\ndef _generate_execution_id() -> str:\n    \"\"\"Generate unique ID for this execution to avoid conflicts.\"\"\"\n    return uuid.uuid4().hex[:12]\n\n\ndef _chunk_path(\n    prefix: str,\n    execution_id: str,\n    stage_name: str,\n    shard_idx: int,\n    chunk_idx: int,\n) -> str:\n    \"\"\"Generate path for a chunk file.\n\n    Format: {prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\n    \"\"\"\n    return f\"{prefix}/{execution_id}/{stage_name}/shard-{shard_idx:04d}/chunk-{chunk_idx:04d}.pkl\"\n\n\ndef _cleanup_execution(prefix: str, execution_id: str) -> None:\n    \"\"\"Remove all chunk files for an execution.\"\"\"\n    exec_dir = f\"{prefix}/{execution_id}\"\n    fs = fsspec.core.url_to_fs(exec_dir)[0]\n\n    if fs.exists(exec_dir):\n        try:\n            fs.rm(exec_dir, recursive=True)\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup chunks at {exec_dir}: {e}\")\n\n\nclass WorkerState(enum.Enum):\n    INIT = \"init\"\n    READY = \"ready\"\n    BUSY = \"busy\"\n    FAILED = \"failed\"\n    DEAD = \"dead\"\n\n\n@dataclass\nclass ShardTask:\n    \"\"\"Describes a unit of work for a worker: one shard through one stage.\"\"\"\n\n    shard_idx: int\n    total_shards: int\n    chunk_size: int\n    shard: Shard\n    operations: list[PhysicalOp]\n    stage_name: str = \"output\"\n    aux_shards: dict[int, Shard] | None = None\n\n\nclass ZephyrWorkerError(RuntimeError):\n    \"\"\"Raised when a worker encounters a fatal (non-transient) error.\"\"\"\n\n\n# Application errors that should never be retried by the execute() retry loop.\n# These are deterministic errors (bad plan, invalid config, programming bugs)\n# that would fail identically on every attempt. Infrastructure errors (OSError,\n# RuntimeError from dead actors, Ray actor errors) are NOT listed here so they\n# remain retryable.\n_NON_RETRYABLE_ERRORS = (ZephyrWorkerError, ValueError, TypeError, KeyError, AttributeError)\n\n\n# ---------------------------------------------------------------------------\n# WorkerContext protocol — the public interface exposed to user task code\n# ---------------------------------------------------------------------------\n\n\nclass WorkerContext(Protocol):\n    def get_shared(self, name: str) -> Any: ...\n\n\n_worker_ctx_var: ContextVar[ZephyrWorker | None] = ContextVar(\"zephyr_worker_ctx\", default=None)\n\n\ndef zephyr_worker_ctx() -> WorkerContext:\n    \"\"\"Get the current worker's context. Only valid inside a worker task.\"\"\"\n    ctx = _worker_ctx_var.get()\n    if ctx is None:\n        raise RuntimeError(\"zephyr_worker_ctx() called outside of a worker task\")\n    return ctx\n\n\n# ---------------------------------------------------------------------------\n# ZephyrCoordinator\n# ---------------------------------------------------------------------------\n\n\n@dataclass\nclass JobStatus:\n    stage: str\n    completed: int\n    total: int\n    retries: int\n    in_flight: int\n    queue_depth: int\n    done: bool\n    fatal_error: str\n    workers: dict[str, dict[str, Any]]\n\n\nclass ZephyrCoordinator:\n    \"\"\"Central coordinator actor that owns and manages the worker pool.\n\n    The coordinator creates workers via current_client(), runs a background\n    loop for discovery and heartbeat checking, and manages all pipeline\n    execution internally. Workers poll the coordinator for tasks until\n    receiving a SHUTDOWN signal.\n    \"\"\"\n\n    def __init__(self):\n        # Task management state\n        self._task_queue: deque[ShardTask] = deque()\n        self._results: dict[int, TaskResult] = {}\n        self._worker_states: dict[str, WorkerState] = {}\n        self._last_seen: dict[str, float] = {}\n        self._shared_data: dict[str, Any] = {}\n        self._stage_name: str = \"\"\n        self._total_shards: int = 0\n        self._completed_shards: int = 0\n        self._retries: int = 0\n        self._in_flight: dict[str, tuple[ShardTask, int]] = {}\n        self._task_attempts: dict[int, int] = {}\n        self._fatal_error: str | None = None\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n        self._no_workers_timeout: float = 60.0\n\n        # Worker management state (workers self-register via register_worker)\n        self._worker_handles: dict[str, ActorHandle] = {}\n        self._coordinator_thread: threading.Thread | None = None\n        self._shutdown: bool = False\n        self._is_last_stage: bool = False\n        self._initialized: bool = False\n\n        # Lock for accessing coordinator state from background thread\n        self._lock = threading.Lock()\n\n    def initialize(\n        self,\n        chunk_prefix: str,\n        coordinator_handle: ActorHandle,\n        no_workers_timeout: float = 60.0,\n    ) -> None:\n        \"\"\"Initialize coordinator for push-based worker registration.\n\n        Workers register themselves by calling register_worker() when they start.\n        This eliminates polling overhead and log spam from discover_new() calls.\n\n        Args:\n            chunk_prefix: Storage prefix for intermediate chunks\n            coordinator_handle: Handle to this coordinator actor (passed from context)\n            no_workers_timeout: Seconds to wait for at least one worker before failing.\n        \"\"\"\n        self._chunk_prefix = chunk_prefix\n        self._self_handle = coordinator_handle\n        self._no_workers_timeout = no_workers_timeout\n\n        logger.info(\"Coordinator initialized\")\n\n        # Start coordinator background loop (heartbeat checking only)\n        self._coordinator_thread = threading.Thread(\n            target=self._coordinator_loop, daemon=True, name=\"zephyr-coordinator-loop\"\n        )\n        self._coordinator_thread.start()\n        self._initialized = True\n\n    def register_worker(self, worker_id: str, worker_handle: ActorHandle) -> None:\n        \"\"\"Called by workers when they come online to register with coordinator.\n\n        Handles re-registration from reconstructed workers (e.g. after node\n        preemption) by updating the stale handle and resetting worker state.\n        \"\"\"\n        with self._lock:\n            if worker_id in self._worker_handles:\n                logger.info(\"Worker %s re-registering (likely reconstructed), updating handle\", worker_id)\n                self._worker_handles[worker_id] = worker_handle\n                self._worker_states[worker_id] = WorkerState.READY\n                self._last_seen[worker_id] = time.monotonic()\n                # NOTE: if there was a task assigned to the worker, there's a race condition between marking\n                # the worker as unhealthy via heartbeat and re-registration. If we do not requeue we may silently\n                # lose tasks.\n                self._maybe_requeue_worker_task(worker_id)\n                return\n\n            self._worker_handles[worker_id] = worker_handle\n            self._worker_states[worker_id] = WorkerState.READY\n            self._last_seen[worker_id] = time.monotonic()\n\n            logger.info(\"Worker %s registered, total: %d\", worker_id, len(self._worker_handles))\n\n    def _coordinator_loop(self) -> None:\n        \"\"\"Background loop for heartbeat checking only.\n\n        Workers register themselves via register_worker() - no polling needed.\n        \"\"\"\n        last_log_time = 0.0\n\n        while not self._shutdown:\n            # Check heartbeats, re-queue stale tasks.\n            # NOTE: we could use self._self_handle.check_heartbeats.remote() to\n            # serialize this with worker RPCs (pull_task, report_result) via the\n            # concurrency queue instead of running it inline.\n            self.check_heartbeats()\n\n            # Log status periodically during active execution\n            now = time.monotonic()\n            if self._has_active_execution() and now - last_log_time > 5.0:\n                self._log_status()\n                last_log_time = now\n\n            time.sleep(0.5)\n\n    def _has_active_execution(self) -> bool:\n        return self._execution_id != \"\" and self._total_shards > 0 and self._completed_shards < self._total_shards\n\n    def _log_status(self) -> None:\n        alive = sum(1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY})\n        dead = sum(1 for s in self._worker_states.values() if s in {WorkerState.FAILED, WorkerState.DEAD})\n        logger.info(\n            \"[%s] %d/%d complete, %d in-flight, %d queued, %d/%d workers alive, %d dead\",\n            self._stage_name,\n            self._completed_shards,\n            self._total_shards,\n            len(self._in_flight),\n            len(self._task_queue),\n            alive,\n            len(self._worker_handles),\n            dead,\n        )\n\n    def _maybe_requeue_worker_task(self, worker_id: str) -> None:\n        \"\"\"If the worker has a task in-flight, re-queue it and mark the worker as failed.\"\"\"\n        task_and_attempt = self._in_flight.pop(worker_id, None)\n        if task_and_attempt is not None:\n            logger.info(\"Worker %s had an in-flight task, re-queuing\", worker_id)\n            task, _old_attempt = task_and_attempt\n            self._task_attempts[task.shard_idx] += 1\n            self._task_queue.append(task)\n            self._retries += 1\n\n    def _check_worker_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Internal heartbeat check (called with lock held).\"\"\"\n        now = time.monotonic()\n        for worker_id, last in list(self._last_seen.items()):\n            if now - last > timeout and self._worker_states.get(worker_id) not in {WorkerState.FAILED, WorkerState.DEAD}:\n                logger.warning(f\"Zephyr worker {worker_id} failed to heartbeat within timeout ({now - last:.1f}s)\")\n                self._worker_states[worker_id] = WorkerState.FAILED\n                self._maybe_requeue_worker_task(worker_id)\n\n    def pull_task(self, worker_id: str) -> tuple[ShardTask, int, dict] | str | None:\n        \"\"\"Called by workers to get next task.\n\n        Returns:\n            - (task, attempt, config) tuple if task available\n            - \"SHUTDOWN\" string if coordinator is shutting down\n            - None if no task available (worker should backoff and retry)\n        \"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._worker_states[worker_id] = WorkerState.READY\n\n            if self._shutdown:\n                return \"SHUTDOWN\"\n\n            if self._fatal_error:\n                return None\n\n            if not self._task_queue:\n                if self._is_last_stage:\n                    return \"SHUTDOWN\"\n                return None\n\n            task = self._task_queue.popleft()\n            attempt = self._task_attempts[task.shard_idx]\n            self._in_flight[worker_id] = (task, attempt)\n            self._worker_states[worker_id] = WorkerState.BUSY\n\n            config = {\n                \"shared_data\": self._shared_data,\n                \"chunk_prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n            return (task, attempt, config)\n\n    def _assert_in_flight_consistent(self, worker_id: str, shard_idx: int) -> None:\n        \"\"\"Assert _in_flight[worker_id], if present, matches the reported shard.\n\n        Workers block on report_result/report_error before calling pull_task,\n        so _in_flight can never point to a different shard. It may be absent\n        if a heartbeat timeout already re-queued the task.\n        \"\"\"\n        in_flight = self._in_flight.get(worker_id)\n        if in_flight is not None:\n            assert in_flight[0].shard_idx == shard_idx, (\n                f\"_in_flight mismatch for {worker_id}: reporting shard {shard_idx}, \"\n                f\"but _in_flight tracks shard {in_flight[0].shard_idx}. \"\n                f\"This indicates report_result/pull_task reordering — workers must block on report_result.\"\n            )\n\n    def report_result(self, worker_id: str, shard_idx: int, attempt: int, result: TaskResult) -> None:\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n\n            current_attempt = self._task_attempts.get(shard_idx, 0)\n            if attempt != current_attempt:\n                logger.warning(\n                    f\"Ignoring stale result from worker {worker_id} for shard {shard_idx} \"\n                    f\"(attempt {attempt}, current {current_attempt})\"\n                )\n                return\n\n            self._results[shard_idx] = result\n            self._completed_shards += 1\n            self._in_flight.pop(worker_id, None)\n            self._worker_states[worker_id] = WorkerState.READY\n\n    def report_error(self, worker_id: str, shard_idx: int, error_info: str) -> None:\n        \"\"\"Worker reports a task failure. All errors are fatal.\"\"\"\n        with self._lock:\n            self._last_seen[worker_id] = time.monotonic()\n            self._assert_in_flight_consistent(worker_id, shard_idx)\n            self._in_flight.pop(worker_id, None)\n            self._fatal_error = error_info\n            self._worker_states[worker_id] = WorkerState.DEAD\n\n    def heartbeat(self, worker_id: str) -> None:\n        # No lock needed: _last_seen is only read by _check_worker_heartbeats\n        # (which holds the lock), and monotonic float writes are atomic on CPython.\n        self._last_seen[worker_id] = time.monotonic()\n\n    def get_status(self) -> JobStatus:\n        with self._lock:\n            return JobStatus(\n                stage=self._stage_name,\n                completed=self._completed_shards,\n                total=self._total_shards,\n                retries=self._retries,\n                in_flight=len(self._in_flight),\n                queue_depth=len(self._task_queue),\n                done=self._shutdown,\n                fatal_error=self._fatal_error,\n                workers={\n                    wid: {\n                        \"state\": state.value,\n                        \"last_seen_ago\": time.monotonic() - self._last_seen.get(wid, 0),\n                    }\n                    for wid, state in self._worker_states.items()\n                },\n            )\n\n    def get_fatal_error(self) -> str | None:\n        with self._lock:\n            return self._fatal_error\n\n    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n        \"\"\"Load a new stage's tasks into the queue.\"\"\"\n        with self._lock:\n            self._task_queue = deque(tasks)\n            self._results = {}\n            self._stage_name = stage_name\n            self._total_shards = len(tasks)\n            self._completed_shards = 0\n            self._retries = 0\n            self._in_flight = {}\n            self._task_attempts = {task.shard_idx: 0 for task in tasks}\n            self._fatal_error = None\n            self._is_last_stage = is_last_stage\n\n    def _wait_for_stage(self) -> None:\n        \"\"\"Block until current stage completes or error occurs.\"\"\"\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n        last_log_completed = -1\n        start_time = time.monotonic()\n        all_dead_since: float | None = None\n        no_workers_timeout = self._no_workers_timeout\n\n        while True:\n            with self._lock:\n                if self._fatal_error:\n                    raise ZephyrWorkerError(self._fatal_error)\n\n                completed = self._completed_shards\n                total = self._total_shards\n\n                if completed >= total:\n                    return\n\n                # Count alive workers (READY or BUSY), not just total registered.\n                # Dead/failed workers stay in _worker_handles but can't make progress.\n                alive_workers = sum(\n                    1 for s in self._worker_states.values() if s in {WorkerState.READY, WorkerState.BUSY}\n                )\n\n                if alive_workers == 0:\n                    now = time.monotonic()\n                    elapsed = now - start_time\n\n                    if all_dead_since is None:\n                        all_dead_since = now\n                        logger.warning(\"All workers are dead/failed. Waiting for workers to recover...\")\n\n                    dead_duration = now - all_dead_since\n                    if dead_duration > no_workers_timeout:\n                        raise ZephyrWorkerError(\n                            f\"No alive workers for {dead_duration:.1f}s \"\n                            f\"(total elapsed {elapsed:.1f}s). \"\n                            f\"All {len(self._worker_handles)} registered workers are dead/failed. \"\n                            \"Check cluster resources and worker group configuration.\"\n                        )\n                else:\n                    # Workers are alive — reset the dead timer\n                    all_dead_since = None\n\n            if completed != last_log_completed:\n                logger.info(\"[%s] %d/%d tasks completed\", self._stage_name, completed, total)\n                last_log_completed = completed\n                backoff.reset()\n\n            time.sleep(backoff.next_interval())\n\n    def _collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage.\"\"\"\n        with self._lock:\n            return dict(self._results)\n\n    def set_execution_config(self, shared_data: dict[str, Any], execution_id: str) -> None:\n        \"\"\"Set config for the current execution.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n    def run_pipeline(\n        self,\n        plan: PhysicalPlan,\n        shared_data: dict[str, Any],\n        execution_id: str,\n        hints: ExecutionHint,\n    ) -> list:\n        \"\"\"Run complete pipeline, blocking until done. Returns flattened results.\"\"\"\n        with self._lock:\n            self._shared_data = shared_data\n            self._execution_id = execution_id\n\n        shards = _build_source_shards(plan.source_items)\n        if not shards:\n            return []\n\n        # Identify the last stage that dispatches work to workers (non-RESHARD).\n        # On that stage, idle workers receive SHUTDOWN once all tasks are\n        # in-flight, so they exit eagerly instead of polling until\n        # coordinator.shutdown().\n        last_worker_stage_idx = max(\n            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n            default=-1,\n        )\n\n        for stage_idx, stage in enumerate(plan.stages):\n            stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n\n            if stage.stage_type == StageType.RESHARD:\n                shards = _reshard_refs(shards, stage.output_shards or len(shards))\n                continue\n\n            # Compute aux data for joins\n            aux_per_shard = self._compute_join_aux(stage.operations, shards, hints, stage_idx)\n\n            # Build and submit tasks\n            tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n            logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))\n\n            # Wait for stage completion\n            self._wait_for_stage()\n\n            # Collect and regroup results for next stage\n            result_refs = self._collect_results()\n            shards = _regroup_result_refs(result_refs, len(shards), output_shard_count=stage.output_shards)\n\n        # Flatten final results\n        flat_result = []\n        for shard in shards:\n            for chunk in shard.chunks:\n                flat_result.extend(list(chunk))\n\n        # Signal workers to shut down now that all stages are complete.\n        self.shutdown()\n\n        return flat_result\n\n    def _compute_join_aux(\n        self,\n        operations: list[PhysicalOp],\n        shard_refs: list[Shard],\n        hints: ExecutionHint,\n        parent_stage_idx: int,\n    ) -> list[dict[int, Shard]] | None:\n        \"\"\"Execute right sub-plans for join operations, returning aux refs per shard.\"\"\"\n        all_right_shard_refs: dict[int, list[Shard]] = {}\n\n        for i, op in enumerate(operations):\n            if not isinstance(op, Join) or op.right_plan is None:\n                continue\n\n            right_refs = _build_source_shards(op.right_plan.source_items)\n\n            for stage_idx, right_stage in enumerate(op.right_plan.stages):\n                if right_stage.stage_type == StageType.RESHARD:\n                    right_refs = _reshard_refs(right_refs, right_stage.output_shards or len(right_refs))\n                    continue\n\n                join_stage_label = f\"join-right-{parent_stage_idx}-{i}-stage{stage_idx}\"\n                right_tasks = _compute_tasks_from_shards(right_refs, right_stage, hints, stage_name=join_stage_label)\n                self._start_stage(join_stage_label, right_tasks)\n                self._wait_for_stage()\n                raw = self._collect_results()\n                right_refs = _regroup_result_refs(raw, len(right_refs), output_shard_count=right_stage.output_shards)\n\n            if len(shard_refs) != len(right_refs):\n                raise ValueError(\n                    f\"Sorted merge join requires equal shard counts. \"\n                    f\"Left has {len(shard_refs)} shards, right has {len(right_refs)} shards.\"\n                )\n            all_right_shard_refs[i] = right_refs\n\n        if not all_right_shard_refs:\n            return None\n\n        return [\n            {op_idx: right_refs[shard_idx] for op_idx, right_refs in all_right_shard_refs.items()}\n            for shard_idx in range(len(shard_refs))\n        ]\n\n    def shutdown(self) -> None:\n        \"\"\"Signal workers to exit. Worker group is managed by ZephyrContext.\"\"\"\n        logger.info(\"[coordinator.shutdown] Starting shutdown\")\n        self._shutdown = True\n\n        # Wait for coordinator thread to exit\n        if self._coordinator_thread is not None:\n            self._coordinator_thread.join(timeout=5.0)\n\n        logger.info(\"Coordinator shutdown complete\")\n\n    # Legacy compatibility methods (used by tests)\n    def set_shared_data(self, data: dict[str, Any]) -> None:\n        with self._lock:\n            self._shared_data = data\n\n    def get_shared_data(self) -> dict[str, Any]:\n        with self._lock:\n            return self._shared_data\n\n    def set_chunk_config(self, prefix: str, execution_id: str) -> None:\n        \"\"\"Configure chunk storage for this execution.\"\"\"\n        with self._lock:\n            self._chunk_prefix = prefix\n            self._execution_id = execution_id\n\n    def get_chunk_config(self) -> dict:\n        \"\"\"Return chunk storage configuration for workers.\"\"\"\n        with self._lock:\n            return {\n                \"prefix\": self._chunk_prefix,\n                \"execution_id\": self._execution_id,\n            }\n\n    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n        \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n        self._start_stage(stage_name, tasks)\n\n    def check_heartbeats(self, timeout: float = 30.0) -> None:\n        \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\n        with self._lock:\n            self._check_worker_heartbeats(timeout)\n\n    def collect_results(self) -> dict[int, TaskResult]:\n        \"\"\"Return results for the completed stage (legacy compat).\"\"\"\n        return self._collect_results()\n\n    def signal_done(self) -> None:\n        \"\"\"Signal workers that no more stages will be submitted (legacy compat).\"\"\"\n        self._shutdown = True\n\n\n# ---------------------------------------------------------------------------\n# ZephyrWorker\n# ---------------------------------------------------------------------------\n\n\nclass ZephyrWorker:\n    \"\"\"Long-lived worker actor that polls coordinator for tasks.\n\n    Workers register themselves with the coordinator on startup via\n    register_worker(), then poll continuously until receiving a SHUTDOWN\n    signal. Each task includes the execution config (shared data, chunk\n    prefix, execution ID), so workers can handle multiple executions\n    without restart.\n    \"\"\"\n\n    def __init__(self, coordinator_handle: ActorHandle):\n        from fray.v2 import current_actor\n\n        self._coordinator = coordinator_handle\n        self._shared_data: dict[str, Any] = {}\n        self._shutdown_event = threading.Event()\n        self._chunk_prefix: str = \"\"\n        self._execution_id: str = \"\"\n\n        # Build descriptive worker ID from actor context\n        actor_ctx = current_actor()\n        self._worker_id = f\"{actor_ctx.group_name}-{actor_ctx.index}\"\n\n        # Register with coordinator - wait is not stricly necessary, but it reduces the complexity\n        self._coordinator.register_worker.remote(self._worker_id, actor_ctx.handle).result(timeout=60.0)\n\n        # Start polling in a background thread\n        self._polling_thread = threading.Thread(\n            target=self._run_polling,\n            args=(coordinator_handle,),\n            daemon=True,\n            name=f\"zephyr-poll-{self._worker_id}\",\n        )\n        self._polling_thread.start()\n\n    def get_shared(self, name: str) -> Any:\n        return self._shared_data[name]\n\n    def _run_polling(self, coordinator: ActorHandle) -> None:\n        \"\"\"Main polling loop. Runs in a background thread started by __init__.\"\"\"\n        logger.info(\"[%s] Starting polling loop\", self._worker_id)\n\n        # Start heartbeat thread\n        heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            args=(coordinator,),\n            daemon=True,\n            name=f\"zephyr-hb-{self._worker_id}\",\n        )\n        heartbeat_thread.start()\n\n        try:\n            self._poll_loop(coordinator)\n        finally:\n            self._shutdown_event.set()\n            heartbeat_thread.join(timeout=5.0)\n            logger.debug(\"[%s] Polling loop ended\", self._worker_id)\n\n    def _heartbeat_loop(self, coordinator: ActorHandle, interval: float = 5.0) -> None:\n        logger.debug(\"[%s] Heartbeat loop starting\", self._worker_id)\n        heartbeat_count = 0\n        while not self._shutdown_event.is_set():\n            try:\n                # Block on result to avoid congesting the coordinator RPC pipe\n                # with fire-and-forget heartbeats.\n                coordinator.heartbeat.remote(self._worker_id).result()\n                heartbeat_count += 1\n                if heartbeat_count % 10 == 1:\n                    logger.debug(\"[%s] Sent heartbeat #%d\", self._worker_id, heartbeat_count)\n            except Exception as e:\n                logger.warning(\"[%s] Heartbeat failed: %s\", self._worker_id, e)\n            self._shutdown_event.wait(timeout=interval)\n        logger.debug(\"[%s] Heartbeat loop exiting after %d beats\", self._worker_id, heartbeat_count)\n\n    def _poll_loop(self, coordinator: ActorHandle) -> None:\n        \"\"\"Pure polling loop. Exits on SHUTDOWN signal, coordinator death, or shutdown event.\"\"\"\n        loop_count = 0\n        task_count = 0\n        backoff = ExponentialBackoff(initial=0.05, maximum=1.0)\n\n        while not self._shutdown_event.is_set():\n            loop_count += 1\n            if loop_count % 100 == 1:\n                logger.debug(\"[%s] Poll iteration #%d, tasks completed: %d\", self._worker_id, loop_count, task_count)\n\n            try:\n                response = coordinator.pull_task.remote(self._worker_id).result(timeout=30.0)\n            except Exception as e:\n                # Coordinator is dead or unreachable - exit gracefully\n                logger.info(\"[%s] pull_task failed (coordinator may be dead): %s\", self._worker_id, e)\n                break\n\n            # SHUTDOWN signal - exit cleanly\n            if response == \"SHUTDOWN\":\n                logger.info(\"[%s] Received SHUTDOWN\", self._worker_id)\n                break\n\n            # No task available - backoff and retry\n            if response is None:\n                time.sleep(backoff.next_interval())\n                continue\n\n            backoff.reset()\n\n            # Unpack task and config\n            task, attempt, config = response\n\n            logger.info(\"[%s] Executing task for shard %d (attempt %d)\", self._worker_id, task.shard_idx, attempt)\n            try:\n                result = self._execute_shard(task, config)\n                # Block until coordinator records the result. This ensures\n                # report_result is fully processed before the next pull_task,\n                # preventing _in_flight tracking races.\n                coordinator.report_result.remote(self._worker_id, task.shard_idx, attempt, result).result()\n                task_count += 1\n            except Exception as e:\n                logger.error(\"Worker %s error on shard %d: %s\", self._worker_id, task.shard_idx, e)\n                import traceback\n\n                coordinator.report_error.remote(\n                    self._worker_id,\n                    task.shard_idx,\n                    \"\".join(traceback.format_exc()),\n                ).result()\n\n    def _execute_shard(self, task: ShardTask, config: dict) -> TaskResult:\n        \"\"\"Execute a stage's operations on a single shard.\n\n        Returns list[TaskResult].\n        \"\"\"\n        # Update config for this execution\n        self._shared_data = config[\"shared_data\"]\n        self._chunk_prefix = config[\"chunk_prefix\"]\n        self._execution_id = config[\"execution_id\"]\n\n        _worker_ctx_var.set(self)\n\n        logger.info(\n            \"[shard %d/%d] Starting stage=%s, %d input chunks, %d ops\",\n            task.shard_idx,\n            task.total_shards,\n            task.stage_name,\n            len(task.shard.chunks),\n            len(task.operations),\n        )\n\n        stage_ctx = StageContext(\n            shard=task.shard,\n            shard_idx=task.shard_idx,\n            total_shards=task.total_shards,\n            chunk_size=task.chunk_size,\n            aux_shards=task.aux_shards,\n        )\n\n        results: list[ResultChunk] = []\n        chunk_idx = 0\n\n        for stage_output in run_stage(stage_ctx, task.operations):\n            chunk_path = _chunk_path(\n                self._chunk_prefix,\n                self._execution_id,\n                task.stage_name,\n                task.shard_idx,\n                chunk_idx,\n            )\n            chunk = list(stage_output.chunk)\n            chunk_ref = DiskChunk.write(chunk_path, chunk)\n            results.append(\n                ResultChunk(\n                    source_shard=stage_output.source_shard,\n                    target_shard=stage_output.target_shard,\n                    data=chunk_ref,\n                )\n            )\n            chunk_idx += 1\n            if chunk_idx % 10 == 0:\n                logger.info(\n                    \"[shard %d] Wrote %d chunks so far (latest: %d items)\",\n                    task.shard_idx,\n                    chunk_idx,\n                    len(stage_output.chunk),\n                )\n\n        logger.info(\"[shard %d] Complete: %d chunks produced\", task.shard_idx, chunk_idx)\n        return TaskResult(chunks=results)\n\n    def shutdown(self) -> None:\n        \"\"\"Stop the worker's polling loop.\"\"\"\n        self._shutdown_event.set()\n        if hasattr(self, \"_polling_thread\") and self._polling_thread.is_alive():\n            self._polling_thread.join(timeout=5.0)\n\n\ndef _regroup_result_refs(\n    result_refs: dict[int, TaskResult],\n    input_shard_count: int,\n    output_shard_count: int | None = None,\n) -> list[Shard]:\n    \"\"\"Regroup worker output refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n\n    for _input_idx, result in result_refs.items():\n        for chunk in result.chunks:\n            output_by_shard[chunk.target_shard].append(chunk.data)\n\n    num_output = max(max(output_by_shard.keys(), default=0) + 1, input_shard_count)\n    if output_shard_count is not None:\n        num_output = max(num_output, output_shard_count)\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_output)]\n\n\n@dataclass\nclass ZephyrContext:\n    \"\"\"Execution context for Zephyr pipelines.\n\n    Each execute() call creates a fresh coordinator and worker pool, runs\n    the pipeline, then tears everything down. Workers are sized to\n    min(max_workers, plan.num_shards) to avoid over-provisioning. Shared\n    data broadcast via put() is delivered to workers with each task.\n\n    Args:\n        client: The fray client to use. If None, auto-detects using current_client().\n        max_workers: Upper bound on worker count. The actual count is\n            min(max_workers, num_shards), computed at first execute(). If None,\n            defaults to os.cpu_count() for LocalClient, or 128 for distributed clients.\n        resources: Resource config per worker.\n        chunk_storage_prefix: Storage prefix for intermediate chunks. If None, defaults\n            to MARIN_PREFIX/tmp/zephyr or /tmp/zephyr.\n        name: Descriptive name for this context, used in actor group names for debugging.\n            Defaults to a random 8-character hex string.\n        no_workers_timeout: Seconds to wait for at least one worker before failing a stage.\n            Defaults to 600s.\n        max_execution_retries: Maximum number of times to retry a pipeline execution after\n            an infrastructure failure (e.g., coordinator VM preemption). Application errors\n            (ZephyrWorkerError) are never retried. Defaults to 3.\n    \"\"\"\n\n    client: Client | None = None\n    max_workers: int | None = None\n    resources: ResourceConfig = field(default_factory=lambda: ResourceConfig(cpu=1, ram=\"1g\"))\n    chunk_storage_prefix: str | None = None\n    name: str = \"\"\n    no_workers_timeout: float | None = None\n    max_execution_retries: int = 3\n\n    _shared_data: dict[str, Any] = field(default_factory=dict, repr=False)\n    _coordinator: ActorHandle | None = field(default=None, repr=False)\n    _coordinator_group: Any = field(default=None, repr=False)\n    _worker_group: Any = field(default=None, repr=False)\n    _worker_count: int = field(default=0, repr=False)\n    # NOTE: execute calls increment this at the very beginning\n    _pipeline_id: int = field(default=-1, repr=False)\n\n    def __post_init__(self):\n        if self.client is None:\n            from fray.v2.client import current_client\n\n            self.client = current_client()\n\n        if self.max_workers is None:\n            from fray.v2.local_backend import LocalClient\n\n            if isinstance(self.client, LocalClient):\n                self.max_workers = os.cpu_count() or 1\n            else:\n                # Default to 128 for distributed, but allow override\n                env_val = os.environ.get(\"ZEPHYR_MAX_WORKERS\")\n                self.max_workers = int(env_val) if env_val else 128\n\n        if self.no_workers_timeout is None:\n            self.no_workers_timeout = 600.0\n\n        if self.chunk_storage_prefix is None:\n            temp_prefix = get_temp_bucket_path(ttl_days=3, prefix=\"zephyr\")\n            if temp_prefix is None:\n                marin_prefix = os.environ.get(\"MARIN_PREFIX\")\n                if not marin_prefix:\n                    raise RuntimeError(\n                        \"MARIN_PREFIX must be set when using a distributed backend.\\n\"\n                        \"  Example: export MARIN_PREFIX=gs://marin-us-central2\"\n                    )\n                temp_prefix = f\"{marin_prefix}/tmp/zephyr\"\n\n            self.chunk_storage_prefix = temp_prefix\n\n        # make sure each context is unique\n        self.name = f\"{self.name}-{uuid.uuid4().hex[:8]}\"\n\n    def put(self, name: str, obj: Any) -> None:\n        \"\"\"Register shared data to broadcast to all workers.\n\n        Must be called before execute(). The object must be picklable.\n        Workers access it via zephyr_worker_ctx().get_shared(name).\n        \"\"\"\n        self._shared_data[name] = obj\n\n    def execute(\n        self,\n        dataset: Dataset,\n        hints: ExecutionHint = ExecutionHint(),\n        verbose: bool = False,\n        dry_run: bool = False,\n    ) -> Sequence:\n        \"\"\"Execute a dataset pipeline.\n\n        Each call creates a fresh coordinator and worker pool, runs the\n        pipeline, then tears everything down. If the coordinator dies\n        mid-execution (e.g., VM preemption), the pipeline is retried\n        with fresh actors up to ``max_execution_retries`` times.\n        Application errors (``ZephyrWorkerError``) are never retried.\n        \"\"\"\n        plan = compute_plan(dataset, hints)\n        if dry_run:\n            _print_plan(dataset.operations, plan)\n            return []\n\n        # NOTE: pipeline ID incremented on clean completion only\n        self._pipeline_id += 1\n        last_exception: Exception | None = None\n        for attempt in range(self.max_execution_retries + 1):\n            execution_id = _generate_execution_id()\n            logger.info(\n                \"Starting zephyr pipeline: %s (pipeline %d, attempt %d)\", execution_id, self._pipeline_id, attempt\n            )\n\n            try:\n                self._create_coordinator(attempt)\n                self._create_workers(plan.num_shards, attempt)\n\n                # Run pipeline on coordinator (blocking call).\n                # run_pipeline() calls coordinator.shutdown() at the end,\n                # which causes workers to receive SHUTDOWN on their next\n                # pull_task() call.\n                results = self._coordinator.run_pipeline.remote(plan, self._shared_data, execution_id, hints).result()\n\n                return results\n\n            except _NON_RETRYABLE_ERRORS:\n                raise\n\n            except Exception as e:\n                last_exception = e\n                if attempt >= self.max_execution_retries:\n                    raise\n\n                logger.warning(\n                    \"Pipeline attempt %d failed (%d retries left), retrying: %s\",\n                    attempt,\n                    self.max_execution_retries - attempt,\n                    e,\n                )\n\n            finally:\n                # Tear down coordinator and workers for this pipeline\n                self.shutdown()\n                # Clean up chunks for this execution\n                _cleanup_execution(self.chunk_storage_prefix, execution_id)\n\n        # Should be unreachable, but just in case\n        raise last_exception  # type: ignore[misc]\n\n    def _create_coordinator(self, attempt: int = 0) -> None:\n        \"\"\"Create a fresh coordinator actor.\"\"\"\n        # max_concurrency allows workers to call pull_task/report_result\n        # while run_pipeline blocks.\n        logger.info(\"Starting coordinator for %s (pipeline %d, attempt %d)\", self.name, self._pipeline_id, attempt)\n        coordinator_resources = ResourceConfig(cpu=1, ram=\"2g\")\n        coordinator_actor_config = ActorConfig(max_concurrency=100)\n        self._coordinator_group = self.client.create_actor_group(\n            ZephyrCoordinator,\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-coord\",\n            count=1,\n            resources=coordinator_resources,\n            actor_config=coordinator_actor_config,\n        )\n        self._coordinator = self._coordinator_group.wait_ready()[0]\n\n        self._coordinator.initialize.remote(\n            self.chunk_storage_prefix,\n            self._coordinator,\n            self.no_workers_timeout,\n        ).result()\n\n        logger.info(\"Coordinator initialized for %s\", self.name)\n\n    def _create_workers(self, num_shards: int, attempt: int = 0) -> None:\n        \"\"\"Create a fresh worker pool sized to demand.\n\n        The worker count is min(max_workers, num_shards) to avoid\n        over-provisioning when there are fewer shards than the cap.\n        \"\"\"\n        if num_shards == 0:\n            return\n\n        assert self.max_workers is not None  # set by __post_init__\n        actual_workers = min(self.max_workers, num_shards)\n        logger.info(\n            \"Starting worker group: %d workers (max_workers=%d, num_shards=%d, attempt=%d)\",\n            actual_workers,\n            self.max_workers,\n            num_shards,\n            attempt,\n        )\n        self._worker_group = self.client.create_actor_group(\n            ZephyrWorker,\n            self._coordinator,  # Pass coordinator handle as init arg\n            name=f\"zephyr-{self.name}-p{self._pipeline_id}-a{attempt}-workers\",\n            count=actual_workers,\n            resources=self.resources,\n        )\n\n        self._worker_count = actual_workers\n\n        # Wait for at least one worker to be ready before proceeding\n        self._worker_group.wait_ready(count=1, timeout=3600.0)\n\n        logger.info(\"ZephyrContext initialized with coordinator and %d workers\", actual_workers)\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown coordinator and all workers.\"\"\"\n        # Terminate worker group\n        if self._worker_group is not None:\n            with suppress(Exception):\n                self._worker_group.shutdown()\n\n        # Terminate coordinator actor group\n        if self._coordinator_group is not None:\n            with suppress(Exception):\n                self._coordinator_group.shutdown()\n\n        self._coordinator = None\n        self._coordinator_group = None\n        self._worker_group = None\n\n\ndef _reshard_refs(shards: list[Shard], num_shards: int) -> list[Shard]:\n    \"\"\"Reshard shard refs by output shard index without loading data.\"\"\"\n    output_by_shard: dict[int, list[Chunk]] = defaultdict(list)\n    output_idx = 0\n    for shard in shards:\n        for chunk in shard.chunks:\n            output_by_shard[output_idx].append(chunk)\n            output_idx = (output_idx + 1) % num_shards\n    return [Shard(chunks=output_by_shard.get(idx, [])) for idx in range(num_shards)]\n\n\ndef _build_source_shards(source_items: list[SourceItem]) -> list[Shard]:\n    \"\"\"Build shard data from source items.\n\n    Each source item becomes a single-element chunk in its assigned shard.\n    \"\"\"\n    items_by_shard: dict[int, list] = defaultdict(list)\n    for item in source_items:\n        items_by_shard[item.shard_idx].append(item.data)\n\n    num_shards = max(items_by_shard.keys()) + 1 if items_by_shard else 0\n    shards = []\n    for i in range(num_shards):\n        shards.append(Shard(chunks=[MemChunk(items=items_by_shard.get(i, []))]))\n\n    return shards\n\n\ndef _compute_tasks_from_shards(\n    shard_refs: list[Shard],\n    stage,\n    hints: ExecutionHint,\n    aux_per_shard: list[dict[int, Shard]] | None = None,\n    stage_name: str | None = None,\n) -> list[ShardTask]:\n    \"\"\"Convert shard references into ShardTasks for the coordinator.\"\"\"\n    total = len(shard_refs)\n    tasks = []\n    output_stage_name = stage_name or stage.stage_name(max_length=60)\n\n    for i, shard in enumerate(shard_refs):\n        aux_shards = None\n        if aux_per_shard and aux_per_shard[i]:\n            aux_shards = aux_per_shard[i]\n\n        tasks.append(\n            ShardTask(\n                shard_idx=i,\n                total_shards=total,\n                chunk_size=hints.chunk_size,\n                shard=shard,\n                operations=stage.operations,\n                stage_name=output_stage_name,\n                aux_shards=aux_shards,\n            )\n        )\n\n    return tasks\n\n\ndef _print_plan(original_ops: list, plan: PhysicalPlan) -> None:\n    \"\"\"Print the physical plan showing shard count and operation fusion.\"\"\"\n    total_physical_ops = sum(len(stage.operations) for stage in plan.stages)\n\n    logger.info(\"\\n=== Physical Execution Plan ===\\n\")\n    logger.info(f\"Shards: {plan.num_shards}\")\n    logger.info(f\"Original operations: {len(original_ops)}\")\n    logger.info(f\"Stages: {len(plan.stages)}\")\n    logger.info(f\"Physical ops: {total_physical_ops}\\n\")\n\n    logger.info(\"Original pipeline:\")\n    for i, op in enumerate(original_ops, 1):\n        logger.info(f\"  {i}. {op}\")\n\n    logger.info(\"\\nPhysical stages:\")\n    for i, stage in enumerate(plan.stages, 1):\n        stage_desc = stage.stage_name()\n        hint_parts = []\n        if stage.stage_type == StageType.RESHARD:\n            hint_parts.append(f\"reshard→{stage.output_shards}\")\n        if any(isinstance(op, Join) for op in stage.operations):\n            hint_parts.append(\"join\")\n        hint_str = f\" [{', '.join(hint_parts)}]\" if hint_parts else \"\"\n        logger.info(f\"  {i}. {stage_desc}{hint_str}\")\n\n    logger.info(\"\\n=== End Plan ===\\n\")\n","structuredPatch":[{"oldStart":703,"oldLines":9,"newStart":703,"newLines":9,"lines":["                 \"execution_id\": self._execution_id,","             }"," ","-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:","+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:","         \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"","-        self._start_stage(stage_name, tasks)","+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)"," ","     def check_heartbeats(self, timeout: float = 30.0) -> None:","         \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\""]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"86ad5c82-6b9a-4c87-ab32-f91e0b4b68ab"}
{"parentUuid":"b55d27db-c882-406d-969e-1f0683500ad9","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_016iWJe9S8mfxqQZswoLxcFx","type":"message","role":"assistant","content":[{"type":"text","text":"Now add the test. I'll add it after `test_pipeline_id_increments`:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":422,"cache_read_input_tokens":93193,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":422},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtveY4SkFFzksn7DYj7","type":"assistant","uuid":"5c67798d-ff08-4f36-83dc-4355f599fb8b","timestamp":"2026-02-23T23:49:26.759Z"}
{"parentUuid":"5c67798d-ff08-4f36-83dc-4355f599fb8b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_016iWJe9S8mfxqQZswoLxcFx","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019teCFvjWCxFXvY8UvAfHuh","name":"Read","input":{"file_path":"/Users/rav/projects/marin/lib/zephyr/tests/test_execution.py","offset":560,"limit":20},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":422,"cache_read_input_tokens":93193,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":422},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtveY4SkFFzksn7DYj7","type":"assistant","uuid":"1e6129ab-f436-4a36-8579-21e9e66b0448","timestamp":"2026-02-23T23:49:27.583Z"}
{"parentUuid":"1e6129ab-f436-4a36-8579-21e9e66b0448","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_019teCFvjWCxFXvY8UvAfHuh","toolUseID":"toolu_019teCFvjWCxFXvY8UvAfHuh","timestamp":"2026-02-23T23:49:27.589Z","uuid":"7840a9cd-d416-410b-8890-d07be4900319"}
{"parentUuid":"1e6129ab-f436-4a36-8579-21e9e66b0448","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019teCFvjWCxFXvY8UvAfHuh","type":"tool_result","content":"   560→def test_pipeline_id_increments(fray_client, tmp_path):\n   561→    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n   562→    ctx = ZephyrContext(\n   563→        client=fray_client,\n   564→        max_workers=10,\n   565→        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n   566→        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n   567→        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n   568→    )\n   569→    ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n   570→    assert ctx._pipeline_id == 0\n   571→\n   572→    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n   573→    assert ctx._pipeline_id == 1\n   574→\n   575→\n   576→def test_execute_retries_on_coordinator_death(tmp_path):\n   577→    \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n   578→    coordinator and worker pool and eventually succeeds.\n   579→"}]},"uuid":"055fe030-3d2b-494e-8d3a-9955feae408b","timestamp":"2026-02-23T23:49:27.588Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/rav/projects/marin/lib/zephyr/tests/test_execution.py","content":"def test_pipeline_id_increments(fray_client, tmp_path):\n    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=10,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n    assert ctx._pipeline_id == 0\n\n    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):\n    \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n    coordinator and worker pool and eventually succeeds.\n","numLines":20,"startLine":560,"totalLines":653}},"sourceToolAssistantUUID":"1e6129ab-f436-4a36-8579-21e9e66b0448"}
{"type":"file-history-snapshot","messageId":"b4481302-64fc-4a87-a3dd-31b91af97cc3","snapshot":{"messageId":"215cd644-6ea1-4f1a-a834-3de8a4126e9b","trackedFileBackups":{"lib/zephyr/src/zephyr/execution.py":{"backupFileName":"a08e09256175bf81@v1","version":1,"backupTime":"2026-02-23T23:48:44.115Z"},"lib/zephyr/tests/test_execution.py":{"backupFileName":"b59eec2f665fcb0a@v1","version":1,"backupTime":"2026-02-23T23:49:40.351Z"}},"timestamp":"2026-02-23T23:48:18.496Z"},"isSnapshotUpdate":true}
{"parentUuid":"055fe030-3d2b-494e-8d3a-9955feae408b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01NiHmH7WagrE3HqzCX54G8R","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01JC7Ad1raq843TmrbMdqVgL","name":"Edit","input":{"replace_all":false,"file_path":"/Users/rav/projects/marin/lib/zephyr/tests/test_execution.py","old_string":"    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):","new_string":"    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):\n    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n\n    # Non-last stage: empty queue returns None\n    coord.start_stage(\"stage-0\", [task], is_last_stage=False)\n    pulled = coord.pull_task(\"worker-A\")\n    assert pulled is not None and pulled != \"SHUTDOWN\"\n    _task, attempt, _config = pulled\n    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n\n    # Queue empty, but not last stage -> None\n    result = coord.pull_task(\"worker-A\")\n    assert result is None\n\n    # Last stage: empty queue returns SHUTDOWN\n    task2 = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test-last\",\n    )\n    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)\n    pulled = coord.pull_task(\"worker-A\")\n    assert pulled is not None and pulled != \"SHUTDOWN\"\n    _task, attempt, _config = pulled\n    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n\n    # Queue empty on last stage -> SHUTDOWN\n    result = coord.pull_task(\"worker-A\")\n    assert result == \"SHUTDOWN\"\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":452,"cache_read_input_tokens":93615,"output_tokens":777,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":452,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"}},"requestId":"req_011CYRtvzCVod7MEHyrgWGHY","type":"assistant","uuid":"b4481302-64fc-4a87-a3dd-31b91af97cc3","timestamp":"2026-02-23T23:49:40.347Z"}
{"parentUuid":"b4481302-64fc-4a87-a3dd-31b91af97cc3","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01JC7Ad1raq843TmrbMdqVgL","toolUseID":"toolu_01JC7Ad1raq843TmrbMdqVgL","timestamp":"2026-02-23T23:49:40.354Z","uuid":"bb661edb-cae6-4e6f-aa84-30420efb5b6d"}
{"parentUuid":"b4481302-64fc-4a87-a3dd-31b91af97cc3","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01JC7Ad1raq843TmrbMdqVgL","type":"tool_result","content":"The file /Users/rav/projects/marin/lib/zephyr/tests/test_execution.py has been updated successfully."}]},"uuid":"2cbd55de-4147-4eae-994c-766527c71fe6","timestamp":"2026-02-23T23:49:40.354Z","toolUseResult":{"filePath":"/Users/rav/projects/marin/lib/zephyr/tests/test_execution.py","oldString":"    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):","newString":"    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):\n    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n\n    # Non-last stage: empty queue returns None\n    coord.start_stage(\"stage-0\", [task], is_last_stage=False)\n    pulled = coord.pull_task(\"worker-A\")\n    assert pulled is not None and pulled != \"SHUTDOWN\"\n    _task, attempt, _config = pulled\n    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n\n    # Queue empty, but not last stage -> None\n    result = coord.pull_task(\"worker-A\")\n    assert result is None\n\n    # Last stage: empty queue returns SHUTDOWN\n    task2 = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test-last\",\n    )\n    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)\n    pulled = coord.pull_task(\"worker-A\")\n    assert pulled is not None and pulled != \"SHUTDOWN\"\n    _task, attempt, _config = pulled\n    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n\n    # Queue empty on last stage -> SHUTDOWN\n    result = coord.pull_task(\"worker-A\")\n    assert result == \"SHUTDOWN\"\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):","originalFile":"# Copyright 2025 The Marin Authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Tests for the actor-based execution engine (ZephyrContext).\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport time\nimport uuid\nfrom pathlib import Path\n\nimport pytest\nfrom fray.v2 import ResourceConfig\nfrom zephyr.dataset import Dataset\nfrom zephyr.execution import ZephyrContext, zephyr_worker_ctx\n\n\ndef test_simple_map(zephyr_ctx):\n    \"\"\"Map pipeline produces correct results.\"\"\"\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [2, 4, 6]\n\n\ndef test_filter(zephyr_ctx):\n    \"\"\"Filter pipeline produces correct results.\"\"\"\n    ds = Dataset.from_list([1, 2, 3, 4, 5]).filter(lambda x: x > 3)\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [4, 5]\n\n\ndef test_shared_data(fray_client):\n    \"\"\"Workers can access shared data via zephyr_worker_ctx().\"\"\"\n\n    def use_shared(x):\n        multiplier = zephyr_worker_ctx().get_shared(\"multiplier\")\n        return x * multiplier\n\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    zctx.put(\"multiplier\", 10)\n    ds = Dataset.from_list([1, 2, 3]).map(use_shared)\n    results = list(zctx.execute(ds))\n    assert sorted(results) == [10, 20, 30]\n    zctx.shutdown()\n\n\ndef test_multi_stage(zephyr_ctx):\n    \"\"\"Multi-stage pipeline (map + filter) works.\"\"\"\n    ds = Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x * 2).filter(lambda x: x > 5)\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [6, 8, 10]\n\n\ndef test_context_manager(fray_client):\n    \"\"\"ZephyrContext works without context manager.\"\"\"\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x + 1)\n    results = list(zctx.execute(ds))\n    assert sorted(results) == [2, 3, 4]\n\n\ndef test_write_jsonl(tmp_path, zephyr_ctx):\n    \"\"\"Pipeline writing to jsonl file.\"\"\"\n    output = str(tmp_path / \"out-{shard}.jsonl\")\n    ds = Dataset.from_list([{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]).write_jsonl(output)\n    results = list(zephyr_ctx.execute(ds))\n    assert len(results) == 3\n    # Verify all files were written and contain correct data\n    all_records = []\n    for path_str in results:\n        written = Path(path_str)\n        assert written.exists()\n        lines = written.read_text().strip().split(\"\\n\")\n        all_records.extend(json.loads(line) for line in lines)\n    assert sorted(all_records, key=lambda r: r[\"a\"]) == [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]\n\n\ndef test_dry_run(zephyr_ctx):\n    \"\"\"Dry run shows plan without executing.\"\"\"\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    results = list(zephyr_ctx.execute(ds, dry_run=True))\n    assert results == []\n\n\ndef test_flat_map(zephyr_ctx):\n    \"\"\"FlatMap pipeline produces correct results.\"\"\"\n    ds = Dataset.from_list([1, 2, 3]).flat_map(lambda x: [x, x * 10])\n    results = list(zephyr_ctx.execute(ds))\n    assert sorted(results) == [1, 2, 3, 10, 20, 30]\n\n\ndef test_empty_dataset(zephyr_ctx):\n    \"\"\"Empty dataset produces empty results.\"\"\"\n    ds = Dataset.from_list([])\n    results = list(zephyr_ctx.execute(ds))\n    assert results == []\n\n\ndef test_chunk_cleanup(fray_client, tmp_path):\n    \"\"\"Verify chunks are cleaned up after execution.\"\"\"\n    chunk_prefix = str(tmp_path / \"chunks\")\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)\n    results = list(ctx.execute(ds))\n\n    assert sorted(results) == [2, 4, 6]\n\n    # Verify chunks directory is cleaned up\n    import os\n\n    if os.path.exists(chunk_prefix):\n        # Should be empty or not exist\n        files = list(Path(chunk_prefix).rglob(\"*\"))\n        assert len(files) == 0, f\"Expected cleanup but found: {files}\"\n\n\ndef test_status_reports_alive_workers_not_total(tmp_path):\n    \"\"\"After heartbeat timeout, get_status workers dict reflects FAILED state,\n    and the status log distinguishes alive from total workers.\n\n    Also verifies that re-registering a worker that had an in-flight task\n    requeues that task so it is not silently lost.\n    \"\"\"\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Register 3 workers\n    for i in range(3):\n        coord.register_worker(f\"worker-{i}\", MagicMock())\n\n    status = coord.get_status()\n    assert len(status.workers) == 3\n    assert all(w[\"state\"] == \"ready\" for w in status.workers.values())\n\n    # worker-0 pulls the task so it becomes in-flight\n    pulled = coord.pull_task(\"worker-0\")\n    assert pulled is not None and pulled != \"SHUTDOWN\"\n\n    # Simulate 2 workers dying via heartbeat timeout\n    coord._last_seen[\"worker-0\"] = 0.0\n    coord._last_seen[\"worker-1\"] = 0.0\n    coord.check_heartbeats(timeout=30.0)\n\n    status = coord.get_status()\n    assert status.workers[\"worker-0\"][\"state\"] == \"failed\"\n    assert status.workers[\"worker-1\"][\"state\"] == \"failed\"\n    assert status.workers[\"worker-2\"][\"state\"] == \"ready\"\n\n    # Total workers in dict is still 3, but only 1 is alive\n    alive = sum(1 for w in status.workers.values() if w[\"state\"] in (\"ready\", \"busy\"))\n    assert alive == 1\n    assert len(status.workers) == 3\n\n    # worker-2 picks up the requeued task\n    pulled2 = coord.pull_task(\"worker-2\")\n    assert pulled2 is not None and pulled2 != \"SHUTDOWN\"\n\n    # Simulate worker-0 re-registering while worker-2 holds the task in-flight\n    # (race between heartbeat requeue and re-registration).\n    # Since worker-0 has no in-flight task anymore, this is a no-op for requeueing.\n    coord.register_worker(\"worker-0\", MagicMock())\n    status = coord.get_status()\n    assert status.workers[\"worker-0\"][\"state\"] == \"ready\"\n    alive = sum(1 for w in status.workers.values() if w[\"state\"] in (\"ready\", \"busy\"))\n    assert alive == 2  # worker-0 ready, worker-1 still failed, worker-2 busy\n\n    # Now test the direct re-registration requeue path:\n    # worker-2 dies while holding the task, and before heartbeat fires,\n    # it re-registers — the in-flight task should be requeued.\n    assert \"worker-2\" in coord._in_flight  # worker-2 has the task\n    coord.register_worker(\"worker-2\", MagicMock())\n    assert \"worker-2\" not in coord._in_flight  # in-flight cleared\n    assert len(coord._task_queue) == 1  # task was requeued\n\n\ndef test_no_duplicate_results_on_heartbeat_timeout(fray_client, tmp_path):\n    \"\"\"When a task is requeued after heartbeat timeout, the original worker's\n    stale result (from a previous attempt) is rejected by the coordinator.\"\"\"\n    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Worker A pulls task (attempt 0)\n    pulled = coord.pull_task(\"worker-A\")\n    assert pulled is not None\n    assert pulled != \"SHUTDOWN\"\n    _task_a, attempt_a, _config = pulled\n\n    # Simulate heartbeat timeout: mark worker-A stale and requeue\n    coord._last_seen[\"worker-A\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n\n    # Task should be requeued with incremented attempt\n    assert coord._task_attempts[0] == 1\n\n    # Worker B picks up the requeued task (attempt 1)\n    pulled_b = coord.pull_task(\"worker-B\")\n    assert pulled_b is not None\n    assert pulled_b != \"SHUTDOWN\"\n    _task_b, attempt_b, _config = pulled_b\n    assert attempt_b == 1\n\n    # Worker B reports success\n    coord.report_result(\"worker-B\", 0, attempt_b, TaskResult(chunks=[]))\n\n    # Worker A's stale result (attempt 0) should be ignored\n    coord.report_result(\"worker-A\", 0, attempt_a, TaskResult(chunks=[]))\n\n    # Only one completion should be counted\n    assert coord._completed_shards == 1\n\n\ndef test_disk_chunk_write_uses_unique_paths(tmp_path):\n    \"\"\"Each DiskChunk.write() writes to a unique location, avoiding collisions.\"\"\"\n    from zephyr.execution import DiskChunk\n\n    base_path = str(tmp_path / \"chunk.pkl\")\n    refs = [DiskChunk.write(base_path, [i]) for i in range(3)]\n\n    # Each written to a distinct UUID path (no rename needed)\n    paths = [r.path for r in refs]\n    assert len(set(paths)) == 3\n    for p in paths:\n        assert \".tmp.\" in p\n        assert Path(p).exists()\n\n    # Each chunk is directly readable\n    for i, ref in enumerate(refs):\n        assert ref.read() == [i]\n\n\ndef test_coordinator_accepts_winner_ignores_stale(tmp_path):\n    \"\"\"Coordinator accepts the winning result and ignores stale ones.\n\n    Stale chunk files are left for context-dir cleanup (no per-chunk deletion).\n    \"\"\"\n    from zephyr.execution import DiskChunk, ResultChunk, Shard, ShardTask, TaskResult, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Worker A pulls task (attempt 0)\n    pulled_a = coord.pull_task(\"worker-A\")\n    _task_a, attempt_a, _config = pulled_a\n\n    # Worker A writes a chunk (simulating slow completion)\n    stale_ref = DiskChunk.write(str(tmp_path / \"stale-chunk.pkl\"), [1, 2, 3])\n    assert Path(stale_ref.path).exists()\n\n    # Heartbeat timeout re-queues the task\n    coord._last_seen[\"worker-A\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n\n    # Worker B pulls and completes the re-queued task (attempt 1)\n    pulled_b = coord.pull_task(\"worker-B\")\n    _task_b, attempt_b, _config = pulled_b\n\n    winner_ref = DiskChunk.write(str(tmp_path / \"winner-chunk.pkl\"), [4, 5, 6])\n\n    coord.report_result(\n        \"worker-B\",\n        0,\n        attempt_b,\n        TaskResult(chunks=[ResultChunk(source_shard=0, target_shard=0, data=winner_ref)]),\n    )\n\n    # Worker A's stale result is rejected\n    coord.report_result(\n        \"worker-A\",\n        0,\n        attempt_a,\n        TaskResult(chunks=[ResultChunk(source_shard=0, target_shard=0, data=stale_ref)]),\n    )\n\n    # Winner's data is directly readable (no rename needed)\n    assert Path(winner_ref.path).exists()\n    assert winner_ref.read() == [4, 5, 6]\n\n    # Stale file still exists (cleaned up by context-dir cleanup, not coordinator)\n    assert Path(stale_ref.path).exists()\n    assert coord._completed_shards == 1\n\n\ndef test_chunk_streaming_low_memory(tmp_path):\n    \"\"\"Shard loads chunks one at a time from disk via iter_chunks.\n\n    Verifies iter_chunks yields data lazily and flat iteration works.\n    \"\"\"\n    from zephyr.execution import DiskChunk, Shard\n\n    # Write 3 chunks to disk (directly readable, no finalize needed)\n    refs = []\n    for i in range(3):\n        path = str(tmp_path / f\"chunk-{i}.pkl\")\n        chunk = DiskChunk.write(path, [i * 10 + j for j in range(5)])\n        refs.append(chunk)\n\n    shard = Shard(chunks=refs)\n\n    # iter_chunks yields correct data\n    chunks = list(shard.iter_chunks())\n    assert len(chunks) == 3\n    assert chunks[0] == [0, 1, 2, 3, 4]\n    assert chunks[2] == [20, 21, 22, 23, 24]\n\n    # flat iteration yields all items in order\n    assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n\n    # Re-iteration works (reads from disk again, not cached)\n    assert list(shard) == [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24]\n\n\ndef test_wait_for_stage_fails_when_all_workers_die(tmp_path):\n    \"\"\"When all registered workers become dead/failed, _wait_for_stage raises\n    after the no_workers_timeout instead of waiting forever.\"\"\"\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, WorkerState, ZephyrCoordinator, ZephyrWorkerError\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n    coord._no_workers_timeout = 0.5  # short timeout for test\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Register 2 workers\n    coord.register_worker(\"worker-0\", MagicMock())\n    coord.register_worker(\"worker-1\", MagicMock())\n\n    # Kill all workers via heartbeat timeout\n    coord._last_seen[\"worker-0\"] = 0.0\n    coord._last_seen[\"worker-1\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n\n    assert coord._worker_states[\"worker-0\"] == WorkerState.FAILED\n    assert coord._worker_states[\"worker-1\"] == WorkerState.FAILED\n\n    # _wait_for_stage should raise after the dead timer expires\n    with pytest.raises(ZephyrWorkerError, match=\"No alive workers\"):\n        coord._wait_for_stage()\n\n\ndef test_wait_for_stage_resets_dead_timer_on_recovery(tmp_path):\n    \"\"\"When a worker recovers (re-registers) after all workers died,\n    the dead timer resets and execution can continue.\"\"\"\n    import threading\n\n    from unittest.mock import MagicMock\n\n    from zephyr.execution import Shard, ShardTask, TaskResult, WorkerState, ZephyrCoordinator\n\n    coord = ZephyrCoordinator()\n    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n    coord.set_shared_data({})\n    coord._no_workers_timeout = 2.0\n\n    task = ShardTask(\n        shard_idx=0,\n        total_shards=1,\n        chunk_size=100,\n        shard=Shard(chunks=[]),\n        operations=[],\n        stage_name=\"test\",\n    )\n    coord.start_stage(\"test\", [task])\n\n    # Register and kill a worker\n    coord.register_worker(\"worker-0\", MagicMock())\n    coord._last_seen[\"worker-0\"] = 0.0\n    coord.check_heartbeats(timeout=0.0)\n    assert coord._worker_states[\"worker-0\"] == WorkerState.FAILED\n\n    # In a background thread, re-register the worker and complete the task\n    # after a short delay (simulating recovery before timeout expires)\n    def recover_and_complete():\n        time.sleep(0.3)\n        coord.register_worker(\"worker-0\", MagicMock())\n        pulled = coord.pull_task(\"worker-0\")\n        assert pulled is not None and pulled != \"SHUTDOWN\"\n        _task, attempt, _config = pulled\n        coord.report_result(\"worker-0\", 0, attempt, TaskResult(chunks=[]))\n\n    t = threading.Thread(target=recover_and_complete)\n    t.start()\n\n    # _wait_for_stage should succeed (worker recovers before timeout)\n    coord._wait_for_stage()\n    t.join(timeout=5.0)\n\n    assert coord._completed_shards == 1\n\n\ndef test_fresh_actors_per_execute(fray_client, tmp_path):\n    \"\"\"Each execute() creates and tears down its own coordinator and workers.\"\"\"\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(lambda x: x + 1)\n    results = list(zctx.execute(ds))\n    assert sorted(results) == [2, 3, 4]\n\n    # After execute(): everything is torn down\n    assert zctx._coordinator is None\n    assert zctx._worker_group is None\n    assert zctx._pipeline_id == 0\n\n    # Can execute again (creates fresh coordinator + workers)\n    ds2 = Dataset.from_list([10, 20]).map(lambda x: x * 2)\n    results2 = list(zctx.execute(ds2))\n    assert sorted(results2) == [20, 40]\n\n    assert zctx._coordinator is None\n    assert zctx._worker_group is None\n    assert zctx._pipeline_id == 1\n\n\ndef test_fatal_errors_fail_fast(fray_client, tmp_path):\n    \"\"\"Application errors (e.g. ValueError) cause immediate failure, no retries.\"\"\"\n    from zephyr.execution import ZephyrWorkerError\n\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    def exploding_map(x):\n        raise ValueError(f\"bad value: {x}\")\n\n    zctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(exploding_map)\n\n    start = time.monotonic()\n    with pytest.raises(ZephyrWorkerError, match=\"ValueError\"):\n        list(zctx.execute(ds))\n    elapsed = time.monotonic() - start\n\n    # Should fail fast — well under the 30s heartbeat timeout\n    assert elapsed < 15.0, f\"Took {elapsed:.1f}s, expected fast failure\"\n\n\ndef test_chunk_storage_with_join(fray_client, tmp_path):\n    \"\"\"Verify chunk storage works with join operations.\"\"\"\n    chunk_prefix = str(tmp_path / \"chunks\")\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n\n    # Create datasets for join\n    left = Dataset.from_list([{\"id\": 1, \"a\": \"x\"}, {\"id\": 2, \"a\": \"y\"}])\n    right = Dataset.from_list([{\"id\": 1, \"b\": \"p\"}, {\"id\": 2, \"b\": \"q\"}])\n\n    joined = left.sorted_merge_join(\n        right,\n        left_key=lambda x: x[\"id\"],\n        right_key=lambda x: x[\"id\"],\n        combiner=lambda left, right: {**left, **right},\n    )\n\n    results = sorted(list(ctx.execute(joined)), key=lambda x: x[\"id\"])\n    assert len(results) == 2\n    assert results[0] == {\"id\": 1, \"a\": \"x\", \"b\": \"p\"}\n    assert results[1] == {\"id\": 2, \"a\": \"y\", \"b\": \"q\"}\n\n\ndef test_workers_capped_to_shard_count(fray_client, tmp_path):\n    \"\"\"When max_workers > num_shards, only num_shards workers are created.\"\"\"\n    ds = Dataset.from_list([1, 2, 3])  # 3 shards\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=10,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    results = list(ctx.execute(ds.map(lambda x: x * 2)))\n    assert sorted(results) == [2, 4, 6]\n    # Everything torn down after execute; correct results prove workers\n    # were created and sized properly (min(10, 3) = 3)\n    assert ctx._pipeline_id == 0\n\n\ndef test_pipeline_id_increments(fray_client, tmp_path):\n    \"\"\"Pipeline ID increments after each execute(), ensuring unique actor names.\"\"\"\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=10,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=str(tmp_path / \"chunks\"),\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ctx.execute(Dataset.from_list([1, 2]).map(lambda x: x))\n    assert ctx._pipeline_id == 0\n\n    ctx.execute(Dataset.from_list([1, 2, 3, 4, 5]).map(lambda x: x))\n    assert ctx._pipeline_id == 1\n\n\ndef test_execute_retries_on_coordinator_death(tmp_path):\n    \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n    coordinator and worker pool and eventually succeeds.\n\n    Uses LocalClient directly because simulating coordinator death requires\n    manipulating the local actor registry.\n    \"\"\"\n    from fray.v2.local_backend import LocalClient, _local_actor_registry\n\n    client = LocalClient()\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    ctx = ZephyrContext(\n        client=client,\n        max_workers=2,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        max_execution_retries=2,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n\n    # First execute() succeeds normally\n    results = list(ctx.execute(Dataset.from_list([1, 2, 3]).map(lambda x: x * 2)))\n    assert sorted(results) == [2, 4, 6]\n\n    # Sabotage the registry so the *next* coordinator creation attempt fails\n    # on the first try. We do this by patching create_actor_group to fail once.\n    original_create = client.create_actor_group\n    fail_count = [0]\n\n    def flaky_create(*args, **kwargs):\n        group = original_create(*args, **kwargs)\n        if fail_count[0] == 0 and \"coord\" in kwargs.get(\"name\", \"\"):\n            fail_count[0] += 1\n            # Kill the coordinator immediately after creation to simulate death\n            handles = group.wait_ready()\n            endpoint = handles[0]._endpoint\n            _local_actor_registry.pop(endpoint, None)\n        return group\n\n    client.create_actor_group = flaky_create\n\n    # Next execute() should: fail on attempt 0 (dead coordinator),\n    # then succeed on attempt 1 with a fresh coordinator.\n    results = list(ctx.execute(Dataset.from_list([10, 20]).map(lambda x: x + 1)))\n    assert sorted(results) == [11, 21]\n\n    ctx.shutdown()\n    client.shutdown(wait=True)\n\n\ndef test_execute_does_not_retry_worker_errors(fray_client, tmp_path):\n    \"\"\"ZephyrWorkerError (application errors) are never retried.\"\"\"\n    from zephyr.execution import ZephyrWorkerError\n\n    chunk_prefix = str(tmp_path / \"chunks\")\n\n    def exploding_map(x):\n        raise ValueError(f\"bad value: {x}\")\n\n    ctx = ZephyrContext(\n        client=fray_client,\n        max_workers=1,\n        resources=ResourceConfig(cpu=1, ram=\"512m\"),\n        chunk_storage_prefix=chunk_prefix,\n        max_execution_retries=3,\n        name=f\"test-execution-{uuid.uuid4().hex[:8]}\",\n    )\n    ds = Dataset.from_list([1, 2, 3]).map(exploding_map)\n\n    start = time.monotonic()\n    with pytest.raises(ZephyrWorkerError, match=\"ValueError\"):\n        list(ctx.execute(ds))\n    elapsed = time.monotonic() - start\n\n    # Should fail fast — no retries for application errors\n    assert elapsed < 15.0, f\"Took {elapsed:.1f}s, expected fast failure (no retries)\"\n","structuredPatch":[{"oldStart":573,"oldLines":6,"newStart":573,"newLines":56,"lines":["     assert ctx._pipeline_id == 1"," "," ","+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):","+    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"","+    from unittest.mock import MagicMock","+","+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator","+","+    coord = ZephyrCoordinator()","+    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")","+    coord.set_shared_data({})","+","+    task = ShardTask(","+        shard_idx=0,","+        total_shards=1,","+        chunk_size=100,","+        shard=Shard(chunks=[]),","+        operations=[],","+        stage_name=\"test\",","+    )","+","+    # Non-last stage: empty queue returns None","+    coord.start_stage(\"stage-0\", [task], is_last_stage=False)","+    pulled = coord.pull_task(\"worker-A\")","+    assert pulled is not None and pulled != \"SHUTDOWN\"","+    _task, attempt, _config = pulled","+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))","+","+    # Queue empty, but not last stage -> None","+    result = coord.pull_task(\"worker-A\")","+    assert result is None","+","+    # Last stage: empty queue returns SHUTDOWN","+    task2 = ShardTask(","+        shard_idx=0,","+        total_shards=1,","+        chunk_size=100,","+        shard=Shard(chunks=[]),","+        operations=[],","+        stage_name=\"test-last\",","+    )","+    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)","+    pulled = coord.pull_task(\"worker-A\")","+    assert pulled is not None and pulled != \"SHUTDOWN\"","+    _task, attempt, _config = pulled","+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))","+","+    # Queue empty on last stage -> SHUTDOWN","+    result = coord.pull_task(\"worker-A\")","+    assert result == \"SHUTDOWN\"","+","+"," def test_execute_retries_on_coordinator_death(tmp_path):","     \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh","     coordinator and worker pool and eventually succeeds."]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"b4481302-64fc-4a87-a3dd-31b91af97cc3"}
{"parentUuid":"2cbd55de-4147-4eae-994c-766527c71fe6","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01FUmhuw17XdTiAP6xmoMfAm","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me generate the patch:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":819,"cache_read_input_tokens":94067,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":819},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtwvNLet8CotQzJTjNQ","type":"assistant","uuid":"e590d4de-f527-4391-8619-173836dc6dd8","timestamp":"2026-02-23T23:49:43.606Z"}
{"parentUuid":"e590d4de-f527-4391-8619-173836dc6dd8","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01FUmhuw17XdTiAP6xmoMfAm","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01H5xUNohso4wAnkCq14Qcr6","name":"Bash","input":{"command":"git diff lib/zephyr/src/zephyr/execution.py lib/zephyr/tests/test_execution.py","description":"Generate patch for the two changed files"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":819,"cache_read_input_tokens":94067,"output_tokens":124,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":819,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[],"speed":"standard"}},"requestId":"req_011CYRtwvNLet8CotQzJTjNQ","type":"assistant","uuid":"37c2c937-9072-4456-801a-c074156031c2","timestamp":"2026-02-23T23:49:44.670Z"}
{"parentUuid":"37c2c937-9072-4456-801a-c074156031c2","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01H5xUNohso4wAnkCq14Qcr6","type":"tool_result","content":"diff --git i/lib/zephyr/src/zephyr/execution.py w/lib/zephyr/src/zephyr/execution.py\nindex 6a1ea1f94..b63288860 100644\n--- i/lib/zephyr/src/zephyr/execution.py\n+++ w/lib/zephyr/src/zephyr/execution.py\n@@ -267,6 +267,7 @@ class ZephyrCoordinator:\n         self._worker_handles: dict[str, ActorHandle] = {}\n         self._coordinator_thread: threading.Thread | None = None\n         self._shutdown: bool = False\n+        self._is_last_stage: bool = False\n         self._initialized: bool = False\n \n         # Lock for accessing coordinator state from background thread\n@@ -403,6 +404,8 @@ class ZephyrCoordinator:\n                 return None\n \n             if not self._task_queue:\n+                if self._is_last_stage:\n+                    return \"SHUTDOWN\"\n                 return None\n \n             task = self._task_queue.popleft()\n@@ -488,7 +491,7 @@ class ZephyrCoordinator:\n         with self._lock:\n             return self._fatal_error\n \n-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue.\"\"\"\n         with self._lock:\n             self._task_queue = deque(tasks)\n@@ -500,6 +503,7 @@ class ZephyrCoordinator:\n             self._in_flight = {}\n             self._task_attempts = {task.shard_idx: 0 for task in tasks}\n             self._fatal_error = None\n+            self._is_last_stage = is_last_stage\n \n     def _wait_for_stage(self) -> None:\n         \"\"\"Block until current stage completes or error occurs.\"\"\"\n@@ -580,6 +584,15 @@ class ZephyrCoordinator:\n         if not shards:\n             return []\n \n+        # Identify the last stage that dispatches work to workers (non-RESHARD).\n+        # On that stage, idle workers receive SHUTDOWN once all tasks are\n+        # in-flight, so they exit eagerly instead of polling until\n+        # coordinator.shutdown().\n+        last_worker_stage_idx = max(\n+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n+            default=-1,\n+        )\n+\n         for stage_idx, stage in enumerate(plan.stages):\n             stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n \n@@ -593,7 +606,7 @@ class ZephyrCoordinator:\n             # Build and submit tasks\n             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n             logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n-            self._start_stage(stage_label, tasks)\n+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))\n \n             # Wait for stage completion\n             self._wait_for_stage()\n@@ -690,9 +703,9 @@ class ZephyrCoordinator:\n                 \"execution_id\": self._execution_id,\n             }\n \n-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n-        self._start_stage(stage_name, tasks)\n+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)\n \n     def check_heartbeats(self, timeout: float = 30.0) -> None:\n         \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\ndiff --git i/lib/zephyr/tests/test_execution.py w/lib/zephyr/tests/test_execution.py\nindex d713fed77..de03ff163 100644\n--- i/lib/zephyr/tests/test_execution.py\n+++ w/lib/zephyr/tests/test_execution.py\n@@ -573,6 +573,56 @@ def test_pipeline_id_increments(fray_client, tmp_path):\n     assert ctx._pipeline_id == 1\n \n \n+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):\n+    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"\n+    from unittest.mock import MagicMock\n+\n+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n+\n+    coord = ZephyrCoordinator()\n+    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n+    coord.set_shared_data({})\n+\n+    task = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test\",\n+    )\n+\n+    # Non-last stage: empty queue returns None\n+    coord.start_stage(\"stage-0\", [task], is_last_stage=False)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty, but not last stage -> None\n+    result = coord.pull_task(\"worker-A\")\n+    assert result is None\n+\n+    # Last stage: empty queue returns SHUTDOWN\n+    task2 = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test-last\",\n+    )\n+    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty on last stage -> SHUTDOWN\n+    result = coord.pull_task(\"worker-A\")\n+    assert result == \"SHUTDOWN\"\n+\n+\n def test_execute_retries_on_coordinator_death(tmp_path):\n     \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n     coordinator and worker pool and eventually succeeds.","is_error":false}]},"uuid":"914f33a7-c890-4e08-bbc8-da92c4b06471","timestamp":"2026-02-23T23:49:44.896Z","toolUseResult":{"stdout":"diff --git i/lib/zephyr/src/zephyr/execution.py w/lib/zephyr/src/zephyr/execution.py\nindex 6a1ea1f94..b63288860 100644\n--- i/lib/zephyr/src/zephyr/execution.py\n+++ w/lib/zephyr/src/zephyr/execution.py\n@@ -267,6 +267,7 @@ class ZephyrCoordinator:\n         self._worker_handles: dict[str, ActorHandle] = {}\n         self._coordinator_thread: threading.Thread | None = None\n         self._shutdown: bool = False\n+        self._is_last_stage: bool = False\n         self._initialized: bool = False\n \n         # Lock for accessing coordinator state from background thread\n@@ -403,6 +404,8 @@ class ZephyrCoordinator:\n                 return None\n \n             if not self._task_queue:\n+                if self._is_last_stage:\n+                    return \"SHUTDOWN\"\n                 return None\n \n             task = self._task_queue.popleft()\n@@ -488,7 +491,7 @@ class ZephyrCoordinator:\n         with self._lock:\n             return self._fatal_error\n \n-    def _start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def _start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue.\"\"\"\n         with self._lock:\n             self._task_queue = deque(tasks)\n@@ -500,6 +503,7 @@ class ZephyrCoordinator:\n             self._in_flight = {}\n             self._task_attempts = {task.shard_idx: 0 for task in tasks}\n             self._fatal_error = None\n+            self._is_last_stage = is_last_stage\n \n     def _wait_for_stage(self) -> None:\n         \"\"\"Block until current stage completes or error occurs.\"\"\"\n@@ -580,6 +584,15 @@ class ZephyrCoordinator:\n         if not shards:\n             return []\n \n+        # Identify the last stage that dispatches work to workers (non-RESHARD).\n+        # On that stage, idle workers receive SHUTDOWN once all tasks are\n+        # in-flight, so they exit eagerly instead of polling until\n+        # coordinator.shutdown().\n+        last_worker_stage_idx = max(\n+            (i for i, s in enumerate(plan.stages) if s.stage_type != StageType.RESHARD),\n+            default=-1,\n+        )\n+\n         for stage_idx, stage in enumerate(plan.stages):\n             stage_label = f\"stage{stage_idx}-{stage.stage_name(max_length=40)}\"\n \n@@ -593,7 +606,7 @@ class ZephyrCoordinator:\n             # Build and submit tasks\n             tasks = _compute_tasks_from_shards(shards, stage, hints, aux_per_shard, stage_name=stage_label)\n             logger.info(\"Starting stage %s with %d tasks\", stage_label, len(tasks))\n-            self._start_stage(stage_label, tasks)\n+            self._start_stage(stage_label, tasks, is_last_stage=(stage_idx == last_worker_stage_idx))\n \n             # Wait for stage completion\n             self._wait_for_stage()\n@@ -690,9 +703,9 @@ class ZephyrCoordinator:\n                 \"execution_id\": self._execution_id,\n             }\n \n-    def start_stage(self, stage_name: str, tasks: list[ShardTask]) -> None:\n+    def start_stage(self, stage_name: str, tasks: list[ShardTask], is_last_stage: bool = False) -> None:\n         \"\"\"Load a new stage's tasks into the queue (legacy compat).\"\"\"\n-        self._start_stage(stage_name, tasks)\n+        self._start_stage(stage_name, tasks, is_last_stage=is_last_stage)\n \n     def check_heartbeats(self, timeout: float = 30.0) -> None:\n         \"\"\"Marks stale workers as FAILED, re-queues their in-flight tasks.\"\"\"\ndiff --git i/lib/zephyr/tests/test_execution.py w/lib/zephyr/tests/test_execution.py\nindex d713fed77..de03ff163 100644\n--- i/lib/zephyr/tests/test_execution.py\n+++ w/lib/zephyr/tests/test_execution.py\n@@ -573,6 +573,56 @@ def test_pipeline_id_increments(fray_client, tmp_path):\n     assert ctx._pipeline_id == 1\n \n \n+def test_pull_task_returns_shutdown_on_last_stage_empty_queue(tmp_path):\n+    \"\"\"When the last stage's tasks are all in-flight or done, pull_task returns SHUTDOWN.\"\"\"\n+    from unittest.mock import MagicMock\n+\n+    from zephyr.execution import Shard, ShardTask, TaskResult, ZephyrCoordinator\n+\n+    coord = ZephyrCoordinator()\n+    coord.set_chunk_config(str(tmp_path / \"chunks\"), \"test-exec\")\n+    coord.set_shared_data({})\n+\n+    task = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test\",\n+    )\n+\n+    # Non-last stage: empty queue returns None\n+    coord.start_stage(\"stage-0\", [task], is_last_stage=False)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty, but not last stage -> None\n+    result = coord.pull_task(\"worker-A\")\n+    assert result is None\n+\n+    # Last stage: empty queue returns SHUTDOWN\n+    task2 = ShardTask(\n+        shard_idx=0,\n+        total_shards=1,\n+        chunk_size=100,\n+        shard=Shard(chunks=[]),\n+        operations=[],\n+        stage_name=\"test-last\",\n+    )\n+    coord.start_stage(\"stage-1\", [task2], is_last_stage=True)\n+    pulled = coord.pull_task(\"worker-A\")\n+    assert pulled is not None and pulled != \"SHUTDOWN\"\n+    _task, attempt, _config = pulled\n+    coord.report_result(\"worker-A\", 0, attempt, TaskResult(chunks=[]))\n+\n+    # Queue empty on last stage -> SHUTDOWN\n+    result = coord.pull_task(\"worker-A\")\n+    assert result == \"SHUTDOWN\"\n+\n+\n def test_execute_retries_on_coordinator_death(tmp_path):\n     \"\"\"When the coordinator dies mid-execution, execute() retries with a fresh\n     coordinator and worker pool and eventually succeeds.","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"37c2c937-9072-4456-801a-c074156031c2"}
{"parentUuid":"914f33a7-c890-4e08-bbc8-da92c4b06471","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01NtDWR24YCYDj2hMKk8W85w","type":"message","role":"assistant","content":[{"type":"text","text":"There's the clean patch. Let me verify the test passes:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1983,"cache_read_input_tokens":94886,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1983},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtxFnPkb6pQcS9Xi8gD","type":"assistant","uuid":"9b27fda4-5c92-4a9b-b9d8-b7fafd644b84","timestamp":"2026-02-23T23:49:50.108Z"}
{"parentUuid":"9b27fda4-5c92-4a9b-b9d8-b7fafd644b84","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01NtDWR24YCYDj2hMKk8W85w","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KPKkEjDkvoQenMySnr2UwE","name":"Bash","input":{"command":"uv run pytest lib/zephyr/tests/test_execution.py::test_pull_task_returns_shutdown_on_last_stage_empty_queue -xvs 2>&1 | tail -20","description":"Run the new test"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1983,"cache_read_input_tokens":94886,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1983},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtxFnPkb6pQcS9Xi8gD","type":"assistant","uuid":"591ac1b9-9893-4ae2-a256-1d5468f78d0b","timestamp":"2026-02-23T23:49:51.573Z"}
{"parentUuid":"591ac1b9-9893-4ae2-a256-1d5468f78d0b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KPKkEjDkvoQenMySnr2UwE","type":"tool_result","content":"============================= test session starts ==============================\nplatform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /Users/rav/projects/marin/.venv/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/rav/projects/marin/lib/zephyr\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, asyncio-0.24.0, respx-0.22.0, jaxtyping-0.3.5, playwright-0.7.2, xdist-3.8.0, timeout-2.4.0, flakefinder-1.1.0, openai-responses-0.13.1, base-url-2.1.0, profiling-1.8.1, cov-7.0.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ntimeout: 60.0s\ntimeout method: signal\ntimeout func_only: False\ncollecting ... collected 1 item\n\nlib/zephyr/tests/test_execution.py::test_pull_task_returns_shutdown_on_last_stage_empty_queue PASSED\n\n============================== 1 passed in 0.03s ===============================","is_error":false}]},"uuid":"682ce292-bcf4-4516-a087-047eb248eafc","timestamp":"2026-02-23T23:49:53.637Z","toolUseResult":{"stdout":"============================= test session starts ==============================\nplatform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /Users/rav/projects/marin/.venv/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/rav/projects/marin/lib/zephyr\nconfigfile: pyproject.toml\nplugins: anyio-4.12.1, asyncio-0.24.0, respx-0.22.0, jaxtyping-0.3.5, playwright-0.7.2, xdist-3.8.0, timeout-2.4.0, flakefinder-1.1.0, openai-responses-0.13.1, base-url-2.1.0, profiling-1.8.1, cov-7.0.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ntimeout: 60.0s\ntimeout method: signal\ntimeout func_only: False\ncollecting ... collected 1 item\n\nlib/zephyr/tests/test_execution.py::test_pull_task_returns_shutdown_on_last_stage_empty_queue PASSED\n\n============================== 1 passed in 0.03s ===============================","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"591ac1b9-9893-4ae2-a256-1d5468f78d0b"}
{"parentUuid":"682ce292-bcf4-4516-a087-047eb248eafc","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_015SozCsctA5k282zBWLNrJj","type":"message","role":"assistant","content":[{"type":"text","text":"Test passes. Let me also run the full test suite quickly to make sure nothing broke:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":456,"cache_read_input_tokens":96869,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":456},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtxuAgY6ELgCasNuQdo","type":"assistant","uuid":"55f0b724-6267-41a2-8cc7-1ac8404e671f","timestamp":"2026-02-23T23:49:57.913Z"}
{"parentUuid":"55f0b724-6267-41a2-8cc7-1ac8404e671f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_015SozCsctA5k282zBWLNrJj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GoD2zr8fSFZemFEWXXEq9B","name":"Bash","input":{"command":"uv run pytest lib/zephyr/tests/test_execution.py -x --timeout=120 2>&1 | tail -10","description":"Run all execution tests"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":456,"cache_read_input_tokens":96869,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":456},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRtxuAgY6ELgCasNuQdo","type":"assistant","uuid":"76ff3916-3217-4149-aaa7-4839053c4744","timestamp":"2026-02-23T23:49:58.760Z"}
{"type":"queue-operation","operation":"enqueue","timestamp":"2026-02-23T23:50:01.773Z","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","content":"{\"task_id\":\"be1ca60\",\"tool_use_id\":\"REDACTED\",\"description\":\"Run all execution tests\",\"task_type\":\"local_bash\"}"}
{"parentUuid":"76ff3916-3217-4149-aaa7-4839053c4744","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":3,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-0","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"750c9eb1-c9cc-40bb-9732-6ae8705dbf03","timestamp":"2026-02-23T23:50:01.773Z"}
{"parentUuid":"750c9eb1-c9cc-40bb-9732-6ae8705dbf03","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":4,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-1","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"71990270-9179-4c79-a942-f8893c6b6ece","timestamp":"2026-02-23T23:50:02.774Z"}
{"parentUuid":"71990270-9179-4c79-a942-f8893c6b6ece","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":5,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-2","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b57ccc40-0e5b-41e4-bec3-5b6dc02477ff","timestamp":"2026-02-23T23:50:03.793Z"}
{"parentUuid":"b57ccc40-0e5b-41e4-bec3-5b6dc02477ff","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":6,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-3","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"ddef9307-c5ef-4a0a-a7de-cbe4c02c3a31","timestamp":"2026-02-23T23:50:04.780Z"}
{"parentUuid":"ddef9307-c5ef-4a0a-a7de-cbe4c02c3a31","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":7,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-4","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"a397b29c-c75f-46a2-a59e-a2b3f1dd6221","timestamp":"2026-02-23T23:50:05.784Z"}
{"parentUuid":"a397b29c-c75f-46a2-a59e-a2b3f1dd6221","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":8,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-5","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b30dbd61-da38-4996-bda4-35916a479435","timestamp":"2026-02-23T23:50:06.786Z"}
{"parentUuid":"b30dbd61-da38-4996-bda4-35916a479435","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":9,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-6","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"5701ba25-66a0-4fe5-b9b2-c831cbe5a56f","timestamp":"2026-02-23T23:50:07.788Z"}
{"parentUuid":"5701ba25-66a0-4fe5-b9b2-c831cbe5a56f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":10,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-7","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"8129c6eb-aff1-419c-894f-15330d677e2c","timestamp":"2026-02-23T23:50:08.790Z"}
{"parentUuid":"8129c6eb-aff1-419c-894f-15330d677e2c","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":11,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-8","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"e381ed23-0a8b-4b30-bfca-dd6d516abcc5","timestamp":"2026-02-23T23:50:09.790Z"}
{"parentUuid":"e381ed23-0a8b-4b30-bfca-dd6d516abcc5","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":12,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-9","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"699090ef-8053-4ecc-bc1a-aa61f24564a2","timestamp":"2026-02-23T23:50:10.790Z"}
{"parentUuid":"699090ef-8053-4ecc-bc1a-aa61f24564a2","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":13,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-10","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"8dfef460-82ec-456f-afdd-df33bb662953","timestamp":"2026-02-23T23:50:11.791Z"}
{"parentUuid":"8dfef460-82ec-456f-afdd-df33bb662953","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":14,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-11","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"c40e8e87-72e1-420c-9846-aa87caaf750c","timestamp":"2026-02-23T23:50:12.792Z"}
{"parentUuid":"c40e8e87-72e1-420c-9846-aa87caaf750c","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":15,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-12","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"2a3ff52b-6a45-4f0f-8bcb-78a62094c87d","timestamp":"2026-02-23T23:50:13.795Z"}
{"parentUuid":"2a3ff52b-6a45-4f0f-8bcb-78a62094c87d","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":16,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-13","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"ef71ae1c-9ec5-41c9-a9a5-7615186d5300","timestamp":"2026-02-23T23:50:14.794Z"}
{"parentUuid":"ef71ae1c-9ec5-41c9-a9a5-7615186d5300","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":17,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-14","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"0f013cd2-8046-44a0-bc53-42f5bd6cc346","timestamp":"2026-02-23T23:50:15.803Z"}
{"parentUuid":"0f013cd2-8046-44a0-bc53-42f5bd6cc346","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":18,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-15","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"f9eb4ec4-868e-4789-888f-548db189e8d1","timestamp":"2026-02-23T23:50:16.805Z"}
{"parentUuid":"f9eb4ec4-868e-4789-888f-548db189e8d1","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":19,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-16","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"8bb4d7ca-bfa4-40e8-9733-22fc32870d5f","timestamp":"2026-02-23T23:50:17.807Z"}
{"parentUuid":"8bb4d7ca-bfa4-40e8-9733-22fc32870d5f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":20,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-17","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"97f1de7b-3307-45d8-a259-d1b9f61963c6","timestamp":"2026-02-23T23:50:18.807Z"}
{"parentUuid":"97f1de7b-3307-45d8-a259-d1b9f61963c6","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":21,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-18","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"d8986ad0-193d-4a16-86b0-728009c03e90","timestamp":"2026-02-23T23:50:19.808Z"}
{"parentUuid":"d8986ad0-193d-4a16-86b0-728009c03e90","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":22,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-19","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"c3d3a163-a68e-498e-aaef-c6e6d70677d9","timestamp":"2026-02-23T23:50:20.815Z"}
{"parentUuid":"c3d3a163-a68e-498e-aaef-c6e6d70677d9","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":23,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-20","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"e41b80d0-894d-4ce7-89ee-61d579f71199","timestamp":"2026-02-23T23:50:21.819Z"}
{"parentUuid":"e41b80d0-894d-4ce7-89ee-61d579f71199","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":24,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-21","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"fa9a0dbc-b468-4fcf-9475-3682e804ba8c","timestamp":"2026-02-23T23:50:22.817Z"}
{"parentUuid":"fa9a0dbc-b468-4fcf-9475-3682e804ba8c","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":25,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-22","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"08e67e76-b4f6-4109-91ee-3fe0f979d04a","timestamp":"2026-02-23T23:50:23.818Z"}
{"parentUuid":"08e67e76-b4f6-4109-91ee-3fe0f979d04a","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":26,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-23","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"1a907c5d-efbc-465a-a3c2-91a0bef83c11","timestamp":"2026-02-23T23:50:24.819Z"}
{"parentUuid":"1a907c5d-efbc-465a-a3c2-91a0bef83c11","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":27,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-24","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"e6ea47fc-ebaa-4785-bd6d-bcde23f4dcea","timestamp":"2026-02-23T23:50:25.821Z"}
{"parentUuid":"e6ea47fc-ebaa-4785-bd6d-bcde23f4dcea","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":28,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-25","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"56616f05-b317-4d62-a40e-d29872308832","timestamp":"2026-02-23T23:50:26.820Z"}
{"parentUuid":"56616f05-b317-4d62-a40e-d29872308832","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":29,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-26","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"3ae66b53-8f7c-405b-9eca-da08667eb363","timestamp":"2026-02-23T23:50:27.823Z"}
{"parentUuid":"3ae66b53-8f7c-405b-9eca-da08667eb363","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":30,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-27","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"aada61fe-e4d7-4fd2-b8fa-5fcb080f188f","timestamp":"2026-02-23T23:50:28.827Z"}
{"parentUuid":"aada61fe-e4d7-4fd2-b8fa-5fcb080f188f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":31,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-28","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"644aa6e0-7684-4683-ab56-35e4397e6f30","timestamp":"2026-02-23T23:50:29.825Z"}
{"parentUuid":"644aa6e0-7684-4683-ab56-35e4397e6f30","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":32,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-29","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"627f958b-19fe-418c-9f5e-9e40b7a74a6b","timestamp":"2026-02-23T23:50:30.826Z"}
{"parentUuid":"627f958b-19fe-418c-9f5e-9e40b7a74a6b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":33,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-30","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"71db93b2-288b-4810-a2cf-eb88bbbd880b","timestamp":"2026-02-23T23:50:31.826Z"}
{"parentUuid":"71db93b2-288b-4810-a2cf-eb88bbbd880b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":34,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-31","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"3667ec1b-df9e-4791-b890-1580a479881c","timestamp":"2026-02-23T23:50:32.827Z"}
{"parentUuid":"3667ec1b-df9e-4791-b890-1580a479881c","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":35,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-32","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"85fd9f97-b2a6-4717-a7e7-6de0ecc611fd","timestamp":"2026-02-23T23:50:33.829Z"}
{"parentUuid":"85fd9f97-b2a6-4717-a7e7-6de0ecc611fd","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":36,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-33","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"f4949c02-9e4b-4ca1-93e5-1c08a235cdbe","timestamp":"2026-02-23T23:50:34.830Z"}
{"parentUuid":"f4949c02-9e4b-4ca1-93e5-1c08a235cdbe","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":37,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-34","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"9f2186e2-2e5a-4eeb-952f-12c1eda7828e","timestamp":"2026-02-23T23:50:35.830Z"}
{"parentUuid":"9f2186e2-2e5a-4eeb-952f-12c1eda7828e","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":38,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-35","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"4a733d25-bb77-42df-944a-e82597a907f3","timestamp":"2026-02-23T23:50:36.832Z"}
{"parentUuid":"4a733d25-bb77-42df-944a-e82597a907f3","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":39,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-36","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"397d053c-c01a-402e-b871-857f7754a405","timestamp":"2026-02-23T23:50:37.833Z"}
{"parentUuid":"397d053c-c01a-402e-b871-857f7754a405","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":40,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-37","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"dd0fcbf8-2b27-4241-a613-ca99bf3018aa","timestamp":"2026-02-23T23:50:38.833Z"}
{"parentUuid":"dd0fcbf8-2b27-4241-a613-ca99bf3018aa","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":41,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-38","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"bb88151e-5df6-410e-8b70-d3401d745e7e","timestamp":"2026-02-23T23:50:39.834Z"}
{"parentUuid":"bb88151e-5df6-410e-8b70-d3401d745e7e","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":42,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-39","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b15da7bc-1729-47cb-9ccb-ababbd3367e9","timestamp":"2026-02-23T23:50:40.834Z"}
{"parentUuid":"b15da7bc-1729-47cb-9ccb-ababbd3367e9","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":43,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-40","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"3cc4c22d-beae-4b80-ac8c-432eb1bef296","timestamp":"2026-02-23T23:50:41.836Z"}
{"parentUuid":"3cc4c22d-beae-4b80-ac8c-432eb1bef296","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":44,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-41","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"f6e71d12-fbc0-478e-b354-0671a1b11371","timestamp":"2026-02-23T23:50:42.836Z"}
{"parentUuid":"f6e71d12-fbc0-478e-b354-0671a1b11371","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":45,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-42","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"a55ec58b-e22f-40d3-9170-e2f0e6f99566","timestamp":"2026-02-23T23:50:43.838Z"}
{"parentUuid":"a55ec58b-e22f-40d3-9170-e2f0e6f99566","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":46,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-43","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"d05522f0-8f20-4149-a37f-73dc844287f0","timestamp":"2026-02-23T23:50:44.839Z"}
{"parentUuid":"d05522f0-8f20-4149-a37f-73dc844287f0","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":47,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-44","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"92b3ce75-3dbe-46c1-b083-2fad67b1e774","timestamp":"2026-02-23T23:50:45.841Z"}
{"parentUuid":"92b3ce75-3dbe-46c1-b083-2fad67b1e774","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":48,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-45","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"1339a286-e5e3-44d8-a902-279386cfa279","timestamp":"2026-02-23T23:50:46.841Z"}
{"parentUuid":"1339a286-e5e3-44d8-a902-279386cfa279","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":49,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-46","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"7bbf0158-9b56-47e2-89ed-72893c798d2b","timestamp":"2026-02-23T23:50:47.842Z"}
{"parentUuid":"7bbf0158-9b56-47e2-89ed-72893c798d2b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":50,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-47","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"ddabeb00-8774-407f-82e5-2a74316a4d62","timestamp":"2026-02-23T23:50:48.843Z"}
{"parentUuid":"ddabeb00-8774-407f-82e5-2a74316a4d62","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":51,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-48","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"fa473e37-6a5a-4654-b8b7-33bd3b6406b2","timestamp":"2026-02-23T23:50:49.844Z"}
{"parentUuid":"fa473e37-6a5a-4654-b8b7-33bd3b6406b2","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":52,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-49","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"274fc5ac-b023-4b83-a1c3-1ce8a85ab332","timestamp":"2026-02-23T23:50:50.846Z"}
{"parentUuid":"274fc5ac-b023-4b83-a1c3-1ce8a85ab332","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":53,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-50","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"50ad7a7c-32de-4357-be4b-6c2624c27c09","timestamp":"2026-02-23T23:50:51.848Z"}
{"parentUuid":"50ad7a7c-32de-4357-be4b-6c2624c27c09","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":54,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-51","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"1023a19c-5470-440c-8d55-687bc52187e9","timestamp":"2026-02-23T23:50:52.855Z"}
{"parentUuid":"1023a19c-5470-440c-8d55-687bc52187e9","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":55,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-52","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"7144d979-d63b-4ba3-9a9e-8a3d029f58fc","timestamp":"2026-02-23T23:50:53.854Z"}
{"parentUuid":"7144d979-d63b-4ba3-9a9e-8a3d029f58fc","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":56,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-53","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"d6e212c7-6382-4ed9-994b-9239021e4044","timestamp":"2026-02-23T23:50:54.854Z"}
{"parentUuid":"d6e212c7-6382-4ed9-994b-9239021e4044","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":57,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-54","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"11e24fe6-0e7f-4b01-a8ea-2cdc6895a905","timestamp":"2026-02-23T23:50:55.856Z"}
{"parentUuid":"11e24fe6-0e7f-4b01-a8ea-2cdc6895a905","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":58,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-55","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"9b63f354-27c3-4aa4-a891-3479d1331149","timestamp":"2026-02-23T23:50:56.857Z"}
{"parentUuid":"9b63f354-27c3-4aa4-a891-3479d1331149","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":59,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-56","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"45df0d07-62de-48cb-9dfd-25162f1813ff","timestamp":"2026-02-23T23:50:57.857Z"}
{"parentUuid":"45df0d07-62de-48cb-9dfd-25162f1813ff","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":60,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-57","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"76987ac1-b46b-41da-a5f3-94cae88d91f1","timestamp":"2026-02-23T23:50:58.858Z"}
{"parentUuid":"76987ac1-b46b-41da-a5f3-94cae88d91f1","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":61,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-58","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"14141592-9f24-416a-a795-42d02799e596","timestamp":"2026-02-23T23:50:59.858Z"}
{"parentUuid":"14141592-9f24-416a-a795-42d02799e596","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":62,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-59","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"213d7591-70fc-4176-87b5-1adf08d5ccdf","timestamp":"2026-02-23T23:51:00.873Z"}
{"parentUuid":"213d7591-70fc-4176-87b5-1adf08d5ccdf","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":63,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-60","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"3b0b15ed-7ed2-4ac5-b7c0-5da1597444ed","timestamp":"2026-02-23T23:51:01.871Z"}
{"parentUuid":"3b0b15ed-7ed2-4ac5-b7c0-5da1597444ed","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":64,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-61","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"187764f3-5209-478b-ae0b-e429fc12bda5","timestamp":"2026-02-23T23:51:02.872Z"}
{"parentUuid":"187764f3-5209-478b-ae0b-e429fc12bda5","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":65,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-62","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b53596dd-d079-4267-883d-598d170e8d40","timestamp":"2026-02-23T23:51:03.873Z"}
{"parentUuid":"b53596dd-d079-4267-883d-598d170e8d40","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":66,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-63","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"52c59ab5-346f-45c4-a172-835982699cf1","timestamp":"2026-02-23T23:51:04.873Z"}
{"parentUuid":"52c59ab5-346f-45c4-a172-835982699cf1","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":67,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-64","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"224c636b-4882-473a-8551-be0171608ca0","timestamp":"2026-02-23T23:51:05.873Z"}
{"parentUuid":"224c636b-4882-473a-8551-be0171608ca0","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":68,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-65","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"27661b70-7134-4687-9abc-2e9f86329efa","timestamp":"2026-02-23T23:51:06.874Z"}
{"parentUuid":"27661b70-7134-4687-9abc-2e9f86329efa","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":69,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-66","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"2593ed4b-0251-43bc-8887-f24d3eec0193","timestamp":"2026-02-23T23:51:07.875Z"}
{"parentUuid":"2593ed4b-0251-43bc-8887-f24d3eec0193","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":70,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-67","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"e468f52e-5be5-4ed7-901c-ed8254555744","timestamp":"2026-02-23T23:51:08.875Z"}
{"parentUuid":"e468f52e-5be5-4ed7-901c-ed8254555744","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":71,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-68","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"8059d297-6afb-4a1d-bec6-c00258261de8","timestamp":"2026-02-23T23:51:09.877Z"}
{"parentUuid":"8059d297-6afb-4a1d-bec6-c00258261de8","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":72,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-69","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b99d6f8e-c125-4a7c-8494-b0c53fe53fd0","timestamp":"2026-02-23T23:51:10.877Z"}
{"parentUuid":"b99d6f8e-c125-4a7c-8494-b0c53fe53fd0","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":73,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-70","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b2c567b1-53b5-4e3f-b590-cb8aa44f6a33","timestamp":"2026-02-23T23:51:11.879Z"}
{"parentUuid":"b2c567b1-53b5-4e3f-b590-cb8aa44f6a33","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":74,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-71","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"d36d7320-1d42-4fcc-85b6-be0c5ead112a","timestamp":"2026-02-23T23:51:12.879Z"}
{"parentUuid":"d36d7320-1d42-4fcc-85b6-be0c5ead112a","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":75,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-72","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"a75d31b3-9bc0-4d5b-bf53-239e0de586ef","timestamp":"2026-02-23T23:51:13.880Z"}
{"parentUuid":"a75d31b3-9bc0-4d5b-bf53-239e0de586ef","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":76,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-73","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"3ed7141e-35b3-4442-8c01-ca601c120f61","timestamp":"2026-02-23T23:51:14.882Z"}
{"parentUuid":"3ed7141e-35b3-4442-8c01-ca601c120f61","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":77,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-74","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"622c97a4-6aaa-4c3d-88d0-e1d87cec4108","timestamp":"2026-02-23T23:51:15.885Z"}
{"parentUuid":"622c97a4-6aaa-4c3d-88d0-e1d87cec4108","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":78,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-75","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"018b4693-131e-44ba-8661-a7a7fadc0b4b","timestamp":"2026-02-23T23:51:16.886Z"}
{"parentUuid":"018b4693-131e-44ba-8661-a7a7fadc0b4b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":79,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-76","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"d0d40d9d-fb72-4d1b-bdfb-bd55472f7c10","timestamp":"2026-02-23T23:51:17.888Z"}
{"parentUuid":"d0d40d9d-fb72-4d1b-bdfb-bd55472f7c10","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":80,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-77","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"62ed652f-b7b9-41b3-8158-f019fc8b232d","timestamp":"2026-02-23T23:51:18.888Z"}
{"parentUuid":"62ed652f-b7b9-41b3-8158-f019fc8b232d","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":81,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-78","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"fe351052-cff9-4836-942e-56a126567ed4","timestamp":"2026-02-23T23:51:19.889Z"}
{"parentUuid":"fe351052-cff9-4836-942e-56a126567ed4","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":82,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-79","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"11593478-bdc7-46da-87fd-da055a5f723d","timestamp":"2026-02-23T23:51:20.889Z"}
{"parentUuid":"11593478-bdc7-46da-87fd-da055a5f723d","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":83,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-80","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"5ffc6180-e29b-4ed9-9787-2b6a0ee1d7e0","timestamp":"2026-02-23T23:51:21.889Z"}
{"parentUuid":"5ffc6180-e29b-4ed9-9787-2b6a0ee1d7e0","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":84,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-81","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"10ec95cf-0617-4d5a-b0d5-9c8e6a4f318b","timestamp":"2026-02-23T23:51:22.890Z"}
{"parentUuid":"10ec95cf-0617-4d5a-b0d5-9c8e6a4f318b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":85,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-82","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"4990a686-f7a4-4c3b-baf1-964ffcef9351","timestamp":"2026-02-23T23:51:23.891Z"}
{"parentUuid":"4990a686-f7a4-4c3b-baf1-964ffcef9351","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":86,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-83","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"87e49297-8181-447a-92c1-56ea88b2d0f6","timestamp":"2026-02-23T23:51:24.894Z"}
{"parentUuid":"87e49297-8181-447a-92c1-56ea88b2d0f6","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":87,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-84","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"c067f005-46dc-4742-a671-cecb69d73ef6","timestamp":"2026-02-23T23:51:25.894Z"}
{"parentUuid":"c067f005-46dc-4742-a671-cecb69d73ef6","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":88,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-85","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"4cb47dd7-2a4f-46bc-807c-9ae820453c6f","timestamp":"2026-02-23T23:51:26.894Z"}
{"parentUuid":"4cb47dd7-2a4f-46bc-807c-9ae820453c6f","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":89,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-86","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"60340835-43f3-41e1-997f-619e8107f2c1","timestamp":"2026-02-23T23:51:27.896Z"}
{"parentUuid":"60340835-43f3-41e1-997f-619e8107f2c1","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":90,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-87","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b8aa3003-f254-42d0-9b69-59bac70bf533","timestamp":"2026-02-23T23:51:28.899Z"}
{"parentUuid":"b8aa3003-f254-42d0-9b69-59bac70bf533","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":91,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-88","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"ce8003c4-3665-41f7-af7f-70bff3f75ddb","timestamp":"2026-02-23T23:51:29.900Z"}
{"parentUuid":"ce8003c4-3665-41f7-af7f-70bff3f75ddb","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":92,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-89","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"b12b2a55-4719-49e9-b95e-92beebcc06c2","timestamp":"2026-02-23T23:51:30.901Z"}
{"parentUuid":"b12b2a55-4719-49e9-b95e-92beebcc06c2","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":93,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-90","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"22402734-3df2-42c4-8a91-2502f50b8533","timestamp":"2026-02-23T23:51:31.903Z"}
{"parentUuid":"22402734-3df2-42c4-8a91-2502f50b8533","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":94,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-91","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"863d2817-f70e-4ceb-a674-57685a691cb3","timestamp":"2026-02-23T23:51:32.903Z"}
{"parentUuid":"863d2817-f70e-4ceb-a674-57685a691cb3","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":95,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-92","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"2d8af4ec-0309-41fb-b501-fc0441009c68","timestamp":"2026-02-23T23:51:33.904Z"}
{"parentUuid":"2d8af4ec-0309-41fb-b501-fc0441009c68","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":96,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-93","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"ebedb536-0729-4510-8242-092a19b4d595","timestamp":"2026-02-23T23:51:34.906Z"}
{"parentUuid":"ebedb536-0729-4510-8242-092a19b4d595","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":97,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-94","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"efd4bcbc-2369-4c0f-ad12-916a1aa2aa7e","timestamp":"2026-02-23T23:51:35.906Z"}
{"parentUuid":"efd4bcbc-2369-4c0f-ad12-916a1aa2aa7e","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":98,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-95","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"ec577859-6bdb-457f-b346-f75eb958dd36","timestamp":"2026-02-23T23:51:36.908Z"}
{"parentUuid":"ec577859-6bdb-457f-b346-f75eb958dd36","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":99,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-96","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"9f206ca9-ae44-4fa0-bb43-040236b464ca","timestamp":"2026-02-23T23:51:37.910Z"}
{"parentUuid":"9f206ca9-ae44-4fa0-bb43-040236b464ca","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":100,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-97","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"436a8814-102f-42e6-984d-1fb91ab110a0","timestamp":"2026-02-23T23:51:38.910Z"}
{"parentUuid":"436a8814-102f-42e6-984d-1fb91ab110a0","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":101,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-98","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"40be10b9-913a-4314-9309-843c96614841","timestamp":"2026-02-23T23:51:39.912Z"}
{"parentUuid":"40be10b9-913a-4314-9309-843c96614841","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":102,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-99","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"57628cf3-399c-4391-a06e-812919a09f1b","timestamp":"2026-02-23T23:51:40.912Z"}
{"parentUuid":"57628cf3-399c-4391-a06e-812919a09f1b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":103,"totalLines":0,"totalBytes":0,"taskId":"be1ca60"},"toolUseID":"bash-progress-100","parentToolUseID":"toolu_01GoD2zr8fSFZemFEWXXEq9B","uuid":"009c6052-35b9-441a-9cc2-bdfcb2b46ef8","timestamp":"2026-02-23T23:51:41.912Z"}
{"parentUuid":"76ff3916-3217-4149-aaa7-4839053c4744","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GoD2zr8fSFZemFEWXXEq9B","type":"tool_result","content":"================== 59 passed, 3 warnings in 99.75s (0:01:39) ===================\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:36,083 INFO zephyr.execution:357 [stage0-Map] 0/3 complete, 0 in-flight, 3 queued, 0/0 workers alive, 0 dead\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,579 INFO zephyr.execution:296 Coordinator initialized\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,583 INFO zephyr.execution:608 Starting stage stage0-Map with 3 tasks\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,583 WARNING zephyr.execution:539 All workers are dead/failed. Waiting for workers to recover...\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,583 INFO zephyr.execution:554 [stage0-Map] 0/3 tasks completed\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:36,335 INFO zephyr.execution:327 Worker zephyr-test-execution-1ca4f4b4-c9599ef3-p0-a0-workers-0 registered, total: 1\n\u001b[36m(_RayActorHost pid=86106)\u001b[0m 2026-02-23 15:51:36,335 INFO zephyr.execution:769 [zephyr-test-execution-1ca4f4b4-c9599ef3-p0-a0-workers-0] Starting polling loop\n\u001b[36m(_RayActorHost pid=86106)\u001b[0m 2026-02-23 15:51:36,339 INFO zephyr.execution:836 [zephyr-test-execution-1ca4f4b4-c9599ef3-p0-a0-workers-0] Executing task for shard 0 (attempt 0)\n\u001b[36m(_RayActorHost pid=86106)\u001b[0m 2026-02-23 15:51:36,340 INFO zephyr.execution:866 [shard 0/3] Starting stage=stage0-Map, 1 input chunks, 1 ops","is_error":false}]},"uuid":"bb8f1276-0bf3-48a6-8d01-14bf25b0a37b","timestamp":"2026-02-23T23:51:42.498Z","toolUseResult":{"stdout":"================== 59 passed, 3 warnings in 99.75s (0:01:39) ===================\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:36,083 INFO zephyr.execution:357 [stage0-Map] 0/3 complete, 0 in-flight, 3 queued, 0/0 workers alive, 0 dead\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,579 INFO zephyr.execution:296 Coordinator initialized\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,583 INFO zephyr.execution:608 Starting stage stage0-Map with 3 tasks\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,583 WARNING zephyr.execution:539 All workers are dead/failed. Waiting for workers to recover...\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:35,583 INFO zephyr.execution:554 [stage0-Map] 0/3 tasks completed\n\u001b[36m(_RayActorHost pid=86101)\u001b[0m 2026-02-23 15:51:36,335 INFO zephyr.execution:327 Worker zephyr-test-execution-1ca4f4b4-c9599ef3-p0-a0-workers-0 registered, total: 1\n\u001b[36m(_RayActorHost pid=86106)\u001b[0m 2026-02-23 15:51:36,335 INFO zephyr.execution:769 [zephyr-test-execution-1ca4f4b4-c9599ef3-p0-a0-workers-0] Starting polling loop\n\u001b[36m(_RayActorHost pid=86106)\u001b[0m 2026-02-23 15:51:36,339 INFO zephyr.execution:836 [zephyr-test-execution-1ca4f4b4-c9599ef3-p0-a0-workers-0] Executing task for shard 0 (attempt 0)\n\u001b[36m(_RayActorHost pid=86106)\u001b[0m 2026-02-23 15:51:36,340 INFO zephyr.execution:866 [shard 0/3] Starting stage=stage0-Map, 1 input chunks, 1 ops","stderr":"","interrupted":false,"isImage":false,"noOutputExpected":false},"sourceToolAssistantUUID":"76ff3916-3217-4149-aaa7-4839053c4744"}
{"parentUuid":"bb8f1276-0bf3-48a6-8d01-14bf25b0a37b","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","message":{"model":"claude-opus-4-6","id":"msg_01YWhBaoi1SQ7Ra3mwxMnts4","type":"message","role":"assistant","content":[{"type":"text","text":"All 59 tests pass. The patch is clean and ready."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":765,"cache_read_input_tokens":97325,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":765},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYRu6vjRGxoY2nXjXyUpE","type":"assistant","uuid":"a5e532bd-0076-400f-bb63-ba26063b7812","timestamp":"2026-02-23T23:51:46.833Z"}
{"parentUuid":"a5e532bd-0076-400f-bb63-ba26063b7812","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"entire hooks claude-code stop"},"parentToolUseID":"1b5fb065-292e-44ce-8ff6-b0beb7fed0da","toolUseID":"1b5fb065-292e-44ce-8ff6-b0beb7fed0da","timestamp":"2026-02-23T23:51:46.939Z","uuid":"0de06617-529a-4860-9fc2-04dd310be814"}
{"type":"queue-operation","operation":"remove","timestamp":"2026-02-23T23:51:50.450Z","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8"}
{"parentUuid":"0de06617-529a-4860-9fc2-04dd310be814","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"system","subtype":"stop_hook_summary","hookCount":1,"hookInfos":[{"command":"entire hooks claude-code stop","durationMs":3420}],"hookErrors":[],"preventedContinuation":false,"stopReason":"","hasOutput":true,"level":"suggestion","timestamp":"2026-02-23T23:51:50.359Z","uuid":"d398675f-0191-44e9-bc64-a533f9585173","toolUseID":"1b5fb065-292e-44ce-8ff6-b0beb7fed0da"}
{"parentUuid":"d398675f-0191-44e9-bc64-a533f9585173","isSidechain":false,"userType":"external","cwd":"/Users/rav/projects/marin","sessionId":"6ef2f338-8b9e-403c-82f8-5261bb727ad8","version":"2.1.50","gitBranch":"rav-shrink-pool","slug":"vivid-waddling-bee","type":"system","subtype":"turn_duration","durationMs":211864,"timestamp":"2026-02-23T23:51:50.360Z","uuid":"07c4f3bc-e613-4151-8da1-5d07f212c1b1","isMeta":false}
