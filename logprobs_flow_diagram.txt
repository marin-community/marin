================================================================================
              LOGPROBS CALCULATION FLOW IN MARIN RL SYSTEM
================================================================================

                      Two Separate Chains of Functions

================================================================================
                    CHAIN 1: ROLLOUT WORKER (Response Logprobs Only)
================================================================================

    rollout_worker.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  RolloutWorker.run()                                            │
    │    ↓                                                            │
    │  RolloutWorker._sample_batch(lesson_id, n_examples, ...)        │
    │    • Line 228-294                                               │
    │    • Creates LevanterInferenceContext (line 240-245)            │
    └─────────────────────────────────────────────────────────────────┘
                                    ↓
    environments/base.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  MarinEnv.sample(inference_ctx, ...)  [Abstract]                │
    │    • Implemented by environment subclasses                      │
    │    • Calls inference_ctx.generate(prompts, ...)                 │
    └─────────────────────────────────────────────────────────────────┘
                                    ↓
    environments/base.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  LevanterInferenceContext.generate(prompts, temp, n_gen)        │
    │    • Line 109-233                                               │
    │                                                                 │
    │    1. Creates OpenAI client (line 122)                          │
    │    2. Calls client.chat.completions.create(                     │
    │         logprobs=True,  ← KEY: Request logprobs!                │
    │         max_tokens=...,                                         │
    │         temperature=...,                                        │
    │         n=n_generations                                         │
    │       )                                                         │
    │                                                                 │
    │    3. Extract logprobs (line 157-186):                          │
    │       for token_logprob in choice.logprobs.content:             │
    │         - token_str = token_logprob.token                       │
    │         - logprob = token_logprob.logprob                       │
    │         - token_id = tokenizer.encode(token_str)                │
    │                                                                 │
    │    4. Create InferenceChoice:                                   │
    │         response_tokens = np.array(tokens)                      │
    │         logprobs = np.array(logprobs)  ← RESPONSE ONLY          │
    └─────────────────────────────────────────────────────────────────┘
                                    ↓
    types.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  InferenceResponse & InferenceChoice                            │
    │    • InferenceResponse:                                         │
    │        - prompt: str                                            │
    │        - prompt_tokens: np.ndarray                              │
    │        - choices: list[InferenceChoice]                         │
    │                                                                 │
    │    • InferenceChoice:                                           │
    │        - response_text: str                                     │
    │        - response_tokens: np.ndarray                            │
    │        - logprobs: np.ndarray  ← RESPONSE LOGPROBS ONLY!        │
    └─────────────────────────────────────────────────────────────────┘
                                    ↓
    Environment creates Rollout
    ┌─────────────────────────────────────────────────────────────────┐
    │  Rollout (types.py line 108-134)                                │
    │    • prompt_tokens: from tokenizer.encode(prompt)               │
    │    • response_tokens: from InferenceChoice.response_tokens      │
    │    • response_logprobs: from InferenceChoice.logprobs           │
    │    • token_rewards: calculated by environment                   │
    │                                                                 │
    │  ⚠️  NOTE: NO prompt logprobs stored!                           │
    └─────────────────────────────────────────────────────────────────┘
                                    ↓
    rollout_storage.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  RolloutWriter.write_batch(rollout_batch)                       │
    │    • Saves rollouts to disk/storage                             │
    └─────────────────────────────────────────────────────────────────┘


================================================================================
                    CHAIN 2: TRAINER (Full Sequence Logprobs)
================================================================================

    train_batch.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  create_training_batch_from_rollouts(rollouts, max_tokens, ...) │
    │    • Line 100-146                                               │
    │    • Loads rollouts with advantages                             │
    │    ↓                                                            │
    │  convert_rollout_to_training_format(rollout, advantage, ...)    │
    │    • Line 45-97                                                 │
    │                                                                 │
    │    1. Concatenate tokens:                                       │
    │       full_tokens = [prompt_tokens + response_tokens]           │
    │                                                                 │
    │    2. Create policy_logprobs (line 83-85):                      │
    │       policy_logprob = concatenate([                            │
    │         zeros(len(prompt_tokens) - 1),  ← ZEROS for prompt!    │
    │         response_logprobs               ← from rollout          │
    │       ])                                                        │
    │       Result: [0, 0, 0, ..., r1, r2, r3, ...]                   │
    │                                                                 │
    │    3. Create loss_mask (line 68-73):                            │
    │       loss_mask = concatenate([                                 │
    │         zeros(len(prompt_tokens) - 1),  ← mask out prompt       │
    │         ones(len(response_tokens))      ← keep response         │
    │       ])                                                        │
    │       Result: [0, 0, 0, ..., 1, 1, 1, ...]                      │
    └─────────────────────────────────────────────────────────────────┘
                                    ↓
    rl_losses.py
    ┌─────────────────────────────────────────────────────────────────┐
    │  rloo_loss_with_importance_sampling(model, ref_model, batch)    │
    │    • Line 53-163                                                │
    │                                                                 │
    │  STEP 1: Get current policy logprobs (ALL POSITIONS)            │
    │    model_output = model(                                        │
    │      input_ids=batch.input_ids,      ← full sequence            │
    │      attn_mask=batch.attention_mask,                            │
    │      pos_ids=batch.position_ids                                 │
    │    )                                                            │
    │    logits = model_output.array                                  │
    │    current_logprobs = -softmax_cross_entropy(logits, targets)   │
    │                                                                 │
    │    ✓ Computes logprobs for PROMPT + RESPONSE positions          │
    │                                                                 │
    │  ─────────────────────────────────────────────────────────────  │
    │                                                                 │
    │  STEP 2: Get reference model logprobs (ALL POSITIONS)           │
    │    reference_output = reference_model(                          │
    │      input_ids=batch.input_ids,      ← full sequence            │
    │      attn_mask=batch.attention_mask,                            │
    │      pos_ids=batch.position_ids                                 │
    │    )                                                            │
    │    reference_logits = reference_output.array                    │
    │    reference_logprobs = -softmax_cross_entropy(...)             │
    │                                                                 │
    │    ✓ Computes logprobs for PROMPT + RESPONSE positions          │
    │                                                                 │
    │  ─────────────────────────────────────────────────────────────  │
    │                                                                 │
    │  STEP 3: Importance sampling ratio (line 107-116)               │
    │    log_ratio = current_logprobs - policy_logprobs               │
    │                                                                 │
    │    For prompt positions:                                        │
    │      log_ratio[i] = current_logprobs[i] - 0                     │
    │                   = current_logprobs[i]                         │
    │                                                                 │
    │    For response positions:                                      │
    │      log_ratio[i] = current_logprobs[i] - policy_logprobs[i]    │
    │                                                                 │
    │    ratio = exp(log_ratio)                                       │
    │    ratio = clip(ratio, 1-epsilon, 1+epsilon)                    │
    │                                                                 │
    │  ─────────────────────────────────────────────────────────────  │
    │                                                                 │
    │  STEP 4: REINFORCE loss (line 118-121)                          │
    │    weighted_loss = -ratio * loss_weights * loss_masks           │
    │    reinforce_loss = sum(weighted_loss) / sum(loss_masks)        │
    │                                                                 │
    │    ⚠️  loss_masks are 0 for prompt → prompt positions ignored!  │
    │                                                                 │
    │  ─────────────────────────────────────────────────────────────  │
    │                                                                 │
    │  STEP 5: KL penalty (line 124-125)                              │
    │    kl_penalty = current_logprobs - reference_logprobs           │
    │    kl_loss = kl_coef * sum(kl_penalty * loss_masks) / ...       │
    │                                                                 │
    │    ⚠️  loss_masks are 0 for prompt → prompt positions ignored!  │
    │                                                                 │
    │  ─────────────────────────────────────────────────────────────  │
    │                                                                 │
    │  FINAL: loss = reinforce_loss + kl_loss                         │
    │                                                                 │
    │  ✓ Only RESPONSE token positions contribute to final loss!      │
    └─────────────────────────────────────────────────────────────────┘


================================================================================
                              KEY DIFFERENCES
================================================================================

┌────────────────────┬──────────────────────────┬─────────────────────────────┐
│ Aspect             │ Rollout Workers          │ Trainer                     │
├────────────────────┼──────────────────────────┼─────────────────────────────┤
│ When               │ During rollout gen       │ During training step        │
├────────────────────┼──────────────────────────┼─────────────────────────────┤
│ What logprobs      │ Response tokens only     │ Full seq (prompt+response)  │
├────────────────────┼──────────────────────────┼─────────────────────────────┤
│ Source             │ Inference API            │ Model forward pass          │
├────────────────────┼──────────────────────────┼─────────────────────────────┤
│ Purpose            │ Store policy logprobs    │ Compare current vs ref      │
├────────────────────┼──────────────────────────┼─────────────────────────────┤
│ Prompt logprobs    │ ✗ NOT computed/stored    │ ✓ Computed but masked out   │
├────────────────────┼──────────────────────────┼─────────────────────────────┤
│ Response logprobs  │ ✓ From inference API     │ ✓ Freshly computed          │
└────────────────────┴──────────────────────────┴─────────────────────────────┘


================================================================================
                              WHY THIS DESIGN?
================================================================================

1. ROLLOUT WORKERS - Efficiency
   • Only need response logprobs for importance sampling later
   • Inference API naturally provides logprobs during generation
   • No need to recompute or store prompt logprobs
   • Reduces memory and storage requirements

2. TRAINER - Completeness
   • Forward pass through model produces logits for ALL positions naturally
   • Easier to compute softmax_cross_entropy on full output
   • loss_masks efficiently filter to only response positions
   • Both current and reference models need same treatment

3. MASKING STRATEGY
   • policy_logprobs: [0, 0, ..., 0, r1, r2, r3, ...]
   • loss_masks:      [0, 0, ..., 0,  1,  1,  1, ...]
   • Even though prompt logprobs are computed, they don't affect loss
   • Simple and efficient: compute everything, mask at the end


================================================================================
                           EXAMPLE WITH NUMBERS
================================================================================

Prompt: "What is 2+2?" → tokens: [101, 2054, 2003, 1016, 1009, 1016, 1029]
Response: "4" → tokens: [1018]

ROLLOUT WORKER OUTPUT:
  Rollout.prompt_tokens = [101, 2054, 2003, 1016, 1009, 1016, 1029]
  Rollout.response_tokens = [1018]
  Rollout.response_logprobs = [-0.002]  ← Only response!

TRAINER BATCH:
  full_tokens = [101, 2054, 2003, 1016, 1009, 1016, 1029, 1018]

  policy_logprobs = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002]
                     └─────── prompt zeros ──────┘  └─res─┘

  loss_masks = [0, 0, 0, 0, 0, 0, 1]
                └─── prompt masked ──┘  └res┘

TRAINER FORWARD PASS:
  current_logprobs =     [-0.01, -0.05, -0.03, -0.02, -0.04, -0.03, -0.001]
  reference_logprobs =   [-0.01, -0.04, -0.03, -0.02, -0.03, -0.03, -0.002]
                          └─────────── All positions computed ──────────────┘

  But loss only uses position 6 (response) because loss_masks[6] = 1!

================================================================================
