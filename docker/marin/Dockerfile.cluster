FROM rayproject/ray:nightly.240914.bb15a3-py311-cpu
RUN sudo apt-get update && sudo apt-get install -y clang curl g++ vim libpython3.11 libpython3.11-dev

# install rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH=$HOME/.cargo/bin:$PATH

RUN sudo mkdir -p /opt/marin/
RUN sudo chown -R $(whoami) /opt/marin/
# Ray uses conda
ENV PATH=/home/ray/anaconda3/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/condabin:$PATH
RUN pip install -U "jax[tpu]==0.4.30" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html

# gcloud
RUN conda install conda-forge::google-cloud-sdk -y
RUN gcloud components install alpha

RUN conda install -c conda-forge ncurses -y

WORKDIR /tmp/
ADD pyproject.toml /tmp/
RUN pip install . --extra-index-url https://download.pytorch.org/whl/cpu


# Our weird fork of OLMo
# The olmo library did not set `exists_ok=True` at
# https://github.com/allenai/OLMo/blob/main/hf_olmo/configuration_olmo.py#L43
# which causes the following error:
# ValueError: 'olmo' is already used by a Transformers config, pick another name.
# Use the following dependency instead where the issue is fixed.
RUN pip install git+https://github.com/teetone/OLMo.git@main

# Install vllm for TPU
# cf https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu
RUN pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html
RUN pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html

RUN sudo mkdir -p /opt/vllm
RUN sudo chown -R $(whoami) /opt/vllm
RUN cd /opt/vllm && git clone https://github.com/vllm-project/vllm.git /opt/vllm
WORKDIR /opt/vllm/
RUN git checkout tags/v0.5.3.post1
RUN pip install -r requirements-tpu.txt
RUN VLLM_TARGET_DEVICE="tpu" python3 setup.py develop

# copy in the autoscaler hacks
ENV VENV_PATH=/home/ray/anaconda3/
RUN sudo mkdir /opt/abhi-ray
RUN sudo chown -R $(whoami) /opt/abhi-ray
WORKDIR /opt/abhi-ray
RUN git clone --branch abhi-2.34.0 --single-branch --depth 1 https://github.com/abhinavg4/ray.git
RUN rm -rf $VENV_PATH/lib/python3.11/site-packages/ray/autoscaler
RUN ln -s /opt/abhi-ray/ray/python/ray/autoscaler $VENV_PATH/lib/python3.11/site-packages/ray/autoscaler

ENV PYTHONPATH=$PYTHONPATH:$VENV_PATH/lib/python3.11/site-packages
ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/ray/anaconda3/lib
ENV PATH=$PATH:/home/ray/anaconda3/bin

# We need higher ulimit for tpu https://cloud.google.com/kubernetes-engine/docs/how-to/tpus#privileged-mode
# see also https://cloud.google.com/kubernetes-engine/docs/troubleshooting/tpus
RUN ulimit -l 68719476736

# Setup gcsfuse
RUN sudo apt install lsb-release -y
RUN export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s` && echo "deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main" | sudo tee /etc/apt/sources.list.d/gcsfuse.list
RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
RUN sudo apt-get update && sudo apt-get install fuse gcsfuse -y
RUN sudo mkdir /opt/gcsfuse_mount
RUN sudo chown -R $(whoami) /opt/gcsfuse_mount

WORKDIR /opt/marin
