FROM rayproject/ray:nightly-py311-cpu
RUN sudo apt-get update && sudo apt-get install -y clang curl g++ vim

# install rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH=$HOME/.cargo/bin:$PATH

RUN sudo mkdir -p /opt/marin/
RUN sudo chown -R $(whoami) /opt/marin/
# Ray uses conda
ENV PATH=/home/ray/anaconda3/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/condabin:$PATH
RUN pip install -U "jax[tpu]==0.4.30" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html

# gcloud
RUN conda install conda-forge::google-cloud-sdk
RUN gcloud components install alpha

# Install package dependencies to make incremental builds faster.
WORKDIR /tmp/
ADD pyproject.toml /tmp/
RUN pip install . --extra-index-url https://download.pytorch.org/whl/cpu

# Our weird fork of OLMo
RUN pip install git+https://github.com/TheQuantumFractal/OLMo.git

# Install vllm for TPU
# cf https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu
RUN pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html
RUN pip install torch_xla[pallas] -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html

RUN sudo mkdir -p /opt/vllm
RUN sudo chown -R $(whoami) /opt/vllm
RUN cd /opt/vllm && git clone https://github.com/vllm-project/vllm.git /opt/vllm
WORKDIR /opt/vllm/
RUN git checkout tags/v0.5.3.post1
RUN pip install -r requirements-tpu.txt
RUN VLLM_TARGET_DEVICE="tpu" python3 setup.py develop

# copy in the autoscaler hacks
ENV VENV_PATH=/home/ray/anaconda3/
RUN sudo mkdir /opt/abhi-ray
RUN sudo chown -R $(whoami) /opt/abhi-ray
WORKDIR /opt/abhi-ray
RUN git clone --branch abhi-2.34.0 --single-branch --depth 1 https://github.com/abhinavg4/ray.git
RUN rm -rf $VENV_PATH/lib/python3.11/site-packages/ray/autoscaler
RUN ln -s /opt/abhi-ray/ray/python/ray/autoscaler $VENV_PATH/lib/python3.11/site-packages/ray/autoscaler

WORKDIR /opt/marin