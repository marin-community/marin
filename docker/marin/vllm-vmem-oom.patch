--- vllm/v1/attention/backends/pallas.py	2025-09-08 20:16:27.153471730 +0000
+++ vllm/v1/attention/backends/pallas.py	2025-09-08 20:21:14.545183309 +0000
@@ -148,7 +148,7 @@
         # For long model length, we use 16 page-size to avoid too much
         # VMEM spill. A more robust solution should be implemented to
         # handle VREG spills.
-        if vllm_config.model_config.max_model_len > 8192:
+        if vllm_config.model_config.max_model_len >= 8192:
             return 16
         page_size = next_power_of_2(
             vllm_config.model_config.max_model_len) // 16
