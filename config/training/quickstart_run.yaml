# This is designed to be quite small and fairly fast.
#data:
# data will be filled in in the experiment script

model: # 150M class model
  type: llama
  seq_len: 1024
  use_flash_attention: True
  # ------- END OF COMMON CONFIGS -------
  hidden_dim: 768
  intermediate_dim: 2688
  num_layers: 12
  num_heads: 6
  num_kv_heads: 6

trainer:
  tracker:
    type: wandb
    project: "marin"
    tags: ["llama", "quickstart"]

  mp: p=f32,c=bfloat16
  train_batch_size: 1
  num_train_steps: 2000
  steps_per_eval: 500
  checkpointer:
    save_interval: 10m
    keep:
      - every: 500

optimizer:
  learning_rate: 4E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1
