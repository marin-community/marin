# non-embedding: 339,788,800
# total: 602,457,088

type: llama
seq_len: 4096
hidden_dim: 1024
intermediate_dim: 3584
num_layers: 24
num_heads: 16
num_kv_heads: 8
use_flash_attention: True
flash_attention_block_size: 1024
