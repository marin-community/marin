# Model configuration
model:
  type: llama
  seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_heads: 32
  num_kv_heads: 32
  use_flash_attention: true
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: false

# Training configuration
trainer:
  mp: p=f32,c=bfloat16
  tracker:
    type: wandb
    project: "levanter-sft"
    tags: ["llama", "sft"]
  num_train_steps: 750000
  train_batch_size: 64
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  steps_per_eval: 1000

# Optimizer settings
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
  min_lr_ratio: 0.1
  warmup: 100

# Supervised data configuration
dataset_type: chat_jsonl
chat_train_urls:
  - "gs://marin-us-central2/documents/allenai--tulu-v2-sft-mixture-0ba27c/data/**/*.jsonl.gz"
supervised_data:
  cache_dir: "gs://marin-us-central2/tokenized/sft_cache/chat-data"
messages_field: "messages"
input_role: "user"
output_role: "assistant"

# Additional settings
tokenizer: "EleutherAI/gpt-neox-20b"
max_seq_len: 2048
epoch: 0

initialize_from_hf: false
