# non-embedding: 22,813,184
# total: 154,147,328

type: llama
seq_len: 4096
use_flash_attention: True
# ------- END OF COMMON CONFIGS -------
hidden_dim: 512
intermediate_dim: 1792
num_layers: 6
num_heads: 8
num_kv_heads: 8
