{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import utils\n",
    "from utils import fit_accuracy_from_task_loss, fit_task_loss_from_ladder_models, plot_actual_vs_predicted, plot_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY = \"stanford-mercury\"\n",
    "PROJECT = \"marin\"\n",
    "\n",
    "# define the list of smaller runs to use as \"ladder\" models\n",
    "RUNS = [\n",
    "    \"tootsie-scaling-512-81c36c\",\n",
    "    \"tootsie-scaling-768-d17a90\",\n",
    "    \"tootsie-scaling-1024-f4e4be\",\n",
    "    \"tootsie-scaling-1536-e2a6d8\",\n",
    "    \"tootsie-scaling-2048-72c648\",\n",
    "]\n",
    "\n",
    "# the large run to predict the performance of\n",
    "PRED_RUN = \"llama-8b-tootsie-0.001-19ad63\"\n",
    "\n",
    "# define the eval task loss metric\n",
    "TASK_LOSS = \"eval/paloma/c4_en/bpb\"\n",
    "# TASK_LOSS = \"internal_eval/mmlu/bpb\"\n",
    "\n",
    "# do we want to take the last step of the run or average over all steps, or just use all steps?\n",
    "AGGREGATION = \"all\"\n",
    "\n",
    "TOKENS_COL = \"throughput/total_tokens\"\n",
    "PARAM_COL = \"parameter_count\"\n",
    "PARAM_COL_TO_USE = \"computed_params\"\n",
    "\n",
    "# accuracy we ultimately want to predict\n",
    "TASK_ACCURACY = \"lm_eval/hellaswag_10shot/acc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict task loss from N, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = fit_task_loss_from_ladder_models(\n",
    "        runs=RUNS,\n",
    "        entity=ENTITY,\n",
    "        project=PROJECT,\n",
    "        metrics=[TASK_LOSS, TOKENS_COL, TASK_ACCURACY],\n",
    "        pred_run=PRED_RUN,\n",
    "        task_loss=TASK_LOSS,\n",
    "        aggregation=AGGREGATION,\n",
    "        tokens_col=TOKENS_COL,\n",
    "        param_col=PARAM_COL,\n",
    "        param_col_to_use=PARAM_COL,\n",
    "        use_log_for_ND=True,\n",
    "        normalize_ND=True,\n",
    "    )\n",
    "\n",
    "plot_actual_vs_predicted(actual, predicted, title=\"Prediction on 7B Run using 5-sub 1.4B ladder models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(actual, predicted, title=\"Prediction on 7B Run using 5-sub 1.4B ladder models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict task accuracy from task loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "# get accuracy, which is hellaswag accuracy\n",
    "actual_acc, pred_acc = fit_accuracy_from_task_loss(\n",
    "    pred_task_losses=predicted,\n",
    "    runs=RUNS,\n",
    "    entity=ENTITY,\n",
    "    project=PROJECT,\n",
    "    x_axis=\"throughput/total_gflops\",\n",
    "    tokens_col=TOKENS_COL,\n",
    "    pred_run=PRED_RUN,\n",
    "    aggregation=AGGREGATION,\n",
    "    task_loss_col=TASK_LOSS,\n",
    "    accuracy_col=TASK_ACCURACY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(\n",
    "    actual_acc, pred_acc,\n",
    "    title=\"Prediction on 7B Run using 5-sub 1.4B ladder models\",\n",
    "    task_metric=TASK_ACCURACY\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
