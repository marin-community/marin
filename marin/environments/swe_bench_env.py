import json
import os
import random
import subprocess
import uuid

from datasets import load_dataset
from swebench.inference.make_datasets.utils import extract_diff
import openai

from marin_env import MarinEnv, EnvStep


class SWEBenchEnv(MarinEnv):
    """
    Environment for the SWE-Bench benchmark, which evaluates code generation models on software engineering tasks.
    https://github.com/SWE-bench/SWE-bench
    """

    def __init__(self, endpoint: str, output_dir_path: str, **kwargs):
        self._environment_id = uuid.uuid4().hex
        self._output_dir_path: str = os.path.join(output_dir_path, self._environment_id)
        os.makedirs(self._output_dir_path, exist_ok=True)

        self.client = openai.OpenAI()
        self.model_name = kwargs["model_name"]
        self.dataset_name = kwargs.get("dataset_name", "princeton-nlp/SWE-bench_oracle")
        self.max_workers = kwargs.get("max_workers", 8)

        # Load the splits from the dataset
        self._train_split = load_dataset(self.dataset_name, split="train")
        self._test_split = load_dataset(self.dataset_name, split="test")

    def step(self, mode: str = "train") -> EnvStep:
        # Create a unique ID for this step
        step_id = uuid.uuid4().hex
        output_path: str = os.path.join(self._output_dir_path, step_id)
        os.makedirs(output_path, exist_ok=True)

        # Pick an example from the correct split
        assert mode in ("train", "test"), "mode must be 'train' or 'test'"
        split = self._train_split if mode == "train" else self._test_split
        example = random.choice(split)

        instance_id: str = example["instance_id"]
        prompt: str = example["text"]

        # Run inference with the model to generate the code patches
        inference_payload = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
        )
        completion = inference_payload.choices[0].message.content
        patch: str = extract_diff(completion)

        # Write out the predictions/patches for SWE-Bench to evaluate
        record = {
            "model_name_or_path": self.model_name,
            "instance_id": instance_id,
            "full_output": completion,
            "model_patch": patch,
        }
        predictions_path: str = os.path.join(output_path, "predictions.jsonl")
        with open(predictions_path, "w") as f:
            f.write(json.dumps(record) + "\n")

        # Evaluate the patches generated by the model
        commands = [
            "python",
            "-m",
            "swebench.harness.run_evaluation",
            "--run_id",
            step_id,
            "--dataset_name",
            self.dataset_name,
            "--split",
            mode,
            "--predictions_path",
            predictions_path,
            "--instance_ids",
            instance_id,
            "--max_workers",
            str(self.max_workers),
            "--report_dir",
            output_path,
        ]
        process = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate()

        # Award a reward of 1 when GitHub issues is resolved
        reward: float = 0
        if "Instances resolved: 1" in stdout.decode("utf-8", errors="ignore"):
            reward = 1

        return EnvStep(llm_in=prompt, llm_out=completion, reward=reward)


def main():
    env = SWEBenchEnv(
        endpoint="",
        output_dir_path="output",
        model_name="o1-2024-12-17",
    )
    for i in range(5):
        # TODO: train does not work out-of-the-box due to repo values
        print(f"step={i}", env.step(mode="test"))


if __name__ == "__main__":
    main()
