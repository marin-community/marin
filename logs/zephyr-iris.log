# Zephyr Iris Tokenization - Debug Log

## Goal

Run zephyr pipelines on Iris cluster using fray v2 distributed actors.

## Summary of All Fixes (Sessions 1-4)

| # | Problem | Fix | File |
|---|---------|-----|------|
| 1 | Docker build: source not available during uv sync | Copy source before uv sync | builder.py |
| 2 | PyTorch CUDA on CPU workers | Use --extra marin:cpu | iris_run.py, builder.py, cli.py |
| 3 | Click `--` separator consumed by parser | Remove `--` check | iris_run.py |
| 4 | Extras flags parsed incorrectly | `_build_extras_flags()` for `pkg:extra` syntax | builder.py |
| 5 | Log streaming crash on connection timeout | Broaden exception catch | client.py |
| 6 | Extras not passed in remote cluster path | Add `extras=extras` to remote path | iris_run.py |
| 7 | Workers run in-process, not as Iris jobs | Inject FRAY_CLIENT_SPEC in container env | task_attempt.py, client.py, iris_backend.py |
| 8 | Coordinator cpu=0 can't be scheduled | Changed to cpu=1 | execution.py |
| 9 | preemptible=False blocks scheduling on preemptible-only cluster | Removed constraint | execution.py |
| 10 | wait_ready timeout too short for Docker builds | 300s → 900s | iris_backend.py |
| 11 | Chunk storage on local /tmp doesn't work across containers | Use GCS for distributed backends | cli.py |

## Issue Filed

- #2597: `FRAY_CLIENT_SPEC` env var duplicates Iris context — `current_client()` should auto-detect

## Session 4: Actor Discovery Works, Pipeline Execution Debugging (2026-01-31)

### Key Discovery: Actor Discovery Was Never Broken

Previous session suspected `wait_ready()` couldn't discover coordinator endpoints. This was wrong.
The real issue was that previous test runs had different failure modes (timeout before image built,
stale orphaned jobs consuming resources, etc.) that masked the underlying problem.

### Experiment 1: Minimal Actor Test

Created `experiments/test_iris_actors.py` with a simple `EchoActor`:
```python
class EchoActor:
    def echo(self, msg: str) -> str:
        return f"echo: {msg}"
```

**Result**: ✅ SUCCESS
- Actor created as Iris job, endpoint discovered, RPC call succeeded
- `Got 1 handles` — `wait_ready()` works
- `SUCCESS: echo: hello` — full round-trip actor call works
- Job: `iris-run-power-test_iris_actors-20260131-214205` (SUCCEEDED)

This proves:
1. `FrayIrisClient.create_actor_group()` correctly submits actor jobs
2. `_host_actor()` correctly instantiates actors and registers endpoints
3. `IrisActorGroup.wait_ready()` correctly resolves endpoints via `NamespacedResolver`
4. `IrisActorHandle` correctly routes RPC calls to remote actors

### Experiment 2: Zephyr Pipeline Test

Created `experiments/test_zephyr_iris.py` with a simple map pipeline:
```python
ds = Dataset.from_list([{"text": f"hello-{i}"} for i in range(10)])
ds = ds.map(lambda record: {**record, "text": record["text"].upper()})
ctx = get_default_zephyr_context()
results = list(ctx.execute(ds))
```

**Attempt 1**: Failed — `zephyr.cli` treats first arg as script file, `tokenize` is not a file.
Zephyr CLI: `python -m zephyr.cli [options] <script.py> [script args]`

**Attempt 2**: Failed — `TypeError: 'Dataset' object is not iterable`
Must use `ctx.execute(ds)` not `list(ds)`.

**Attempt 3**: Failed — `ModuleNotFoundError: No module named 'test_zephyr_iris'`
The `upper()` function defined in the script gets cloudpickled and sent to the coordinator.
The coordinator (separate Iris job) doesn't have the `test_zephyr_iris` module.
**Fix**: Use inline lambda instead of named function.

**Attempt 4**: ✅ Coordinator and workers discovered, ActorService calls working!
```
HTTP Request: POST http://169.254.123.5:30001/iris.actor.ActorService/Call "HTTP/1.1 200 OK"
HTTP Request: POST http://169.254.123.6:30002/iris.actor.ActorService/Call "HTTP/1.1 200 OK"
HTTP Request: POST http://169.254.123.7:30004/iris.actor.ActorService/Call "HTTP/1.1 200 OK"
Starting stage stage0-Map with 10 tasks
```
But failed: `FileNotFoundError: /tmp/zephyr/cdd4e2f4dbc7/source/shard-0001/chunk-0000.pkl`

### Fix 11: Chunk Storage Must Be GCS for Distributed Backends

**Root Cause**: `ZephyrContext.chunk_storage_prefix` defaults to `/tmp/zephyr`. The parent job
writes chunk files to local `/tmp`, but workers are in separate containers on different machines.
Workers can't read files from the parent's `/tmp`.

**Fix**: In `cli.py:run_local()`, when `FRAY_CLIENT_SPEC` starts with `iris://` or `ray`, set
`chunk_storage_prefix` to a GCS path (`$MARIN_PREFIX/tmp/zephyr` or default
`gs://marin-us-central2/scratch/tmp/zephyr`).

**File**: `lib/zephyr/src/zephyr/cli.py`

### Debug Logging Added

Added debug logging to `IrisActorGroup.wait_ready()` in `iris_backend.py`:
- Logs actor_name, job_id, resolve_result, and all registered endpoints
- On timeout, dumps full endpoint state for diagnosis

### Current Status

- ✅ Actor creation and discovery: WORKING
- ✅ Coordinator + worker creation: WORKING
- ✅ Actor RPC calls: WORKING
- ✅ Pipeline stage submission: WORKING
- ✅ GCS chunk storage: WORKING (Fix 11)
- ✅ **FULL PIPELINE SUCCESS**: 10/10 records processed correctly
- Job: `iris-run-power-test_zephyr_iris-20260131-223214`
- Known issue: parent job reports FAILED (state 4) because orphaned actor jobs remain
- Next: test actual tokenization pipeline, fix orphaned job cleanup

### Session 4b: Repeated Success + Engineering Improvements (2026-01-31)

Resubmitted `test_zephyr_iris.py` — confirmed reproducible success:
- Job: `iris-run-power-test_zephyr_iris-20260131-224502`
- 10/10 records processed, `HELLO-0` through `HELLO-9`
- Parent still reports state 4 (FAILED) due to orphaned actor jobs

**In-progress fixes (senior engineer agent):**

1. **Continuous worker discovery**: Replace blocking `wait_ready(count=N)` with
   a model where the coordinator continuously probes for new workers and adds
   work to them as they appear. This removes the hard timeout and lets processing
   start as soon as ANY worker is ready.

2. **Orphaned actor job cleanup**: Parent should cancel child actor jobs on exit
   (normal or exception) so the job reports SUCCEEDED. Likely via context manager
   or atexit on `FrayIrisClient` / `IrisActorGroup`.

### Correction: State 4 = SUCCEEDED, Not FAILED

Previous analysis incorrectly assumed state 4 meant FAILED. From `cluster.proto`:
```
JOB_STATE_SUCCEEDED = 4
JOB_STATE_FAILED = 5
```
The test pipeline jobs (state 4) were SUCCEEDING all along. The `iris-run` exit code
is 0 for state 4. The earlier tokenization attempt (state 5) genuinely failed because
it used wrong CLI syntax (`zephyr.cli tokenize` instead of a script file).

### Status: Simple Pipeline Works, Real Tokenization Not Yet Tested

The `.upper()` map pipeline works reliably on Iris. The actual tokenization pipeline
has not been tested yet — it requires a proper tokenization script that imports the
tokenizer and runs it through Zephyr's map/reduce stages.

Key difference between test and real tokenization:
- Test: 10 records, simple lambda, no external deps
- Tokenization: millions of records, HuggingFace tokenizer, streaming from GCS,
  writing tokenized output back to GCS. May hit: pickle issues with tokenizer objects,
  memory pressure, GCS I/O bottlenecks, worker crashes under load.

### Session 4c: Test Validation + Tokenization (2026-01-31)

**Iris local test timeout is PRE-EXISTING**: Verified by running `test_simple_map[iris]`
on commit 414e1e31c (before any session 4 changes) — same 300s timeout. The local iris
backend hangs during task execution, not during `wait_ready()`. Unrelated to our changes.

**Local tests pass**: All 15 `test_execution.py[local]` tests pass in 10s.

**Reverted `wait_ready(count=1)`**: The optimization to start with 1 worker didn't work
for single-stage pipelines since `_discover_workers()` only runs between stages.
Reverted to `wait_ready()` (all workers). `discover_new()` API kept for future use.

**Changes committed**:
- `discover_new()` on ActorGroup/IrisActorGroup (non-blocking worker discovery)
- `_discover_workers()` in ZephyrContext (discovers workers between stages)
- atexit handler for orphaned actor cleanup
- GCS chunk storage for distributed backends

### Tokenization Attempt 1: `HfTokenizeConfig` — FAILED (torch CUDA import)

Job: `iris-run-power-test_tokenize_iris-20260131-232206` (state 5 = FAILED)

**Error**: `ValueError: libcublas.so.*[0-9] not found`
The import chain: `marin.processing.tokenize.__init__` → `data_configs.py` →
`import transformers` → `import torch` → tries to load CUDA libs → fails.

**Root Cause**: The venv has CUDA torch installed (default). Even with `--extra marin:cpu`,
the `uv sync` in Docker might resolve to CUDA torch due to the lock file having CUDA
variants pinned. The `cpu` extra changes `torch` to use the `pytorch-cpu` index, but
the UV cache mount (`iris-uv-global`) may have cached CUDA wheels.

**Workaround**: Use `tiktoken` (no torch dependency) instead of `transformers.AutoTokenizer`
for the tokenization test. This avoids the torch import entirely.

### Tokenization Attempt 2: `tiktoken` — FAILED (not installed)

Job: `iris-run-power-test_tokenize_iris-20260131-232804` (state 5)
`ModuleNotFoundError: No module named 'tiktoken'` — not a dependency.

### Tokenization Attempt 3: Named function — FAILED (pickle issue)

Job: `iris-run-power-test_tokenize_iris-20260131-233233` (state 5)
`ModuleNotFoundError: No module named 'test_tokenize_iris'`
Same issue from session 4: named functions in the script module get cloudpickled
with a reference to the script's module name. The coordinator (separate Iris job)
doesn't have that module. Must use lambdas.

### Tokenization Attempt 4: Lambda + shard_ctx — SUCCESS ✅

Job: `iris-run-power-test_tokenize_iris-20260131-233848` (state 4 = SUCCEEDED)
- 5/5 records tokenized correctly
- Shared vocabulary broadcast via `ctx.put("vocab", vocab)`
- Workers access via `shard_ctx().get_shared("vocab")`
- All token IDs correct

This validates the full tokenization flow: shared state + distributed map.
The only remaining blocker for real tokenization is the torch CUDA import issue.

| # | Problem | Fix | File |
|---|---------|-----|------|
| 12 | torch CUDA import on CPU workers | Need CPU torch in Docker | pyproject.toml / builder.py |
| 13 | Named functions fail pickle on workers | Use lambdas or library functions | user scripts |

### Iris Backend Test: FIXED ✅

| # | Problem | Fix | File |
|---|---------|-----|------|
| 14 | IrisActorHandle can't resolve in RPC threads | Store controller_address for fallback | iris_backend.py |

**Root Cause**: `IrisActorHandle` pickled to workers loses `iris_ctx()` ContextVar
in RPC dispatch threads (threads don't inherit ContextVars). `_resolve()` failed with
`RuntimeError("No client available in context")`, killing workers silently.

**Fix**: Store `controller_address` in handle state (`__getstate__`/`__setstate__`).
In `_resolve()`, when no iris context available, create lightweight `IrisClient` from
`controller_address` + `job_id` to build a resolver.

**Result**: All 15 iris tests pass in 58s (was 300s+ timeout). Local tests unaffected.

### Current Status Summary

- ✅ Simple pipeline on Iris cluster: WORKING (10/10 records)
- ✅ Tokenization with shard_ctx: WORKING (5/5 records, shared vocab)
- ✅ Local tests: 15/15 pass (11s)
- ✅ Iris tests: 15/15 pass (58s)
- ❌ Real tokenization (transformers.AutoTokenizer): Blocked by torch CUDA import
- ❌ Ray tests: Separately broken, not our concern

**Next**: Fix torch CUDA import to enable real tokenization with `transformers`.

### Architecture Notes

The Zephyr-on-Iris execution flow:
1. `iris-run --config cluster.yaml -- uv run python -m zephyr.cli script.py`
2. iris-run submits parent job to Iris controller
3. Parent job builds Docker image, starts container
4. Container has `FRAY_CLIENT_SPEC=iris://controller:port` (injected by build_iris_env)
5. `python -m zephyr.cli` → `run_local()` → `current_client()` → `FrayIrisClient`
6. `ZephyrContext` creates coordinator actor group (1 job) and worker actor group (N jobs)
7. Each actor job builds its own Docker image and starts an `ActorServer`
8. Actors register endpoints via `iris_ctx().registry.register()`
9. Parent discovers actors via `IrisActorGroup.wait_ready()` → `NamespacedResolver.resolve()`
10. Pipeline stages are submitted to coordinator, which distributes to workers

### Key Files Changed (This Session)

- `lib/fray/src/fray/v2/iris_backend.py`: Added debug logging to `wait_ready()`
- `lib/zephyr/src/zephyr/cli.py`: GCS chunk storage for distributed backends (Fix 11)
- `experiments/test_iris_actors.py`: Minimal actor test (created)
- `experiments/test_zephyr_iris.py`: Zephyr pipeline test (created)

### Cluster Management

```bash
# Start cluster
.venv/bin/iris cluster --config lib/iris/examples/eu-west4.yaml start

# List jobs
.venv/bin/python lib/iris/scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models list-jobs

# View task logs
.venv/bin/python lib/iris/scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models show-task-logs <JOB_ID>

# Clean up (remove all VMs)
.venv/bin/python lib/iris/scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models cleanup --no-dry-run

# Submit test
.venv/bin/iris-run --config lib/iris/examples/eu-west4.yaml -- uv run python -m zephyr.cli --max-parallelism 2 experiments/test_zephyr_iris.py
```
