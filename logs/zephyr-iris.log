# Zephyr Iris Tokenization - Debug Log

## Goal

Run zephyr pipelines on Iris cluster using fray v2 distributed actors.

## Summary of All Fixes (Sessions 1-4)

| # | Problem | Fix | File |
|---|---------|-----|------|
| 1 | Docker build: source not available during uv sync | Copy source before uv sync | builder.py |
| 2 | PyTorch CUDA on CPU workers | Use --extra marin:cpu | iris_run.py, builder.py, cli.py |
| 3 | Click `--` separator consumed by parser | Remove `--` check | iris_run.py |
| 4 | Extras flags parsed incorrectly | `_build_extras_flags()` for `pkg:extra` syntax | builder.py |
| 5 | Log streaming crash on connection timeout | Broaden exception catch | client.py |
| 6 | Extras not passed in remote cluster path | Add `extras=extras` to remote path | iris_run.py |
| 7 | Workers run in-process, not as Iris jobs | Inject FRAY_CLIENT_SPEC in container env | task_attempt.py, client.py, iris_backend.py |
| 8 | Coordinator cpu=0 can't be scheduled | Changed to cpu=1 | execution.py |
| 9 | preemptible=False blocks scheduling on preemptible-only cluster | Removed constraint | execution.py |
| 10 | wait_ready timeout too short for Docker builds | 300s ‚Üí 900s | iris_backend.py |
| 11 | Chunk storage on local /tmp doesn't work across containers | Use GCS for distributed backends | cli.py |

## Issue Filed

- #2597: `FRAY_CLIENT_SPEC` env var duplicates Iris context ‚Äî `current_client()` should auto-detect

## Session 4: Actor Discovery Works, Pipeline Execution Debugging (2026-01-31)

### Key Discovery: Actor Discovery Was Never Broken

Previous session suspected `wait_ready()` couldn't discover coordinator endpoints. This was wrong.
The real issue was that previous test runs had different failure modes (timeout before image built,
stale orphaned jobs consuming resources, etc.) that masked the underlying problem.

### Experiment 1: Minimal Actor Test

Created `experiments/test_iris_actors.py` with a simple `EchoActor`:
```python
class EchoActor:
    def echo(self, msg: str) -> str:
        return f"echo: {msg}"
```

**Result**: ‚úÖ SUCCESS
- Actor created as Iris job, endpoint discovered, RPC call succeeded
- `Got 1 handles` ‚Äî `wait_ready()` works
- `SUCCESS: echo: hello` ‚Äî full round-trip actor call works
- Job: `iris-run-power-test_iris_actors-20260131-214205` (SUCCEEDED)

This proves:
1. `FrayIrisClient.create_actor_group()` correctly submits actor jobs
2. `_host_actor()` correctly instantiates actors and registers endpoints
3. `IrisActorGroup.wait_ready()` correctly resolves endpoints via `NamespacedResolver`
4. `IrisActorHandle` correctly routes RPC calls to remote actors

### Experiment 2: Zephyr Pipeline Test

Created `experiments/test_zephyr_iris.py` with a simple map pipeline:
```python
ds = Dataset.from_list([{"text": f"hello-{i}"} for i in range(10)])
ds = ds.map(lambda record: {**record, "text": record["text"].upper()})
ctx = get_default_zephyr_context()
results = list(ctx.execute(ds))
```

**Attempt 1**: Failed ‚Äî `zephyr.cli` treats first arg as script file, `tokenize` is not a file.
Zephyr CLI: `python -m zephyr.cli [options] <script.py> [script args]`

**Attempt 2**: Failed ‚Äî `TypeError: 'Dataset' object is not iterable`
Must use `ctx.execute(ds)` not `list(ds)`.

**Attempt 3**: Failed ‚Äî `ModuleNotFoundError: No module named 'test_zephyr_iris'`
The `upper()` function defined in the script gets cloudpickled and sent to the coordinator.
The coordinator (separate Iris job) doesn't have the `test_zephyr_iris` module.
**Fix**: Use inline lambda instead of named function.

**Attempt 4**: ‚úÖ Coordinator and workers discovered, ActorService calls working!
```
HTTP Request: POST http://169.254.123.5:30001/iris.actor.ActorService/Call "HTTP/1.1 200 OK"
HTTP Request: POST http://169.254.123.6:30002/iris.actor.ActorService/Call "HTTP/1.1 200 OK"
HTTP Request: POST http://169.254.123.7:30004/iris.actor.ActorService/Call "HTTP/1.1 200 OK"
Starting stage stage0-Map with 10 tasks
```
But failed: `FileNotFoundError: /tmp/zephyr/cdd4e2f4dbc7/source/shard-0001/chunk-0000.pkl`

### Fix 11: Chunk Storage Must Be GCS for Distributed Backends

**Root Cause**: `ZephyrContext.chunk_storage_prefix` defaults to `/tmp/zephyr`. The parent job
writes chunk files to local `/tmp`, but workers are in separate containers on different machines.
Workers can't read files from the parent's `/tmp`.

**Fix**: In `cli.py:run_local()`, when `FRAY_CLIENT_SPEC` starts with `iris://` or `ray`, set
`chunk_storage_prefix` to a GCS path (`$MARIN_PREFIX/tmp/zephyr` or default
`gs://marin-us-central2/scratch/tmp/zephyr`).

**File**: `lib/zephyr/src/zephyr/cli.py`

### Debug Logging Added

Added debug logging to `IrisActorGroup.wait_ready()` in `iris_backend.py`:
- Logs actor_name, job_id, resolve_result, and all registered endpoints
- On timeout, dumps full endpoint state for diagnosis

### Current Status

- ‚úÖ Actor creation and discovery: WORKING
- ‚úÖ Coordinator + worker creation: WORKING
- ‚úÖ Actor RPC calls: WORKING
- ‚úÖ Pipeline stage submission: WORKING
- ‚úÖ GCS chunk storage: WORKING (Fix 11)
- ‚úÖ **FULL PIPELINE SUCCESS**: 10/10 records processed correctly
- Job: `iris-run-power-test_zephyr_iris-20260131-223214`
- Known issue: parent job reports FAILED (state 4) because orphaned actor jobs remain
- Next: test actual tokenization pipeline, fix orphaned job cleanup

### Session 4b: Repeated Success + Engineering Improvements (2026-01-31)

Resubmitted `test_zephyr_iris.py` ‚Äî confirmed reproducible success:
- Job: `iris-run-power-test_zephyr_iris-20260131-224502`
- 10/10 records processed, `HELLO-0` through `HELLO-9`
- Parent still reports state 4 (FAILED) due to orphaned actor jobs

**In-progress fixes (senior engineer agent):**

1. **Continuous worker discovery**: Replace blocking `wait_ready(count=N)` with
   a model where the coordinator continuously probes for new workers and adds
   work to them as they appear. This removes the hard timeout and lets processing
   start as soon as ANY worker is ready.

2. **Orphaned actor job cleanup**: Parent should cancel child actor jobs on exit
   (normal or exception) so the job reports SUCCEEDED. Likely via context manager
   or atexit on `FrayIrisClient` / `IrisActorGroup`.

### Correction: State 4 = SUCCEEDED, Not FAILED

Previous analysis incorrectly assumed state 4 meant FAILED. From `cluster.proto`:
```
JOB_STATE_SUCCEEDED = 4
JOB_STATE_FAILED = 5
```
The test pipeline jobs (state 4) were SUCCEEDING all along. The `iris-run` exit code
is 0 for state 4. The earlier tokenization attempt (state 5) genuinely failed because
it used wrong CLI syntax (`zephyr.cli tokenize` instead of a script file).

### Status: Simple Pipeline Works, Real Tokenization Not Yet Tested

The `.upper()` map pipeline works reliably on Iris. The actual tokenization pipeline
has not been tested yet ‚Äî it requires a proper tokenization script that imports the
tokenizer and runs it through Zephyr's map/reduce stages.

Key difference between test and real tokenization:
- Test: 10 records, simple lambda, no external deps
- Tokenization: millions of records, HuggingFace tokenizer, streaming from GCS,
  writing tokenized output back to GCS. May hit: pickle issues with tokenizer objects,
  memory pressure, GCS I/O bottlenecks, worker crashes under load.

### Session 4c: Test Validation + Tokenization (2026-01-31)

**Iris local test timeout is PRE-EXISTING**: Verified by running `test_simple_map[iris]`
on commit 414e1e31c (before any session 4 changes) ‚Äî same 300s timeout. The local iris
backend hangs during task execution, not during `wait_ready()`. Unrelated to our changes.

**Local tests pass**: All 15 `test_execution.py[local]` tests pass in 10s.

**Reverted `wait_ready(count=1)`**: The optimization to start with 1 worker didn't work
for single-stage pipelines since `_discover_workers()` only runs between stages.
Reverted to `wait_ready()` (all workers). `discover_new()` API kept for future use.

**Changes committed**:
- `discover_new()` on ActorGroup/IrisActorGroup (non-blocking worker discovery)
- `_discover_workers()` in ZephyrContext (discovers workers between stages)
- atexit handler for orphaned actor cleanup
- GCS chunk storage for distributed backends

### Tokenization Attempt 1: `HfTokenizeConfig` ‚Äî FAILED (torch CUDA import)

Job: `iris-run-power-test_tokenize_iris-20260131-232206` (state 5 = FAILED)

**Error**: `ValueError: libcublas.so.*[0-9] not found`
The import chain: `marin.processing.tokenize.__init__` ‚Üí `data_configs.py` ‚Üí
`import transformers` ‚Üí `import torch` ‚Üí tries to load CUDA libs ‚Üí fails.

**Root Cause**: The venv has CUDA torch installed (default). Even with `--extra marin:cpu`,
the `uv sync` in Docker might resolve to CUDA torch due to the lock file having CUDA
variants pinned. The `cpu` extra changes `torch` to use the `pytorch-cpu` index, but
the UV cache mount (`iris-uv-global`) may have cached CUDA wheels.

**Workaround**: Use `tiktoken` (no torch dependency) instead of `transformers.AutoTokenizer`
for the tokenization test. This avoids the torch import entirely.

### Tokenization Attempt 2: `tiktoken` ‚Äî FAILED (not installed)

Job: `iris-run-power-test_tokenize_iris-20260131-232804` (state 5)
`ModuleNotFoundError: No module named 'tiktoken'` ‚Äî not a dependency.

### Tokenization Attempt 3: Named function ‚Äî FAILED (pickle issue)

Job: `iris-run-power-test_tokenize_iris-20260131-233233` (state 5)
`ModuleNotFoundError: No module named 'test_tokenize_iris'`
Same issue from session 4: named functions in the script module get cloudpickled
with a reference to the script's module name. The coordinator (separate Iris job)
doesn't have that module. Must use lambdas.

### Tokenization Attempt 4: Lambda + shard_ctx ‚Äî SUCCESS ‚úÖ

Job: `iris-run-power-test_tokenize_iris-20260131-233848` (state 4 = SUCCEEDED)
- 5/5 records tokenized correctly
- Shared vocabulary broadcast via `ctx.put("vocab", vocab)`
- Workers access via `shard_ctx().get_shared("vocab")`
- All token IDs correct

This validates the full tokenization flow: shared state + distributed map.
The only remaining blocker for real tokenization is the torch CUDA import issue.

| # | Problem | Fix | File |
|---|---------|-----|------|
| 12 | torch CUDA import on CPU workers | Need CPU torch in Docker | pyproject.toml / builder.py |
| 13 | Named functions fail pickle on workers | Use lambdas or library functions | user scripts |

### Iris Backend Test: FIXED ‚úÖ

| # | Problem | Fix | File |
|---|---------|-----|------|
| 14 | IrisActorHandle can't resolve in RPC threads | Store controller_address for fallback | iris_backend.py |

**Root Cause**: `IrisActorHandle` pickled to workers loses `iris_ctx()` ContextVar
in RPC dispatch threads (threads don't inherit ContextVars). `_resolve()` failed with
`RuntimeError("No client available in context")`, killing workers silently.

**Fix**: Store `controller_address` in handle state (`__getstate__`/`__setstate__`).
In `_resolve()`, when no iris context available, create lightweight `IrisClient` from
`controller_address` + `job_id` to build a resolver.

**Result**: All 15 iris tests pass in 58s (was 300s+ timeout). Local tests unaffected.

### Current Status Summary

- ‚úÖ Simple pipeline on Iris cluster: WORKING (10/10 records)
- ‚úÖ Tokenization with shard_ctx: WORKING (5/5 records, shared vocab)
- ‚úÖ Local tests: 15/15 pass (11s)
- ‚úÖ Iris tests: 15/15 pass (58s)
- ‚ùå Real tokenization (transformers.AutoTokenizer): Blocked by torch CUDA import
- ‚ùå Ray tests: Separately broken, not our concern

**Next**: Fix torch CUDA import to enable real tokenization with `transformers`.

### Architecture Notes

The Zephyr-on-Iris execution flow:
1. `iris-run --config cluster.yaml -- uv run python -m zephyr.cli script.py`
2. iris-run submits parent job to Iris controller
3. Parent job builds Docker image, starts container
4. Container has `FRAY_CLIENT_SPEC=iris://controller:port` (injected by build_iris_env)
5. `python -m zephyr.cli` ‚Üí `run_local()` ‚Üí `current_client()` ‚Üí `FrayIrisClient`
6. `ZephyrContext` creates coordinator actor group (1 job) and worker actor group (N jobs)
7. Each actor job builds its own Docker image and starts an `ActorServer`
8. Actors register endpoints via `iris_ctx().registry.register()`
9. Parent discovers actors via `IrisActorGroup.wait_ready()` ‚Üí `NamespacedResolver.resolve()`
10. Pipeline stages are submitted to coordinator, which distributes to workers

### Key Files Changed (This Session)

- `lib/fray/src/fray/v2/iris_backend.py`: Added debug logging to `wait_ready()`
- `lib/zephyr/src/zephyr/cli.py`: GCS chunk storage for distributed backends (Fix 11)
- `experiments/test_iris_actors.py`: Minimal actor test (created)
- `experiments/test_zephyr_iris.py`: Zephyr pipeline test (created)

### Cluster Management

```bash
# Start cluster
.venv/bin/iris cluster --config lib/iris/examples/eu-west4.yaml start

# List jobs
.venv/bin/python lib/iris/scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models list-jobs

# View task logs
.venv/bin/python lib/iris/scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models show-task-logs <JOB_ID>

# Clean up (remove all VMs)
.venv/bin/python lib/iris/scripts/cluster-tools.py --zone europe-west4-b --project hai-gcp-models cleanup --no-dry-run

# Submit test
.venv/bin/iris-run --config lib/iris/examples/eu-west4.yaml -- uv run python -m zephyr.cli --max-parallelism 2 experiments/test_zephyr_iris.py
```

## Session 5: Full Tokenization Pipeline (2026-02-01)

### Codebase Summary: Critical Components for Debugging

**Docker Customization Flow (NEW as of this session):**
- Zephyr CLI (`cli.py:269`) hardcodes `--extra marin:cpu` for Iris submissions
- `iris_run.py` receives extras CLI flag, creates `EnvironmentSpec(extras=["marin:cpu"])`
- `EnvironmentSpec.to_proto()` calls `generate_dockerfile(extras=["marin:cpu"])`
- `generate_dockerfile()` builds Dockerfile with `uv sync --extra marin:cpu`
- Full Dockerfile sent to Iris controller via protobuf
- Worker builds Docker image with CPU-only PyTorch dependencies
- **This means we no longer need to restart cluster to change Docker builds**

**Zephyr-on-Iris Execution Model:**
1. User runs: `uv run zephyr --backend=iris --cluster=us-central2 script.py`
2. Zephyr CLI launches `iris-run` with `--extra marin:cpu` flag
3. iris-run submits parent job to Iris controller
4. Parent job builds Docker image (with CPU extras), starts container
5. Container has `FRAY_CLIENT_SPEC=iris://controller:port` (injected)
6. `python -m zephyr.cli` ‚Üí `run_local()` ‚Üí `current_client()` ‚Üí `FrayIrisClient`
7. `ZephyrContext` creates:
   - Coordinator actor group (1 job)
   - Worker actor group (N jobs)
8. Each actor job builds its own Docker image, starts `ActorServer`
9. Actors register endpoints via `iris_ctx().registry.register()`
10. Parent discovers actors via `IrisActorGroup.wait_ready()` ‚Üí `NamespacedResolver.resolve()`
11. Pipeline stages submitted to coordinator, distributed to workers

**Key Files:**
- `lib/zephyr/src/zephyr/cli.py` - Entry point, sets `--extra marin:cpu`
- `lib/iris/src/iris/cluster/types.py` - `generate_dockerfile()`, extras ‚Üí Dockerfile template
- `lib/iris/src/iris/cluster/client/remote_client.py` - Job submission
- `lib/fray/src/fray/v2/iris_backend.py` - Fray‚ÜíIris EnvironmentConfig conversion
- `lib/zephyr/src/zephyr/execution.py` - ZephyrContext, coordinator/worker creation
- `lib/iris/src/iris/cluster/worker/builder.py` - Docker image build
- `lib/iris/src/iris/cluster/worker/task_attempt.py` - `FRAY_CLIENT_SPEC` injection

**Known Issues from Previous Sessions:**
- ‚úÖ Fixed: Docker build source availability (Fix #1)
- ‚úÖ Fixed: PyTorch CUDA on CPU workers via `--extra marin:cpu` (Fix #2)
- ‚úÖ Fixed: Workers run in-process ‚Üí Iris jobs via `FRAY_CLIENT_SPEC` (Fix #7)
- ‚úÖ Fixed: GCS chunk storage for distributed backends (Fix #11)
- ‚úÖ Fixed: IrisActorHandle resolution in RPC threads (Fix #14)
- ‚ùå Unresolved: torch CUDA import when using `transformers.AutoTokenizer` (Fix #12)

**Current Goal:**
Run full tokenization pipeline:
```bash
uv run zephyr --max-parallelism=64 --cluster=us-central2 \
  lib/marin/src/marin/processing/tokenize/tokenize.py \
  --train_paths '["gs://marin-us-central2/raw/ar5iv/ar5iv-04-2024-no-problem-d1522a/99*.jsonl.gz"]' \
  --cache_path gs://marin-us-central2/tmp/zephyr/test-tokenize \
  --tokenizer gpt2 \
  --window_size_bytes=100000000 \
  --validation_paths '[]' \
  --format.text_key content
```

### Hypothesis 1: Docker Customization Enables CPU-Only Torch

With the new Docker customization, `--extra marin:cpu` should now work properly.
The previous issue (Fix #12) was that torch CUDA was getting installed despite the CPU extra.
Now that extras are passed through the full chain (Zephyr CLI ‚Üí iris_run ‚Üí EnvironmentSpec ‚Üí Dockerfile),
the Docker image should have CPU-only torch.

### Changes to Make

1. Restart Iris cluster in eu-west4
2. Submit tokenization job
3. Monitor logs for torch CUDA import errors
4. If still fails, investigate pyproject.toml extras definition

### Test Run 1: Restart Cluster and Submit Job

**Cluster restarted successfully**: http://10.164.0.3:10000 (europe-west4-b)

**IMPORTANT FIX**: Initial attempt used `--cluster=eu-west4` which defaults to Ray backend!
Correct syntax for Iris:
```bash
uv run zephyr --backend=iris --cluster=lib/iris/examples/eu-west4.yaml [...]
```

NOT:
```bash
uv run zephyr --cluster=eu-west4 [...]  # This uses Ray!
```

**Correct job submission**:
```bash
uv run zephyr --backend=iris --cluster=lib/iris/examples/eu-west4.yaml \
  --max-parallelism=64 \
  lib/marin/src/marin/processing/tokenize/tokenize.py \
  --train_paths '["gs://marin-us-central2/raw/ar5iv/ar5iv-04-2024-no-problem-d1522a/99*.jsonl.gz"]' \
  --cache_path gs://marin-us-central2/tmp/zephyr/test-tokenize \
  --tokenizer gpt2 \
  --window_size_bytes=100000000 \
  --validation_paths '[]' \
  --format.text_key content
```

Job ID: `iris-run-power-tokenize-20260201-193531`
Controller: `http://localhost:10000` (via SSH tunnel)

### Results

**FAILED**: Docker build error!
```
error: invalid value 'marin:cpu' for '--extra <EXTRA>': Extra names must start and end with a letter or digit and may only contain -, _, ., and alphanumeric characters
```

**Root Cause**: The Dockerfile template generated `uv sync --extra marin:cpu`, but `uv` doesn't accept the `package:extra` syntax in the `--extra` flag. The correct syntax should be `uv sync --package marin --extra cpu`.

**Fix #15**: Parse `package:extra` syntax in `generate_dockerfile()`

| # | Problem | Fix | File |
|---|---------|-----|------|
| 15 | `uv sync --extra marin:cpu` invalid syntax | Parse "package:extra" ‚Üí "--package package --extra extra" | types.py:314-336 |

**File**: `lib/iris/src/iris/cluster/types.py` lines 314-336

**Change**: Modified `generate_dockerfile()` to parse extras:
- If extra contains `:`, split into package and extra: `"marin:cpu"` ‚Üí `"--package marin --extra cpu"`
- Otherwise, use as-is: `"cpu"` ‚Üí `"--extra cpu"`

This aligns with the comment in `zephyr/cli.py:268` which said `"marin:cpu" tells the builder to pass --package marin --extra cpu to uv sync`.

### Test Run 2: Retry with Fix (eu-west4 data path)

**Job ID**: `iris-run-power-tokenize-20260201-194055`
**Command**:
```bash
uv run zephyr --backend=iris --cluster=lib/iris/examples/eu-west4.yaml \
  --max-parallelism=64 \
  lib/marin/src/marin/processing/tokenize/tokenize.py \
  --train_paths '["gs://marin-eu-west4/raw/ar5iv/ar5iv-04-2024-no-problem-d1522a/99*.jsonl.gz"]' \
  --cache_path gs://marin-us-central2/tmp/zephyr/test-tokenize \
  --tokenizer gpt2 --window_size_bytes=100000000 \
  --validation_paths '[]' --format.text_key content
```

**Results** (in progress):
- ‚úÖ Docker build succeeded with `uv sync --package marin --extra cpu`
- ‚úÖ Parent job: RUNNING
- ‚úÖ Coordinator job (`zephyr-controller-17b37a1b-0`): RUNNING
- ‚úÖ Worker jobs: 2 running, ~60 building (waiting for Docker image builds)
- üîÑ Parent job polling for worker endpoints (waiting for all 64 workers to register)

**Status**: Workers are building their Docker images with CPU-only torch. Once all workers register their endpoints, the tokenization pipeline should start executing.

### Key Observations

The fix for `package:extra` syntax parsing is working correctly:
- Dockerfile generates: `uv sync --package marin --extra cpu`
- Workers download CPU torch (175.9MiB) instead of CUDA torch
- No torch CUDA import errors during build

Next step: Wait for workers to complete building and verify the tokenization pipeline runs successfully without torch CUDA errors.

### Test Run 3: 4 Workers (Simplified)

**Job ID**: `iris-run-power-tokenize-20260201-195216`
**Changes**:
- Reduced to 4 workers (`--max-parallelism=4`) for faster iteration
- Silenced httpx logs: `logging.getLogger("httpx").setLevel(logging.WARNING)` in cli.py

**Results**:
- ‚úÖ Docker build succeeded with CPU torch (`torch==2.9.0+cpu`, `torchvision==0.24.0+cpu`)
- ‚úÖ Parent job started, coordinator discovered (1/1 ready)
- ‚ùå **NEW ISSUE**: Connection refused when calling coordinator

**Error**:
```
connectrpc.errors.ConnectError: [Errno 111] Connection refused
```

Stack trace shows failure at: `coordinator.set_chunk_config.remote()` ‚Üí `ActorClient.call()` ‚Üí connection refused

**Root Cause** (hypothesis): The coordinator actor's RPC endpoint is registered but not yet accepting connections, or there's a networking/port accessibility issue between the parent job container and the coordinator job container.

## Summary of Session 5

### Fixes Applied

| # | Problem | Fix | Status | File |
|---|---------|-----|--------|------|
| 15 | `uv sync --extra marin:cpu` invalid syntax | Parse "package:extra" ‚Üí "--package package --extra extra" | ‚úÖ FIXED | types.py |
| 16 | Noisy httpx logs | `logging.getLogger("httpx").setLevel(logging.WARNING)` | ‚úÖ FIXED | cli.py |

### Verified Working

1. ‚úÖ Docker customization flow: `--extra marin:cpu` ‚Üí `--package marin --extra cpu`
2. ‚úÖ CPU-only torch installation (no CUDA dependencies)
3. ‚úÖ Actor discovery (coordinator found and registered)
4. ‚úÖ Multiple worker jobs created and built successfully

### Outstanding Issues

1. ‚ùå **Connection refused when calling coordinator actor** (new issue in Test Run 3)
   - Coordinator endpoint discovered but RPC calls fail
   - Needs investigation: networking, port accessibility, or timing issue

2. ‚ö†Ô∏è **Design question**: Workers as separate jobs vs replicas
   - Currently: 1 coordinator job + N worker jobs (1 replica each)
   - Alternative: 1 coordinator job + 1 worker job (N replicas)
   - User asked if this is intentional

3. ‚ö†Ô∏è **Incomplete**: Actual tokenization pipeline not yet tested
   - Got past Docker build and actor discovery
   - Need to resolve connection issue to see full pipeline execution

## Sessions 6-7: Multi-Replica, --network=host, Dockerfile Inheritance (2026-02-01)

### Fixes Applied (Sessions 6-7)

| # | Problem | Fix | File |
|---|---------|-----|------|
| 17 | N separate worker jobs ‚Üí overhead | Single N-replica job | iris_backend.py |
| 18 | All workers register as `-0` | Use `get_job_info().task_index` | iris_backend.py |
| 19 | `discover_new()` IndexError | Use `self._jobs[0]` for single multi-replica job | iris_backend.py |
| 20 | Coordinator OOM (128MB) | RAM 128MB ‚Üí 2GB | execution.py |
| 21 | Wrong endpoint address (Docker bridge) | Use `job_info.advertise_host` | iris_backend.py |
| 22 | Docker bridge networking unreachable | `--network=host` + `IRIS_ADVERTISE_HOST` | docker.py, task_attempt.py |
| 23 | Child jobs get CUDA torch (no extras) | Inherit parent's dockerfile via `IRIS_DOCKERFILE` | job_info.py, task_attempt.py, iris_backend.py |

### Rebase onto upstream/main

Rebased onto 3 upstream commits:
- `715ccdb0a` ‚Äî Remove `FRAY_CLIENT_SPEC`, use auto-detection (#2605)
- `66ea12307` ‚Äî Migrate to type-safe time primitives (#2599)
- `635671e08` ‚Äî Disable httpx logging by default (#2607)

**Architecture preserved**: Client-side dockerfile generation stays in `types.py`.
Flow: `EnvironmentSpec.to_proto()` ‚Üí `generate_dockerfile(extras=...)` ‚Üí dockerfile
in proto ‚Üí worker uses it as-is. Worker falls back to `generate_dockerfile()` only
if no dockerfile in proto.

### Test Results (post-rebase)

- ‚úÖ Iris tests: 478 passed, 2 skipped (71s)
- ‚úÖ Zephyr local tests: 15 passed (10.8s)
- ‚ö†Ô∏è Zephyr iris tests: Senior engineer investigating hang

### Tokenization Test Run 5 (post-rebase)

Submitting to eu-west4 with dockerfile inheritance fix.
This should resolve the `libcublas.so not found` error ‚Äî child jobs will
inherit the parent's CPU-only dockerfile via `IRIS_DOCKERFILE` env var.

### Tokenization Test Run 6 (current_client fix)

**Job**: `iris-run-power-tokenize-20260202-014036`

**Observation**: Parent job built successfully with CPU torch (`torch==2.9.0+cpu`).
Container started and ran `python -m zephyr.cli --backend local ...`.

**Problem**: `current_client()` returned `LocalClient` instead of `FrayIrisClient`.
The container has `IRIS_JOB_ID` env var set, but the old `current_client()` only checked
`get_iris_ctx()` (ContextVar ‚Äî None inside fresh container). Fixed in session 7 to also
check `IRIS_JOB_ID` env var and call `iris_ctx()`.

However, the fix didn't trigger because the chunk storage detection in `cli.py` was
still checking the removed `FRAY_CLIENT_SPEC` env var:
```python
fray_spec = os.environ.get("FRAY_CLIENT_SPEC", "")  # always "" now!
if fray_spec.startswith("iris://"):  # never true
```

### Fix 24: Chunk Storage Detection Uses Client Type

| # | Problem | Fix | File |
|---|---------|-----|------|
| 24 | Chunk storage detection uses removed `FRAY_CLIENT_SPEC` | Check `isinstance(client, LocalClient)` instead | cli.py |

**Change**: `cli.py:run_local()` now checks `not isinstance(client, LocalClient)` to
determine if running on a distributed backend (iris or ray). Also cleaned up the stale
`FRAY_CLIENT_SPEC` log message.

### Zephyr Iris Tests: FIXED

Senior engineer added missing `fray_client` and `ctx` fixtures to `conftest.py`.
- 16 passed, 15 iris tests properly skipped (no local controller)
- Tests now parametrize over `local`/`iris`/`ray` backends

### Current Status

- ‚úÖ Iris tests: 478 passed
- ‚úÖ Zephyr local tests: 15 passed
- ‚úÖ Zephyr iris tests: 16 passed (iris skipped when no controller)
- üîÑ Tokenization: Needs resubmit with Fix 24 (chunk storage detection)

### Fix 25: Controller crash ‚Äî `float + Duration` type mismatch

The cluster restart after rebase failed: controller container crashed with
`unsupported operand type(s) for +: 'float' and 'Duration'`.

**Root cause**: Upstream commit `66ea12307` migrated to type-safe time primitives,
but `SshConnection.run()` takes `timeout: int`, while callers passed
`Duration.from_seconds(N)`. When `subprocess.run(..., timeout=Duration_object)`
executes, Python does `time.monotonic() + Duration_object` ‚Üí TypeError.

**Affected files**:
- `lib/iris/src/iris/cluster/vm/controller.py` ‚Äî 5 instances
- `lib/iris/src/iris/cluster/vm/managed_vm.py` ‚Äî 3 instances
- `lib/iris/src/iris/cluster/vm/manual_platform.py` ‚Äî 1 instance

**Fix**: Replace `Duration.from_seconds(N)` with plain `int` N for all
`conn.run(timeout=...)` calls. Senior engineer auditing remaining raw time
usages and adding regression tests.

| # | Problem | Fix | File |
|---|---------|-----|------|
| 25 | Controller crash: `float + Duration` TypeError | Use int timeout for `conn.run()` | controller.py, managed_vm.py, manual_platform.py |

Also added `try/except` with `logger.exception()` to `_run_scheduling_loop`
and `_run_heartbeat_loop` in controller.py so daemon thread crashes produce
stack traces.

### Fix 26: Child jobs always get a dockerfile

`create_actor_group()` was passing `environment=None` when no parent
dockerfile available. The deployed worker raised `RuntimeError: No dockerfile
in environment config`. Fixed to always generate a default dockerfile.

| # | Problem | Fix | File |
|---|---------|-----|------|
| 26 | Child jobs get no dockerfile ‚Üí worker crash | Always generate dockerfile in `create_actor_group()` | iris_backend.py |

### Current Status

- ‚úÖ Iris tests: running (pending Duration fix validation)
- ‚úÖ Zephyr local tests: 15 passed
- ‚úÖ Zephyr iris tests: 16 passed (iris skipped when no controller)
- üîÑ Cluster restart: in progress with Duration fix
- üîÑ Senior engineer: auditing Duration type safety, adding tests

### Fix 27: Log streaming crash on ConnectError

Log streaming in `client.py` caught only `ValueError` but the controller returned
`ConnectError: Task has no assigned worker` when a task wasn't yet scheduled.
Broadened to `except Exception` in both the polling loop and final drain.

| # | Problem | Fix | File |
|---|---------|-----|------|
| 27 | Log streaming crash: `ConnectError: Task has no assigned worker` | Broaden exception catch in log polling | client.py |

### Fix 25b: Senior Engineer ‚Äî Duration type safety audit

Updated `SshConnection.run()` protocol and all implementations to use `Duration`
natively instead of `int`. All callers now pass `Duration.from_seconds(N)`.
Added regression tests in `lib/iris/tests/cluster/vm/test_ssh.py`.

- 454 passed, 1 skipped (pre-existing `test_builder` failure)

### Tokenization SUCCESS ‚úÖ

**Job**: `iris-run-power-tokenize-20260202-025733`

The full tokenization pipeline completed successfully on Iris eu-west4 cluster:
1. ‚úÖ Docker build: CPU torch (`torch==2.9.0+cpu`)
2. ‚úÖ `current_client()` ‚Üí `FrayIrisClient` (Fix 24: not LocalClient)
3. ‚úÖ GCS chunk storage: `gs://marin-us-central2/scratch/tmp/zephyr`
4. ‚úÖ Child jobs get dockerfile (Fix 26: always generate)
5. ‚úÖ Coordinator discovered (1/1 ready)
6. ‚úÖ 4 workers discovered (4/4 ready)
7. ‚úÖ Map stage: 12 tasks ‚Üí 4 groups
8. ‚úÖ Write stage: 4 tasks
9. ‚úÖ Output: `gs://marin-eu-west4/tmp/zephyr/test-tokenize/train/` (4 parts + input_ids)

Worker logs show:
- Shard 0: ~15 min to tokenize
- Shard 2: ~12 min to tokenize
- Shard 3: ~4 min to tokenize
- All shards completed and written to GCS

### Final Status

All 27 fixes applied. Zephyr-on-Iris tokenization pipeline works end-to-end.

| Component | Status |
|-----------|--------|
| Docker build (CPU torch) | ‚úÖ |
| Client auto-detection | ‚úÖ |
| GCS chunk storage | ‚úÖ |
| Dockerfile inheritance | ‚úÖ |
| Actor discovery | ‚úÖ |
| Pipeline execution | ‚úÖ |
| Tokenization output | ‚úÖ |
| Duration type safety | ‚úÖ |
| Controller crash logging | ‚úÖ |
