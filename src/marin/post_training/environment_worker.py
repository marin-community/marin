# Copyright 2025 The Marin Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Inference worker for RL/post-training rollout generation.

This worker loads model checkpoints, generates rollouts from a single environment,
and writes the rollout data to files for training workers to consume.
"""

import logging
import os
import socket
import time

import jax

from marin.post_training.inference_server import InferenceServer

from .rl_dataset import create_dataset_from_environment
from .rollout_storage import RolloutBatch, RolloutWriter, TaggedRolloutBatch
from .training_config import TrainingConfig
from .utils import (
    jax_distributed_barrier,
)

logger = logging.getLogger(__name__)


class InferenceWorker:
    """Asynchonous inference & rollout worker for RL training.

    Inference workers periodically load model checkpoints generated by the training job,
    and continously generate rollouts from a single environment. Rollouts are communicated to the
    training job via a rollout queue.
    """

    _running: bool = True
    training_config: TrainingConfig
    rollout_writer: RolloutWriter

    def __init__(
        self,
        training_config: TrainingConfig,
        inference_server: InferenceServer,
        environment_spec: str,
        rollout_writer: RolloutWriter,
        rollout_batch_size: int = 32,
        max_rollouts: int | None = None,
        coordinator=None,
    ):
        """Initialize inference worker.

        Args:
            training_config: Training configuration.
            environment_spec: Environment specification string.
            rollout_writer: Writer for rollout output.
            rollout_batch_size: Size of rollout batches.
            max_rollouts: Maximum number of rollouts to generate. None for unlimited.
            coordinator: Coordinator for weight transfer (required for RAY_REMOTING and JAX_TRANSFER_SERVER modes).
        """
        self.training_config = training_config
        self.environment_spec = environment_spec
        self.rollout_batch_size = rollout_batch_size
        self.max_rollouts = max_rollouts
        self.coordinator = coordinator

    def stop(self):
        """Stop the inference worker loop."""
        self._running = False

    def _generate_rollout_batch(self, rng) -> tuple[list[dict], dict]:
        """Generate a set of rollout batches from the environment."""
        # Create RL dataset from environment
        jax_distributed_barrier()
        logger.info("Current params: %s", type(self.current_params))
        logger.info("Reference params: %s", type(self.reference_params))
        rl_dataset, dataset_metrics = create_dataset_from_environment(
            environment=self.environment,
            sampler=self.sampler,
            params=self.current_params,
            reference_params=self.reference_params,
            get_logprobs_fn=self.get_logprobs,
            n_examples=self.training_config.hyperparameters.n_prompts_per_step,
            n_generations=self.training_config.generation_config.n_generations,
            prng_key=rng,
            reference_logprobs_bsize=self.reference_logprobs_bsize,
            max_input_length=self.max_input_length,
            max_output_length=self.max_output_length,
            pad_token_id=self.pad_token_id,
            tokenizer=self.tokenizer,
            mode="train",
        )
        jax_distributed_barrier()

        return (
            list(rl_dataset.iterate_batches(batch_size=self.rollout_batch_size, shuffle=True, loop=False)),
            dataset_metrics,
        )

    def run(self):
        """Main inference worker loop."""
        logger.info("Starting inference worker...")

        rollouts_generated = 0
        last_checkpoint_check = 0

        self.reference_params = self._load_reference_params()
        self.current_params = self._load_model_params()

        self._initialize_weight_transfer()

        step = 0

        rng = jax.random.PRNGKey(0)

        while self._running:
            jax_distributed_barrier()
            # Check for new weights periodically using weight transfer manager
            current_time = time.time()
            if current_time - last_checkpoint_check >= self.training_config.weight_transfer.poll_interval_seconds:
                self._check_for_new_weights()
                last_checkpoint_check = current_time

            if self.max_rollouts is not None and rollouts_generated >= self.max_rollouts:
                logger.info(f"Reached max rollouts ({self.max_rollouts}), stopping")
                break

            rng, input_rng = jax.random.split(rng)
            rollout_batches, metrics = self._generate_rollout_batch(input_rng)
            for batch_data in rollout_batches:
                step += 1

                if self.training_config.logging.log_freq > 0 and step % self.training_config.logging.log_freq == 0:
                    log_metrics = {"step": step}
                    log_metrics.update(jax.device_get(metrics))
                    log_metrics.update(self.weight_transfer_client.get_metrics())
                    log_metrics = {"inference." + k: v for k, v in log_metrics.items()}
                    logger.info(f"Logging metrics at step {step}... {log_metrics}")

                rollout_batch = TaggedRolloutBatch(
                    batch=RolloutBatch(
                        input_ids=batch_data["input_ids"],
                        attention_mask=batch_data["attention_mask"],
                        position_ids=batch_data["position_ids"],
                        target_ids=batch_data["target_ids"],
                        loss_weights=batch_data["loss_weights"],
                        loss_masks=batch_data["loss_masks"],
                        reference_logprobs=batch_data["reference_logprobs"],
                        policy_logprobs=batch_data["policy_logprobs"],
                    ),
                    env_name=self.environment_name,
                    worker_id=f"{socket.gethostname()}_{os.getpid()}",
                    timestamp=time.time(),
                    rollout_id=f"{socket.gethostname()}_{int(time.time() * 1000000)}_{step}",
                )
                self.rollout_writer.write_batch(rollout_batch)
                rollouts_generated += 1
            logger.info(f"Generating rollout batch {rollouts_generated}")

        self.weight_transfer_client.cleanup()
        logger.info(f"Inference worker completed after generating {rollouts_generated} rollouts")
        jax_distributed_barrier()
